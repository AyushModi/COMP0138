{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tADNkHDwIsytxuQ1KEkkohn3V4P2MrQA","timestamp":1679964065035}],"authorship_tag":"ABX9TyOEAd+eC7mJuxGB3OBxssX0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUOk5e2ZTBSA","executionInfo":{"status":"ok","timestamp":1683590113965,"user_tz":-60,"elapsed":32977,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"f0b1c978-6c9c-4fa0-8d1e-02fbd8fc37d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Question Generation/vae/'\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jw-3YrgUop3","executionInfo":{"status":"ok","timestamp":1683590115488,"user_tz":-60,"elapsed":1527,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"7d0e279b-c59a-4ad2-e6a8-821d9e3bbaa1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Question Generation/vae\n","/content/drive/MyDrive/Question Generation/vae\n"]}]},{"cell_type":"code","source":["!nvcc --version\n","\n","import torch\n","print(\"\\nPytorch version: \", torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcvum9KrUrLJ","executionInfo":{"status":"ok","timestamp":1683590118661,"user_tz":-60,"elapsed":3182,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"89104292-f729-4f5d-b1c4-25d2738d691e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n","\n","Pytorch version:  2.0.0+cu118\n"]}]},{"cell_type":"code","source":["!pip install -q transformers\n","!pip install -q json-lines\n","## scatter 1.12+cu113\n","# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n","# scatter 1.13+cu116\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n","!pip install -q import-ipynb\n","import import_ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itqzzI9tUtsx","executionInfo":{"status":"ok","timestamp":1683590158876,"user_tz":-60,"elapsed":36309,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"772480d5-6e0a-466a-b239-890cdf925eb4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import argparse\n","import pickle\n","\n","import torch\n","from transformers import BertTokenizer\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np\n","import os\n"],"metadata":{"id":"501buVVNU0PE","executionInfo":{"status":"ok","timestamp":1683590159892,"user_tz":-60,"elapsed":1021,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"eXMtLlEYqkEv"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwBEoCNIU36U","executionInfo":{"status":"ok","timestamp":1683590159893,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"dbf8ae12-d33a-4ea5-8243-cd175dd3b497"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["# Background code"],"metadata":{"id":"kTCpGkTjqn73"}},{"cell_type":"markdown","source":["## Models.py"],"metadata":{"id":"omrYQtk5qv59"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch_scatter import scatter_max\n","from transformers import BertModel, BertTokenizer\n","\n","\n","def return_mask_lengths(ids):\n","    mask = torch.sign(ids).float()\n","    lengths = torch.sum(mask, 1)\n","    return mask, lengths\n","\n","\n","def cal_attn(query, memories, mask):\n","    mask = (1.0 - mask.float()) * -10000.0\n","    attn_logits = torch.matmul(query, memories.transpose(-1, -2).contiguous())\n","    attn_logits = attn_logits + mask\n","    attn_weights = F.softmax(attn_logits, dim=-1)\n","    attn_outputs = torch.matmul(attn_weights, memories)\n","    return attn_outputs, attn_logits\n","\n","\n","def gumbel_softmax(logits, tau=1, hard=False, eps=1e-20, dim=-1):\n","    # type: (Tensor, float, bool, float, int) -> Tensor\n","\n","    gumbels = -(torch.empty_like(logits).exponential_() +\n","                eps).log()  # ~Gumbel(0,1)\n","    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n","    y_soft = gumbels.softmax(dim)\n","\n","    if hard:\n","        # Straight through.\n","        index = y_soft.max(dim, keepdim=True)[1]\n","        y_hard = torch.zeros_like(logits).scatter_(dim, index, 1.0)\n","        ret = y_hard - y_soft.detach() + y_soft\n","    else:\n","        # Re-parametrization trick.\n","        ret = y_soft\n","    return ret\n","\n","\n","class CategoricalKLLoss(nn.Module):\n","    def __init__(self):\n","        super(CategoricalKLLoss, self).__init__()\n","\n","    def forward(self, P, Q):\n","        log_P = P.log()\n","        log_Q = Q.log()\n","        kl = (P * (log_P - log_Q)).sum(dim=-1).sum(dim=-1)\n","        return kl.mean(dim=0)\n","\n","\n","class GaussianKLLoss(nn.Module):\n","    def __init__(self):\n","        super(GaussianKLLoss, self).__init__()\n","\n","    def forward(self, mu1, logvar1, mu2, logvar2):\n","        numerator = logvar1.exp() + torch.pow(mu1 - mu2, 2)\n","        fraction = torch.div(numerator, (logvar2.exp()))\n","        kl = 0.5 * torch.sum(logvar2 - logvar1 + fraction - 1, dim=1)\n","        return kl.mean(dim=0)\n","\n","\n","class Embedding(nn.Module):\n","    def __init__(self, bert_model):\n","        super(Embedding, self).__init__()\n","        bert_embeddings = BertModel.from_pretrained(bert_model).embeddings\n","        self.word_embeddings = bert_embeddings.word_embeddings\n","        self.token_type_embeddings = bert_embeddings.token_type_embeddings\n","        self.position_embeddings = bert_embeddings.position_embeddings\n","        self.LayerNorm = bert_embeddings.LayerNorm\n","        self.dropout = bert_embeddings.dropout\n","\n","    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","        if position_ids is None:\n","            seq_length = input_ids.size(1)\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device)\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","\n","        embeddings = words_embeddings + token_type_embeddings + position_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","\n","        return embeddings\n","\n","\n","class ContextualizedEmbedding(nn.Module):\n","    def __init__(self, bert_model):\n","        super(ContextualizedEmbedding, self).__init__()\n","        bert = BertModel.from_pretrained(bert_model)\n","        self.embedding = bert.embeddings\n","        self.encoder = bert.encoder\n","        self.num_hidden_layers = bert.config.num_hidden_layers\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(\n","            seq_length, dtype=torch.long, device=input_ids.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","\n","        extended_attention_mask = attention_mask.unsqueeze(\n","            1).unsqueeze(2).float()\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","        head_mask = [None] * self.num_hidden_layers\n","\n","        embedding_output = self.embedding(\n","            input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n","        encoder_outputs = self.encoder(embedding_output,\n","                                       extended_attention_mask,\n","                                       head_mask=head_mask)\n","        sequence_output = encoder_outputs[0]\n","\n","        return sequence_output\n","\n","\n","class CustomLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional=False):\n","        super(CustomLSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.bidirectional = bidirectional\n","        self.dropout = nn.Dropout(dropout)\n","        if dropout > 0.0 and num_layers == 1:\n","            dropout = 0.0\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                            num_layers=num_layers, dropout=dropout,\n","                            bidirectional=bidirectional, batch_first=True)\n","\n","    def forward(self, inputs, input_lengths, state=None):\n","        _, total_length, _ = inputs.size()\n","\n","        input_packed = pack_padded_sequence(inputs, input_lengths.cpu(),\n","                                            batch_first=True, enforce_sorted=False)\n","\n","        self.lstm.flatten_parameters()\n","        output_packed, state = self.lstm(input_packed, state)\n","\n","        output = pad_packed_sequence(\n","            output_packed, batch_first=True, total_length=total_length)[0]\n","        output = self.dropout(output)\n","\n","        return output, state\n","\n","\n","class PosteriorEncoder(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 nzqdim, nza, nzadim, nqtypes,\n","                 dropout=0.0):\n","        super(PosteriorEncoder, self).__init__()\n","\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.nlayers = nlayers\n","        self.nzqdim = nzqdim\n","        self.nza = nza\n","        self.nzadim = nzadim\n","        self.nqtypes = nqtypes\n","\n","        self.encoder = CustomLSTM(input_size=emsize,\n","                                  hidden_size=nhidden,\n","                                  num_layers=nlayers,\n","                                  dropout=dropout,\n","                                  bidirectional=True)\n","\n","        \n","\n","        self.question_attention = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.context_attention = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.zq_attention = nn.Linear(nzqdim, 2 * nhidden)\n","\n","        self.zq_linear = nn.Linear(4 * 2 * nhidden + nqtypes, 2 * nzqdim)\n","        self.za_linear = nn.Linear(nzqdim + 2 * 2 * nhidden + nqtypes, nza * nzadim)\n","\n","    def forward(self, c_ids, q_ids, a_ids, qtype_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        q_mask, q_lengths = return_mask_lengths(q_ids)\n","\n","        # question enc\n","        q_embeddings = self.embedding(q_ids)\n","        q_hs, q_state = self.encoder(q_embeddings, q_lengths)\n","        q_h = q_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        q_h = q_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        # context enc\n","        c_embeddings = self.embedding(c_ids)\n","        c_hs, c_state = self.encoder(c_embeddings, c_lengths)\n","        c_h = c_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_h = c_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        # context and answer enc\n","        c_a_embeddings = self.embedding(c_ids, a_ids, None)\n","        c_a_hs, c_a_state = self.encoder(c_a_embeddings, c_lengths)\n","        c_a_h = c_a_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_a_h = c_a_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        # attetion q, c\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_q, _ = cal_attn(self.question_attention(q_h).unsqueeze(1),\n","                                    c_hs,\n","                                    mask)\n","        c_attned_by_q = c_attned_by_q.squeeze(1)\n","\n","        # attetion c, q\n","        mask = q_mask.unsqueeze(1)\n","        q_attned_by_c, _ = cal_attn(self.context_attention(c_h).unsqueeze(1),\n","                                    q_hs,\n","                                    mask)\n","        q_attned_by_c = q_attned_by_c.squeeze(1)\n","\n","        h = torch.cat([q_h, q_attned_by_c, c_h, c_attned_by_q, qtype_ids], dim=-1)\n","\n","        zq_mu, zq_logvar = torch.split(self.zq_linear(h), self.nzqdim, dim=1)\n","        zq = zq_mu + torch.randn_like(zq_mu) * torch.exp(0.5 * zq_logvar)\n","\n","        # attention zq, c_a\n","        mask = c_mask.unsqueeze(1)\n","        c_a_attned_by_zq, _ = cal_attn(self.zq_attention(zq).unsqueeze(1),\n","                                       c_a_hs,\n","                                       mask)\n","        c_a_attned_by_zq = c_a_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([zq, c_a_attned_by_zq, c_a_h, qtype_ids], dim=-1)\n","\n","        za_logits = self.za_linear(h).view(-1, self.nza, self.nzadim)\n","        za_prob = F.softmax(za_logits, dim=-1)\n","        za = gumbel_softmax(za_logits, hard=True)\n","\n","        return zq_mu, zq_logvar, zq, za_prob, za\n","\n","\n","class PriorEncoder(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 nzqdim, nza, nzadim, nqtypes,\n","                 dropout=0):\n","        super(PriorEncoder, self).__init__()\n","\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.nlayers = nlayers\n","        self.nzqdim = nzqdim\n","        self.nza = nza\n","        self.nzadim = nzadim\n","        self.nqtypes = nqtypes\n","\n","        self.context_encoder = CustomLSTM(input_size=emsize,\n","                                          hidden_size=nhidden,\n","                                          num_layers=nlayers,\n","                                          dropout=dropout,\n","                                          bidirectional=True)\n","\n","        self.zq_attention = nn.Linear(nzqdim, 2 * nhidden)\n","\n","        self.zq_linear = nn.Linear(2 * nhidden + nqtypes, 2 * nzqdim)\n","        self.za_linear = nn.Linear(nzqdim + 2 * 2 * nhidden + nqtypes, nza * nzadim)\n","\n","    def forward(self, c_ids, qtype_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","\n","        c_embeddings = self.embedding(c_ids)\n","        c_hs, c_state = self.context_encoder(c_embeddings, c_lengths)\n","        c_h = c_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_h = c_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","        h = torch.cat([c_h, qtype_ids], dim=-1)\n","        zq_mu, zq_logvar = torch.split(self.zq_linear(h), self.nzqdim, dim=1)\n","        zq = zq_mu + torch.randn_like(zq_mu)*torch.exp(0.5*zq_logvar)\n","\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_zq, _ = cal_attn(self.zq_attention(zq).unsqueeze(1),\n","                                     c_hs,\n","                                     mask)\n","        c_attned_by_zq = c_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([zq, c_attned_by_zq, c_h, qtype_ids], dim=-1)\n","\n","        za_logits = self.za_linear(h).view(-1, self.nza, self.nzadim)\n","        za_prob = F.softmax(za_logits, dim=-1)\n","        za = gumbel_softmax(za_logits, hard=True)\n","\n","        return zq_mu, zq_logvar, zq, za_prob, za\n","\n","    def interpolation(self, c_ids, zq, qtype_ids):\n","\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","\n","        c_embeddings = self.embedding(c_ids)\n","        c_hs, c_state = self.context_encoder(c_embeddings, c_lengths)\n","        c_h = c_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_h = c_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_zq, _ = cal_attn(\n","            self.zq_attention(zq).unsqueeze(1), c_hs, mask)\n","        c_attned_by_zq = c_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([zq, c_attned_by_zq, c_h, qtype_ids], dim=-1)\n","\n","        za_logits = self.za_linear(h).view(-1, self.nza, self.nzadim)\n","        za = gumbel_softmax(za_logits, hard=True)\n","\n","        return za\n","\n","\n","class AnswerDecoder(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 dropout=0.0):\n","        super(AnswerDecoder, self).__init__()\n","\n","        self.embedding = embedding\n","\n","        self.context_lstm = CustomLSTM(input_size=4 * emsize,\n","                                       hidden_size=nhidden,\n","                                       num_layers=nlayers,\n","                                       dropout=dropout,\n","                                       bidirectional=True)\n","\n","        self.start_linear = nn.Linear(2 * nhidden, 1)\n","        self.end_linear = nn.Linear(2 * nhidden, 1)\n","        self.ls = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, init_state, c_ids):\n","        _, max_c_len = c_ids.size()\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","\n","        H = self.embedding(c_ids, c_mask)\n","        U = init_state.unsqueeze(1).repeat(1, max_c_len, 1)\n","        G = torch.cat([H, U, H * U, torch.abs(H - U)], dim=-1)\n","        M, _ = self.context_lstm(G, c_lengths)\n","\n","        start_logits = self.start_linear(M).squeeze(-1)\n","        end_logits = self.end_linear(M).squeeze(-1)\n","\n","        start_end_mask = (c_mask == 0)\n","        masked_start_logits = start_logits.masked_fill(\n","            start_end_mask, -10000.0)\n","        masked_end_logits = end_logits.masked_fill(start_end_mask, -10000.0)\n","\n","        return masked_start_logits, masked_end_logits\n","\n","    def generate(self, init_state, c_ids):\n","        start_logits, end_logits = self.forward(init_state, c_ids)\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        batch_size, max_c_len = c_ids.size()\n","\n","        mask = torch.matmul(c_mask.unsqueeze(2).float(),\n","                            c_mask.unsqueeze(1).float())\n","        mask = torch.triu(mask) == 0\n","        score = (self.ls(start_logits).unsqueeze(2)\n","                 + self.ls(end_logits).unsqueeze(1))\n","        score = score.masked_fill(mask, -10000.0)\n","        score, start_positions = score.max(dim=1)\n","        score, end_positions = score.max(dim=1)\n","        start_positions = torch.gather(start_positions,\n","                                       1,\n","                                       end_positions.view(-1, 1)).squeeze(1)\n","\n","        idxes = torch.arange(0, max_c_len, out=torch.LongTensor(max_c_len))\n","        idxes = idxes.unsqueeze(0).to(\n","            start_logits.device).repeat(batch_size, 1)\n","\n","        start_positions = start_positions.unsqueeze(1)\n","        start_mask = (idxes >= start_positions).long()\n","        end_positions = end_positions.unsqueeze(1)\n","        end_mask = (idxes <= end_positions).long()\n","        a_ids = start_mask + end_mask - 1\n","\n","        return a_ids, start_positions.squeeze(1), end_positions.squeeze(1)\n","\n","\n","class ContextEncoderforQG(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 dropout=0.0):\n","        super(ContextEncoderforQG, self).__init__()\n","        self.embedding = embedding\n","        self.context_lstm = CustomLSTM(input_size=emsize,\n","                                       hidden_size=nhidden,\n","                                       num_layers=nlayers,\n","                                       dropout=dropout,\n","                                       bidirectional=True)\n","        self.context_linear = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.fusion = nn.Linear(4 * nhidden, 2 * nhidden, bias=False)\n","        self.gate = nn.Linear(4 * nhidden, 2 * nhidden, bias=False)\n","\n","    def forward(self, c_ids, a_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        c_embeddings = self.embedding(c_ids, c_mask, a_ids)\n","        c_outputs, _ = self.context_lstm(c_embeddings, c_lengths)\n","        # attention\n","        mask = torch.matmul(c_mask.unsqueeze(2), c_mask.unsqueeze(1))\n","        c_attned_by_c, _ = cal_attn(self.context_linear(c_outputs),\n","                                    c_outputs,\n","                                    mask)\n","        c_concat = torch.cat([c_outputs, c_attned_by_c], dim=2)\n","        c_fused = self.fusion(c_concat).tanh()\n","        c_gate = self.gate(c_concat).sigmoid()\n","        c_outputs = c_gate * c_fused + (1 - c_gate) * c_outputs\n","        return c_outputs\n","\n","\n","class QuestionDecoder(nn.Module):\n","    def __init__(self, sos_id, eos_id,\n","                 embedding, contextualized_embedding, emsize,\n","                 nhidden, ntokens, nlayers, nqtypes,question_types,\n","                 dropout=0.0,\n","                 max_q_len=64):\n","        super(QuestionDecoder, self).__init__()\n","\n","        self.sos_id = sos_id\n","        self.eos_id = eos_id\n","        self.emsize = emsize\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.ntokens = ntokens\n","        self.nlayers = nlayers\n","        # this max_len include sos eos\n","        self.max_q_len = max_q_len\n","        self.nqtypes = nqtypes\n","        self.question_types = question_types\n","        self.context_lstm = ContextEncoderforQG(contextualized_embedding, emsize,\n","                                                nhidden // 2, nlayers, dropout)\n","\n","        self.question_lstm = CustomLSTM(input_size=emsize,\n","                                        hidden_size=nhidden,\n","                                        num_layers=nlayers,\n","                                        dropout=dropout,\n","                                        bidirectional=False)\n","\n","        self.question_linear = nn.Linear(nhidden, nhidden)\n","\n","        self.concat_linear = nn.Sequential(nn.Linear(2*nhidden, 2*nhidden),\n","                                           nn.Dropout(dropout),\n","                                           nn.Linear(2*nhidden, 2*emsize))\n","\n","        self.logit_linear = nn.Linear(emsize, ntokens, bias=False)\n","\n","        # fix output word matrix\n","        self.logit_linear.weight = embedding.word_embeddings.weight\n","        for param in self.logit_linear.parameters():\n","            param.requires_grad = False\n","\n","        self.discriminator = nn.Bilinear(emsize, nhidden, 1)\n","\n","    def postprocess(self, q_ids):\n","        eos_mask = q_ids == self.eos_id\n","        no_eos_idx_sum = (eos_mask.sum(dim=1) == 0).long() * \\\n","            (self.max_q_len - 1)\n","        eos_mask = eos_mask.cpu().numpy()\n","        q_lengths = np.argmax(eos_mask, axis=1) + 1\n","        q_lengths = torch.tensor(q_lengths).to(\n","            q_ids.device).long() + no_eos_idx_sum\n","        batch_size, max_len = q_ids.size()\n","        idxes = torch.arange(0, max_len).to(q_ids.device)\n","        idxes = idxes.unsqueeze(0).repeat(batch_size, 1)\n","        q_mask = (idxes < q_lengths.unsqueeze(1))\n","        q_ids = q_ids.long() * q_mask.long()\n","        return q_ids\n","\n","    def forward(self, init_state, c_ids, q_ids, a_ids):\n","        batch_size, max_q_len = q_ids.size()\n","\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        q_mask, q_lengths = return_mask_lengths(q_ids)\n","\n","        # question dec\n","        q_embeddings = self.embedding(q_ids)\n","        q_outputs, _ = self.question_lstm(q_embeddings, q_lengths, init_state)\n","\n","        # attention\n","        mask = torch.matmul(q_mask.unsqueeze(2), c_mask.unsqueeze(1))\n","        c_attned_by_q, attn_logits = cal_attn(self.question_linear(q_outputs),\n","                                              c_outputs,\n","                                              mask)\n","\n","        # gen logits\n","        q_concated = torch.cat([q_outputs, c_attned_by_q], dim=2)\n","        q_concated = self.concat_linear(q_concated)\n","        q_maxouted, _ = q_concated.view(\n","            batch_size, max_q_len, self.emsize, 2).max(dim=-1)\n","        gen_logits = self.logit_linear(q_maxouted)\n","\n","        # copy logits\n","        bq = batch_size * max_q_len\n","        c_ids = c_ids.unsqueeze(1).repeat(\n","            1, max_q_len, 1).view(bq, -1).contiguous()\n","        attn_logits = attn_logits.view(bq, -1).contiguous()\n","        copy_logits = torch.zeros(bq, self.ntokens).to(c_ids.device)\n","        copy_logits = copy_logits - 10000.0\n","        copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","        copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","        copy_logits = copy_logits.view(batch_size, max_q_len, -1).contiguous()\n","\n","        logits = gen_logits + copy_logits\n","\n","        # mutual information btw answer and question\n","        a_emb = c_outputs * a_ids.float().unsqueeze(2)\n","        a_mean_emb = torch.sum(a_emb, 1) / a_ids.sum(1).unsqueeze(1).float()\n","        fake_a_mean_emb = torch.cat([a_mean_emb[-1].unsqueeze(0),\n","                                     a_mean_emb[:-1]], dim=0)\n","\n","        q_emb = q_maxouted * q_mask.unsqueeze(2)\n","        q_mean_emb = torch.sum(q_emb, 1) / q_lengths.unsqueeze(1).float()\n","        fake_q_mean_emb = torch.cat([q_mean_emb[-1].unsqueeze(0),\n","                                     q_mean_emb[:-1]], dim=0)\n","\n","        bce_loss = nn.BCEWithLogitsLoss()\n","        true_logits = self.discriminator(q_mean_emb, a_mean_emb)\n","        true_labels = torch.ones_like(true_logits)\n","\n","        fake_a_logits = self.discriminator(q_mean_emb, fake_a_mean_emb)\n","        fake_q_logits = self.discriminator(fake_q_mean_emb, a_mean_emb)\n","        fake_logits = torch.cat([fake_a_logits, fake_q_logits], dim=0)\n","        fake_labels = torch.zeros_like(fake_logits)\n","\n","        true_loss = bce_loss(true_logits, true_labels)\n","        fake_loss = 0.5 * bce_loss(fake_logits, fake_labels)\n","        loss_info = 0.5 * (true_loss + fake_loss)\n","\n","        return logits, loss_info\n","\n","    def generate(self, init_state, c_ids, a_ids):\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        batch_size = c_ids.size(0)\n","\n","        q_ids = torch.LongTensor([self.sos_id] * batch_size).unsqueeze(1)\n","        q_ids = q_ids.to(c_ids.device)\n","        token_type_ids = torch.zeros_like(q_ids)\n","        position_ids = torch.zeros_like(q_ids)\n","        q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        state = init_state\n","\n","        # unroll\n","        all_q_ids = list()\n","        all_q_ids.append(q_ids)\n","        for _ in range(self.max_q_len - 1):\n","            position_ids = position_ids + 1\n","            q_outputs, state = self.question_lstm.lstm(q_embeddings, state)\n","\n","            # attention\n","            mask = c_mask.unsqueeze(1)\n","            c_attned_by_q, attn_logits = cal_attn(self.question_linear(q_outputs),\n","                                                  c_outputs,\n","                                                  mask)\n","\n","            # gen logits\n","            q_concated = torch.cat([q_outputs, c_attned_by_q], dim=2)\n","            q_concated = self.concat_linear(q_concated)\n","            q_maxouted, _ = q_concated.view(\n","                batch_size, 1, self.emsize, 2).max(dim=-1)\n","            gen_logits = self.logit_linear(q_maxouted)\n","\n","            # copy logits\n","            attn_logits = attn_logits.squeeze(1)\n","            copy_logits = torch.zeros(\n","                batch_size, self.ntokens).to(c_ids.device)\n","            copy_logits = copy_logits - 10000.0\n","            copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","            copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","\n","            logits = gen_logits + copy_logits.unsqueeze(1)\n","\n","            q_ids = torch.argmax(logits, 2)\n","            all_q_ids.append(q_ids)\n","\n","            q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        q_ids = torch.cat(all_q_ids, 1)\n","        q_ids = self.postprocess(q_ids)\n","\n","        return q_ids\n","\n","    def sample(self, init_state, c_ids, a_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        batch_size = c_ids.size(0)\n","\n","        q_ids = torch.LongTensor([self.sos_id] * batch_size).unsqueeze(1)\n","        q_ids = q_ids.to(c_ids.device)\n","        token_type_ids = torch.zeros_like(q_ids)\n","        position_ids = torch.zeros_like(q_ids)\n","        q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        state = init_state\n","\n","        # unroll\n","        all_q_ids = list()\n","        all_q_ids.append(q_ids)\n","        for _ in range(self.max_q_len - 1):\n","            position_ids = position_ids + 1\n","            q_outputs, state = self.question_lstm.lstm(q_embeddings, state)\n","\n","            # attention\n","            mask = c_mask.unsqueeze(1)\n","            c_attned_by_q, attn_logits = cal_attn(self.question_linear(q_outputs),\n","                                                  c_outputs,\n","                                                  mask)\n","\n","            # gen logits\n","            q_concated = torch.cat([q_outputs, c_attned_by_q], dim=2)\n","            q_concated = self.concat_linear(q_concated)\n","            q_maxouted, _ = q_concated.view(batch_size, 1, self.emsize, 2).max(dim=-1)\n","            gen_logits = self.logit_linear(q_maxouted)\n","\n","            # copy logits\n","            attn_logits = attn_logits.squeeze(1)\n","            copy_logits = torch.zeros(batch_size, self.ntokens).to(c_ids.device)\n","            copy_logits = copy_logits - 10000.0\n","            copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","            copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","\n","            logits = gen_logits + copy_logits.unsqueeze(1)\n","            logits = logits.squeeze(1)\n","            logits = self.top_k_top_p_filtering(logits, 2, top_p=0.8)\n","            probs = F.softmax(logits, dim=-1)\n","            q_ids = torch.multinomial(probs, num_samples=1)  # [b,1]\n","            all_q_ids.append(q_ids)\n","\n","            q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        q_ids = torch.cat(all_q_ids, 1)\n","        q_ids = self.postprocess(q_ids)\n","\n","        return q_ids\n","\n","\n","class DiscreteVAE(nn.Module):\n","    def __init__(self, args):\n","        super(DiscreteVAE, self).__init__()\n","        self.tokenizer = tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n","        padding_idx = tokenizer.vocab['[PAD]']\n","        sos_id = tokenizer.vocab['[CLS]']\n","        eos_id = tokenizer.vocab['[SEP]']\n","        ntokens = len(tokenizer.vocab)\n","\n","        bert_model = args.bert_model\n","        if \"large\" in bert_model:\n","            emsize = 1024\n","        else:\n","            emsize = 768\n","\n","        self.nqtypes = args.nqtypes\n","        self.lambda_qtype = args.lambda_qtype\n","        enc_nhidden = args.enc_nhidden\n","        enc_nlayers = args.enc_nlayers\n","        enc_dropout = args.enc_dropout\n","        dec_a_nhidden = args.dec_a_nhidden\n","        dec_a_nlayers = args.dec_a_nlayers\n","        dec_a_dropout = args.dec_a_dropout\n","        self.dec_q_nhidden = dec_q_nhidden = args.dec_q_nhidden\n","        self.dec_q_nlayers = dec_q_nlayers = args.dec_q_nlayers\n","        dec_q_dropout = args.dec_q_dropout\n","        self.nzqdim = nzqdim = args.nzqdim\n","        self.nza = nza = args.nza\n","        self.nzadim = nzadim = args.nzadim\n","\n","        self.lambda_kl = args.lambda_kl\n","        self.lambda_info = args.lambda_info\n","\n","        max_q_len = args.max_q_len\n","\n","        embedding = Embedding(bert_model)\n","        contextualized_embedding = ContextualizedEmbedding(bert_model)\n","        # freeze embedding\n","        for param in embedding.parameters():\n","            param.requires_grad = False\n","        for param in contextualized_embedding.parameters():\n","            param.requires_grad = False\n","\n","        self.posterior_encoder = PosteriorEncoder(embedding, emsize,\n","                                                  enc_nhidden, enc_nlayers,\n","                                                  nzqdim, nza, nzadim,self.nqtypes,\n","                                                  enc_dropout)\n","\n","        self.prior_encoder = PriorEncoder(embedding, emsize,\n","                                          enc_nhidden, enc_nlayers,\n","                                          nzqdim, nza, nzadim,self.nqtypes, enc_dropout)\n","\n","        self.answer_decoder = AnswerDecoder(contextualized_embedding, emsize,\n","                                            dec_a_nhidden, dec_a_nlayers,\n","                                            dec_a_dropout)\n","\n","        self.question_decoder = QuestionDecoder(sos_id, eos_id,\n","                                                embedding, contextualized_embedding, emsize,\n","                                                dec_q_nhidden, ntokens, dec_q_nlayers,\n","                                                dec_q_dropout,\n","                                                max_q_len)\n","\n","        self.q_h_linear = nn.Linear(nzqdim+self.nqtypes, dec_q_nlayers * dec_q_nhidden)\n","        self.q_c_linear = nn.Linear(nzqdim+self.nqtypes, dec_q_nlayers * dec_q_nhidden)\n","        self.a_linear = nn.Linear(nza * nzadim+self.nqtypes, emsize, False)\n","\n","        self.q_rec_criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n","        self.q_type_rec_criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n","        self.gaussian_kl_criterion = GaussianKLLoss()\n","        self.categorical_kl_criterion = CategoricalKLLoss()\n","\n","    def return_init_state(self, zq, za, qtype_ids):\n","        h = torch.cat([zq,qtype_ids], dim = -1)\n","        q_init_h = self.q_h_linear(h)\n","        q_init_c = self.q_c_linear(h)\n","        q_init_h = q_init_h.view(-1, self.dec_q_nlayers,\n","                                 self.dec_q_nhidden).transpose(0, 1).contiguous()\n","        q_init_c = q_init_c.view(-1, self.dec_q_nlayers,\n","                                 self.dec_q_nhidden).transpose(0, 1).contiguous()\n","        q_init_state = (q_init_h, q_init_c)\n","\n","        za_flatten = za.view(-1, self.nza * self.nzadim)\n","        h = torch.cat([za_flatten, qtype_ids], dim=-1)\n","        a_init_state = self.a_linear(h)\n","\n","        return q_init_state, a_init_state\n","\n","\n","    def forward(self, c_ids, q_ids, a_ids, start_positions, end_positions,qtype_ids):\n","        \n","      \n","        posterior_zq_mu, posterior_zq_logvar, posterior_zq, \\\n","            posterior_za_prob, posterior_za \\\n","            = self.posterior_encoder(c_ids, q_ids, a_ids, qtype_ids)\n","\n","        prior_zq_mu, prior_zq_logvar, _, \\\n","            prior_za_prob, _ \\\n","            = self.prior_encoder(c_ids,qtype_ids)\n","\n","        q_init_state, a_init_state = self.return_init_state(\n","            posterior_zq, posterior_za,qtype_ids)\n","\n","        # answer decoding\n","        start_logits, end_logits = self.answer_decoder(a_init_state, c_ids)\n","        # question decoding\n","        q_logits, loss_info = self.question_decoder(\n","            q_init_state, c_ids, q_ids, a_ids)\n","\n","        # q rec loss\n","        loss_q_rec = self.q_rec_criterion(q_logits[:, :-1, :].transpose(1, 2).contiguous(),\n","                                          q_ids[:, 1:])\n","        # q_type rec loss\n","        loss_q_type = self.q_type_rec_criterion(q_logits[:, :-1, :].transpose(1, 2).contiguous()[:,:,1],\n","                                          q_ids[:, 1])\n","        # print(f\"\\nbatch_size: {c_ids.size(0)}\")\n","        # print(f\"Logits shape: {q_logits[:, :-1, :].transpose(1, 2).contiguous().shape}\")\n","        # print(f\"Logits: {q_logits[:, :-1, :].transpose(1, 2).contiguous()[0,0][0]}\")\n","        # print(f\"Logits parsed shape: {torch.argmax(q_logits, 2).shape}\")\n","        # print(f\"q_ids shape: {q_ids.shape}\")\n","        # print(f\"Logits parsed: {torch.argmax(q_logits, 2)[0,:2]}\")\n","        # print(f\"q_ids: {q_ids[0,:2]}\")\n","        # print(f\"question: {self.tokenizer.decode(q_ids[0], skip_special_tokens=True)}\")\n","        # a rec loss\n","        max_c_len = c_ids.size(1)\n","        a_rec_criterion = nn.CrossEntropyLoss(ignore_index=max_c_len)\n","        start_positions.clamp_(0, max_c_len)\n","        end_positions.clamp_(0, max_c_len)\n","        loss_start_a_rec = a_rec_criterion(start_logits, start_positions)\n","        loss_end_a_rec = a_rec_criterion(end_logits, end_positions)\n","        loss_a_rec = 0.5 * (loss_start_a_rec + loss_end_a_rec)\n","\n","        # kl loss\n","        loss_zq_kl = self.gaussian_kl_criterion(posterior_zq_mu,\n","                                                posterior_zq_logvar,\n","                                                prior_zq_mu,\n","                                                prior_zq_logvar)\n","\n","        loss_za_kl = self.categorical_kl_criterion(posterior_za_prob,\n","                                                   prior_za_prob)\n","\n","        loss_kl = self.lambda_kl * (loss_zq_kl + loss_za_kl)\n","        loss_info = self.lambda_info * loss_info\n","        loss_qtype = self.lambda_qtype * loss_q_type\n","        loss = loss_q_rec + loss_a_rec + loss_kl + loss_info + loss_qtype\n","\n","        return loss, \\\n","            loss_q_rec, loss_a_rec, \\\n","            loss_zq_kl, loss_za_kl, \\\n","            loss_info, loss_qtype\n","\n","    def generate(self, zq, za, c_ids, qtype_ids):\n","        q_init_state, a_init_state = self.return_init_state(zq, za, qtype_ids)\n","\n","        a_ids, start_positions, end_positions = self.answer_decoder.generate(\n","            a_init_state, c_ids)\n","\n","        q_ids = self.question_decoder.generate(q_init_state, c_ids, a_ids)\n","\n","        return q_ids, start_positions, end_positions\n","\n","    def return_answer_logits(self, zq, za, c_ids, qtype_ids):\n","        _, a_init_state = self.return_init_state(zq, za, qtype_ids)\n","\n","        start_logits, end_logits = self.answer_decoder(a_init_state, c_ids)\n","\n","        return start_logits, end_logits\n"],"metadata":{"id":"r_Hj9I-5qp2d","executionInfo":{"status":"ok","timestamp":1683590159893,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Squad utils"],"metadata":{"id":"Z1o7CdRMqyRf"}},{"cell_type":"code","source":["import collections\n","import gzip\n","import json\n","import math\n","import re\n","import string\n","import sys\n","from copy import deepcopy\n","\n","import json_lines\n","import numpy as np\n","from transformers.models.bert import BasicTokenizer\n","from transformers.data.processors.squad import whitespace_tokenize\n","from tqdm import tqdm\n","\n","\n","class SquadExample(object):\n","    \"\"\"\n","       A single training/test example for the Squad dataset.\n","       For examples without an answer, the start and end position are -1.\n","       \"\"\"\n","    def __init__(self,\n","                 qas_id,\n","                 question_text,\n","                 doc_tokens,\n","                 orig_answer_text=None,\n","                 start_position=None,\n","                 end_position=None,\n","                 is_impossible=None):\n","        self.qas_id = qas_id\n","        self.question_text = question_text\n","        self.doc_tokens = doc_tokens\n","        self.orig_answer_text = orig_answer_text\n","        self.start_position = start_position\n","        self.end_position = end_position\n","        self.is_impossible = is_impossible\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __repr__(self):\n","        s = \"\"\n","        s += \"qas_id: %s\" % self.qas_id\n","        s += \", question_text: %s\" % self.question_text\n","        s += \", doc_tokens: [%s]\" % \" \".join(self.doc_tokens)\n","        if self.start_position:\n","            s += \", start_position: %d\" % self.start_position\n","        if self.end_position:\n","            s += \", end_position: %d\" % self.end_position\n","        if self.is_impossible:\n","            s += \", is_impossible: %r\" % self.is_impossible\n","        return s\n","\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","    def __init__(self,\n","                 unique_id,\n","                 example_index,\n","                 doc_span_index,\n","                 tokens,\n","                 token_to_orig_map,\n","                 token_is_max_context,\n","                 input_ids,\n","                 c_ids,\n","                 context_tokens,\n","                 q_ids,\n","                 q_tokens,\n","                 answer_text,\n","                 tag_ids,\n","                 input_mask,\n","                 segment_ids,\n","                 qtype_ids,\n","                 context_segment_ids=None,\n","                 noq_start_position=None,\n","                 noq_end_position=None,\n","                 start_position=None,\n","                 end_position=None,\n","                 is_impossible=None):\n","        self.unique_id = unique_id\n","        self.example_index = example_index\n","        self.doc_span_index = doc_span_index\n","        self.tokens = tokens\n","        self.token_to_orig_map = token_to_orig_map\n","        self.token_is_max_context = token_is_max_context\n","        self.input_ids = input_ids\n","        self.c_ids = c_ids\n","        self.context_tokens = context_tokens\n","        self.q_ids = q_ids\n","        self.q_tokens = q_tokens\n","        self.answer_text = answer_text\n","        self.tag_ids = tag_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.qtype_ids = qtype_ids\n","        self.context_segment_ids = context_segment_ids\n","        self.noq_start_position = noq_start_position\n","        self.noq_end_position = noq_end_position\n","        self.start_position = start_position\n","        self.end_position = end_position\n","        self.is_impossible = is_impossible\n","\n","\n","def convert_examples_to_features(examples, tokenizer, max_seq_length,\n","                                 doc_stride, max_query_length, is_training):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    unique_id = 1000000000\n","\n","    features = []\n","    for (example_index, example) in tqdm(enumerate(examples), total=len(examples)):\n","        query_tokens = tokenizer.tokenize(example.question_text)\n","\n","        if len(query_tokens) > max_query_length:\n","            query_tokens = query_tokens[0:max_query_length]\n","\n","        tok_to_orig_index = []\n","        orig_to_tok_index = []\n","        all_doc_tokens = []\n","        for (i, token) in enumerate(example.doc_tokens):\n","            orig_to_tok_index.append(len(all_doc_tokens))\n","            sub_tokens = tokenizer.tokenize(token)\n","            for sub_token in sub_tokens:\n","                tok_to_orig_index.append(i)\n","                all_doc_tokens.append(sub_token)\n","\n","        tok_start_position = None\n","        tok_end_position = None\n","        if is_training and example.is_impossible:\n","            tok_start_position = -1\n","            tok_end_position = -1\n","        if is_training and not example.is_impossible:\n","            tok_start_position = orig_to_tok_index[example.start_position]\n","            if example.end_position < len(example.doc_tokens) - 1:\n","                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n","            else:\n","                tok_end_position = len(all_doc_tokens) - 1\n","            (tok_start_position, tok_end_position) = _improve_answer_span(\n","                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n","                example.orig_answer_text)\n","\n","        # The -3 accounts for [CLS], [SEP] and [SEP]\n","        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n","\n","        # We can have documents that are longer than the maximum sequence length.\n","        # To deal with this we do a sliding window approach, where we take chunks\n","        # of the up to our max length with a stride of `doc_stride`.\n","        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"DocSpan\", [\"start\", \"length\"])\n","        doc_spans = []\n","        start_offset = 0\n","        while start_offset < len(all_doc_tokens):\n","            length = len(all_doc_tokens) - start_offset\n","            if length > max_tokens_for_doc:\n","                length = max_tokens_for_doc\n","            doc_spans.append(_DocSpan(start=start_offset, length=length))\n","            if start_offset + length == len(all_doc_tokens):\n","                break\n","            start_offset += min(length, doc_stride)\n","\n","        for (doc_span_index, doc_span) in enumerate(doc_spans):\n","            tokens = []\n","            token_to_orig_map = {}\n","            token_is_max_context = {}\n","            segment_ids = []\n","            tokens.append(\"[CLS]\")\n","            segment_ids.append(0)\n","            for token in query_tokens:\n","                tokens.append(token)\n","                segment_ids.append(0)\n","            tokens.append(\"[SEP]\")\n","            segment_ids.append(0)\n","\n","            context_tokens = list()\n","            context_tokens.append(\"[CLS]\")\n","            for i in range(doc_span.length):\n","                split_token_index = doc_span.start + i\n","                token_to_orig_map[len(\n","                    tokens)] = tok_to_orig_index[split_token_index]\n","\n","                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n","                                                       split_token_index)\n","                token_is_max_context[len(tokens)] = is_max_context\n","                tokens.append(all_doc_tokens[split_token_index])\n","                segment_ids.append(1)\n","                context_tokens.append(all_doc_tokens[split_token_index])\n","            tokens.append(\"[SEP]\")\n","            segment_ids.append(1)\n","            context_tokens.append(\"[SEP]\")\n","\n","            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","            # tokens are attended to.\n","            input_mask = [1] * len(input_ids)\n","\n","            # Zero-pad up to the sequence length.\n","            while len(input_ids) < max_seq_length:\n","                input_ids.append(0)\n","                input_mask.append(0)\n","                segment_ids.append(0)\n","\n","            input_ids = np.asarray(input_ids, dtype=np.int32)\n","            input_mask = np.asarray(input_mask, dtype=np.uint8)\n","            segment_ids = np.asarray(segment_ids, dtype=np.uint8)\n","\n","            assert len(input_ids) == max_seq_length\n","            assert len(input_mask) == max_seq_length\n","            assert len(segment_ids) == max_seq_length\n","\n","            start_position = None\n","            end_position = None\n","\n","            if is_training and not example.is_impossible:\n","                # For training, if our document chunk does not contain an annotation\n","                # we throw it out, since there is nothing to predict.\n","                doc_start = doc_span.start\n","                doc_end = doc_span.start + doc_span.length - 1\n","                out_of_span = False\n","                if not (tok_start_position >= doc_start and\n","                        tok_end_position <= doc_end):\n","                    out_of_span = True\n","                if out_of_span:\n","                    start_position = 0\n","                    end_position = 0\n","                else:\n","                    doc_offset = len(query_tokens) + 2\n","                    start_position = tok_start_position - doc_start + doc_offset\n","                    end_position = tok_end_position - doc_start + doc_offset\n","\n","                if out_of_span:\n","                    continue\n","\n","            if is_training and example.is_impossible:\n","                start_position = 0\n","                end_position = 0\n","            c_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n","\n","            while len(c_ids) < max_seq_length:\n","                c_ids.append(0)\n","            c_ids = np.asarray(c_ids, dtype=np.int32)\n","\n","            features.append(\n","                InputFeatures(\n","                    unique_id=unique_id,\n","                    example_index=example_index,\n","                    doc_span_index=doc_span_index,\n","                    tokens=tokens,\n","                    token_to_orig_map=token_to_orig_map,\n","                    token_is_max_context=token_is_max_context,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    c_ids=c_ids,\n","                    context_tokens=None,\n","                    q_ids=None,\n","                    q_tokens=None,\n","                    answer_text=example.orig_answer_text,\n","                    tag_ids=None,\n","                    segment_ids=segment_ids,\n","                    noq_start_position=None,\n","                    noq_end_position=None,\n","                    start_position=start_position,\n","                    end_position=end_position,\n","                    is_impossible=example.is_impossible))\n","            unique_id += 1\n","\n","    return features\n","\n","\n","def convert_examples_to_harv_features(examples, tokenizer, max_seq_length,\n","                                      doc_stride, max_query_length, is_training):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\n","       each example only contains a sequence of ids for context(paragraph)\n","    \"\"\"\n","\n","    unique_id = 1000000000\n","\n","    features = []\n","    for example in tqdm(examples, total=len(examples)):\n","        query_tokens = tokenizer.tokenize(example.question_text)\n","\n","        if len(query_tokens) > max_query_length:\n","            query_tokens = query_tokens[0:max_query_length]\n","\n","        tok_to_orig_index = []\n","        orig_to_tok_index = []\n","        all_doc_tokens = []\n","        for (i, token) in enumerate(example.doc_tokens):\n","            orig_to_tok_index.append(len(all_doc_tokens))\n","            sub_tokens = tokenizer.tokenize(token)\n","            for sub_token in sub_tokens:\n","                tok_to_orig_index.append(i)\n","                all_doc_tokens.append(sub_token)\n","\n","        tok_start_position = None\n","        tok_end_position = None\n","        if is_training and example.is_impossible:\n","            tok_start_position = -1\n","            tok_end_position = -1\n","        if is_training and not example.is_impossible:\n","            tok_start_position = orig_to_tok_index[example.start_position]\n","            if example.end_position < len(example.doc_tokens) - 1:\n","                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n","            else:\n","                tok_end_position = len(all_doc_tokens) - 1\n","            (tok_start_position, tok_end_position) = _improve_answer_span(\n","                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n","                example.orig_answer_text)\n","\n","        # The -3 accounts for [CLS], [SEP] and [SEP]\n","        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n","\n","        # We can have documents that are longer than the maximum sequence length.\n","        # To deal with this we do a sliding window approach, where we take chunks\n","        # of the up to our max length with a stride of `doc_stride`.\n","        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"DocSpan\", [\"start\", \"length\"])\n","        doc_spans = []\n","        start_offset = 0\n","        while start_offset < len(all_doc_tokens):\n","            length = len(all_doc_tokens) - start_offset\n","            if length > max_tokens_for_doc:\n","                length = max_tokens_for_doc\n","            doc_spans.append(_DocSpan(start=start_offset, length=length))\n","            if start_offset + length == len(all_doc_tokens):\n","                break\n","            start_offset += min(length, doc_stride)\n","\n","        for (doc_span_index, doc_span) in enumerate(doc_spans):\n","            tokens = []\n","            token_to_orig_map = {}\n","            token_is_max_context = {}\n","            segment_ids = []\n","            tokens.append(\"[CLS]\")\n","            segment_ids.append(0)\n","            for token in query_tokens:\n","                tokens.append(token)\n","                segment_ids.append(0)\n","            tokens.append(\"[SEP]\")\n","\n","            context_tokens = list()\n","            context_tokens.append(\"[CLS]\")\n","            for i in range(doc_span.length):\n","                split_token_index = doc_span.start + i\n","                token_to_orig_map[len(\n","                    tokens)] = tok_to_orig_index[split_token_index]\n","\n","                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n","                                                       split_token_index)\n","                token_is_max_context[len(tokens)] = is_max_context\n","                tokens.append(all_doc_tokens[split_token_index])\n","                context_tokens.append(all_doc_tokens[split_token_index])\n","\n","            tokens.append(\"[SEP]\")\n","            context_tokens.append(\"[SEP]\")\n","\n","            if is_training and not example.is_impossible:\n","                # For training, if our document chunk does not contain an annotation\n","                # we throw it out, since there is nothing to predict.\n","                doc_start = doc_span.start\n","                doc_end = doc_span.start + doc_span.length - 1\n","                out_of_span = False\n","                if not (tok_start_position >= doc_start and\n","                        tok_end_position <= doc_end):\n","                    out_of_span = True\n","                if out_of_span:\n","                    continue\n","\n","            c_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n","\n","            while len(c_ids) < max_seq_length:\n","                c_ids.append(0)\n","\n","            features.append(\n","                InputFeatures(\n","                    unique_id=None,\n","                    example_index=None,\n","                    doc_span_index=None,\n","                    tokens=None,\n","                    token_to_orig_map=None,\n","                    token_is_max_context=None,\n","                    input_ids=None,\n","                    input_mask=None,\n","                    c_ids=c_ids,\n","                    context_tokens=None,\n","                    q_ids=None,\n","                    q_tokens=None,\n","                    answer_text=None,\n","                    tag_ids=None,\n","                    segment_ids=None,\n","                    noq_start_position=None,\n","                    noq_end_position=None,\n","                    start_position=None,\n","                    end_position=None,\n","                    is_impossible=None))\n","            unique_id += 1\n","\n","    return features\n","\n","\n","def convert_examples_to_features_answer_id(examples, tokenizer, max_seq_length,\n","                                           doc_stride, max_query_length, max_ans_length, is_training):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\n","       In addition to the original InputFeature class, it contains \n","       c_ids: ids for context\n","       tag ids: indicate the answer span of context,\n","       noq_start_position: start position of answer in context without concatenation of question\n","       noq_end_position: end position of answer in context without concatenation of question\n","    \"\"\"\n","\n","    unique_id = 1000000000\n","\n","    features = []\n","    question_types = ['who', 'what', 'when', 'where', 'why', 'which', 'how', 'misc']\n","    for (example_index, example) in tqdm(enumerate(examples), total=len(examples)):\n","        query_tokens = tokenizer.tokenize(example.question_text)\n","        first_word = example.question_text.lower().split()[0]\n","        qtype_ids = [0]*len(question_types)\n","        if first_word in question_types:\n","            qtype_ids[question_types.index(first_word)] = 1\n","        else:\n","            qtype_ids[-1] = 1\n","        if len(query_tokens) > max_query_length:\n","            query_tokens = query_tokens[0:max_query_length]\n","\n","        tok_to_orig_index = []\n","        orig_to_tok_index = []\n","        all_doc_tokens = []\n","        for (i, token) in enumerate(example.doc_tokens):\n","            orig_to_tok_index.append(len(all_doc_tokens))\n","            sub_tokens = tokenizer.tokenize(token)\n","            for sub_token in sub_tokens:\n","                tok_to_orig_index.append(i)\n","                all_doc_tokens.append(sub_token)\n","\n","        tok_start_position = None\n","        tok_end_position = None\n","        if is_training and example.is_impossible:\n","            tok_start_position = -1\n","            tok_end_position = -1\n","        if is_training and not example.is_impossible:\n","            tok_start_position = orig_to_tok_index[example.start_position]\n","            if example.end_position < len(example.doc_tokens) - 1:\n","                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n","            else:\n","                tok_end_position = len(all_doc_tokens) - 1\n","            (tok_start_position, tok_end_position) = _improve_answer_span(\n","                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n","                example.orig_answer_text)\n","\n","        # The -3 accounts for [CLS], [SEP] and [SEP]\n","        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n","\n","        # We can have documents that are longer than the maximum sequence length.\n","        # To deal with this we do a sliding window approach, where we take chunks\n","        # of the up to our max length with a stride of `doc_stride`.\n","        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"DocSpan\", [\"start\", \"length\"])\n","        doc_spans = []\n","        start_offset = 0\n","        while start_offset < len(all_doc_tokens):\n","            length = len(all_doc_tokens) - start_offset\n","            if length > max_tokens_for_doc:\n","                length = max_tokens_for_doc\n","            doc_spans.append(_DocSpan(start=start_offset, length=length))\n","            if start_offset + length == len(all_doc_tokens):\n","                break\n","            start_offset += min(length, doc_stride)\n","\n","        for (doc_span_index, doc_span) in enumerate(doc_spans):\n","            tokens = []\n","            token_to_orig_map = {}\n","            token_is_max_context = {}\n","            segment_ids = []\n","            tokens.append(\"[CLS]\")\n","            segment_ids.append(0)\n","            for token in query_tokens:\n","                tokens.append(token)\n","                segment_ids.append(0)\n","            tokens.append(\"[SEP]\")\n","            segment_ids.append(0)\n","\n","            context_tokens = list()\n","            context_tokens.append(\"[CLS]\")\n","            for i in range(doc_span.length):\n","                split_token_index = doc_span.start + i\n","                token_to_orig_map[len(\n","                    tokens)] = tok_to_orig_index[split_token_index]\n","\n","                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n","                                                       split_token_index)\n","                token_is_max_context[len(tokens)] = is_max_context\n","                tokens.append(all_doc_tokens[split_token_index])\n","                segment_ids.append(1)\n","                context_tokens.append(all_doc_tokens[split_token_index])\n","            tokens.append(\"[SEP]\")\n","            segment_ids.append(1)\n","            context_tokens.append(\"[SEP]\")\n","\n","            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","            # tokens are attended to.\n","            input_mask = [1] * len(input_ids)\n","\n","            # Zero-pad up to the sequence length.\n","            while len(input_ids) < max_seq_length:\n","                input_ids.append(0)\n","                input_mask.append(0)\n","                segment_ids.append(0)\n","\n","            assert len(input_ids) == max_seq_length\n","            assert len(input_mask) == max_seq_length\n","            assert len(segment_ids) == max_seq_length\n","\n","            start_position = None\n","            end_position = None\n","            noq_start_position = None\n","            noq_end_position = None\n","\n","            if is_training and not example.is_impossible:\n","                # For training, if our document chunk does not contain an annotation\n","                # we throw it out, since there is nothing to predict.\n","                doc_start = doc_span.start\n","                doc_end = doc_span.start + doc_span.length - 1\n","                out_of_span = False\n","                if not (tok_start_position >= doc_start and\n","                        tok_end_position <= doc_end):\n","                    out_of_span = True\n","                if out_of_span:\n","                    start_position = 0\n","                    end_position = 0\n","                    noq_start_position = 0\n","                    noq_end_position = 0\n","                else:\n","                    doc_offset = len(query_tokens) + 2\n","                    start_position = tok_start_position - doc_start + doc_offset\n","                    end_position = tok_end_position - doc_start + doc_offset\n","\n","                    # plus one for [CLS] token\n","                    noq_start_position = tok_start_position - doc_start + 1\n","                    noq_end_position = tok_end_position - doc_start + 1\n","\n","                # skip the context that does not contain any answer span\n","                if out_of_span:\n","                    continue\n","\n","            if is_training and example.is_impossible:\n","                start_position = 0\n","                end_position = 0\n","                noq_start_position = 0\n","                noq_end_position = 0\n","            q_tokens = deepcopy(query_tokens)[:max_query_length - 2]\n","            q_tokens.insert(0, \"[CLS]\")\n","            q_tokens.append(\"[SEP]\")\n","            q_ids = tokenizer.convert_tokens_to_ids(q_tokens)\n","            c_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n","\n","            # pad up to maximum length\n","            while len(q_ids) < max_query_length:\n","                q_ids.append(0)\n","\n","            while len(c_ids) < max_seq_length:\n","                c_ids.append(0)\n","\n","            # answer_text = example.orig_answer_text\n","\n","            # answer_tokens = tokenizer.tokenize(answer_text)[:max_ans_length - 2]\n","            # answer_tokens.insert(0, \"[CLS]\")\n","            # answer_tokens.append(\"[SEP]\")\n","            # answer_ids = tokenizer.convert_tokens_to_ids(answer_tokens)\n","\n","            # while len(answer_ids) < max_ans_length:\n","            #     answer_ids.append(0)\n","\n","            context_segment_ids = [0] * len(c_ids)\n","            for answer_idx in range(noq_start_position, noq_end_position + 1):\n","                context_segment_ids[answer_idx] = 1\n","            # BIO tagging scheme\n","            tag_ids = [0] * len(c_ids)  # Outside\n","            if noq_start_position is not None and noq_end_position is not None:\n","                tag_ids[noq_start_position] = 1  # Begin\n","                # Inside tag\n","                for idx in range(noq_start_position + 1, noq_end_position + 1):\n","                    tag_ids[idx] = 2\n","\n","            assert len(tag_ids) == len(c_ids), \"length of tag :{}, length of c :{}\".format(\n","                len(tag_ids), len(c_ids))\n","            features.append(\n","                InputFeatures(\n","                    unique_id=unique_id,\n","                    example_index=example_index,\n","                    doc_span_index=doc_span_index,\n","                    tokens=tokens,\n","                    token_to_orig_map=token_to_orig_map,\n","                    token_is_max_context=token_is_max_context,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    c_ids=c_ids,\n","                    context_tokens=context_tokens,\n","                    q_ids=q_ids,\n","                    q_tokens=q_tokens,\n","                    answer_text=example.orig_answer_text,\n","                    tag_ids=tag_ids,\n","                    segment_ids=segment_ids,\n","                    qtype_ids=qtype_ids,\n","                    context_segment_ids=context_segment_ids,\n","                    noq_start_position=noq_start_position,\n","                    noq_end_position=noq_end_position,\n","                    start_position=start_position,\n","                    end_position=end_position,\n","                    is_impossible=example.is_impossible))\n","            unique_id += 1\n","\n","    return features\n","\n","\n","\n","def read_examples(input_file, debug=False, is_training=False):\n","    # Read data\n","    unproc_data = []\n","    with gzip.open(input_file, 'rt', encoding='utf-8') as f:  # opening file in binary(rb) mode\n","        for item in json_lines.reader(f):\n","            # print(item) #or use print(item['X']) for printing specific data\n","            unproc_data.append(item)\n","\n","    # Delete header\n","    unproc_data = unproc_data[1:]\n","    if debug:\n","        unproc_data = unproc_data[:100]\n","\n","    examples = []\n","    skip_tags = ['<Table>', '<Tr>', '<Td>', '<Ol>', '<Ul>', '<Li>']\n","    for item in unproc_data:\n","        # in case of NQ dataset, context containing tags is excluded for training\n","        context = item[\"context\"]\n","        skip_flag = False\n","        for tag in skip_tags:\n","            if tag in context:\n","                skip_flag = True\n","                break\n","        if skip_flag and is_training:\n","            continue\n","\n","        doc_tokens = []\n","        for token in item[\"context_tokens\"]:\n","            if token[0] in ['[TLE]', '[PAR]', '[DOC]']:\n","                token[0] = '[SEP]'\n","            doc_tokens.append(token[0])\n","\n","        # 2. qas\n","        for qa in item['qas']:\n","            qas_id = qa['qid']\n","            question_text = qa['question']\n","\n","            # Only take the first answer\n","            answer = qa['detected_answers'][0]\n","            orig_answer_text = answer['text']\n","            # Only take the first span\n","            start_position = answer['token_spans'][0][0]\n","            end_position = answer['token_spans'][0][1]\n","\n","            example = SquadExample(\n","                qas_id=qas_id,\n","                question_text=question_text,\n","                doc_tokens=doc_tokens,\n","                orig_answer_text=orig_answer_text,\n","                start_position=start_position,\n","                end_position=end_position)\n","            examples.append(example)\n","\n","    return examples\n","\n","\n","def read_squad_examples(input_file, is_training, version_2_with_negative=False,\n","                        debug=False, reduce_size=False, ratio=1.0):\n","    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n","    with open(input_file, \"r\", encoding='utf-8') as reader:\n","        input_data = json.load(reader)[\"data\"]\n","\n","    def is_whitespace(c):\n","        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","            return True\n","        return False\n","\n","    examples = []\n","    if debug:\n","        input_data = input_data[:5]\n","\n","    for entry in tqdm(input_data, total=len(input_data)):\n","        paragraphs = entry[\"paragraphs\"]\n","        for paragraph in paragraphs:\n","            paragraph_text = paragraph[\"context\"]\n","            doc_tokens = []\n","            char_to_word_offset = []\n","            prev_is_whitespace = True\n","            for c in paragraph_text:\n","                if is_whitespace(c):\n","                    prev_is_whitespace = True\n","                else:\n","                    if prev_is_whitespace:\n","                        doc_tokens.append(c)\n","                    else:\n","                        doc_tokens[-1] += c\n","                    prev_is_whitespace = False\n","                char_to_word_offset.append(len(doc_tokens) - 1)\n","\n","            for qa in paragraph[\"qas\"]:\n","                qas_id = qa[\"id\"]\n","                question_text = qa[\"question\"]\n","                start_position = None\n","                end_position = None\n","                orig_answer_text = None\n","                is_impossible = False\n","                if is_training:\n","                    if version_2_with_negative:\n","                        is_impossible = qa[\"is_impossible\"]\n","                    if not is_impossible:\n","                        answer = qa[\"answers\"][0]\n","                        orig_answer_text = answer[\"text\"]\n","                        answer_offset = answer[\"answer_start\"]\n","                        answer_length = len(orig_answer_text)\n","                        start_position = char_to_word_offset[answer_offset]\n","                        end_position = char_to_word_offset[answer_offset +\n","                                                           answer_length - 1]\n","                        # Only add answers where the text can be exactly recovered from the\n","                        # document. If this CAN'T happen it's likely due to weird Unicode\n","                        # stuff so we will just skip the example.\n","                        #\n","                        # Note that this means for training mode, every example is NOT\n","                        # guaranteed to be preserved.\n","                        actual_text = \" \".join(\n","                            doc_tokens[start_position:(end_position + 1)])\n","                        cleaned_answer_text = \" \".join(\n","                            whitespace_tokenize(orig_answer_text))\n","                        if actual_text.find(cleaned_answer_text) == -1:\n","                            continue\n","                    else:\n","                        start_position = -1\n","                        end_position = -1\n","                        orig_answer_text = \"\"\n","\n","                example = SquadExample(\n","                    qas_id=qas_id,\n","                    question_text=question_text,\n","                    doc_tokens=doc_tokens,\n","                    orig_answer_text=orig_answer_text,\n","                    start_position=start_position,\n","                    end_position=end_position,\n","                    is_impossible=is_impossible)\n","                examples.append(example)\n","    return examples\n","\n","\n","def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n","                         orig_answer_text):\n","    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n","\n","    # The SQuAD annotations are character based. We first project them to\n","    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n","    # often find a \"better match\". For example:\n","    #\n","    #   Question: What year was John Smith born?\n","    #   Context: The leader was John Smith (1895-1943).\n","    #   Answer: 1895\n","    #\n","    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n","    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n","    # the exact answer, 1895.\n","    #\n","    # However, this is not always possible. Consider the following:\n","    #\n","    #   Question: What country is the top exporter of electornics?\n","    #   Context: The Japanese electronics industry is the lagest in the world.\n","    #   Answer: Japan\n","    #\n","    # In this case, the annotator chose \"Japan\" as a character sub-span of\n","    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n","    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n","    # in SQuAD, but does happen.\n","    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n","\n","    for new_start in range(input_start, input_end + 1):\n","        for new_end in range(input_end, new_start - 1, -1):\n","            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n","            if text_span == tok_answer_text:\n","                return (new_start, new_end)\n","\n","    return (input_start, input_end)\n","\n","\n","def _check_is_max_context(doc_spans, cur_span_index, position):\n","    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n","\n","    # Because of the sliding window approach taken to scoring documents, a single\n","    # token can appear in multiple documents. E.g.\n","    #  Doc: the man went to the store and bought a gallon of milk\n","    #  Span A: the man went to the\n","    #  Span B: to the store and bought\n","    #  Span C: and bought a gallon of\n","    #  ...\n","    #\n","    # Now the word 'bought' will have two scores from spans B and C. We only\n","    # want to consider the score with \"maximum context\", which we define as\n","    # the *minimum* of its left and right context (the *sum* of left and\n","    # right context will always be the same, of course).\n","    #\n","    # In the example the maximum context for 'bought' would be span C since\n","    # it has 1 left context and 3 right context, while span B has 4 left context\n","    # and 0 right context.\n","    best_score = None\n","    best_span_index = None\n","    for (span_index, doc_span) in enumerate(doc_spans):\n","        end = doc_span.start + doc_span.length - 1\n","        if position < doc_span.start:\n","            continue\n","        if position > end:\n","            continue\n","        num_left_context = position - doc_span.start\n","        num_right_context = end - position\n","        score = min(num_left_context, num_right_context) + \\\n","            0.01 * doc_span.length\n","        if best_score is None or score > best_score:\n","            best_score = score\n","            best_span_index = span_index\n","\n","    return cur_span_index == best_span_index\n","\n","\n","def write_predictions(all_examples, all_features, all_results, n_best_size,\n","                      max_answer_length, do_lower_case, output_prediction_file,\n","                      verbose_logging, version_2_with_negative, null_score_diff_threshold,\n","                      noq_position=False):\n","    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n","    # logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n","    # logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n","\n","    example_index_to_features = collections.defaultdict(list)\n","    for feature in all_features:\n","        example_index_to_features[feature.example_index].append(feature)\n","\n","    unique_id_to_result = {}\n","    for result in all_results:\n","        unique_id_to_result[result.unique_id] = result\n","\n","    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","        \"PrelimPrediction\",\n","        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n","\n","    all_predictions = collections.OrderedDict()\n","    all_nbest_json = collections.OrderedDict()\n","    scores_diff_json = collections.OrderedDict()\n","\n","    for (example_index, example) in enumerate(all_examples):\n","        features = example_index_to_features[example_index]\n","\n","        prelim_predictions = []\n","        # keep track of the minimum score of null start+end of position 0\n","        score_null = 1000000  # large and positive\n","        min_null_feature_index = 0  # the paragraph slice with min null score\n","        null_start_logit = 0  # the start logit at the slice with min null score\n","        null_end_logit = 0  # the end logit at the slice with min null score\n","        for (feature_index, feature) in enumerate(features):\n","            result = unique_id_to_result[feature.unique_id]\n","            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n","            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n","            # if we could have irrelevant answers, get the min score of irrelevant\n","            if version_2_with_negative:\n","                feature_null_score = result.start_logits[0] + \\\n","                    result.end_logits[0]\n","                if feature_null_score < score_null:\n","                    score_null = feature_null_score\n","                    min_null_feature_index = feature_index\n","                    null_start_logit = result.start_logits[0]\n","                    null_end_logit = result.end_logits[0]\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    if noq_position:\n","                        # start and end is computed without question tokens\n","                        # add length of q and and -1 for [CLS] in q_ids\n","                        q_ids = feature.q_ids\n","                        q_len = np.sum(np.sign(q_ids))\n","                        noq_start_index = start_index\n","                        noq_end_index = end_index\n","                        start_index = start_index + q_len - 1\n","                        end_index = end_index + q_len - 1\n","                    # We could hypothetically create invalid predictions, e.g., predict\n","                    # that the start of the span is in the question. We throw out all\n","                    # invalid predictions.\n","                    if start_index >= len(feature.tokens):\n","                        continue\n","                    if end_index >= len(feature.tokens):\n","                        continue\n","                    if start_index not in feature.token_to_orig_map:\n","                        continue\n","                    if end_index not in feature.token_to_orig_map:\n","                        continue\n","                    if not feature.token_is_max_context.get(start_index, False):\n","                        continue\n","                    if end_index < start_index:\n","                        continue\n","                    length = end_index - start_index + 1\n","                    if length > max_answer_length:\n","                        continue\n","                    if noq_position:\n","                        prelim_predictions.append(\n","                            _PrelimPrediction(\n","                                feature_index=feature_index,\n","                                start_index=start_index,\n","                                end_index=end_index,\n","                                start_logit=result.start_logits[noq_start_index],\n","                                end_logit=result.end_logits[noq_end_index]))\n","                    else:\n","                        prelim_predictions.append(\n","                            _PrelimPrediction(\n","                                feature_index=feature_index,\n","                                start_index=start_index,\n","                                end_index=end_index,\n","                                start_logit=result.start_logits[start_index],\n","                                end_logit=result.end_logits[end_index]))\n","\n","        if version_2_with_negative:\n","            prelim_predictions.append(\n","                _PrelimPrediction(\n","                    feature_index=min_null_feature_index,\n","                    start_index=0,\n","                    end_index=0,\n","                    start_logit=null_start_logit,\n","                    end_logit=null_end_logit))\n","        prelim_predictions = sorted(\n","            prelim_predictions,\n","            key=lambda x: (x.start_logit + x.end_logit),\n","            reverse=True)\n","\n","        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n","\n","        seen_predictions = {}\n","        nbest = []\n","        for pred in prelim_predictions:\n","            if len(nbest) >= n_best_size:\n","                break\n","            feature = features[pred.feature_index]\n","            if pred.start_index > 0:  # this is a non-null prediction\n","                tok_tokens = feature.tokens[pred.start_index:(\n","                    pred.end_index + 1)]\n","                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","                orig_tokens = example.doc_tokens[orig_doc_start:(\n","                    orig_doc_end + 1)]\n","                tok_text = \" \".join(tok_tokens)\n","\n","                # De-tokenize WordPieces that have been split off.\n","                tok_text = tok_text.replace(\" ##\", \"\")\n","                tok_text = tok_text.replace(\"##\", \"\")\n","\n","                # Clean whitespace\n","                tok_text = tok_text.strip()\n","                tok_text = \" \".join(tok_text.split())\n","                orig_text = \" \".join(orig_tokens)\n","\n","                final_text = get_final_text(\n","                    tok_text, orig_text, do_lower_case, verbose_logging)\n","                if final_text in seen_predictions:\n","                    continue\n","\n","                seen_predictions[final_text] = True\n","            else:\n","                final_text = \"\"\n","                seen_predictions[final_text] = True\n","\n","            nbest.append(\n","                _NbestPrediction(text=final_text,\n","                                 start_logit=pred.start_logit,\n","                                 end_logit=pred.end_logit))\n","        # if we didn't include the empty option in the n-best, include it\n","        if version_2_with_negative:\n","            if \"\" not in seen_predictions:\n","                nbest.append(\n","                    _NbestPrediction(text=\"\",\n","                                     start_logit=null_start_logit,\n","                                     end_logit=null_end_logit))\n","\n","            # In very rare edge cases we could only have single null prediction.\n","            # So we just create a nonce prediction in this case to avoid failure.\n","            if len(nbest) == 1:\n","                nbest.insert(0, _NbestPrediction(text=\"empty\",\n","                                                 start_logit=0.0,\n","                                                 end_logit=0.0))\n","\n","        # In very rare edge cases we could have no valid predictions. So we\n","        # just create a nonce prediction in this case to avoid failure.\n","        if not nbest:\n","            nbest.append(_NbestPrediction(text=\"empty\",\n","                                          start_logit=0.0,\n","                                          end_logit=0.0))\n","\n","        assert len(nbest) >= 1\n","\n","        total_scores = []\n","        best_non_null_entry = None\n","        for entry in nbest:\n","            total_scores.append(entry.start_logit + entry.end_logit)\n","            if not best_non_null_entry:\n","                if entry.text:\n","                    best_non_null_entry = entry\n","\n","        probs = _compute_softmax(total_scores)\n","\n","        nbest_json = []\n","        for (i, entry) in enumerate(nbest):\n","            output = collections.OrderedDict()\n","            output[\"text\"] = entry.text\n","            output[\"probability\"] = probs[i]\n","            output[\"start_logit\"] = entry.start_logit\n","            output[\"end_logit\"] = entry.end_logit\n","            nbest_json.append(output)\n","        assert len(nbest_json) >= 1\n","\n","        if not version_2_with_negative:\n","            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n","        else:\n","            # predict \"\" iff the null score - the score of best non-null > threshold\n","            score_diff = score_null - best_non_null_entry.start_logit - (\n","                best_non_null_entry.end_logit)\n","            scores_diff_json[example.qas_id] = score_diff\n","            if score_diff > null_score_diff_threshold:\n","                all_predictions[example.qas_id] = \"\"\n","            else:\n","                all_predictions[example.qas_id] = best_non_null_entry.text\n","            all_nbest_json[example.qas_id] = nbest_json\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n","\n","\n","def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n","    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n","\n","    # When we created the data, we kept track of the alignment between original\n","    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n","    # now `orig_text` contains the span of our original text corresponding to the\n","    # span that we predicted.\n","    #\n","    # However, `orig_text` may contain extra characters that we don't want in\n","    # our prediction.\n","    #\n","    # For example, let's say:\n","    #   pred_text = steve smith\n","    #   orig_text = Steve Smith's\n","    #\n","    # We don't want to return `orig_text` because it contains the extra \"'s\".\n","    #\n","    # We don't want to return `pred_text` because it's already been normalized\n","    # (the SQuAD eval script also does punctuation stripping/lower casing but\n","    # our tokenizer does additional normalization like stripping accent\n","    # characters).\n","    #\n","    # What we really want to return is \"Steve Smith\".\n","    #\n","    # Therefore, we have to apply a semi-complicated alignment heuristic between\n","    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n","    # can fail in certain cases in which case we just return `orig_text`.\n","\n","    def _strip_spaces(text):\n","        ns_chars = []\n","        ns_to_s_map = collections.OrderedDict()\n","        for (i, c) in enumerate(text):\n","            if c == \" \":\n","                continue\n","            ns_to_s_map[len(ns_chars)] = i\n","            ns_chars.append(c)\n","        ns_text = \"\".join(ns_chars)\n","        return (ns_text, ns_to_s_map)\n","\n","    # We first tokenize `orig_text`, strip whitespace from the result\n","    # and `pred_text`, and check if they are the same length. If they are\n","    # NOT the same length, the heuristic has failed. If they are the same\n","    # length, we assume the characters are one-to-one aligned.\n","    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","\n","    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n","\n","    start_position = tok_text.find(pred_text)\n","    if start_position == -1:\n","        # if verbose_logging:\n","        #     logger.info(\n","        #         \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n","        return orig_text\n","    end_position = start_position + len(pred_text) - 1\n","\n","    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n","    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n","\n","    if len(orig_ns_text) != len(tok_ns_text):\n","        # if verbose_logging:\n","        #     logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n","        #                 orig_ns_text, tok_ns_text)\n","        return orig_text\n","\n","    # We then project the characters in `pred_text` back to `orig_text` using\n","    # the character-to-character alignment.\n","    tok_s_to_ns_map = {}\n","    for (i, tok_index) in tok_ns_to_s_map.items():\n","        tok_s_to_ns_map[tok_index] = i\n","\n","    orig_start_position = None\n","    if start_position in tok_s_to_ns_map:\n","        ns_start_position = tok_s_to_ns_map[start_position]\n","        if ns_start_position in orig_ns_to_s_map:\n","            orig_start_position = orig_ns_to_s_map[ns_start_position]\n","\n","    if orig_start_position is None:\n","        # if verbose_logging:\n","        #     logger.info(\"Couldn't map start position\")\n","        return orig_text\n","\n","    orig_end_position = None\n","    if end_position in tok_s_to_ns_map:\n","        ns_end_position = tok_s_to_ns_map[end_position]\n","        if ns_end_position in orig_ns_to_s_map:\n","            orig_end_position = orig_ns_to_s_map[ns_end_position]\n","\n","    if orig_end_position is None:\n","        # if verbose_logging:\n","        return orig_text\n","\n","    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n","    return output_text\n","\n","\n","def _get_best_indexes(logits, n_best_size):\n","    \"\"\"Get the n-best logits from a list.\"\"\"\n","    index_and_score = sorted(\n","        enumerate(logits), key=lambda x: x[1], reverse=True)\n","\n","    best_indexes = []\n","    for i in range(len(index_and_score)):\n","        if i >= n_best_size:\n","            break\n","        best_indexes.append(index_and_score[i][0])\n","    return best_indexes\n","\n","\n","def _compute_softmax(scores):\n","    \"\"\"Compute softmax probability over raw logits.\"\"\"\n","    if not scores:\n","        return []\n","\n","    max_score = None\n","    for score in scores:\n","        if max_score is None or score > max_score:\n","            max_score = score\n","\n","    exp_scores = []\n","    total_sum = 0.0\n","    for score in scores:\n","        x = math.exp(score - max_score)\n","        exp_scores.append(x)\n","        total_sum += x\n","\n","    probs = []\n","    for score in exp_scores:\n","        probs.append(score / total_sum)\n","    return probs\n","\n","\n","def write_answer_predictions(all_examples, all_features, all_results, n_best_size,\n","                             max_answer_length, do_lower_case, output_prediction_file,\n","                             output_nbest_file, output_null_log_odds_file, verbose_logging,\n","                             version_2_with_negative, null_score_diff_threshold):\n","    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n","    # logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n","    # logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n","\n","    example_index_to_features = collections.defaultdict(list)\n","    for feature in all_features:\n","        example_index_to_features[feature.example_index].append(feature)\n","\n","    unique_id_to_result = {}\n","    for result in all_results:\n","        unique_id_to_result[result.unique_id] = result\n","\n","    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","        \"PrelimPrediction\",\n","        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n","\n","    all_predictions = collections.OrderedDict()\n","    all_nbest_json = collections.OrderedDict()\n","    scores_diff_json = collections.OrderedDict()\n","\n","    for (example_index, example) in enumerate(all_examples):\n","        features = example_index_to_features[example_index]\n","\n","        prelim_predictions = []\n","        # keep track of the minimum score of null start+end of position 0\n","        score_null = 1000000  # large and positive\n","        min_null_feature_index = 0  # the paragraph slice with min null score\n","        null_start_logit = 0  # the start logit at the slice with min null score\n","        null_end_logit = 0  # the end logit at the slice with min null score\n","        for (feature_index, feature) in enumerate(features):\n","            result = unique_id_to_result[feature.unique_id]\n","\n","            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n","            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n","            # if we could have irrelevant answers, get the min score of irrelevant\n","            if version_2_with_negative:\n","                feature_null_score = result.start_logits[0] + \\\n","                    result.end_logits[0]\n","                if feature_null_score < score_null:\n","                    score_null = feature_null_score\n","                    min_null_feature_index = feature_index\n","                    null_start_logit = result.start_logits[0]\n","                    null_end_logit = result.end_logits[0]\n","\n","            # start and end index is from [CLS] [UNK] [SEP] C [SEP]\n","            # each should be deducted 1 and added length of Q\n","            offset = len(feature.q_tokens) - 2  # -2 for [CLS] and [SEP]\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    start_index = offset + start_index - 1  # -1 for [UNK]\n","                    end_index = offset + end_index - 1  # -1 for [UNK]\n","                    # We could hypothetically create invalid predictions, e.g., predict\n","                    # that the start of the span is in the question. We throw out all\n","                    # invalid predictions.\n","                    if start_index >= len(feature.tokens):\n","                        continue\n","                    if end_index >= len(feature.tokens):\n","                        continue\n","                    if start_index not in feature.token_to_orig_map:\n","                        continue\n","                    if end_index not in feature.token_to_orig_map:\n","                        continue\n","                    if not feature.token_is_max_context.get(start_index, False):\n","                        continue\n","                    if end_index < start_index:\n","                        continue\n","                    length = end_index - start_index + 1\n","                    if length > max_answer_length:\n","                        continue\n","                    prelim_predictions.append(\n","                        _PrelimPrediction(\n","                            feature_index=feature_index,\n","                            start_index=start_index,\n","                            end_index=end_index,\n","                            start_logit=result.start_logits[start_index],\n","                            end_logit=result.end_logits[end_index]))\n","        if version_2_with_negative:\n","            prelim_predictions.append(\n","                _PrelimPrediction(\n","                    feature_index=min_null_feature_index,\n","                    start_index=0,\n","                    end_index=0,\n","                    start_logit=null_start_logit,\n","                    end_logit=null_end_logit))\n","        prelim_predictions = sorted(\n","            prelim_predictions,\n","            key=lambda x: (x.start_logit + x.end_logit),\n","            reverse=True)\n","\n","        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n","\n","        seen_predictions = {}\n","        nbest = []\n","        for pred in prelim_predictions:\n","            if len(nbest) >= n_best_size:\n","                break\n","            feature = features[pred.feature_index]\n","            if pred.start_index > 0:  # this is a non-null prediction\n","                tok_tokens = feature.tokens[pred.start_index:(\n","                    pred.end_index + 1)]\n","                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","                orig_tokens = example.doc_tokens[orig_doc_start:(\n","                    orig_doc_end + 1)]\n","                tok_text = \" \".join(tok_tokens)\n","\n","                # De-tokenize WordPieces that have been split off.\n","                tok_text = tok_text.replace(\" ##\", \"\")\n","                tok_text = tok_text.replace(\"##\", \"\")\n","\n","                # Clean whitespace\n","                tok_text = tok_text.strip()\n","                tok_text = \" \".join(tok_text.split())\n","                orig_text = \" \".join(orig_tokens)\n","\n","                final_text = get_final_text(\n","                    tok_text, orig_text, do_lower_case, verbose_logging)\n","                if final_text in seen_predictions:\n","                    continue\n","\n","                seen_predictions[final_text] = True\n","            else:\n","                final_text = \"\"\n","                seen_predictions[final_text] = True\n","\n","            nbest.append(\n","                _NbestPrediction(\n","                    text=final_text,\n","                    start_logit=pred.start_logit,\n","                    end_logit=pred.end_logit))\n","        # if we didn't include the empty option in the n-best, include it\n","        if version_2_with_negative:\n","            if \"\" not in seen_predictions:\n","                nbest.append(\n","                    _NbestPrediction(\n","                        text=\"\",\n","                        start_logit=null_start_logit,\n","                        end_logit=null_end_logit))\n","\n","            # In very rare edge cases we could only have single null prediction.\n","            # So we just create a nonce prediction in this case to avoid failure.\n","            if len(nbest) == 1:\n","                nbest.insert(0,\n","                             _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n","\n","        # In very rare edge cases we could have no valid predictions. So we\n","        # just create a nonce prediction in this case to avoid failure.\n","        if not nbest:\n","            nbest.append(\n","                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n","\n","        assert len(nbest) >= 1\n","\n","        total_scores = []\n","        best_non_null_entry = None\n","        for entry in nbest:\n","            total_scores.append(entry.start_logit + entry.end_logit)\n","            if not best_non_null_entry:\n","                if entry.text:\n","                    best_non_null_entry = entry\n","\n","        probs = _compute_softmax(total_scores)\n","\n","        nbest_json = []\n","        for (i, entry) in enumerate(nbest):\n","            output = collections.OrderedDict()\n","            output[\"text\"] = entry.text\n","            output[\"probability\"] = probs[i]\n","            output[\"start_logit\"] = entry.start_logit\n","            output[\"end_logit\"] = entry.end_logit\n","            nbest_json.append(output)\n","        assert len(nbest_json) >= 1\n","\n","        if not version_2_with_negative:\n","            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n","        else:\n","            # predict \"\" iff the null score - the score of best non-null > threshold\n","            score_diff = score_null - best_non_null_entry.start_logit - (\n","                best_non_null_entry.end_logit)\n","            scores_diff_json[example.qas_id] = score_diff\n","            if score_diff > null_score_diff_threshold:\n","                all_predictions[example.qas_id] = \"\"\n","            else:\n","                all_predictions[example.qas_id] = best_non_null_entry.text\n","        all_nbest_json[example.qas_id] = nbest_json\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n","\n","    with open(output_nbest_file, \"w\") as writer:\n","        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n","\n","    if version_2_with_negative:\n","        with open(output_null_log_odds_file, \"w\") as writer:\n","            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n","\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def f1_score(prediction, ground_truth):\n","    prediction_tokens = normalize_answer(prediction).split()\n","    ground_truth_tokens = normalize_answer(ground_truth).split()\n","    common = collections.Counter(\n","        prediction_tokens) & collections.Counter(ground_truth_tokens)\n","    num_same = sum(common.values())\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(prediction_tokens)\n","    recall = 1.0 * num_same / len(ground_truth_tokens)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def exact_match_score(prediction, ground_truth):\n","    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n","\n","\n","def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n","    scores_for_ground_truths = []\n","    for ground_truth in ground_truths:\n","        score = metric_fn(prediction, ground_truth)\n","        scores_for_ground_truths.append(score)\n","    return max(scores_for_ground_truths)\n","\n","\n","def evaluate(dataset, predictions):\n","    f1 = exact_match = total = 0\n","    for article in dataset:\n","        for paragraph in article['paragraphs']:\n","            for qa in paragraph['qas']:\n","                total += 1\n","                if qa['id'] not in predictions:\n","                    message = 'Unanswered question ' + qa['id'] + \\\n","                              ' will receive score 0.'\n","                    print(message, file=sys.stderr)\n","                    continue\n","                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n","                prediction = predictions[qa['id']]\n","                exact_match += metric_max_over_ground_truths(\n","                    exact_match_score, prediction, ground_truths)\n","                f1 += metric_max_over_ground_truths(\n","                    f1_score, prediction, ground_truths)\n","\n","    exact_match = 100.0 * exact_match / total\n","    f1 = 100.0 * f1 / total\n","\n","    return {'exact_match': exact_match, 'f1': f1}\n","\n","\n","def read_predictions(prediction_file):\n","    with open(prediction_file) as f:\n","        predictions = json.load(f)\n","    return predictions\n","\n","\n","def read_answers(gold_file):\n","    answers = {}\n","    with gzip.open(gold_file, 'rb') as f:\n","        for i, line in enumerate(f):\n","            example = json.loads(line)\n","            if i == 0 and 'header' in example:\n","                continue\n","            for qa in example['qas']:\n","                answers[qa['qid']] = qa['answers']\n","    return answers\n","\n","\n","def evaluate_mrqa(answers, predictions, skip_no_answer=False):\n","    f1 = exact_match = total = 0\n","    for qid, ground_truths in answers.items():\n","        if qid not in predictions:\n","            if not skip_no_answer:\n","                message = 'Unanswered question %s will receive score 0.' % qid\n","                print(message)\n","                total += 1\n","            continue\n","        total += 1\n","        prediction = predictions[qid]\n","        exact_match += metric_max_over_ground_truths(\n","            exact_match_score, prediction, ground_truths)\n","        f1 += metric_max_over_ground_truths(\n","            f1_score, prediction, ground_truths)\n","\n","    exact_match = 100.0 * exact_match / total\n","    f1 = 100.0 * f1 / total\n","\n","    return {'exact_match': exact_match, 'f1': f1}\n"],"metadata":{"id":"v4nUjfMLq5I_","executionInfo":{"status":"ok","timestamp":1683590164716,"user_tz":-60,"elapsed":4828,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## utils.py"],"metadata":{"id":"GQoXIRcirLLu"}},{"cell_type":"code","source":["import random\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def get_squad_data_loader(tokenizer, file, shuffle, args):\n","    examples = read_squad_examples(file, is_training=True, debug=args.debug)\n","    features = convert_examples_to_features_answer_id(examples,\n","                                                      tokenizer=tokenizer,\n","                                                      max_seq_length=args.max_c_len,\n","                                                      max_query_length=args.max_q_len,\n","                                                      max_ans_length=args.max_q_len,\n","                                                      doc_stride=128,\n","                                                      is_training=True)\n","\n","    all_c_ids = torch.tensor([f.c_ids for f in features], dtype=torch.long)\n","    all_q_ids = torch.tensor([f.q_ids for f in features], dtype=torch.long)\n","    all_qtype_ids = torch.tensor([f.qtype_ids for f in features], dtype=torch.long)\n","    all_tag_ids = torch.tensor([f.tag_ids for f in features], dtype=torch.long)\n","    all_a_ids = (all_tag_ids != 0).long()\n","    all_start_positions = torch.tensor([f.noq_start_position for f in features], dtype=torch.long)\n","    all_end_positions = torch.tensor([f.noq_end_position for f in features], dtype=torch.long)\n","\n","    all_data = TensorDataset(all_c_ids, all_q_ids, all_a_ids, all_start_positions, all_end_positions,all_qtype_ids)\n","    data_loader = DataLoader(all_data, args.batch_size, shuffle=shuffle)\n","\n","    return data_loader, examples, features\n","\n","def get_harv_data_loader(tokenizer, file, shuffle, ratio, args):\n","    examples = read_squad_examples(file, is_training=True, debug=args.debug)\n","    random.shuffle(examples)\n","    num_ex = int(len(examples) * ratio)\n","    examples = examples[:num_ex]\n","    features = convert_examples_to_harv_features(examples,\n","                                                 tokenizer=tokenizer,\n","                                                 max_seq_length=args.max_c_len,\n","                                                 max_query_length=args.max_q_len,\n","                                                 doc_stride=128,\n","                                                 is_training=True)\n","    all_c_ids = torch.tensor([f.c_ids for f in features], dtype=torch.long)\n","    dataset = TensorDataset(all_c_ids)\n","    dataloader = DataLoader(dataset, shuffle=shuffle, batch_size=args.batch_size)\n","\n","    return features, dataloader\n","\n","def batch_to_device(batch, device):\n","    batch = (b.to(device) for b in batch)\n","    c_ids, q_ids, a_ids, start_positions, end_positions, all_qtype_ids = batch\n","\n","    c_len = torch.sum(torch.sign(c_ids), 1)\n","    max_c_len = torch.max(c_len)\n","    c_ids = c_ids[:, :max_c_len]\n","    a_ids = a_ids[:, :max_c_len]\n","\n","    q_len = torch.sum(torch.sign(q_ids), 1)\n","    max_q_len = torch.max(q_len)\n","    q_ids = q_ids[:, :max_q_len]\n","\n","    return c_ids, q_ids, a_ids, start_positions, end_positions, all_qtype_ids\n"],"metadata":{"id":"mqiaf4XerUKH","executionInfo":{"status":"ok","timestamp":1683590164717,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Rest"],"metadata":{"id":"xpZvNnAKqmmH"}},{"cell_type":"code","source":["# def return_mask_lengths(ids):\n","#     mask = torch.sign(ids).float()\n","#     lengths = torch.sum(mask, 1).long()\n","#     return mask, lengths\n","\n","\n","# def post_process(q_ids, start_positions, end_positions, c_ids, total_max_len=384):\n","#     \"\"\"\n","#        concatenate question and context for BERT QA model:\n","#        [CLS] Question [SEP] Context [SEP]\n","#     \"\"\"\n","#     batch_size = q_ids.size(0)\n","#     # exclude CLS token in c_ids\n","#     c_ids = c_ids[:, 1:]\n","#     start_positions = start_positions - 1\n","#     end_positions = end_positions - 1\n","\n","#     _, q_lengths = return_mask_lengths(q_ids)\n","#     _, c_lengths = return_mask_lengths(c_ids)\n","\n","#     all_input_ids = []\n","#     all_seg_ids = []\n","#     for i in range(batch_size):\n","#         q_length = q_lengths[i]\n","#         c_length = c_lengths[i]\n","#         q = q_ids[i, :q_length]  # exclude pad tokens\n","#         c = c_ids[i, :c_length]  # exclude pad tokens\n"],"metadata":{"id":"tlGzOs11U5Br","executionInfo":{"status":"ok","timestamp":1683590164718,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","def loadDataset(args,chosen_q_type=None):  \n","    tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n","    args.tokenizer = tokenizer\n","\n","    device = torch.cuda.current_device()\n","    checkpoint = torch.load(args.checkpoint, map_location=\"cpu\")\n","    vae = DiscreteVAE(checkpoint[\"args\"])\n","    vae.load_state_dict(checkpoint[\"state_dict\"])\n","    vae.eval()\n","    vae = vae.to(device)\n","    \n","    examples = read_squad_examples(args.file, is_training=True, debug=args.debug)\n","    indices = np.sort(np.random.choice(np.arange(len(examples)), min(len(examples),args.maxParaCount), replace=False))\n","    examples = [examples[i] for i in indices]\n","    features = convert_examples_to_features_answer_id(examples,\n","                                                      tokenizer=tokenizer,\n","                                                      max_seq_length=args.max_c_len,\n","                                                      max_query_length=args.max_q_len,\n","                                                      max_ans_length=args.max_q_len,\n","                                                      doc_stride=128,\n","                                                      is_training=True)\n","    all_c_ids = torch.tensor([f.c_ids for f in features], dtype=torch.long)\n","    if args.maxParaCount < 1000:\n","      n = all_c_ids.shape[0]\n","      all_c_ids = all_c_ids.repeat(8,1)\n","      all_qtype_ids = torch.zeros((all_c_ids.shape[0],8))\n","      for i in range(8):\n","        all_qtype_ids[i*n:(i+1)*n,i] = 1\n","    else:\n","      all_qtype_ids = torch.tensor([f.qtype_ids if not chosen_q_type else chosen_q_type for f in features], dtype=torch.long)\n","\n","    all_data = TensorDataset(all_c_ids,all_qtype_ids)\n","    data_loader = DataLoader(all_data, args.batch_size, shuffle=False)\n","\n","    return data_loader, tokenizer, vae\n","\n","\n","def main(args,data_loader, tokenizer, vae):\n","    genList = list()\n","    genPairs = dict()\n","    contextPairs = dict()\n","    for batch in tqdm(data_loader, total=len(data_loader)):\n","        c_ids = batch[0].to(device)\n","        qtype_ids = batch[1].to(device)\n","        \n","        # sample latent variable K times\n","        for _ in range(args.k):\n","            with torch.no_grad():\n","                _, _, zq, _, za = vae.prior_encoder(c_ids, qtype_ids)\n","                batch_q_ids, batch_start, batch_end = vae.generate(zq, za, c_ids, qtype_ids)\n","\n","            for i in range(c_ids.size(0)):\n","                _c_ids = c_ids[i].cpu().tolist()\n","                q_ids = batch_q_ids[i].cpu().tolist()\n","                start_pos_a = batch_start[i].item()\n","                end_pos_a = batch_end[i].item()\n","                q_one_hot_vector = tuple(qtype_ids[i].cpu().tolist())\n","                \n","                a_ids = _c_ids[start_pos_a: end_pos_a+1]\n","                c_text = tokenizer.decode(_c_ids, skip_special_tokens=True)\n","                q_text = tokenizer.decode(q_ids, skip_special_tokens=True)\n","                a_text = tokenizer.decode(a_ids, skip_special_tokens=True)\n","                \n","                json_dict = {\n","                    \"context\":c_text,\n","                    \"question\": q_text,\n","                    \"answer\": a_text,\n","                    \"q_vector\": q_one_hot_vector,\n","                    \"zq\": zq[i].cpu().tolist(),\n","                    \"za\": za[i].cpu().tolist()\n","                }\n","                genList.append(json_dict)\n","                contextPairs[(tuple(q_one_hot_vector),c_text)] = json_dict\n","                if q_one_hot_vector in genPairs: \n","                    genPairs[q_one_hot_vector].append(json_dict)\n","                else: \n","                    genPairs[q_one_hot_vector] = [json_dict] \n","\n","    return genPairs,genList,contextPairs"],"metadata":{"id":"8PDSFn_RU612","executionInfo":{"status":"ok","timestamp":1683590256468,"user_tz":-60,"elapsed":447,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__\n","\n","def setArguments(dataFile,checkpoint, maxParaCount=float(\"inf\"), seed=1004, bert_model=\"bert-base-uncased\", max_length=384, batch_size=64, ratio=1, k = 1):\n","    args = dict()\n","    args['maxParaCount'] = maxParaCount\n","    args['seed'] = seed\n","    args['bert_model'] = bert_model\n","    args['max_length'] = max_length\n","    args['batch_size'] = batch_size\n","    args['file'] = dataFile\n","    args['checkpoint'] = checkpoint#\"../save/vae-checkpoint/controlled_qgen/best_f1_model.pt\"\n","    args['output_dir'] = \"../data/synthetic_data/\"\n","    args['ratio'] = ratio\n","    args['k'] = k\n","    args['max_c_len']=384\n","    args['max_q_len']=64\n","    args['max_d_len']=5\n","    args['squad'] = True\n","    \n","    return dotdict(args)    "],"metadata":{"id":"DIDGlGjUU-TI","executionInfo":{"status":"ok","timestamp":1683590164718,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# k = 1\n","# args = setArguments(dataFile='../data/squad/original data/my_test.json',maxParaCount=128,k=k)\n","# data_loader, tokenizer, vae = loadDataset(args, chosen_q_type=None)"],"metadata":{"id":"-3z4LhdaVEIr","executionInfo":{"status":"ok","timestamp":1683590164719,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# genPairs = main(args,data_loader, tokenizer, vae)"],"metadata":{"id":"sUOEl6x2VHUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = f\"../../Question Generation/save/vae-checkpoint/controlled_qgen/best_f1_model.pt\"\n","args = setArguments(dataFile='../data/squad/original data/my_test.json',k=1,checkpoint=checkpoint)\n","data_loader, tokenizer, vae = loadDataset(args)\n","genPairs, gen_controlled_list,_  = main(args,data_loader, tokenizer, vae)\n","import pickle\n","with open('../../Beta-HCVAE/data/gen_controlled_list.pickle', 'wb') as f:\n","  pickle.dump(gen_controlled_list, f)\n","with open('../../Beta-HCVAE/data/gen_controlled_pairs_dict.pickle', 'wb') as f:\n","  pickle.dump(genPairs, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdrvwM0lOPK4","executionInfo":{"status":"ok","timestamp":1680910641972,"user_tz":-60,"elapsed":423081,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"d105663d-c9ce-4f3c-c2dd-ca6f63b033b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 48/48 [00:01<00:00, 40.04it/s]\n","100%|██████████| 5378/5378 [00:52<00:00, 102.90it/s]\n","100%|██████████| 85/85 [06:01<00:00,  4.25s/it]\n"]}]},{"cell_type":"code","source":["checkpoint = f\"../../Question Generation/save/vae-checkpoint/controlled_qgen/best_f1_model.pt\"\n","args = setArguments(dataFile='../data/squad/original data/my_test.json',k=1,checkpoint=checkpoint,maxParaCount = 100)\n","data_loader, tokenizer, vae = loadDataset(args)\n","genPairs, gen_controlled_list,_  = main(args,data_loader, tokenizer, vae)\n","import pickle\n","with open('../../Beta-HCVAE/data/gen_controlled_list2.pickle', 'wb') as f:\n","  pickle.dump(gen_controlled_list, f)\n","with open('../../Beta-HCVAE/data/gen_controlled_pairs_dict2.pickle', 'wb') as f:\n","  pickle.dump(genPairs, f)"],"metadata":{"id":"wx-0sPHgzBfu","executionInfo":{"status":"ok","timestamp":1680910706161,"user_tz":-60,"elapsed":64207,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a13cfc09-b08c-4260-ae72-2c88356ac49b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 48/48 [00:01<00:00, 33.41it/s]\n","100%|██████████| 100/100 [00:01<00:00, 63.78it/s]\n","100%|██████████| 13/13 [00:56<00:00,  4.33s/it]\n"]}]},{"cell_type":"code","source":["len(all_keys)"],"metadata":{"id":"7vGoQAyb03mW","executionInfo":{"status":"ok","timestamp":1680116881336,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c5b18b9-8e5e-4cfe-c3a0-09748d7c6592"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["[(genPairs[all_keys[i]][0]['question'],genPairs[all_keys[i]][0]['answer']) for i in range(8)]"],"metadata":{"id":"Q5hE6lE01Kdc","executionInfo":{"status":"ok","timestamp":1680116906837,"user_tz":-60,"elapsed":442,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"816eeb0d-e59a-4857-e015-23760cce1360"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('what area has the first super bowl held in atlantic city during the 2014, 2013s.',\n","  'san francisco bay area'),\n"," ('how many passes for 739, does cam ginn ginn hold?', '44'),\n"," ('who are the actors watching for the game?',\n","  'kevin harlan as play - by - play announcer, boomer esiason and dan fouts'),\n"," (\"where did tesla's ideas begin to get high - frequency?\",\n","  'new york and colorado springs'),\n"," ('when did tesla become a naturalized citizen of the united states?',\n","  '30 july 1891'),\n"," ('in what rpm capacity was tesla tested?', '16, 000 rpm bladeless turbine'),\n"," ('which article defines the \" ordinary \" of the new kindnare?',\n","  'tfeu article 294 defines the \" ordinary legislative procedure \" that applies for most eu acts'),\n"," ('why is a library so accessible?', 'the curriculum led learning')]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# When set to give only 'why' output\n","# [(genPairs[all_keys[0]][i]['question'],genPairs[all_keys[0]][i]['answer']) for i in range(8)]\n","\"\"\"\n","[('why would nfl league opponents the league want to make the 50th super bowl?',\n","  'an important game for us as a league'),\n"," ('why was the carolina broncos second super bowl?',\n","  'their other appearance being super bowl xxxviii. coincidentally, both teams were coached by john fox in their last super bowl appearance'),\n"," ('why was the denver broncos skills during the week?',\n","  \"to blend in with quarterback peyton manning's shotgun passing skills, but struggled with numerous changes and injuries to the offensive line, as well as manning having his worst statistical season since his rookie year with the indianapolis colts in 1998, due to a plantar fasciitis injury in his heel that he had suffered since the summer, and the simple fact that manning was getting old, as he turned 39 in the 2015 off - season. although the team had a 7 – 0 start, manning led the nfl in interceptions. in week 10, manning suffered a partial tear of the plantar fasciitis in his left foot. he set the nfl's all - time record for career passing yards in this game, but was benched after throwing four interceptions in favor of backup quarterback brock osweiler, who took over as the starter for most of the remainder of the regular season. osweiler was injured, however, leading to manning's return during the week 17 regular season finale, where the broncos were losing 13 – 7 against the 4 – 11 san diego chargers, resulting in manning re - claiming the starting quarterback position for the playoffs by leading the team to a key 27 – 20 win that enabled the team to clinch the number one overall afc seed. under defensive coordinator wade phillips, the broncos'defense ranked number one in total yards allowed, passing yards allowed and sacks, and like the previous three seasons\"),\n"," ('why was the first time the broncos were in place in the nfl?',\n","  \"the broncos'defense ranked first in the nfl yards allowed\"),\n"," ('why was the first probol commission designed in the pro bowl?',\n","  'brandon marshall led the team'),\n"," ('why did the seattle radio get to carry the seattle seahawks in the interim round?',\n","  'running up a 31 – 0'),\n"," (\"why didn't the broncos make back in the first 10 years?\",\n","  \"despite manning's problems with interceptions\"),\n"," ('why was the logo replaced?', 'vince lombardi trophy')]\n","\"\"\""],"metadata":{"id":"bzx904szzxVf","executionInfo":{"status":"ok","timestamp":1680116750625,"user_tz":-60,"elapsed":440,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df9d7320-9bd1-48f5-d81f-e2deb9305d87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('why would nfl league opponents the league want to make the 50th super bowl?',\n","  'an important game for us as a league'),\n"," ('why was the carolina broncos second super bowl?',\n","  'their other appearance being super bowl xxxviii. coincidentally, both teams were coached by john fox in their last super bowl appearance'),\n"," ('why was the denver broncos skills during the week?',\n","  \"to blend in with quarterback peyton manning's shotgun passing skills, but struggled with numerous changes and injuries to the offensive line, as well as manning having his worst statistical season since his rookie year with the indianapolis colts in 1998, due to a plantar fasciitis injury in his heel that he had suffered since the summer, and the simple fact that manning was getting old, as he turned 39 in the 2015 off - season. although the team had a 7 – 0 start, manning led the nfl in interceptions. in week 10, manning suffered a partial tear of the plantar fasciitis in his left foot. he set the nfl's all - time record for career passing yards in this game, but was benched after throwing four interceptions in favor of backup quarterback brock osweiler, who took over as the starter for most of the remainder of the regular season. osweiler was injured, however, leading to manning's return during the week 17 regular season finale, where the broncos were losing 13 – 7 against the 4 – 11 san diego chargers, resulting in manning re - claiming the starting quarterback position for the playoffs by leading the team to a key 27 – 20 win that enabled the team to clinch the number one overall afc seed. under defensive coordinator wade phillips, the broncos'defense ranked number one in total yards allowed, passing yards allowed and sacks, and like the previous three seasons\"),\n"," ('why was the first time the broncos were in place in the nfl?',\n","  \"the broncos'defense ranked first in the nfl yards allowed\"),\n"," ('why was the first probol commission designed in the pro bowl?',\n","  'brandon marshall led the team'),\n"," ('why did the seattle radio get to carry the seattle seahawks in the interim round?',\n","  'running up a 31 – 0'),\n"," (\"why didn't the broncos make back in the first 10 years?\",\n","  \"despite manning's problems with interceptions\"),\n"," ('why was the logo replaced?', 'vince lombardi trophy')]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["checkpoint = f\"../../Question Generation/save/vae-checkpoint/controlled_qgen/best_f1_model.pt\"\n","args = setArguments(dataFile='../data/squad/original data/my_test.json',k=1,checkpoint=checkpoint,maxParaCount = 100)\n","data_loader, tokenizer, vae = loadDataset(args)\n","_,_,contextPairs  = main(args,data_loader, tokenizer, vae)"],"metadata":{"id":"fZKUDf8nwjj0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683590315888,"user_tz":-60,"elapsed":56307,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"ff5c3142-5123-4c45-f545-93080e98c6de"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 48/48 [00:01<00:00, 33.57it/s]\n","100%|██████████| 100/100 [00:02<00:00, 45.37it/s]\n","100%|██████████| 13/13 [00:46<00:00,  3.55s/it]\n"]}]},{"cell_type":"code","source":["context_example = list(contextPairs.keys())[8*5][1]\n","context_example"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"U8tMCoEzy4Wm","executionInfo":{"status":"ok","timestamp":1683591520238,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"d5afe014-032a-4adc-ad07-419423bd0c3b"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the rocks collected from the moon are extremely old compared to rocks found on earth, as measured by radiometric dating techniques. they range in age from about 3. 2 billion years for the basaltic samples derived from the lunar maria, to about 4. 6 billion years for samples derived from the highlands crust. as such, they represent samples from a very early period in the development of the solar system, that are largely absent on earth. one important rock found during the apollo program is dubbed the genesis rock, retrieved by astronauts david scott and james irwin during the apollo 15 mission. this anorthosite rock is composed almost exclusively of the calcium - rich feldspar mineral anorthite, and is believed to be representative of the highland crust. a geochemical component called kreep was discovered, which has no known terrestrial counterpart. kreep and the anorthositic samples have been used to infer that the outer portion of the moon was once completely molten ( see lunar magma ocean ).'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["[(contextPairs[(tuple(i==j for j in range(8)), context_example)]['question'],contextPairs[(tuple(i==j for j in range(8)), context_example)]['answer']) for i in range(8)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CE5OPi1xzd2O","executionInfo":{"status":"ok","timestamp":1683591520841,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"89e00180-2d8d-49b0-a23d-41550708e767"},"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('who retrieved genesis from the moon?',\n","  'astronauts david scott and james irwin'),\n"," ('what is used to show the outer portion of the moon?', 'anorthositic'),\n"," ('when was the genesis of the genesis mariac samples derived from the rocks crust samples?',\n","  '3. 2 billion years'),\n"," ('where are the samples found in the development of the genesis ocean?',\n","  'the solar system, that are largely absent on earth'),\n"," ('why are the rocks collected on earth?',\n","  'samples derived from the highlands crust'),\n"," ('which scientist analyze the rock on the earth?', 'radiometric'),\n"," ('how long is the term for the solar rock collected from the basalt on earth?',\n","  '3. 2 billion years'),\n"," ('the solar rock found from the highlands crust is derived from what period of time?',\n","  'early period')]"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":[],"metadata":{"id":"8eObtFqv0MNk"},"execution_count":null,"outputs":[]}]}