The phrase "51st state" can be used in a positive sense, meaning that a region or territory is so aligned, supportive, and conducive with the United States, that it is like a U.S. state. It can also be used in a pejorative sense, meaning an area or region is perceived to be under excessive American cultural or military influence or control. In various countries around the world, people who believe their local or national culture has become too Americanized sometimes use the term "51st state" in reference to their own countries.
Under Article IV, Section Three of the United States Constitution, which outlines the relationship among the states, Congress has the power to admit new states to the union. The states are required to give "full faith and credit" to the acts of each other's legislatures and courts, which is generally held to include the recognition of legal contracts, marriages, and criminal judgments. The states are guaranteed military and civil defense by the federal government, which is also obliged by Article IV, Section Four, to "guarantee to every state in this union a republican form of government".
Puerto Rico has been discussed as a potential 51st state of the United States. In a 2012 status referendum a majority of voters, 54%, expressed dissatisfaction with the current political relationship. In a separate question, 61% of voters supported statehood (excluding the 26% of voters who left this question blank). On December 11, 2012, Puerto Rico's legislature resolved to request that the President and the U.S. Congress act on the results, end the current form of territorial status and begin the process of admitting Puerto Rico to the Union as a state.
Since 1898, Puerto Rico has had limited representation in the Congress in the form of a Resident Commissioner, a nonvoting delegate. The 110th Congress returned the Commissioner's power to vote in the Committee of the Whole, but not on matters where the vote would represent a decisive participation. Puerto Rico has elections on the United States presidential primary or caucus of the Democratic Party and the Republican Party to select delegates to the respective parties' national conventions although presidential electors are not granted on the Electoral College. As American citizens, Puerto Ricans can vote in U.S. presidential elections, provided they reside in one of the 50 states or the District of Columbia and not in Puerto Rico itself.
Residents of Puerto Rico pay U.S. federal taxes: import/export taxes, federal commodity taxes, social security taxes, therefore contributing to the American Government. Most Puerto Rico residents do not pay federal income tax but do pay federal payroll taxes (Social Security and Medicare). However, federal employees, those who do business with the federal government, Puerto Rico–based corporations that intend to send funds to the U.S. and others do pay federal income taxes. Puerto Ricans may enlist in the U.S. military. Puerto Ricans have participated in all American wars since 1898; 52 Puerto Ricans had been killed in the Iraq War and War in Afghanistan by November 2012.
Puerto Rico has been under U.S. sovereignty for over a century when it was ceded to the U.S. by Spain following the end of the Spanish–American War, and Puerto Ricans have been U.S. citizens since 1917. The island's ultimate status has not been determined as of 2012[update], its residents do not have voting representation in their federal government. Puerto Rico has limited representation in the U.S. Congress in the form of a Resident Commissioner, a delegate with limited no voting rights. Like the states, Puerto Rico has self-rule, a republican form of government organized pursuant to a constitution adopted by its people, and a bill of rights.
This constitution was created when the U.S. Congress directed local government to organize a constitutional convention to write the Puerto Rico Constitution in 1951. The acceptance of that constitution by Puerto Rico's electorate, the U.S. Congress, and the U.S. president occurred in 1952. In addition, the rights, privileges and immunities attendant to United States citizens are "respected in Puerto Rico to the same extent as though Puerto Rico were a state of the union" through the express extension of the Privileges and Immunities Clause of the U.S. Constitution by the U.S. Congress in 1948.
Puerto Rico is designated in its constitution as the "Commonwealth of Puerto Rico". The Constitution of Puerto Rico which became effective in 1952 adopted the name of Estado Libre Asociado (literally translated as "Free Associated State"), officially translated into English as Commonwealth, for its body politic. The island is under the jurisdiction of the Territorial Clause of the U.S. Constitution, which has led to doubts about the finality of the Commonwealth status for Puerto Rico. In addition, all people born in Puerto Rico become citizens of the U.S. at birth (under provisions of the Jones–Shafroth Act in 1917), but citizens residing in Puerto Rico cannot vote for president nor for full members of either house of Congress. Statehood would grant island residents full voting rights at the Federal level. The Puerto Rico Democracy Act (H.R. 2499) was approved on April 29, 2010, by the United States House of Representatives 223–169, but was not approved by the Senate before the end of the 111th Congress. It would have provided for a federally sanctioned self-determination process for the people of Puerto Rico. This act would provide for referendums to be held in Puerto Rico to determine the island's ultimate political status. It had also been introduced in 2007.
In November 2012, a referendum resulted in 54 percent of respondents voting to reject the current status under the territorial clause of the U.S. Constitution, while a second question resulted in 61 percent of voters identifying statehood as the preferred alternative to the current territorial status. The 2012 referendum was by far the most successful referendum for statehood advocates and support for statehood has risen in each successive popular referendum. However, more than one in four voters abstained from answering the question on the preferred alternative status. Statehood opponents have argued that the statehood option garnered only 45 percent of the votes if abstentions are included. If abstentions are considered, the result of the referendum is much closer to 44 percent for statehood, a number that falls under the 50 percent majority mark.
The Washington Post, The New York Times and the Boston Herald have published opinion pieces expressing support for the statehood of Puerto Rico. On November 8, 2012, Washington, D.C. newspaper The Hill published an article saying that Congress will likely ignore the results of the referendum due to the circumstances behind the votes. and U.S. Congressman Luis Gutiérrez U.S. Congresswoman Nydia Velázquez, both of Puerto Rican ancestry, agreed with the The Hill 's statements. Shortly after the results were published Puerto Rico-born U.S. Congressman José Enrique Serrano commented "I was particularly impressed with the outcome of the 'status' referendum in Puerto Rico. A majority of those voting signaled the desire to change the current territorial status. In a second question an even larger majority asked to become a state. This is an earthquake in Puerto Rican politics. It will demand the attention of Congress, and a definitive answer to the Puerto Rican request for change. This is a history-making moment where voters asked to move forward."
Several days after the referendum, the Resident Commissioner Pedro Pierluisi, Governor Luis Fortuño, and Governor-elect Alejandro García Padilla wrote separate letters to the President of the United States Barack Obama addressing the results of the voting. Pierluisi urged Obama to begin legislation in favor of the statehood of Puerto Rico, in light of its win in the referendum. Fortuño urged him to move the process forward. García Padilla asked him to reject the results because of their ambiguity. The White House stance related to the November 2012 plebiscite was that the results were clear, the people of Puerto Rico want the issue of status resolved, and a majority chose statehood in the second question. Former White House director of Hispanic media stated, "Now it is time for Congress to act and the administration will work with them on that effort, so that the people of Puerto Rico can determine their own future."
On May 15, 2013, Resident Commissioner Pierluisi introduced H.R. 2000 to Congress to "set forth the process for Puerto Rico to be admitted as a state of the Union," asking for Congress to vote on ratifying Puerto Rico as the 51st state. On February 12, 2014, Senator Martin Heinrich introduced a bill in the US Senate. The bill would require a binding referendum to be held in Puerto Rico asking whether the territory wants to be admitted as a state. In the event of a yes vote, the president would be asked to submit legislation to Congress to admit Puerto Rico as a state.
Washington, D.C. is often mentioned as a candidate for statehood. In Federalist No. 43 of The Federalist Papers, James Madison considered the implications of the definition of the "seat of government" found in the United States Constitution. Although he noted potential conflicts of interest, and the need for a "municipal legislature for local purposes," Madison did not address the district's role in national voting. Legal scholars disagree on whether a simple act of Congress can admit the District as a state, due to its status as the seat of government of the United States, which Article I, Section 8 of the Constitution requires to be under the exclusive jurisdiction of Congress; depending on the interpretation of this text, admission of the full District as a state may require a Constitutional amendment, which is much more difficult to enact. However, the Constitution does not set a minimum size for the District. Its size has already changed once before, when Virginia reclaimed the portion of the District south of the Potomac. So the constitutional requirement for a federal district can be satisfied by reducing its size to the small central core of government buildings and monuments, giving the rest of the territory to the new state.
Washington, D.C. residents who support the statehood movement sometimes use a shortened version of the Revolutionary War protest motto "No taxation without representation", omitting the initial "No", denoting their lack of Congressional representation; the phrase is now printed on newly issued Washington, D.C. license plates (although a driver may choose to have the Washington, D.C. website address instead). President Bill Clinton's presidential limousine had the "Taxation without representation" license plate late in his term, while President George W. Bush had the vehicle's plates changed shortly after beginning his term in office. President Barack Obama had the license plates changed back to the protest style at the beginning of his second term.
This position was carried by the D.C. Statehood Party, a political party; it has since merged with the local Green Party affiliate to form the D.C. Statehood Green Party. The nearest this movement ever came to success was in 1978, when Congress passed the District of Columbia Voting Rights Amendment. Two years later in 1980, local citizens passed an initiative calling for a constitutional convention for a new state. In 1982, voters ratified the constitution of the state, which was to be called New Columbia. The drive for statehood stalled in 1985, however, when the Washington, D.C. Voting Rights Amendment failed because not enough states ratified the amendment within the seven-year span specified.
Other less likely contenders are Guam and the United States Virgin Islands, both of which are unincorporated organized territories of the United States. Also, the Northern Mariana Islands and American Samoa, an unorganized, unincorporated territory, could both attempt to gain statehood. Some proposals call for the Virgin Islands to be admitted with Puerto Rico as one state (often known as the proposed "Commonwealth of Prusvi", for Puerto Rico/U.S. Virgin Islands, or as "Puerto Virgo"), and for the amalgamation of U.S. territories or former territories in the Pacific Ocean, in the manner of the "Greater Hawaii" concept of the 1960s. Guam and the Northern Mariana Islands would be admitted as one state, along with Palau, the Federated States of Micronesia, and the Marshall Islands (although these latter three entities are now separate sovereign nations, which have Compact of Free Association relationships with the United States). Such a state would have a population of 412,381 (slightly lower than Wyoming's population) and a land area of 911.82 square miles (2,361.6 km2) (slightly smaller than Rhode Island). American Samoa could possibly be part of such a state, increasing the population to 467,900 and the area to 988.65 square miles (2,560.6 km2). Radio Australia, in late May 2008, issued signs of Guam and the Northern Mariana Islands becoming one again and becoming the 51st state.
The Philippines has had small grassroots movements for U.S. statehood. Originally part of the platform of the Progressive Party, then known as the Federalista Party, the party dropped it in 1907, which coincided with the name change. As recently as 2004, the concept of the Philippines becoming a U.S. state has been part of a political platform in the Philippines. Supporters of this movement include Filipinos who believe that the quality of life in the Philippines would be higher and that there would be less poverty there if the Philippines were an American state or territory. Supporters also include Filipinos that had fought as members of the United States Armed Forces in various wars during the Commonwealth period.
In Canada, "the 51st state" is a phrase generally used in such a way as to imply that if a certain political course is taken, Canada's destiny will be as little more than a part of the United States. Examples include the Canada-United States Free Trade Agreement in 1988, the debate over the creation of a common defense perimeter, and as a potential consequence of not adopting proposals intended to resolve the issue of Quebec sovereignty, the Charlottetown Accord in 1992 and the Clarity Act in 1999.
The phrase is usually used in local political debates, in polemic writing or in private conversations. It is rarely used by politicians themselves in a public context, although at certain times in Canadian history political parties have used other similarly loaded imagery. In the 1988 federal election, the Liberals asserted that the proposed Free Trade Agreement amounted to an American takeover of Canada—notably, the party ran an ad in which Progressive Conservative (PC) strategists, upon the adoption of the agreement, slowly erased the Canada-U.S. border from a desktop map of North America. Within days, however, the PCs responded with an ad which featured the border being drawn back on with a permanent marker, as an announcer intoned "Here's where we draw the line."
The implication has historical basis and dates to the breakup of British America during the American Revolution. The colonies that had confederated to form the United States invaded Canada (at the time a term referring specifically to the modern-day provinces of Quebec and Ontario, which had only been in British hands since 1763) at least twice, neither time succeeding in taking control of the territory. The first invasion was during the Revolution, under the assumption that French-speaking Canadians' presumed hostility towards British colonial rule combined with the Franco-American alliance would make them natural allies to the American cause; the Continental Army successfully recruited two Canadian regiments for the invasion. That invasion's failure forced the members of those regiments into exile, and they settled mostly in upstate New York. The Articles of Confederation, written during the Revolution, included a provision for Canada to join the United States, should they ever decide to do so, without needing to seek U.S. permission as other states would. The United States again invaded Canada during the War of 1812, but this effort was made more difficult due to the large number of Loyalist Americans that had fled to what is now Ontario and still resisted joining the republic. The Hunter Patriots in the 1830s and the Fenian raids after the American Civil War were private attacks on Canada from the U.S. Several U.S. politicians in the 19th century also spoke in favour of annexing Canada.
In the late 1940s, during the last days of the Dominion of Newfoundland (at the time a dominion-dependency in the Commonwealth and independent of Canada), there was mainstream support, although not majority, for Newfoundland to form an economic union with the United States, thanks to the efforts of the Economic Union Party and significant U.S. investment in Newfoundland stemming from the U.S.-British alliance in World War II. The movement ultimately failed when, in a 1948 referendum, voters narrowly chose to confederate with Canada (the Economic Union Party supported an independent "responsible government" that they would then push toward their goals).
In the United States, the term "the 51st state" when applied to Canada can serve to highlight the similarities and close relationship between the United States and Canada. Sometimes the term is used disparagingly, intended to deride Canada as an unimportant neighbor. In the Quebec general election, 1989, the political party Parti 51 ran 11 candidates on a platform of Quebec seceding from Canada to join the United States (with its leader, André Perron, claiming Quebec could not survive as an independent nation). The party attracted just 3,846 votes across the province, 0.11% of the total votes cast. In comparison, the other parties in favour of sovereignty of Quebec in that election got 40.16% (PQ) and 1.22% (NPDQ).
Due to geographical proximity of the Central American countries to the U.S. which has powerful military, economic, and political influences, there were several movements and proposals by the United States during the 19th and 20th centuries to annex some or all of the Central American republics (Costa Rica, El Salvador, Guatemala, Honduras with the formerly British-ruled Bay Islands, Nicaragua, Panama which had the U.S.-ruled Canal Zone territory from 1903 to 1979, and formerly British Honduras or Belize since 1981). However, the U.S. never acted on these proposals from some U.S. politicians; some of which were never delivered or considered seriously. In 2001, El Salvador adopted the U.S. dollar as its currency, while Panama has used it for decades due to its ties to the Canal Zone.
Cuba, like many Spanish territories, wanted to break free from Spain. A pro-independence movement in Cuba was supported by the U.S., and Cuban guerrilla leaders wanted annexation to the United States, but Cuban revolutionary leader José Martí called for Cuban nationhood. When the U.S. battleship Maine sank in Havana Harbor, the U.S. blamed Spain and the Spanish–American War broke out in 1898. After the U.S. won, Spain relinquished claim of sovereignty over territories, including Cuba. The U.S. administered Cuba as a protectorate until 1902. Several decades later in 1959, the corrupt Cuban government of U.S.-backed Fulgencio Batista was overthrown by Fidel Castro. Castro installed a Marxist–Leninist government allied with the Soviet Union, which has been in power ever since.
Several websites assert that Israel is the 51st state due to the annual funding and defense support it receives from the United States. An example of this concept can be found in 2003 when Martine Rothblatt published a book called Two Stars for Peace that argued for the addition of Israel and the Palestinian territories surrounding it as the 51st state in the Union. The American State of Canaan, is a book published by Prof. Alfred de Grazia, political science and sociologist, in March 2009, proposing the creation of the 51st and 52nd states from Israel and the Palestinian territories.
In Article 3 of the Treaty of San Francisco between the Allied Powers and Japan, which came into force in April 1952, the U.S. put the outlying islands of the Ryukyus, including the island of Okinawa—home to over 1 million Okinawans related to the Japanese—and the Bonin Islands, the Volcano Islands, and Iwo Jima into U.S. trusteeship. All these trusteeships were slowly returned to Japanese rule. Okinawa was returned on May 15, 1972, but the U.S. stations troops in the island's bases as a defense for Japan.
In 2010 there was an attempt to register a 51st State Party with the New Zealand Electoral Commission. The party advocates New Zealand becoming the 51st state of the United States of America. The party's secretary is Paulus Telfer, a former Christchurch mayoral candidate. On February 5, 2010, the party applied to register a logo with the Electoral Commission. The logo – a US flag with 51 stars – was rejected by the Electoral Commission on the grounds that it was likely to cause confusion or mislead electors. As of 2014[update], the party remains unregistered and cannot appear on a ballot.
Albania has often been called the 51st state for its perceived strongly pro-American positions, mainly because of the United States' policies towards it. In reference to President George W. Bush's 2007 European tour, Edi Rama, Tirana's mayor and leader of the opposition Socialists, said: "Albania is for sure the most pro-American country in Europe, maybe even in the world ... Nowhere else can you find such respect and hospitality for the President of the United States. Even in Michigan, he wouldn't be as welcome." At the time of ex-Secretary of State James Baker's visit in 1992, there was even a move to hold a referendum declaring the country as the 51st American state. In addition to Albania, Kosovo which is predominately Albanian is seen as a 51st state due to the heavily presence and influence of the United States. The US has had troops and the largest base outside US territory, Camp Bondsteel in the territory since 1999.
During World War II, when Denmark was occupied by Nazi Germany, the United States briefly controlled Greenland for battlefields and protection. In 1946, the United States offered to buy Greenland from Denmark for $100 million ($1.2 billion today) but Denmark refused to sell it. Several politicians and others have in recent years argued that Greenland could hypothetically be in a better financial situation as a part of the United States; for instance mentioned by professor Gudmundur Alfredsson at University of Akureyri in 2014. One of the actual reasons behind US interest in Greenland could be the vast natural resources of the island. According to Wikileaks, the U.S. appears to be highly interested in investing in the resource base of the island and in tapping the vast expected hydrocarbons off the Greenlandic coast.
Poland has historically been staunchly pro-American, dating back to General Tadeusz Kościuszko and Casimir Pulaski's involvement in the American Revolution. This pro-American stance was reinforced following favorable American intervention in World War I (leading to the creation of an independent Poland) and the Cold War (culminating in a Polish state independent of Soviet influence). Poland contributed a large force to the "Coalition of the Willing" in Iraq. A quote referring to Poland as "the 51st state" has been attributed to James Pavitt, then Central Intelligence Agency Deputy Director for Operations, especially in connection to extraordinary rendition.
The Party of Reconstruction in Sicily, which claimed 40,000 members in 1944, campaigned for Sicily to be admitted as a U.S. state. This party was one of several Sicilian separatist movements active after the downfall of Italian Fascism. Sicilians felt neglected or underrepresented by the Italian government after the annexation of 1861 that ended the rule of the Kingdom of the Two Sicilies based in Naples. The large population of Sicilians in America and the American-led Allied invasion of Sicily in July–August 1943 may have contributed to the sentiment.
There are four categories of terra nullius, land that is unclaimed by any state: the small unclaimed territory of Bir Tawil between Egypt and Sudan, Antarctica, the oceans, and celestial bodies such as the Moon or Mars. In the last three of these, international treaties (the Antarctic Treaty, the United Nations Convention on the Law of the Sea, and the Outer Space Treaty respectively) prevent colonization and potential statehood of any of these uninhabited (and, given current technology, not permanently inhabitable) territories.
Pitch is an auditory sensation in which a listener assigns musical tones to relative positions on a musical scale based primarily on their perception of the frequency of vibration. Pitch is closely related to frequency, but the two are not equivalent. Frequency is an objective, scientific attribute that can be measured. Pitch is each person's subjective perception of a sound, which cannot be directly measured. However, this does not necessarily mean that most people won't agree on which notes are higher and lower.
This creates a linear pitch space in which octaves have size 12, semitones (the distance between adjacent keys on the piano keyboard) have size 1, and A440 is assigned the number 69. (See Frequencies of notes.) Distance in this space corresponds to musical intervals as understood by musicians. An equal-tempered semitone is subdivided into 100 cents. The system is flexible enough to include "microtones" not found on standard piano keyboards. For example, the pitch halfway between C (60) and C♯ (61) can be labeled 60.5.
The relative pitches of individual notes in a scale may be determined by one of a number of tuning systems. In the west, the twelve-note chromatic scale is the most common method of organization, with equal temperament now the most widely used method of tuning that scale. In it, the pitch ratio between any two successive notes of the scale is exactly the twelfth root of two (or about 1.05946). In well-tempered systems (as used in the time of Johann Sebastian Bach, for example), different methods of musical tuning were used. Almost all of these systems have one interval in common, the octave, where the pitch of one note is double the frequency of another. For example, if the A above middle C is 440 Hz, the A an octave above that is 880 Hz (info).
According to the American National Standards Institute, pitch is the auditory attribute of sound according to which sounds can be ordered on a scale from low to high. Since pitch is such a close proxy for frequency, it is almost entirely determined by how quickly the sound wave is making the air vibrate and has almost nothing to do with the intensity, or amplitude, of the wave. That is, "high" pitch means very rapid oscillation, and "low" pitch corresponds to slower oscillation. Despite that, the idiom relating vertical height to sound pitch is shared by most languages. At least in English, it is just one of many deep conceptual metaphors that involve up/down. The exact etymological history of the musical sense of high and low pitch is still unclear. There is evidence that humans do actually perceive that the source of a sound is slightly higher or lower in vertical space when the sound frequency is increased or decreased.
A sound generated on any instrument produces many modes of vibration that occur simultaneously. A listener hears numerous frequencies at once. The vibration with the lowest frequency is called the fundamental frequency; the other frequencies are overtones. Harmonics are an important class of overtones with frequencies that are integer multiples of the fundamental. Whether or not the higher frequencies are integer multiples, they are collectively called the partials, referring to the different parts that make up the total spectrum.
The pitch of complex tones can be ambiguous, meaning that two or more different pitches can be perceived, depending upon the observer. When the actual fundamental frequency can be precisely determined through physical measurement, it may differ from the perceived pitch because of overtones, also known as upper partials, harmonic or otherwise. A complex tone composed of two sine waves of 1000 and 1200 Hz may sometimes be heard as up to three pitches: two spectral pitches at 1000 and 1200 Hz, derived from the physical frequencies of the pure tones, and the combination tone at 200 Hz, corresponding to the repetition rate of the waveform. In a situation like this, the percept at 200 Hz is commonly referred to as the missing fundamental, which is often the greatest common divisor of the frequencies present.
The just-noticeable difference (jnd) (the threshold at which a change is perceived) depends on the tone's frequency content. Below 500 Hz, the jnd is about 3 Hz for sine waves, and 1 Hz for complex tones; above 1000 Hz, the jnd for sine waves is about 0.6% (about 10 cents). The jnd is typically tested by playing two tones in quick succession with the listener asked if there was a difference in their pitches. The jnd becomes smaller if the two tones are played simultaneously as the listener is then able to discern beat frequencies. The total number of perceptible pitch steps in the range of human hearing is about 1,400; the total number of notes in the equal-tempered scale, from 16 to 16,000 Hz, is 120.
It is still possible for two sounds of indefinite pitch to clearly be higher or lower than one another. For instance, a snare drum sounds higher pitched than a bass drum though both have indefinite pitch, because its sound contains higher frequencies. In other words, it is possible and often easy to roughly discern the relative pitches of two sounds of indefinite pitch, but sounds of indefinite pitch do not neatly correspond to any specific pitch. A special type of pitch often occurs in free nature when sound reaches the ear of an observer directly from the source, and also after reflecting off a sound-reflecting surface. This phenomenon is called repetition pitch, because the addition of a true repetition of the original sound to itself is the basic prerequisite.
For example, one might refer to the A above middle C as a', A4, or 440 Hz. In standard Western equal temperament, the notion of pitch is insensitive to "spelling": the description "G4 double sharp" refers to the same pitch as A4; in other temperaments, these may be distinct pitches. Human perception of musical intervals is approximately logarithmic with respect to fundamental frequency: the perceived interval between the pitches "A220" and "A440" is the same as the perceived interval between the pitches A440 and A880. Motivated by this logarithmic perception, music theorists sometimes represent pitches using a numerical scale based on the logarithm of fundamental frequency. For example, one can adopt the widely used MIDI standard to map fundamental frequency, f, to a real number, p, as follows
Temporal theories offer an alternative that appeals to the temporal structure of action potentials, mostly the phase-locking and mode-locking of action potentials to frequencies in a stimulus. The precise way this temporal structure helps code for pitch at higher levels is still debated, but the processing seems to be based on an autocorrelation of action potentials in the auditory nerve. However, it has long been noted that a neural mechanism that may accomplish a delay—a necessary operation of a true autocorrelation—has not been found. At least one model shows that a temporal delay is unnecessary to produce an autocorrelation model of pitch perception, appealing to phase shifts between cochlear filters; however, earlier work has shown that certain sounds with a prominent peak in their autocorrelation function do not elicit a corresponding pitch percept, and that certain sounds without a peak in their autocorrelation function nevertheless elicit a pitch. To be a more complete model, autocorrelation must therefore apply to signals that represent the output of the cochlea, as via auditory-nerve interspike-interval histograms. Some theories of pitch perception hold that pitch has inherent octave ambiguities, and therefore is best decomposed into a pitch chroma, a periodic value around the octave, like the note names in western music—and a pitch height, which may be ambiguous, that indicates the octave the pitch is in.
In the Pre-Modern era, many people's sense of self and purpose was often expressed via a faith in some form of deity, be that in a single God or in many gods. Pre-modern cultures have not been thought of creating a sense of distinct individuality, though. Religious officials, who often held positions of power, were the spiritual intermediaries to the common person. It was only through these intermediaries that the general masses had access to the divine. Tradition was sacred to ancient cultures and was unchanging and the social order of ceremony and morals in a culture could be strictly enforced.
The term "modern" was coined in the 16th century to indicate present or recent times (ultimately derived from the Latin adverb modo, meaning "just now). The European Renaissance (about 1420–1630), which marked the transition between the Late Middle Ages and Early Modern times, started in Italy and was spurred in part by the rediscovery of classical art and literature, as well as the new perspectives gained from the Age of Discovery and the invention of the telescope and microscope, expanding the borders of thought and knowledge.
The term "Early Modern" was introduced in the English language in the 1930s. to distinguish the time between what we call Middle Ages and time of the late Enlightenment (1800) (when the meaning of the term Modern Ages was developing its contemporary form). It is important to note that these terms stem from European history. In usage in other parts of the world, such as in Asia, and in Muslim countries, the terms are applied in a very different way, but often in the context with their contact with European culture in the Age of Discovery.
In the Contemporary era, there were various socio-technological trends. Regarding the 21st century and the late modern world, the Information age and computers were forefront in use, not completely ubiquitous but often present in daily life. The development of Eastern powers was of note, with China and India becoming more powerful. In the Eurasian theater, the European Union and Russian Federation were two forces recently developed. A concern for Western world, if not the whole world, was the late modern form of terrorism and the warfare that has resulted from the contemporary terrorist acts.
In Asia, various Chinese dynasties and Japanese shogunates controlled the Asian sphere. In Japan, the Edo period from 1600 to 1868 is also referred to as the early modern period. And in Korea, from the rising of Joseon Dynasty to the enthronement of King Gojong is referred to as the early modern period. In the Americas, Native Americans had built a large and varied civilization, including the Aztec Empire and alliance, the Inca civilization, the Mayan Empire and cities, and the Chibcha Confederation. In the west, the European kingdoms and movements were in a movement of reformation and expansion. Russia reached the Pacific coast in 1647 and consolidated its control over the Russian Far East in the 19th century.
In China, urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil. Despite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with the treasure voyages of Zheng He.
The Qing dynasty (1644–1911) was founded after the fall of the Ming, the last Han Chinese dynasty, by the Manchus. The Manchus were formerly known as the Jurchens. When Beijing was captured by Li Zicheng's peasant rebels in 1644, the Chongzhen Emperor, the last Ming emperor, committed suicide. The Manchus then allied with former Ming general Wu Sangui and seized control of Beijing, which became the new capital of the Qing dynasty. The Mancus adopted the Confucian norms of traditional Chinese government in their rule of China proper. Schoppa, the editor of The Columbia Guide to Modern Chinese History argues, "A date around 1780 as the beginning of modern China is thus closer to what we know today as historical 'reality'. It also allows us to have a better baseline to understand the precipitous decline of the Chinese polity in the nineteenth and twentieth centuries."
Society in the Japanese "Tokugawa period" (Edo society), unlike the shogunates before it, was based on the strict class hierarchy originally established by Toyotomi Hideyoshi. The daimyo, or lords, were at the top, followed by the warrior-caste of samurai, with the farmers, artisans, and traders ranking below. In some parts of the country, particularly smaller regions, daimyo and samurai were more or less identical, since daimyo might be trained as samurai, and samurai might act as local lords. Otherwise, the largely inflexible nature of this social stratification system unleashed disruptive forces over time. Taxes on the peasantry were set at fixed amounts which did not account for inflation or other changes in monetary value. As a result, the tax revenues collected by the samurai landowners were worth less and less over time. This often led to numerous confrontations between noble but impoverished samurai and well-to-do peasants, ranging from simple local disturbances to much bigger rebellions. None, however, proved compelling enough to seriously challenge the established order until the arrival of foreign powers.
On the Indian subcontinent, the Mughal Empire ruled most of India in the early 18th century. The "classic period" ended with the death and defeat of Emperor Aurangzeb in 1707 by the rising Hindu Maratha Empire, although the dynasty continued for another 150 years. During this period, the Empire was marked by a highly centralized administration connecting the different regions. All the significant monuments of the Mughals, their most visible legacy, date to this period which was characterised by the expansion of Persian cultural influence in the Indian subcontinent, with brilliant literary, artistic, and architectural results. The Maratha Empire was located in the south west of present-day India and expanded greatly under the rule of the Peshwas, the prime ministers of the Maratha empire. In 1761, the Maratha army lost the Third Battle of Panipat which halted imperial expansion and the empire was then divided into a confederacy of Maratha states.
The development of New Imperialism saw the conquest of nearly all eastern hemisphere territories by colonial powers. The commercial colonization of India commenced in 1757, after the Battle of Plassey, when the Nawab of Bengal surrendered his dominions to the British East India Company, in 1765, when the Company was granted the diwani, or the right to collect revenue, in Bengal and Bihar, or in 1772, when the Company established a capital in Calcutta, appointed its first Governor-General, Warren Hastings, and became directly involved in governance.
The Maratha states, following the Anglo-Maratha wars, eventually lost to the British East India Company in 1818 with the Third Anglo-Maratha War. The rule lasted until 1858, when, after the Indian rebellion of 1857 and consequent of the Government of India Act 1858, the British government assumed the task of directly administering India in the new British Raj. In 1819 Stamford Raffles established Singapore as a key trading post for Britain in their rivalry with the Dutch. However, their rivalry cooled in 1824 when an Anglo-Dutch treaty demarcated their respective interests in Southeast Asia. From the 1850s onwards, the pace of colonization shifted to a significantly higher gear.
The Dutch East India Company (1800) and British East India Company (1858) were dissolved by their respective governments, who took over the direct administration of the colonies. Only Thailand was spared the experience of foreign rule, although, Thailand itself was also greatly affected by the power politics of the Western powers. Colonial rule had a profound effect on Southeast Asia. While the colonial powers profited much from the region's vast resources and large market, colonial rule did develop the region to a varying extent.
Many major events caused Europe to change around the start of the 16th century, starting with the Fall of Constantinople in 1453, the fall of Muslim Spain and the discovery of the Americas in 1492, and Martin Luther's Protestant Reformation in 1517. In England the modern period is often dated to the start of the Tudor period with the victory of Henry VII over Richard III at the Battle of Bosworth in 1485. Early modern European history is usually seen to span from the start of the 15th century, through the Age of Reason and the Age of Enlightenment in the 17th and 18th centuries, until the beginning of the Industrial Revolution in the late 18th century.
Russia experienced territorial growth through the 17th century, which was the age of Cossacks. Cossacks were warriors organized into military communities, resembling pirates and pioneers of the New World. In 1648, the peasants of Ukraine joined the Zaporozhian Cossacks in rebellion against Poland-Lithuania during the Khmelnytsky Uprising, because of the social and religious oppression they suffered under Polish rule. In 1654 the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian Tsar, Aleksey I. Aleksey's acceptance of this offer led to another Russo-Polish War (1654–1667). Finally, Ukraine was split along the river Dnieper, leaving the western part (or Right-bank Ukraine) under Polish rule and eastern part (Left-bank Ukraine and Kiev) under Russian. Later, in 1670–71 the Don Cossacks led by Stenka Razin initiated a major uprising in the Volga region, but the Tsar's troops were successful in defeating the rebels. In the east, the rapid Russian exploration and colonisation of the huge territories of Siberia was led mostly by Cossacks hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian river routes, and by the mid-17th century there were Russian settlements in the Eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the Pacific coast. In 1648 the Bering Strait between Asia and North America was passed for the first time by Fedot Popov and Semyon Dezhnyov.
Traditionally, the European intellectual transformation of and after the Renaissance bridged the Middle Ages and the Modern era. The Age of Reason in the Western world is generally regarded as being the start of modern philosophy, and a departure from the medieval approach, especially Scholasticism. Early 17th-century philosophy is often called the Age of Rationalism and is considered to succeed Renaissance philosophy and precede the Age of Enlightenment, but some consider it as the earliest part of the Enlightenment era in philosophy, extending that era to two centuries. The 18th century saw the beginning of secularization in Europe, rising to notability in the wake of the French Revolution.
The Age of Enlightenment is a time in Western philosophy and cultural life centered upon the 18th century in which reason was advocated as the primary source and legitimacy for authority. Enlightenment gained momentum more or less simultaneously in many parts of Europe and America. Developing during the Enlightenment era, Renaissance humanism as an intellectual movement spread across Europe. The basic training of the humanist was to speak well and write (typically, in the form of a letter). The term umanista comes from the latter part of the 15th century. The people were associated with the studia humanitatis, a novel curriculum that was competing with the quadrivium and scholastic logic.
Renaissance humanism took a close study of the Latin and Greek classical texts, and was antagonistic to the values of scholasticism with its emphasis on the accumulated commentaries; and humanists were involved in the sciences, philosophies, arts and poetry of classical antiquity. They self-consciously imitated classical Latin and deprecated the use of medieval Latin. By analogy with the perceived decline of Latin, they applied the principle of ad fontes, or back to the sources, across broad areas of learning.
The quarrel of the Ancients and the Moderns was a literary and artistic quarrel that heated up in the early 1690s and shook the Académie française. The opposing two sides were, the Ancients (Anciens) who constrain choice of subjects to those drawn from the literature of Antiquity and the Moderns (Modernes), who supported the merits of the authors of the century of Louis XIV. Fontenelle quickly followed with his Digression sur les anciens et les modernes (1688), in which he took the Modern side, pressing the argument that modern scholarship allowed modern man to surpass the ancients in knowledge.
The Scientific Revolution was a period when European ideas in classical physics, astronomy, biology, human anatomy, chemistry, and other classical sciences were rejected and led to doctrines supplanting those that had prevailed from Ancient Greece to the Middle Ages which would lead to a transition to modern science. This period saw a fundamental transformation in scientific ideas across physics, astronomy, and biology, in institutions supporting scientific investigation, and in the more widely held picture of the universe. Individuals started to question all manners of things and it was this questioning that led to the Scientific Revolution, which in turn formed the foundations of contemporary sciences and the establishment of several modern scientific fields.
The changes were accompanied by violent turmoil which included the trial and execution of the king, vast bloodshed and repression during the Reign of Terror, and warfare involving every other major European power. Subsequent events that can be traced to the Revolution include the Napoleonic Wars, two separate restorations of the monarchy, and two additional revolutions as modern France took shape. In the following century, France would be governed at one point or another as a republic, constitutional monarchy, and two different empires.
The campaigns of French Emperor and General Napoleon Bonaparte characterized the Napoleonic Era. Born on Corsica as the French invaded, and dying suspiciously on the tiny British Island of St. Helena, this brilliant commander, controlled a French Empire that, at its height, ruled a large portion of Europe directly from Paris, while many of his friends and family ruled countries such as Spain, Poland, several parts of Italy and many other Kingdoms Republics and dependencies. The Napoleonic Era changed the face of Europe forever, and old Empires and Kingdoms fell apart as a result of the mighty and "Glorious" surge of Republicanism.
Italian unification was the political and social movement that annexed different states of the Italian peninsula into the single state of Italy in the 19th century. There is a lack of consensus on the exact dates for the beginning and the end of this period, but many scholars agree that the process began with the end of Napoleonic rule and the Congress of Vienna in 1815, and approximately ended with the Franco-Prussian War in 1871, though the last città irredente did not join the Kingdom of Italy until after World War I.
Beginning the Age of Revolution, the American Revolution and the ensuing political upheaval during the last half of the 18th century saw the Thirteen Colonies of North America overthrow the governance of the Parliament of Great Britain, and then reject the British monarchy itself to become the sovereign United States of America. In this period the colonies first rejected the authority of the Parliament to govern them without representation, and formed self-governing independent states. The Second Continental Congress then joined together against the British to defend that self-governance in the armed conflict from 1775 to 1783 known as the American Revolutionary War (also called American War of Independence).
The American Revolution begun with fighting at Lexington and Concord. On July 4, 1776, they issued the Declaration of Independence, which proclaimed their independence from Great Britain and their formation of a cooperative union. In June 1776, Benjamin Franklin was appointed a member of the Committee of Five that drafted the Declaration of Independence. Although he was temporarily disabled by gout and unable to attend most meetings of the Committee, Franklin made several small changes to the draft sent to him by Thomas Jefferson.
The decolonization of the Americas was the process by which the countries in the Americas gained their independence from European rule. Decolonization began with a series of revolutions in the late 18th and early-to-mid-19th centuries. The Spanish American wars of independence were the numerous wars against Spanish rule in Spanish America that took place during the early 19th century, from 1808 until 1829, directly related to the Napoleonic French invasion of Spain. The conflict started with short-lived governing juntas established in Chuquisaca and Quito opposing the composition of the Supreme Central Junta of Seville.
When the Central Junta fell to the French, numerous new Juntas appeared all across the Americas, eventually resulting in a chain of newly independent countries stretching from Argentina and Chile in the south, to Mexico in the north. After the death of the king Ferdinand VII, in 1833, only Cuba and Puerto Rico remained under Spanish rule, until the Spanish–American War in 1898. Unlike the Spanish, the Portuguese did not divide their colonial territory in America. The captaincies they created were subdued to a centralized administration in Salvador (later relocated to Rio de Janeiro) which reported directly to the Portuguese Crown until its independence in 1822, becoming the Empire of Brazil.
The first Industrial Revolution merged into the Second Industrial Revolution around 1850, when technological and economic progress gained momentum with the development of steam-powered ships and railways, and later in the 19th century with the internal combustion engine and electric power generation. The Second Industrial Revolution was a phase of the Industrial Revolution; labeled as the separate Technical Revolution. From a technological and a social point of view there is no clean break between the two. Major innovations during the period occurred in the chemical, electrical, petroleum, and steel industries. Specific advancements included the introduction of oil fired steam turbine and internal combustion driven steel ships, the development of the airplane, the practical commercialization of the automobile, mass production of consumer goods, the perfection of canning, mechanical refrigeration and other food preservation techniques, and the invention of the telephone.
Industrialization is the process of social and economic change whereby a human group is transformed from a pre-industrial society into an industrial one. It is a subdivision of a more general modernization process, where social change and economic development are closely related with technological innovation, particularly with the development of large-scale energy and metallurgy production. It is the extensive organization of an economy for the purpose of manufacturing. Industrialization also introduces a form of philosophical change, where people obtain a different attitude towards their perception of nature.
The modern petroleum industry started in 1846 with the discovery of the process of refining kerosene from coal by Nova Scotian Abraham Pineo Gesner. Ignacy Łukasiewicz improved Gesner's method to develop a means of refining kerosene from the more readily available "rock oil" ("petr-oleum") seeps in 1852 and the first rock oil mine was built in Bóbrka, near Krosno in Galicia in the following year. In 1854, Benjamin Silliman, a science professor at Yale University in New Haven, was the first to fractionate petroleum by distillation. These discoveries rapidly spread around the world.
Engineering achievements of the revolution ranged from electrification to developments in materials science. The advancements made a great contribution to the quality of life. In the first revolution, Lewis Paul was the original inventor of roller spinning, the basis of the water frame for spinning cotton in a cotton mill. Matthew Boulton and James Watt's improvements to the steam engine were fundamental to the changes brought by the Industrial Revolution in both the Kingdom of Great Britain and the world.
In the latter part of the second revolution, Thomas Alva Edison developed many devices that greatly influenced life around the world and is often credited with the creation of the first industrial research laboratory. In 1882, Edison switched on the world's first large-scale electrical supply network that provided 110 volts direct current to fifty-nine customers in lower Manhattan. Also toward the end of the second industrial revolution, Nikola Tesla made many contributions in the field of electricity and magnetism in the late 19th and early 20th centuries.
The European Revolutions of 1848, known in some countries as the Spring of Nations or the Year of Revolution, were a series of political upheavals throughout the European continent. Described as a revolutionary wave, the period of unrest began in France and then, further propelled by the French Revolution of 1848, soon spread to the rest of Europe. Although most of the revolutions were quickly put down, there was a significant amount of violence in many areas, with tens of thousands of people tortured and killed. While the immediate political effects of the revolutions were reversed, the long-term reverberations of the events were far-reaching.
Following the Enlightenment's ideas, the reformers looked to the Scientific Revolution and industrial progress to solve the social problems which arose with the Industrial Revolution. Newton's natural philosophy combined a mathematics of axiomatic proof with the mechanics of physical observation, yielding a coherent system of verifiable predictions and replacing a previous reliance on revelation and inspired truth. Applied to public life, this approach yielded several successful campaigns for changes in social policy.
Under Peter I (the Great), Russia was proclaimed an Empire in 1721 and became recognized as a world power. Ruling from 1682 to 1725, Peter defeated Sweden in the Great Northern War, forcing it to cede West Karelia and Ingria (two regions lost by Russia in the Time of Troubles), as well as Estland and Livland, securing Russia's access to the sea and sea trade. On the Baltic Sea Peter founded a new capital called Saint Petersburg, later known as Russia's Window to Europe. Peter the Great's reforms brought considerable Western European cultural influences to Russia. Catherine II (the Great), who ruled in 1762–96, extended Russian political control over the Polish-Lithuanian Commonwealth and incorporated most of its territories into Russia during the Partitions of Poland, pushing the Russian frontier westward into Central Europe. In the south, after successful Russo-Turkish Wars against the Ottoman Empire, Catherine advanced Russia's boundary to the Black Sea, defeating the Crimean khanate.
The Victorian era of the United Kingdom was the period of Queen Victoria's reign from June 1837 to January 1901. This was a long period of prosperity for the British people, as profits gained from the overseas British Empire, as well as from industrial improvements at home, allowed a large, educated middle class to develop. Some scholars would extend the beginning of the period—as defined by a variety of sensibilities and political games that have come to be associated with the Victorians—back five years to the passage of the Reform Act 1832.
In Britain's "imperial century", victory over Napoleon left Britain without any serious international rival, other than Russia in central Asia. Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the Pax Britannica, and a foreign policy of "splendid isolation". Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many nominally independent countries, such as China, Argentina and Siam, which has been generally characterized as "informal empire". Of note during this time was the Anglo-Zulu War, which was fought in 1879 between the British Empire and the Zulu Empire.
British imperial strength was underpinned by the steamship and the telegraph, new technologies invented in the second half of the 19th century, allowing it to control and defend the Empire. By 1902, the British Empire was linked together by a network of telegraph cables, the so-called All Red Line. Growing until 1922, around 13,000,000 square miles (34,000,000 km2) of territory and roughly 458 million people were added to the British Empire. The British established colonies in Australia in 1788, New Zealand in 1840 and Fiji in 1872, with much of Oceania becoming part of the British Empire.
The Bourbon Restoration followed the ousting of Napoleon I of France in 1814. The Allies restored the Bourbon Dynasty to the French throne. The ensuing period is called the Restoration, following French usage, and is characterized by a sharp conservative reaction and the re-establishment of the Roman Catholic Church as a power in French politics. The July Monarchy was a period of liberal constitutional monarchy in France under King Louis-Philippe starting with the July Revolution (or Three Glorious Days) of 1830 and ending with the Revolution of 1848. The Second Empire was the Imperial Bonapartist regime of Napoleon III from 1852 to 1870, between the Second Republic and the Third Republic, in France.
The Franco-Prussian War was a conflict between France and Prussia, while Prussia was backed up by the North German Confederation, of which it was a member, and the South German states of Baden, Württemberg and Bavaria. The complete Prussian and German victory brought about the final unification of Germany under King Wilhelm I of Prussia. It also marked the downfall of Napoleon III and the end of the Second French Empire, which was replaced by the Third Republic. As part of the settlement, almost all of the territory of Alsace-Lorraine was taken by Prussia to become a part of Germany, which it would retain until the end of World War I.
The major European powers laid claim to the areas of Africa where they could exhibit a sphere of influence over the area. These claims did not have to have any substantial land holdings or treaties to be legitimate. The European power that demonstrated its control over a territory accepted the mandate to rule that region as a national colony. The European nation that held the claim developed and benefited from their colony’s commercial interests without having to fear rival European competition. With the colonial claim came the underlying assumption that the European power that exerted control would use its mandate to offer protection and provide welfare for its colonial peoples, however, this principle remained more theory than practice. There were many documented instances of material and moral conditions deteriorating for native Africans in the late nineteenth and early twentieth centuries under European colonial rule, to the point where the colonial experience for them has been described as "hell on earth."
At the time of the Berlin Conference, Africa contained one-fifth of the world’s population living in one-quarter of the world’s land area. However, from Europe's perspective, they were dividing an unknown continent. European countries established a few coastal colonies in Africa by the mid-nineteenth century, which included Cape Colony (Great Britain), Angola (Portugal), and Algeria (France), but until the late nineteenth century Europe largely traded with free African states without feeling the need for territorial possession. Until the 1880s most of Africa remained unchartered, with western maps from the period generally showing blank spaces for the continent’s interior.
From the 1880s to 1914, the European powers expanded their control across the African continent, competing with each other for Africa’s land and resources. Great Britain controlled various colonial holdings in East Africa that spanned the length of the African continent from Egypt in the north to South Africa. The French gained major ground in West Africa, and the Portuguese held colonies in southern Africa. Germany, Italy, and Spain established a small number of colonies at various points throughout the continent, which included German East Africa (Tanganyika) and German Southwest Africa for Germany, Eritrea and Libya for Italy, and the Canary Islands and Rio de Oro in northwestern Africa for Spain. Finally, for King Leopold (ruled from 1865–1909), there was the large “piece of that great African cake” known as the Congo, which, unfortunately for the native Congolese, became his personal fiefdom to do with as he pleased in Central Africa. By 1914, almost the entire continent was under European control. Liberia, which was settled by freed American slaves in the 1820s, and Abyssinia (Ethiopia) in eastern Africa were the last remaining independent African states. (John Merriman, A History of Modern Europe, Volume Two: From the French Revolution to the Present, Third Edition (New York: W. W. Norton & Company, 2010), pp. 819–859).
Around the end of the 19th century and into the 20th century, the Meiji era was marked by the reign of the Meiji Emperor. During this time, Japan started its modernization and rose to world power status. This era name means "Enlightened Rule". In Japan, the Meiji Restoration started in the 1860s, marking the rapid modernization by the Japanese themselves along European lines. Much research has focused on the issues of discontinuity versus continuity with the previous Tokugawa Period. In the 1960s younger Japanese scholars led by Irokawa Daikichi, reacted against the bureaucratic superstate, and began searching for the historic role of the common people . They avoided the elite, and focused not on political events but on social forces and attitudes. They rejected both Marxism and modernization theory as alien and confining. They stressed the importance of popular energies in the development of modern Japan. They enlarged history by using the methods of social history. It was not until the beginning of the Meiji Era that the Japanese government began taking modernization seriously. Japan expanded its military production base by opening arsenals in various locations. The hyobusho (war office) was replaced with a War Department and a Naval Department. The samurai class suffered great disappointment the following years.
Laws were instituted that required every able-bodied male Japanese citizen, regardless of class, to serve a mandatory term of three years with the first reserves and two additional years with the second reserves. This action, the deathblow for the samurai warriors and their daimyo feudal lords, initially met resistance from both the peasant and warrior alike. The peasant class interpreted the term for military service, ketsu-eki (blood tax) literally, and attempted to avoid service by any means necessary. The Japanese government began modelling their ground forces after the French military. The French government contributed greatly to the training of Japanese officers. Many were employed at the military academy in Kyoto, and many more still were feverishly translating French field manuals for use in the Japanese ranks.
The Antebellum Age was a period of increasing division in the country based on the growth of slavery in the American South and in the western territories of Kansas and Nebraska that eventually lead to the Civil War in 1861. The Antebellum Period is often considered to have begun with the Kansas–Nebraska Act of 1854,[citation needed] although it may have begun as early as 1812. This period is also significant because it marked the transition of American manufacturing to the industrial revolution.[citation needed]
Northern leaders agreed that victory would require more than the end of fighting. Secession and Confederate nationalism had to be totally repudiated and all forms of slavery or quasi-slavery had to be eliminated. Lincoln proved effective in mobilizing support for the war goals, raising large armies and supplying them, avoiding foreign interference, and making the end of slavery a war goal. The Confederacy had a larger area than it could defend, and it failed to keep its ports open and its rivers clear. The North kept up the pressure as the South could barely feed and clothe its soldiers. Its soldiers, especially those in the East under the command of General Robert E. Lee proved highly resourceful until they finally were overwhelmed by Generals Ulysses S. Grant and William T. Sherman in 1864–65, The Reconstruction Era (1863–77) began with the Emancipation proclamation in 1863, and included freedom, full citizenship and the vote for the Southern blacks. It was followed by a reaction that left the blacks in a second class status legally, politically, socially and economically until the 1960s.
During the Gilded Age, there was substantial growth in population in the United States and extravagant displays of wealth and excess of America's upper-class during the post-Civil War and post-Reconstruction era, in the late 19th century. The wealth polarization derived primarily from industrial and population expansion. The businessmen of the Second Industrial Revolution created industrial towns and cities in the Northeast with new factories, and contributed to the creation of an ethnically diverse industrial working class which produced the wealth owned by rising super-rich industrialists and financiers called the "robber barons". An example is the company of John D. Rockefeller, who was an important figure in shaping the new oil industry. Using highly effective tactics and aggressive practices, later widely criticized, Standard Oil absorbed or destroyed most of its competition.
The creation of a modern industrial economy took place. With the creation of a transportation and communication infrastructure, the corporation became the dominant form of business organization and a managerial revolution transformed business operations. In 1890, Congress passed the Sherman Antitrust Act—the source of all American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase "restraint of trade" remained subjective. By the beginning of the 20th century, per capita income and industrial production in the United States exceeded that of any other country except Britain. Long hours and hazardous working conditions led many workers to attempt to form labor unions despite strong opposition from industrialists and the courts. But the courts did protect the marketplace, declaring the Standard Oil group to be an "unreasonable" monopoly under the Sherman Antitrust Act in 1911. It ordered Standard to break up into 34 independent companies with different boards of directors.
Replacing the classical physics in use since the end of the scientific revolution, modern physics arose in the early 20th century with the advent of quantum physics, substituting mathematical studies for experimental studies and examining equations to build a theoretical structure.[citation needed] The old quantum theory was a collection of results which predate modern quantum mechanics, but were never complete or self-consistent. The collection of heuristic prescriptions for quantum mechanics were the first corrections to classical mechanics. Outside the realm of quantum physics, the various aether theories in classical physics, which supposed a "fifth element" such as the Luminiferous aether, were nullified by the Michelson-Morley experiment—an attempt to detect the motion of earth through the aether. In biology, Darwinism gained acceptance, promoting the concept of adaptation in the theory of natural selection. The fields of geology, astronomy and psychology also made strides and gained new insights. In medicine, there were advances in medical theory and treatments.
The assertions of Chinese philosophy began to integrate concepts of Western philosophy, as steps toward modernization. By the time of the Xinhai Revolution in 1911, there were many calls, such as the May Fourth Movement, to completely abolish the old imperial institutions and practices of China. There were attempts to incorporate democracy, republicanism, and industrialism into Chinese philosophy, notably by Sun Yat-Sen (Sūn yì xiān, in one Mandarin form of the name) at the beginning of the 20th century. Mao Zedong (Máo zé dōng) added Marxist-Leninist thought. When the Communist Party of China took over power, previous schools of thought, excepting notably Legalism, were denounced as backward, and later even purged during the Cultural Revolution.
Starting one-hundred years before the 20th century, the enlightenment spiritual philosophy was challenged in various quarters around the 1900s. Developed from earlier secular traditions, modern Humanist ethical philosophies affirmed the dignity and worth of all people, based on the ability to determine right and wrong by appealing to universal human qualities, particularly rationality, without resorting to the supernatural or alleged divine authority from religious texts. For liberal humanists such as Rousseau and Kant, the universal law of reason guided the way toward total emancipation from any kind of tyranny. These ideas were challenged, for example by the young Karl Marx, who criticized the project of political emancipation (embodied in the form of human rights), asserting it to be symptomatic of the very dehumanization it was supposed to oppose. For Friedrich Nietzsche, humanism was nothing more than a secular version of theism. In his Genealogy of Morals, he argues that human rights exist as a means for the weak to collectively constrain the strong. On this view, such rights do not facilitate emancipation of life, but rather deny it. In the 20th century, the notion that human beings are rationally autonomous was challenged by the concept that humans were driven by unconscious irrational desires.
Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics, especially his mathematical treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, quantum field theory, primarily through his theoretical studies of the photon.
In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.
The last days of the Qing Dynasty were marked by civil unrest and foreign invasions. Responding to these civil failures and discontent, the Qing Imperial Court did attempt to reform the government in various ways, such as the decision to draft a constitution in 1906, the establishment of provincial legislatures in 1909, and the preparation for a national parliament in 1910. However, many of these measures were opposed by the conservatives of the Qing Court, and many reformers were either imprisoned or executed outright. The failures of the Imperial Court to enact such reforming measures of political liberalization and modernization caused the reformists to steer toward the road of revolution.
In 1912, the Republic of China was established and Sun Yat-sen was inaugurated in Nanjing as the first Provisional President. But power in Beijing already had passed to Yuan Shikai, who had effective control of the Beiyang Army, the most powerful military force in China at the time. To prevent civil war and possible foreign intervention from undermining the infant republic, leaders agreed to Army's demand that China be united under a Beijing government. On March 10, in Beijing, Shikai was sworn in as the second Provisional President of the Republic of China.
After the early 20th century revolutions, shifting alliances of China's regional warlords waged war for control of the Beijing government. Despite the fact that various warlords gained control of the government in Beijing during the warlord era, this did not constitute a new era of control or governance, because other warlords did not acknowledge the transitory governments in this period and were a law unto themselves. These military-dominated governments were collectively known as the Beiyang government. The warlord era ended around 1927.
Four years into the 20th century saw the Russo-Japanese War with the Battle of Port Arthur establishing the Empire of Japan as a world power. The Russians were in constant pursuit of a warm water port on the Pacific Ocean, for their navy as well as for maritime trade. The Manchurian Campaign of the Russian Empire was fought against the Japanese over Manchuria and Korea. The major theatres of operations were Southern Manchuria, specifically the area around the Liaodong Peninsula and Mukden, and the seas around Korea, Japan, and the Yellow Sea. The resulting campaigns, in which the fledgling Japanese military consistently attained victory over the Russian forces arrayed against them, were unexpected by world observers. These victories, as time transpired, would dramatically transform the distribution of power in East Asia, resulting in a reassessment of Japan's recent entry onto the world stage. The embarrassing string of defeats increased Russian popular dissatisfaction with the inefficient and corrupt Tsarist government.
The Edwardian era in the United Kingdom is the period spanning the reign of King Edward VII up to the end of the First World War, including the years surrounding the sinking of the RMS Titanic. In the early years of the period, the Second Boer War in South Africa split the country into anti- and pro-war factions. The imperial policies of the Conservatives eventually proved unpopular and in the general election of 1906 the Liberals won a huge landslide. The Liberal government was unable to proceed with all of its radical programme without the support of the House of Lords, which was largely Conservative. Conflict between the two Houses of Parliament over the People's Budget led to a reduction in the power of the peers in 1910. The general election in January that year returned a hung parliament with the balance of power held by Labour and Irish Nationalist members.
The causes of World War I included many factors, including the conflicts and antagonisms of the four decades leading up to the war. The Triple Entente was the name given to the loose alignment between the United Kingdom, France, and Russia after the signing of the Anglo-Russian Entente in 1907. The alignment of the three powers, supplemented by various agreements with Japan, the United States, and Spain, constituted a powerful counterweight to the Triple Alliance of Germany, Austria-Hungary, and Italy, the third having concluded an additional secret agreement with France effectively nullifying her Alliance commitments. Militarism, alliances, imperialism, and nationalism played major roles in the conflict. The immediate origins of the war lay in the decisions taken by statesmen and generals during the July Crisis of 1914, the spark (or casus belli) for which was the assassination of Archduke Franz Ferdinand of Austria.
However, the crisis did not exist in a void; it came after a long series of diplomatic clashes between the Great Powers over European and colonial issues in the decade prior to 1914 which had left tensions high. The diplomatic clashes can be traced to changes in the balance of power in Europe since 1870. An example is the Baghdad Railway which was planned to connect the Ottoman Empire cities of Konya and Baghdad with a line through modern-day Turkey, Syria and Iraq. The railway became a source of international disputes during the years immediately preceding World War I. Although it has been argued that they were resolved in 1914 before the war began, it has also been argued that the railroad was a cause of the First World War. Fundamentally the war was sparked by tensions over territory in the Balkans. Austria-Hungary competed with Serbia and Russia for territory and influence in the region and they pulled the rest of the great powers into the conflict through their various alliances and treaties. The Balkan Wars were two wars in South-eastern Europe in 1912–1913 in the course of which the Balkan League (Bulgaria, Montenegro, Greece, and Serbia) first captured Ottoman-held remaining part of Thessaly, Macedonia, Epirus, Albania and most of Thrace and then fell out over the division of the spoils, with incorporation of Romania this time.
The First World War began in 1914 and lasted to the final Armistice in 1918. The Allied Powers, led by the British Empire, France, Russia until March 1918, Japan and the United States after 1917, defeated the Central Powers, led by the German Empire, Austro-Hungarian Empire and the Ottoman Empire. The war caused the disintegration of four empires—the Austro-Hungarian, German, Ottoman, and Russian ones—as well as radical change in the European and West Asian maps. The Allied powers before 1917 are referred to as the Triple Entente, and the Central Powers are referred to as the Triple Alliance.
Much of the fighting in World War I took place along the Western Front, within a system of opposing manned trenches and fortifications (separated by a "No man's land") running from the North Sea to the border of Switzerland. On the Eastern Front, the vast eastern plains and limited rail network prevented a trench warfare stalemate from developing, although the scale of the conflict was just as large. Hostilities also occurred on and under the sea and—for the first time—from the air. More than 9 million soldiers died on the various battlefields, and nearly that many more in the participating countries' home fronts on account of food shortages and genocide committed under the cover of various civil wars and internal conflicts. Notably, more people died of the worldwide influenza outbreak at the end of the war and shortly after than died in the hostilities. The unsanitary conditions engendered by the war, severe overcrowding in barracks, wartime propaganda interfering with public health warnings, and migration of so many soldiers around the world helped the outbreak become a pandemic.
Ultimately, World War I created a decisive break with the old world order that had emerged after the Napoleonic Wars, which was modified by the mid-19th century's nationalistic revolutions. The results of World War I would be important factors in the development of World War II approximately 20 years later. More immediate to the time, the partitioning of the Ottoman Empire was a political event that redrew the political boundaries of West Asia. The huge conglomeration of territories and peoples formerly ruled by the Sultan of the Ottoman Empire was divided into several new nations. The partitioning brought the creation of the modern Arab world and the Republic of Turkey. The League of Nations granted France mandates over Syria and Lebanon and granted the United Kingdom mandates over Mesopotamia and Palestine (which was later divided into two regions: Palestine and Transjordan). Parts of the Ottoman Empire on the Arabian Peninsula became parts of what are today Saudi Arabia and Yemen.
The Russian Revolution is the series of revolutions in Russia in 1917, which destroyed the Tsarist autocracy and led to the creation of the Soviet Union. Following the abdication of Nicholas II of Russia, the Russian Provisional Government was established. In October 1917, a red faction revolution occurred in which the Red Guard, armed groups of workers and deserting soldiers directed by the Bolshevik Party, seized control of Saint Petersburg (then known as Petrograd) and began an immediate armed takeover of cities and villages throughout the former Russian Empire.
Another action in 1917 that is of note was the armistice signed between Russia and the Central Powers at Brest-Litovsk. As a condition for peace, the treaty by the Central Powers conceded huge portions of the former Russian Empire to Imperial Germany and the Ottoman Empire, greatly upsetting nationalists and conservatives. The Bolsheviks made peace with the German Empire and the Central Powers, as they had promised the Russian people prior to the Revolution. Vladimir Lenin's decision has been attributed to his sponsorship by the foreign office of Wilhelm II, German Emperor, offered by the latter in hopes that with a revolution, Russia would withdraw from World War I. This suspicion was bolstered by the German Foreign Ministry's sponsorship of Lenin's return to Petrograd. The Western Allies expressed their dismay at the Bolsheviks, upset at:
The Russian Civil War was a multi-party war that occurred within the former Russian Empire after the Russian provisional government collapsed and the Soviets under the domination of the Bolshevik party assumed power, first in Petrograd (St. Petersburg) and then in other places. In the wake of the October Revolution, the old Russian Imperial Army had been demobilized; the volunteer-based Red Guard was the Bolsheviks' main military force, augmented by an armed military component of the Cheka, the Bolshevik state security apparatus. There was an instituted mandatory conscription of the rural peasantry into the Red Army. Opposition of rural Russians to Red Army conscription units was overcome by taking hostages and shooting them when necessary in order to force compliance. Former Tsarist officers were utilized as "military specialists" (voenspetsy), taking their families hostage in order to ensure loyalty. At the start of the war, three-fourths of the Red Army officer corps was composed of former Tsarist officers. By its end, 83% of all Red Army divisional and corps commanders were ex-Tsarist soldiers.
The principal fighting occurred between the Bolshevik Red Army and the forces of the White Army. Many foreign armies warred against the Red Army, notably the Allied Forces, yet many volunteer foreigners fought in both sides of the Russian Civil War. Other nationalist and regional political groups also participated in the war, including the Ukrainian nationalist Green Army, the Ukrainian anarchist Black Army and Black Guards, and warlords such as Ungern von Sternberg. The most intense fighting took place from 1918 to 1920. Major military operations ended on 25 October 1922 when the Red Army occupied Vladivostok, previously held by the Provisional Priamur Government. The last enclave of the White Forces was the Ayano-Maysky District on the Pacific coast. The majority of the fighting ended in 1920 with the defeat of General Pyotr Wrangel in the Crimea, but a notable resistance in certain areas continued until 1923 (e.g., Kronstadt Uprising, Tambov Rebellion, Basmachi Revolt, and the final resistance of the White movement in the Far East).
The May Fourth Movement helped to rekindle the then-fading cause of republican revolution. In 1917 Sun Yat-sen had become commander-in-chief of a rival military government in Guangzhou in collaboration with southern warlords. Sun's efforts to obtain aid from the Western democracies were ignored, however, and in 1920 he turned to the Soviet Union, which had recently achieved its own revolution. The Soviets sought to befriend the Chinese revolutionists by offering scathing attacks on Western imperialism. But for political expediency, the Soviet leadership initiated a dual policy of support for both Sun and the newly established Chinese Communist Party (CCP).
In North America, especially the first half of this period, people experienced considerable prosperity in the Roaring Twenties. The social and societal upheaval known as the Roaring Twenties began in North America and spread to Europe in the aftermath of World War I. The Roaring Twenties, often called "The Jazz Age", saw an exposition of social, artistic, and cultural dynamism. 'Normalcy' returned to politics, jazz music blossomed, the flapper redefined modern womanhood, Art Deco peaked. The spirit of the Roaring Twenties was marked by a general feeling of discontinuity associated with modernity, a break with traditions. Everything seemed to be feasible through modern technology. New technologies, especially automobiles, movies and radio proliferated 'modernity' to a large part of the population. The 1920s saw the general favor of practicality, in architecture as well as in daily life. The 1920s was further distinguished by several inventions and discoveries, extensive industrial growth and the rise in consumer demand and aspirations, and significant changes in lifestyle.
Europe spent these years rebuilding and coming to terms with the vast human cost of the conflict. The economy of the United States became increasingly intertwined with that of Europe. In Germany, the Weimar Republic gave way to episodes of political and economic turmoil, which culminated with the German hyperinflation of 1923 and the failed Beer Hall Putsch of that same year. When Germany could no longer afford war payments, Wall Street invested heavily in European debts to keep the European economy afloat as a large consumer market for American mass-produced goods. By the middle of the decade, economic development soared in Europe, and the Roaring Twenties broke out in Germany, Britain and France, the second half of the decade becoming known as the "Golden Twenties". In France and francophone Canada, they were also called the "années folles" ("Crazy Years").
Worldwide prosperity changed dramatically with the onset of the Great Depression in 1929. The Wall Street Crash of 1929 served to punctuate the end of the previous era, as The Great Depression set in. The Great Depression was a worldwide economic downturn starting in most places in 1929 and ending at different times in the 1930s or early 1940s for different countries. It was the largest and most important economic depression in the 20th century, and is used in the 21st century as an example of how far the world's economy can fall.
The depression had devastating effects in virtually every country, rich or poor. International trade plunged by half to two-thirds, as did personal income, tax revenue, prices and profits. Cities all around the world were hit hard, especially those dependent on heavy industry. Construction was virtually halted in many countries. Farming and rural areas suffered as crop prices fell by roughly 60 percent. Facing plummeting demand with few alternate sources of jobs, areas dependent on primary sector industries suffered the most.
The Great Depression ended at different times in different countries with the effect lasting into the next era. America's Great Depression ended in 1941 with America's entry into World War II. The majority of countries set up relief programs, and most underwent some sort of political upheaval, pushing them to the left or right. In some world states, the desperate citizens turned toward nationalist demagogues—the most infamous being Adolf Hitler—setting the stage for the next era of war. The convulsion brought on by the worldwide depression resulted in the rise of Nazism. In Asia, Japan became an ever more assertive power, especially with regards to China.
The interwar period was also marked by a radical change in the international order, away from the balance of power that had dominated pre–World War I Europe. One main institution that was meant to bring stability was the League of Nations, which was created after the First World War with the intention of maintaining world security and peace and encouraging economic growth between member countries. The League was undermined by the bellicosity of Nazi Germany, Imperial Japan, the Soviet Union, and Mussolini's Italy, and by the non-participation of the United States, leading many to question its effectiveness and legitimacy.
A series of international crises strained the League to its limits, the earliest being the invasion of Manchuria by Japan and the Abyssinian crisis of 1935/36 in which Italy invaded Abyssinia, one of the only free African nations at that time. The League tried to enforce economic sanctions upon Italy, but to no avail. The incident highlighted French and British weakness, exemplified by their reluctance to alienate Italy and lose her as their ally. The limited actions taken by the Western powers pushed Mussolini's Italy towards alliance with Hitler's Germany anyway. The Abyssinian war showed Hitler how weak the League was and encouraged the remilitarization of the Rhineland in flagrant disregard of the Treaty of Versailles. This was the first in a series of provocative acts culminating in the invasion of Poland in September 1939 and the beginning of the Second World War.
Few Chinese had any illusions about Japanese designs on China. Hungry for raw materials and pressed by a growing population, Japan initiated the seizure of Manchuria in September 1931 and established ex-Qing emperor Puyi as head of the puppet state of Manchukuo in 1932. During the Sino-Japanese War (1937–1945), the loss of Manchuria, and its vast potential for industrial development and war industries, was a blow to the Kuomintang economy. The League of Nations, established at the end of World War I, was unable to act in the face of the Japanese defiance. After 1940, conflicts between the Kuomintang and Communists became more frequent in the areas not under Japanese control. The Communists expanded their influence wherever opportunities presented themselves through mass organizations, administrative reforms, and the land- and tax-reform measures favoring the peasants—while the Kuomintang attempted to neutralize the spread of Communist influence.
The Second Sino-Japanese War had seen tensions rise between Imperial Japan and the United States; events such as the Panay incident and the Nanking Massacre turned American public opinion against Japan. With the occupation of French Indochina in the years of 1940–41, and with the continuing war in China, the United States placed embargoes on Japan of strategic materials such as scrap metal and oil, which were vitally needed for the war effort. The Japanese were faced with the option of either withdrawing from China and losing face or seizing and securing new sources of raw materials in the resource-rich, European-controlled colonies of South East Asia—specifically British Malaya and the Dutch East Indies (modern-day Indonesia). In 1940, Imperial Japan signed the Tripartite Pact with Nazi Germany and Fascist Italy.
On December 7, 1941, Japan attacked the United States at Pearl Harbor, bringing it too into the war on the Allied side. China also joined the Allies, as eventually did most of the rest of the world. China was in turmoil at the time, and attacked Japanese armies through guerilla-type warfare. By the beginning of 1942, the major combatants were aligned as follows: the British Commonwealth, the United States, and the Soviet Union were fighting Germany and Italy; and the British Commonwealth, China, and the United States were fighting Japan. The United Kingdom, the United States, the Soviet Union and China were referred as a "trusteeship of the powerful" during the World War II  and were recognized as the Allied "Big Four" in Declaration by United Nations These four countries were considered as the "Four Policemen" or "Four Sheriffs" of the Allies power and primary victors of World War II. From then through August 1945, battles raged across all of Europe, in the North Atlantic Ocean, across North Africa, throughout Southeast Asia, throughout China, across the Pacific Ocean and in the air over Japan.
It is possible that around 62 million people died in the war; estimates vary greatly. About 60% of all casualties were civilians, who died as a result of disease, starvation, genocide (in particular, the Holocaust), and aerial bombing. The former Soviet Union and China suffered the most casualties. Estimates place deaths in the Soviet Union at around 23 million, while China suffered about 10 million. No country lost a greater portion of its population than Poland: approximately 5.6 million, or 16%, of its pre-war population of 34.8 million died.
The Holocaust (which roughly means "burnt whole") was the deliberate and systematic murder of millions of Jews and other "unwanted" during World War II by the Nazi regime in Germany. Several differing views exist regarding whether it was intended to occur from the war's beginning, or if the plans for it came about later. Regardless, persecution of Jews extended well before the war even started, such as in the Kristallnacht (Night of Broken Glass). The Nazis used propaganda to great effect to stir up anti-Semitic feelings within ordinary Germans.
After World War II, Europe was informally split into Western and Soviet spheres of influence. Western Europe later aligned as the North Atlantic Treaty Organization (NATO) and Eastern Europe as the Warsaw Pact. There was a shift in power from Western Europe and the British Empire to the two new superpowers, the United States and the Soviet Union. These two rivals would later face off in the Cold War. In Asia, the defeat of Japan led to its democratization. China's civil war continued through and after the war, resulting eventually in the establishment of the People's Republic of China. The former colonies of the European powers began their road to independence.
Over the course of the 20th century, the world's per-capita gross domestic product grew by a factor of five, much more than all earlier centuries combined (including the 19th with its Industrial Revolution). Many economists make the case that this understates the magnitude of growth, as many of the goods and services consumed at the end of the 20th century, such as improved medicine (causing world life expectancy to increase by more than two decades) and communications technologies, were not available at any price at its beginning. However, the gulf between the world's rich and poor grew wider, and the majority of the global population remained in the poor side of the divide.
Still, advancing technology and medicine has had a great impact even in the Global South. Large-scale industry and more centralized media made brutal dictatorships possible on an unprecedented scale in the middle of the century, leading to wars that were also unprecedented. However, the increased communications contributed to democratization. Technological developments included the development of airplanes and space exploration, nuclear technology, advancement in genetics, and the dawning of the Information Age.
The Soviet Union created the Eastern Bloc of countries that it occupied, annexing some as Soviet Socialist Republics and maintaining others as satellite states that would later form the Warsaw Pact. The United States and various western European countries began a policy of "containment" of communism and forged myriad alliances to this end, including NATO. Several of these western countries also coordinated efforts regarding the rebuilding of western Europe, including western Germany, which the Soviets opposed. In other regions of the world, such as Latin America and Southeast Asia, the Soviet Union fostered communist revolutionary movements, which the United States and many of its allies opposed and, in some cases, attempted to "roll back". Many countries were prompted to align themselves with the nations that would later form either NATO or the Warsaw Pact, though other movements would also emerge.
The Cold War saw periods of both heightened tension and relative calm. International crises arose, such as the Berlin Blockade (1948–1949), the Korean War (1950–1953), the Berlin Crisis of 1961, the Vietnam War (1959–1975), the Cuban Missile Crisis (1962), the Soviet war in Afghanistan (1979–1989) and NATO exercises in November 1983. There were also periods of reduced tension as both sides sought détente. Direct military attacks on adversaries were deterred by the potential for mutual assured destruction using deliverable nuclear weapons. In the Cold War era, the Generation of Love and the rise of computers changed society in very different, complex ways, including higher social and local mobility.
The Cold War drew to a close in the late 1980s and the early 1990s. The United States under President Ronald Reagan increased diplomatic, military, and economic pressure on the Soviet Union, which was already suffering from severe economic stagnation. In the second half of the 1980s, newly appointed Soviet leader Mikhail Gorbachev introduced the perestroika and glasnost reforms. The Soviet Union collapsed in 1991, leaving the United States as the dominant military power, though Russia retained much of the massive Soviet nuclear arsenal.
In Latin America in the 1970s, leftists acquired a significant political influence which prompted the right-wing, ecclesiastical authorities and a large portion of the individual country's upper class to support coup d'états to avoid what they perceived as a communist threat. This was further fueled by Cuban and United States intervention which led to a political polarization. Most South American countries were in some periods ruled by military dictatorships that were supported by the United States of America. In the 1970s, the regimes of the Southern Cone collaborated in Operation Condor killing many leftist dissidents, including some urban guerrillas. However, by the early 1990s all countries had restored their democracies.
The Space Age is a period encompassing the activities related to the Space Race, space exploration, space technology, and the cultural developments influenced by these events. The Space Age began with the development of several technologies that culminated with the launch of Sputnik 1 by the Soviet Union. This was the world's first artificial satellite, orbiting the Earth in 98.1 minutes and weighing in at 83 kg. The launch of Sputnik 1 ushered a new era of political, scientific and technological achievements that became known as the Space Age. The Space Age was characterized by rapid development of new technology in a close race mostly between the United States and the Soviet Union. The Space Age brought the first human spaceflight during the Vostok programme and reached its peak with the Apollo program which captured the imagination of much of the world's population. The landing of Apollo 11 was an event watched by over 500 million people around the world and is widely recognized as one of the defining moments of the 20th century. Since then and with the end of the space race due to the dissolution of the Soviet Union, public attention has largely moved to other areas.
Comics are a medium used to express ideas by images, often combined with text or other visual information. Comics frequently takes the form of juxtaposed sequences of panels of images. Often textual devices such as speech balloons, captions, and onomatopoeia indicate dialogue, narration, sound effects, or other information. Size and arrangement of panels contribute to narrative pacing. Cartooning and similar forms of illustration are the most common image-making means in comics; fumetti is a form which uses photographic images. Common forms of comics include comic strips, editorial and gag cartoons, and comic books. Since the late 20th century, bound volumes such as graphic novels, comics albums, and tankōbon have become increasingly common, and online webcomics have proliferated in the 21st century.
The history of comics has followed different paths in different cultures. Scholars have posited a pre-history as far back as the Lascaux cave paintings. By the mid-20th century, comics flourished particularly in the United States, western Europe (especially in France and Belgium), and Japan. The history of European comics is often traced to Rodolphe Töpffer's cartoon strips of the 1830s, and became popular following the success in the 1930s of strips and books such as The Adventures of Tintin. American comics emerged as a mass medium in the early 20th century with the advent of newspaper comic strips; magazine-style comic books followed in the 1930s, in which the superhero genre became prominent after Superman appeared in 1938. Histories of Japanese comics and cartooning (manga) propose origins as early as the 12th century. Modern comic strips emerged in Japan in the early 20th century, and the output of comics magazines and books rapidly expanded in the post-World War II era with the popularity of cartoonists such as Osamu Tezuka. Comics has had a lowbrow reputation for much of its history, but towards the end of the 20th century began to find greater acceptance with the public and in academia.
The English term comics is used as a singular noun when it refers to the medium and a plural when referring to particular instances, such as individual strips or comic books. Though the term derives from the humorous (or comic) work that predominated in early American newspaper comic strips, it has become standard also for non-humorous works. It is common in English to refer to the comics of different cultures by the terms used in their original languages, such as manga for Japanese comics, or bandes dessinées for French-language comics. There is no consensus amongst theorists and historians on a definition of comics; some emphasize the combination of images and text, some sequentiality or other image relations, and others historical aspects such as mass reproduction or the use of recurring characters. The increasing cross-pollination of concepts from different comics cultures and eras has further made definition difficult.
The European, American, and Japanese comics traditions have followed different paths. Europeans have seen their tradition as beginning with the Swiss Rodolphe Töpffer from as early as 1827 and Americans have seen the origin of theirs in Richard F. Outcault's 1890s newspaper strip The Yellow Kid, though many Americans have come to recognize Töpffer's precedence. Japan had a long prehistory of satirical cartoons and comics leading up to the World War II era. The ukiyo-e artist Hokusai popularized the Japanese term for comics and cartooning, manga, in the early 19th century. In the post-war era modern Japanese comics began to flourish when Osamu Tezuka produced a prolific body of work. Towards the close of the 20th century, these three traditions converged in a trend towards book-length comics: the comics album in Europe, the tankōbon[a] in Japan, and the graphic novel in the English-speaking countries.
Outside of these genealogies, comics theorists and historians have seen precedents for comics in the Lascaux cave paintings in France (some of which appear to be chronological sequences of images), Egyptian hieroglyphs, Trajan's Column in Rome, the 11th-century Norman Bayeux Tapestry, the 1370 bois Protat woodcut, the 15th-century Ars moriendi and block books, Michelangelo's The Last Judgment in the Sistine Chapel, and William Hogarth's 17th-century sequential engravings, amongst others.[b]
Illustrated humour periodicals were popular in 19th-century Britain, the earliest of which was the short-lived The Glasgow Looking Glass in 1825. The most popular was Punch, which popularized the term cartoon for its humorous caricatures. On occasion the cartoons in these magazines appeared in sequences; the character Ally Sloper featured in the earliest serialized comic strip when the character began to feature in its own weekly magazine in 1884.
American comics developed out of such magazines as Puck, Judge, and Life. The success of illustrated humour supplements in the New York World and later the New York American, particularly Outcault's The Yellow Kid, led to the development of newspaper comic strips. Early Sunday strips were full-page and often in colour. Between 1896 and 1901 cartoonists experimented with sequentiality, movement, and speech balloons.
Shorter, black-and-white daily strips began to appear early in the 20th century, and became established in newspapers after the success in 1907 of Bud Fisher's Mutt and Jeff. Humour strips predominated at first, and in the 1920s and 1930s strips with continuing stories in genres such as adventure and drama also became popular. Thin periodicals called comic books appeared in the 1930s, at first reprinting newspaper comic strips; by the end of the decade, original content began to dominate. The success in 1938 of Action Comics and its lead hero Superman marked the beginning of the Golden Age of Comic Books, in which the superhero genre was prominent.
The popularity of superhero comic books declined following World War II, while comic book sales continued to increase as other genres proliferated, such as romance, westerns, crime, horror, and humour. Following a sales peak in the early 1950s, the content of comic books (particularly crime and horror) was subjected to scrutiny from parent groups and government agencies, which culminated in Senate hearings that led to the establishment of the Comics Code Authority self-censoring body. The Code has been blamed for stunting the growth of American comics and maintaining its low status in American society for much of the remainder of the century. Superheroes re-established themselves as the most prominent comic book genre by the early 1960s. Underground comix challenged the Code and readers with adult, countercultural content in the late 1960s and early 1970s. The underground gave birth to the alternative comics movement in the 1980s and its mature, often experimental content in non-superhero genres.
From the 1980s, mainstream sensibilities were reasserted and serialization became less common as the number of comics magazines decreased and many comics began to be published directly as albums. Smaller publishers such as L'Association that published longer works in non-traditional formats by auteur-istic creators also became common. Since the 1990s, mergers resulted in fewer large publishers, while smaller publishers proliferated. Sales overall continued to grow despite the trend towards a shrinking print market.
Japanese comics and cartooning (manga),[g] have a history that has been seen as far back as the anthropomorphic characters in the 12th-to-13th-century Chōjū-jinbutsu-giga, 17th-century toba-e and kibyōshi picture books, and woodblock prints such as ukiyo-e which were popular between the 17th and 20th centuries. The kibyōshi contained examples of sequential images, movement lines, and sound effects.
Illustrated magazines for Western expatriates introduced Western-style satirical cartoons to Japan in the late 19th century. New publications in both the Western and Japanese styles became popular, and at the end of the 1890s, American-style newspaper comics supplements began to appear in Japan, as well as some American comic strips. 1900 saw the debut of the Jiji Manga in the Jiji Shinpō newspaper—the first use of the word "manga" in its modern sense, and where, in 1902, Rakuten Kitazawa began the first modern Japanese comic strip. By the 1930s, comic strips were serialized in large-circulation monthly girls' and boys' magazine and collected into hardback volumes.
The modern era of comics in Japan began after World War II, propelled by the success of the serialized comics of the prolific Osamu Tezuka and the comic strip Sazae-san. Genres and audiences diversified over the following decades. Stories are usually first serialized in magazines which are often hundreds of pages thick and may over a dozen stories; they are later compiled in tankōbon-format books. At the turn of the 20th and 21st centuries, nearly a quarter of all printed material in Japan was comics. translations became extremely popular in foreign markets—in some cases equaling or surpassing the sales of domestic comics.
Comic strips are generally short, multipanel comics that traditionally most commonly appeared in newspapers. In the US, daily strips have normally occupied a single tier, while Sunday strips have been given multiple tiers. In the early 20th century, daily strips were typically in black-and-white and Sundays were usually in colour and often occupied a full page.
Specialized comics periodicals formats vary greatly in different cultures. Comic books, primarily an American format, are thin periodicals usually published in colour. European and Japanese comics are frequently serialized in magazines—monthly or weekly in Europe, and usually black-and-white and weekly in Japan. Japanese comics magazine typically run to hundreds of pages.
Book-length comics take different forms in different cultures. European comics albums are most commonly printed in A4-size colour volumes. In English-speaking countries, bound volumes of comics are called graphic novels and are available in various formats. Despite incorporating the term "novel"—a term normally associated with fiction—"graphic novel" also refers to non-fiction and collections of short works. Japanese comics are collected in volumes called tankōbon following magazine serialization.
Gag and editorial cartoons usually consist of a single panel, often incorporating a caption or speech balloon. Definitions of comics which emphasize sequence usually exclude gag, editorial, and other single-panel cartoons; they can be included in definitions that emphasize the combination of word and image. Gag cartoons first began to proliferate in broadsheets published in Europe in the 18th and 19th centuries, and the term "cartoon"[h] was first used to describe them in 1843 in the British humour magazine Punch.
Comics in the US has had a lowbrow reputation stemming from its roots in mass culture; cultural elites sometimes saw popular culture as threatening culture and society. In the latter half of the 20th century, popular culture won greater acceptance, and the lines between high and low culture began to blur. Comics nevertheless continued to be stigmatized, as the medium was seen as entertainment for children and illiterates.
The graphic novel—book-length comics—began to gain attention after Will Eisner popularized the term with his book A Contract with God (1978). The term became widely known with the public after the commercial success of Maus, Watchmen, and The Dark Knight Returns in the mid-1980s. In the 21st century graphic novels became established in mainstream bookstores and libraries and webcomics became common.
The francophone Swiss Rodolphe Töpffer produced comic strips beginning in 1827, and published theories behind the form. Cartoons appeared widely in newspapers and magazines from the 19th century. The success of Zig et Puce in 1925 popularized the use of speech balloons in European comics, after which Franco-Belgian comics began to dominate. The Adventures of Tintin, with its signature clear line style, was first serialized in newspaper comics supplements beginning in 1929, and became an icon of Franco-Belgian comics.
Following the success of Le Journal de Mickey (1934–44), dedicated comics magazines and full-colour comics albums became the primary outlet for comics in the mid-20th century. As in the US, at the time comics were seen as infantile and a threat to culture and literacy; commentators stated that "none bear up to the slightest serious analysis",[c] and that comics were "the sabotage of all art and all literature".[d]
In the 1960s, the term bandes dessinées ("drawn strips") came into wide use in French to denote the medium. Cartoonists began creating comics for mature audiences, and the term "Ninth Art"[e] was coined, as comics began to attract public and academic attention as an artform. A group including René Goscinny and Albert Uderzo founded the magazine Pilote in 1959 to give artists greater freedom over their work. Goscinny and Uderzo's The Adventures of Asterix appeared in it and went on to become the best-selling French-language comics series. From 1960, the satirical and taboo-breaking Hara-Kiri defied censorship laws in the countercultural spirit that led to the May 1968 events.
Frustration with censorship and editorial interference led to a group of Pilote cartoonists to found the adults-only L'Écho des savanes in 1972. Adult-oriented and experimental comics flourished in the 1970s, such as in the experimental science fiction of Mœbius and others in Métal hurlant, even mainstream publishers took to publishing prestige-format adult comics.
Historical narratives of manga tend to focus either on its recent, post-WWII history, or on attempts to demonstrates deep roots in the past, such as to the Chōjū-jinbutsu-giga picture scroll of the 12th and 13th centuries, or the early 19th-century Hokusai Manga. The first historical overview of Japanese comics was Seiki Hosokibara's Nihon Manga-Shi[i] in 1924. Early post-war Japanese criticism was mostly of a left-wing political nature until the 1986 publication for Tomofusa Kure's Modern Manga: The Complete Picture,[j] which de-emphasized politics in favour of formal aspects, such as structure and a "grammar" of comics. The field of manga studies increased rapidly, with numerous books on the subject appearing in the 1990s. Formal theories of manga have focused on developing a "manga expression theory",[k] with emphasis on spatial relationships in the structure of images on the page, distinguishing the medium from film or literature, in which the flow of time is the basic organizing element. Comics studies courses have proliferated at Japanese universities, and Japan Society for Studies in Cartoon and Comics (ja)[l] was established in 2001 to promote comics scholarship. The publication of Frederik L. Schodt's Manga! Manga! The World of Japanese Comics in 1983 led to the spread of use of the word manga outside Japan to mean "Japanese comics" or "Japanese-style comics".
Coulton Waugh attempted the first comprehensive history of American comics with The Comics (1947). Will Eisner's Comics and Sequential Art (1985) and Scott McCloud's Understanding Comics (1993) were early attempts in English to formalize the study of comics. David Carrier's The Aesthetics of Comics (2000) was the first full-length treatment of comics from a philosophical perspective. Prominent American attempts at definitions of comics include Eisner's, McCloud's, and Harvey's. Eisner described what he called "sequential art" as "the arrangement of pictures or images and words to narrate a story or dramatize an idea"; Scott McCloud defined comics "juxtaposed pictorial and other images in deliberate sequence, intended to convey information and/or to produce an aesthetic response in the viewer", a strictly formal definition which detached comics from its historical and cultural trappings. R. C. Harvey defined comics as "pictorial narratives or expositions in which words (often lettered into the picture area within speech balloons) usually contribute to the meaning of the pictures and vice versa". Each definition has had its detractors. Harvey saw McCloud's definition as excluding single-panel cartoons, and objected to McCloud's de-emphasizing verbal elements, insisting "the essential characteristic of comics is the incorporation of verbal content". Aaron Meskin saw McCloud's theories as an artificial attempt to legitimize the place of comics in art history.
Cross-cultural study of comics is complicated by the great difference in meaning and scope of the words for "comics" in different languages. The French term for comics, bandes dessinées ("drawn strip") emphasizes the juxtaposition of drawn images as a defining factor, which can imply the exclusion of even photographic comics. The term manga is used in Japanese to indicate all forms of comics, cartooning, and caricature.
Webcomics are comics that are available on the internet. They are able to reach large audiences, and new readers usually can access archived installments. Webcomics can make use of an infinite canvas—meaning they are not constrained by size or dimensions of a page.
Some consider storyboards and wordless novels to be comics. Film studios, especially in animation, often use sequences of images as guides for film sequences. These storyboards are not intended as an end product and are rarely seen by the public. Wordless novels are books which use sequences of captionless images to deliver a narrative.
Similar to the problems of defining literature and film, no consensus has been reached on a definition of the comics medium, and attempted definitions and descriptions have fallen prey to numerous exceptions. Theorists such as Töpffer, R. C. Harvey, Will Eisner, David Carrier, Alain Rey, and Lawrence Grove emphasize the combination of text and images, though there are prominent examples of pantomime comics throughout its history. Other critics, such as Thierry Groensteen and Scott McCloud, have emphasized the primacy of sequences of images. Towards the close of the 20th century, different cultures' discoveries of each other's comics traditions, the rediscovery of forgotten early comics forms, and the rise of new forms made defining comics a more complicated task.
European comics studies began with Töpffer's theories of his own work in the 1840s, which emphasized panel transitions and the visual–verbal combination. No further progress was made until the 1970s. Pierre Fresnault-Deruelle then took a semiotics approach to the study of comics, analyzing text–image relations, page-level image relations, and image discontinuities, or what Scott McCloud later dubbed "closure". In 1987, Henri Vanlier introduced the term multicadre, or "multiframe", to refer to the comics a page as a semantic unit. By the 1990s, theorists such as Benoît Peeters and Thierry Groensteen turned attention to artists' poïetic creative choices. Thierry Smolderen and Harry Morgan have held relativistic views of the definition of comics, a medium that has taken various, equally valid forms over its history. Morgan sees comics as a subset of "les littératures dessinées" (or "drawn literatures"). French theory has come to give special attention to the page, in distinction from American theories such as McCloud's which focus on panel-to-panel transitions. Since the mid-2000s, Neil Cohn has begun analyzing how comics are understood using tools from cognitive science, extending beyond theory by using actual psychological and neuroscience experiments. This work has argued that sequential images and page layouts both use separate rule-bound "grammars" to be understood that extend beyond panel-to-panel transitions and categorical distinctions of types of layouts, and that the brain's comprehension of comics is similar to comprehending other domains, such as language and music.
Many cultures have taken their words for comics from English, including Russian (Russian: Комикс, komiks) and German (comic). Similarly, the Chinese term manhua and the Korean manhwa derive from the Chinese characters with which the Japanese term manga is written.
The English term comics derives from the humorous (or "comic") work which predominated in early American newspaper comic strips; usage of the term has become standard for non-humorous works as well. The term "comic book" has a similarly confusing history: they are most often not humorous; nor are they regular books, but rather periodicals. It is common in English to refer to the comics of different cultures by the terms used in their original languages, such as manga for Japanese comics, or bandes dessinées for French-language Franco-Belgian comics.
While comics are often the work of a single creator, the labour of making them is frequently divided between a number of specialists. There may be separate writers and artists, and artists may specialize in parts of the artwork such as characters or backgrounds, as is common in Japan. Particularly in American superhero comic books, the art may be divided between a penciller, who lays out the artwork in pencil; an inker, who finishes the artwork in ink; a colourist; and a letterer, who adds the captions and speech balloons.
Panels are individual images containing a segment of action, often surrounded by a border. Prime moments in a narrative are broken down into panels via a process called encapsulation. The reader puts the pieces together via the process of closure by using background knowledge and an understanding of panel relations to combine panels mentally into events. The size, shape, and arrangement of panels each affect the timing and pacing of the narrative. The contents of a panel may be asynchronous, with events depicted in the same image not necessarily occurring at the same time.
Text is frequently incorporated into comics via speech balloons, captions, and sound effects. Speech balloons indicate dialogue (or thought, in the case of thought balloons), with tails pointing at their respective speakers. Captions can give voice to a narrator, convey characters' dialogue or thoughts, or indicate place or time. Speech balloons themselves are strongly associated with comics, such that the addition of one to an image is sufficient to turn the image into comics. Sound effects mimic non-vocal sounds textually using onomatopoeia sound-words.
Cartooning is most frequently used in making comics, traditionally using ink (especially India ink) with dip pens or ink brushes; mixed media and digital technology have become common. Cartooning techniques such as motion lines and abstract symbols are often employed.
Depleted uranium is also used as a shielding material in some containers used to store and transport radioactive materials. While the metal itself is radioactive, its high density makes it more effective than lead in halting radiation from strong sources such as radium. Other uses of depleted uranium include counterweights for aircraft control surfaces, as ballast for missile re-entry vehicles and as a shielding material. Due to its high density, this material is found in inertial guidance systems and in gyroscopic compasses. Depleted uranium is preferred over similarly dense metals due to its ability to be easily machined and cast as well as its relatively low cost. The main risk of exposure to depleted uranium is chemical poisoning by uranium oxide rather than radioactivity (uranium being only a weak alpha emitter).
The discovery and isolation of radium in uranium ore (pitchblende) by Marie Curie sparked the development of uranium mining to extract the radium, which was used to make glow-in-the-dark paints for clock and aircraft dials. This left a prodigious quantity of uranium as a waste product, since it takes three tonnes of uranium to extract one gram of radium. This waste product was diverted to the glazing industry, making uranium glazes very inexpensive and abundant. Besides the pottery glazes, uranium tile glazes accounted for the bulk of the use, including common bathroom and kitchen tiles which can be produced in green, yellow, mauve, black, blue, red and other colors.
The X-10 Graphite Reactor at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, formerly known as the Clinton Pile and X-10 Pile, was the world's second artificial nuclear reactor (after Enrico Fermi's Chicago Pile) and was the first reactor designed and built for continuous operation. Argonne National Laboratory's Experimental Breeder Reactor I, located at the Atomic Energy Commission's National Reactor Testing Station near Arco, Idaho, became the first nuclear reactor to create electricity on 20 December 1951. Initially, four 150-watt light bulbs were lit by the reactor, but improvements eventually enabled it to power the whole facility (later, the town of Arco became the first in the world to have all its electricity come from nuclear power generated by BORAX-III, another reactor designed and operated by Argonne National Laboratory). The world's first commercial scale nuclear power station, Obninsk in the Soviet Union, began generation with its reactor AM-1 on 27 June 1954. Other early nuclear power plants were Calder Hall in England, which began generation on 17 October 1956, and the Shippingport Atomic Power Station in Pennsylvania, which began on 26 May 1958. Nuclear power was used for the first time for propulsion by a submarine, the USS Nautilus, in 1954.
Uranium is a naturally occurring element that can be found in low levels within all rock, soil, and water. Uranium is the 51st element in order of abundance in the Earth's crust. Uranium is also the highest-numbered element to be found naturally in significant quantities on Earth and is almost always found combined with other elements. Along with all elements having atomic weights higher than that of iron, it is only naturally formed in supernovae. The decay of uranium, thorium, and potassium-40 in the Earth's mantle is thought to be the main source of heat that keeps the outer core liquid and drives mantle convection, which in turn drives plate tectonics.
Uranium-235 was the first isotope that was found to be fissile. Other naturally occurring isotopes are fissionable, but not fissile. On bombardment with slow neutrons, its uranium-235 isotope will most of the time divide into two smaller nuclei, releasing nuclear binding energy and more neutrons. If too many of these neutrons are absorbed by other uranium-235 nuclei, a nuclear chain reaction occurs that results in a burst of heat or (in special circumstances) an explosion. In a nuclear reactor, such a chain reaction is slowed and controlled by a neutron poison, absorbing some of the free neutrons. Such neutron absorbent materials are often part of reactor control rods (see nuclear reactor physics for a description of this process of reactor control).
In nature, uranium(VI) forms highly soluble carbonate complexes at alkaline pH. This leads to an increase in mobility and availability of uranium to groundwater and soil from nuclear wastes which leads to health hazards. However, it is difficult to precipitate uranium as phosphate in the presence of excess carbonate at alkaline pH. A Sphingomonas sp. strain BSAR-1 has been found to express a high activity alkaline phosphatase (PhoK) that has been applied for bioprecipitation of uranium as uranyl phosphate species from alkaline solutions. The precipitation ability was enhanced by overexpressing PhoK protein in E. coli.
It is estimated that 5.5 million tonnes of uranium exists in ore reserves that are economically viable at US$59 per lb of uranium, while 35 million tonnes are classed as mineral resources (reasonable prospects for eventual economic extraction). Prices went from about $10/lb in May 2003 to $138/lb in July 2007. This has caused a big increase in spending on exploration, with US$200 million being spent worldwide in 2005, a 54% increase on the previous year. This trend continued through 2006, when expenditure on exploration rocketed to over $774 million, an increase of over 250% compared to 2004. The OECD Nuclear Energy Agency said exploration figures for 2007 would likely match those for 2006.
A team led by Enrico Fermi in 1934 observed that bombarding uranium with neutrons produces the emission of beta rays (electrons or positrons from the elements produced; see beta particle). The fission products were at first mistaken for new elements of atomic numbers 93 and 94, which the Dean of the Faculty of Rome, Orso Mario Corbino, christened ausonium and hesperium, respectively. The experiments leading to the discovery of uranium's ability to fission (break apart) into lighter elements and release binding energy were conducted by Otto Hahn and Fritz Strassmann in Hahn's laboratory in Berlin. Lise Meitner and her nephew, the physicist Otto Robert Frisch, published the physical explanation in February 1939 and named the process "nuclear fission". Soon after, Fermi hypothesized that the fission of uranium might release enough neutrons to sustain a fission reaction. Confirmation of this hypothesis came in 1939, and later work found that on average about 2.5 neutrons are released by each fission of the rare uranium isotope uranium-235. Further work found that the far more common uranium-238 isotope can be transmuted into plutonium, which, like uranium-235, is also fissile by thermal neutrons. These discoveries led numerous countries to begin working on the development of nuclear weapons and nuclear power.
The interactions of carbonate anions with uranium(VI) cause the Pourbaix diagram to change greatly when the medium is changed from water to a carbonate containing solution. While the vast majority of carbonates are insoluble in water (students are often taught that all carbonates other than those of alkali metals are insoluble in water), uranium carbonates are often soluble in water. This is because a U(VI) cation is able to bind two terminal oxides and three or more carbonates to form anionic complexes.
Uranium is more plentiful than antimony, tin, cadmium, mercury, or silver, and it is about as abundant as arsenic or molybdenum. Uranium is found in hundreds of minerals, including uraninite (the most common uranium ore), carnotite, autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from sources with as little as 0.1% uranium).
Natural uranium consists of three major isotopes: uranium-238 (99.28% natural abundance), uranium-235 (0.71%), and uranium-234 (0.0054%). All three are radioactive, emitting alpha particles, with the exception that all three of these isotopes have small probabilities of undergoing spontaneous fission, rather than alpha emission. There are also five other trace isotopes: uranium-239, which is formed when 238U undergoes spontaneous fission, releasing neutrons that are captured by another 238U atom; uranium-237, which is formed when 238U captures a neutron but emits two more, which then decays to neptunium-237; uranium-233, which is formed in the decay chain of that neptunium-237; and finally, uranium-236 and -240, which appear in the decay chain of primordial plutonium-244. It is also expected that thorium-232 should be able to undergo double beta decay, which would produce uranium-232, but this has not yet been observed experimentally.
An additional 4.6 billion tonnes of uranium are estimated to be in sea water (Japanese scientists in the 1980s showed that extraction of uranium from sea water using ion exchangers was technically feasible). There have been experiments to extract uranium from sea water, but the yield has been low due to the carbonate present in the water. In 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap which performs surface retention of solid or gas molecules, atoms or ions and also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory.
Many contemporary uses of uranium exploit its unique nuclear properties. Uranium-235 has the distinction of being the only naturally occurring fissile isotope. Uranium-238 is fissionable by fast neutrons, and is fertile, meaning it can be transmuted to fissile plutonium-239 in a nuclear reactor. Another fissile isotope, uranium-233, can be produced from natural thorium and is also important in nuclear technology. While uranium-238 has a small probability for spontaneous fission or even induced fission with fast neutrons, uranium-235 and to a lesser degree uranium-233 have a much higher fission cross-section for slow neutrons. In sufficient concentration, these isotopes maintain a sustained nuclear chain reaction. This generates the heat in nuclear power reactors, and produces the fissile material for nuclear weapons. Depleted uranium (238U) is used in kinetic energy penetrators and armor plating.
A person can be exposed to uranium (or its radioactive daughters, such as radon) by inhaling dust in air or by ingesting contaminated water and food. The amount of uranium in air is usually very small; however, people who work in factories that process phosphate fertilizers, live near government facilities that made or tested nuclear weapons, live or work near a modern battlefield where depleted uranium weapons have been used, or live or work near a coal-fired power plant, facilities that mine or process uranium ore, or enrich uranium for reactor fuel, may have increased exposure to uranium. Houses or structures that are over uranium deposits (either natural or man-made slag deposits) may have an increased incidence of exposure to radon gas. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for uranium exposure in the workplace as 0.25 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.2 mg/m3 over an 8-hour workday and a short-term limit of 0.6 mg/m3. At levels of 10 mg/m3, uranium is immediately dangerous to life and health.
The most common forms of uranium oxide are triuranium octoxide (U
3O
8) and UO
2. Both oxide forms are solids that have low solubility in water and are relatively stable over a wide range of environmental conditions. Triuranium octoxide is (depending on conditions) the most stable compound of uranium and is the form most commonly found in nature. Uranium dioxide is the form in which uranium is most commonly used as a nuclear reactor fuel. At ambient temperatures, UO
2 will gradually convert to U
3O
8. Because of their stability, uranium oxides are generally considered the preferred chemical form for storage or disposal.
The use of uranium in its natural oxide form dates back to at least the year 79 CE, when it was used to add a yellow color to ceramic glazes. Yellow glass with 1% uranium oxide was found in a Roman villa on Cape Posillipo in the Bay of Naples, Italy, by R. T. Gunther of the University of Oxford in 1912. Starting in the late Middle Ages, pitchblende was extracted from the Habsburg silver mines in Joachimsthal, Bohemia (now Jáchymov in the Czech Republic), and was used as a coloring agent in the local glassmaking industry. In the early 19th century, the world's only known sources of uranium ore were these mines.
On 2 December 1942, as part of the Manhattan Project, another team led by Enrico Fermi was able to initiate the first artificial self-sustained nuclear chain reaction, Chicago Pile-1. Working in a lab below the stands of Stagg Field at the University of Chicago, the team created the conditions needed for such a reaction by piling together 400 short tons (360 metric tons) of graphite, 58 short tons (53 metric tons) of uranium oxide, and six short tons (5.5 metric tons) of uranium metal, a majority of which was supplied by Westinghouse Lamp Plant in a makeshift production process.
Uranium is used as a colorant in uranium glass producing orange-red to lemon yellow hues. It was also used for tinting and shading in early photography. The 1789 discovery of uranium in the mineral pitchblende is credited to Martin Heinrich Klaproth, who named the new element after the planet Uranus. Eugène-Melchior Péligot was the first person to isolate the metal and its radioactive properties were discovered in 1896 by Henri Becquerel. Research by Otto Hahn, Lise Meitner, Enrico Fermi and others, such as J. Robert Oppenheimer starting in 1934 led to its use as a fuel in the nuclear power industry and in Little Boy, the first nuclear weapon used in war. An ensuing arms race during the Cold War between the United States and the Soviet Union produced tens of thousands of nuclear weapons that used uranium metal and uranium-derived plutonium-239. The security of those weapons and their fissile material following the breakup of the Soviet Union in 1991 is an ongoing concern for public health and safety. See Nuclear proliferation.
In nature, uranium is found as uranium-238 (99.2742%) and uranium-235 (0.7204%). Isotope separation concentrates (enriches) the fissionable uranium-235 for nuclear weapons and most nuclear power plants, except for gas cooled reactors and pressurised heavy water reactors. Most neutrons released by a fissioning atom of uranium-235 must impact other uranium-235 atoms to sustain the nuclear chain reaction. The concentration and amount of uranium-235 needed to achieve this is called a 'critical mass'.
The major application of uranium in the military sector is in high-density penetrators. This ammunition consists of depleted uranium (DU) alloyed with 1–2% other elements, such as titanium or molybdenum. At high impact speed, the density, hardness, and pyrophoricity of the projectile enable the destruction of heavily armored targets. Tank armor and other removable vehicle armor can also be hardened with depleted uranium plates. The use of depleted uranium became politically and environmentally contentious after the use of such munitions by the US, UK and other countries during wars in the Persian Gulf and the Balkans raised questions concerning uranium compounds left in the soil (see Gulf War Syndrome).
The discovery of the element is credited to the German chemist Martin Heinrich Klaproth. While he was working in his experimental laboratory in Berlin in 1789, Klaproth was able to precipitate a yellow compound (likely sodium diuranate) by dissolving pitchblende in nitric acid and neutralizing the solution with sodium hydroxide. Klaproth assumed the yellow substance was the oxide of a yet-undiscovered element and heated it with charcoal to obtain a black powder, which he thought was the newly discovered metal itself (in fact, that powder was an oxide of uranium). He named the newly discovered element after the planet Uranus, (named after the primordial Greek god of the sky), which had been discovered eight years earlier by William Herschel.
Uranium ore is mined in several ways: by open pit, underground, in-situ leaching, and borehole mining (see uranium mining). Low-grade uranium ore mined typically contains 0.01 to 0.25% uranium oxides. Extensive measures must be employed to extract the metal from its ore. High-grade ores found in Athabasca Basin deposits in Saskatchewan, Canada can contain up to 23% uranium oxides on average. Uranium ore is crushed and rendered into a fine powder and then leached with either an acid or alkali. The leachate is subjected to one of several sequences of precipitation, solvent extraction, and ion exchange. The resulting mixture, called yellowcake, contains at least 75% uranium oxides U3O8. Yellowcake is then calcined to remove impurities from the milling process before refining and conversion.
Two major types of atomic bombs were developed by the United States during World War II: a uranium-based device (codenamed "Little Boy") whose fissile material was highly enriched uranium, and a plutonium-based device (see Trinity test and "Fat Man") whose plutonium was derived from uranium-238. The uranium-based Little Boy device became the first nuclear weapon used in war when it was detonated over the Japanese city of Hiroshima on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed approximately 75,000 people (see Atomic bombings of Hiroshima and Nagasaki). Initially it was believed that uranium was relatively rare, and that nuclear proliferation could be avoided by simply buying up all known uranium stocks, but within a decade large deposits of it were discovered in many places around the world.
In 2005, seventeen countries produced concentrated uranium oxides, with Canada (27.9% of world production) and Australia (22.8%) being the largest producers and Kazakhstan (10.5%), Russia (8.0%), Namibia (7.5%), Niger (7.4%), Uzbekistan (5.5%), the United States (2.5%), Argentina (2.1%), Ukraine (1.9%) and China (1.7%) also producing significant amounts. Kazakhstan continues to increase production and may have become the world's largest producer of uranium by 2009 with an expected production of 12,826 tonnes, compared to Canada with 11,100 t and Australia with 9,430 t. In the late 1960s, UN geologists also discovered major uranium deposits and other rare mineral reserves in Somalia. The find was the largest of its kind, with industry experts estimating the deposits at over 25% of the world's then known uranium reserves of 800,000 tons.
During the Cold War between the Soviet Union and the United States, huge stockpiles of uranium were amassed and tens of thousands of nuclear weapons were created using enriched uranium and plutonium made from uranium. Since the break-up of the Soviet Union in 1991, an estimated 600 short tons (540 metric tons) of highly enriched weapons grade uranium (enough to make 40,000 nuclear warheads) have been stored in often inadequately guarded facilities in the Russian Federation and several other former Soviet states. Police in Asia, Europe, and South America on at least 16 occasions from 1993 to 2005 have intercepted shipments of smuggled bomb-grade uranium or plutonium, most of which was from ex-Soviet sources. From 1993 to 2005 the Material Protection, Control, and Accounting Program, operated by the federal government of the United States, spent approximately US $550 million to help safeguard uranium and plutonium stockpiles in Russia. This money was used for improvements and security enhancements at research and storage facilities. Scientific American reported in February 2006 that in some of the facilities security consisted of chain link fences which were in severe states of disrepair. According to an interview from the article, one facility had been storing samples of enriched (weapons grade) uranium in a broom closet before the improvement project; another had been keeping track of its stock of nuclear warheads using index cards kept in a shoe box.
Salts of many oxidation states of uranium are water-soluble and may be studied in aqueous solutions. The most common ionic forms are U3+ (brown-red), U4+ (green), UO+
2 (unstable), and UO2+
2 (yellow), for U(III), U(IV), U(V), and U(VI), respectively. A few solid and semi-metallic compounds such as UO and US exist for the formal oxidation state uranium(II), but no simple ions are known to exist in solution for that state. Ions of U3+ liberate hydrogen from water and are therefore considered to be highly unstable. The UO2+
2 ion represents the uranium(VI) state and is known to form compounds such as uranyl carbonate, uranyl chloride and uranyl sulfate. UO2+
2 also forms complexes with various organic chelating agents, the most commonly encountered of which is uranyl acetate.
Some organisms, such as the lichen Trapelia involuta or microorganisms such as the bacterium Citrobacter, can absorb concentrations of uranium that are up to 300 times the level of their environment. Citrobacter species absorb uranyl ions when given glycerol phosphate (or other similar organic phosphates). After one day, one gram of bacteria can encrust themselves with nine grams of uranyl phosphate crystals; this creates the possibility that these organisms could be used in bioremediation to decontaminate uranium-polluted water. The proteobacterium Geobacter has also been shown to bioremediate uranium in ground water. The mycorrhizal fungus Glomus intraradices increases uranium content in the roots of its symbiotic plant.
Uranium metal heated to 250 to 300 °C (482 to 572 °F) reacts with hydrogen to form uranium hydride. Even higher temperatures will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. Two crystal modifications of uranium hydride exist: an α form that is obtained at low temperatures and a β form that is created when the formation temperature is above 250 °C.
Uranium carbides and uranium nitrides are both relatively inert semimetallic compounds that are minimally soluble in acids, react with water, and can ignite in air to form U
3O
8. Carbides of uranium include uranium monocarbide (UC), uranium dicarbide (UC
2), and diuranium tricarbide (U
2C
3). Both UC and UC
2 are formed by adding carbon to molten uranium or by exposing the metal to carbon monoxide at high temperatures. Stable below 1800 °C, U
2C
3 is prepared by subjecting a heated mixture of UC and UC
2 to mechanical stress. Uranium nitrides obtained by direct exposure of the metal to nitrogen include uranium mononitride (UN), uranium dinitride (UN
2), and diuranium trinitride (U
2N
3).
To be considered 'enriched', the uranium-235 fraction should be between 3% and 5%. This process produces huge quantities of uranium that is depleted of uranium-235 and with a correspondingly increased fraction of uranium-238, called depleted uranium or 'DU'. To be considered 'depleted', the uranium-235 isotope concentration should be no more than 0.3%. The price of uranium has risen since 2001, so enrichment tailings containing more than 0.35% uranium-235 are being considered for re-enrichment, driving the price of depleted uranium hexafluoride above $130 per kilogram in July 2007 from $5 in 2001.
Normal functioning of the kidney, brain, liver, heart, and other systems can be affected by uranium exposure, because, besides being weakly radioactive, uranium is a toxic metal. Uranium is also a reproductive toxicant. Radiological effects are generally local because alpha radiation, the primary form of 238U decay, has a very short range, and will not penetrate skin. Uranyl (UO2+
2) ions, such as from uranium trioxide or uranyl nitrate and other hexavalent uranium compounds, have been shown to cause birth defects and immune system damage in laboratory animals. While the CDC has published one study that no human cancer has been seen as a result of exposure to natural or depleted uranium, exposure to uranium and its decay products, especially radon, are widely known and significant health threats. Exposure to strontium-90, iodine-131, and other fission products is unrelated to uranium exposure, but may result from medical procedures or exposure to spent reactor fuel or fallout from nuclear weapons. Although accidental inhalation exposure to a high concentration of uranium hexafluoride has resulted in human fatalities, those deaths were associated with the generation of highly toxic hydrofluoric acid and uranyl fluoride rather than with uranium itself. Finely divided uranium metal presents a fire hazard because uranium is pyrophoric; small grains will ignite spontaneously in air at room temperature.
The gas centrifuge process, where gaseous uranium hexafluoride (UF
6) is separated by the difference in molecular weight between 235UF6 and 238UF6 using high-speed centrifuges, is the cheapest and leading enrichment process. The gaseous diffusion process had been the leading method for enrichment and was used in the Manhattan Project. In this process, uranium hexafluoride is repeatedly diffused through a silver-zinc membrane, and the different isotopes of uranium are separated by diffusion rate (since uranium 238 is heavier it diffuses slightly slower than uranium-235). The molecular laser isotope separation method employs a laser beam of precise energy to sever the bond between uranium-235 and fluorine. This leaves uranium-238 bonded to fluorine and allows uranium-235 metal to precipitate from the solution. An alternative laser method of enrichment is known as atomic vapor laser isotope separation (AVLIS) and employs visible tunable lasers such as dye lasers. Another method used is liquid thermal diffusion.
Uranium is a chemical element with symbol U and atomic number 92. It is a silvery-white metal in the actinide series of the periodic table. A uranium atom has 92 protons and 92 electrons, of which 6 are valence electrons. Uranium is weakly radioactive because all its isotopes are unstable (with half-lives of the six naturally known isotopes, uranium-233 to uranium-238, varying between 69 years and 4.5 billion years). The most common isotopes of uranium are uranium-238 (which has 146 neutrons and accounts for almost 99.3% of the uranium found in nature) and uranium-235 (which has 143 neutrons, accounting for 0.7% of the element found naturally). Uranium has the second highest atomic weight of the primordially occurring elements, lighter only than plutonium. Its density is about 70% higher than that of lead, but slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in soil, rock and water, and is commercially extracted from uranium-bearing minerals such as uraninite.
Uranium metal reacts with almost all non-metal elements (with an exception of the noble gases) and their compounds, with reactivity increasing with temperature. Hydrochloric and nitric acids dissolve uranium, but non-oxidizing acids other than hydrochloric acid attack the element very slowly. When finely divided, it can react with cold water; in air, uranium metal becomes coated with a dark layer of uranium oxide. Uranium in ores is extracted chemically and converted into uranium dioxide or other chemical forms usable in industry.
During the later stages of World War II, the entire Cold War, and to a lesser extent afterwards, uranium-235 has been used as the fissile explosive material to produce nuclear weapons. Initially, two major types of fission bombs were built: a relatively simple device that uses uranium-235 and a more complicated mechanism that uses plutonium-239 derived from uranium-238. Later, a much more complicated and far more powerful type of fission/fusion bomb (thermonuclear weapon) was built, that uses a plutonium-based device to cause a mixture of tritium and deuterium to undergo nuclear fusion. Such bombs are jacketed in a non-fissile (unenriched) uranium case, and they derive more than half their power from the fission of this material by fast neutrons from the nuclear fusion process.
Uranium was also used in photographic chemicals (especially uranium nitrate as a toner), in lamp filaments for stage lighting bulbs, to improve the appearance of dentures, and in the leather and wood industries for stains and dyes. Uranium salts are mordants of silk or wool. Uranyl acetate and uranyl formate are used as electron-dense "stains" in transmission electron microscopy, to increase the contrast of biological specimens in ultrathin sections and in negative staining of viruses, isolated cell organelles and macromolecules.
In 1972, the French physicist Francis Perrin discovered fifteen ancient and no longer active natural nuclear fission reactors in three separate ore deposits at the Oklo mine in Gabon, West Africa, collectively known as the Oklo Fossil Reactors. The ore deposit is 1.7 billion years old; then, uranium-235 constituted about 3% of the total uranium on Earth. This is high enough to permit a sustained nuclear fission chain reaction to occur, provided other supporting conditions exist. The capacity of the surrounding sediment to contain the nuclear waste products has been cited by the U.S. federal government as supporting evidence for the feasibility to store spent nuclear fuel at the Yucca Mountain nuclear waste repository.
Uranium's average concentration in the Earth's crust is (depending on the reference) 2 to 4 parts per million, or about 40 times as abundant as silver. The Earth's crust from the surface to 25 km (15 mi) down is calculated to contain 1017 kg (2×1017 lb) of uranium while the oceans may contain 1013 kg (2×1013 lb). The concentration of uranium in soil ranges from 0.7 to 11 parts per million (up to 15 parts per million in farmland soil due to use of phosphate fertilizers), and its concentration in sea water is 3 parts per billion.
Uranium-238 is the most stable isotope of uranium, with a half-life of about 4.468×109 years, roughly the age of the Earth. Uranium-235 has a half-life of about 7.13×108 years, and uranium-234 has a half-life of about 2.48×105 years. For natural uranium, about 49% of its alpha rays are emitted by each of 238U atom, and also 49% by 234U (since the latter is formed from the former) and about 2.0% of them by the 235U. When the Earth was young, probably about one-fifth of its uranium was uranium-235, but the percentage of 234U was probably much lower than this.
Most ingested uranium is excreted during digestion. Only 0.5% is absorbed when insoluble forms of uranium, such as its oxide, are ingested, whereas absorption of the more soluble uranyl ion can be up to 5%. However, soluble uranium compounds tend to quickly pass through the body, whereas insoluble uranium compounds, especially when inhaled by way of dust into the lungs, pose a more serious exposure hazard. After entering the bloodstream, the absorbed uranium tends to bioaccumulate and stay for many years in bone tissue because of uranium's affinity for phosphates. Uranium is not absorbed through the skin, and alpha particles released by uranium cannot penetrate the skin.
The flowering plants (angiosperms), also known as Angiospermae or Magnoliophyta, are the most diverse group of land plants, with about 350,000 species. Like gymnosperms, angiosperms are seed-producing plants; they are distinguished from gymnosperms by characteristics including flowers, endosperm within the seeds, and the production of fruits that contain the seeds. Etymologically, angiosperm means a plant that produces seeds within an enclosure, in other words, a fruiting plant. The term "angiosperm" comes from the Greek composite word (angeion-, "case" or "casing", and sperma, "seed") meaning "enclosed seeds", after the enclosed condition of the seeds.
Fossilized spores suggest that higher plants (embryophytes) have lived on land for at least 475 million years. Early land plants reproduced sexually with flagellated, swimming sperm, like the green algae from which they evolved. An adaptation to terrestrialization was the development of upright meiosporangia for dispersal by spores to new habitats. This feature is lacking in the descendants of their nearest algal relatives, the Charophycean green algae. A later terrestrial adaptation took place with retention of the delicate, avascular sexual stage, the gametophyte, within the tissues of the vascular sporophyte. This occurred by spore germination within sporangia rather than spore release, as in non-seed plants. A current example of how this might have happened can be seen in the precocious spore germination in Selaginella, the spike-moss. The result for the ancestors of angiosperms was enclosing them in a case, the seed. The first seed bearing plants, like the ginkgo, and conifers (such as pines and firs), did not produce flowers. The pollen grains (males) of Ginkgo and cycads produce a pair of flagellated, mobile sperm cells that "swim" down the developing pollen tube to the female and her eggs.
The apparently sudden appearance of nearly modern flowers in the fossil record initially posed such a problem for the theory of evolution that it was called an "abominable mystery" by Charles Darwin. However, the fossil record has considerably grown since the time of Darwin, and recently discovered angiosperm fossils such as Archaefructus, along with further discoveries of fossil gymnosperms, suggest how angiosperm characteristics may have been acquired in a series of steps. Several groups of extinct gymnosperms, in particular seed ferns, have been proposed as the ancestors of flowering plants, but there is no continuous fossil evidence showing exactly how flowers evolved. Some older fossils, such as the upper Triassic Sanmiguelia, have been suggested. Based on current evidence, some propose that the ancestors of the angiosperms diverged from an unknown group of gymnosperms in the Triassic period (245–202 million years ago). Fossil angiosperm-like pollen from the Middle Triassic (247.2–242.0 Ma) suggests an older date for their origin. A close relationship between angiosperms and gnetophytes, proposed on the basis of morphological evidence, has more recently been disputed on the basis of molecular evidence that suggest gnetophytes are instead more closely related to other gymnosperms.[citation needed]
The evolution of seed plants and later angiosperms appears to be the result of two distinct rounds of whole genome duplication events. These occurred at 319 million years ago and 192 million years ago. Another possible whole genome duplication event at 160 million years ago perhaps created the ancestral line that led to all modern flowering plants. That event was studied by sequencing the genome of an ancient flowering plant, Amborella trichopoda, and directly addresses Darwin's "abominable mystery."
The earliest known macrofossil confidently identified as an angiosperm, Archaefructus liaoningensis, is dated to about 125 million years BP (the Cretaceous period), whereas pollen considered to be of angiosperm origin takes the fossil record back to about 130 million years BP. However, one study has suggested that the early-middle Jurassic plant Schmeissneria, traditionally considered a type of ginkgo, may be the earliest known angiosperm, or at least a close relative. In addition, circumstantial chemical evidence has been found for the existence of angiosperms as early as 250 million years ago. Oleanane, a secondary metabolite produced by many flowering plants, has been found in Permian deposits of that age together with fossils of gigantopterids. Gigantopterids are a group of extinct seed plants that share many morphological traits with flowering plants, although they are not known to have been flowering plants themselves.
The great angiosperm radiation, when a great diversity of angiosperms appears in the fossil record, occurred in the mid-Cretaceous (approximately 100 million years ago). However, a study in 2007 estimated that the division of the five most recent (the genus Ceratophyllum, the family Chloranthaceae, the eudicots, the magnoliids, and the monocots) of the eight main groups occurred around 140 million years ago. By the late Cretaceous, angiosperms appear to have dominated environments formerly occupied by ferns and cycadophytes, but large canopy-forming trees replaced conifers as the dominant trees only close to the end of the Cretaceous 66 million years ago or even later, at the beginning of the Tertiary. The radiation of herbaceous angiosperms occurred much later. Yet, many fossil plants recognizable as belonging to modern families (including beech, oak, maple, and magnolia) had already appeared by the late Cretaceous.
Island genetics provides one proposed explanation for the sudden, fully developed appearance of flowering plants. Island genetics is believed to be a common source of speciation in general, especially when it comes to radical adaptations that seem to have required inferior transitional forms. Flowering plants may have evolved in an isolated setting like an island or island chain, where the plants bearing them were able to develop a highly specialized relationship with some specific animal (a wasp, for example). Such a relationship, with a hypothetical wasp carrying pollen from one plant to another much the way fig wasps do today, could result in the development of a high degree of specialization in both the plant(s) and their partners. Note that the wasp example is not incidental; bees, which, it is postulated, evolved specifically due to mutualistic plant relationships, are descended from wasps.
Animals are also involved in the distribution of seeds. Fruit, which is formed by the enlargement of flower parts, is frequently a seed-dispersal tool that attracts animals to eat or otherwise disturb it, incidentally scattering the seeds it contains (see frugivory). Although many such mutualistic relationships remain too fragile to survive competition and to spread widely, flowering proved to be an unusually effective means of reproduction, spreading (whatever its origin) to become the dominant form of land plant life.
Flower ontogeny uses a combination of genes normally responsible for forming new shoots. The most primitive flowers probably had a variable number of flower parts, often separate from (but in contact with) each other. The flowers tended to grow in a spiral pattern, to be bisexual (in plants, this means both male and female parts on the same flower), and to be dominated by the ovary (female part). As flowers evolved, some variations developed parts fused together, with a much more specific number and design, and with either specific sexes per flower or plant or at least "ovary-inferior".
Flower evolution continues to the present day; modern flowers have been so profoundly influenced by humans that some of them cannot be pollinated in nature. Many modern domesticated flower species were formerly simple weeds, which sprouted only when the ground was disturbed. Some of them tended to grow with human crops, perhaps already having symbiotic companion plant relationships with them, and the prettiest did not get plucked because of their beauty, developing a dependence upon and special adaptation to human affection.
The exact relationship between these eight groups is not yet clear, although there is agreement that the first three groups to diverge from the ancestral angiosperm were Amborellales, Nymphaeales, and Austrobaileyales. The term basal angiosperms refers to these three groups. Among the rest, the relationship between the three broadest of these groups (magnoliids, monocots, and eudicots) remains unclear. Some analyses make the magnoliids the first to diverge, others the monocots. Ceratophyllum seems to group with the eudicots rather than with the monocots.
The botanical term "Angiosperm", from the Ancient Greek αγγείον, angeíon (bottle, vessel) and σπέρμα, (seed), was coined in the form Angiospermae by Paul Hermann in 1690, as the name of one of his primary divisions of the plant kingdom. This included flowering plants possessing seeds enclosed in capsules, distinguished from his Gymnospermae, or flowering plants with achenial or schizo-carpic fruits, the whole fruit or each of its pieces being here regarded as a seed and naked. The term and its antonym were maintained by Carl Linnaeus with the same sense, but with restricted application, in the names of the orders of his class Didynamia. Its use with any approach to its modern scope became possible only after 1827, when Robert Brown established the existence of truly naked ovules in the Cycadeae and Coniferae, and applied to them the name Gymnosperms.[citation needed] From that time onward, as long as these Gymnosperms were, as was usual, reckoned as dicotyledonous flowering plants, the term Angiosperm was used antithetically by botanical writers, with varying scope, as a group-name for other dicotyledonous plants.
In most taxonomies, the flowering plants are treated as a coherent group. The most popular descriptive name has been Angiospermae (Angiosperms), with Anthophyta ("flowering plants") a second choice. These names are not linked to any rank. The Wettstein system and the Engler system use the name Angiospermae, at the assigned rank of subdivision. The Reveal system treated flowering plants as subdivision Magnoliophytina (Frohne & U. Jensen ex Reveal, Phytologia 79: 70 1996), but later split it to Magnoliopsida, Liliopsida, and Rosopsida. The Takhtajan system and Cronquist system treat this group at the rank of division, leading to the name Magnoliophyta (from the family name Magnoliaceae). The Dahlgren system and Thorne system (1992) treat this group at the rank of class, leading to the name Magnoliopsida. The APG system of 1998, and the later 2003 and 2009 revisions, treat the flowering plants as a clade called angiosperms without a formal botanical name. However, a formal classification was published alongside the 2009 revision in which the flowering plants form the Subclass Magnoliidae.
The internal classification of this group has undergone considerable revision. The Cronquist system, proposed by Arthur Cronquist in 1968 and published in its full form in 1981, is still widely used but is no longer believed to accurately reflect phylogeny. A consensus about how the flowering plants should be arranged has recently begun to emerge through the work of the Angiosperm Phylogeny Group (APG), which published an influential reclassification of the angiosperms in 1998. Updates incorporating more recent research were published as APG II in 2003 and as APG III in 2009.
Recent studies, as by the APG, show that the monocots form a monophyletic group (clade) but that the dicots do not (they are paraphyletic). Nevertheless, the majority of dicot species do form a monophyletic group, called the eudicots or tricolpates. Of the remaining dicot species, most belong to a third major clade known as the magnoliids, containing about 9,000 species. The rest include a paraphyletic grouping of primitive species known collectively as the basal angiosperms, plus the families Ceratophyllaceae and Chloranthaceae.
The number of species of flowering plants is estimated to be in the range of 250,000 to 400,000. This compares to around 12,000 species of moss or 11,000 species of pteridophytes, showing that the flowering plants are much more diverse. The number of families in APG (1998) was 462. In APG II (2003) it is not settled; at maximum it is 457, but within this number there are 55 optional segregates, so that the minimum number of families in this system is 402. In APG III (2009) there are 415 families.
In the dicotyledons, the bundles in the very young stem are arranged in an open ring, separating a central pith from an outer cortex. In each bundle, separating the xylem and phloem, is a layer of meristem or active formative tissue known as cambium. By the formation of a layer of cambium between the bundles (interfascicular cambium), a complete ring is formed, and a regular periodical increase in thickness results from the development of xylem on the inside and phloem on the outside. The soft phloem becomes crushed, but the hard wood persists and forms the bulk of the stem and branches of the woody perennial. Owing to differences in the character of the elements produced at the beginning and end of the season, the wood is marked out in transverse section into concentric rings, one for each season of growth, called annual rings.
The characteristic feature of angiosperms is the flower. Flowers show remarkable variation in form and elaboration, and provide the most trustworthy external characteristics for establishing relationships among angiosperm species. The function of the flower is to ensure fertilization of the ovule and development of fruit containing seeds. The floral apparatus may arise terminally on a shoot or from the axil of a leaf (where the petiole attaches to the stem). Occasionally, as in violets, a flower arises singly in the axil of an ordinary foliage-leaf. More typically, the flower-bearing portion of the plant is sharply distinguished from the foliage-bearing or vegetative portion, and forms a more or less elaborate branch-system called an inflorescence.
The flower may consist only of these parts, as in willow, where each flower comprises only a few stamens or two carpels. Usually, other structures are present and serve to protect the sporophylls and to form an envelope attractive to pollinators. The individual members of these surrounding structures are known as sepals and petals (or tepals in flowers such as Magnolia where sepals and petals are not distinguishable from each other). The outer series (calyx of sepals) is usually green and leaf-like, and functions to protect the rest of the flower, especially the bud. The inner series (corolla of petals) is, in general, white or brightly colored, and is more delicate in structure. It functions to attract insect or bird pollinators. Attraction is effected by color, scent, and nectar, which may be secreted in some part of the flower. The characteristics that attract pollinators account for the popularity of flowers and flowering plants among humans.
While the majority of flowers are perfect or hermaphrodite (having both pollen and ovule producing parts in the same flower structure), flowering plants have developed numerous morphological and physiological mechanisms to reduce or prevent self-fertilization. Heteromorphic flowers have short carpels and long stamens, or vice versa, so animal pollinators cannot easily transfer pollen to the pistil (receptive part of the carpel). Homomorphic flowers may employ a biochemical (physiological) mechanism called self-incompatibility to discriminate between self and non-self pollen grains. In other species, the male and female parts are morphologically separated, developing on different flowers.
Double fertilization refers to a process in which two sperm cells fertilize cells in the ovary. This process begins when a pollen grain adheres to the stigma of the pistil (female reproductive structure), germinates, and grows a long pollen tube. While this pollen tube is growing, a haploid generative cell travels down the tube behind the tube nucleus. The generative cell divides by mitosis to produce two haploid (n) sperm cells. As the pollen tube grows, it makes its way from the stigma, down the style and into the ovary. Here the pollen tube reaches the micropyle of the ovule and digests its way into one of the synergids, releasing its contents (which include the sperm cells). The synergid that the cells were released into degenerates and one sperm makes its way to fertilize the egg cell, producing a diploid (2n) zygote. The second sperm cell fuses with both central cell nuclei, producing a triploid (3n) cell. As the zygote develops into an embryo, the triploid cell develops into the endosperm, which serves as the embryo's food supply. The ovary now will develop into fruit and the ovule will develop into seed.
The character of the seed coat bears a definite relation to that of the fruit. They protect the embryo and aid in dissemination; they may also directly promote germination. Among plants with indehiscent fruits, in general, the fruit provides protection for the embryo and secures dissemination. In this case, the seed coat is only slightly developed. If the fruit is dehiscent and the seed is exposed, in general, the seed-coat is well developed, and must discharge the functions otherwise executed by the fruit.
Agriculture is almost entirely dependent on angiosperms, which provide virtually all plant-based food, and also provide a significant amount of livestock feed. Of all the families of plants, the Poaceae, or grass family (grains), is by far the most important, providing the bulk of all feedstocks (rice, corn — maize, wheat, barley, rye, oats, pearl millet, sugar cane, sorghum). The Fabaceae, or legume family, comes in second place. Also of high importance are the Solanaceae, or nightshade family (potatoes, tomatoes, and peppers, among others), the Cucurbitaceae, or gourd family (also including pumpkins and melons), the Brassicaceae, or mustard plant family (including rapeseed and the innumerable varieties of the cabbage species Brassica oleracea), and the Apiaceae, or parsley family. Many of our fruits come from the Rutaceae, or rue family (including oranges, lemons, grapefruits, etc.), and the Rosaceae, or rose family (including apples, pears, cherries, apricots, plums, etc.).
Russian (ру́сский язы́к, russkiy yazyk, pronounced [ˈruskʲɪj jɪˈzɨk] ( listen)) is an East Slavic language and an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan and many minor or unrecognised territories. It is an unofficial but widely-spoken language in Ukraine, Latvia, Estonia, and to a lesser extent, the other countries that were once constituent republics of the Soviet Union and former participants of the Eastern Bloc. Russian belongs to the family of Indo-European languages and is one of the three living members of the East Slavic languages. Written examples of Old East Slavonic are attested from the 10th century onwards.
Russian distinguishes between consonant phonemes with palatal secondary articulation and those without, the so-called soft and hard sounds. This distinction is found between pairs of almost all consonants and is one of the most distinguishing features of the language. Another important aspect is the reduction of unstressed vowels. Stress, which is unpredictable, is not normally indicated orthographically though an optional acute accent (знак ударения, znak udareniya) may be used to mark stress, such as to distinguish between homographic words, for example замо́к (zamok, meaning a lock) and за́мок (zamok, meaning a castle), or to indicate the proper pronunciation of uncommon words or names.
Russian is a Slavic language of the Indo-European family. It is a lineal descendant of the language used in Kievan Rus'. From the point of view of the spoken language, its closest relatives are Ukrainian, Belarusian, and Rusyn, the other three languages in the East Slavic group. In many places in eastern and southern Ukraine and throughout Belarus, these languages are spoken interchangeably, and in certain areas traditional bilingualism resulted in language mixtures, e.g. Surzhyk in eastern Ukraine and Trasianka in Belarus. An East Slavic Old Novgorod dialect, although vanished during the 15th or 16th century, is sometimes considered to have played a significant role in the formation of modern Russian. Also Russian has notable lexical similarities with Bulgarian due to a common Church Slavonic influence on both languages, as well as because of later interaction in the 19th–20th centuries, although Bulgarian grammar differs markedly from Russian. In the 19th century, the language was often called "Great Russian" to distinguish it from Belarusian, then called "White Russian" and Ukrainian, then called "Little Russian".
The vocabulary (mainly abstract and literary words), principles of word formations, and, to some extent, inflections and literary style of Russian have been also influenced by Church Slavonic, a developed and partly russified form of the South Slavic Old Church Slavonic language used by the Russian Orthodox Church. However, the East Slavic forms have tended to be used exclusively in the various dialects that are experiencing a rapid decline. In some cases, both the East Slavic and the Church Slavonic forms are in use, with many different meanings. For details, see Russian phonology and History of the Russian language.
Until the 20th century, the language's spoken form was the language of only the upper noble classes and urban population, as Russian peasants from the countryside continued to speak in their own dialects. By the mid-20th century, such dialects were forced out with the introduction of the compulsory education system that was established by the Soviet government. Despite the formalization of Standard Russian, some nonstandard dialectal features (such as fricative [ɣ] in Southern Russian dialects) are still observed in colloquial speech.
Ethnic Russians constitute 25.5% of the country's current population and 58.6% of the native Estonian population is also able to speak Russian. In all, 67.8% of Estonia's population can speak Russian. Command of Russian language, however, is rapidly decreasing among younger Estonians (primarily being replaced by the command of English). For example, if 53% of ethnic Estonians between 15 and 19 claim to speak some Russian, then among the 10- to 14-year-old group, command of Russian has fallen to 19% (which is about one-third the percentage of those who claim to have command of English in the same age group).
As the Grand Duchy of Finland was part of the Russian Empire from 1809 to 1918, a number of Russian speakers have remained in Finland. There are 33,400 Russian-speaking Finns, amounting to 0.6% of the population. Five thousand (0.1%) of them are late 19th century and 20th century immigrants or their descendants, and the remaining majority are recent immigrants who moved there in the 1990s and later.[citation needed] Russian is spoken by 1.4% of the population of Finland according to a 2014 estimate from the World Factbook.
In Ukraine, Russian is seen as a language of inter-ethnic communication, and a minority language, under the 1996 Constitution of Ukraine. According to estimates from Demoskop Weekly, in 2004 there were 14,400,000 native speakers of Russian in the country, and 29 million active speakers. 65% of the population was fluent in Russian in 2006, and 38% used it as the main language with family, friends or at work. Russian is spoken by 29.6% of the population according to a 2001 estimate from the World Factbook. 20% of school students receive their education primarily in Russian.
In the 20th century, Russian was mandatorily taught in the schools of the members of the old Warsaw Pact and in other countries that used to be satellites of the USSR. In particular, these countries include Poland, Bulgaria, the Czech Republic, Slovakia, Hungary, Albania, former East Germany and Cuba. However, younger generations are usually not fluent in it, because Russian is no longer mandatory in the school system. According to the Eurobarometer 2005 survey, though, fluency in Russian remains fairly high (20–40%) in some countries, in particular those where the people speak a Slavic language and thereby have an edge in learning Russian (namely, Poland, Czech Republic, Slovakia, and Bulgaria).
Significant Russian-speaking groups also exist in Western Europe. These have been fed by several waves of immigrants since the beginning of the 20th century, each with its own flavor of language. The United Kingdom, Spain, Portugal, France, Italy, Belgium, Greece, Brazil, Norway, and Austria have significant Russian-speaking communities. According to the 2011 Census of Ireland, there were 21,639 people in the nation who use Russian as a home language. However, of this only 13% were Russian nationals. 20% held Irish citizenship, while 27% and 14% were holding the passports of Latvia and Lithuania respectively.
In Armenia Russian has no official status, but it's recognised as a minority language under the Framework Convention for the Protection of National Minorities. According to estimates from Demoskop Weekly, in 2004 there were 15,000 native speakers of Russian in the country, and 1 million active speakers. 30% of the population was fluent in Russian in 2006, and 2% used it as the main language with family, friends or at work. Russian is spoken by 1.4% of the population according to a 2009 estimate from the World Factbook.
In Georgia Russian has no official status, but it's recognised as a minority language under the Framework Convention for the Protection of National Minorities. According to estimates from Demoskop Weekly, in 2004 there were 130,000 native speakers of Russian in the country, and 1.7 million active speakers. 27% of the population was fluent in Russian in 2006, and 1% used it as the main language with family, friends or at work. Russian is the language of 9% of the population according to the World Factook. Ethnologue cites Russian as the country's de facto working language.
In Kazakhstan Russian is not a state language, but according to article 7 of the Constitution of Kazakhstan its usage enjoys equal status to that of the Kazakh language in state and local administration. According to estimates from Demoskop Weekly, in 2004 there were 4,200,000 native speakers of Russian in the country, and 10 million active speakers. 63% of the population was fluent in Russian in 2006, and 46% used it as the main language with family, friends or at work. According to a 2001 estimate from the World Factbook, 95% of the population can speak Russian. Large Russian-speaking communities still exist in northern Kazakhstan, and ethnic Russians comprise 25.6% of Kazakhstan's population. The 2009 census reported that 10,309,500 people, or 84.8% of the population aged 15 and above, could read and write well in Russian, as well as understand the spoken language.
The language was first introduced in North America when Russian explorers voyaged into Alaska and claimed it for Russia during the 1700s. Although most colonists left after the United States bought the land in 1867, a handful stayed and preserved the Russian language in this region to this day, although only a few elderly speakers of this unique dialect are left. Sizable Russian-speaking communities also exist in North America, especially in large urban centers of the U.S. and Canada, such as New York City, Philadelphia, Boston, Los Angeles, Nashville, San Francisco, Seattle, Spokane, Toronto, Baltimore, Miami, Chicago, Denver and Cleveland. In a number of locations they issue their own newspapers, and live in ethnic enclaves (especially the generation of immigrants who started arriving in the early 1960s). Only about 25% of them are ethnic Russians, however. Before the dissolution of the Soviet Union, the overwhelming majority of Russophones in Brighton Beach, Brooklyn in New York City were Russian-speaking Jews. Afterward, the influx from the countries of the former Soviet Union changed the statistics somewhat, with ethnic Russians and Ukrainians immigrating along with some more Russian Jews and Central Asians. According to the United States Census, in 2007 Russian was the primary language spoken in the homes of over 850,000 individuals living in the United States.
Russian is one of the official languages (or has similar status and interpretation must be provided into Russian) of the United Nations, International Atomic Energy Agency, World Health Organization, International Civil Aviation Organization, UNESCO, World Intellectual Property Organization, International Telecommunication Union, World Meteorological Organization, Food and Agriculture Organization, International Fund for Agricultural Development, International Criminal Court, International Monetary Fund, International Olympic Committee, Universal Postal Union, World Bank, Commonwealth of Independent States, Organization for Security and Co-operation in Europe, Shanghai Cooperation Organisation, Eurasian Economic Community, Collective Security Treaty Organization, Antarctic Treaty Secretariat, International Organization for Standardization, GUAM Organization for Democracy and Economic Development, International Mathematical Olympiad. The Russian language is also one of two official languages aboard the International Space Station - NASA astronauts who serve alongside Russian cosmonauts usually take Russian language courses. This practice goes back to the Apollo-Soyuz mission, which first flew in 1975.
In March 2013 it was announced that Russian is now the second-most used language on the Internet after English. People use the Russian language on 5.9% of all websites, slightly ahead of German and far behind English (54.7%). Russian is used not only on 89.8% of .ru sites, but also on 88.7% of sites with the former Soviet Union domain .su. The websites of former Soviet Union nations also use high levels of Russian: 79.0% in Ukraine, 86.9% in Belarus, 84.0% in Kazakhstan, 79.6% in Uzbekistan, 75.9% in Kyrgyzstan and 81.8% in Tajikistan. However, Russian is the sixth-most used language on the top 1,000 sites, behind English, Chinese, French, German and Japanese.
Despite leveling after 1900, especially in matters of vocabulary and phonetics, a number of dialects still exist in Russia. Some linguists divide the dialects of Russian into two primary regional groupings, "Northern" and "Southern", with Moscow lying on the zone of transition between the two. Others divide the language into three groupings, Northern, Central (or Middle) and Southern, with Moscow lying in the Central region. All dialects also divided in two main chronological categories: the dialects of primary formation (the territory of the Eastern Rus' or Muscovy, roughly consists of the modern Central and Northwestern Federal districts); and secondary formation (other territory). Dialectology within Russia recognizes dozens of smaller-scale variants. The dialects often show distinct and non-standard features of pronunciation and intonation, vocabulary and grammar. Some of these are relics of ancient usage now completely discarded by the standard language.
The Northern Russian dialects and those spoken along the Volga River typically pronounce unstressed /o/ clearly (the phenomenon called okanye/оканье). Besides the absence of vowel reduction, some dialects have high or diphthongal /e~i̯ɛ/ in the place of Proto-Slavic *ě and /o~u̯ɔ/ in stressed closed syllables (as in Ukrainian) instead of Standard Russian /e/ and /o/. An interesting morphological feature is a post-posed definite article -to, -ta, -te similarly to that existing in Bulgarian and Macedonian.
In the Southern Russian dialects, instances of unstressed /e/ and /a/ following palatalized consonants and preceding a stressed syllable are not reduced to [ɪ] (as occurs in the Moscow dialect), being instead pronounced [a] in such positions (e.g. несли is pronounced [nʲaˈslʲi], not [nʲɪsˈlʲi]) – this is called yakanye/яканье. Consonants include a fricative /ɣ/, a semivowel /w~u̯/ and /x~xv~xw/, whereas the Standard and Northern dialects have the consonants /ɡ/, /v/, and final /l/ and /f/, respectively. The morphology features a palatalized final /tʲ/ in 3rd person forms of verbs (this is unpalatalized in the Standard and Northern dialects). Some of these features such as akanye/yakanye, a debuccalized or lenited /ɡ/, a semivowel /w~u̯/ and palatalized final /tʲ/ in 3rd person forms of verbs are also present in modern Belarusian and some dialects of Ukrainian (Eastern Polesian), indicating a linguistic continuum.
Among the first to study Russian dialects was Lomonosov in the 18th century. In the 19th, Vladimir Dal compiled the first dictionary that included dialectal vocabulary. Detailed mapping of Russian dialects began at the turn of the 20th century. In modern times, the monumental Dialectological Atlas of the Russian Language (Диалектологический атлас русского языка [dʲɪɐˌlʲɛktəlɐˈɡʲitɕɪskʲɪj ˈatləs ˈruskəvə jɪzɨˈka]), was published in three folio volumes 1986–1989, after four decades of preparatory work.
Older letters of the Russian alphabet include ⟨ѣ⟩, which merged to ⟨е⟩ (/je/ or /ʲe/); ⟨і⟩ and ⟨ѵ⟩, which both merged to ⟨и⟩ (/i/); ⟨ѳ⟩, which merged to ⟨ф⟩ (/f/); ⟨ѫ⟩, which merged to ⟨у⟩ (/u/); ⟨ѭ⟩, which merged to ⟨ю⟩ (/ju/ or /ʲu/); and ⟨ѧ/⟨ѩ⟩⟩, which later were graphically reshaped into ⟨я⟩ and merged phonetically to /ja/ or /ʲa/. While these older letters have been abandoned at one time or another, they may be used in this and related articles. The yers ⟨ъ⟩ and ⟨ь⟩ originally indicated the pronunciation of ultra-short or reduced /ŭ/, /ĭ/.
Because of many technical restrictions in computing and also because of the unavailability of Cyrillic keyboards abroad, Russian is often transliterated using the Latin alphabet. For example, мороз ('frost') is transliterated moroz, and мышь ('mouse'), mysh or myš'. Once commonly used by the majority of those living outside Russia, transliteration is being used less frequently by Russian-speaking typists in favor of the extension of Unicode character encoding, which fully incorporates the Russian alphabet. Free programs leveraging this Unicode extension are available which allow users to type Russian characters, even on Western 'QWERTY' keyboards.
The Russian alphabet has many systems of character encoding. KOI8-R was designed by the Soviet government and was intended to serve as the standard encoding. This encoding was and still is widely used in UNIX-like operating systems. Nevertheless, the spread of MS-DOS and OS/2 (IBM866), traditional Macintosh (ISO/IEC 8859-5) and Microsoft Windows (CP1251) created chaos and ended by establishing different encodings as de facto standards, with Windows-1251 becoming a de facto standard in Russian Internet and e-mail communication during the period of roughly 1995–2005.
According to the Institute of Russian Language of the Russian Academy of Sciences, an optional acute accent (знак ударения) may, and sometimes should, be used to mark stress. For example, it is used to distinguish between otherwise identical words, especially when context does not make it obvious: замо́к/за́мок (lock/castle), сто́ящий/стоя́щий (worthwhile/standing), чудно́/чу́дно (this is odd/this is marvelous), молоде́ц/мо́лодец (attaboy/fine young man), узна́ю/узнаю́ (I shall learn it/I recognize it), отреза́ть/отре́зать (to be cutting/to have cut); to indicate the proper pronunciation of uncommon words, especially personal and family names (афе́ра, гу́ру, Гарси́я, Оле́ша, Фе́рми), and to show which is the stressed word in a sentence (Ты́ съел печенье?/Ты съе́л печенье?/Ты съел пече́нье? – Was it you who ate the cookie?/Did you eat the cookie?/Was it the cookie that you ate?). Stress marks are mandatory in lexical dictionaries and books for children or Russian learners.
The language possesses five vowels (or six, under the St. Petersburg Phonological School), which are written with different letters depending on whether or not the preceding consonant is palatalized. The consonants typically come in plain vs. palatalized pairs, which are traditionally called hard and soft. (The hard consonants are often velarized, especially before front vowels, as in Irish). The standard language, based on the Moscow dialect, possesses heavy stress and moderate variation in pitch. Stressed vowels are somewhat lengthened, while unstressed vowels tend to be reduced to near-close vowels or an unclear schwa. (See also: vowel reduction in Russian.)
Russian is notable for its distinction based on palatalization of most of the consonants. While /k/, /ɡ/, /x/ do have palatalized allophones [kʲ, ɡʲ, xʲ], only /kʲ/ might be considered a phoneme, though it is marginal and generally not considered distinctive (the only native minimal pair which argues for /kʲ/ to be a separate phoneme is "это ткёт" ([ˈɛtə tkʲɵt], 'it weaves')/"этот кот" ([ˈɛtət kot], 'this cat')). Palatalization means that the center of the tongue is raised during and after the articulation of the consonant. In the case of /tʲ/ and /dʲ/, the tongue is raised enough to produce slight frication (affricate sounds). These sounds: /t, d, ts, s, z, n and rʲ/ are dental, that is pronounced with the tip of the tongue against the teeth rather than against the alveolar ridge.
Judging by the historical records, by approximately 1000 AD the predominant ethnic group over much of modern European Russia, Ukraine and Belarus was the Eastern branch of the Slavs, speaking a closely related group of dialects. The political unification of this region into Kievan Rus' in about 880, from which modern Russia, Ukraine and Belarus trace their origins, established Old East Slavic as a literary and commercial language. It was soon followed by the adoption of Christianity in 988 and the introduction of the South Slavic Old Church Slavonic as the liturgical and official language. Borrowings and calques from Byzantine Greek began to enter the Old East Slavic and spoken dialects at this time, which in their turn modified the Old Church Slavonic as well.
The political reforms of Peter the Great (Пётр Вели́кий, Pyótr Velíkiy) were accompanied by a reform of the alphabet, and achieved their goal of secularization and Westernization. Blocks of specialized vocabulary were adopted from the languages of Western Europe. By 1800, a significant portion of the gentry spoke French daily, and German sometimes. Many Russian novels of the 19th century, e.g. Leo Tolstoy's (Лев Толсто́й) War and Peace, contain entire paragraphs and even pages in French with no translation given, with an assumption that educated readers would not need one.
The modern literary language is usually considered to date from the time of Alexander Pushkin (Алекса́ндр Пу́шкин) in the first third of the 19th century. Pushkin revolutionized Russian literature by rejecting archaic grammar and vocabulary (so-called "высо́кий стиль" — "high style") in favor of grammar and vocabulary found in the spoken language of the time. Even modern readers of younger age may only experience slight difficulties understanding some words in Pushkin's texts, since relatively few words used by Pushkin have become archaic or changed meaning. In fact, many expressions used by Russian writers of the early 19th century, in particular Pushkin, Mikhail Lermontov (Михаи́л Ле́рмонтов), Nikolai Gogol (Никола́й Го́голь), Aleksander Griboyedov (Алекса́ндр Грибое́дов), became proverbs or sayings which can be frequently found even in modern Russian colloquial speech.
During the Soviet period, the policy toward the languages of the various other ethnic groups fluctuated in practice. Though each of the constituent republics had its own official language, the unifying role and superior status was reserved for Russian, although it was declared the official language only in 1990. Following the break-up of the USSR in 1991, several of the newly independent states have encouraged their native languages, which has partly reversed the privileged status of Russian, though its role as the language of post-Soviet national discourse throughout the region has continued.
According to figures published in 2006 in the journal "Demoskop Weekly" research deputy director of Research Center for Sociological Research of the Ministry of Education and Science (Russia) Arefyev A. L., the Russian language is gradually losing its position in the world in general, and in Russia in particular. In 2012, A. L. Arefyev published a new study "Russian language at the turn of the 20th-21st centuries", in which he confirmed his conclusion about the trend of further weakening of the Russian language in all regions of the world (findings published in 2013 in the journal "Demoskop Weekly"). In the countries of the former Soviet Union the Russian language is gradually being replaced by local languages. Currently the number speakers of Russian language in the world depends on the number of Russians in the world (as the main sources distribution Russian language) and total population Russia (where Russian is an official language).
Oklahoma i/ˌoʊkləˈhoʊmə/ (Cherokee: Asgaya gigageyi / ᎠᏍᎦᏯ ᎩᎦᎨᏱ; or translated ᎣᎦᎳᎰᎹ (òɡàlàhoma), Pawnee: Uukuhuúwa, Cayuga: Gahnawiyoˀgeh) is a state located in the South Central United States. Oklahoma is the 20th most extensive and the 28th most populous of the 50 United States. The state's name is derived from the Choctaw words okla and humma, meaning "red people". It is also known informally by its nickname, The Sooner State, in reference to the non-Native settlers who staked their claims on the choicest pieces of land before the official opening date, and the Indian Appropriations Act of 1889, which opened the door for white settlement in America's Indian Territory. The name was settled upon statehood, Oklahoma Territory and Indian Territory were merged and Indian was dropped from the name. On November 16, 1907, Oklahoma became the 46th state to enter the union. Its residents are known as Oklahomans, or informally "Okies", and its capital and largest city is Oklahoma City.
The name Oklahoma comes from the Choctaw phrase okla humma, literally meaning red people. Choctaw Chief Allen Wright suggested the name in 1866 during treaty negotiations with the federal government regarding the use of Indian Territory, in which he envisioned an all-Indian state controlled by the United States Superintendent of Indian Affairs. Equivalent to the English word Indian, okla humma was a phrase in the Choctaw language used to describe Native American people as a whole. Oklahoma later became the de facto name for Oklahoma Territory, and it was officially approved in 1890, two years after the area was opened to white settlers.
Oklahoma is the 20th largest state in the United States, covering an area of 69,898 square miles (181,035 km2), with 68,667 square miles (177847 km2) of land and 1,281 square miles (3,188 km2) of water. It is one of six states on the Frontier Strip and lies partly in the Great Plains near the geographical center of the 48 contiguous states. It is bounded on the east by Arkansas and Missouri, on the north by Kansas, on the northwest by Colorado, on the far west by New Mexico, and on the south and near-west by Texas.
The western edge of the Oklahoma panhandle is out of alignment with its Texas border. The Oklahoma/New Mexico border is actually 2.1 to 2.2 miles east of the Texas line. The border between Texas and New Mexico was set first as a result of a survey by Spain in 1819. It was then set along the 103rd Meridian. In the 1890s, when Oklahoma was formally surveyed using more accurate surveying equipment and techniques, it was discovered that the Texas line was not set along the 103rd Meridian. Surveying techniques were not as accurate in 1819, and the actual 103rd Meridian was approximately 2.2 miles to the east. It was much easier to leave the mistake as it was than for Texas to cede land to New Mexico to correct the original surveying error. The placement of the Oklahoma/New Mexico border represents the true 103rd Meridian.
Oklahoma is between the Great Plains and the Ozark Plateau in the Gulf of Mexico watershed, generally sloping from the high plains of its western boundary to the low wetlands of its southeastern boundary. Its highest and lowest points follow this trend, with its highest peak, Black Mesa, at 4,973 feet (1,516 m) above sea level, situated near its far northwest corner in the Oklahoma Panhandle. The state's lowest point is on the Little River near its far southeastern boundary near the town of Idabel, OK, which dips to 289 feet (88 m) above sea level.
Oklahoma has four primary mountain ranges: the Ouachita Mountains, the Arbuckle Mountains, the Wichita Mountains, and the Ozark Mountains. Contained within the U.S. Interior Highlands region, the Ozark and Ouachita Mountains mark the only major mountainous region between the Rocky Mountains and the Appalachians. A portion of the Flint Hills stretches into north-central Oklahoma, and near the state's eastern border, Cavanal Hill is regarded by the Oklahoma Tourism & Recreation Department as the world's tallest hill; at 1,999 feet (609 m), it fails their definition of a mountain by one foot.
The semi-arid high plains in the state's northwestern corner harbor few natural forests; the region has a rolling to flat landscape with intermittent canyons and mesa ranges like the Glass Mountains. Partial plains interrupted by small, sky island mountain ranges like the Antelope Hills and the Wichita Mountains dot southwestern Oklahoma; transitional prairie and oak savannahs cover the central portion of the state. The Ozark and Ouachita Mountains rise from west to east over the state's eastern third, gradually increasing in elevation in an eastward direction.
Forests cover 24 percent of Oklahoma and prairie grasslands composed of shortgrass, mixed-grass, and tallgrass prairie, harbor expansive ecosystems in the state's central and western portions, although cropland has largely replaced native grasses. Where rainfall is sparse in the western regions of the state, shortgrass prairie and shrublands are the most prominent ecosystems, though pinyon pines, red cedar (junipers), and ponderosa pines grow near rivers and creek beds in the far western reaches of the panhandle. Southwestern Oklahoma contains many rare, disjunct species including sugar maple, bigtooth maple, nolina and southern live oak.
The state holds populations of white-tailed deer, mule deer, antelope, coyotes, mountain lions, bobcats, elk, and birds such as quail, doves, cardinals, bald eagles, red-tailed hawks, and pheasants. In prairie ecosystems, American bison, greater prairie chickens, badgers, and armadillo are common, and some of the nation's largest prairie dog towns inhabit shortgrass prairie in the state's panhandle. The Cross Timbers, a region transitioning from prairie to woodlands in Central Oklahoma, harbors 351 vertebrate species. The Ouachita Mountains are home to black bear, red fox, grey fox, and river otter populations, which coexist with a total of 328 vertebrate species in southeastern Oklahoma. Also, in southeastern Oklahoma lives the American alligator.
With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states. In addition, the Black Kettle National Grassland covers 31,300 acres (127 km2) of prairie in southwestern Oklahoma. The Wichita Mountains Wildlife Refuge is the oldest and largest of nine national wildlife refuges in the state and was founded in 1901, encompassing 59,020 acres (238.8 km2).
Oklahoma is located in a humid subtropical region. Oklahoma lies in a transition zone between humid continental climate to the north, semi-arid climate to the west, and humid subtropical climate in the central, south and eastern portions of the state. Most of the state lies in an area known as Tornado Alley characterized by frequent interaction between cold, dry air from Canada, warm to hot, dry air from Mexico and the Southwestern U.S., and warm, moist air from the Gulf of Mexico. The interactions between these three contrasting air currents produces severe weather (severe thunderstorms, damaging thunderstorm winds, large hail and tornadoes) with a frequency virtually unseen anywhere else on planet Earth. An average 62 tornadoes strike the state per year—one of the highest rates in the world.
Because of Oklahoma's position between zones of differing prevailing temperature and winds, weather patterns within the state can vary widely over relatively short distances and can change drastically in a short time. As an example, on November 11, 1911, the temperature at Oklahoma City reached 83 °F (28 °C) in the afternoon (the record high for that date), then an Arctic cold front of unprecedented intensity slammed across the state, causing the temperature to crash 66 degrees, down to 17 °F (−8 °C) at midnight (the record low for that date); thus, both the record high and record low for November 11 were set on the same date. This type of phenomenon is also responsible for many of the tornadoes in the area, such as the 1912 Oklahoma tornado outbreak, when a warm front traveled along a stalled cold front, resulting in an average of about one tornado per hour over the course of a day.
The humid subtropical climate (Koppen Cfa) of central, southern and eastern Oklahoma is influenced heavily by southerly winds bringing moisture from the Gulf of Mexico. Traveling westward, the climate transitions progressively toward a semi-arid zone (Koppen BSk) in the high plains of the Panhandle and other western areas from about Lawton westward, less frequently touched by southern moisture. Precipitation and temperatures decline from east to west accordingly, with areas in the southeast averaging an annual temperature of 62 °F (17 °C) and an annual rainfall of generally over 40 inches (1,020 mm) and up to 56 inches (1,420 mm), while areas of the (higher-elevation) panhandle average 58 °F (14 °C), with an annual rainfall under 17 inches (430 mm).
Over almost all of Oklahoma, winter is the driest season. Average monthly precipitation increases dramatically in the spring to a peak in May, the wettest month over most of the state, with its frequent and not uncommonly severe thunderstorm activity. Early June can still be wet, but most years see a marked decrease in rainfall during June and early July. Mid-summer (July and August) represents a secondary dry season over much of Oklahoma, with long stretches of hot weather with only sporadic thunderstorm activity not uncommon many years. Severe drought is common in the hottest summers, such as those of 1934, 1954, 1980 and 2011, all of which featured weeks on end of virtual rainlessness and high temperatures well over 100 °F (38 °C). Average precipitation rises again from September to mid-October, representing a secondary wetter season, then declines from late October through December.
All of the state frequently experiences temperatures above 100 °F (38 °C) or below 0 °F (−18 °C), though below-zero temperatures are rare in south-central and southeastern Oklahoma. Snowfall ranges from an average of less than 4 inches (10 cm) in the south to just over 20 inches (51 cm) on the border of Colorado in the panhandle. The state is home to the Storm Prediction Center, the National Severe Storms Laboratory, and the Warning Decision Training Branch, all part of the National Weather Service and located in Norman. Oklahoma's highest recorded temperature of 120 °F (49 °C) was recorded at Tipton on June 27, 1994 and the lowest recorded temperature of −31 °F (−35 °C) was recorded at Nowata on February 10, 2011.
Evidence exists that native peoples traveled through Oklahoma as early as the last ice age. Ancestors of the Wichita and Caddo lived in what is now Oklahoma. The Panhandle culture peoples were precontact residents of the panhandle region. The westernmost center of the Mississippian culture was Spiro Mounds, in what is now Spiro, Oklahoma, which flourished between AD 850 and 1450. Spaniard Francisco Vásquez de Coronado traveled through the state in 1541, but French explorers claimed the area in the 1700s and it remained under French rule until 1803, when all the French territory west of the Mississippi River was purchased by the United States in the Louisiana Purchase.
The new state became a focal point for the emerging oil industry, as discoveries of oil pools prompted towns to grow rapidly in population and wealth. Tulsa eventually became known as the "Oil Capital of the World" for most of the 20th century and oil investments fueled much of the state's early economy. In 1927, Oklahoman businessman Cyrus Avery, known as the "Father of Route 66", began the campaign to create U.S. Route 66. Using a stretch of highway from Amarillo, Texas to Tulsa, Oklahoma to form the original portion of Highway 66, Avery spearheaded the creation of the U.S. Highway 66 Association to oversee the planning of Route 66, based in his hometown of Tulsa.
During the 1930s, parts of the state began suffering the consequences of poor farming practices, extended drought and high winds. Known as the Dust Bowl, areas of Kansas, Texas, New Mexico and northwestern Oklahoma were hampered by long periods of little rainfall and abnormally high temperatures, sending thousands of farmers into poverty and forcing them to relocate to more fertile areas of the western United States. Over a twenty-year period ending in 1950, the state saw its only historical decline in population, dropping 6.9 percent as impoverished families migrated out of the state after the Dust Bowl.
In 1995, Oklahoma City was the site of one of the most destructive acts of domestic terrorism in American history. The Oklahoma City bombing of April 19, 1995, in which Timothy McVeigh and Terry Nichols detonated an explosive outside of the Alfred P. Murrah Federal Building, killed 168 people, including 19 children. The two men were convicted of the bombing: McVeigh was sentenced to death and executed by the federal government on June 11, 2001; his partner Nichols is serving a sentence of life in prison without the possibility of parole. McVeigh's army buddy, Michael Fortier, was sentenced to 12 years in federal prison and ordered to pay a $75,000 fine for his role in the bombing plot (i.e. assisting in the sale of guns to raise funds for the bombing, and examining the Murrah Federal building as a possible target before the terrorist attack). His wife, Lori Fortier, who has since died, was granted immunity from prosecution in return for her testimony in the case.
The English language has been official in the state of Oklahoma since 2010. The variety of North American English spoken is called Oklahoma English, and this dialect is quite diverse with its uneven blending of features of North Midland, South Midland, and Southern dialects. In 2000, 2,977,187 Oklahomans—92.6% of the resident population five years or older—spoke only English at home, a decrease from 95% in 1990. 238,732 Oklahoma residents reported speaking a language other than English in the 2000 census, about 7.4% of the total population of the state. Spanish is the second most commonly spoken language in the state, with 141,060 speakers counted in 2000. The next most commonly spoken language is Cherokee, with about 22,000 speakers living within the Cherokee Nation tribal jurisdiction area of eastern Oklahoma. Cherokee is an official language in the Cherokee Nation tribal jurisdiction area and in the United Keetoowah Band of Cherokee Indians.
German is the fourth most commonly used language, with 13,444 speakers representing about 0.4% of the total state population. Fifth is Vietnamese, spoken by 11,330 people, or about 0.4% of the population, many of whom live in the Asia District of Oklahoma City. Other languages include French with 8,258 speakers (0.3%), Chinese with 6,413 (0.2%), Korean with 3,948 (0.1%), Arabic with 3,265 (0.1%), other Asian languages with 3,134 (0.1%), Tagalog with 2,888 (0.1%), Japanese with 2,546 (0.1%), and African languages with 2,546 (0.1%). In addition to Cherokee, more than 25 Native American languages are spoken in Oklahoma, second only to California (though, it should be noted that only Cherokee exhibits language vitality at present).
Oklahoma is part of a geographical region characterized by conservative and Evangelical Christianity known as the "Bible Belt". Spanning the southern and eastern parts of the United States, the area is known for politically and socially conservative views, even though Oklahoma has more voters registered with the Democratic Party than with any other party. Tulsa, the state's second largest city, home to Oral Roberts University, is sometimes called the "buckle of the Bible Belt". According to the Pew Research Center, the majority of Oklahoma's religious adherents – 85 percent – are Christian, accounting for about 80 percent of the population. The percentage of Oklahomans affiliated with Catholicism is half of the national average, while the percentage affiliated with Evangelical Protestantism is more than twice the national average – tied with Arkansas for the largest percentage of any state.
Oklahoma is host to a diverse range of sectors including aviation, energy, transportation equipment, food processing, electronics, and telecommunications. Oklahoma is an important producer of natural gas, aircraft, and food. The state ranks third in the nation for production of natural gas, is the 27th-most agriculturally productive state, and also ranks 5th in production of wheat. Four Fortune 500 companies and six Fortune 1000 companies are headquartered in Oklahoma, and it has been rated one of the most business-friendly states in the nation, with the 7th-lowest tax burden in 2007.
Oklahoma is the nation's third-largest producer of natural gas, fifth-largest producer of crude oil, and has the second-greatest number of active drilling rigs, and ranks fifth in crude oil reserves. While the state ranked eighth for installed wind energy capacity in 2011, it is at the bottom of states in usage of renewable energy, with 94 percent of its electricity being generated by non-renewable sources in 2009, including 25 percent from coal and 46 percent from natural gas. Oklahoma has no nuclear power. Ranking 13th for total energy consumption per capita in 2009, Oklahoma's energy costs were 8th lowest in the nation.
According to Forbes magazine, Oklahoma City-based Devon Energy Corporation, Chesapeake Energy Corporation, and SandRidge Energy Corporation are the largest private oil-related companies in the nation, and all of Oklahoma's Fortune 500 companies are energy-related. Tulsa's ONEOK and Williams Companies are the state's largest and second-largest companies respectively, also ranking as the nation's second and third-largest companies in the field of energy, according to Fortune magazine. The magazine also placed Devon Energy as the second-largest company in the mining and crude oil-producing industry in the nation, while Chesapeake Energy ranks seventh respectively in that sector and Oklahoma Gas & Electric ranks as the 25th-largest gas and electric utility company.
The state has a rich history in ballet with five Native American ballerinas attaining worldwide fame. These were Yvonne Chouteau, sisters Marjorie and Maria Tallchief, Rosella Hightower and Moscelyne Larkin, known collectively as the Five Moons. The New York Times rates the Tulsa Ballet as one of the top ballet companies in the United States. The Oklahoma City Ballet and University of Oklahoma's dance program were formed by ballerina Yvonne Chouteau and husband Miguel Terekhov. The University program was founded in 1962 and was the first fully accredited program of its kind in the United States.
In Sand Springs, an outdoor amphitheater called "Discoveryland!" is the official performance headquarters for the musical Oklahoma! Ridge Bond, native of McAlester, Oklahoma, starred in the Broadway and International touring productions of Oklahoma!, playing the role of "Curly McClain" in more than 2,600 performances. In 1953 he was featured along with the Oklahoma! cast on a CBS Omnibus television broadcast. Bond was instrumental in the title song becoming the Oklahoma state song and is also featured on the U.S. postage stamp commemorating the musical's 50th anniversary. Historically, the state has produced musical styles such as The Tulsa Sound and western swing, which was popularized at Cain's Ballroom in Tulsa. The building, known as the "Carnegie Hall of Western Swing", served as the performance headquarters of Bob Wills and the Texas Playboys during the 1930s. Stillwater is known as the epicenter of Red Dirt music, the best-known proponent of which is the late Bob Childers.
Prominent theatre companies in Oklahoma include, in the capital city, Oklahoma City Theatre Company, Carpenter Square Theatre, Oklahoma Shakespeare in the Park, and CityRep. CityRep is a professional company affording equity points to those performers and technical theatre professionals. In Tulsa, Oklahoma's oldest resident professional company is American Theatre Company, and Theatre Tulsa is the oldest community theatre company west of the Mississippi. Other companies in Tulsa include Heller Theatre and Tulsa Spotlight Theater. The cities of Norman, Lawton, and Stillwater, among others, also host well-reviewed community theatre companies.
Oklahoma is in the nation's middle percentile in per capita spending on the arts, ranking 17th, and contains more than 300 museums. The Philbrook Museum of Tulsa is considered one of the top 50 fine art museums in the United States, and the Sam Noble Oklahoma Museum of Natural History in Norman, one of the largest university-based art and history museums in the country, documents the natural history of the region. The collections of Thomas Gilcrease are housed in the Gilcrease Museum of Tulsa, which also holds the world's largest, most comprehensive collection of art and artifacts of the American West.
The Egyptian art collection at the Mabee-Gerrer Museum of Art in Shawnee is considered to be the finest Egyptian collection between Chicago and Los Angeles. The Oklahoma City Museum of Art contains the most comprehensive collection of glass sculptures by artist Dale Chihuly in the world, and Oklahoma City's National Cowboy and Western Heritage Museum documents the heritage of the American Western frontier. With remnants of the Holocaust and artifacts relevant to Judaism, the Sherwin Miller Museum of Jewish Art of Tulsa preserves the largest collection of Jewish art in the Southwest United States.
Oklahoma's centennial celebration was named the top event in the United States for 2007 by the American Bus Association, and consisted of multiple celebrations saving with the 100th anniversary of statehood on November 16, 2007. Annual ethnic festivals and events take place throughout the state such as Native American powwows and ceremonial events, and include festivals (as examples) in Scottish, Irish, German, Italian, Vietnamese, Chinese, Czech, Jewish, Arab, Mexican and African-American communities depicting cultural heritage or traditions.
During a 10-day run in Oklahoma City, the State Fair of Oklahoma attracts roughly one million people along with the annual Festival of the Arts. Large national pow-wows, various Latin and Asian heritage festivals, and cultural festivals such as the Juneteenth celebrations are held in Oklahoma City each year. The Tulsa State Fair attracts over one million people during its 10-day run, and the city's Mayfest festival entertained more than 375,000 people in four days during 2007. In 2006, Tulsa's Oktoberfest was named one of the top 10 in the world by USA Today and one of the top German food festivals in the nation by Bon Appetit magazine.
Norman plays host to the Norman Music Festival, a festival that highlights native Oklahoma bands and musicians. Norman is also host to the Medieval Fair of Norman, which has been held annually since 1976 and was Oklahoma's first medieval fair. The Fair was held first on the south oval of the University of Oklahoma campus and in the third year moved to the Duck Pond in Norman until the Fair became too big and moved to Reaves Park in 2003. The Medieval Fair of Norman is Oklahoma's "largest weekend event and the third largest event in Oklahoma, and was selected by Events Media Network as one of the top 100 events in the nation".
With an educational system made up of public school districts and independent private institutions, Oklahoma had 638,817 students enrolled in 1,845 public primary, secondary, and vocational schools in 533 school districts as of 2008[update]. Oklahoma has the highest enrollment of Native American students in the nation with 126,078 students in the 2009-10 school year. Ranked near the bottom of states in expenditures per student, Oklahoma spent $7,755 for each student in 2008, 47th in the nation, though its growth of total education expenditures between 1992 and 2002 ranked 22nd.
The state is among the best in pre-kindergarten education, and the National Institute for Early Education Research rated it first in the United States with regard to standards, quality, and access to pre-kindergarten education in 2004, calling it a model for early childhood schooling. High school dropout rate decreased from 3.1 to 2.5 percent between 2007 and 2008 with Oklahoma ranked among 18 other states with 3 percent or less dropout rate. In 2004, the state ranked 36th in the nation for the relative number of adults with high school diplomas, though at 85.2 percent, it had the highest rate among southern states.
Oklahoma holds eleven public regional universities, including Northeastern State University, the second-oldest institution of higher education west of the Mississippi River, also containing the only College of Optometry in Oklahoma and the largest enrollment of Native American students in the nation by percentage and amount. Langston University is Oklahoma's only historically black college. Six of the state's universities were placed in the Princeton Review's list of best 122 regional colleges in 2007, and three made the list of top colleges for best value. The state has 55 post-secondary technical institutions operated by Oklahoma's CareerTech program for training in specific fields of industry or trade.
In the 2007–2008 school year, there were 181,973 undergraduate students, 20,014 graduate students, and 4,395 first-professional degree students enrolled in Oklahoma colleges. Of these students, 18,892 received a bachelor's degree, 5,386 received a master's degree, and 462 received a first professional degree. This means the state of Oklahoma produces an average of 38,278 degree-holders per completions component (i.e. July 1, 2007 – June 30, 2008). National average is 68,322 total degrees awarded per completions component.
The Cherokee Nation instigated a 10-year language preservation plan that involved growing new fluent speakers of the Cherokee language from childhood on up through school immersion programs as well as a collaborative community effort to continue to use the language at home. This plan was part of an ambitious goal that in 50 years, 80% or more of the Cherokee people will be fluent in the language. The Cherokee Preservation Foundation has invested $3 million into opening schools, training teachers, and developing curricula for language education, as well as initiating community gatherings where the language can be actively used. There is a Cherokee language immersion school in Tahlequah, Oklahoma that educates students from pre-school through eighth grade. Graduates are fluent speakers of the language. Several universities offer Cherokee as a second language, including the University of Oklahoma and Northeastern State University.
Oklahoma has teams in basketball, football, arena football, baseball, soccer, hockey, and wrestling located in Oklahoma City, Tulsa, Enid, Norman, and Lawton. The Oklahoma City Thunder of the National Basketball Association (NBA) is the state's only major league sports franchise. The state had a team in the Women's National Basketball Association, the Tulsa Shock, from 2010 through 2015, but the team relocated to Dallas–Fort Worth after that season and became the Dallas Wings. Oklahoma supports teams in several minor leagues, including Minor League Baseball at the AAA and AA levels (Oklahoma City Dodgers and Tulsa Drillers, respectively), hockey's ECHL with the Tulsa Oilers, and a number of indoor football leagues. In the last-named sport, the state's most notable team was the Tulsa Talons, which played in the Arena Football League until 2012, when the team was moved to San Antonio. The Oklahoma Defenders replaced the Talons as Tulsa's only professional arena football team, playing the CPIFL. The Oklahoma City Blue, of the NBA Development League, relocated to Oklahoma City from Tulsa in 2014, where they were formerly known as the Tulsa 66ers. Tulsa is the base for the Tulsa Revolution, which plays in the American Indoor Soccer League. Enid and Lawton host professional basketball teams in the USBL and the CBA.
The NBA's New Orleans Hornets became the first major league sports franchise based in Oklahoma when the team was forced to relocate to Oklahoma City's Ford Center, now known as Chesapeake Energy Arena, for two seasons following Hurricane Katrina in 2005. In July 2008, the Seattle SuperSonics, a franchise owned by the Professional Basketball Club LLC, a group of Oklahoma City businessmen led by Clayton Bennett, relocated to Oklahoma City and announced that play would begin at the Ford Center as the Oklahoma City Thunder for the 2008–09 season, becoming the state's first permanent major league franchise.
Collegiate athletics are a popular draw in the state. The state has four schools that compete at the highest level of college sports, NCAA Division I. The most prominent are the state's two members of the Big 12 Conference, one of the so-called Power Five conferences of the top tier of college football, Division I FBS. The University of Oklahoma and Oklahoma State University average well over 50,000 fans attending their football games, and Oklahoma's football program ranked 12th in attendance among American colleges in 2010, with an average of 84,738 people attending its home games. The two universities meet several times each year in rivalry matches known as the Bedlam Series, which are some of the greatest sporting draws to the state. Sports Illustrated magazine rates Oklahoma and Oklahoma State among the top colleges for athletics in the nation. Two private institutions in Tulsa, the University of Tulsa and Oral Roberts University; are also Division I members. Tulsa competes in FBS football and other sports in the American Athletic Conference, while Oral Roberts, which does not sponsor football, is a member of The Summit League. In addition, 12 of the state's smaller colleges and universities compete in NCAA Division II as members of four different conferences, and eight other Oklahoma institutions participate in the NAIA, mostly within the Sooner Athletic Conference.
Regular LPGA tournaments are held at Cedar Ridge Country Club in Tulsa, and major championships for the PGA or LPGA have been played at Southern Hills Country Club in Tulsa, Oak Tree Country Club in Oklahoma City, and Cedar Ridge Country Club in Tulsa. Rated one of the top golf courses in the nation, Southern Hills has hosted four PGA Championships, including one in 2007, and three U.S. Opens, the most recent in 2001. Rodeos are popular throughout the state, and Guymon, in the state's panhandle, hosts one of the largest in the nation.
The state has two primary newspapers. The Oklahoman, based in Oklahoma City, is the largest newspaper in the state and 54th-largest in the nation by circulation, with a weekday readership of 138,493 and a Sunday readership of 202,690. The Tulsa World, the second most widely circulated newspaper in Oklahoma and 79th in the nation, holds a Sunday circulation of 132,969 and a weekday readership of 93,558. Oklahoma's first newspaper was established in 1844, called the Cherokee Advocate, and was written in both Cherokee and English. In 2006, there were more than 220 newspapers located in the state, including 177 with weekly publications and 48 with daily publications.
More than 12,000 miles (19,000 km) of roads make up the state's major highway skeleton, including state-operated highways, ten turnpikes or major toll roads, and the longest drivable stretch of Route 66 in the nation. In 2008, Interstate 44 in Oklahoma City was Oklahoma's busiest highway, with a daily traffic volume of 123,300 cars. In 2010, the state had the nation's third highest number of bridges classified as structurally deficient, with nearly 5,212 bridges in disrepair, including 235 National Highway System Bridges.
Oklahoma's largest commercial airport is Will Rogers World Airport in Oklahoma City, averaging a yearly passenger count of more than 3.5 million (1.7 million boardings) in 2010. Tulsa International Airport, the state's second largest commercial airport, served more than 1.3 million boardings in 2010. Between the two, six airlines operate in Oklahoma. In terms of traffic, R. L. Jones Jr. (Riverside) Airport in Tulsa is the state's busiest airport, with 335,826 takeoffs and landings in 2008. In total, Oklahoma has over 150 public-use airports.
Two inland ports on rivers serve Oklahoma: the Port of Muskogee and the Tulsa Port of Catoosa. The only port handling international cargo in the state, the Tulsa Port of Catoosa is the most inland ocean-going port in the nation and ships over two million tons of cargo each year. Both ports are located on the McClellan-Kerr Arkansas River Navigation System, which connects barge traffic from Tulsa and Muskogee to the Mississippi River via the Verdigris and Arkansas rivers, contributing to one of the busiest waterways in the world.
Oklahoma's judicial branch consists of the Oklahoma Supreme Court, the Oklahoma Court of Criminal Appeals, and 77 District Courts that each serves one county. The Oklahoma judiciary also contains two independent courts: a Court of Impeachment and the Oklahoma Court on the Judiciary. Oklahoma has two courts of last resort: the state Supreme Court hears civil cases, and the state Court of Criminal Appeals hears criminal cases (this split system exists only in Oklahoma and neighboring Texas). Judges of those two courts, as well as the Court of Civil Appeals are appointed by the Governor upon the recommendation of the state Judicial Nominating Commission, and are subject to a non-partisan retention vote on a six-year rotating schedule.
The executive branch consists of the Governor, their staff, and other elected officials. The principal head of government, the Governor is the chief executive of the Oklahoma executive branch, serving as the ex officio Commander-in-Chief of the Oklahoma National Guard when not called into Federal use and reserving the power to veto bills passed through the Legislature. The responsibilities of the Executive branch include submitting the budget, ensuring that state laws are enforced, and ensuring peace within the state is preserved.
The state is divided into 77 counties that govern locally, each headed by a three-member council of elected commissioners, a tax assessor, clerk, court clerk, treasurer, and sheriff. While each municipality operates as a separate and independent local government with executive, legislative and judicial power, county governments maintain jurisdiction over both incorporated cities and non-incorporated areas within their boundaries, but have executive power but no legislative or judicial power. Both county and municipal governments collect taxes, employ a separate police force, hold elections, and operate emergency response services within their jurisdiction. Other local government units include school districts, technology center districts, community college districts, rural fire departments, rural water districts, and other special use districts.
Thirty-nine Native American tribal governments are based in Oklahoma, each holding limited powers within designated areas. While Indian reservations typical in most of the United States are not present in Oklahoma, tribal governments hold land granted during the Indian Territory era, but with limited jurisdiction and no control over state governing bodies such as municipalities and counties. Tribal governments are recognized by the United States as quasi-sovereign entities with executive, judicial, and legislative powers over tribal members and functions, but are subject to the authority of the United States Congress to revoke or withhold certain powers. The tribal governments are required to submit a constitution and any subsequent amendments to the United States Congress for approval.
After the 1948 election, the state turned firmly Republican. Although registered Republicans were a minority in the state until 2015, starting in 1952, Oklahoma has been carried by Republican presidential candidates in all but one election (1964). This is not to say that every election has been a landslide for Republicans: Jimmy Carter lost the state by less than 1.5% in 1976, while Michael Dukakis and Bill Clinton both won 40% or more of the state's popular vote in 1988 and 1996 respectively. Al Gore in 2000, though, was the last Democrat to even win any counties in the state. Oklahoma was the only state where Barack Obama failed to carry any of its counties in both 2008 and 2012.
Following the 2000 census, the Oklahoma delegation to the U.S. House of Representatives was reduced from six to five representatives, each serving one congressional district. For the 112th Congress (2011–2013), there were no changes in party strength, and the delegation included four Republicans and one Democrat. In the 112th Congress, Oklahoma's U.S. senators were Republicans Jim Inhofe and Tom Coburn, and its U.S. Representatives were John Sullivan (R-OK-1), Dan Boren (D-OK-2), Frank D. Lucas (R-OK-3), Tom Cole (R-OK-4), and James Lankford (R-OK-5).
Oklahoma had 598 incorporated places in 2010, including four cities over 100,000 in population and 43 over 10,000. Two of the fifty largest cities in the United States are located in Oklahoma, Oklahoma City and Tulsa, and 65 percent of Oklahomans live within their metropolitan areas, or spheres of economic and social influence defined by the United States Census Bureau as a metropolitan statistical area. Oklahoma City, the state's capital and largest city, had the largest metropolitan area in the state in 2010, with 1,252,987 people, and the metropolitan area of Tulsa had 937,478 residents. Between 2000 and 2010, the cities that led the state in population growth were Blanchard (172.4%), Elgin (78.2%), Jenks (77.0%), Piedmont (56.7%), Bixby (56.6%), and Owasso (56.3%).
In descending order of population, Oklahoma's largest cities in 2010 were: Oklahoma City (579,999, +14.6%), Tulsa (391,906, −0.3%), Norman (110,925, +15.9%), Broken Arrow (98,850, +32.0%), Lawton (96,867, +4.4%), Edmond (81,405, +19.2%), Moore (55,081, +33.9%), Midwest City (54,371, +0.5%), Enid (49,379, +5.0%), and Stillwater (45,688, +17.0%). Of the state's ten largest cities, three are outside the metropolitan areas of Oklahoma City and Tulsa, and only Lawton has a metropolitan statistical area of its own as designated by the United States Census Bureau, though the metropolitan statistical area of Fort Smith, Arkansas extends into the state.
State law codifies Oklahoma's state emblems and honorary positions; the Oklahoma Senate or House of Representatives may adopt resolutions designating others for special events and to benefit organizations. Currently the State Senate is waiting to vote on a change to the state's motto. The House passed HCR 1024, which will change the state motto from "Labor Omnia Vincit" to "Oklahoma-In God We Trust!" The author of the resolution stated that a constituent researched the Oklahoma Constitution and found no "official" vote regarding "Labor Omnia Vincit", therefore opening the door for an entirely new motto.
Popper is known for his rejection of the classical inductivist views on the scientific method, in favour of empirical falsification: A theory in the empirical sciences can never be proven, but it can be falsified, meaning that it can and should be scrutinized by decisive experiments. He used the black swan fallacy to discuss falsification. If the outcome of an experiment contradicts the theory, one should refrain from ad hoc manoeuvres that evade the contradiction merely by making it less falsifiable. Popper is also known for his opposition to the classical justificationist account of knowledge, which he replaced with critical rationalism, "the first non-justificational philosophy of criticism in the history of philosophy."
Karl Popper was born in Vienna (then in Austria-Hungary) in 1902, to upper middle-class parents. All of Karl Popper's grandparents were Jewish, but the Popper family converted to Lutheranism before Karl was born, and so he received Lutheran baptism. They understood this as part of their cultural assimilation, not as an expression of devout belief. Karl's father Simon Siegmund Carl Popper was a lawyer from Bohemia and a doctor of law at the Vienna University, and mother Jenny Schiff was of Silesian and Hungarian descent. After establishing themselves in Vienna, the Poppers made a rapid social climb in Viennese society: Simon Siegmund Carl became a partner in the law firm of Vienna's liberal Burgomaster Herr Grübl and, after Grübl's death in 1898, Simon took over the business. (Malachi Hacohen records that Herr Grübl's first name was Raimund, after which Karl received his middle name. Popper himself, in his autobiography, erroneously recalls that Herr Grübl's first name was Carl.) His father was a bibliophile who had 12,000–14,000 volumes in his personal library. Popper inherited both the library and the disposition from him.
Popper left school at the age of 16 and attended lectures in mathematics, physics, philosophy, psychology and the history of music as a guest student at the University of Vienna. In 1919, Popper became attracted by Marxism and subsequently joined the Association of Socialist School Students. He also became a member of the Social Democratic Workers' Party of Austria, which was at that time a party that fully adopted the Marxist ideology. After the street battle in the Hörlgasse on 15 June 1919, when police shot eight of his unarmed party comrades, he became disillusioned by what he saw to be the "pseudo-scientific" historical materialism of Marx, abandoned the ideology, and remained a supporter of social liberalism throughout his life.
He worked in street construction for a short amount of time, but was unable to cope with the heavy labour. Continuing to attend university as a guest student, he started an apprenticeship as cabinetmaker, which he completed as a journeyman. He was dreaming at that time of starting a daycare facility for children, for which he assumed the ability to make furniture might be useful. After that he did voluntary service in one of psychoanalyst Alfred Adler's clinics for children. In 1922, he did his matura by way of a second chance education and finally joined the University as an ordinary student. He completed his examination as an elementary teacher in 1924 and started working at an after-school care club for socially endangered children. In 1925, he went to the newly founded Pädagogisches Institut and continued studying philosophy and psychology. Around that time he started courting Josefine Anna Henninger, who later became his wife.
In 1928, he earned a doctorate in psychology, under the supervision of Karl Bühler. His dissertation was entitled "Die Methodenfrage der Denkpsychologie" (The question of method in cognitive psychology). In 1929, he obtained the authorisation to teach mathematics and physics in secondary school, which he started doing. He married his colleague Josefine Anna Henninger (1906–1985) in 1930. Fearing the rise of Nazism and the threat of the Anschluss, he started to use the evenings and the nights to write his first book Die beiden Grundprobleme der Erkenntnistheorie (The Two Fundamental Problems of the Theory of Knowledge). He needed to publish one to get some academic position in a country that was safe for people of Jewish descent. However, he ended up not publishing the two-volume work, but a condensed version of it with some new material, Logik der Forschung (The Logic of Scientific Discovery), in 1934. Here, he criticised psychologism, naturalism, inductionism, and logical positivism, and put forth his theory of potential falsifiability as the criterion demarcating science from non-science. In 1935 and 1936, he took unpaid leave to go to the United Kingdom for a study visit.
In 1937, Popper finally managed to get a position that allowed him to emigrate to New Zealand, where he became lecturer in philosophy at Canterbury University College of the University of New Zealand in Christchurch. It was here that he wrote his influential work The Open Society and its Enemies. In Dunedin he met the Professor of Physiology John Carew Eccles and formed a lifelong friendship with him. In 1946, after the Second World War, he moved to the United Kingdom to become reader in logic and scientific method at the London School of Economics. Three years later, in 1949, he was appointed professor of logic and scientific method at the University of London. Popper was president of the Aristotelian Society from 1958 to 1959. He retired from academic life in 1969, though he remained intellectually active for the rest of his life. In 1985, he returned to Austria so that his wife could have her relatives around her during the last months of her life; she died in November that year. After the Ludwig Boltzmann Gesellschaft failed to establish him as the director of a newly founded branch researching the philosophy of science, he went back again to the United Kingdom in 1986, settling in Kenley, Surrey.
Popper died of "complications of cancer, pneumonia and kidney failure" in Kenley at the age of 92 on 17 September 1994. He had been working continuously on his philosophy until two weeks before, when he suddenly fell terminally ill. After cremation, his ashes were taken to Vienna and buried at Lainzer cemetery adjacent to the ORF Centre, where his wife Josefine Anna Popper (called ‘Hennie’) had already been buried. Popper's estate is managed by his secretary and personal assistant Melitta Mew and her husband Raymond. Popper's manuscripts went to the Hoover Institution at Stanford University, partly during his lifetime and partly as supplementary material after his death. Klagenfurt University possesses Popper's library, including his precious bibliophilia, as well as hard copies of the original Hoover material and microfilms of the supplementary material. The remaining parts of the estate were mostly transferred to The Karl Popper Charitable Trust. In October 2008 Klagenfurt University acquired the copyrights from the estate.
Popper won many awards and honours in his field, including the Lippincott Award of the American Political Science Association, the Sonning Prize, the Otto Hahn Peace Medal of the United Nations Association of Germany in Berlin and fellowships in the Royal Society, British Academy, London School of Economics, King's College London, Darwin College, Cambridge, and Charles University, Prague. Austria awarded him the Grand Decoration of Honour in Gold for Services to the Republic of Austria in 1986, and the Federal Republic of Germany its Grand Cross with Star and Sash of the Order of Merit, and the peace class of the Order Pour le Mérite. He received the Humanist Laureate Award from the International Academy of Humanism. He was knighted by Queen Elizabeth II in 1965, and was elected a Fellow of the Royal Society in 1976. He was invested with the Insignia of a Companion of Honour in 1982.
Other awards and recognition for Popper included the City of Vienna Prize for the Humanities (1965), Karl Renner Prize (1978), Austrian Decoration for Science and Art (1980), Dr. Leopold Lucas Prize (1981), Ring of Honour of the City of Vienna (1983) and the Premio Internazionale of the Italian Federico Nietzsche Society (1988). In 1992, he was awarded the Kyoto Prize in Arts and Philosophy for "symbolising the open spirit of the 20th century" and for his "enormous influence on the formation of the modern intellectual climate".
Karl Popper's rejection of Marxism during his teenage years left a profound mark on his thought. He had at one point joined a socialist association, and for a few months in 1919 considered himself a communist. During this time he became familiar with the Marxist view of economics, class-war, and history. Although he quickly became disillusioned with the views expounded by Marxism, his flirtation with the ideology led him to distance himself from those who believed that spilling blood for the sake of a revolution was necessary. He came to realise that when it came to sacrificing human lives, one was to think and act with extreme prudence.
The failure of democratic parties to prevent fascism from taking over Austrian politics in the 1920s and 1930s traumatised Popper. He suffered from the direct consequences of this failure, since events after the Anschluss, the annexation of Austria by the German Reich in 1938, forced him into permanent exile. His most important works in the field of social science—The Poverty of Historicism (1944) and The Open Society and Its Enemies (1945)—were inspired by his reflection on the events of his time and represented, in a sense, a reaction to the prevalent totalitarian ideologies that then dominated Central European politics. His books defended democratic liberalism as a social and political philosophy. They also represented extensive critiques of the philosophical presuppositions underpinning all forms of totalitarianism.
Popper puzzled over the stark contrast between the non-scientific character of Freud and Adler's theories in the field of psychology and the revolution set off by Einstein's theory of relativity in physics in the early 20th century. Popper thought that Einstein's theory, as a theory properly grounded in scientific thought and method, was highly "risky", in the sense that it was possible to deduce consequences from it which were, in the light of the then-dominant Newtonian physics, highly improbable (e.g., that light is deflected towards solid bodies—confirmed by Eddington's experiments in 1919), and which would, if they turned out to be false, falsify the whole theory. In contrast, nothing could, even in principle, falsify psychoanalytic theories. He thus came to the conclusion that psychoanalytic theories had more in common with primitive myths than with genuine science.
This led Popper to conclude that what were regarded[by whom?] as the remarkable strengths of psychoanalytical theories were actually their weaknesses. Psychoanalytical theories were crafted in a way that made them able to refute any criticism and to give an explanation for every possible form of human behaviour. The nature of such theories made it impossible for any criticism or experiment - even in principle - to show them to be false. This realisation had an important consequence when Popper later tackled the problem of demarcation in the philosophy of science, as it led him to posit that the strength of a scientific theory lies in its both being susceptible to falsification, and not actually being falsified by criticism made of it. He considered that if a theory cannot, in principle, be falsified by criticism, it is not a scientific theory.
Popper coined the term "critical rationalism" to describe his philosophy. Concerning the method of science, the term indicates his rejection of classical empiricism, and the classical observationalist-inductivist account of science that had grown out of it. Popper argued strongly against the latter, holding that scientific theories are abstract in nature, and can be tested only indirectly, by reference to their implications. He also held that scientific theory, and human knowledge generally, is irreducibly conjectural or hypothetical, and is generated by the creative imagination to solve problems that have arisen in specific historico-cultural settings.
Logically, no number of positive outcomes at the level of experimental testing can confirm a scientific theory, but a single counterexample is logically decisive: it shows the theory, from which the implication is derived, to be false. To say that a given statement (e.g., the statement of a law of some scientific theory) -- [call it "T"] -- is "falsifiable" does not mean that "T" is false. Rather, it means that, if "T" is false, then (in principle), "T" could be shown to be false, by observation or by experiment. Popper's account of the logical asymmetry between verification and falsifiability lies at the heart of his philosophy of science. It also inspired him to take falsifiability as his criterion of demarcation between what is, and is not, genuinely scientific: a theory should be considered scientific if, and only if, it is falsifiable. This led him to attack the claims of both psychoanalysis and contemporary Marxism to scientific status, on the basis that their theories are not falsifiable.
In All Life is Problem Solving, Popper sought to explain the apparent progress of scientific knowledge – that is, how it is that our understanding of the universe seems to improve over time. This problem arises from his position that the truth content of our theories, even the best of them, cannot be verified by scientific testing, but can only be falsified. Again, in this context the word "falsified" does not refer to something being "fake"; rather, that something can be (i.e., is capable of being) shown to be false by observation or experiment. Some things simply do not lend themselves to being shown to be false, and therefore, are not falsifiable. If so, then how is it that the growth of science appears to result in a growth in knowledge? In Popper's view, the advance of scientific knowledge is an evolutionary process characterised by his formula:
In response to a given problem situation (), a number of competing conjectures, or tentative theories (), are systematically subjected to the most rigorous attempts at falsification possible. This process, error elimination (), performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more "fit"—in other words, more applicable to the problem situation at hand (). Consequently, just as a species' biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may, in Popper's view, reflect a certain type of progress: toward more and more interesting problems (). For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances toward greater and greater problems; in a process very much akin to the interplay between genetic variation and natural selection.
Among his contributions to philosophy is his claim to have solved the philosophical problem of induction. He states that while there is no way to prove that the sun will rise, it is possible to formulate the theory that every day the sun will rise; if it does not rise on some particular day, the theory will be falsified and will have to be replaced by a different one. Until that day, there is no need to reject the assumption that the theory is true. Nor is it rational according to Popper to make instead the more complex assumption that the sun will rise until a given day, but will stop doing so the day after, or similar statements with additional conditions.
Popper held that rationality is not restricted to the realm of empirical or scientific theories, but that it is merely a special case of the general method of criticism, the method of finding and eliminating contradictions in knowledge without ad-hoc-measures. According to this view, rational discussion about metaphysical ideas, about moral values and even about purposes is possible. Popper's student W.W. Bartley III tried to radicalise this idea and made the controversial claim that not only can criticism go beyond empirical knowledge, but that everything can be rationally criticised.
To Popper, who was an anti-justificationist, traditional philosophy is misled by the false principle of sufficient reason. He thinks that no assumption can ever be or needs ever to be justified, so a lack of justification is not a justification for doubt. Instead, theories should be tested and scrutinised. It is not the goal to bless theories with claims of certainty or justification, but to eliminate errors in them. He writes, "there are no such things as good positive reasons; nor do we need such things [...] But [philosophers] obviously cannot quite bring [themselves] to believe that this is my opinion, let alone that it is right" (The Philosophy of Karl Popper, p. 1043)
In The Open Society and Its Enemies and The Poverty of Historicism, Popper developed a critique of historicism and a defence of the "Open Society". Popper considered historicism to be the theory that history develops inexorably and necessarily according to knowable general laws towards a determinate end. He argued that this view is the principal theoretical presupposition underpinning most forms of authoritarianism and totalitarianism. He argued that historicism is founded upon mistaken assumptions regarding the nature of scientific law and prediction. Since the growth of human knowledge is a causal factor in the evolution of human history, and since "no society can predict, scientifically, its own future states of knowledge", it follows, he argued, that there can be no predictive science of human history. For Popper, metaphysical and historical indeterminism go hand in hand.
As early as 1934, Popper wrote of the search for truth as "one of the strongest motives for scientific discovery." Still, he describes in Objective Knowledge (1972) early concerns about the much-criticised notion of truth as correspondence. Then came the semantic theory of truth formulated by the logician Alfred Tarski and published in 1933. Popper writes of learning in 1935 of the consequences of Tarski's theory, to his intense joy. The theory met critical objections to truth as correspondence and thereby rehabilitated it. The theory also seemed, in Popper's eyes, to support metaphysical realism and the regulative idea of a search for truth.
According to this theory, the conditions for the truth of a sentence as well as the sentences themselves are part of a metalanguage. So, for example, the sentence "Snow is white" is true if and only if snow is white. Although many philosophers have interpreted, and continue to interpret, Tarski's theory as a deflationary theory, Popper refers to it as a theory in which "is true" is replaced with "corresponds to the facts". He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. He identifies Tarski's formulation of the truth conditions of sentences as the introduction of a "metalinguistic predicate" and distinguishes the following cases:
Upon this basis, along with that of the logical content of assertions (where logical content is inversely proportional to probability), Popper went on to develop his important notion of verisimilitude or "truthlikeness". The intuitive idea behind verisimilitude is that the assertions or hypotheses of scientific theories can be objectively measured with respect to the amount of truth and falsity that they imply. And, in this way, one theory can be evaluated as more or less true than another on a quantitative basis which, Popper emphasises forcefully, has nothing to do with "subjective probabilities" or other merely "epistemic" considerations.
Knowledge, for Popper, was objective, both in the sense that it is objectively true (or truthlike), and also in the sense that knowledge has an ontological status (i.e., knowledge as object) independent of the knowing subject (Objective Knowledge: An Evolutionary Approach, 1972). He proposed three worlds: World One, being the physical world, or physical states; World Two, being the world of mind, or mental states, ideas, and perceptions; and World Three, being the body of human knowledge expressed in its manifold forms, or the products of the second world made manifest in the materials of the first world (i.e., books, papers, paintings, symphonies, and all the products of the human mind). World Three, he argued, was the product of individual human beings in exactly the same sense that an animal path is the product of individual animals, and that, as such, has an existence and evolution independent of any individual knowing subjects. The influence of World Three, in his view, on the individual human mind (World Two) is at least as strong as the influence of World One. In other words, the knowledge held by a given individual mind owes at least as much to the total accumulated wealth of human knowledge, made manifest, as to the world of direct experience. As such, the growth of human knowledge could be said to be a function of the independent evolution of World Three. Many contemporary philosophers, such as Daniel Dennett, have not embraced Popper's Three World conjecture, due mostly, it seems, to its resemblance to mind-body dualism.
The creation–evolution controversy in the United States raises the issue of whether creationistic ideas may be legitimately called science and whether evolution itself may be legitimately called science. In the debate, both sides and even courts in their decisions have frequently invoked Popper's criterion of falsifiability (see Daubert standard). In this context, passages written by Popper are frequently quoted in which he speaks about such issues himself. For example, he famously stated "Darwinism is not a testable scientific theory, but a metaphysical research program—a possible framework for testable scientific theories." He continued:
Popper had his own sophisticated views on evolution that go much beyond what the frequently-quoted passages say. In effect, Popper agreed with some of the points of both creationists and naturalists, but also disagreed with both views on crucial aspects. Popper understood the universe as a creative entity that invents new things, including life, but without the necessity of something like a god, especially not one who is pulling strings from behind the curtain. He said that evolution must, as the creationists say, work in a goal-directed way but disagreed with their view that it must necessarily be the hand of god that imposes these goals onto the stage of life.
Instead, he formulated the spearhead model of evolution, a version of genetic pluralism. According to this model, living organisms themselves have goals, and act according to these goals, each guided by a central control. In its most sophisticated form, this is the brain of humans, but controls also exist in much less sophisticated ways for species of lower complexity, such as the amoeba. This control organ plays a special role in evolution—it is the "spearhead of evolution". The goals bring the purpose into the world. Mutations in the genes that determine the structure of the control may then cause drastic changes in behaviour, preferences and goals, without having an impact on the organism's phenotype. Popper postulates that such purely behavioural changes are less likely to be lethal for the organism compared to drastic changes of the phenotype.
Popper contrasts his views with the notion of the "hopeful monster" that has large phenotype mutations and calls it the "hopeful behavioural monster". After behaviour has changed radically, small but quick changes of the phenotype follow to make the organism fitter to its changed goals. This way it looks as if the phenotype were changing guided by some invisible hand, while it is merely natural selection working in combination with the new behaviour. For example, according to this hypothesis, the eating habits of the giraffe must have changed before its elongated neck evolved. Popper contrasted this view as "evolution from within" or "active Darwinism" (the organism actively trying to discover new ways of life and being on a quest for conquering new ecological niches), with the naturalistic "evolution from without" (which has the picture of a hostile environment only trying to kill the mostly passive organism, or perhaps segregate some of its groups).
About the creation-evolution controversy, Popper wrote that he considered it "a somewhat sensational clash between a brilliant scientific hypothesis concerning the history of the various species of animals and plants on earth, and an older metaphysical theory which, incidentally, happened to be part of an established religious belief" with a footnote to the effect that "[he] agree[s] with Professor C.E. Raven when, in his Science, Religion, and the Future, 1943, he calls this conflict "a storm in a Victorian tea-cup"; though the force of this remark is perhaps a little impaired by the attention he pays to the vapours still emerging from the cup—to the Great Systems of Evolutionist Philosophy, produced by Bergson, Whitehead, Smuts, and others."
In an interview that Popper gave in 1969 with the condition that it shall be kept secret until after his death, he summarised his position on God as follows: "I don't know whether God exists or not. ... Some forms of atheism are arrogant and ignorant and should be rejected, but agnosticism—to admit that we don't know and to search—is all right. ... When I look at what I call the gift of life, I feel a gratitude which is in tune with some religious ideas of God. However, the moment I even speak of it, I am embarrassed that I may do something wrong to God in talking about God." He objected to organised religion, saying "it tends to use the name of God in vain", noting the danger of fanaticism because of religious conflicts: "The whole thing goes back to myths which, though they may have a kernel of truth, are untrue. Why then should the Jewish myth be true and the Indian and Egyptian myths not be true?" In a letter unrelated to the interview, he stressed his tolerant attitude: "Although I am not for religion, I do think that we should show respect for anybody who believes honestly."
Popper played a vital role in establishing the philosophy of science as a vigorous, autonomous discipline within philosophy, through his own prolific and influential works, and also through his influence on his own contemporaries and students. Popper founded in 1946 the Department of Philosophy, Logic and Scientific Method at the London School of Economics and there lectured and influenced both Imre Lakatos and Paul Feyerabend, two of the foremost philosophers of science in the next generation of philosophy of science. (Lakatos significantly modified Popper's position,:1 and Feyerabend repudiated it entirely, but the work of both is deeply influenced by Popper and engaged with many of the problems that Popper set.)
While there is some dispute as to the matter of influence, Popper had a long-standing and close friendship with economist Friedrich Hayek, who was also brought to the London School of Economics from Vienna. Each found support and similarities in the other's work, citing each other often, though not without qualification. In a letter to Hayek in 1944, Popper stated, "I think I have learnt more from you than from any other living thinker, except perhaps Alfred Tarski." Popper dedicated his Conjectures and Refutations to Hayek. For his part, Hayek dedicated a collection of papers, Studies in Philosophy, Politics, and Economics, to Popper, and in 1982 said, "...ever since his Logik der Forschung first came out in 1934, I have been a complete adherent to his general theory of methodology."
He does not argue that any such conclusions are therefore true, or that this describes the actual methods of any particular scientist.[citation needed] Rather, it is recommended as an essential principle of methodology that, if enacted by a system or community, will lead to slow but steady progress of a sort (relative to how well the system or community enacts the method). It has been suggested that Popper's ideas are often mistaken for a hard logical account of truth because of the historical co-incidence of their appearing at the same time as logical positivism, the followers of which mistook his aims for their own.
The Quine-Duhem thesis argues that it's impossible to test a single hypothesis on its own, since each one comes as part of an environment of theories. Thus we can only say that the whole package of relevant theories has been collectively falsified, but cannot conclusively say which element of the package must be replaced. An example of this is given by the discovery of the planet Neptune: when the motion of Uranus was found not to match the predictions of Newton's laws, the theory "There are seven planets in the solar system" was rejected, and not Newton's laws themselves. Popper discussed this critique of naïve falsificationism in Chapters 3 and 4 of The Logic of Scientific Discovery. For Popper, theories are accepted or rejected via a sort of selection process. Theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value. Thus Newton's laws, with their wide general application, are to be preferred over the much more specific "the solar system has seven planets".[dubious – discuss]
Popper claimed to have recognised already in the 1934 version of his Logic of Discovery a fact later stressed by Kuhn, "that scientists necessarily develop their ideas within a definite theoretical framework", and to that extent to have anticipated Kuhn's central point about "normal science". (But Popper criticised what he saw as Kuhn's relativism.) Also, in his collection Conjectures and Refutations: The Growth of Scientific Knowledge (Harper & Row, 1963), Popper writes, "Science must begin with myths, and with the criticism of myths; neither with the collection of observations, nor with the invention of experiments, but with the critical discussion of myths, and of magical techniques and practices. The scientific tradition is distinguished from the pre-scientific tradition in having two layers. Like the latter, it passes on its theories; but it also passes on a critical attitude towards them. The theories are passed on, not as dogmas, but rather with the challenge to discuss them and improve upon them."
Another objection is that it is not always possible to demonstrate falsehood definitively, especially if one is using statistical criteria to evaluate a null hypothesis. More generally it is not always clear, if evidence contradicts a hypothesis, that this is a sign of flaws in the hypothesis rather than of flaws in the evidence. However, this is a misunderstanding of what Popper's philosophy of science sets out to do. Rather than offering a set of instructions that merely need to be followed diligently to achieve science, Popper makes it clear in The Logic of Scientific Discovery that his belief is that the resolution of conflicts between hypotheses and observations can only be a matter of the collective judgment of scientists, in each individual case.
In a book called Science Versus Crime, Houck writes that Popper's falsificationism can be questioned logically: it is not clear how Popper would deal with a statement like "for every metal, there is a temperature at which it will melt." The hypothesis cannot be falsified by any possible observation, for there will always be a higher temperature than tested at which the metal may in fact melt, yet it seems to be a valid scientific hypothesis. These examples were pointed out by Carl Gustav Hempel. Hempel came to acknowledge that Logical Positivism's verificationism was untenable, but argued that falsificationism was equally untenable on logical grounds alone. The simplest response to this is that, because Popper describes how theories attain, maintain and lose scientific status, individual consequences of currently accepted scientific theories are scientific in the sense of being part of tentative scientific knowledge, and both of Hempel's examples fall under this category. For instance, atomic theory implies that all metals melt at some temperature.
In 2004, philosopher and psychologist Michel ter Hark (Groningen, The Netherlands) published a book, called Popper, Otto Selz and the rise of evolutionary epistemology, in which he claimed that Popper took some of his ideas from his tutor, the German psychologist Otto Selz. Selz never published his ideas, partly because of the rise of Nazism, which forced him to quit his work in 1933, and the prohibition of referring to Selz' work. Popper, the historian of ideas and his scholarship, is criticised in some academic quarters for his rejection of Plato, Hegel and Marx.
According to John N. Gray, Popper held that "a theory is scientific only in so far as it is falsifiable, and should be given up as soon as it is falsified." By applying Popper's account of scientific method, Gray's Straw Dogs states that this would have "killed the theories of Darwin and Einstein at birth." When they were first advanced, Gray claims, each of them was "at odds with some available evidence; only later did evidence become available that gave them crucial support." Against this, Gray seeks to establish the irrationalist thesis that "the progress of science comes from acting against reason."
Gray does not, however, give any indication of what available evidence these theories were at odds with, and his appeal to "crucial support" illustrates the very inductivist approach to science that Popper sought to show was logically illegitimate. For, according to Popper, Einstein's theory was at least equally as well corroborated as Newton's upon its initial conception; they both equally well accounted for all the hitherto available evidence. Moreover, since Einstein also explained the empirical refutations of Newton's theory, general relativity was immediately deemed suitable for tentative acceptance on the Popperian account. Indeed, Popper wrote, several decades before Gray's criticism, in reply to a critical essay by Imre Lakatos:
Such a theory would be true with higher probability, because it cannot be attacked so easily: to falsify the first one, it is sufficient to find that the sun has stopped rising; to falsify the second one, one additionally needs the assumption that the given day has not yet been reached. Popper held that it is the least likely, or most easily falsifiable, or simplest theory (attributes which he identified as all the same thing) that explains known facts that one should rationally prefer. His opposition to positivism, which held that it is the theory most likely to be true that one should prefer, here becomes very apparent. It is impossible, Popper argues, to ensure a theory to be true; it is more important that its falsity can be detected as easily as possible.
In his early years Popper was impressed by Marxism, whether of Communists or socialists. An event that happened in 1919 had a profound effect on him: During a riot, caused by the Communists, the police shot several unarmed people, including some of Popper's friends, when they tried to free party comrades from prison. The riot had, in fact, been part of a plan by which leaders of the Communist party with connections to Béla Kun tried to take power by a coup; Popper did not know about this at that time. However, he knew that the riot instigators were swayed by the Marxist doctrine that class struggle would produce vastly more dead men than the inevitable revolution brought about as quickly as possible, and so had no scruples to put the life of the rioters at risk to achieve their selfish goal of becoming the future leaders of the working class. This was the start of his later criticism of historicism. Popper began to reject Marxist historicism, which he associated with questionable means, and later socialism, which he associated with placing equality before freedom (to the possible disadvantage of equality).
Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.
As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.
The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.
The College of Engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the College of Science since the 1870s. Today the college, housed in the Fitzpatrick, Cushing, and Stinson-Remick Halls of Engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight B.S. degrees offered. Additionally, the college offers five-year dual degree programs with the Colleges of Arts and Letters and of Business awarding additional B.A. and Master of Business Administration (MBA) degrees, respectively.
All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.
The university first offered graduate degrees, in the form of a Master of Arts (MA), in the 1854–1855 academic year. The program expanded to include Master of Laws (LL.M.) and Master of Civil Engineering in its early stages of growth, before a formal graduate school education was developed with a thesis not required to receive the degrees. This changed in 1924 with formal requirements developed for graduate degrees, including offering Doctorate (PhD) degrees. Today each of the five colleges offer graduate education. Most of the departments from the College of Arts and Letters offer PhD programs, while a professional Master of Divinity (M.Div.) program also exists. All of the departments in the College of Science offer PhD programs, except for the Department of Pre-Professional Studies. The School of Architecture offers a Master of Architecture, while each of the departments of the College of Engineering offer PhD programs. The College of Business offers multiple professional programs including MBA and Master of Science in Accountancy programs. It also operates facilities in Chicago and Cincinnati for its executive MBA program. Additionally, the Alliance for Catholic Education program offers a Master of Education program where students study at the university during the summer and teach in Catholic elementary schools, middle schools, and high schools across the Southern United States for two school years.
The Joan B. Kroc Institute for International Peace Studies at the University of Notre Dame is dedicated to research, education and outreach on the causes of violent conflict and the conditions for sustainable peace. It offers PhD, Master's, and undergraduate degrees in peace studies. It was founded in 1986 through the donations of Joan B. Kroc, the widow of McDonald's owner Ray Kroc. The institute was inspired by the vision of the Rev. Theodore M. Hesburgh CSC, President Emeritus of the University of Notre Dame. The institute has contributed to international policy discussions about peace building practices.
The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as "Touchdown Jesus" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown.
Notre Dame is known for its competitive admissions, with the incoming class enrolling in fall 2015 admitting 3,577 from a pool of 18,156 (19.7%). The academic profile of the enrolled class continues to rate among the top 10 to 15 in the nation for national research universities. The university practices a non-restrictive early action policy that allows admitted students to consider admission to Notre Dame as well as any other colleges to which they were accepted. 1,400 of the 3,577 (39.1%) were admitted under the early action plan. Admitted students came from 1,311 high schools and the average student traveled more than 750 miles to Notre Dame, making it arguably the most representative university in the United States. While all entering students begin in the College of the First Year of Studies, 25% have indicated they plan to study in the liberal arts or social sciences, 24% in engineering, 24% in business, 24% in science, and 3% in architecture.
In 2015-2016, Notre Dame ranked 18th overall among "national universities" in the United States in U.S. News & World Report's Best Colleges 2016. In 2014, USA Today ranked Notre Dame 10th overall for American universities based on data from College Factual. Forbes.com's America's Best Colleges ranks Notre Dame 13th among colleges in the United States in 2015, 8th among Research Universities, and 1st in the Midwest. U.S. News & World Report also lists Notre Dame Law School as 22nd overall. BusinessWeek ranks Mendoza College of Business undergraduate school as 1st overall. It ranks the MBA program as 20th overall. The Philosophical Gourmet Report ranks Notre Dame's graduate philosophy program as 15th nationally, while ARCHITECT Magazine ranked the undergraduate architecture program as 12th nationally. Additionally, the study abroad program ranks sixth in highest participation percentage in the nation, with 57.6% of students choosing to study abroad in 17 countries. According to payscale.com, undergraduate alumni of University of Notre Dame have a mid-career median salary $110,000, making it the 24th highest among colleges and universities in the United States. The median starting salary of $55,300 ranked 58th in the same peer group.
Father Joseph Carrier, C.S.C. was Director of the Science Museum and the Library and Professor of Chemistry and Physics until 1874. Carrier taught that scientific research and its promise for progress were not antagonistic to the ideals of intellectual and moral culture endorsed by the Church. One of Carrier's students was Father John Augustine Zahm (1851–1921) who was made Professor and Co-Director of the Science Department at age 23 and by 1900 was a nationally prominent scientist and naturalist. Zahm was active in the Catholic Summer School movement, which introduced Catholic laity to contemporary intellectual issues. His book Evolution and Dogma (1896) defended certain aspects of evolutionary theory as true, and argued, moreover, that even the great Church teachers Thomas Aquinas and Augustine taught something like it. The intervention of Irish American Catholics in Rome prevented Zahm's censure by the Vatican. In 1913, Zahm and former President Theodore Roosevelt embarked on a major expedition through the Amazon.
In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models. Around 1899, Professor Jerome Green became the first American to send a wireless message. In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene. Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nuclear Astrophysics.
The Lobund Institute grew out of pioneering research in germ-free-life which began in 1928. This area of research originated in a question posed by Pasteur as to whether animal life was possible without bacteria. Though others had taken up this idea, their research was short lived and inconclusive. Lobund was the first research organization to answer definitively, that such life is possible and that it can be prolonged through generations. But the objective was not merely to answer Pasteur's question but also to produce the germ free animal as a new tool for biological and medical research. This objective was reached and for years Lobund was a unique center for the study and production of germ free animals and for their use in biological and medical investigations. Today the work has spread to other universities. In the beginning it was under the Department of Biology and a program leading to the master's degree accompanied the research program. In the 1940s Lobund achieved independent status as a purely research organization and in 1950 was raised to the status of an Institute. In 1958 it was brought back into the Department of Biology as integral part of that department, but with its own program leading to the degree of PhD in Gnotobiotics.
The Review of Politics was founded in 1939 by Gurian, modeled after German Catholic journals. It quickly emerged as part of an international Catholic intellectual revival, offering an alternative vision to positivist philosophy. For 44 years, the Review was edited by Gurian, Matthew Fitzsimons, Frederick Crosson, and Thomas Stritch. Intellectual leaders included Gurian, Jacques Maritain, Frank O'Malley, Leo Richard Ward, F. A. Hermens, and John U. Nef. It became a major forum for political ideas and modern political concerns, especially from a Catholic and scholastic tradition.
As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become "one of the pre–eminent research institutions in the world" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.
In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. Around 21–24% of students are children of alumni, and although 37% of students come from the Midwestern United States, the student body represents all 50 states and 100 countries. As of March 2007[update] The Princeton Review ranked the school as the fifth highest 'dream school' for parents to send their children. As of March 2015[update] The Princeton Review ranked Notre Dame as the ninth highest. The school has been previously criticized for its lack of diversity, and The Princeton Review ranks the university highly among schools at which "Alternative Lifestyles [are] Not an Alternative." It has also been commended by some diversity oriented publications; Hispanic Magazine in 2004 ranked the university ninth on its list of the top–25 colleges for Latinos, and The Journal of Blacks in Higher Education recognized the university in 2006 for raising enrollment of African-American students. With 6,000 participants, the university's intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where "Everyone Plays Intramural Sports." The annual Bookstore Basketball tournament is the largest outdoor five-on-five tournament in the world with over 700 teams participating each year, while the Notre Dame Men's Boxing Club hosts the annual Bengal Bouts tournament that raises money for the Holy Cross Missions in Bangladesh.
About 80% of undergraduates and 20% of graduate students live on campus. The majority of the graduate students on campus live in one of four graduate housing complexes on campus, while all on-campus undergraduates live in one of the 29 residence halls. Because of the religious affiliation of the university, all residence halls are single-sex, with 15 male dorms and 14 female dorms. The university maintains a visiting policy (known as parietal hours) for those students who live in dormitories, specifying times when members of the opposite sex are allowed to visit other students' dorm rooms; however, all residence halls have 24-hour social spaces for students regardless of gender. Many residence halls have at least one nun and/or priest as a resident. There are no traditional social fraternities or sororities at the university, but a majority of students live in the same residence hall for all four years. Some intramural sports are based on residence hall teams, where the university offers the only non-military academy program of full-contact intramural American football. At the end of the intramural season, the championship game is played on the field in Notre Dame Stadium.
The university is affiliated with the Congregation of Holy Cross (Latin: Congregatio a Sancta Cruce, abbreviated postnominals: "CSC"). While religious affiliation is not a criterion for admission, more than 93% of students identify as Christian, with over 80% of the total being Catholic. Collectively, Catholic Mass is celebrated over 100 times per week on campus, and a large campus ministry program provides for the faith needs of the community. There are multitudes of religious statues and artwork around campus, most prominent of which are the statue of Mary on the Main Building, the Notre Dame Grotto, and the Word of Life mural on Hesburgh Library depicting Christ as a teacher. Additionally, every classroom displays a crucifix. There are many religious clubs (catholic and non-Catholic) at the school, including Council #1477 of the Knights of Columbus (KOC), Baptist Collegiate Ministry (BCM), Jewish Club, Muslim Student Association, Orthodox Christian Fellowship, The Mormon Club, and many more. The Notre Dame KofC are known for being the first collegiate council of KofC, operating a charitable concession stand during every home football game and owning their own building on campus which can be used as a cigar lounge. Fifty-seven chapels are located throughout the campus.
This Main Building, and the library collection, was entirely destroyed by a fire in April 1879, and the school closed immediately and students were sent home. The university founder, Fr. Sorin and the president at the time, the Rev. William Corby, immediately planned for the rebuilding of the structure that had housed virtually the entire University. Construction was started on the 17th of May and by the incredible zeal of administrator and workers the building was completed before the fall semester of 1879. The library collection was also rebuilt and stayed housed in the new Main Building for years afterwards. Around the time of the fire, a music hall was opened. Eventually becoming known as Washington Hall, it hosted plays and musical acts put on by the school. By 1880, a science program was established at the university, and a Science Hall (today LaFortune Student Center) was built in 1883. The hall housed multiple classrooms and science labs needed for early research at the university.
In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president.
One of the main driving forces in the growth of the University was its football team, the Notre Dame Fighting Irish. Knute Rockne became head coach in 1918. Under Rockne, the Irish would post a record of 105 wins, 12 losses, and five ties. During his 13 years the Irish won three national championships, had five undefeated seasons, won the Rose Bowl in 1925, and produced players such as George Gipp and the "Four Horsemen". Knute Rockne has the highest winning percentage (.881) in NCAA Division I/FBS football history. Rockne's offenses employed the Notre Dame Box and his defenses ran a 7–2–2 scheme. The last game Rockne coached was on December 14, 1930 when he led a group of Notre Dame all-stars against the New York Giants in New York City.
The success of its football team made Notre Dame a household name. The success of Note Dame reflected rising status of Irish Americans and Catholics in the 1920s. Catholics rallied up around the team and listen to the games on the radio, especially when it knocked off the schools that symbolized the Protestant establishment in America — Harvard, Yale, Princeton, and Army. Yet this role as high-profile flagship institution of Catholicism made it an easy target of anti-Catholicism. The most remarkable episode of violence was the clash between Notre Dame students and the Ku Klux Klan in 1924. Nativism and anti-Catholicism, especially when directed towards immigrants, were cornerstones of the KKK's rhetoric, and Notre Dame was seen as a symbol of the threat posed by the Catholic Church. The Klan decided to have a week-long Klavern in South Bend. Clashes with the student body started on March 17, when students, aware of the anti-Catholic animosity, blocked the Klansmen from descending from their trains in the South Bend station and ripped the KKK clothes and regalia. On May 19 thousands of students massed downtown protesting the Klavern, and only the arrival of college president Fr. Matthew Walsh prevented any further clashes. The next day, football coach Knute Rockne spoke at a campus rally and implored the students to obey the college president and refrain from further violence. A few days later the Klavern broke up, but the hostility shown by the students was an omen and a contribution to the downfall of the KKK in Indiana.
Holy Cross Father John Francis O'Hara was elected vice-president in 1933 and president of Notre Dame in 1934. During his tenure at Notre Dame, he brought numerous refugee intellectuals to campus; he selected Frank H. Spearman, Jeremiah D. M. Ford, Irvin Abell, and Josephine Brownson for the Laetare Medal, instituted in 1883. O'Hara strongly believed that the Fighting Irish football team could be an effective means to "acquaint the public with the ideals that dominate" Notre Dame. He wrote, "Notre Dame football is a spiritual service because it is played for the honor and glory of God and of his Blessed Mother. When St. Paul said: 'Whether you eat or drink, or whatsoever else you do, do all for the glory of God,' he included football."
The Rev. John J. Cavanaugh, C.S.C. served as president from 1946 to 1952. Cavanaugh's legacy at Notre Dame in the post-war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when Notre Dame quadrupled in student census, undergraduate enrollment increased by more than half, and graduate student enrollment grew fivefold. Cavanaugh also established the Lobund Institute for Animal Studies and Notre Dame's Medieval Institute. Cavanaugh also presided over the construction of the Nieuwland Science Hall, Fisher Hall, and the Morris Inn, as well as the Hall of Liberal Arts (now O'Shaughnessy Hall), made possible by a donation from I.A. O'Shaughnessy, at the time the largest ever made to an American Catholic university. Cavanaugh also established a system of advisory councils at the university, which continue today and are vital to the university's governance and development
The Rev. Theodore Hesburgh, C.S.C., (1917–2015) served as president for 35 years (1952–87) of dramatic transformations. In that time the annual operating budget rose by a factor of 18 from $9.7 million to $176.6 million, and the endowment by a factor of 40 from $9 million to $350 million, and research funding by a factor of 20 from $735,000 to $15 million. Enrollment nearly doubled from 4,979 to 9,600, faculty more than doubled 389 to 950, and degrees awarded annually doubled from 1,212 to 2,500.
Hesburgh is also credited with transforming the face of Notre Dame by making it a coeducational institution. In the mid-1960s Notre Dame and Saint Mary's College developed a co-exchange program whereby several hundred students took classes not offered at their home institution, an arrangement that added undergraduate women to a campus that already had a few women in the graduate schools. After extensive debate, merging with St. Mary's was rejected, primarily because of the differential in faculty qualifications and pay scales. "In American college education," explained the Rev. Charles E. Sheedy, C.S.C., Notre Dame's Dean of Arts and Letters, "certain features formerly considered advantageous and enviable are now seen as anachronistic and out of place.... In this environment of diversity, the integration of the sexes is a normal and expected aspect, replacing separatism." Thomas Blantz, C.S.C., Notre Dame's Vice President of Student Affairs, added that coeducation "opened up a whole other pool of very bright students." Two of the male residence halls were converted for the newly admitted female students that first year, while two others were converted for the next school year. In 1971 Mary Ann Proctor became the first female undergraduate; she transferred from St. Mary's College. In 1972 the first woman to graduate was Angela Sienko, who earned a bachelor's degree in marketing.
In the 18 years under the presidency of Edward Malloy, C.S.C., (1987–2005), there was a rapid growth in the school's reputation, faculty, and resources. He increased the faculty by more than 500 professors; the academic quality of the student body has improved dramatically, with the average SAT score rising from 1240 to 1360; the number of minority students more than doubled; the endowment grew from $350 million to more than $3 billion; the annual operating budget rose from $177 million to more than $650 million; and annual research funding improved from $15 million to more than $70 million. Notre Dame's most recent[when?] capital campaign raised $1.1 billion, far exceeding its goal of $767 million, and is the largest in the history of Catholic higher education.
Since 2005, Notre Dame has been led by John I. Jenkins, C.S.C., the 17th president of the university. Jenkins took over the position from Malloy on July 1, 2005. In his inaugural address, Jenkins described his goals of making the university a leader in research that recognizes ethics and building the connection between faith and studies. During his tenure, Notre Dame has increased its endowment, enlarged its student body, and undergone many construction projects on campus, including Compton Family Ice Arena, a new architecture hall, additional residence halls, and the Campus Crossroads, a $400m enhancement and expansion of Notre Dame Stadium.
Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the spot of Fr. Sorin's original church, which became too small for the growing college. It is built in French Revival style and it is decorated by stained glass windows imported directly from France. The interior was painted by Luigi Gregori, an Italian painter invited by Fr. Sorin to be artist in residence. The Basilica also features a bell tower with a carillon. Inside the church there are also sculptures by Ivan Mestrovic. The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the most beloved spots on campus.
A Science Hall was built in 1883 under the direction of Fr. Zahm, but in 1950 it was converted to a student union building and named LaFortune Center, after Joseph LaFortune, an oil executive from Tulsa, Oklahoma. Commonly known as "LaFortune" or "LaFun," it is a 4-story building of 83,000 square feet that provides the Notre Dame community with a meeting place for social, recreational, cultural, and educational activities. LaFortune employs 35 part-time student staff and 29 full-time non-student staff and has an annual budget of $1.2 million. Many businesses, services, and divisions of The Office of Student Affairs are found within. The building also houses restaurants from national restaurant chains.
Since the construction of its oldest buildings, the university's physical plant has grown substantially. Over the years 29 residence halls have been built to accommodate students and each has been constructed with its own chapel. Many academic building were added together with a system of libraries, the most prominent of which is the Theodore Hesburgh Library, built in 1963 and today containing almost 4 million books. Since 2004, several buildings have been added, including the DeBartolo Performing Arts Center, the Guglielmino Complex, and the Jordan Hall of Science. Additionally, a new residence for men, Duncan Hall, was begun on March 8, 2007, and began accepting residents for the Fall 2008 semester. Ryan Hall was completed and began housing undergraduate women in the fall of 2009. A new engineering building, Stinson-Remick Hall, a new combination Center for Social Concerns/Institute for Church Life building, Geddes Hall, and a law school addition have recently been completed as well. Additionally the new hockey arena opened in the fall of 2011. The Stayer Center for Executive Education, which houses the Mendoza College of Business Executive Education Department opened in March 2013 just South of the Mendoza College of Business building. Because of its long athletic tradition, the university features also many building dedicated to sport. The most famous is Notre Dame Stadium, home of the Fighting Irish football team; it has been renovated several times and today it can hold more than 80 thousand people. Prominent venues include also the Edmund P. Joyce Center, with indoor basketball and volleyball courts, and the Compton Family Ice Arena, a two-rink facility dedicated to hockey. Also, there are many outdoor fields, as the Frank Eck Stadium for baseball.
The University of Notre Dame has made being a sustainability leader an integral part of its mission, creating the Office of Sustainability in 2008 to achieve a number of goals in the areas of power generation, design and construction, waste reduction, procurement, food services, transportation, and water.As of 2012[update] four building construction projects were pursuing LEED-Certified status and three were pursuing LEED Silver. Notre Dame's dining services sources 40% of its food locally and offers sustainably caught seafood as well as many organic, fair-trade, and vegan options. On the Sustainable Endowments Institute's College Sustainability Report Card 2010, University of Notre Dame received a "B" grade. The university also houses the Kroc Institute for International Peace Studies. Father Gustavo Gutierrez, the founder of Liberation Theology is a current faculty member.
The university owns several centers around the world used for international studies and research, conferences abroad, and alumni support. The university has had a presence in London, England, since 1968. Since 1998, its London center has been based in the former United University Club at 1 Suffolk Street in Trafalgar Square. The center enables the Colleges of Arts & Letters, Business Administration, Science, Engineering and the Law School to develop their own programs in London, as well as hosting conferences and symposia. Other Global Gateways are located in Beijing, Chicago, Dublin, Jerusalem and Rome.
The College of Arts and Letters was established as the university's first college in 1842 with the first degrees given in 1849. The university's first academic curriculum was modeled after the Jesuit Ratio Studiorum from Saint Louis University. Today the college, housed in O'Shaughnessy Hall, includes 20 departments in the areas of fine arts, humanities, and social sciences, and awards Bachelor of Arts (B.A.) degrees in 33 majors, making it the largest of the university's colleges. There are around 2,500 undergraduates and 750 graduates enrolled in the college.
The College of Science was established at the university in 1865 by president Father Patrick Dillon. Dillon's scientific courses were six years of work, including higher-level mathematics courses. Today the college, housed in the newly built Jordan Hall of Science, includes over 1,200 undergraduates in six departments of study – biology, chemistry, mathematics, physics, pre-professional studies, and applied and computational mathematics and statistics (ACMS) – each awarding Bachelor of Science (B.S.) degrees. According to university statistics, its science pre-professional program has one of the highest acceptance rates to medical school of any university in the United States.
The School of Architecture was established in 1899, although degrees in architecture were first awarded by the university in 1898. Today the school, housed in Bond Hall, offers a five-year undergraduate program leading to the Bachelor of Architecture degree. All undergraduate students study the third year of the program in Rome. The university is globally recognized for its Notre Dame School of Architecture, a faculty that teaches (pre-modernist) traditional and classical architecture and urban planning (e.g. following the principles of New Urbanism and New Classical Architecture). It also awards the renowned annual Driehaus Architecture Prize.
The library system also includes branch libraries for Architecture, Chemistry & Physics, Engineering, Law, and Mathematics as well as information centers in the Mendoza College of Business, the Kellogg Institute for International Studies, the Joan B. Kroc Institute for International Peace Studies, and a slide library in O'Shaughnessy Hall. A theology library was also opened in fall of 2015. Located on the first floor of Stanford Hall, it is the first branch of the library system to be housed in a dorm room. The library system holds over three million volumes, was the single largest university library in the world upon its completion, and remains one of the 100 largest libraries in the country.
The rise of Hitler and other dictators in the 1930s forced numerous Catholic intellectuals to flee Europe; president John O'Hara brought many to Notre Dame. From Germany came Anton-Hermann Chroust (1907–1982) in classics and law, and Waldemar Gurian a German Catholic intellectual of Jewish descent. Positivism dominated American intellectual life in the 1920s onward but in marked contrast, Gurian received a German Catholic education and wrote his doctoral dissertation under Max Scheler. Ivan Meštrović (1883–1962), a renowned sculptor, brought Croatian culture to campus, 1955–62. Yves Simon (1903–61), brought to ND in the 1940s the insights of French studies in the Aristotelian-Thomistic tradition of philosophy; his own teacher Jacques Maritain (1882–73) was a frequent visitor to campus.
The University of Notre Dame du Lac (or simply Notre Dame /ˌnoʊtərˈdeɪm/ NOH-tər-DAYM) is a Catholic research university located adjacent to South Bend, Indiana, in the United States. In French, Notre Dame du Lac means "Our Lady of the Lake" and refers to the university's patron saint, the Virgin Mary. The main campus covers 1,250 acres in a suburban setting and it contains a number of recognizable landmarks, such as the Golden Dome, the "Word of Life" mural (commonly known as Touchdown Jesus), and the Basilica.
Notre Dame rose to national prominence in the early 1900s for its Fighting Irish football team, especially under the guidance of the legendary coach Knute Rockne. The university's athletic teams are members of the NCAA Division I and are known collectively as the Fighting Irish. The football team, an Independent, has accumulated eleven consensus national championships, seven Heisman Trophy winners, 62 members in the College Football Hall of Fame and 13 members in the Pro Football Hall of Fame and is considered one of the most famed and successful college football teams in history. Other ND teams, chiefly in the Atlantic Coast Conference, have accumulated 16 national championships. The Notre Dame Victory March is often regarded as the most famous and recognizable collegiate fight song.
Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university. The undergraduate component of the university is organized into four colleges (Arts and Letters, Science, Engineering, Business) and the Architecture School. The latter is known for teaching New Classical Architecture and for awarding the globally renowned annual Driehaus Architecture Prize. Notre Dame's graduate program has more than 50 master's, doctoral and professional degree programs offered by the five schools, with the addition of the Notre Dame Law School and a MD-PhD program offered in combination with IU medical School. It maintains a system of libraries, cultural venues, artistic and scientific museums, including Hesburgh Library and the Snite Museum of Art. Over 80% of the university's 8,000 undergraduates live on campus in one of 29 single-sex residence halls, each with its own traditions, legacies, events and intramural sports teams. The university counts approximately 120,000 alumni, considered among the strongest alumni networks among U.S. colleges.
In 1842, the Bishop of Vincennes, Célestine Guynemer de la Hailandière, offered land to Father Edward Sorin of the Congregation of the Holy Cross, on the condition that he build a college in two years. Fr. Sorin arrived on the site with eight Holy Cross brothers from France and Ireland on November 26, 1842, and began the school using Father Stephen Badin's old log chapel. He soon erected additional buildings, including Old College, the first church, and the first main building. They immediately acquired two students and set about building additions to the campus.
The first degrees from the college were awarded in 1849. The university was expanded with new buildings to accommodate more students and faculty. With each new president, new academic programs were offered and new buildings built to accommodate them. The original Main Building built by Sorin just after he arrived was replaced by a larger "Main Building" in 1865, which housed the university's administration, classrooms, and dormitories. Beginning in 1873, a library collection was started by Father Lemonnier. By 1879 it had grown to ten thousand volumes that were housed in the Main Building.
The television station, NDtv, grew from one show in 2002 to a full 24-hour channel with original programming by September 2006. WSND-FM serves the student body and larger South Bend community at 88.9 FM, offering students a chance to become involved in bringing classical music, fine arts and educational programming, and alternative rock to the airwaves. Another radio station, WVFI, began as a partner of WSND-FM. More recently, however, WVFI has been airing independently and is streamed on the Internet.
The first phase of Eddy Street Commons, a $215 million development located adjacent to the University of Notre Dame campus and funded by the university, broke ground on June 3, 2008. The Eddy Street Commons drew union protests when workers hired by the City of South Bend to construct the public parking garage picketed the private work site after a contractor hired non-union workers. The developer, Kite Realty out of Indianapolis, has made agreements with major national chains rather than local businesses, a move that has led to criticism from alumni and students.
Notre Dame teams are known as the Fighting Irish. They compete as a member of the National Collegiate Athletic Association (NCAA) Division I, primarily competing in the Atlantic Coast Conference (ACC) for all sports since the 2013–14 school year. The Fighting Irish previously competed in the Horizon League from 1982-83 to 1985-86, and again from 1987-88 to 1994-95, and then in the Big East Conference through 2012–13. Men's sports include baseball, basketball, crew, cross country, fencing, football, golf, ice hockey, lacrosse, soccer, swimming & diving, tennis and track & field; while women's sports include basketball, cross country, fencing, golf, lacrosse, rowing, soccer, softball, swimming & diving, tennis, track & field and volleyball. The football team competes as an Football Bowl Subdivision (FBS) Independent since its inception in 1887. Both fencing teams compete in the Midwest Fencing Conference, and the men's ice hockey team competes in Hockey East.
Notre Dame's conference affiliations for all of its sports except football and fencing changed in July 2013 as a result of major conference realignment, and its fencing affiliation will change in July 2014. The Irish left the Big East for the ACC during a prolonged period of instability in the Big East; while they maintain their football independence, they have committed to play five games per season against ACC opponents. In ice hockey, the Irish were forced to find a new conference home after the Big Ten Conference's decision to add the sport in 2013–14 led to a cascade of conference moves that culminated in the dissolution of the school's former hockey home, the Central Collegiate Hockey Association, after the 2012–13 season. Notre Dame moved its hockey team to Hockey East. After Notre Dame joined the ACC, the conference announced it would add fencing as a sponsored sport beginning in the 2014–15 school year. There are many theories behind the adoption of the athletics moniker but it is known that the Fighting Irish name was used in the early 1920s with respect to the football team and was popularized by alumnus Francis Wallace in his New York Daily News columns. The official colors of Notre Dame are Navy Blue and Gold Rush which are worn in competition by its athletic teams. In addition, the color green is often worn because of the Fighting Irish nickname. The Notre Dame Leprechaun is the mascot of the athletic teams. Created by Theodore W. Drake in 1964, the leprechaun was first used on the football pocket schedule and later on the football program covers. The leprechaun was featured on the cover of Time in November 1964 and gained national exposure.
On July 1, 2014, the University of Notre Dame and Under Armour reached an agreement in which Under Armour will provide uniforms, apparel,equipment, and monetary compensation to Notre Dame for 10 years. This contract, worth almost $100 million, is the most lucrative in the history of the NCAA. The university marching band plays at home games for most of the sports. The band, which began in 1846 and has a claim as the oldest university band in continuous existence in the United States, was honored by the National Music Council as a "Landmark of American Music" during the United States Bicentennial. The band regularly plays the school's fight song the Notre Dame Victory March, which was named as the most played and most famous fight song by Northern Illinois Professor William Studwell. According to College Fight Songs: An Annotated Anthology published in 1998, the "Notre Dame Victory March" ranks as the greatest fight song of all time.
The Notre Dame football team has a long history, first beginning when the Michigan Wolverines football team brought football to Notre Dame in 1887 and played against a group of students. In the long history since then, 13 Fighting Irish teams have won consensus national championships (although the university only claims 11), along with another nine teams being named national champion by at least one source. Additionally, the program has the most members in the College Football Hall of Fame, is tied with Ohio State University with the most Heisman Trophies won, and have the highest winning percentage in NCAA history. With the long history, Notre Dame has accumulated many rivals, and its annual game against USC for the Jeweled Shillelagh has been named by some as one of the most important in college football and is often called the greatest intersectional rivalry in college football in the country.
George Gipp was the school's legendary football player during 1916–20. He played semiprofessional baseball and smoked, drank, and gambled when not playing sports. He was also humble, generous to the needy, and a man of integrity. It was in 1928 that famed coach Knute Rockne used his final conversation with the dying Gipp to inspire the Notre Dame team to beat the Army team and "win one for the Gipper." The 1940 film, Knute Rockne, All American, starred Pat O'Brien as Knute Rockne and Ronald Reagan as Gipp. Today the team competes in Notre Dame Stadium, an 80,795-seat stadium on campus. The current head coach is Brian Kelly, hired from the University of Cincinnati on December 11, 2009. Kelly's record in midway through his sixth season at Notre Dame is 52–21. In 2012, Kelly's Fighting Irish squad went undefeated and played in the BCS National Championship Game. Kelly succeeded Charlie Weis, who was fired in November 2009 after five seasons. Although Weis led his team to two Bowl Championship Series bowl games, his overall record was 35–27, mediocre by Notre Dame standards, and the 2007 team had the most losses in school history. The football team generates enough revenue to operate independently while $22.1 million is retained from the team's profits for academic use. Forbes named the team as the most valuable in college football, worth a total of $101 million in 2007.
Football gameday traditions During home games, activities occur all around campus and different dorms decorate their halls with a traditional item (e.g. Zahm House's two-story banner). Traditional activities begin at the stroke of midnight with the Drummers' Circle. This tradition involves the drum line of the Band of the Fighting Irish and ushers in the rest of the festivities that will continue the rest of the gameday Saturday. Later that day, the trumpet section will play the Notre Dame Victory March and the Notre Dame Alma Mater under the dome. The band entire will play a concert at the steps of Bond Hall, from where they will march into Notre Dame Stadium, leading fans and students alike across campus to the game.
The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.
The "Notre Dame Victory March" is the fight song for the University of Notre Dame. It was written by two brothers who were Notre Dame graduates. The Rev. Michael J. Shea, a 1904 graduate, wrote the music, and his brother, John F. Shea, who earned degrees in 1906 and 1908, wrote the original lyrics. The lyrics were revised in the 1920s; it first appeared under the copyright of the University of Notre Dame in 1928. The chorus is, "Cheer cheer for old Notre Dame, wake up the echos cheering her name. Send a volley cheer on high, shake down the thunder from the sky! What though the odds be great or small, old Notre Dame will win over all. While her loyal sons are marching, onward to victory!"
In the film Knute Rockne, All American, Knute Rockne (played by Pat O'Brien) delivers the famous "Win one for the Gipper" speech, at which point the background music swells with the "Notre Dame Victory March". George Gipp was played by Ronald Reagan, whose nickname "The Gipper" was derived from this role. This scene was parodied in the movie Airplane! with the same background music, only this time honoring George Zipp, one of Ted Striker's former comrades. The song also was prominent in the movie Rudy, with Sean Astin as Daniel "Rudy" Ruettiger, who harbored dreams of playing football at the University of Notre Dame despite significant obstacles.
Notre Dame alumni work in various fields. Alumni working in political fields include state governors, members of the United States Congress, and former United States Secretary of State Condoleezza Rice. A notable alumnus of the College of Science is Medicine Nobel Prize winner Eric F. Wieschaus. A number of university heads are alumni, including Notre Dame's current president, the Rev. John Jenkins. Additionally, many alumni are in the media, including talk show hosts Regis Philbin and Phil Donahue, and television and radio personalities such as Mike Golic and Hannah Storm. With the university having high profile sports teams itself, a number of alumni went on to become involved in athletics outside the university, including professional baseball, basketball, football, and ice hockey players, such as Joe Theismann, Joe Montana, Tim Brown, Ross Browner, Rocket Ismail, Ruth Riley, Jeff Samardzija, Jerome Bettis, Brett Lebda, Olympic gold medalist Mariel Zagunis, professional boxer Mike Lee, former football coaches such as Charlie Weis, Frank Leahy and Knute Rockne, and Basketball Hall of Famers Austin Carr and Adrian Dantley. Other notable alumni include prominent businessman Edward J. DeBartolo, Jr. and astronaut Jim Wetherbee.
It has been said that GE got into computer manufacturing because in the 1950s they were the largest user of computers outside the United States federal government, aside from being the first business in the world to own a computer. Its major appliance manufacturing plant "Appliance Park" was the first non-governmental site to host one. However, in 1970, GE sold its computer division to Honeywell, exiting the computer manufacturing industry, though it retained its timesharing operations for some years afterwards. GE was a major provider of computer timesharing services, through General Electric Information Services (GEIS, now GXS), offering online computing services that included GEnie.
During 1889, Thomas Edison had business interests in many electricity-related companies: Edison Lamp Company, a lamp manufacturer in East Newark, New Jersey; Edison Machine Works, a manufacturer of dynamos and large electric motors in Schenectady, New York; Bergmann & Company, a manufacturer of electric lighting fixtures, sockets, and other electric lighting devices; and Edison Electric Light Company, the patent-holding company and the financial arm backed by J.P. Morgan and the Vanderbilt family for Edison's lighting experiments. In 1889, Drexel, Morgan & Co., a company founded by J.P. Morgan and Anthony J. Drexel, financed Edison's research and helped merge those companies under one corporation to form Edison General Electric Company which was incorporated in New York on April 24, 1889. The new company also acquired Sprague Electric Railway & Motor Company in the same year.
Since over half of GE's revenue is derived from financial services, it is arguably a financial company with a manufacturing arm. It is also one of the largest lenders in countries other than the United States, such as Japan. Even though the first wave of conglomerates (such as ITT Corporation, Ling-Temco-Vought, Tenneco, etc.) fell by the wayside by the mid-1980s, in the late 1990s, another wave (consisting of Westinghouse, Tyco, and others) tried and failed to emulate GE's success.[citation needed]
The changes included a new corporate color palette, small modifications to the GE logo, a new customized font (GE Inspira) and a new slogan, "Imagination at work", composed by David Lucas, to replace the slogan "We Bring Good Things to Life" used since 1979. The standard requires many headlines to be lowercased and adds visual "white space" to documents and advertising. The changes were designed by Wolff Olins and are used on GE's marketing, literature and website. In 2014, a second typeface family was introduced: GE Sans and Serif by Bold Monday created under art direction by Wolff Olins.
GE has a history of some of its activities giving rise to large-scale air and water pollution. Based on year 2000 data, researchers at the Political Economy Research Institute listed the corporation as the fourth-largest corporate producer of air pollution in the United States, with more than 4.4 million pounds per year (2,000 tons) of toxic chemicals released into the air. GE has also been implicated in the creation of toxic waste. According to EPA documents, only the United States Government, Honeywell, and Chevron Corporation are responsible for producing more Superfund toxic waste sites.
At about the same time, Charles Coffin, leading the Thomson-Houston Electric Company, acquired a number of competitors and gained access to their key patents. General Electric was formed through the 1892 merger of Edison General Electric Company of Schenectady, New York, and Thomson-Houston Electric Company of Lynn, Massachusetts, with the support of Drexel, Morgan & Co. Both plants continue to operate under the GE banner to this day. The company was incorporated in New York, with the Schenectady plant used as headquarters for many years thereafter. Around the same time, General Electric's Canadian counterpart, Canadian General Electric, was formed.
From circa 1932 until 1977, General Electric polluted the Housatonic River with PCBs discharges from the General Electric plant at Pittsfield, Massachusetts. Aroclor 1254 and Aroclor 1260, made by Monsanto was the primary contaminant of the pollution. The highest concentrations of PCBs in the Housatonic River are found in Woods Pond in Lenox, Massachusetts, just south of Pittsfield, where they have been measured up to 110 mg/kg in the sediment. About 50% of all the PCBs currently in the river are estimated to be retained in the sediment behind Woods Pond dam. This is estimated to be about 11,000 pounds of PCBs. Former filled oxbows are also polluted. Waterfowl and fish who live in and around the river contain significant levels of PCBs and can present health risks if consumed.
GE (General Electric) Energy's renewable energy business has expanded greatly, to keep up with growing U.S. and global demand for clean energy. Since entering the renewable energy industry in 2002, GE has invested more than $850 million in renewable energy commercialization. In August 2008 it acquired Kelman Ltd, a Northern Ireland company specializing in advanced monitoring and diagnostics technologies for transformers used in renewable energy generation, and announced an expansion of its business in Northern Ireland in May 2010. In 2009, GE's renewable energy initiatives, which include solar power, wind power and GE Jenbacher gas engines using renewable and non-renewable methane-based gases, employ more than 4,900 people globally and have created more than 10,000 supporting jobs.
In May 2005, GE announced the launch of a program called "Ecomagination," intended, in the words of CEO Jeff Immelt "to develop tomorrow's solutions such as solar energy, hybrid locomotives, fuel cells, lower-emission aircraft engines, lighter and stronger durable materials, efficient lighting, and water purification technology". The announcement prompted an op-ed piece in The New York Times to observe that, "while General Electric's increased emphasis on clean technology will probably result in improved products and benefit its bottom line, Mr. Immelt's credibility as a spokesman on national environmental policy is fatally flawed because of his company's intransigence in cleaning up its own toxic legacy."
GE's history of working with turbines in the power-generation field gave them the engineering know-how to move into the new field of aircraft turbosuperchargers.[citation needed] Led by Sanford Alexander Moss, GE introduced the first superchargers during World War I, and continued to develop them during the Interwar period. Superchargers became indispensable in the years immediately prior to World War II, and GE was the world leader in exhaust-driven supercharging when the war started. This experience, in turn, made GE a natural selection to develop the Whittle W.1 jet engine that was demonstrated in the United States in 1941. GE ranked ninth among United States corporations in the value of wartime production contracts. Although their early work with Whittle's designs was later handed to Allison Engine Company, GE Aviation emerged as one of the world's largest engine manufacturers, second only to the British company, Rolls-Royce plc.
General Electric heavily contaminated the Hudson River with polychlorinated biphenyls (PCBs) between 1947-77. This pollution caused a range of harmful effects to wildlife and people who eat fish from the river or drink the water. In response to this contamination, activists protested in various ways. Musician Pete Seeger founded the Hudson River Sloop Clearwater and the Clearwater Festival to draw attention to the problem. The activism led to the site being designated by the EPA as one of the superfund sites requiring extensive cleanup. Other sources of pollution, including mercury contamination and sewage dumping, have also contributed to problems in the Hudson River watershed.
GE has said that it will invest $1.4 billion in clean technology research and development in 2008 as part of its Ecomagination initiative. As of October 2008, the scheme had resulted in 70 green products being brought to market, ranging from halogen lamps to biogas engines. In 2007, GE raised the annual revenue target for its Ecomagination initiative from $20 billion in 2010 to $25 billion following positive market response to its new product lines. In 2010, GE continued to raise its investment by adding $10 billion into Ecomagination over the next five years.
Short Films, Big Ideas was launched at the 2011 Toronto International Film Festival in partnership with cinelan. Stories included breakthroughs in Slingshot (water vapor distillation system), cancer research, energy production, pain management and food access. Each of the 30 films received world premiere screenings at a major international film festival, including the Sundance Film Festival and the Tribeca Film Festival. The winning amateur director film, The Cyborg Foundation, was awarded a US$100,000 prize at the 2013 at Sundance Film Festival.[citation needed] According to GE, the campaign garnered more than 1.5 billion total media impressions, 14 million online views, and was seen in 156 countries.[citation needed]
In April 2014, it was announced that GE was in talks to acquire the global power division of French engineering group Alstom for a figure of around $13 billion. A rival joint bid was submitted in June 2014 by Siemens and Mitsubishi Heavy Industries (MHI) with Siemens seeking to acquire Alstom's gas turbine business for €3.9 billion, and MHI proposing a joint venture in steam turbines, plus a €3.1 billion cash investment. In June 2014 a formal offer From GE worth $17 billion was agreed by the Alstom board. Part of the transaction involved the French government taking a 20% stake in Alstom to help secure France's energy and transport interests, and French jobs. A rival offer from Siemens-Mitsubishi Heavy Industries was rejected. The acquisition was expected to be completed in 2015.
GE is a multinational conglomerate headquartered in Fairfield, Connecticut. Its main offices are located at 30 Rockefeller Plaza at Rockefeller Center in New York City, known now as the Comcast Building. It was formerly known as the GE Building for the prominent GE logo on the roof; NBC's headquarters and main studios are also located in the building. Through its RCA subsidiary, it has been associated with the center since its construction in the 1930s. GE moved its corporate headquarters from the GE Building on Lexington Avenue to Fairfield in 1974.
In 1983, New York State Attorney General Robert Abrams filed suit in the United States District Court for the Northern District of New York to compel GE to pay for the cleanup of what was claimed to be more than 100,000 tons of chemicals dumped from their plant in Waterford, New York. In 1999, the company agreed to pay a $250 million settlement in connection with claims it polluted the Housatonic River (Pittsfield, Massachusetts) and other sites with polychlorinated biphenyls (PCBs) and other hazardous substances.
The ultimate substantive legacy of Principia Mathematica is mixed. It is generally accepted that Kurt Gödel's incompleteness theorem of 1931 definitively demonstrated that for any set of axioms and inference rules proposed to encapsulate mathematics, there would in fact be some truths of mathematics which could not be deduced from them, and hence that Principia Mathematica could never achieve its aims. However, Gödel could not have come to this conclusion without Whitehead and Russell's book. In this way, Principia Mathematica's legacy might be described as its key role in disproving the possibility of achieving its own stated goals. But beyond this somewhat ironic legacy, the book popularized modern mathematical logic and drew important connections between logic, epistemology, and metaphysics.
Whitehead's most complete work on education is the 1929 book The Aims of Education and Other Essays, which collected numerous essays and addresses by Whitehead on the subject published between 1912 and 1927. The essay from which Aims of Education derived its name was delivered as an address in 1916 when Whitehead was president of the London Branch of the Mathematical Association. In it, he cautioned against the teaching of what he called "inert ideas" – ideas that are disconnected scraps of information, with no application to real life or culture. He opined that "education with inert ideas is not only useless: it is, above all things, harmful."
Rather than teach small parts of a large number of subjects, Whitehead advocated teaching a relatively few important concepts that the student could organically link to many different areas of knowledge, discovering their application in actual life. For Whitehead, education should be the exact opposite of the multidisciplinary, value-free school model – it should be transdisciplinary, and laden with values and general principles that provide students with a bedrock of wisdom and help them to make connections between areas of knowledge that are usually regarded as separate.
Whitehead did not begin his career as a philosopher. In fact, he never had any formal training in philosophy beyond his undergraduate education. Early in his life he showed great interest in and respect for philosophy and metaphysics, but it is evident that he considered himself a rank amateur. In one letter to his friend and former student Bertrand Russell, after discussing whether science aimed to be explanatory or merely descriptive, he wrote: "This further question lands us in the ocean of metaphysic, onto which my profound ignorance of that science forbids me to enter." Ironically, in later life Whitehead would become one of the 20th century's foremost metaphysicians.
Whitehead was unimpressed by this objection. In the notes of one his students for a 1927 class, Whitehead was quoted as saying: "Every scientific man in order to preserve his reputation has to say he dislikes metaphysics. What he means is he dislikes having his metaphysics criticized." In Whitehead's view, scientists and philosophers make metaphysical assumptions about how the universe works all the time, but such assumptions are not easily seen precisely because they remain unexamined and unquestioned. While Whitehead acknowledged that "philosophers can never hope finally to formulate these metaphysical first principles," he argued that people need to continually re-imagine their basic assumptions about how the universe works if philosophy and science are to make any real progress, even if that progress remains permanently asymptotic. For this reason Whitehead regarded metaphysical investigations as essential to both good science and good philosophy.
Perhaps foremost among what Whitehead considered faulty metaphysical assumptions was the Cartesian idea that reality is fundamentally constructed of bits of matter that exist totally independently of one another, which he rejected in favor of an event-based or "process" ontology in which events are primary and are fundamentally interrelated and dependent on one another. He also argued that the most basic elements of reality can all be regarded as experiential, indeed that everything is constituted by its experience. He used the term "experience" very broadly, so that even inanimate processes such as electron collisions are said to manifest some degree of experience. In this, he went against Descartes' separation of two different kinds of real existence, either exclusively material or else exclusively mental. Whitehead referred to his metaphysical system as "philosophy of organism", but it would become known more widely as "process philosophy."
In Whitehead's view, then, concepts such as "quality", "matter", and "form" are problematic. These "classical" concepts fail to adequately account for change, and overlook the active and experiential nature of the most basic elements of the world. They are useful abstractions, but are not the world's basic building blocks. What is ordinarily conceived of as a single person, for instance, is philosophically described as a continuum of overlapping events. After all, people change all the time, if only because they have aged by another second and had some further experience. These occasions of experience are logically distinct, but are progressively connected in what Whitehead calls a "society" of events. By assuming that enduring objects are the most real and fundamental things in the universe, materialists have mistaken the abstract for the concrete (what Whitehead calls the "fallacy of misplaced concreteness").
To put it another way, a thing or person is often seen as having a "defining essence" or a "core identity" that is unchanging, and describes what the thing or person really is. In this way of thinking, things and people are seen as fundamentally the same through time, with any changes being qualitative and secondary to their core identity (e.g. "Mark's hair has turned gray as he has gotten older, but he is still the same person"). But in Whitehead's cosmology, the only fundamentally existent things are discrete "occasions of experience" that overlap one another in time and space, and jointly make up the enduring person or thing. On the other hand, what ordinary thinking often regards as "the essence of a thing" or "the identity/core of a person" is an abstract generalization of what is regarded as that person or thing's most important or salient features across time. Identities do not define people, people define identities. Everything changes from moment to moment, and to think of anything as having an "enduring essence" misses the fact that "all things flow", though it is often a useful way of speaking.
Whitehead pointed to the limitations of language as one of the main culprits in maintaining a materialistic way of thinking, and acknowledged that it may be difficult to ever wholly move past such ideas in everyday speech. After all, each moment of each person's life can hardly be given a different proper name, and it is easy and convenient to think of people and objects as remaining fundamentally the same things, rather than constantly keeping in mind that each thing is a different thing from what it was a moment ago. Yet the limitations of everyday living and everyday speech should not prevent people from realizing that "material substances" or "essences" are a convenient generalized description of a continuum of particular, concrete processes. No one questions that a ten-year-old person is quite different by the time he or she turns thirty years old, and in many ways is not the same person at all; Whitehead points out that it is not philosophically or ontologically sound to think that a person is the same from one second to the next.
A second problem with materialism is that it obscures the importance of relations. It sees every object as distinct and discrete from all other objects. Each object is simply an inert clump of matter that is only externally related to other things. The idea of matter as primary makes people think of objects as being fundamentally separate in time and space, and not necessarily related to anything. But in Whitehead's view, relations take a primary role, perhaps even more important than the relata themselves. A student taking notes in one of Whitehead's fall 1924 classes wrote that:
In fact, Whitehead describes any entity as in some sense nothing more and nothing less than the sum of its relations to other entities – its synthesis of and reaction to the world around it. A real thing is just that which forces the rest of the universe to in some way conform to it; that is to say, if theoretically a thing made strictly no difference to any other entity (i.e. it was not related to any other entity), it could not be said to really exist. Relations are not secondary to what a thing is, they are what the thing is.
Isabelle Stengers wrote that "Whiteheadians are recruited among both philosophers and theologians, and the palette has been enriched by practitioners from the most diverse horizons, from ecology to feminism, practices that unite political struggle and spirituality with the sciences of education." Indeed, in recent decades attention to Whitehead's work has become more widespread, with interest extending to intellectuals in Europe and China, and coming from such diverse fields as ecology, physics, biology, education, economics, and psychology. One of the first theologians to attempt to interact with Whitehead's thought was the future Archbishop of Canterbury, William Temple. In Temple's Gifford Lectures of 1932-1934 (subsequently published as "Nature, Man and God"), Whitehead is one of a number of philosophers of the emergent evolution approach Temple interacts with. However, it was not until the 1970s and 1980s that Whitehead's thought drew much attention outside of a small group of philosophers and theologians, primarily Americans, and even today he is not considered especially influential outside of relatively specialized circles.
Early followers of Whitehead were found primarily at the University of Chicago's Divinity School, where Henry Nelson Wieman initiated an interest in Whitehead's work that would last for about thirty years. Professors such as Wieman, Charles Hartshorne, Bernard Loomer, Bernard Meland, and Daniel Day Williams made Whitehead's philosophy arguably the most important intellectual thread running through the Divinity School. They taught generations of Whitehead scholars, the most notable of which is John B. Cobb, Jr.
But while Claremont remains the most concentrated hub of Whiteheadian activity, the place where Whitehead's thought currently seems to be growing the most quickly is in China. In order to address the challenges of modernization and industrialization, China has begun to blend traditions of Taoism, Buddhism, and Confucianism with Whitehead's "constructive post-modern" philosophy in order to create an "ecological civilization." To date, the Chinese government has encouraged the building of twenty-three university-based centers for the study of Whitehead's philosophy, and books by process philosophers John Cobb and David Ray Griffin are becoming required reading for Chinese graduate students. Cobb has attributed China's interest in process philosophy partly to Whitehead's stress on the mutual interdependence of humanity and nature, as well as his emphasis on an educational system that includes the teaching of values rather than simply bare facts.
Deleuze's and Latour's opinions, however, are minority ones, as Whitehead has not been recognized as particularly influential within the most dominant philosophical schools. It is impossible to say exactly why Whitehead's influence has not been more widespread, but it may be partly due to his metaphysical ideas seeming somewhat counter-intuitive (such as his assertion that matter is an abstraction), or his inclusion of theistic elements in his philosophy, or the perception of metaphysics itself as passé, or simply the sheer difficulty and density of his prose.
One philosophical school which has historically had a close relationship with process philosophy is American pragmatism. Whitehead himself thought highly of William James and John Dewey, and acknowledged his indebtedness to them in the preface to Process and Reality. Charles Hartshorne (along with Paul Weiss) edited the collected papers of Charles Sanders Peirce, one of the founders of pragmatism. Noted neopragmatist Richard Rorty was in turn a student of Hartshorne. Today, Nicholas Rescher is one example of a philosopher who advocates both process philosophy and pragmatism.
In physics, Whitehead's thought has had some influence. He articulated a view that might perhaps be regarded as dual to Einstein's general relativity, see Whitehead's theory of gravitation. It has been severely criticized. Yutaka Tanaka, who suggests that the gravitational constant disagrees with experimental findings, proposes that Einstein's work does not actually refute Whitehead's formulation. Whitehead's view has now been rendered obsolete, with the discovery of gravitational waves. They are phenonena observed locally that largely violate the kind of local flatness of space that Whitehead assumes. Consequently, Whitehead's cosmology must be regarded as a local approximation, and his assumption of a uniform spatio-temporal geometry, Minkowskian in particular, as an often-locally-adequate approximation. An exact replacement of Whitehead's cosmology would need to admit a Riemannian geometry. Also, although Whitehead himself gave only secondary consideration to quantum theory, his metaphysics of processes has proved attractive to some physicists in that field. Henry Stapp and David Bohm are among those whose work has been influenced by Whitehead.
This work has been pioneered by John B. Cobb, Jr., whose book Is It Too Late? A Theology of Ecology (1971) was the first single-authored book in environmental ethics. Cobb also co-authored a book with economist Herman Daly entitled For the Common Good: Redirecting the Economy toward Community, the Environment, and a Sustainable Future (1989), which applied Whitehead's thought to economics, and received the Grawemeyer Award for Ideas Improving World Order. Cobb followed this with a second book, Sustaining the Common Good: A Christian Perspective on the Global Economy (1994), which aimed to challenge "economists' zealous faith in the great god of growth."
Another model is the FEELS model developed by Xie Bangxiu and deployed successfully in China. "FEELS" stands for five things in curriculum and education: Flexible-goals, Engaged-learner, Embodied-knowledge, Learning-through-interactions, and Supportive-teacher. It is used for understanding and evaluating educational curriculum under the assumption that the purpose of education is to "help a person become whole." This work is in part the product of cooperation between Chinese government organizations and the Institute for the Postmodern Development of China.
Whitehead has had some influence on philosophy of business administration and organizational theory. This has led in part to a focus on identifying and investigating the effect of temporal events (as opposed to static things) within organizations through an “organization studies” discourse that accommodates a variety of 'weak' and 'strong' process perspectives from a number of philosophers. One of the leading figures having an explicitly Whiteheadian and panexperientialist stance towards management is Mark Dibben, who works in what he calls "applied process thought" to articulate a philosophy of management and business administration as part of a wider examination of the social sciences through the lens of process metaphysics. For Dibben, this allows "a comprehensive exploration of life as perpetually active experiencing, as opposed to occasional – and thoroughly passive – happening." Dibben has published two books on applied process thought, Applied Process Thought I: Initial Explorations in Theory and Research (2008), and Applied Process Thought II: Following a Trail Ablaze (2009), as well as other papers in this vein in the fields of philosophy of management and business ethics.
Beginning in the late 1910s and early 1920s, Whitehead gradually turned his attention from mathematics to philosophy of science, and finally to metaphysics. He developed a comprehensive metaphysical system which radically departed from most of western philosophy. Whitehead argued that reality consists of processes rather than material objects, and that processes are best defined by their relations with other processes, thus rejecting the theory that reality is fundamentally constructed by bits of matter that exist independently of one another. Today Whitehead's philosophical works – particularly Process and Reality – are regarded as the foundational texts of process philosophy.
Alfred North Whitehead was born in Ramsgate, Kent, England, in 1861. His father, Alfred Whitehead, was a minister and schoolmaster of Chatham House Academy, a successful school for boys established by Thomas Whitehead, Alfred North's grandfather. Whitehead himself recalled both of them as being very successful schoolmasters, but that his grandfather was the more extraordinary man. Whitehead's mother was Maria Sarah Whitehead, formerly Maria Sarah Buckmaster. Whitehead was apparently not particularly close with his mother, as he never mentioned her in any of his writings, and there is evidence that Whitehead's wife, Evelyn, had a low opinion of her.
In 1918 Whitehead's academic responsibilities began to seriously expand as he accepted a number of high administrative positions within the University of London system, of which Imperial College London was a member at the time. He was elected Dean of the Faculty of Science at the University of London in late 1918 (a post he held for four years), a member of the University of London's Senate in 1919, and chairman of the Senate's Academic (leadership) Council in 1920, a post which he held until he departed for America in 1924. Whitehead was able to exert his newfound influence to successfully lobby for a new history of science department, help establish a Bachelor of Science degree (previously only Bachelor of Arts degrees had been offered), and make the school more accessible to less wealthy students.
The two volume biography of Whitehead by Victor Lowe is the most definitive presentation of the life of Whitehead. However, many details of Whitehead's life remain obscure because he left no Nachlass; his family carried out his instructions that all of his papers be destroyed after his death. Additionally, Whitehead was known for his "almost fanatical belief in the right to privacy", and for writing very few personal letters of the kind that would help to gain insight on his life. This led to Lowe himself remarking on the first page of Whitehead's biography, "No professional biographer in his right mind would touch him."
In addition to numerous articles on mathematics, Whitehead wrote three major books on the subject: A Treatise on Universal Algebra (1898), Principia Mathematica (co-written with Bertrand Russell and published in three volumes between 1910 and 1913), and An Introduction to Mathematics (1911). The former two books were aimed exclusively at professional mathematicians, while the latter book was intended for a larger audience, covering the history of mathematics and its philosophical foundations. Principia Mathematica in particular is regarded as one of the most important works in mathematical logic of the 20th century.
At the time structures such as Lie algebras and hyperbolic quaternions drew attention to the need to expand algebraic structures beyond the associatively multiplicative class. In a review Alexander Macfarlane wrote: "The main idea of the work is not unification of the several methods, nor generalization of ordinary algebra so as to include them, but rather the comparative study of their several structures." In a separate review, G. B. Mathews wrote, "It possesses a unity of design which is really remarkable, considering the variety of its themes."
Whitehead and Russell had thought originally that Principia Mathematica would take a year to complete; it ended up taking them ten years. To add insult to injury, when it came time for publication, the three-volume work was so massive (more than 2,000 pages) and its audience so narrow (professional mathematicians) that it was initially published at a loss of 600 pounds, 300 of which was paid by Cambridge University Press, 200 by the Royal Society of London, and 50 apiece by Whitehead and Russell themselves. Despite the initial loss, today there is likely no major academic library in the world which does not hold a copy of Principia Mathematica.
This is not to say that Whitehead's thought was widely accepted or even well-understood. His philosophical work is generally considered to be among the most difficult to understand in all of the western canon. Even professional philosophers struggled to follow Whitehead's writings. One famous story illustrating the level of difficulty of Whitehead's philosophy centers around the delivery of Whitehead's Gifford lectures in 1927–28 – following Arthur Eddington's lectures of the year previous – which Whitehead would later publish as Process and Reality:
However, Mathews' frustration with Whitehead's books did not negatively affect his interest. In fact, there were numerous philosophers and theologians at Chicago's Divinity School that perceived the importance of what Whitehead was doing without fully grasping all of the details and implications. In 1927 they invited one of America's only Whitehead experts – Henry Nelson Wieman – to Chicago to give a lecture explaining Whitehead's thought. Wieman's lecture was so brilliant that he was promptly hired to the faculty and taught there for twenty years, and for at least thirty years afterward Chicago's Divinity School was closely associated with Whitehead's thought.
Wieman's words proved prophetic. Though Process and Reality has been called "arguably the most impressive single metaphysical text of the twentieth century," it has been little-read and little-understood, partly because it demands – as Isabelle Stengers puts it – "that its readers accept the adventure of the questions that will separate them from every consensus." Whitehead questioned western philosophy's most dearly held assumptions about how the universe works, but in doing so he managed to anticipate a number of 21st century scientific and philosophical problems and provide novel solutions.
It must be emphasized, however, that an entity is not merely a sum of its relations, but also a valuation of them and reaction to them. For Whitehead, creativity is the absolute principle of existence, and every entity (whether it is a human being, a tree, or an electron) has some degree of novelty in how it responds to other entities, and is not fully determined by causal or mechanistic laws. Of course, most entities do not have consciousness. As a human being's actions cannot always be predicted, the same can be said of where a tree's roots will grow, or how an electron will move, or whether it will rain tomorrow. Moreover, inability to predict an electron's movement (for instance) is not due to faulty understanding or inadequate technology; rather, the fundamental creativity/freedom of all entities means that there will always remain phenomena that are unpredictable.
Since Whitehead's metaphysics described a universe in which all entities experience, he needed a new way of describing perception that was not limited to living, self-conscious beings. The term he coined was "prehension", which comes from the Latin prehensio, meaning "to seize." The term is meant to indicate a kind of perception that can be conscious or unconscious, applying to people as well as electrons. It is also intended to make clear Whitehead's rejection of the theory of representative perception, in which the mind only has private ideas about other entities. For Whitehead, the term "prehension" indicates that the perceiver actually incorporates aspects of the perceived thing into itself. In this way, entities are constituted by their perceptions and relations, rather than being independent of them. Further, Whitehead regards perception as occurring in two modes, causal efficacy (or "physical prehension") and presentational immediacy (or "conceptual prehension").
Whitehead describes causal efficacy as "the experience dominating the primitive living organisms, which have a sense for the fate from which they have emerged, and the fate towards which they go." It is, in other words, the sense of causal relations between entities, a feeling of being influenced and affected by the surrounding environment, unmediated by the senses. Presentational immediacy, on the other hand, is what is usually referred to as "pure sense perception", unmediated by any causal or symbolic interpretation, even unconscious interpretation. In other words, it is pure appearance, which may or may not be delusive (e.g. mistaking an image in a mirror for "the real thing").
In higher organisms (like people), these two modes of perception combine into what Whitehead terms "symbolic reference", which links appearance with causation in a process that is so automatic that both people and animals have difficulty refraining from it. By way of illustration, Whitehead uses the example of a person's encounter with a chair. An ordinary person looks up, sees a colored shape, and immediately infers that it is a chair. However, an artist, Whitehead supposes, "might not have jumped to the notion of a chair", but instead "might have stopped at the mere contemplation of a beautiful color and a beautiful shape." This is not the normal human reaction; most people place objects in categories by habit and instinct, without even thinking about it. Moreover, animals do the same thing. Using the same example, Whitehead points out that a dog "would have acted immediately on the hypothesis of a chair and would have jumped onto it by way of using it as such." In this way symbolic reference is a fusion of pure sense perceptions on the one hand and causal relations on the other, and that it is in fact the causal relationships that dominate the more basic mentality (as the dog illustrates), while it is the sense perceptions which indicate a higher grade mentality (as the artist illustrates).
Whitehead makes the startling observation that "life is comparatively deficient in survival value." If humans can only exist for about a hundred years, and rocks for eight hundred million, then one is forced to ask why complex organisms ever evolved in the first place; as Whitehead humorously notes, "they certainly did not appear because they were better at that game than the rocks around them." He then observes that the mark of higher forms of life is that they are actively engaged in modifying their environment, an activity which he theorizes is directed toward the three-fold goal of living, living well, and living better. In other words, Whitehead sees life as directed toward the purpose of increasing its own satisfaction. Without such a goal, he sees the rise of life as totally unintelligible.
Whitehead's idea of God differs from traditional monotheistic notions. Perhaps his most famous and pointed criticism of the Christian conception of God is that "the Church gave unto God the attributes which belonged exclusively to Caesar." Here Whitehead is criticizing Christianity for defining God as primarily a divine king who imposes his will on the world, and whose most important attribute is power. As opposed to the most widely accepted forms of Christianity, Whitehead emphasized an idea of God that he called "the brief Galilean vision of humility":
It should be emphasized, however, that for Whitehead God is not necessarily tied to religion. Rather than springing primarily from religious faith, Whitehead saw God as necessary for his metaphysical system. His system required that an order exist among possibilities, an order that allowed for novelty in the world and provided an aim to all entities. Whitehead posited that these ordered potentials exist in what he called the primordial nature of God. However, Whitehead was also interested in religious experience. This led him to reflect more intensively on what he saw as the second nature of God, the consequent nature. Whitehead's conception of God as a "dipolar" entity has called for fresh theological thinking.
God's consequent nature, on the other hand, is anything but unchanging – it is God's reception of the world's activity. As Whitehead puts it, "[God] saves the world as it passes into the immediacy of his own life. It is the judgment of a tenderness which loses nothing that can be saved." In other words, God saves and cherishes all experiences forever, and those experiences go on to change the way God interacts with the world. In this way, God is really changed by what happens in the world and the wider universe, lending the actions of finite creatures an eternal significance.
Whitehead thus sees God and the world as fulfilling one another. He sees entities in the world as fluent and changing things that yearn for a permanence which only God can provide by taking them into God's self, thereafter changing God and affecting the rest of the universe throughout time. On the other hand, he sees God as permanent but as deficient in actuality and change: alone, God is merely eternally unrealized possibilities, and requires the world to actualize them. God gives creatures permanence, while the creatures give God actuality and change. Here it is worthwhile to quote Whitehead at length:
For Whitehead the core of religion was individual. While he acknowledged that individuals cannot ever be fully separated from their society, he argued that life is an internal fact for its own sake before it is an external fact relating to others. His most famous remark on religion is that "religion is what the individual does with his own solitariness ... and if you are never solitary, you are never religious." Whitehead saw religion as a system of general truths that transformed a person's character. He took special care to note that while religion is often a good influence, it is not necessarily good – an idea which he called a "dangerous delusion" (e.g., a religion might encourage the violent extermination of a rival religion's adherents).
However, while Whitehead saw religion as beginning in solitariness, he also saw religion as necessarily expanding beyond the individual. In keeping with his process metaphysics in which relations are primary, he wrote that religion necessitates the realization of "the value of the objective world which is a community derivative from the interrelations of its component individuals." In other words, the universe is a community which makes itself whole through the relatedness of each individual entity to all the others – meaning and value do not exist for the individual alone, but only in the context of the universal community. Whitehead writes further that each entity "can find no such value till it has merged its individual claim with that of the objective universe. Religion is world-loyalty. The spirit at once surrenders itself to this universal claim and appropriates it for itself." In this way the individual and universal/social aspects of religion are mutually dependent.
Overall, however, Whitehead's influence is very difficult to characterize. In English-speaking countries, his primary works are little-studied outside of Claremont and a select number of liberal graduate-level theology and philosophy programs. Outside of these circles his influence is relatively small and diffuse, and has tended to come chiefly through the work of his students and admirers rather than Whitehead himself. For instance, Whitehead was a teacher and long-time friend and collaborator of Bertrand Russell, and he also taught and supervised the dissertation of Willard Van Orman Quine, both of whom are important figures in analytic philosophy – the dominant strain of philosophy in English-speaking countries in the 20th century. Whitehead has also had high-profile admirers in the continental tradition, such as French post-structuralist philosopher Gilles Deleuze, who once dryly remarked of Whitehead that "he stands provisionally as the last great Anglo-American philosopher before Wittgenstein's disciples spread their misty confusion, sufficiency, and terror." French sociologist and anthropologist Bruno Latour even went so far as to call Whitehead "the greatest philosopher of the 20th century."
Historically Whitehead's work has been most influential in the field of American progressive theology. The most important early proponent of Whitehead's thought in a theological context was Charles Hartshorne, who spent a semester at Harvard as Whitehead's teaching assistant in 1925, and is widely credited with developing Whitehead's process philosophy into a full-blown process theology. Other notable process theologians include John B. Cobb, Jr., David Ray Griffin, Marjorie Hewitt Suchocki, C. Robert Mesle, Roland Faber, and Catherine Keller.
Process theology typically stresses God's relational nature. Rather than seeing God as impassive or emotionless, process theologians view God as "the fellow sufferer who understands", and as the being who is supremely affected by temporal events. Hartshorne points out that people would not praise a human ruler who was unaffected by either the joys or sorrows of his followers – so why would this be a praise-worthy quality in God? Instead, as the being who is most affected by the world, God is the being who can most appropriately respond to the world. However, process theology has been formulated in a wide variety of ways. C. Robert Mesle, for instance, advocates a "process naturalism", i.e. a process theology without God.
In fact, process theology is difficult to define because process theologians are so diverse and transdisciplinary in their views and interests. John B. Cobb, Jr. is a process theologian who has also written books on biology and economics. Roland Faber and Catherine Keller integrate Whitehead with poststructuralist, postcolonialist, and feminist theory. Charles Birch was both a theologian and a geneticist. Franklin I. Gamwell writes on theology and political theory. In Syntheism - Creating God in The Internet Age, futurologists Alexander Bard and Jan Söderqvist repeatedly credit Whitehead for the process theology they see rising out of the participatory culture expected to dominate the digital era.
Whitehead also described religion more technically as "an ultimate craving to infuse into the insistent particularity of emotion that non-temporal generality which primarily belongs to conceptual thought alone." In other words, religion takes deeply felt emotions and contextualizes them within a system of general truths about the world, helping people to identify their wider meaning and significance. For Whitehead, religion served as a kind of bridge between philosophy and the emotions and purposes of a particular society. It is the task of religion to make philosophy applicable to the everyday lives of ordinary people.
Margaret Stout and Carrie M. Staton have also written recently on the mutual influence of Whitehead and Mary Parker Follett, a pioneer in the fields of organizational theory and organizational behavior. Stout and Staton see both Whitehead and Follett as sharing an ontology that "understands becoming as a relational process; difference as being related, yet unique; and the purpose of becoming as harmonizing difference." This connection is further analyzed by Stout and Jeannine M. Love in Integrative Process: Follettian Thinking from Ontology to Administration 
Greek colonies and communities have been historically established on the shores of the Mediterranean Sea and Black Sea, but the Greek people have always been centered around the Aegean and Ionian seas, where the Greek language has been spoken since the Bronze Age. Until the early 20th century, Greeks were distributed between the Greek peninsula, the western coast of Asia Minor, the Black Sea coast, Cappadocia in central Anatolia, Egypt, the Balkans, Cyprus, and Constantinople. Many of these regions coincided to a large extent with the borders of the Byzantine Empire of the late 11th century and the Eastern Mediterranean areas of ancient Greek colonization. The cultural centers of the Greeks have included Athens, Thessalonica, Alexandria, Smyrna, and Constantinople at various periods.
The evolution of Proto-Greek should be considered within the context of an early Paleo-Balkan sprachbund that makes it difficult to delineate exact boundaries between individual languages. The characteristically Greek representation of word-initial laryngeals by prothetic vowels is shared, for one, by the Armenian language, which also seems to share some other phonological and morphological peculiarities of Greek; this has led some linguists to propose a hypothetical closer relationship between Greek and Armenian, although evidence remains scant.
Around 1200 BC, the Dorians, another Greek-speaking people, followed from Epirus. Traditionally, historians have believed that the Dorian invasion caused the collapse of the Mycenaean civilization, but it is likely the main attack was made by seafaring raiders (sea peoples) who sailed into the eastern Mediterranean around 1180 BC. The Dorian invasion was followed by a poorly attested period of migrations, appropriately called the Greek Dark Ages, but by 800 BC the landscape of Archaic and Classical Greece was discernible.
The Greeks of classical antiquity idealized their Mycenaean ancestors and the Mycenaean period as a glorious era of heroes, closeness of the gods and material wealth. The Homeric Epics (i.e. Iliad and Odyssey) were especially and generally accepted as part of the Greek past and it was not until the 19th century that scholars began to question Homer's historicity. As part of the Mycenaean heritage that survived, the names of the gods and goddesses of Mycenaean Greece (e.g. Zeus, Poseidon and Hades) became major figures of the Olympian Pantheon of later antiquity.
The ethnogenesis of the Greek nation is linked to the development of Pan-Hellenism in the 8th century BC. According to some scholars, the foundational event was the Olympic Games in 776 BC, when the idea of a common Hellenism among the Greek tribes was first translated into a shared cultural experience and Hellenism was primarily a matter of common culture. The works of Homer (i.e. Iliad and Odyssey) and Hesiod (i.e. Theogony) were written in the 8th century BC, becoming the basis of the national religion, ethos, history and mythology. The Oracle of Apollo at Delphi was established in this period.
The classical period of Greek civilization covers a time spanning from the early 5th century BC to the death of Alexander the Great, in 323 BC (some authors prefer to split this period into 'Classical', from the end of the Persian wars to the end of the Peloponnesian War, and 'Fourth Century', up to the death of Alexander). It is so named because it set the standards by which Greek civilization would be judged in later eras. The Classical period is also described as the "Golden Age" of Greek civilization, and its art, philosophy, architecture and literature would be instrumental in the formation and development of Western culture.
In any case, Alexander's toppling of the Achaemenid Empire, after his victories at the battles of the Granicus, Issus and Gaugamela, and his advance as far as modern-day Pakistan and Tajikistan, provided an important outlet for Greek culture, via the creation of colonies and trade routes along the way. While the Alexandrian empire did not survive its creator's death intact, the cultural implications of the spread of Hellenism across much of the Middle East and Asia were to prove long lived as Greek became the lingua franca, a position it retained even in Roman times. Many Greeks settled in Hellenistic cities like Alexandria, Antioch and Seleucia. Two thousand years later, there are still communities in Pakistan and Afghanistan, like the Kalash, who claim to be descended from Greek settlers.
This age saw the Greeks move towards larger cities and a reduction in the importance of the city-state. These larger cities were parts of the still larger Kingdoms of the Diadochi. Greeks, however, remained aware of their past, chiefly through the study of the works of Homer and the classical authors. An important factor in maintaining Greek identity was contact with barbarian (non-Greek) peoples, which was deepened in the new cosmopolitan environment of the multi-ethnic Hellenistic kingdoms. This led to a strong desire among Greeks to organize the transmission of the Hellenic paideia to the next generation. Greek science, technology and mathematics are generally considered to have reached their peak during the Hellenistic period.
In the religious sphere, this was a period of profound change. The spiritual revolution that took place, saw a waning of the old Greek religion, whose decline beginning in the 3rd century BC continued with the introduction of new religious movements from the East. The cults of deities like Isis and Mithra were introduced into the Greek world. Greek-speaking communities of the Hellenized East were instrumental in the spread of early Christianity in the 2nd and 3rd centuries, and Christianity's early leaders and writers (notably St Paul) were generally Greek-speaking, though none were from Greece. However, Greece itself had a tendency to cling to paganism and was not one of the influential centers of early Christianity: in fact, some ancient Greek religious practices remained in vogue until the end of the 4th century, with some areas such as the southeastern Peloponnese remaining pagan until well into the 10th century AD.
Of the new eastern religions introduced into the Greek world, the most successful was Christianity. From the early centuries of the Common Era, the Greeks identified as Romaioi ("Romans"), by that time the name ‘Hellenes’ denoted pagans. While ethnic distinctions still existed in the Roman Empire, they became secondary to religious considerations and the renewed empire used Christianity as a tool to support its cohesion and promoted a robust Roman national identity. Concurrently the secular, urban civilization of late antiquity survived in the Eastern Mediterranean along with Greco-Roman educational system, although it was from Christianity that the culture's essential values were drawn.
The Eastern Roman Empire – today conventionally named the Byzantine Empire, a name not in use during its own time – became increasingly influenced by Greek culture after the 7th century, when Emperor Heraclius (AD 575 - 641) decided to make Greek the empire's official language. Certainly from then on, but likely earlier, the Roman and Greek cultures were virtually fused into a single Greco-Roman world. Although the Latin West recognized the Eastern Empire's claim to the Roman legacy for several centuries, after Pope Leo III crowned Charlemagne, king of the Franks, as the "Roman Emperor" on 25 December 800, an act which eventually led to the formation of the Holy Roman Empire, the Latin West started to favour the Franks and began to refer to the Eastern Roman Empire largely as the Empire of the Greeks (Imperium Graecorum).
A distinct Greek political identity re-emerged in the 11th century in educated circles and became more forceful after the fall of Constantinople to the Crusaders of the Fourth Crusade in 1204, so that when the empire was revived in 1261, it became in many ways a Greek national state. That new notion of nationhood engendered a deep interest in the classical past culminating in the ideas of the Neoplatonist philosopher Gemistus Pletho, who abandoned Christianity. However, it was the combination of Orthodox Christianity with a specifically Greek identity that shaped the Greeks' notion of themselves in the empire's twilight years. The interest in the Classical Greek heritage was complemented by a renewed emphasis on Greek Orthodox identity, which was reinforced in the late Medieval and Ottoman Greeks' links with their fellow Orthodox Christians in the Russian Empire. These were further strengthened following the fall of the Empire of Trebizond in 1461, after which and until the second Russo-Turkish War of 1828-29 hundreds of thousands of Pontic Greeks fled or migrated from the Pontic Alps and Armenian Highlands to southern Russia and the Russian South Caucasus (see also Greeks in Russia, Greeks in Armenia, Greeks in Georgia, and Caucasian Greeks).
Following the Fall of Constantinople on 29 May 1453, many Greeks sought better employment and education opportunities by leaving for the West, particularly Italy, Central Europe, Germany and Russia. Greeks are greatly credited for the European cultural revolution, later called, the Renaissance. In Greek-inhabited territory itself, Greeks came to play a leading role in the Ottoman Empire, due in part to the fact that the central hub of the empire, politically, culturally, and socially, was based on Western Thrace and Greek Macedonia, both in Northern Greece, and of course was centred on the mainly Greek-populated, former Byzantine capital, Constantinople. As a direct consequence of this situation, Greek-speakers came to play a hugely important role in the Ottoman trading and diplomatic establishment, as well as in the church. Added to this, in the first half of the Ottoman period men of Greek origin made up a significant proportion of the Ottoman army, navy, and state bureaucracy, having been levied as adolescents (along with especially Albanians and Serbs) into Ottoman service through the devshirme. Many Ottomans of Greek (or Albanian or Serb) origin were therefore to be found within the Ottoman forces which governed the provinces, from Ottoman Egypt, to Ottomans occupied Yemen and Algeria, frequently as provincial governors.
For those that remained under the Ottoman Empire's millet system, religion was the defining characteristic of national groups (milletler), so the exonym "Greeks" (Rumlar from the name Rhomaioi) was applied by the Ottomans to all members of the Orthodox Church, regardless of their language or ethnic origin. The Greek speakers were the only ethnic group to actually call themselves Romioi, (as opposed to being so named by others) and, at least those educated, considered their ethnicity (genos) to be Hellenic. There were, however, many Greeks who escaped the second-class status of Christians inherent in the Ottoman millet system, according to which Muslims were explicitly awarded senior status and preferential treatment. These Greeks either emigrated, particularly to their fellow Greek Orthodox protector, the Russian Empire, or simply converted to Islam, often only very superficially and whilst remaining crypto-Christian. The most notable examples of large-scale conversion to Turkish Islam among those today defined as Greek Muslims - excluding those who had to convert as a matter of course on being recruited through the devshirme - were to be found in Crete (Cretan Turks), Greek Macedonia (for example among the Vallahades of western Macedonia), and among Pontic Greeks in the Pontic Alps and Armenian Highlands. Several Ottoman sultans and princes were also of part Greek origin, with mothers who were either Greek concubines or princesses from Byzantine noble families, one famous example being sultan Selim the Grim, whose mother Gülbahar Hatun was a Pontic Greek.
The roots of Greek success in the Ottoman Empire can be traced to the Greek tradition of education and commerce. It was the wealth of the extensive merchant class that provided the material basis for the intellectual revival that was the prominent feature of Greek life in the half century and more leading to the outbreak of the Greek War of Independence in 1821. Not coincidentally, on the eve of 1821, the three most important centres of Greek learning were situated in Chios, Smyrna and Aivali, all three major centres of Greek commerce. Greek success was also favoured by Greek domination of the Christian Orthodox church.
The relationship between ethnic Greek identity and Greek Orthodox religion continued after the creation of the Modern Greek state in 1830. According to the second article of the first Greek constitution of 1822, a Greek was defined as any Christian resident of the Kingdom of Greece, a clause removed by 1840. A century later, when the Treaty of Lausanne was signed between Greece and Turkey in 1923, the two countries agreed to use religion as the determinant for ethnic identity for the purposes of population exchange, although most of the Greeks displaced (over a million of the total 1.5 million) had already been driven out by the time the agreement was signed.[note 1] The Greek genocide, in particular the harsh removal of Pontian Greeks from the southern shore area of the Black Sea, contemporaneous with and following the failed Greek Asia Minor Campaign, was part of this process of Turkification of the Ottoman Empire and the placement of its economy and trade, then largely in Greek hands under ethnic Turkish control.
The terms used to define Greekness have varied throughout history but were never limited or completely identified with membership to a Greek state. By Western standards, the term Greeks has traditionally referred to any native speakers of the Greek language, whether Mycenaean, Byzantine or modern Greek. Byzantine Greeks called themselves Romioi and considered themselves the political heirs of Rome, but at least by the 12th century a growing number of those educated, deemed themselves the heirs of ancient Greece as well, although for most of the Greek speakers, "Hellene" still meant pagan. On the eve of the Fall of Constantinople the Last Emperor urged his soldiers to remember that they were the descendants of Greeks and Romans.
Before the establishment of the Modern Greek state, the link between ancient and modern Greeks was emphasized by the scholars of Greek Enlightenment especially by Rigas Feraios. In his "Political Constitution", he addresses to the nation as "the people descendant of the Greeks". The modern Greek state was created in 1829, when the Greeks liberated a part of their historic homelands, Peloponnese, from the Ottoman Empire. The large Greek diaspora and merchant class were instrumental in transmitting the ideas of western romantic nationalism and philhellenism, which together with the conception of Hellenism, formulated during the last centuries of the Byzantine Empire, formed the basis of the Diafotismos and the current conception of Hellenism.
Homer refers to the "Hellenes" (/ˈhɛliːnz/) as a relatively small tribe settled in Thessalic Phthia, with its warriors under the command of Achilleus. The Parian Chronicle says that Phthia was the homeland of the Hellenes and that this name was given to those previously called Greeks (Γραικοί). In Greek mythology, Hellen, the patriarch of Hellenes, was son of Pyrrha and Deucalion, who ruled around Phthia, the only survivors after the great deluge. It seems that the myth was invented when the Greek tribes started to separate from each other in certain areas of Greece and it indicates their common origin. Aristotle names ancient Hellas as an area in Epirus between Dodona and the Achelous river, the location of the great deluge of Deucalion, a land occupied by the Selloi and the "Greeks" who later came to be known as "Hellenes". Selloi were the priests of Dodonian Zeus and the word probably means "sacrificers" (compare Gothic saljan, "present, sacrifice"). There is currently no satisfactory etymology of the name Hellenes. Some scholars assert that the name Selloi changed to Sellanes and then to Hellanes-Hellenes. However this etymology connects the name Hellenes with the Dorians who occupied Epirus and the relation with the name Greeks given by the Romans becomes uncertain. The name Hellenes seems to be older and it was probably used by the Greeks with the establishment of the Great Amphictyonic League. This was an ancient association of Greek tribes with twelve founders which was organized to protect the great temples of Apollo in Delphi (Phocis) and of Demeter near Thermopylae (Locris). According to the legend it was founded after the Trojan War by the eponymous Amphictyon, brother of Hellen.
In the Hesiodic Catalogue of Women, Graecus is presented as the son of Zeus and Pandora II, sister of Hellen the patriarch of Hellenes. Hellen was the son of Deucalion who ruled around Phthia in central Greece. The Parian Chronicle mentions that when Deucalion became king of Phthia, the previously called Graikoi were named Hellenes. Aristotle notes that the Hellenes were related with Grai/Greeks (Meteorologica I.xiv) a native name of a Dorian tribe in Epirus which was used by the Illyrians. He also claims that the great deluge must have occurred in the region around Dodona, where the Selloi dwelt. However, according to the Greek tradition it is more possible that the homeland of the Greeks was originally in central Greece. A modern theory derives the name Greek (Latin Graeci) from Graikos, "inhabitant of Graia/Graea," a town on the coast of Boeotia. Greek colonists from Graia helped to found Cumae (900 BC) in Italy, where they were called Graeces. When the Romans encountered them they used this name for the colonists and then for all Greeks (Graeci.) The word γραῖα graia "old woman" comes from the PIE root *ǵerh2-/*ǵreh2-, "to grow old" via Proto-Greek *gera-/grau-iu; the same root later gave γέρας geras (/keras/), "gift of honour" in Mycenean Greek. The Germanic languages borrowed the word Greeks with an initial "k" sound which probably was their initial sound closest to the Latin "g" at the time (Goth. Kreks). The area out of ancient Attica including Boeotia was called Graïke and is connected with the older deluge of Ogyges, the mythological ruler of Boeotia. The region was originally occupied by the Minyans who were autochthonous or Proto-Greek speaking people. In ancient Greek the name Ogygios came to mean "from earliest days".
Homer uses the terms Achaeans and Danaans (Δαναοί) as a generic term for Greeks in Iliad, and they were probably a part of the Mycenean civilization. The names Achaioi and Danaoi seem to be pre-Dorian belonging to the people who were overthrown. They were forced to the region that later bore the name Achaea after the Dorian invasion. In the 5th century BC, they were redefined as contemporary speakers of Aeolic Greek which was spoken mainly in Thessaly, Boeotia and Lesbos. There are many controversial theories on the origin of the Achaeans. According to one view, the Achaeans were one of the fair-headed tribes of upper Europe, who pressed down over the Alps during the early Iron age (1300 BC) to southern Europe. Another theory suggests that the Peloponnesian Dorians were the Achaeans. These theories are rejected by other scholars who, based on linguistic criteria, suggest that the Achaeans were mainland pre-Dorian Greeks. There is also the theory that there was an Achaean ethnos that migrated from Asia minor to lower Thessaly prior to 2000 BC. Some Hittite texts mention a nation lying to the west called Ahhiyava or Ahhiya. Egyptian documents refer to Ekwesh, one of the groups of sea peoples who attached Egypt during the reign of Merneptah (1213-1203 BCE), who may have been Achaeans.
In Homer's Iliad, the names Danaans (or Danaoi: Δαναοί) and Argives (Argives: Αργείοι) are used to designate the Greek forces opposed to the Trojans. The myth of Danaus, whose origin is Egypt, is a foundation legend of Argos. His daughters Danaides, were forced in Tartarus to carry a jug to fill a bathtub without a bottom. This myth is connected with a task that can never be fulfilled (Sisyphos) and the name can be derived from the PIE root *danu: "river". There is not any satisfactory theory on their origin. Some scholars connect Danaans with the Denyen, one of the groups of the sea peoples who attacked Egypt during the reign of Ramesses III (1187-1156 BCE). The same inscription mentions the Weshesh who might have been the Achaeans. The Denyen seem to have been inhabitants of the city Adana in Cilicia. Pottery similar to that of Mycenae itself has been found in Tarsus of Cilicia and it seems that some refugees from the Aegean went there after the collapse of the Mycenean civilization. These Cilicians seem to have been called Dananiyim, the same word as Danaoi who attacked Egypt in 1191 BC along with the Quaouash (or Weshesh) who may be Achaeans. They were also called Danuna according to a Hittite inscription and the same name is mentioned in the Amarna letters. Julius Pokorny reconstructs the name from the PIE root da:-: "flow, river", da:-nu: "any moving liquid, drops", da: navo "people living by the river, Skyth. nomadic people (in Rigveda water-demons, fem.Da:nu primordial goddess), in Greek Danaoi, Egypt. Danuna". It is also possible that the name Danaans is pre-Greek. A country Danaja with a city Mukana (propaply: Mycenea) is mentioned in inscriptions from Egypt from Amenophis III (1390-1352 BC), Thutmosis III (1437 BC).
The most obvious link between modern and ancient Greeks is their language, which has a documented tradition from at least the 14th century BC to the present day, albeit with a break during the Greek Dark Ages (lasting from the 11th to the 8th century BC). Scholars compare its continuity of tradition to Chinese alone. Since its inception, Hellenism was primarily a matter of common culture and the national continuity of the Greek world is a lot more certain than its demographic. Yet, Hellenism also embodied an ancestral dimension through aspects of Athenian literature that developed and influenced ideas of descent based on autochthony. During the later years of the Eastern Roman Empire, areas such as Ionia and Constantinople experienced a Hellenic revival in language, philosophy, and literature and on classical models of thought and scholarship. This revival provided a powerful impetus to the sense of cultural affinity with ancient Greece and its classical heritage. The cultural changes undergone by the Greeks are, despite a surviving common sense of ethnicity, undeniable. At the same time, the Greeks have retained their language and alphabet, certain values and cultural traditions, customs, a sense of religious and cultural difference and exclusion, (the word barbarian was used by 12th-century historian Anna Komnene to describe non-Greek speakers), a sense of Greek identity and common sense of ethnicity despite the global political and social changes of the past two millennia.
Today, Greeks are the majority ethnic group in the Hellenic Republic, where they constitute 93% of the country's population, and the Republic of Cyprus where they make up 78% of the island's population (excluding Turkish settlers in the occupied part of the country). Greek populations have not traditionally exhibited high rates of growth; nonetheless, the population of Greece has shown regular increase since the country's first census in 1828. A large percentage of the population growth since the state's foundation has resulted from annexation of new territories and the influx of 1.5 million Greek refugees after the 1923 population exchange between Greece and Turkey. About 80% of the population of Greece is urban, with 28% concentrated in the city of Athens
Greeks from Cyprus have a similar history of emigration, usually to the English-speaking world because of the island's colonization by the British Empire. Waves of emigration followed the Turkish invasion of Cyprus in 1974, while the population decreased between mid-1974 and 1977 as a result of emigration, war losses, and a temporary decline in fertility. After the ethnic cleansing of a third of the Greek population of the island in 1974, there was also an increase in the number of Greek Cypriots leaving, especially for the Middle East, which contributed to a decrease in population that tapered off in the 1990s. Today more than two-thirds of the Greek population in Cyprus is urban.
There is a sizeable Greek minority of about 105,000 (disputed, sources claim higher) people, in Albania. The Greek minority of Turkey, which numbered upwards of 200,000 people after the 1923 exchange, has now dwindled to a few thousand, after the 1955 Constantinople Pogrom and other state sponsored violence and discrimination. This effectively ended, though not entirely, the three-thousand-year-old presence of Hellenism in Asia Minor. There are smaller Greek minorities in the rest of the Balkan countries, the Levant and the Black Sea states, remnants of the Old Greek Diaspora (pre-19th century).
The total number of Greeks living outside Greece and Cyprus today is a contentious issue. Where Census figures are available, they show around 3 million Greeks outside Greece and Cyprus. Estimates provided by the SAE - World Council of Hellenes Abroad put the figure at around 7 million worldwide. According to George Prevelakis of Sorbonne University, the number is closer to just below 5 million. Integration, intermarriage, and loss of the Greek language influence the self-identification of the Omogeneia. Important centres of the New Greek Diaspora today are London, New York, Melbourne and Toronto. In 2010, the Hellenic Parliament introduced a law that enables Diaspora Greeks in Greece to vote in the elections of the Greek state. This law was later repealed in early 2014.
In ancient times, the trading and colonizing activities of the Greek tribes and city states spread the Greek culture, religion and language around the Mediterranean and Black Sea basins, especially in Sicily and southern Italy (also known as Magna Grecia), Spain, the south of France and the Black sea coasts. Under Alexander the Great's empire and successor states, Greek and Hellenizing ruling classes were established in the Middle East, India and in Egypt. The Hellenistic period is characterized by a new wave of Greek colonization that established Greek cities and kingdoms in Asia and Africa. Under the Roman Empire, easier movement of people spread Greeks across the Empire and in the eastern territories, Greek became the lingua franca rather than Latin. The modern-day Griko community of southern Italy, numbering about 60,000, may represent a living remnant of the ancient Greek populations of Italy.
During and after the Greek War of Independence, Greeks of the diaspora were important in establishing the fledgling state, raising funds and awareness abroad. Greek merchant families already had contacts in other countries and during the disturbances many set up home around the Mediterranean (notably Marseilles in France, Livorno in Italy, Alexandria in Egypt), Russia (Odessa and Saint Petersburg), and Britain (London and Liverpool) from where they traded, typically in textiles and grain. Businesses frequently comprised the extended family, and with them they brought schools teaching Greek and the Greek Orthodox Church.
Greek culture has evolved over thousands of years, with its beginning in the Mycenaean civilization, continuing through the Classical period, the Roman and Eastern Roman periods and was profoundly affected by Christianity, which it in turn influenced and shaped. Ottoman Greeks had to endure through several centuries of adversity that culminated in genocide in the 20th century but nevertheless included cultural exchanges and enriched both cultures. The Diafotismos is credited with revitalizing Greek culture and giving birth to the synthesis of ancient and medieval elements that characterize it today.
Greek demonstrates several linguistic features that are shared with other Balkan languages, such as Albanian, Bulgarian and Eastern Romance languages (see Balkan sprachbund), and has absorbed many foreign words, primarily of Western European and Turkish origin. Because of the movements of Philhellenism and the Diafotismos in the 19th century, which emphasized the modern Greeks' ancient heritage, these foreign influences were excluded from official use via the creation of Katharevousa, a somewhat artificial form of Greek purged of all foreign influence and words, as the official language of the Greek state. In 1976, however, the Hellenic Parliament voted to make the spoken Dimotiki the official language, making Katharevousa obsolete.
Modern Greek has, in addition to Standard Modern Greek or Dimotiki, a wide variety of dialects of varying levels of mutual intelligibility, including Cypriot, Pontic, Cappadocian, Griko and Tsakonian (the only surviving representative of ancient Doric Greek). Yevanic is the language of the Romaniotes, and survives in small communities in Greece, New York and Israel. In addition to Greek, many Greeks in Greece and the Diaspora are bilingual in other languages or dialects such as English, Arvanitika/Albanian, Aromanian, Macedonian Slavic, Russian and Turkish.
Most Greeks are Christians, belonging to the Greek Orthodox Church. During the first centuries after Jesus Christ, the New Testament was originally written in Koine Greek, which remains the liturgical language of the Greek Orthodox Church, and most of the early Christians and Church Fathers were Greek-speaking. There are small groups of ethnic Greeks adhering to other Christian denominations like Greek Catholics, Greek Evangelicals, Pentecostals, and groups adhering to other religions including Romaniot and Sephardic Jews and Greek Muslims. About 2,000 Greeks are members of Hellenic Polytheistic Reconstructionism congregations.
Greek art has a long and varied history. Greeks have contributed to the visual, literary and performing arts. In the West, ancient Greek art was influential in shaping the Roman and later the modern western artistic heritage. Following the Renaissance in Europe, the humanist aesthetic and the high technical standards of Greek art inspired generations of European artists. Well into the 19th century, the classical tradition derived from Greece played an important role in the art of the western world. In the East, Alexander the Great's conquests initiated several centuries of exchange between Greek, Central Asian and Indian cultures, resulting in Greco-Buddhist art, whose influence reached as far as Japan.
Notable modern Greek artists include Renaissance painter Dominikos Theotokopoulos (El Greco), Panagiotis Doxaras, Nikolaos Gyzis, Nikiphoros Lytras, Yannis Tsarouchis, Nikos Engonopoulos, Constantine Andreou, Jannis Kounellis, sculptors such as Leonidas Drosis, Georgios Bonanos, Yannoulis Chalepas and Joannis Avramidis, conductor Dimitri Mitropoulos, soprano Maria Callas, composers such as Mikis Theodorakis, Nikos Skalkottas, Iannis Xenakis, Manos Hatzidakis, Eleni Karaindrou, Yanni and Vangelis, one of the best-selling singers worldwide Nana Mouskouri and poets such as Kostis Palamas, Dionysios Solomos, Angelos Sikelianos and Yannis Ritsos. Alexandrian Constantine P. Cavafy and Nobel laureates Giorgos Seferis and Odysseas Elytis are among the most important poets of the 20th century. Novel is also represented by Alexandros Papadiamantis and Nikos Kazantzakis.
The Greeks of the Classical era made several notable contributions to science and helped lay the foundations of several western scientific traditions, like philosophy, historiography and mathematics. The scholarly tradition of the Greek academies was maintained during Roman times with several academic institutions in Constantinople, Antioch, Alexandria and other centres of Greek learning while Eastern Roman science was essentially a continuation of classical science. Greeks have a long tradition of valuing and investing in paideia (education). Paideia was one of the highest societal values in the Greek and Hellenistic world while the first European institution described as a university was founded in 5th century Constantinople and operated in various incarnations until the city's fall to the Ottomans in 1453. The University of Constantinople was Christian Europe's first secular institution of higher learning since no theological subjects were taught, and considering the original meaning of the world university as a corporation of students, the world’s first university as well.
As of 2007, Greece had the eighth highest percentage of tertiary enrollment in the world (with the percentages for female students being higher than for male) while Greeks of the Diaspora are equally active in the field of education. Hundreds of thousands of Greek students attend western universities every year while the faculty lists of leading Western universities contain a striking number of Greek names. Notable modern Greek scientists of modern times include Dimitrios Galanos, Georgios Papanikolaou (inventor of the Pap test), Nicholas Negroponte, Constantin Carathéodory, Manolis Andronikos, Michael Dertouzos, John Argyris, Panagiotis Kondylis, John Iliopoulos (2007 Dirac Prize for his contributions on the physics of the charm quark, a major contribution to the birth of the Standard Model, the modern theory of Elementary Particles), Joseph Sifakis (2007 Turing Award, the "Nobel Prize" of Computer Science), Christos Papadimitriou (2002 Knuth Prize, 2012 Gödel Prize), Mihalis Yannakakis (2005 Knuth Prize) and Dimitri Nanopoulos.
The most widely used symbol is the flag of Greece, which features nine equal horizontal stripes of blue alternating with white representing the nine syllables of the Greek national motto Eleftheria i thanatos (freedom or death), which was the motto of the Greek War of Independence. The blue square in the upper hoist-side corner bears a white cross, which represents Greek Orthodoxy. The Greek flag is widely used by the Greek Cypriots, although Cyprus has officially adopted a neutral flag to ease ethnic tensions with the Turkish Cypriot minority – see flag of Cyprus).
Greek surnames were widely in use by the 9th century supplanting the ancient tradition of using the father’s name, however Greek surnames are most commonly patronymics. Commonly, Greek male surnames end in -s, which is the common ending for Greek masculine proper nouns in the nominative case. Exceptionally, some end in -ou, indicating the genitive case of this proper noun for patronymic reasons. Although surnames in mainland Greece are static today, dynamic and changing patronymic usage survives in middle names where the genitive of father's first name is commonly the middle name (this usage having been passed on to the Russians). In Cyprus, by contrast, surnames follow the ancient tradition of being given according to the father’s name. Finally, in addition to Greek-derived surnames many have Latin, Turkish and Italian origin.
The traditional Greek homelands have been the Greek peninsula and the Aegean Sea, the Southern Italy (Magna Graecia), the Black Sea, the Ionian coasts of Asia Minor and the islands of Cyprus and Sicily. In Plato's Phaidon, Socrates remarks, "we (Greeks) live around a sea like frogs around a pond" when describing to his friends the Greek cities of the Aegean. This image is attested by the map of the Old Greek Diaspora, which corresponded to the Greek world until the creation of the Greek state in 1832. The sea and trade were natural outlets for Greeks since the Greek peninsula is rocky and does not offer good prospects for agriculture.
Notable Greek seafarers include people such as Pytheas of Marseilles, Scylax of Caryanda who sailed to Iberia and beyond, Nearchus, the 6th century merchant and later monk Cosmas Indicopleustes (Cosmas who sailed to India) and the explorer of the Northwestern passage Juan de Fuca. In later times, the Romioi plied the sea-lanes of the Mediterranean and controlled trade until an embargo imposed by the Roman Emperor on trade with the Caliphate opened the door for the later Italian pre-eminence in trade.
The Greek shipping tradition recovered during Ottoman rule when a substantial merchant middle class developed, which played an important part in the Greek War of Independence. Today, Greek shipping continues to prosper to the extent that Greece has the largest merchant fleet in the world, while many more ships under Greek ownership fly flags of convenience. The most notable shipping magnate of the 20th century was Aristotle Onassis, others being Yiannis Latsis, George Livanos, and Stavros Niarchos.
Another study from 2012 included 150 dental school students from University of Athens, the result showed that light hair colour (blonde/light ash brown) was predominant in 10.7% of the students. 36% had medium hair colour (Light brown/Medium darkest brown). 32% had darkest brown and 21% black (15.3 off black, 6% midnight black). In conclusion the hair colour of young Greeks are mostly brown, ranging from light to dark brown. with significant minorities having black and blonde hair. The same study also showed that the eye colour of the students was 14.6% blue/green, 28% medium (light brown) and 57.4% dark brown.
The history of the Greek people is closely associated with the history of Greece, Cyprus, Constantinople, Asia Minor and the Black Sea. During the Ottoman rule of Greece, a number of Greek enclaves around the Mediterranean were cut off from the core, notably in Southern Italy, the Caucasus, Syria and Egypt. By the early 20th century, over half of the overall Greek-speaking population was settled in Asia Minor (now Turkey), while later that century a huge wave of migration to the United States, Australia, Canada and elsewhere created the modern Greek diaspora.
Even though there is a broad scientific agreement that essentialist and typological conceptualizations of race are untenable, scientists around the world continue to conceptualize race in widely differing ways, some of which have essentialist implications. While some researchers sometimes use the concept of race to make distinctions among fuzzy sets of traits, others in the scientific community suggest that the idea of race often is used in a naive or simplistic way,[page needed] and argue that, among humans, race has no taxonomic significance by pointing out that all living humans belong to the same species, Homo sapiens, and subspecies, Homo sapiens sapiens.
There is a wide consensus that the racial categories that are common in everyday usage are socially constructed, and that racial groups cannot be biologically defined. Nonetheless, some scholars argue that racial categories obviously correlate with biological traits (e.g. phenotype) to some degree, and that certain genetic markers have varying frequencies among human populations, some of which correspond more or less to traditional racial groupings. For this reason, there is no current consensus about whether racial categories can be considered to have significance for understanding human genetic variation.
When people define and talk about a particular conception of race, they create a social reality through which social categorization is achieved. In this sense, races are said to be social constructs. These constructs develop within various legal, economic, and sociopolitical contexts, and may be the effect, rather than the cause, of major social situations. While race is understood to be a social construct by many, most scholars agree that race has real material effects in the lives of people through institutionalized practices of preference and discrimination.
Socioeconomic factors, in combination with early but enduring views of race, have led to considerable suffering within disadvantaged racial groups. Racial discrimination often coincides with racist mindsets, whereby the individuals and ideologies of one group come to perceive the members of an outgroup as both racially defined and morally inferior. As a result, racial groups possessing relatively little power often find themselves excluded or oppressed, while hegemonic individuals and institutions are charged with holding racist attitudes. Racism has led to many instances of tragedy, including slavery and genocide.
In some countries, law enforcement uses race to profile suspects. This use of racial categories is frequently criticized for perpetuating an outmoded understanding of human biological variation, and promoting stereotypes. Because in some societies racial groupings correspond closely with patterns of social stratification, for social scientists studying social inequality, race can be a significant variable. As sociological factors, racial categories may in part reflect subjective attributions, self-identities, and social institutions.
Groups of humans have always identified themselves as distinct from neighboring groups, but such differences have not always been understood to be natural, immutable and global. These features are the distinguishing features of how the concept of race is used today. In this way the idea of race as we understand it today came about during the historical process of exploration and conquest which brought Europeans into contact with groups from different continents, and of the ideology of classification and typology found in the natural sciences.
The European concept of "race", along with many of the ideas now associated with the term, arose at the time of the scientific revolution, which introduced and privileged the study of natural kinds, and the age of European imperialism and colonization which established political relations between Europeans and peoples with distinct cultural and political traditions. As Europeans encountered people from different parts of the world, they speculated about the physical, social, and cultural differences among various human groups. The rise of the Atlantic slave trade, which gradually displaced an earlier trade in slaves from throughout the world, created a further incentive to categorize human groups in order to justify the subordination of African slaves. Drawing on Classical sources and upon their own internal interactions — for example, the hostility between the English and Irish powerfully influenced early European thinking about the differences between people — Europeans began to sort themselves and others into groups based on physical appearance, and to attribute to individuals belonging to these groups behaviors and capacities which were claimed to be deeply ingrained. A set of folk beliefs took hold that linked inherited physical differences between groups to inherited intellectual, behavioral, and moral qualities. Similar ideas can be found in other cultures, for example in China, where a concept often translated as "race" was associated with supposed common descent from the Yellow Emperor, and used to stress the unity of ethnic groups in China. Brutal conflicts between ethnic groups have existed throughout history and across the world.
The first post-Classical published classification of humans into distinct races seems to be François Bernier's Nouvelle division de la terre par les différents espèces ou races qui l'habitent ("New division of Earth by the different species or races which inhabit it"), published in 1684. In the 18th century the differences among human groups became a focus of scientific investigation. But the scientific classification of phenotypic variation was frequently coupled with racist ideas about innate predispositions of different groups, always attributing the most desirable features to the White, European race and arranging the other races along a continuum of progressively undesirable attributes. The 1735 classification of Carl Linnaeus, inventor of zoological taxonomy, divided the human race Homo sapiens into continental varieties of europaeus, asiaticus, americanus, and afer, each associated with a different humour: sanguine, melancholic, choleric, and phlegmatic, respectively. Homo sapiens europaeus was described as active, acute, and adventurous, whereas Homo sapiens afer was said to be crafty, lazy, and careless.
The 1775 treatise "The Natural Varieties of Mankind", by Johann Friedrich Blumenbach proposed five major divisions: the Caucasoid race, Mongoloid race, Ethiopian race (later termed Negroid, and not to be confused with the narrower Ethiopid race), American Indian race, and Malayan race, but he did not propose any hierarchy among the races. Blumenbach also noted the graded transition in appearances from one group to adjacent groups and suggested that "one variety of mankind does so sensibly pass into the other, that you cannot mark out the limits between them".
From the 17th through 19th centuries, the merging of folk beliefs about group differences with scientific explanations of those differences produced what one scholar has called an "ideology of race". According to this ideology, races are primordial, natural, enduring and distinct. It was further argued that some groups may be the result of mixture between formerly distinct populations, but that careful study could distinguish the ancestral races that had combined to produce admixed groups. Subsequent influential classifications by Georges Buffon, Petrus Camper and Christoph Meiners all classified "Negros" as inferior to Europeans. In the United States the racial theories of Thomas Jefferson were influential. He saw Africans as inferior to Whites especially in regards to their intellect, and imbued with unnatural sexual appetites, but described Native Americans as equals to whites.
In the last two decades of the 18th century, the theory of polygenism, the belief that different races had evolved separately in each continent and shared no common ancestor, was advocated in England by historian Edward Long and anatomist Charles White, in Germany by ethnographers Christoph Meiners and Georg Forster, and in France by Julien-Joseph Virey. In the US, Samuel George Morton, Josiah Nott and Louis Agassiz promoted this theory in the mid-nineteenth century. Polygenism was popular and most widespread in the 19th century, culminating in the founding of the Anthropological Society of London (1863) during the period of the American Civil War, in opposition to the Ethnological Society, which had abolitionist sympathies.
Today, all humans are classified as belonging to the species Homo sapiens and sub-species Homo sapiens sapiens. However, this is not the first species of homininae: the first species of genus Homo, Homo habilis, are theorized to have evolved in East Africa at least 2 million years ago, and members of this species populated different parts of Africa in a relatively short time. Homo erectus is theorized to have evolved more than 1.8 million years ago, and by 1.5 million years ago had spread throughout Europe and Asia. Virtually all physical anthropologists agree that Archaic Homo sapiens (A group including the possible species H. heidelbergensis, H. rhodesiensis and H. neanderthalensis) evolved out of African Homo erectus ((sensu lato) or Homo ergaster).
In the early 20th century, many anthropologists accepted and taught the belief that biologically distinct races were isomorphic with distinct linguistic, cultural, and social groups, while popularly applying that belief to the field of eugenics, in conjunction with a practice that is now called scientific racism. After the Nazi eugenics program, racial essentialism lost widespread popularity. Race anthropologists were pressured to acknowledge findings coming from studies of culture and population genetics, and to revise their conclusions about the sources of phenotypic variation. A significant number of modern anthropologists and biologists in the West came to view race as an invalid genetic or biological designation.
Population geneticists have debated whether the concept of population can provide a basis for a new conception of race. In order to do this, a working definition of population must be found. Surprisingly, there is no generally accepted concept of population that biologists use. Although the concept of population is central to ecology, evolutionary biology and conservation biology, most definitions of population rely on qualitative descriptions such as "a group of organisms of the same species occupying a particular space at a particular time" Waples and Gaggiotti identify two broad types of definitions for populations; those that fall into an ecological paradigm, and those that fall into an evolutionary paradigm. Examples of such definitions are:
Traditionally, subspecies are seen as geographically isolated and genetically differentiated populations. That is, "the designation 'subspecies' is used to indicate an objective degree of microevolutionary divergence" One objection to this idea is that it does not specify what degree of differentiation is required. Therefore, any population that is somewhat biologically different could be considered a subspecies, even to the level of a local population. As a result, Templeton has argued that it is necessary to impose a threshold on the level of difference that is required for a population to be designated a subspecies.
This effectively means that populations of organisms must have reached a certain measurable level of difference to be recognised as subspecies. Dean Amadon proposed in 1949 that subspecies would be defined according to the seventy-five percent rule which means that 75% of a population must lie outside 99% of the range of other populations for a given defining morphological character or a set of characters. The seventy-five percent rule still has defenders but other scholars argue that it should be replaced with ninety or ninety-five percent rule.
In 1978, Sewall Wright suggested that human populations that have long inhabited separated parts of the world should, in general, be considered different subspecies by the usual criterion that most individuals of such populations can be allocated correctly by inspection. Wright argued that it does not require a trained anthropologist to classify an array of Englishmen, West Africans, and Chinese with 100% accuracy by features, skin color, and type of hair despite so much variability within each of these groups that every individual can easily be distinguished from every other. However, it is customary to use the term race rather than subspecies for the major subdivisions of the human species as well as for minor ones.
Cladistics is another method of classification. A clade is a taxonomic group of organisms consisting of a single common ancestor and all the descendants of that ancestor. Every creature produced by sexual reproduction has two immediate lineages, one maternal and one paternal. Whereas Carl Linnaeus established a taxonomy of living organisms based on anatomical similarities and differences, cladistics seeks to establish a taxonomy—the phylogenetic tree—based on genetic similarities and differences and tracing the process of acquisition of multiple characteristics by single organisms. Some researchers have tried to clarify the idea of race by equating it to the biological idea of the clade. Often mitochondrial DNA or Y chromosome sequences are used to study ancient human migration paths. These single-locus sources of DNA do not recombine and are inherited from a single parent. Individuals from the various continental groups tend to be more similar to one another than to people from other continents, and tracing either mitochondrial DNA or non-recombinant Y-chromosome DNA explains how people in one place may be largely derived from people in some remote location.
Often taxonomists prefer to use phylogenetic analysis to determine whether a population can be considered a subspecies. Phylogenetic analysis relies on the concept of derived characteristics that are not shared between groups, usually applying to populations that are allopatric (geographically separated) and therefore discretely bounded. This would make a subspecies, evolutionarily speaking, a clade – a group with a common evolutionary ancestor population. The smooth gradation of human genetic variation in general tends to rule out any idea that human population groups can be considered monophyletic (cleanly divided), as there appears to always have been considerable gene flow between human populations. Rachel Caspari (2003) have argued that clades are by definition monophyletic groups (a taxon that includes all descendants of a given ancestor) and since no groups currently regarded as races are monophyletic, none of those groups can be clades.
For the anthropologists Lieberman and Jackson (1995), however, there are more profound methodological and conceptual problems with using cladistics to support concepts of race. They claim that "the molecular and biochemical proponents of this model explicitly use racial categories in their initial grouping of samples". For example, the large and highly diverse macroethnic groups of East Indians, North Africans, and Europeans are presumptively grouped as Caucasians prior to the analysis of their DNA variation. This is claimed to limit and skew interpretations, obscure other lineage relationships, deemphasize the impact of more immediate clinal environmental factors on genomic diversity, and can cloud our understanding of the true patterns of affinity. They argue that however significant the empirical research, these studies use the term race in conceptually imprecise and careless ways. They suggest that the authors of these studies find support for racial distinctions only because they began by assuming the validity of race. "For empirical reasons we prefer to place emphasis on clinal variation, which recognizes the existence of adaptive human hereditary variation and simultaneously stresses that such variation is not found in packages that can be labeled races."
One crucial innovation in reconceptualizing genotypic and phenotypic variation was the anthropologist C. Loring Brace's observation that such variations, insofar as it is affected by natural selection, slow migration, or genetic drift, are distributed along geographic gradations or clines. In part this is due to isolation by distance. This point called attention to a problem common to phenotype-based descriptions of races (for example, those based on hair texture and skin color): they ignore a host of other similarities and differences (for example, blood type) that do not correlate highly with the markers for race. Thus, anthropologist Frank Livingstone's conclusion, that since clines cross racial boundaries, "there are no races, only clines".
In a response to Livingstone, Theodore Dobzhansky argued that when talking about race one must be attentive to how the term is being used: "I agree with Dr. Livingstone that if races have to be 'discrete units,' then there are no races, and if 'race' is used as an 'explanation' of the human variability, rather than vice versa, then the explanation is invalid." He further argued that one could use the term race if one distinguished between "race differences" and "the race concept." The former refers to any distinction in gene frequencies between populations; the latter is "a matter of judgment." He further observed that even when there is clinal variation, "Race differences are objectively ascertainable biological phenomena… but it does not follow that racially distinct populations must be given racial (or subspecific) labels." In short, Livingstone and Dobzhansky agree that there are genetic differences among human beings; they also agree that the use of the race concept to classify people, and how the race concept is used, is a matter of social convention. They differ on whether the race concept remains a meaningful and useful social convention.
In 1964, the biologists Paul Ehrlich and Holm pointed out cases where two or more clines are distributed discordantly—for example, melanin is distributed in a decreasing pattern from the equator north and south; frequencies for the haplotype for beta-S hemoglobin, on the other hand, radiate out of specific geographical points in Africa. As the anthropologists Leonard Lieberman and Fatimah Linda Jackson observed, "Discordant patterns of heterogeneity falsify any description of a population as if it were genotypically or even phenotypically homogeneous".
Patterns such as those seen in human physical and genetic variation as described above, have led to the consequence that the number and geographic location of any described races is highly dependent on the importance attributed to, and quantity of, the traits considered. Scientists discovered a skin-lighting mutation that partially accounts for the appearance of Light skin in humans (people who migrated out of Africa northward into what is now Europe) which they estimate occurred 20,000 to 50,000 years ago. The East Asians owe their relatively light skin to different mutations. On the other hand, the greater the number of traits (or alleles) considered, the more subdivisions of humanity are detected, since traits and gene frequencies do not always correspond to the same geographical location. Or as Ossorio & Duster (2005) put it:
Coop et al. (2009) found "a selected allele that strongly differentiates the French from both the Yoruba and Han could be strongly clinal across Europe, or at high frequency in Europe and absent elsewhere, or follow any other distribution according to the geographic nature of the selective pressure. However, we see that the global geographic distributions of these putatively selected alleles are largely determined simply by their frequencies in Yoruba, French and Han (Figure 3). The global distributions fall into three major geographic patterns that we interpret as non-African sweeps, west Eurasian sweeps and East Asian sweeps, respectively."
Another way to look at differences between populations is to measure genetic differences rather than physical differences between groups. The mid-20th-century anthropologist William C. Boyd defined race as: "A population which differs significantly from other populations in regard to the frequency of one or more of the genes it possesses. It is an arbitrary matter which, and how many, gene loci we choose to consider as a significant 'constellation'". Leonard Lieberman and Rodney Kirk have pointed out that "the paramount weakness of this statement is that if one gene can distinguish races then the number of races is as numerous as the number of human couples reproducing." Moreover, the anthropologist Stephen Molnar has suggested that the discordance of clines inevitably results in a multiplication of races that renders the concept itself useless. The Human Genome Project states "People who have lived in the same geographic region for many generations may have some alleles in common, but no allele will be found in all members of one population and in no members of any other."
The population geneticist Sewall Wright developed one way of measuring genetic differences between populations known as the Fixation index, which is often abbreviated to FST. This statistic is often used in taxonomy to compare differences between any two given populations by measuring the genetic differences among and between populations for individual genes, or for many genes simultaneously. It is often stated that the fixation index for humans is about 0.15. This translates to an estimated 85% of the variation measured in the overall human population is found within individuals of the same population, and about 15% of the variation occurs between populations. These estimates imply that any two individuals from different populations are almost as likely to be more similar to each other than either is to a member of their own group. Richard Lewontin, who affirmed these ratios, thus concluded neither "race" nor "subspecies" were appropriate or useful ways to describe human populations. However, others have noticed that group variation was relatively similar to the variation observed in other mammalian species.
Wright himself believed that values >0.25 represent very great genetic variation and that an FST of 0.15–0.25 represented great variation. However, about 5% of human variation occurs between populations within continents, therefore FST values between continental groups of humans (or races) of as low as 0.1 (or possibly lower) have been found in some studies, suggesting more moderate levels of genetic variation. Graves (1996) has countered that FST should not be used as a marker of subspecies status, as the statistic is used to measure the degree of differentiation between populations, although see also Wright (1978).
Jeffrey Long and Rick Kittles give a long critique of the application of FST to human populations in their 2003 paper "Human Genetic Diversity and the Nonexistence of Biological Races". They find that the figure of 85% is misleading because it implies that all human populations contain on average 85% of all genetic diversity. They claim that this does not correctly reflect human population history, because it treats all human groups as independent. A more realistic portrayal of the way human groups are related is to understand that some human groups are parental to other groups and that these groups represent paraphyletic groups to their descent groups. For example, under the recent African origin theory the human population in Africa is paraphyletic to all other human groups because it represents the ancestral group from which all non-African populations derive, but more than that, non-African groups only derive from a small non-representative sample of this African population. This means that all non-African groups are more closely related to each other and to some African groups (probably east Africans) than they are to others, and further that the migration out of Africa represented a genetic bottleneck, with much of the diversity that existed in Africa not being carried out of Africa by the emigrating groups. This view produces a version of human population movements that do not result in all human populations being independent; but rather, produces a series of dilutions of diversity the further from Africa any population lives, each founding event representing a genetic subset of its parental population. Long and Kittles find that rather than 85% of human genetic diversity existing in all human populations, about 100% of human diversity exists in a single African population, whereas only about 70% of human genetic diversity exists in a population derived from New Guinea. Long and Kittles argued that this still produces a global human population that is genetically homogeneous compared to other mammalian populations.
In his 2003 paper, "Human Genetic Diversity: Lewontin's Fallacy", A. W. F. Edwards argued that rather than using a locus-by-locus analysis of variation to derive taxonomy, it is possible to construct a human classification system based on characteristic genetic patterns, or clusters inferred from multilocus genetic data. Geographically based human studies since have shown that such genetic clusters can be derived from analyzing of a large number of loci which can assort individuals sampled into groups analogous to traditional continental racial groups. Joanna Mountain and Neil Risch cautioned that while genetic clusters may one day be shown to correspond to phenotypic variations between groups, such assumptions were premature as the relationship between genes and complex traits remains poorly understood. However, Risch denied such limitations render the analysis useless: "Perhaps just using someone's actual birth year is not a very good way of measuring age. Does that mean we should throw it out? ... Any category you come up with is going to be imperfect, but that doesn't preclude you from using it or the fact that it has utility."
Early human genetic cluster analysis studies were conducted with samples taken from ancestral population groups living at extreme geographic distances from each other. It was thought that such large geographic distances would maximize the genetic variation between the groups sampled in the analysis and thus maximize the probability of finding cluster patterns unique to each group. In light of the historically recent acceleration of human migration (and correspondingly, human gene flow) on a global scale, further studies were conducted to judge the degree to which genetic cluster analysis can pattern ancestrally identified groups as well as geographically separated groups. One such study looked at a large multiethnic population in the United States, and "detected only modest genetic differentiation between different current geographic locales within each race/ethnicity group. Thus, ancient geographic ancestry, which is highly correlated with self-identified race/ethnicity—as opposed to current residence—is the major determinant of genetic structure in the U.S. population." (Tang et al. (2005))
Witherspoon et al. (2007) have argued that even when individuals can be reliably assigned to specific population groups, it may still be possible for two randomly chosen individuals from different populations/clusters to be more similar to each other than to a randomly chosen member of their own cluster. They found that many thousands of genetic markers had to be used in order for the answer to the question "How often is a pair of individuals from one population genetically more dissimilar than two individuals chosen from two different populations?" to be "never". This assumed three population groups separated by large geographic ranges (European, African and East Asian). The entire world population is much more complex and studying an increasing number of groups would require an increasing number of markers for the same answer. The authors conclude that "caution should be used when using geographic or genetic ancestry to make inferences about individual phenotypes." Witherspoon, et al. concluded that, "The fact that, given enough genetic data, individuals can be correctly assigned to their populations of origin is compatible with the observation that most human genetic variation is found within populations, not between them. It is also compatible with our ﬁnding that, even when the most distinct populations are considered and hundreds of loci are used, individuals are frequently more similar to members of other populations than to members of their own population."
Anthropologists such as C. Loring Brace, the philosophers Jonathan Kaplan and Rasmus Winther, and the geneticist Joseph Graves,[page needed] have argued that while there it is certainly possible to find biological and genetic variation that corresponds roughly to the groupings normally defined as "continental races", this is true for almost all geographically distinct populations. The cluster structure of the genetic data is therefore dependent on the initial hypotheses of the researcher and the populations sampled. When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clustering would be different. Weiss and Fullerton have noted that if one sampled only Icelanders, Mayans and Maoris, three distinct clusters would form and all other populations could be described as being clinally composed of admixtures of Maori, Icelandic and Mayan genetic materials. Kaplan and Winther therefore argue that, seen in this way, both Lewontin and Edwards are right in their arguments. They conclude that while racial groups are characterized by different allele frequencies, this does not mean that racial classification is a natural taxonomy of the human species, because multiple other genetic patterns can be found in human populations that crosscut racial distinctions. Moreover, the genomic data underdetermines whether one wishes to see subdivisions (i.e., splitters) or a continuum (i.e., lumpers). Under Kaplan and Winther's view, racial groupings are objective social constructions (see Mills 1998 ) that have conventional biological reality only insofar as the categories are chosen and constructed for pragmatic scientific reasons. In earlier work, Winther had identified "diversity partitioning" and "clustering analysis" as two separate methodologies, with distinct questions, assumptions, and protocols. Each is also associated with opposing ontological consequences vis-a-vis the metaphysics of race.
Many social scientists have replaced the word race with the word "ethnicity" to refer to self-identifying groups based on beliefs concerning shared culture, ancestry and history. Alongside empirical and conceptual problems with "race", following the Second World War, evolutionary and social scientists were acutely aware of how beliefs about race had been used to justify discrimination, apartheid, slavery, and genocide. This questioning gained momentum in the 1960s during the U.S. civil rights movement and the emergence of numerous anti-colonial movements worldwide. They thus came to believe that race itself is a social construct, a concept that was believed to correspond to an objective reality but which was believed in because of its social functions.
Craig Venter and Francis Collins of the National Institute of Health jointly made the announcement of the mapping of the human genome in 2000. Upon examining the data from the genome mapping, Venter realized that although the genetic variation within the human species is on the order of 1–3% (instead of the previously assumed 1%), the types of variations do not support notion of genetically defined races. Venter said, "Race is a social concept. It's not a scientific one. There are no bright lines (that would stand out), if we could compare all the sequenced genomes of everyone on the planet." "When we try to apply science to try to sort out these social differences, it all falls apart."
The theory that race is merely a social construct has been challenged by the findings of researchers at the Stanford University School of Medicine, published in the American Journal of Human Genetics as "Genetic Structure, Self-Identified Race/Ethnicity, and Confounding in Case-Control Association Studies". One of the researchers, Neil Risch, noted: "we looked at the correlation between genetic structure [based on microsatellite markers] versus self-description, we found 99.9% concordance between the two. We actually had a higher discordance rate between self-reported sex and markers on the X chromosome! So you could argue that sex is also a problematic category. And there are differences between sex and gender; self-identification may not be correlated with biology perfectly. And there is sexism."
Basically, race in Brazil was "biologized", but in a way that recognized the difference between ancestry (which determines genotype) and phenotypic differences. There, racial identity was not governed by rigid descent rule, such as the one-drop rule, as it was in the United States. A Brazilian child was never automatically identified with the racial type of one or both parents, nor were there only a very limited number of categories to choose from, to the extent that full siblings can pertain to different racial groups.
Over a dozen racial categories would be recognized in conformity with all the possible combinations of hair color, hair texture, eye color, and skin color. These types grade into each other like the colors of the spectrum, and not one category stands significantly isolated from the rest. That is, race referred preferentially to appearance, not heredity, and appearance is a poor indication of ancestry, because only a few genes are responsible for someone's skin color and traits: a person who is considered white may have more African ancestry than a person who is considered black, and the reverse can be also true about European ancestry. The complexity of racial classifications in Brazil reflects the extent of miscegenation in Brazilian society, a society that remains highly, but not strictly, stratified along color lines. These socioeconomic factors are also significant to the limits of racial lines, because a minority of pardos, or brown people, are likely to start declaring themselves white or black if socially upward, and being seen as relatively "whiter" as their perceived social status increases (much as in other regions of Latin America).
Fluidity of racial categories aside, the "biologification" of race in Brazil referred above would match contemporary concepts of race in the United States quite closely, though, if Brazilians are supposed to choose their race as one among, Asian and Indigenous apart, three IBGE's census categories. While assimilated Amerindians and people with very high quantities of Amerindian ancestry are usually grouped as caboclos, a subgroup of pardos which roughly translates as both mestizo and hillbilly, for those of lower quantity of Amerindian descent a higher European genetic contribution is expected to be grouped as a pardo. In several genetic tests, people with less than 60-65% of European descent and 5-10% of Amerindian descent usually cluster with Afro-Brazilians (as reported by the individuals), or 6.9% of the population, and those with about 45% or more of Subsaharan contribution most times do so (in average, Afro-Brazilian DNA was reported to be about 50% Subsaharan African, 37% European and 13% Amerindian).
If a more consistent report with the genetic groups in the gradation of miscegenation is to be considered (e.g. that would not cluster people with a balanced degree of African and non-African ancestry in the black group instead of the multiracial one, unlike elsewhere in Latin America where people of high quantity of African descent tend to classify themselves as mixed), more people would report themselves as white and pardo in Brazil (47.7% and 42.4% of the population as of 2010, respectively), because by research its population is believed to have between 65 and 80% of autosomal European ancestry, in average (also >35% of European mt-DNA and >95% of European Y-DNA).
This is not surprising, though: While the greatest number of slaves imported from Africa were sent to Brazil, totalizing roughly 3.5 million people, they lived in such miserable conditions that male African Y-DNA there is significantly rare due to the lack of resources and time involved with raising of children, so that most African descent originarily came from relations between white masters and female slaves. From the last decades of the Empire until the 1950s, the proportion of the white population increased significantly while Brazil welcomed 5.5 million immigrants between 1821 and 1932, not much behind its neighbor Argentina with 6.4 million, and it received more European immigrants in its colonial history than the United States. Between 1500 and 1760, 700.000 Europeans settled in Brazil, while 530.000 Europeans settled in the United States for the same given time. Thus, the historical construction of race in Brazilian society dealt primarily with gradations between persons of majoritarily European ancestry and little minority groups with otherwise lower quantity therefrom in recent times.
The European Union uses the terms racial origin and ethnic origin synonymously in its documents and according to it "the use of the term 'racial origin' in this directive does not imply an acceptance of such [racial] theories".[full citation needed] Haney López warns that using "race" as a category within the law tends to legitimize its existence in the popular imagination. In the diverse geographic context of Europe, ethnicity and ethnic origin are arguably more resonant and are less encumbered by the ideological baggage associated with "race". In European context, historical resonance of "race" underscores its problematic nature. In some states, it is strongly associated with laws promulgated by the Nazi and Fascist governments in Europe during the 1930s and 1940s. Indeed, in 1996, the European Parliament adopted a resolution stating that "the term should therefore be avoided in all official texts".
The concept of racial origin relies on the notion that human beings can be separated into biologically distinct "races", an idea generally rejected by the scientific community. Since all human beings belong to the same species, the ECRI (European Commission against Racism and Intolerance) rejects theories based on the existence of different "races". However, in its Recommendation ECRI uses this term in order to ensure that those persons who are generally and erroneously perceived as belonging to "another race" are not excluded from the protection provided for by the legislation. The law claims to reject the existence of "race", yet penalize situations where someone is treated less favourably on this ground.
Since the end of the Second World War, France has become an ethnically diverse country. Today, approximately five percent of the French population is non-European and non-white. This does not approach the number of non-white citizens in the United States (roughly 28–37%, depending on how Latinos are classified; see Demographics of the United States). Nevertheless, it amounts to at least three million people, and has forced the issues of ethnic diversity onto the French policy agenda. France has developed an approach to dealing with ethnic problems that stands in contrast to that of many advanced, industrialized countries. Unlike the United States, Britain, or even the Netherlands, France maintains a "color-blind" model of public policy. This means that it targets virtually no policies directly at racial or ethnic groups. Instead, it uses geographic or class criteria to address issues of social inequalities. It has, however, developed an extensive anti-racist policy repertoire since the early 1970s. Until recently, French policies focused primarily on issues of hate speech—going much further than their American counterparts—and relatively less on issues of discrimination in jobs, housing, and in provision of goods and services.
Since the early history of the United States, Amerindians, African–Americans, and European Americans have been classified as belonging to different races. Efforts to track mixing between groups led to a proliferation of categories, such as mulatto and octoroon. The criteria for membership in these races diverged in the late 19th century. During Reconstruction, increasing numbers of Americans began to consider anyone with "one drop" of known "Black blood" to be Black, regardless of appearance.3 By the early 20th century, this notion was made statutory in many states.4 Amerindians continue to be defined by a certain percentage of "Indian blood" (called blood quantum). To be White one had to have perceived "pure" White ancestry. The one-drop rule or hypodescent rule refers to the convention of defining a person as racially black if he or she has any known African ancestry. This rule meant that those that were mixed race but with some discernible African ancestry were defined as black. The one-drop rule is specific to not only those with African ancestry but to the United States, making it a particularly African-American experience.
The term "Hispanic" as an ethnonym emerged in the 20th century with the rise of migration of laborers from the Spanish-speaking countries of Latin America to the United States. Today, the word "Latino" is often used as a synonym for "Hispanic". The definitions of both terms are non-race specific, and include people who consider themselves to be of distinct races (Black, White, Amerindian, Asian, and mixed groups). However, there is a common misconception in the US that Hispanic/Latino is a race or sometimes even that national origins such as Mexican, Cuban, Colombian, Salvadoran, etc. are races. In contrast to "Latino" or "Hispanic", "Anglo" refers to non-Hispanic White Americans or non-Hispanic European Americans, most of whom speak the English language but are not necessarily of English descent.
Wang, Štrkalj et al. (2003) examined the use of race as a biological concept in research papers published in China's only biological anthropology journal, Acta Anthropologica Sinica. The study showed that the race concept was widely used among Chinese anthropologists. In a 2007 review paper, Štrkalj suggested that the stark contrast of the racial approach between the United States and China was due to the fact that race is a factor for social cohesion among the ethnically diverse people of China, whereas "race" is a very sensitive issue in America and the racial approach is considered to undermine social cohesion - with the result that in the socio-political context of US academics scientists are encouraged not to use racial categories, whereas in China they are encouraged to use them.
Kaszycka et al. (2009) in 2002–2003 surveyed European anthropologists' opinions toward the biological race concept. Three factors, country of academic education, discipline, and age, were found to be significant in differentiating the replies. Those educated in Western Europe, physical anthropologists, and middle-aged persons rejected race more frequently than those educated in Eastern Europe, people in other branches of science, and those from both younger and older generations." The survey shows that the views on race are sociopolitically (ideologically) influenced and highly dependent on education."
One result of debates over the meaning and validity of the concept of race is that the current literature across different disciplines regarding human variation lacks consensus, though within some fields, such as some branches of anthropology, there is strong consensus. Some studies use the word race in its early essentialist taxonomic sense. Many others still use the term race, but use it to mean a population, clade, or haplogroup. Others eschew the concept of race altogether, and use the concept of population as a less problematic unit of analysis.
Eduardo Bonilla-Silva, Sociology professor at Duke University, remarks, "I contend that racism is, more than anything else, a matter of group power; it is about a dominant racial group (whites) striving to maintain its systemic advantages and minorities fighting to subvert the racial status quo."  The types of practices that take place under this new color-blind racism is subtle, institutionalized, and supposedly not racial. Color-blind racism thrives on the idea that race is no longer an issue in the United States. There are contradictions between the alleged color-blindness of most whites and the persistence of a color-coded system of inequality.
The concept of biological race has declined significantly in frequency of use in physical anthropology in the United States during the 20th century. A majority of physical anthropologists in the United States have rejected the concept of biological races. Since 1932, an increasing number of college textbooks introducing physical anthropology have rejected race as a valid concept: from 1932 to 1976, only seven out of thirty-two rejected race; from 1975 to 1984, thirteen out of thirty-three rejected race; from 1985 to 1993, thirteen out of nineteen rejected race. According to one academic journal entry, where 78 percent of the articles in the 1931 Journal of Physical Anthropology employed these or nearly synonymous terms reflecting a bio-race paradigm, only 36 percent did so in 1965, and just 28 percent did in 1996.
According to the 2000 edition of a popular physical anthropology textbook, forensic anthropologists are overwhelmingly in support of the idea of the basic biological reality of human races. Forensic physical anthropologist and professor George W. Gill has said that the idea that race is only skin deep "is simply not true, as any experienced forensic anthropologist will affirm" and "Many morphological features tend to follow geographic boundaries coinciding often with climatic zones. This is not surprising since the selective forces of climate are probably the primary forces of nature that have shaped human races with regard not only to skin color and hair form but also the underlying bony structures of the nose, cheekbones, etc. (For example, more prominent noses humidify air better.)" While he can see good arguments for both sides, the complete denial of the opposing evidence "seems to stem largely from socio-political motivation and not science at all". He also states that many biological anthropologists see races as real yet "not one introductory textbook of physical anthropology even presents that perspective as a possibility. In a case as flagrant as this, we are not dealing with science but rather with blatant, politically motivated censorship".
"Race" is still sometimes used within forensic anthropology (when analyzing skeletal remains), biomedical research, and race-based medicine. Brace has criticized this, the practice of forensic anthropologists for using the controversial concept "race" out of convention when they in fact should be talking about regional ancestry. He argues that while forensic anthropologists can determine that a skeletal remain comes from a person with ancestors in a specific region of Africa, categorizing that skeletal as being "black" is a socially constructed category that is only meaningful in the particular context of the United States, and which is not itself scientifically valid.
The authors of the study also examined 77 college textbooks in biology and 69 in physical anthropology published between 1932 and 1989. Physical anthropology texts argued that biological races exist until the 1970s, when they began to argue that races do not exist. In contrast, biology textbooks did not undergo such a reversal but many instead dropped their discussion of race altogether. The authors attributed this to biologists trying to avoid discussing the political implications of racial classifications, instead of discussing them, and to the ongoing discussions in biology about the validity of the concept "subspecies". The authors also noted that some widely used textbooks in biology such as Douglas J. Futuyama's 1986 "Evolutionary Biology" had abandoned the race concept, "The concept of race, masking the overwhelming genetic similarity of all peoples and the mosaic patterns of variation that do not correspond to racial divisions, is not only socially dysfunctional but is biologically indefensible as well (pp. 5 18-5 19)." (Lieberman et al. 1992, pp. 316–17)
Morning (2008) looked at high school biology textbooks during the 1952-2002 period and initially found a similar pattern with only 35% directly discussing race in the 1983–92 period from initially 92% doing so. However, this has increased somewhat after this to 43%. More indirect and brief discussions of race in the context of medical disorders have increased from none to 93% of textbooks. In general, the material on race has moved from surface traits to genetics and evolutionary history. The study argues that the textbooks' fundamental message about the existence of races has changed little.
In the United States, federal government policy promotes the use of racially categorized data to identify and address health disparities between racial or ethnic groups. In clinical settings, race has sometimes been considered in the diagnosis and treatment of medical conditions. Doctors have noted that some medical conditions are more prevalent in certain racial or ethnic groups than in others, without being sure of the cause of those differences. Recent interest in race-based medicine, or race-targeted pharmacogenomics, has been fueled by the proliferation of human genetic data which followed the decoding of the human genome in the first decade of the twenty-first century. There is an active debate among biomedical researchers about the meaning and importance of race in their research. Proponents of the use of racial categories in biomedicine argue that continued use of racial categorizations in biomedical research and clinical practice makes possible the application of new genetic findings, and provides a clue to diagnosis.
Other researchers point out that finding a difference in disease prevalence between two socially defined groups does not necessarily imply genetic causation of the difference. They suggest that medical practices should maintain their focus on the individual rather than an individual's membership to any group. They argue that overemphasizing genetic contributions to health disparities carries various risks such as reinforcing stereotypes, promoting racism or ignoring the contribution of non-genetic factors to health disparities. International epidemiological data show that living conditions rather than race make the biggest difference in health outcomes even for diseases that have "race-specific" treatments. Some studies have found that patients are reluctant to accept racial categorization in medical practice.
In an attempt to provide general descriptions that may facilitate the job of law enforcement officers seeking to apprehend suspects, the United States FBI employs the term "race" to summarize the general appearance (skin color, hair texture, eye shape, and other such easily noticed characteristics) of individuals whom they are attempting to apprehend. From the perspective of law enforcement officers, it is generally more important to arrive at a description that will readily suggest the general appearance of an individual than to make a scientifically valid categorization by DNA or other such means. Thus, in addition to assigning a wanted individual to a racial category, such a description will include: height, weight, eye color, scars and other distinguishing characteristics.
Criminal justice agencies in England and Wales use at least two separate racial/ethnic classification systems when reporting crime, as of 2010. One is the system used in the 2001 Census when individuals identify themselves as belonging to a particular ethnic group: W1 (White-British), W2 (White-Irish), W9 (Any other white background); M1 (White and black Caribbean), M2 (White and black African), M3 (White and Asian), M9 (Any other mixed background); A1 (Asian-Indian), A2 (Asian-Pakistani), A3 (Asian-Bangladeshi), A9 (Any other Asian background); B1 (Black Caribbean), B2 (Black African), B3 (Any other black background); O1 (Chinese), O9 (Any other). The other is categories used by the police when they visually identify someone as belonging to an ethnic group, e.g. at the time of a stop and search or an arrest: White – North European (IC1), White – South European (IC2), Black (IC3), Asian (IC4), Chinese, Japanese, or South East Asian (IC5), Middle Eastern (IC6), and Unknown (IC0). "IC" stands for "Identification Code;" these items are also referred to as Phoenix classifications. Officers are instructed to "record the response that has been given" even if the person gives an answer which may be incorrect; their own perception of the person's ethnic background is recorded separately. Comparability of the information being recorded by officers was brought into question by the Office for National Statistics (ONS) in September 2007, as part of its Equality Data Review; one problem cited was the number of reports that contained an ethnicity of "Not Stated."
In the United States, the practice of racial profiling has been ruled to be both unconstitutional and a violation of civil rights. There is active debate regarding the cause of a marked correlation between the recorded crimes, punishments meted out, and the country's populations. Many consider de facto racial profiling an example of institutional racism in law enforcement. The history of misuse of racial categories to impact adversely one or more groups and/or to offer protection and advantage to another has a clear impact on debate of the legitimate use of known phenotypical or genotypical characteristics tied to the presumed race of both victims and perpetrators by the government.
Mass incarceration in the United States disproportionately impacts African American and Latino communities. Michelle Alexander, author of The New Jim Crow: Mass Incarceration in the Age of Colorblindness (2010), argues that mass incarceration is best understood as not only a system of overcrowded prisons. Mass incarceration is also, "the larger web of laws, rules, policies, and customs that control those labeled criminals both in and out of prison." She defines it further as "a system that locks people not only behind actual bars in actual prisons, but also behind virtual bars and virtual walls", illustrating the second-class citizenship that is imposed on a disproportionate number of people of color, specifically African-Americans. She compares mass incarceration to Jim Crow laws, stating that both work as racial caste systems.
Similarly, forensic anthropologists draw on highly heritable morphological features of human remains (e.g. cranial measurements) to aid in the identification of the body, including in terms of race. In a 1992 article, anthropologist Norman Sauer noted that anthropologists had generally abandoned the concept of race as a valid representation of human biological diversity, except for forensic anthropologists. He asked, "If races don't exist, why are forensic anthropologists so good at identifying them?" He concluded:
Abu el-Haj argues that genomics and the mapping of lineages and clusters liberates "the new racial science from the older one by disentangling ancestry from culture and capacity."[citation needed] As an example, she refers to recent work by Hammer et al., which aimed to test the claim that present-day Jews are more closely related to one another than to neighbouring non-Jewish populations. Hammer et al. found that the degree of genetic similarity among Jews shifted depending on the locus investigated, and suggested that this was the result of natural selection acting on particular loci. They focused on the non-recombining Y-chromosome to "circumvent some of the complications associated with selection".
As another example, she points to work by Thomas et al., who sought to distinguish between the Y chromosomes of Jewish priests (Kohanim), (in Judaism, membership in the priesthood is passed on through the father's line) and the Y chromosomes of non-Jews. Abu el-Haj concluded that this new "race science" calls attention to the importance of "ancestry" (narrowly defined, as it does not include all ancestors) in some religions and in popular culture, and people's desire to use science to confirm their claims about ancestry; this "race science", she argues, is fundamentally different from older notions of race that were used to explain differences in human behaviour or social status:
One problem with these assignments is admixture. Many people have a highly varied ancestry. For example, in the United States, colonial and early federal history were periods of numerous interracial relationships, both outside and inside slavery. This has resulted in a majority of people who identify as African American having some European ancestors. Similarly, many people who identify as white have some African ancestors. In a survey in a northeastern U.S. university of college students who identified as "white", about 30% were estimated to have up to 10% African ancestry.
Slavs are the largest Indo-European ethno-linguistic group in Europe. They inhabit Central Europe, Eastern Europe, Southeast Europe, North Asia and Central Asia. Slavs speak Indo-European Slavic languages and share, to varying degrees, some cultural traits and historical backgrounds. From the early 6th century they spread to inhabit most of Central and Eastern Europe and Southeast Europe, whilst Slavic mercenaries fighting for the Byzantines and Arabs settled Asia Minor and even as far as Syria. The East Slavs colonised Siberia and Central Asia.[better source needed] Presently over half of Europe's territory is inhabited by Slavic-speaking communities, but every Slavic ethnicity has emigrated to other continents.
Present-day Slavic people are classified into West Slavic (chiefly Poles, Czechs and Slovaks), East Slavic (chiefly Russians, Belarusians, and Ukrainians), and South Slavic (chiefly Serbs, Bulgarians, Croats, Bosniaks, Macedonians, Slovenes, and Montenegrins), though sometimes the West Slavs and East Slavs are combined into a single group known as North Slavs. For a more comprehensive list, see the ethnocultural subdivisions. Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual ethnic groups themselves – are varied, ranging from a sense of connection to mutual feelings of hostility.
The Slavic autonym is reconstructed in Proto-Slavic as *Slověninъ, plural *Slověne. The oldest documents written in Old Church Slavonic and dating from the 9th century attest Словѣне Slověne to describe the Slavs. Other early Slavic attestations include Old East Slavic Словѣнѣ Slověně for "an East Slavic group near Novgorod." However, the earliest written references to the Slavs under this name are in other languages. In the 6th century AD Procopius, writing in Byzantine Greek, refers to the Σκλάβοι Sklaboi, Σκλαβηνοί Sklabēnoi, Σκλαυηνοί Sklauenoi, Σθλαβηνοί Sthlabenoi, or Σκλαβῖνοι Sklabinoi, while his contemporary Jordanes refers to the Sclaveni in Latin.
The Slavic autonym *Slověninъ is usually considered (e.g. by Roman Jakobson) a derivation from slovo "word", originally denoting "people who speak (the same language)," i.e. people who understand each other, in contrast to the Slavic word denoting "foreign people" – němci, meaning "mumbling, murmuring people" (from Slavic *němъ – "mumbling, mute").
The word slovo ("word") and the related slava ("fame") and slukh ("hearing") originate from the Proto-Indo-European root *ḱlew- ("be spoken of, fame"), cognate with Ancient Greek κλῆς (klês - "famous"), whence the name Pericles, and Latin clueo ("be called"), and English loud.
The English word Slav could be derived from the Middle English word sclave, which was borrowed from Medieval Latin sclavus or slavus, itself a borrowing and Byzantine Greek σκλάβος sklábos "slave," which was in turn apparently derived from a misunderstanding of the Slavic autonym (denoting a speaker of their own languages). The Byzantine term Sklavinoi was loaned into Arabic as Saqaliba صقالبة (sing. Saqlabi صقلبي) by medieval Arab historiographers. However, the origin of this word is disputed.
Alternative proposals for the etymology of *Slověninъ propounded by some scholars have much less support. Lozinski argues that the word *slava once had the meaning of worshipper, in this context meaning "practicer of a common Slavic religion," and from that evolved into an ethnonym. S.B. Bernstein speculates that it derives from a reconstructed Proto-Indo-European *(s)lawos, cognate to Ancient Greek λαός laós "population, people," which itself has no commonly accepted etymology. Meanwhile, others have pointed out that the suffix -enin indicates a man from a certain place, which in this case should be a place called Slova or Slava, possibly a river name. The Old East Slavic Slavuta for the Dnieper River was argued by Henrich Bartek (1907–1986) to be derived from slova and also the origin of Slovene.
The earliest mentions of Slavic raids across the lower River Danube may be dated to the first half of the 6th century, yet no archaeological evidence of a Slavic settlement in the Balkans could be securely dated before c. 600 AD.
The Slavs under name of the Antes and the Sclaveni make their first appearance in Byzantine records in the early 6th century. Byzantine historiographers under Justinian I (527–565), such as Procopius of Caesarea, Jordanes and Theophylact Simocatta describe tribes of these names emerging from the area of the Carpathian Mountains, the lower Danube and the Black Sea, invading the Danubian provinces of the Eastern Empire.
Procopius wrote in 545 that "the Sclaveni and the Antae actually had a single name in the remote past; for they were both called Spori in olden times." He describes their social structure and beliefs:
Jordanes tells us that the Sclaveni had swamps and forests for their cities. Another 6th-century source refers to them living among nearly impenetrable forests, rivers, lakes, and marshes.
Menander Protector mentions a Daurentius (577–579) that slew an Avar envoy of Khagan Bayan I. The Avars asked the Slavs to accept the suzerainty of the Avars, he however declined and is reported as saying: "Others do not conquer our land, we conquer theirs – so it shall always be for us".
The relationship between the Slavs and a tribe called the Veneti east of the River Vistula in the Roman period is uncertain. The name may refer both to Balts and Slavs.
According to eastern homeland theory, prior to becoming known to the Roman world, Slavic-speaking tribes were part of the many multi-ethnic confederacies of Eurasia – such as the Sarmatian, Hun and Gothic empires. The Slavs emerged from obscurity when the westward movement of Germans in the 5th and 6th centuries CE (thought to be in conjunction with the movement of peoples from Siberia and Eastern Europe: Huns, and later Avars and Bulgars) started the great migration of the Slavs, who settled the lands abandoned by Germanic tribes fleeing the Huns and their allies: westward into the country between the Oder and the Elbe-Saale line; southward into Bohemia, Moravia, much of present-day Austria, the Pannonian plain and the Balkans; and northward along the upper Dnieper river. Perhaps some Slavs migrated with the movement of the Vandals to Iberia and north Africa.
Around the 6th century, Slavs appeared on Byzantine borders in great numbers.[page needed] The Byzantine records note that grass would not regrow in places where the Slavs had marched through, so great were their numbers. After a military movement even the Peloponnese and Asia Minor were reported to have Slavic settlements. This southern movement has traditionally been seen as an invasive expansion. By the end of the 6th century, Slavs had settled the Eastern Alps regions.
When their migratory movements ended, there appeared among the Slavs the first rudiments of state organizations, each headed by a prince with a treasury and a defense force. Moreover, it was the beginnings of class differentiation, and nobles pledged allegiance either to the Frankish/ Holy Roman Emperors or the Byzantine Emperors.
In the 7th century, the Frankish merchant Samo, who supported the Slavs fighting their Avar rulers, became the ruler of the first known Slav state in Central Europe, which, however, most probably did not outlive its founder and ruler. This provided the foundation for subsequent Slavic states to arise on the former territory of this realm with Carantania being the oldest of them. Very old also are the Principality of Nitra and the Moravian principality (see under Great Moravia). In this period, there existed central Slavic groups and states such as the Balaton Principality, but the subsequent expansion of the Magyars, as well as the Germanisation of Austria, separated the northern and southern Slavs. The First Bulgarian Empire was founded in 681, the Slavic language Old Bulgarian became the main and official of the empire in 864. Bulgaria was instrumental in the spread of Slavic literacy and Christianity to the rest of the Slavic world.
As of 1878, there were only three free Slavic states in the world: the Russian Empire, Serbia and Montenegro. Bulgaria was also free but was de jure vassal to the Ottoman Empire until official independence was declared in 1908. In the entire Austro-Hungarian Empire of approximately 50 million people, about 23 million were Slavs. The Slavic peoples who were, for the most part, denied a voice in the affairs of the Austro-Hungarian Empire, were calling for national self-determination. During World War I, representatives of the Czechs, Slovaks, Poles, Serbs, Croats, and Slovenes set up organizations in the Allied countries to gain sympathy and recognition. In 1918, after World War I ended, the Slavs established such independent states as Czechoslovakia, the Second Polish Republic, and the State of Slovenes, Croats and Serbs.
During World War II, Hitler's Generalplan Ost (general plan for the East) entailed killing, deporting, or enslaving the Slavic and Jewish population of occupied Eastern Europe to create Lebensraum (living space) for German settlers. The Nazi Hunger Plan and Generalplan Ost would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the deaths of an estimated 19.3 million civilians and prisoners of war.
The first half of the 20th century in Russia and the Soviet Union was marked by a succession of wars, famines and other disasters, each accompanied by large-scale population losses. Stephen J. Lee estimates that, by the end of World War II in 1945, the Russian population was about 90 million fewer than it could have been otherwise.
Because of the vastness and diversity of the territory occupied by Slavic people, there were several centers of Slavic consolidation. In the 19th century, Pan-Slavism developed as a movement among intellectuals, scholars, and poets, but it rarely influenced practical politics and did not find support in some nations that had Slavic origins. Pan-Slavism became compromised when the Russian Empire started to use it as an ideology justifying its territorial conquests in Central Europe as well as subjugation of other ethnic groups of Slavic origins such as Poles and Ukrainians, and the ideology became associated with Russian imperialism. The common Slavic experience of communism combined with the repeated usage of the ideology by Soviet propaganda after World War II within the Eastern bloc (Warsaw Pact) was a forced high-level political and economic hegemony of the USSR dominated by Russians. A notable political union of the 20th century that covered most South Slavs was Yugoslavia, but it ultimately broke apart in the 1990s along with the Soviet Union.
The word "Slavs" was used in the national anthem of the Slovak Republic (1939–1945), Yugoslavia (1943–1992) and the Federal Republic of Yugoslavia (1992–2003), later Serbia and Montenegro (2003–2006).
Former Soviet states, as well as countries that used to be satellite states or territories of the Warsaw Pact, have numerous minority Slavic populations, many of whom are originally from the Russian SFSR, Ukrainian SSR and Byelorussian SSR. As of now, Kazakhstan has the largest Slavic minority population with most being Russians (Ukrainians, Belarusians and Poles are present as well but in much smaller numbers).
Pan-Slavism, a movement which came into prominence in the mid-19th century, emphasized the common heritage and unity of all the Slavic peoples. The main focus was in the Balkans where the South Slavs had been ruled for centuries by other empires: the Byzantine Empire, Austria-Hungary, the Ottoman Empire, and Venice. The Russian Empire used Pan-Slavism as a political tool; as did the Soviet Union, which gained political-military influence and control over most Slavic-majority nations between 1945 and 1948 and retained a hegemonic role until the period 1989–1991.
Slavic studies began as an almost exclusively linguistic and philological enterprise. As early as 1833, Slavic languages were recognized as Indo-European.
Slavic standard languages which are official in at least one country: Belarusian, Bosnian, Bulgarian, Croatian, Czech, Macedonian, Montenegrin, Polish, Russian, Serbian, Slovak, Slovene, and Ukrainian. The alphabet depends on what religion is usual for the respective Slavic ethnic groups. The Orthodox use the Cyrillic alphabet and the Roman Catholics use Latin alphabet, the Bosniaks who are Muslims also use the Latin. Few Greek Roman and Roman Catholics use the Cyrillic alphabet however. The Serbian language and Montenegrin language uses both Cyrillic and Latin alphabets. There is also a Latin script to write in Belarusian, called the Lacinka alphabet.
Proto-Slavic, the supposed ancestor language of all Slavic languages, is a descendant of common Proto-Indo-European, via a Balto-Slavic stage in which it developed numerous lexical and morphophonological isoglosses with the Baltic languages. In the framework of the Kurgan hypothesis, "the Indo-Europeans who remained after the migrations [from the steppe] became speakers of Balto-Slavic".
Proto-Slavic, sometimes referred to as Common Slavic or Late Proto-Slavic, is defined as the last stage of the language preceding the geographical split of the historical Slavic languages. That language was uniform, and on the basis of borrowings from foreign languages and Slavic borrowings into other languages, cannot be said to have any recognizable dialects, suggesting a comparatively compact homeland. Slavic linguistic unity was to some extent visible as late as Old Church Slavonic manuscripts which, though based on local Slavic speech of Thessaloniki, could still serve the purpose of the first common Slavic literary language.
The pagan Slavic populations were Christianized between the 6th and 10th centuries. Orthodox Christianity is predominant in the East and South Slavs, while Roman Catholicism is predominant in West Slavs and the western South Slavs. The religious borders are largely comparable to the East–West Schism which began in the 11th century. The majority of contemporary Slavic populations who profess a religion are Orthodox, followed by Catholic, while a small minority are Protestant. There are minor Slavic Muslim groups. Religious delineations by nationality can be very sharp; usually in the Slavic ethnic groups the vast majority of religious people share the same religion. Some Slavs are atheist or agnostic: only 19% of Czechs professed belief in god/s in the 2005 Eurobarometer survey.
Slavs are customarily divided along geographical lines into three major subgroups: West Slavs, East Slavs, and South Slavs, each with a different and a diverse background based on unique history, religion and culture of particular Slavic groups within them. Apart from prehistorical archaeological cultures, the subgroups have had notable cultural contact with non-Slavic Bronze- and Iron Age civilisations.
^1 Also considered part of Rusyns
^2 Considered transitional between Ukrainians and Belarusians
^3 The ethnic affiliation of the Lemkos has become an ideological conflict. It has been alleged that among the Lemkos the idea of "Carpatho-Ruthenian" nation is supported only by Lemkos residing in Transcarpathia and abroad
^4 Most inhabitants of historic Moravia considered themselves as Czechs but significant amount declared their Moravian nationality, different from that Czech (although people from Bohemia and Moravia use the same official language).
^5 Also considered Poles.
^6 There are sources that show Silesians as part of the Poles. Parts of the southmost population of Upper Silesia is sometimes considered Czech (controversial).
^7 A census category recognized as an ethnic group. Most Slavic Muslims (especially in Bosnia, Croatia, Montenegro and Serbia) now opt for Bosniak ethnicity, but some still use the "Muslim" designation. Bosniak and Muslim are considered two ethnonyms for a single ethnicity and the terms may even be used interchangeably. However, a small number of people within Bosnia and Herzegovina declare themselves Bosniak but are not necessarily Muslim by faith.
^8 This identity continues to be used by a minority throughout the former Yugoslav republics. The nationality is also declared by diasporans living in the USA and Canada. There are a multitude of reasons as to why people prefer this affiliation, some published on the article.
^9 Sub-groups of Croats include Bunjevci (in Bačka), Šokci (in Slavonia and Vojvodina), Janjevci (in Kosovo), Burgenland Croats (in Austria), Bosniaks (in Hungary), Molise Croats (in Italy), Krashovans (in Romania), Moravian Croats (in the Czech Republic)
^10 Sub-groups of Slovenes include Prekmurians, Hungarian Slovenes, Carinthian Slovenes, Venetian Slovenes, Resians, and the extinct Carantanians and Somogy Slovenes.
Note: Besides ethnic groups, Slavs often identify themselves with the local geographical region in which they live. Some of the major regional South Slavic groups include: Zagorci in northern Croatia, Istrijani in westernmost Croatia, Dalmatinci in southern Croatia, Boduli in Adriatic islands, Vlaji in hinterland of Dalmatia, Slavonci in eastern Croatia, Bosanci in Bosnia, Hercegovci in Herzegovina, Krajišnici in western Bosnia, but is more commonly used to refer to the Serbs of Croatia, most of whom are descendants of the Grenzers, and continued to live in the area which made up the Military Frontier until the Croatian war of independence, Semberci in northeast Bosnia, Srbijanci in Serbia proper, Šumadinci in central Serbia, Vojvođani in northern Serbia, Sremci in Syrmia, Bačvani in northwest Vojvodina, Banaćani in Banat, Sandžaklije (Muslims in Serbia/Montenegro border), Kosovci in Kosovo, Bokelji in southwest Montenegro, Trakiytsi in Upper Thracian Lowlands, Dobrudzhantsi in north-east Bulgarian region, Balkandzhii in Central Balkan Mountains, Miziytsi in north Bulgarian region, Warmiaks and Masurians in north-east Polish regions Warmia and Mazuria, Pirintsi in Blagoevgrad Province, Ruptsi in the Rhodopes etc.
The modern Slavic peoples carry a variety of mitochondrial DNA haplogroups and Y-chromosome DNA haplogroups. Yet two paternal haplogroups predominate: R1a1a [M17] and I2a2a [L69.2=T/S163.2]. The frequency of Haplogroup R1a ranges from 63.39% in the Sorbs, through 56.4% in Poland, 54% in Ukraine, 52% in Russia, Belarus, to 15.2% in Republic of Macedonia, 14.7% in Bulgaria and 12.1% in Herzegovina. The correlation between R1a1a [M17] and the speakers of Indo-European languages, particularly those of Eastern Europe (Russian) and Central and Southern Asia, was noticed in the late 1990s. From this Spencer Wells and colleagues, following the Kurgan hypothesis, deduced that R1a1a arose on the Pontic-Caspian steppe.
Specific studies of Slavic genetics followed. In 2007 Rębała and colleagues studied several Slavic populations with the aim of localizing the Proto-Slavic homeland. The significant findings of this study are that:
Marcin Woźniak and colleagues (2010) searched for specifically Slavic sub-group of R1a1a [M17]. Working with haplotypes, they found a pattern among Western Slavs which turned out to correspond to a newly discovered marker, M458, which defines subclade R1a1a7. This marker correlates remarkably well with the distribution of Slavic-speakers today. The team led by Peter Underhill, which discovered M458, did not consider the possibility that this was a Slavic marker, since they used the "evolutionary effective" mutation rate, which gave a date far too old to be Slavic. Woźniak and colleagues pointed out that the pedigree mutation rate, giving a later date, is more consistent with the archaeological record.
Pomors are distinguished by the presence of Y Haplogroup N among them. Postulated to originate from southeast Asia, it is found at high rates in Uralic peoples. Its presence in Pomors (called "Northern Russians" in the report) attests to the non-Slavic tribes (mixing with Finnic tribes of northern Eurasia). Autosomally, Russians are generally similar to populations in central-eastern Europe but some northern Russians are intermediate to Finno-Ugric groups.
On the other hand, I2a1b1 (P41.2) is typical of the South Slavic populations, being highest in Bosnia-Herzegovina (>50%). Haplogroup I2a2 is also commonly found in north-eastern Italians. There is also a high concentration of I2a2a in the Moldavian region of Romania, Moldova and western Ukraine. According to original studies, Hg I2a2 was believed to have arisen in the west Balkans sometime after the LGM, subsequently spreading from the Balkans through Central Russian Plain. Recently, Ken Nordtvedt has split I2a2 into two clades – N (northern) and S (southern), in relation where they arose compared to Danube river. He proposes that N is slightly older than S. He recalculated the age of I2a2 to be ~ 2550 years and proposed that the current distribution is explained by a Slavic expansion from the area north-east of the Carpathians.
In 2008, biochemist Boris Arkadievich Malyarchuk (Russian: Борис Аркадьевич Малярчук) et al. of the Institute of Biological Problems of the North, Russian Academy of Sciences, Magadan, Russia, used a sample (n=279) of Czech individuals to determine the frequency of "Mongoloid" "mtDNA lineages". Malyarchuk found Czech mtDNA lineages were typical of "Slavic populations" with "1.8%" Mongoloid mtDNA lineage. Malyarchuk added that "Slavic populations" "almost always" contain Mongoloid mtDNA lineage. Malyarchuk said the Mongoloid component of Slavic people was partially added before the split of "Balto-Slavics" in 2,000–3,000 BC with additional Mongoloid mixture occurring among Slavics in the last 4,000 years. Malyarchuk said the "Russian population" was developed by the "assimilation of the indigenous pre-Slavic population of Eastern Europe by true Slavs" with additional "assimilation of Finno-Ugric populations" and "long-lasting" interactions with the populations of "Siberia" and "Central Asia". Malyarchuk said that other Slavs "Mongoloid component" was increased during the waves of migration from "steppe populations (Huns, Avars, Bulgars and Mongols)", especially the decay of the "Avar Khaganate".
DNA samples from 1228 Russians show that the Y chromosomes analyzed, all except 20 (1.6%) fall into seven major haplogroups all characteristic to West Eurasian populations. Taken together, they account for 95% of the total Russian Y chromosomal pool. Only (0.7%) fell into haplogroups that are specific to East and South Asian populations. Mitochondrial DNA (mtDNA) examined in Poles and Russians revealed the presence of all major European haplogroups, which were characterized by similar patterns of distribution in Poles and Russians. An analysis of the DNA did not reveal any specific combinations of unique mtDNA haplotypes and their subclusters. The DNA clearly shows that both Poles and Russians are not different from the neighbouring European populations.
Throughout their history, Slavs came into contact with non-Slavic groups. In the postulated homeland region (present-day Ukraine), they had contacts with the Iranic Sarmatians and the Germanic Goths. After their subsequent spread, they began assimilating non-Slavic peoples. For example, in the Balkans, there were Paleo-Balkan peoples, such as Romanized and Hellenized (Jireček Line) Illyrians, Thracians and Dacians, as well as Greeks and Celtic Scordisci. Over time, due to the larger number of Slavs, most descendants of the indigenous populations of the Balkans were Slavicized. The Thracians and Illyrians vanished from the population during this period – although the modern Albanian nation claims descent from the Illyrians. Exceptions are Greece, where the lesser numbered Slavs scattered there came to be Hellenized (aided in time by more Greeks returning to Greece in the 9th century and the role of the church and administration) and Romania where Slavic people settled en route for present-day Greece, Republic of Macedonia, Bulgaria and East Thrace whereby the Slavic population had come to assimilate. Bulgars were also assimilated by local Slavs but their ruling status and subsequent land cast the nominal legacy of Bulgarian country and people onto all future generations. The Romance speakers within the fortified Dalmatian cities managed to retain their culture and language for a long time, as Dalmatian Romance was spoken until the high Middle Ages. However, they too were eventually assimilated into the body of Slavs.
In the Western Balkans, South Slavs and Germanic Gepids intermarried with Avar invaders, eventually producing a Slavicized population.[citation needed] In Central Europe, the Slavs intermixed with Germanic and Celtic, while the eastern Slavs encountered Uralic and Scandinavian peoples. Scandinavians (Varangians) and Finnic peoples were involved in the early formation of the Rus' state but were completely Slavicized after a century. Some Finno-Ugric tribes in the north were also absorbed into the expanding Rus population. At the time of the Magyar migration, the present-day Hungary was inhabited by Slavs, numbering about 200,000, and by Romano-Dacians who were either assimilated or enslaved by the Magyars. In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchaks and the Pechenegs, caused a massive migration of East Slavic populations to the safer, heavily forested regions of the north. In the Middle Ages, groups of Saxon ore miners settled in medieval Bosnia, Serbia and Bulgaria where they were Slavicized.
Polabian Slavs (Wends) settled in parts of England (Danelaw), apparently as Danish allies. Polabian-Pomeranian Slavs are also known to have even settled on Norse age Iceland. Saqaliba refers to the Slavic mercenaries and slaves in the medieval Arab world in North Africa, Sicily and Al-Andalus. Saqaliba served as caliph's guards. In the 12th century, there was intensification of Slavic piracy in the Baltics. The Wendish Crusade was started against the Polabian Slavs in 1147, as a part of the Northern Crusades. Niklot, pagan chief of the Slavic Obodrites, began his open resistance when Lothar III, Holy Roman Emperor, invaded Slavic lands. In August 1160 Niklot was killed and German colonization (Ostsiedlung) of the Elbe-Oder region began. In Hanoverian Wendland, Mecklenburg-Vorpommern and Lusatia invaders started germanization. Early forms of germanization were described by German monks: Helmold in the manuscript Chronicon Slavorum and Adam of Bremen in Gesta Hammaburgensis ecclesiae pontificum. The Polabian language survived until the beginning of the 19th century in what is now the German state of Lower Saxony. In Eastern Germany, around 20% of Germans have Slavic paternal ancestry. Similarly, in Germany, around 20% of the foreign surnames are of Slavic origin.
Cossacks, although Slavic-speaking and Orthodox Christians, came from a mix of ethnic backgrounds, including Tatars and other Turks. Many early members of the Terek Cossacks were Ossetians.
The Gorals of southern Poland and northern Slovakia are partially descended from Romance-speaking Vlachs who migrated into the region from the 14th to 17th centuries and were absorbed into the local population. The population of Moravian Wallachia also descend of this population.
Conversely, some Slavs were assimilated into other populations. Although the majority continued south, attracted by the riches of the territory which would become Bulgaria, a few remained in the Carpathian basin and were ultimately assimilated into the Magyar or Romanian population. There is a large number of river names and other placenames of Slavic origin in Romania.[better source needed]
An exhibition game (also known as a friendly, a scrimmage, a demonstration, a preseason game, a warmup match, or a preparation match, depending at least in part on the sport) is a sporting event whose prize money and impact on the player's or the team's rankings is either zero or otherwise greatly reduced. In team sports, matches of this type are often used to help coaches and managers select players for the competitive matches of a league season or tournament. If the players usually play in different teams in other leagues, exhibition games offer an opportunity for the players to learn to work with each other. The games can be held between separate teams or between parts of the same team.
An exhibition game may also be used to settle a challenge, to provide professional entertainment, to promote the sport, or to raise money for charities. Several sports leagues hold all-star games to showcase their best players against each other, while other exhibitions games may pit participants from two different leagues or countries to unofficially determine who would be the best in the world. International competitions like the Olympic Games may also hold exhibition games as part of a demonstration sport.
In the early days of association football, known simply as football or soccer, friendly matches (or "friendlies") were the most common type of match. However, since the development of The Football League in England in 1888, league tournaments became established, in addition to lengthy derby and cup tournaments. By the year 2000, national leagues were established in almost every country throughout the world, as well as local or regional leagues for lower level teams, thus the significance of friendlies has seriously declined since the 19th century.
Since the introduction of league football, most club sides play a number of friendlies before the start of each season (called pre-season friendlies). Friendly football matches are considered to be non-competitive and are only used to "warm up" players for a new season/competitive match. There is generally nothing competitive at stake and some rules may be changed or experimented with (such as unlimited substitutions, which allow teams to play younger, less experienced, players, and no cards). Although most friendlies are simply one-off matches arranged by the clubs themselves, in which a certain amount is paid by the challenger club to the incumbent club, some teams do compete in short tournaments, such as the Emirates Cup, Teresa Herrera Trophy and the Amsterdam Tournament. Although these events may involve sponsorship deals and the awarding of a trophy and may even be broadcast on television, there is little prestige attached to them.
International teams also play friendlies, generally in preparation for the qualifying or final stages of major tournaments. This is essential, since national squads generally have much less time together in which to prepare. The biggest difference between friendlies at the club and international levels is that international friendlies mostly take place during club league seasons, not between them. This has on occasion led to disagreement between national associations and clubs as to the availability of players, who could become injured or fatigued in a friendly.
International friendlies give team managers the opportunity to experiment with team selection and tactics before the tournament proper, and also allow them to assess the abilities of players they may potentially select for the tournament squad. Players can be booked in international friendlies, and can be suspended from future international matches based on red cards or accumulated yellows in a specified period. Caps and goals scored also count towards a player's career records. In 2004, FIFA ruled that substitutions by a team be limited to six per match in international friendlies, in response to criticism that such matches were becoming increasingly farcical with managers making as many as 11 substitutions per match.
In the UK and Ireland, "exhibition match" and "friendly match" refer to two different types of matches. The types described above as friendlies are not termed exhibition matches, while annual all-star matches such as those held in the US Major League Soccer or Japan's Japanese League are called exhibition matches rather than friendly matches. A one-off match for charitable fundraising, usually involving one or two all-star teams, or a match held in honor of a player for contribution to his/her club, may also be described as exhibition matches but they are normally referred to as charity matches and testimonial matches respectively.
Under the 1995–2004 National Hockey League collective bargaining agreement, teams were limited to nine preseason games. From 1975 to 1991, NHL teams sometimes played exhibition games against teams from the Soviet Union in the Super Series, and in 1978, played against World Hockey Association teams also in preseason training. Like the NFL, the NHL sometimes schedules exhibition games for cities without their own NHL teams, often at a club's minor league affiliate (e.g. Carolina Hurricanes games at Time Warner Cable Arena in Charlotte, home of their AHL affiliate; Los Angeles Kings games at Citizens Business Bank Arena in Ontario, California, home of their ECHL affiliate; Montreal Canadiens games at Colisée Pepsi in Quebec City, which has no pro hockey but used to have an NHL team until 1995; Washington Capitals at 1st Mariner Arena in the Baltimore Hockey Classic; various Western Canada teams at Credit Union Centre in Saskatoon, a potential NHL expansion venue). Since the 2000s, some preseason games have been played in Europe against European teams, as part of the NHL Challenge and NHL Premiere series. In addition to the standard preseason, there also exist prospect tournaments such as the Vancouver Canucks' YoungStars tournament and the Detroit Red Wings' training camp, in which NHL teams' younger prospects face off against each other under their parent club's banner.
The Flying Fathers, a Canadian group of Catholic priests, regularly toured North America playing exhibition hockey games for charity. One of the organization's founders, Les Costello, was a onetime NHL player who was ordained as a priest after retiring from professional hockey. Another prominent exhibition hockey team is the Buffalo Sabres Alumni Hockey Team, which is composed almost entirely of retired NHL players, the majority of whom (as the name suggests) played at least a portion of their career for the Buffalo Sabres.
The Major League Baseball's preseason is also known as spring training. All MLB teams maintain a spring-training base in Arizona or Florida. The teams in Arizona make up the Cactus League, while the teams in Florida play in the Grapefruit League. Each team plays about 30 preseason games against other MLB teams. They may also play exhibitions against a local college team or a minor-league team from their farm system. Some days feature the team playing two games with two different rosters evenly divided up, which are known as "split-squad" games.
Several MLB teams used to play regular exhibition games during the year against nearby teams in the other major league, but regular-season interleague play has made such games unnecessary. The two Canadian MLB teams, the Toronto Blue Jays of the American League and the Montreal Expos of the National League, met annually to play the Pearson Cup exhibition game; this tradition ended when the Expos moved to Washington DC for the 2005 season. Similarly, the New York Yankees played in the Mayor's Trophy Game against various local rivals from 1946 to 1983.
It also used to be commonplace to have a team play an exhibition against Minor League affiliates during the regular season, but worries of injuries to players, along with travel issues, have made this very rare. Exhibitions between inter-city teams in different leagues, like Chicago's Crosstown Classic and New York's Subway Series which used to be played solely as exhibitions for bragging rights are now blended into interleague play. The annual MLB All-Star Game, played in July between players from AL teams and players from NL teams, was long considered an exhibition match, but as of 2003 this status was questioned because the league whose team wins the All-Star game has been awarded home field advantage for the upcoming World Series.
National Basketball Association teams play eight preseason games per year. Today, NBA teams almost always play each other in the preseason, but mainly at neutral sites within their market areas in order to allow those who can't usually make a trip to a home team's arena during the regular season to see a game close to home; for instance the Minnesota Timberwolves will play games in arenas in North Dakota and South Dakota, while the Phoenix Suns schedule one exhibition game outdoors at Indian Wells Tennis Garden in Indian Wells, California yearly, the only such instance an NBA game takes place in an outdoor venue.
However, from 1971 to 1975, NBA teams played preseason exhibitions against American Basketball Association teams. In the early days of the NBA, league clubs sometimes challenged the legendary barnstorming Harlem Globetrotters, with mixed success. The NBA has played preseason games in Europe and Asia. In the 2006 and 2007 seasons, the NBA and the primary European club competition, the Euroleague, conducted a preseason tournament featuring two NBA teams and the finalists from that year's Euroleague.[citation needed] In the 1998-99 and 2011-12 seasons, teams were limited to only two preseason games due to lockouts.
Traditionally, major college basketball teams began their seasons with a few exhibition games. They played travelling teams made up of former college players on teams such as Athletes in Action or a team sponsored by Marathon Oil. On occasion before 1992, when FIBA allowed professional players on foreign national teams, colleges played those teams in exhibitions. However, in 2003, the National Collegiate Athletic Association banned games with non-college teams. Some teams have begun scheduling exhibition games against teams in NCAA Division II and NCAA Division III, or even against colleges and universities located in Canada. Major college basketball teams still travel to other countries during the summer to play in exhibition games, although a college team is allowed one foreign tour every four years, and a maximum of ten games in each tour.
Compared to other team sports, the National Football League preseason is very structured. Every NFL team plays exactly four pre-season exhibition games a year, two at home and two away, with the exception of two teams each year who play a fifth game, the Pro Football Hall of Fame Game. These exhibition games, most of which are held in the month of August, are played for the purpose of helping coaches narrow down the roster from the offseason limit of 90 players to the regular-season limit of 53 players. While the scheduling formula is not as rigid for preseason games as they are for the regular season, there are numerous restrictions and traditions that limit the choices of preseason opponents; teams are also restricted on what days and times they can play these games. Split-squad games, a practice common in baseball and hockey, where a team that is scheduled to play two games on the same day splits their team into two squads, are prohibited. The NFL has played exhibition games in Europe, Japan, Canada, Australia (including the American Bowl in 1999) and Mexico to spread the league's popularity (a game of this type was proposed for China but, due to financial and logistical problems, was eventually canceled). The league has tacitly forbidden the playing of non-league opponents, with the last interleague game having come in 1972 and the last game against a team other than an NFL team (the all-NFL rookie College All-Stars) was held in 1976. Exhibition games are quite unpopular with many fans, who resent having to pay regular-season prices for two home exhibition games as part of a season-ticket package. Numerous lawsuits have been brought by fans and classes of fans against the NFL or its member teams regarding this practice, but none have been successful in halting it.[citation needed] The Pro Bowl, traditionally played after the end of the NFL season (since 2011 is played the week prior to the Super Bowl), is also considered an exhibition game.
The Arena Football League briefly had a two-game exhibition season in the early 2000s, a practice that ended in 2003 with a new television contract. Exhibition games outside of a structured season are relatively common among indoor American football leagues; because teams switch leagues frequently at that level of play, it is not uncommon to see some of the smaller leagues schedule exhibition games against teams that are from another league, about to join the league as a probational franchise, or a semi-pro outdoor team to fill holes in a schedule.
True exhibition games between opposing colleges at the highest level do not exist in college football; due to the importance of opinion polling in the top level of college football, even exhibition games would not truly be exhibitions because they could influence the opinions of those polled. Intramural games are possible because a team playing against itself leaves little ability for poll participants to make judgments, and at levels below the Football Bowl Subdivision (FBS), championships are decided by objective formulas and thus those teams can play non-league games without affecting their playoff hopes.
However, most of the major FBS teams annually schedule early season non-conference preseason home games against lesser opponents that are lower-tier FBS, Football Championship, or Division II schools, which often result in lopsided victories in favor of the FBS teams and act as exhibition games in all but name, though they additionally provide a large appearance fee and at least one guaranteed television appearance for the smaller school. These games also receive the same criticism as NFL exhibition games, but instead it is targeted to schools scheduling low-quality opponents and the simplicity for a team to run up the score against a weak opponent. However, these games are susceptible to backfiring, resulting in damage in poll position and public perception, especially if the higher ranked team loses, although the mere act of scheduling a weak opponent is harmful to a team's overall strength of schedule itself. Games an FBS team schedules against lower division opponents do not count toward the minimum seven wins required for bowl eligibility, and only one game against an FCS team can be counted. With the start of the College Football Playoff system for the 2014 season, major teams are now discouraged from scheduling weaker opponents for their non-conference schedule because of a much higher emphasis on strength of schedule than in the Bowl Championship Series era.
High school football teams frequently participate in controlled scrimmages with other teams during preseason practice, but exhibition games are rare because of league rules and concerns about finances, travel and player injuries, along with enrollments not being registered until the early part of August in most school districts under the traditional September–June academic term. A more common exhibition is the high school football all-star game, which brings together top players from a region. These games are typically played by graduating seniors during the summer or at the end of the season. Many of these games, which include the U.S. Army All-American Bowl and Under Armour All-America Game, are used as showcases for players to be seen by colleges.
Various auto racing organizations hold exhibition events; these events usually award no championship points to participants, but they do offer prize money to participants. The NASCAR Sprint Cup Series holds two exhibition events annually - the Sprint Unlimited, held at Daytona International Speedway at the start of the season, and the NASCAR Sprint All-Star Race, held at Charlotte Motor Speedway midway through the season. Both events carry a hefty purse of over USD $1,000,000. NASCAR has also held exhibition races at Suzuka Circuit and Twin Ring Motegi in Japan and Calder Park Thunderdome in Australia.
Armenians constitute the main population of Armenia and the de facto independent Nagorno-Karabakh Republic. There is a wide-ranging diaspora of around 5 million people of full or partial Armenian ancestry living outside of modern Armenia. The largest Armenian populations today exist in Russia, the United States, France, Georgia, Iran, Ukraine, Lebanon, and Syria. With the exceptions of Iran and the former Soviet states, the present-day Armenian diaspora was formed mainly as a result of the Armenian Genocide.
Historically, the name Armenian has come to internationally designate this group of people. It was first used by neighbouring countries of ancient Armenia. The earliest attestations of the exonym Armenia date around the 6th century BC. In his trilingual Behistun Inscription dated to 517 BC, Darius I the Great of Persia refers to Urashtu (in Babylonian) as Armina (in Old Persian; Armina (    ) and Harminuya (in Elamite). In Greek, Αρμένιοι "Armenians" is attested from about the same time, perhaps the earliest reference being a fragment attributed to Hecataeus of Miletus (476 BC). Xenophon, a Greek general serving in some of the Persian expeditions, describes many aspects of Armenian village life and hospitality in around 401 BC. He relates that the people spoke a language that to his ear sounded like the language of the Persians.
The Armenian Highland lies in the highlands surrounding Mount Ararat, the highest peak of the region. In the Bronze Age, several states flourished in the area of Greater Armenia, including the Hittite Empire (at the height of its power), Mitanni (South-Western historical Armenia), and Hayasa-Azzi (1600–1200 BC). Soon after Hayasa-Azzi were Arme-Shupria (1300s–1190 BC), the Nairi (1400–1000 BC) and the Kingdom of Urartu (860–590 BC), who successively established their sovereignty over the Armenian Highland. Each of the aforementioned nations and tribes participated in the ethnogenesis of the Armenian people. Under Ashurbanipal (669–627 BC), the Assyrian empire reached the Caucasus Mountains (modern Armenia, Georgia and Azerbaijan).
Eric P. Hamp in his 2012 Indo-European family tree, groups the Armenian language along with Greek and Ancient Macedonian ("Helleno-Macedonian") in the Pontic Indo-European (also called Helleno-Armenian) subgroup. In Hamp's view the homeland of this subgroup is the northeast coast of the Black Sea and its hinterlands. He assumes that they migrated from there southeast through the Caucasus with the Armenians remaining after Batumi while the pre-Greeks proceeded westwards along the southern coast of the Black Sea.
The first geographical entity that was called Armenia by neighboring peoples (such as by Hecataeus of Miletus and on the Achaemenid Behistun Inscription) was established in the late 6th century BC under the Orontid dynasty within the Achaemenid Persian Empire as part of the latters' territories, and which later became a kingdom. At its zenith (95–65 BC), the state extended from the Caucasus all the way to what is now central Turkey, Lebanon, and northern Iran. The imperial reign of Tigranes the Great is thus the span of time during which Armenia itself conquered areas populated by other peoples.
The Arsacid Kingdom of Armenia, itself a branch of the Arsacid dynasty of Parthia, was the first state to adopt Christianity as its religion (it had formerly been adherent to Armenian paganism, which was influenced by Zoroastrianism, while later on adopting a few elements regarding identification of its pantheon with Greco-Roman deities). in the early years of the 4th century, likely AD 301, partly in defiance of the Sassanids it seems. In the late Parthian period, Armenia was a predominantly Zoroastrian-adhering land, but by the Christianisation, previously predominant Zoroastrianism and paganism in Armenia gradually declined. Later on, in order to further strengthen Armenian national identity, Mesrop Mashtots invented the Armenian alphabet, in 405 AD. This event ushered the Golden Age of Armenia, during which many foreign books and manuscripts were translated to Armenian by Mesrop's pupils. Armenia lost its sovereignty again in 428 AD to the rivalling Byzantine and Sassanid Persian empires, until the Muslim conquest of Persia overran also the regions in which Armenians lived.
In 885 AD the Armenians reestablished themselves as a sovereign kingdom under the leadership of Ashot I of the Bagratid Dynasty. A considerable portion of the Armenian nobility and peasantry fled the Byzantine occupation of Bagratid Armenia in 1045, and the subsequent invasion of the region by Seljuk Turks in 1064. They settled in large numbers in Cilicia, an Anatolian region where Armenians were already established as a minority since Roman times. In 1080, they founded an independent Armenian Principality then Kingdom of Cilicia, which became the focus of Armenian nationalism. The Armenians developed close social, cultural, military, and religious ties with nearby Crusader States, but eventually succumbed to Mamluk invasions. In the next few centuries, Djenghis Khan, Timurids, and the tribal Turkic federations of the Ak Koyunlu and the Kara Koyunlu ruled over the Armenians.
From the early 16th century, both Western Armenia and Eastern Armenia fell under Iranian Safavid rule. Owing to the century long Turco-Iranian geo-political rivalry that would last in Western Asia, significant parts of the region were frequently fought over between the two rivalling empires. From the mid 16th century with the Peace of Amasya, and decisively from the first half of the 17th century with the Treaty of Zuhab until the first half of the 19th century, Eastern Armenia was ruled by the successive Iranian Safavid, Afsharid and Qajar empires, while Western Armenia remained under Ottoman rule. In the late 1820s, the parts of historic Armenia under Iranian control centering on Yerevan and Lake Sevan (all of Eastern Armenia) were incorporated into the Russian Empire following Iran's forced ceding of the territories after its loss in the Russo-Persian War (1826-1828) and the outcoming Treaty of Turkmenchay. Western Armenia however, remained in Ottoman hands.
Governments of Republic of Turkey since that time have consistently rejected charges of genocide, typically arguing either that those Armenians who died were simply in the way of a war or that killings of Armenians were justified by their individual or collective support for the enemies of the Ottoman Empire. Passage of legislation in various foreign countries condemning the persecution of the Armenians as genocide has often provoked diplomatic conflict. (See Recognition of the Armenian Genocide)
Following the breakup of the Russian Empire in the aftermath of World War I for a brief period, from 1918 to 1920, Armenia was an independent republic. In late 1920, the communists came to power following an invasion of Armenia by the Red Army, and in 1922, Armenia became part of the Transcaucasian SFSR of the Soviet Union, later forming the Armenian Soviet Socialist Republic (1936 to September 21, 1991). In 1991, Armenia declared independence from the USSR and established the second Republic of Armenia.
Armenians have had a presence in the Armenian Highland for over four thousand years, since the time when Hayk, the legendary patriarch and founder of the first Armenian nation, led them to victory over Bel of Babylon. Today, with a population of 3.5 million, they not only constitute an overwhelming majority in Armenia, but also in the disputed region of Nagorno-Karabakh. Armenians in the diaspora informally refer to them as Hayastantsis (Հայաստանցի), meaning those that are from Armenia (that is, those born and raised in Armenia). They, as well as the Armenians of Iran and Russia speak the Eastern dialect of the Armenian language. The country itself is secular as a result of Soviet domination, but most of its citizens identify themselves as Apostolic Armenian Christian.
Small Armenian trading and religious communities have existed outside of Armenia for centuries. For example, a community has existed for over a millennium in the Holy Land, and one of the four quarters of the walled Old City of Jerusalem has been called the Armenian Quarter. An Armenian Catholic monastic community of 35 founded in 1717 exists on an island near Venice, Italy. There are also remnants of formerly populous communities in India, Myanmar, Thailand, Belgium, Portugal, Italy, Poland, Austria, Hungary, Bulgaria, Romania, Serbia, Ethiopia, Sudan and Egypt.[citation needed]
Within the diasporan Armenian community, there is an unofficial classification of the different kinds of Armenians. For example, Armenians who originate from Iran are referred to as Parskahay (Պարսկահայ), while Armenians from Lebanon are usually referred to as Lipananahay (Լիբանանահայ). Armenians of the Diaspora are the primary speakers of the Western dialect of the Armenian language. This dialect has considerable differences with Eastern Armenian, but speakers of either of the two variations can usually understand each other. Eastern Armenian in the diaspora is primarily spoken in Iran and European countries such as Ukraine, Russia, and Georgia (where they form a majority in the Samtskhe-Javakheti province). In diverse communities (such as in Canada and the U.S.) where many different kinds of Armenians live together, there is a tendency for the different groups to cluster together.
Armenia established a Church that still exists independently of both the Catholic and the Eastern Orthodox churches, having become so in 451 AD as a result of its stance regarding the Council of Chalcedon. Today this church is known as the Armenian Apostolic Church, which is a part of the Oriental Orthodox communion, not to be confused with the Eastern Orthodox communion. During its later political eclipses, Armenia depended on the church to preserve and protect its unique identity. The original location of the Armenian Catholicosate is Echmiadzin. However, the continuous upheavals, which characterized the political scenes of Armenia, made the political power move to safer places. The Church center moved as well to different locations together with the political authority. Therefore, it eventually moved to Cilicia as the Holy See of Cilicia.
The Armenians collective has, at times, constituted a Christian "island" in a mostly Muslim region. There is, however, a minority of ethnic Armenian Muslims, known as Hamshenis but many Armenians view them as a separate race, while the history of the Jews in Armenia dates back 2,000 years. The Armenian Kingdom of Cilicia had close ties to European Crusader States. Later on, the deteriorating situation in the region led the bishops of Armenia to elect a Catholicos in Etchmiadzin, the original seat of the Catholicosate. In 1441, a new Catholicos was elected in Etchmiadzin in the person of Kirakos Virapetsi, while Krikor Moussapegiants preserved his title as Catholicos of Cilicia. Therefore, since 1441, there have been two Catholicosates in the Armenian Church with equal rights and privileges, and with their respective jurisdictions. The primacy of honor of the Catholicosate of Etchmiadzin has always been recognized by the Catholicosate of Cilicia.
While the Armenian Apostolic Church remains the most prominent church in the Armenian community throughout the world, Armenians (especially in the diaspora) subscribe to any number of other Christian denominations. These include the Armenian Catholic Church (which follows its own liturgy but recognizes the Roman Catholic Pope), the Armenian Evangelical Church, which started as a reformation in the Mother church but later broke away, and the Armenian Brotherhood Church, which was born in the Armenian Evangelical Church, but later broke apart from it. There are other numerous Armenian churches belonging to Protestant denominations of all kinds.
Armenian literature dates back to 400 AD, when Mesrop Mashtots first invented the Armenian alphabet. This period of time is often viewed as the Golden Age of Armenian literature. Early Armenian literature was written by the "father of Armenian history", Moses of Chorene, who authored The History of Armenia. The book covers the time-frame from the formation of the Armenian people to the fifth century AD. The nineteenth century beheld a great literary movement that was to give rise to modern Armenian literature. This period of time, during which Armenian culture flourished, is known as the Revival period (Zartonki sherchan). The Revivalist authors of Constantinople and Tiflis, almost identical to the Romanticists of Europe, were interested in encouraging Armenian nationalism. Most of them adopted the newly created Eastern or Western variants of the Armenian language depending on the targeted audience, and preferred them over classical Armenian (grabar). This period ended after the Hamidian massacres, when Armenians experienced turbulent times. As Armenian history of the 1920s and of the Genocide came to be more openly discussed, writers like Paruyr Sevak, Gevork Emin, Silva Kaputikyan and Hovhannes Shiraz began a new era of literature.
The first Armenian churches were built between the 4th and 7th century, beginning when Armenia converted to Christianity, and ending with the Arab invasion of Armenia. The early churches were mostly simple basilicas, but some with side apses. By the fifth century the typical cupola cone in the center had become widely used. By the seventh century, centrally planned churches had been built and a more complicated niched buttress and radiating Hrip'simé style had formed. By the time of the Arab invasion, most of what we now know as classical Armenian architecture had formed.
From the 9th to 11th century, Armenian architecture underwent a revival under the patronage of the Bagratid Dynasty with a great deal of building done in the area of Lake Van, this included both traditional styles and new innovations. Ornately carved Armenian Khachkars were developed during this time. Many new cities and churches were built during this time, including a new capital at Lake Van and a new Cathedral on Akdamar Island to match. The Cathedral of Ani was also completed during this dynasty. It was during this time that the first major monasteries, such as Haghpat and Haritchavank were built. This period was ended by the Seljuk invasion.
During Soviet rule, Armenian athletes rose to prominence winning plenty of medals and helping the USSR win the medal standings at the Olympics on numerous occasions. The first medal won by an Armenian in modern Olympic history was by Hrant Shahinyan, who won two golds and two silvers in gymnastics at the 1952 Summer Olympics in Helsinki. In football, their most successful team was Yerevan's FC Ararat, which had claimed most of the Soviet championships in the 70s and had also gone to post victories against professional clubs like FC Bayern Munich in the Euro cup.
Instruments like the duduk, the dhol, the zurna and the kanun are commonly found in Armenian folk music. Artists such as Sayat Nova are famous due to their influence in the development of Armenian folk music. One of the oldest types of Armenian music is the Armenian chant which is the most common kind of religious music in Armenia. Many of these chants are ancient in origin, extending to pre-Christian times, while others are relatively modern, including several composed by Saint Mesrop Mashtots, the inventor of the Armenian alphabet. Whilst under Soviet rule, Armenian classical music composer Aram Khatchaturian became internationally well known for his music, for various ballets and the Sabre Dance from his composition for the ballet Gayane.
The Armenian Genocide caused widespread emigration that led to the settlement of Armenians in various countries in the world. Armenians kept to their traditions and certain diasporans rose to fame with their music. In the post-Genocide Armenian community of the United States, the so-called "kef" style Armenian dance music, using Armenian and Middle Eastern folk instruments (often electrified/amplified) and some western instruments, was popular. This style preserved the folk songs and dances of Western Armenia, and many artists also played the contemporary popular songs of Turkey and other Middle Eastern countries from which the Armenians emigrated. Richard Hagopian is perhaps the most famous artist of the traditional "kef" style and the Vosbikian Band was notable in the 40s and 50s for developing their own style of "kef music" heavily influenced by the popular American Big Band Jazz of the time. Later, stemming from the Middle Eastern Armenian diaspora and influenced by Continental European (especially French) pop music, the Armenian pop music genre grew to fame in the 60s and 70s with artists such as Adiss Harmandian and Harout Pamboukjian performing to the Armenian diaspora and Armenia. Also with artists such as Sirusho, performing pop music combined with Armenian folk music in today's entertainment industry. Other Armenian diasporans that rose to fame in classical or international music circles are world-renowned French-Armenian singer and composer Charles Aznavour, pianist Sahan Arzruni, prominent opera sopranos such as Hasmik Papian and more recently Isabel Bayrakdarian and Anna Kasyan. Certain Armenians settled to sing non-Armenian tunes such as the heavy metal band System of a Down (which nonetheless often incorporates traditional Armenian instrumentals and styling into their songs) or pop star Cher. Ruben Hakobyan (Ruben Sasuntsi) is a well recognized Armenian ethnographic and patriotic folk singer who has achieved widespread national recognition due to his devotion to Armenian folk music and exceptional talent. In the Armenian diaspora, Armenian revolutionary songs are popular with the youth.[citation needed] These songs encourage Armenian patriotism and are generally about Armenian history and national heroes.
Carpet-weaving is historically a major traditional profession for the majority of Armenian women, including many Armenian families. Prominent Karabakh carpet weavers there were men too. The oldest extant Armenian carpet from the region, referred to as Artsakh (see also Karabakh carpet) during the medieval era, is from the village of Banants (near Gandzak) and dates to the early 13th century. The first time that the Armenian word for carpet, gorg, was used in historical sources was in a 1242–1243 Armenian inscription on the wall of the Kaptavan Church in Artsakh.
Art historian Hravard Hakobyan notes that "Artsakh carpets occupy a special place in the history of Armenian carpet-making." Common themes and patterns found on Armenian carpets were the depiction of dragons and eagles. They were diverse in style, rich in color and ornamental motifs, and were even separated in categories depending on what sort of animals were depicted on them, such as artsvagorgs (eagle-carpets), vishapagorgs (dragon-carpets) and otsagorgs (serpent-carpets). The rug mentioned in the Kaptavan inscriptions is composed of three arches, "covered with vegatative ornaments", and bears an artistic resemblance to the illuminated manuscripts produced in Artsakh.
Armenians enjoy many different native and foreign foods. Arguably the favorite food is khorovats an Armenian-styled barbecue. Lavash is a very popular Armenian flat bread, and Armenian paklava is a popular dessert made from filo dough. Other famous Armenian foods include the kabob (a skewer of marinated roasted meat and vegetables), various dolmas (minced lamb, or beef meat and rice wrapped in grape leaves, cabbage leaves, or stuffed into hollowed vegetables), and pilaf, a rice dish. Also, ghapama, a rice-stuffed pumpkin dish, and many different salads are popular in Armenian culture. Fruits play a large part in the Armenian diet. Apricots (Prunus armeniaca, also known as Armenian Plum) have been grown in Armenia for centuries and have a reputation for having an especially good flavor. Peaches are popular as well, as are grapes, figs, pomegranates, and melons. Preserves are made from many fruits, including cornelian cherries, young walnuts, sea buckthorn, mulberries, sour cherries, and many others.
Classical music is art music produced or rooted in the traditions of Western music, including both liturgical (religious) and secular music. While a similar term is also used to refer to the period from 1750 to 1820 (the Classical period), this article is about the broad span of time from roughly the 11th century to the present day, which includes the Classical period and various other periods. The central norms of this tradition became codified between 1550 and 1900, which is known as the common practice period. The major time divisions of classical music are as follows: the early music period, which includes the Medieval (500–1400) and the Renaissance (1400–1600) eras; the Common practice period, which includes the Baroque (1600–1750), Classical (1750–1820), and Romantic eras (1804–1910); and the 20th century (1901–2000) which includes the modern (1890–1930) that overlaps from the late 19th-century, the high modern (mid 20th-century), and contemporary or postmodern (1975–2015) eras.[citation needed]
European art music is largely distinguished from many other non-European and popular musical forms by its system of staff notation, in use since about the 16th century. Western staff notation is used by composers to prescribe to the performer the pitches (e.g., melodies, basslines and/or chords), tempo, meter and rhythms for a piece of music. This leaves less room for practices such as improvisation and ad libitum ornamentation, which are frequently heard in non-European art music and in popular music  styles such as jazz and blues. Another difference is that whereas most popular styles lend themselves to the song form, classical music has been noted for its development of highly sophisticated forms of instrumental music such as the concerto, symphony, sonata, and mixed vocal and instrumental styles such as opera which, since they are written down, can attain a high level of complexity.
The term "classical music" did not appear until the early 19th century, in an attempt to distinctly canonize the period from Johann Sebastian Bach to Beethoven as a golden age. The earliest reference to "classical music" recorded by the Oxford English Dictionary is from about 1836.
Given the wide range of styles in classical music, from Medieval plainchant sung by monks to Classical and Romantic symphonies for orchestra from the 1700s and 1800s to avant-garde atonal compositions for solo piano from the 1900s, it is difficult to list characteristics that can be attributed to all works of that type. However, there are characteristics that classical music contains that few or no other genres of music contain, such as the use of a printed score and the performance of very complex instrumental works (e.g., the fugue). As well, although the symphony did not exist through the entire classical music period, from the mid-1700s to the 2000s the symphony ensemble—and the works written for it—have become a defining feature of classical music.
The key characteristic of classical music that distinguishes it from popular music and folk music is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: J.S. Bach's fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic that would be impossible in the heat of live improvisation. The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago. Musical notation enables 2000s-era performers to sing a choral work from the 1300s Renaissance era or a 1700s Baroque concerto with many of the features of the music (the melodies, lyrics, forms, and rhythms) being reproduced.
That said, the score does not provide complete and exact instructions on how to perform a historical work. Even if the tempo is written with an Italian instruction (e.g., Allegro), we do not know exactly how fast the piece should be played. As well, in the Baroque era, many works that were designed for basso continuo accompaniment do not specify which instruments should play the accompaniment or exactly how the chordal instrument (harpsichord, lute, etc.) should play the chords, which are not notated in the part (only a figured bass symbol beneath the bass part is used to guide the chord-playing performer). The performer and/or the conductor have a range of options for musical expression and interpretation of a scored piece, including the phrasing of melodies, the time taken during fermatas (held notes) or pauses, and the use (or choice not to use) of effects such as vibrato or glissando (these effects are possible on various stringed, brass and woodwind instruments and with the human voice).
Although Classical music in the 2000s has lost most of its tradition for musical improvisation, from the Baroque era to the Romantic era, there are examples of performers who could improvise in the style of their era. In the Baroque era, organ performers would improvise preludes, keyboard performers playing harpsichord would improvise chords from the figured bass symbols beneath the bass notes of the basso continuo part and both vocal and instrumental performers would improvise musical ornaments. J.S. Bach was particularly noted for his complex improvisations. During the Classical era, the composer-performer Mozart was noted for his ability to improvise melodies in different styles. During the Classical era, some virtuoso soloists would improvise the cadenza sections of a concerto. During the Romantic era, Beethoven would improvise at the piano. For more information, see Improvisation.
The instruments currently used in most classical music were largely invented before the mid-19th century (often much earlier) and codified in the 18th and 19th centuries. They consist of the instruments found in an orchestra or in a concert band, together with several other solo instruments (such as the piano, harpsichord, and organ). The symphony orchestra is the most widely known medium for classical music and includes members of the string, woodwind, brass, and percussion families of instruments. The concert band consists of members of the woodwind, brass, and percussion families. It generally has a larger variety and amount of woodwind and brass instruments than the orchestra but does not have a string section. However, many concert bands use a double bass. The vocal practices changed a great deal over the classical period, from the single line monophonic Gregorian chant done by monks in the Medieval period to the complex, polyphonic choral works of the Renaissance and subsequent periods, which used multiple independent vocal melodies at the same time.
Many of the instruments used to perform medieval music still exist, but in different forms. Medieval instruments included the wood flute (which in the 21st century is made of metal), the recorder and plucked string instruments like the lute. As well, early versions of the organ, fiddle (or vielle), and trombone (called the sackbut) existed. Medieval instruments in Europe had most commonly been used singly, often self accompanied with a drone note, or occasionally in parts. From at least as early as the 13th century through the 15th century there was a division of instruments into haut (loud, shrill, outdoor instruments) and bas (quieter, more intimate instruments).
During the earlier medieval period, the vocal music from the liturgical genre, predominantly Gregorian chant, was monophonic, using a single, unaccompanied vocal melody line. Polyphonic vocal genres, which used multiple independent vocal melodies, began to develop during the high medieval era, becoming prevalent by the later 13th and early 14th century.
Many instruments originated during the Renaissance; others were variations of, or improvements upon, instruments that had existed previously. Some have survived to the present day; others have disappeared, only to be recreated in order to perform music of the period on authentic instruments. As in the modern day, instruments may be classified as brass, strings, percussion, and woodwind.
Brass instruments in the Renaissance were traditionally played by professionals who were members of Guilds and they included the slide trumpet, the wooden cornet, the valveless trumpet and the sackbut. Stringed instruments included the viol, the harp-like lyre, the hurdy-gurdy, the cittern and the lute. Keyboard instruments with strings included the harpsichord and the virginal. Percussion instruments include the triangle, the Jew's harp, the tambourine, the bells, the rumble-pot, and various kinds of drums. Woodwind instruments included the double reed shawm, the reed pipe, the bagpipe, the transverse flute and the recorder.
Vocal music in the Renaissance is noted for the flourishing of an increasingly elaborate polyphonic style. The principal liturgical forms which endured throughout the entire Renaissance period were masses and motets, with some other developments towards the end, especially as composers of sacred music began to adopt secular forms (such as the madrigal) for their own designs. Towards the end of the period, the early dramatic precursors of opera such as monody, the madrigal comedy, and the intermedio are seen.
Baroque instruments included some instruments from the earlier periods (e.g., the hurdy-gurdy and recorder) and a number of new instruments (e.g, the cello, contrabass and fortepiano). Some instruments from previous eras fell into disuse, such as the shawm and the wooden cornet. The key Baroque instruments for strings included the violin, viol, viola, viola d'amore, cello, contrabass, lute, theorbo (which often played the basso continuo parts), mandolin, cittern, Baroque guitar, harp and hurdy-gurdy. Woodwinds included the Baroque flute, Baroque oboe, rackett, recorder and the bassoon. Brass instruments included the cornett, natural horn, Baroque trumpet, serpent and the trombone. Keyboard instruments included the clavichord, tangent piano, the fortepiano (an early version of the piano), the harpsichord and the pipe organ. Percussion instruments included the timpani, snare drum, tambourine and the castanets.
One major difference between Baroque music and the classical era that followed it is that the types of instruments used in ensembles were much less standardized. Whereas a classical era string quartet consists almost exclusively of two violins, a viola and a cello, a Baroque group accompanying a soloist or opera could include one of several different types of keyboard instruments (e.g., pipe organ, harpsichord, or clavichord), additional stringed chordal instruments (e.g., a lute) and an unspecified number of bass instruments performing the basso continuo bassline, including bowed strings, woodwinds and brass instruments (e.g., a cello, contrabass, viol, bassoon, serpent, etc.).
The term "classical music" has two meanings: the broader meaning includes all Western art music from the Medieval era to today, and the specific meaning refers to the music from the 1750s to the early 1830s—the era of Mozart and Haydn. This section is about the more specific meaning.
Classical musicians continued to use many of instruments from the Baroque era, such as the cello, contrabass, recorder, trombone, timpani, fortepiano and organ. While some Baroque instruments fell into disuse (e.g., the theorbo and rackett), many Baroque instruments were changed into the versions that are still in use today, such as the Baroque violin (which became the violin), the Baroque oboe (which became the oboe) and the Baroque trumpet, which transitioned to the regular valved trumpet.
The Classical era stringed instruments were the four instruments which form the string section of the orchestra: the violin, viola, cello and contrabass. Woodwinds included the basset clarinet, basset horn, clarinette d'amour, the Classical clarinet, the chalumeau, the flute, oboe and bassoon. Keyboard instruments included the clavichord and the fortepiano. While the harpsichord was still used in basso continuo accompaniment in the 1750s and 1760s, it fell out of use in the end of the century. Brass instruments included the buccin, the ophicleide (a serpent replacement which was the precursor of tuba) and the natural horn.
The "standard complement" of double winds and brass in the orchestra from the first half of the 19th century is generally attributed to Beethoven. The exceptions to this are his Symphony No. 4, Violin Concerto, and Piano Concerto No. 4, which each specify a single flute. The composer's instrumentation usually included paired flutes, oboes, clarinets, bassoons, horns and trumpets. Beethoven carefully calculated the expansion of this particular timbral "palette" in Symphonies 3, 5, 6, and 9 for an innovative effect. The third horn in the "Eroica" Symphony arrives to provide not only some harmonic flexibility, but also the effect of "choral" brass in the Trio. Piccolo, contrabassoon, and trombones add to the triumphal finale of his Symphony No. 5. A piccolo and a pair of trombones help deliver "storm" and "sunshine" in the Sixth. The Ninth asks for a second pair of horns, for reasons similar to the "Eroica" (four horns has since become standard); Beethoven's use of piccolo, contrabassoon, trombones, and untuned percussion—plus chorus and vocal soloists—in his finale, are his earliest suggestion that the timbral boundaries of symphony should be expanded. For several decades after he died, symphonic instrumentation was faithful to Beethoven's well-established model, with few exceptions.
In the Romantic era, the modern piano, with a more powerful, sustained tone and a wider range took over from the more delicate-sounding fortepiano. In the orchestra, the existing Classical instruments and sections were retained (string section, woodwinds, brass and percussion), but these sections were typically expanded to make a fuller, bigger sound. For example, while a Baroque orchestra may have had two double bass players, a Romantic orchestra could have as many as ten. "As music grew more expressive, the standard orchestral palette just wasn't rich enough for many Romantic composers."  New woodwind instruments were added, such as the contrabassoon, bass clarinet and piccolo and new percussion instruments were added, including xylophones, drums, celestes (a bell-like keyboard instrument), large orchestral harps, bells, and triangles  and even wind machines for sound effects.
Saxophones appear in some scores from the late 19th century onwards. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's Pictures at an Exhibition and Sergei Rachmaninoff's Symphonic Dances, the saxophone is included in other works, such as Ravel's Boléro, Sergei Prokofiev's Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble. The euphonium is featured in a few late Romantic and 20th-century works, usually playing parts marked "tenor tuba", including Gustav Holst's The Planets, and Richard Strauss's Ein Heldenleben.
The Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle Der Ring des Nibelungen and several other works by Strauss, Béla Bartók, and others; it has a prominent role in Anton Bruckner's Symphony No. 7 in E Major. Cornets appear in Pyotr Ilyich Tchaikovsky's ballet Swan Lake, Claude Debussy's La Mer, and several orchestral works by Hector Berlioz. Unless these instruments are played by members doubling on another instrument (for example, a trombone player changing to euphonium for a certain passage), orchestras will use freelance musicians to augment their regular rosters.
Electric instruments such as the electric guitar, the electric bass and the ondes Martenot appear occasionally in the classical music of the 20th and 21st centuries. Both classical and popular musicians have experimented in recent decades with electronic instruments such as the synthesizer, electric and digital techniques such as the use of sampled or computer-generated sounds, and instruments from other cultures such as the gamelan.
Many instruments today associated with popular music filled important roles in early classical music, such as bagpipes, vihuelas, hurdy-gurdies, and some woodwind instruments. On the other hand, instruments such as the acoustic guitar, once associated mainly with popular music, gained prominence in classical music in the 19th and 20th centuries.
While equal temperament became gradually accepted as the dominant musical temperament during the 18th century, different historical temperaments are often used for music from earlier periods. For instance, music of the English Renaissance is often performed in meantone temperament.
Performers who have studied classical music extensively are said to be "classically trained". This training may be from private lessons from instrument or voice teachers or from completion of a formal program offered by a Conservatory, college or university, such as a B.mus. or M.mus. degree (which includes individual lessons from professors). In classical music, "...extensive formal music education and training, often to postgraduate [Master's degree] level" is required.
Performance of classical music repertoire requires a proficiency in sight-reading and ensemble playing, harmonic principles, strong ear training (to correct and adjust pitches by ear), knowledge of performance practice (e.g., Baroque ornamentation), and a familiarity with the style/musical idiom expected for a given composer or musical work (e.g., a Brahms symphony or a Mozart concerto).
Some "popular" genre musicians have had significant classical training, such as Billy Joel, Elton John, the Van Halen brothers, Randy Rhoads and Ritchie Blackmore. Moreover, formal training is not unique to the classical genre. Many rock and pop musicians have completed degrees in commercial music programs such as those offered by the Berklee College of Music and many jazz musicians have completed degrees in music from universities with jazz programs, such as the Manhattan School of Music and McGill University.
Historically, major professional orchestras have been mostly or entirely composed of male musicians. Some of the earliest cases of women being hired in professional orchestras was in the position of harpist. The Vienna Philharmonic, for example, did not accept women to permanent membership until 1997, far later than the other orchestras ranked among the world's top five by Gramophone in 2008. The last major orchestra to appoint a woman to a permanent position was the Berlin Philharmonic. As late as February 1996, the Vienna Philharmonic's principal flute, Dieter Flury, told Westdeutscher Rundfunk that accepting women would be "gambling with the emotional unity (emotionelle Geschlossenheit) that this organism currently has". In April 1996, the orchestra's press secretary wrote that "compensating for the expected leaves of absence" of maternity leave would be a problem.
In 1997, the Vienna Philharmonic was "facing protests during a [US] tour" by the National Organization for Women and the International Alliance for Women in Music. Finally, "after being held up to increasing ridicule even in socially conservative Austria, members of the orchestra gathered [on 28 February 1997] in an extraordinary meeting on the eve of their departure and agreed to admit a woman, Anna Lelkes, as harpist." As of 2013, the orchestra has six female members; one of them, violinist Albena Danailova became one of the orchestra's concertmasters in 2008, the first woman to hold that position. In 2012, women still made up just 6% of the orchestra's membership. VPO president Clemens Hellsberg said the VPO now uses completely screened blind auditions.
In 2013, an article in Mother Jones stated that while "[m]any prestigious orchestras have significant female membership—women outnumber men in the New York Philharmonic's violin section—and several renowned ensembles, including the National Symphony Orchestra, the Detroit Symphony, and the Minnesota Symphony, are led by women violinists", the double bass, brass, and percussion sections of major orchestras "...are still predominantly male." A 2014 BBC article stated that the "...introduction of 'blind' auditions, where a prospective instrumentalist performs behind a screen so that the judging panel can exercise no gender or racial prejudice, has seen the gender balance of traditionally male-dominated symphony orchestras gradually shift."
Works of classical repertoire often exhibit complexity in their use of orchestration, counterpoint, harmony, musical development, rhythm, phrasing, texture, and form. Whereas most popular styles are usually written in song forms, classical music is noted for its development of highly sophisticated musical forms, like the concerto, symphony, sonata, and opera.
Longer works are often divided into self-contained pieces, called movements, often with contrasting characters or moods. For instance, symphonies written during the Classical period are usually divided into four movements: (1) an opening Allegro in sonata form, (2) a slow movement, (3) a minuet or scherzo, and (4) a final Allegro. These movements can then be further broken down into a hierarchy of smaller units: first sections, then periods, and finally phrases.
The major time divisions of classical music up to 1900 are the early music period, which includes Medieval (500–1400) and Renaissance (1400–1600) eras, and the Common practice period, which includes the Baroque (1600–1750), Classical (1750–1830) and Romantic (1804–1910) eras. Since 1900, classical periods have been reckoned more by calendar century than by particular stylistic movements that have become fragmented and difficult to define. The 20th century calendar period (1901–2000) includes most of the early modern musical era (1890–1930), the entire high modern (mid 20th-century), and the first 25 years of the contemporary or postmodern musical era (1975–current). The 21st century has so far been characterized by a continuation of the contemporary/postmodern musical era.
The dates are generalizations, since the periods and eras overlap and the categories are somewhat arbitrary, to the point that some authorities reverse terminologies and refer to a common practice "era" comprising baroque, classical, and romantic "periods". For example, the use of counterpoint and fugue, which is considered characteristic of the Baroque era (or period), was continued by Haydn, who is classified as typical of the Classical era. Beethoven, who is often described as a founder of the Romantic era, and Brahms, who is classified as Romantic, also used counterpoint and fugue, but other characteristics of their music define their era.
The prefix neo is used to describe a 20th-century or contemporary composition written in the style of an earlier era, such as Classical or Romantic. Stravinsky's Pulcinella, for example, is a neoclassical composition because it is stylistically similar to works of the Classical era.
Burgh (2006), suggests that the roots of Western classical music ultimately lie in ancient Egyptian art music via cheironomy and the ancient Egyptian orchestra, which dates to 2695 BC. This was followed by early Christian liturgical music, which itself dates back to the Ancient Greeks[citation needed]. The development of individual tones and scales was made by ancient Greeks such as Aristoxenus and Pythagoras. Pythagoras created a tuning system and helped to codify musical notation. Ancient Greek instruments such as the aulos (a reed instrument) and the lyre (a stringed instrument similar to a small harp) eventually led to the modern-day instruments of a classical orchestra. The antecedent to the early period was the era of ancient music before the fall of the Roman Empire (476 AD). Very little music survives from this time, most of it from ancient Greece.
The Medieval period includes music from after the fall of Rome to about 1400. Monophonic chant, also called plainsong or Gregorian chant, was the dominant form until about 1100. Polyphonic (multi-voiced) music developed from monophonic chant throughout the late Middle Ages and into the Renaissance, including the more complex voicings of motets.
The Renaissance era was from 1400 to 1600. It was characterized by greater use of instrumentation, multiple interweaving melodic lines, and the use of the first bass instruments. Social dancing became more widespread, so musical forms appropriate to accompanying dance began to standardize.
It is in this time that the notation of music on a staff and other elements of musical notation began to take shape. This invention made possible the separation of the composition of a piece of music from its transmission; without written music, transmission was oral, and subject to change every time it was transmitted. With a musical score, a work of music could be performed without the composer's presence. The invention of the movable-type printing press in the 15th century had far-reaching consequences on the preservation and transmission of music.
Typical stringed instruments of the early period include the harp, lute, vielle, and psaltery, while wind instruments included the flute family (including recorder), shawm (an early member of the oboe family), trumpet, and the bagpipes. Simple pipe organs existed, but were largely confined to churches, although there were portable varieties. Later in the period, early versions of keyboard instruments like the clavichord and harpsichord began to appear. Stringed instruments such as the viol had emerged by the 16th century, as had a wider variety of brass and reed instruments. Printing enabled the standardization of descriptions and specifications of instruments, as well as instruction in their use.
The common practice period is when many of the ideas that make up western classical music took shape, standardized, or were codified. It began with the Baroque era, running from roughly 1600 to the middle of the 18th century. The Classical era followed, ending roughly around 1820. The Romantic era ran through the 19th century, ending about 1910.
Baroque music is characterized by the use of complex tonal counterpoint and the use of a basso continuo, a continuous bass line. Music became more complex in comparison with the songs of earlier periods. The beginnings of the sonata form took shape in the canzona, as did a more formalized notion of theme and variations. The tonalities of major and minor as means for managing dissonance and chromaticism in music took full shape.
During the Baroque era, keyboard music played on the harpsichord and pipe organ became increasingly popular, and the violin family of stringed instruments took the form generally seen today. Opera as a staged musical drama began to differentiate itself from earlier musical and dramatic forms, and vocal forms like the cantata and oratorio became more common. Vocalists began adding embellishments to melodies. Instrumental ensembles began to distinguish and standardize by size, giving rise to the early orchestra for larger ensembles, with chamber music being written for smaller groups of instruments where parts are played by individual (instead of massed) instruments. The concerto as a vehicle for solo performance accompanied by an orchestra became widespread, although the relationship between soloist and orchestra was relatively simple.
The theories surrounding equal temperament began to be put in wider practice, especially as it enabled a wider range of chromatic possibilities in hard-to-tune keyboard instruments. Although Bach did not use equal temperament, as a modern piano is generally tuned, changes in the temperaments from the meantone system, common at the time, to various temperaments that made modulation between all keys musically acceptable, made possible Bach's Well-Tempered Clavier.
The Classical era, from about 1750 to 1820, established many of the norms of composition, presentation, and style, and was also when the piano became the predominant keyboard instrument. The basic forces required for an orchestra became somewhat standardized (although they would grow as the potential of a wider array of instruments was developed in the following centuries). Chamber music grew to include ensembles with as many as 8 to 10 performers for serenades. Opera continued to develop, with regional styles in Italy, France, and German-speaking lands. The opera buffa, a form of comic opera, rose in popularity. The symphony came into its own as a musical form, and the concerto was developed as a vehicle for displays of virtuoso playing skill. Orchestras no longer required a harpsichord (which had been part of the traditional continuo in the Baroque style), and were often led by the lead violinist (now called the concertmaster).
Wind instruments became more refined in the Classical era. While double reeded instruments like the oboe and bassoon became somewhat standardized in the Baroque, the clarinet family of single reeds was not widely used until Mozart expanded its role in orchestral, chamber, and concerto settings.
The music of the Romantic era, from roughly the first decade of the 19th century to the early 20th century, was characterized by increased attention to an extended melodic line, as well as expressive and emotional elements, paralleling romanticism in other art forms. Musical forms began to break from the Classical era forms (even as those were being codified), with free-form pieces like nocturnes, fantasias, and preludes being written where accepted ideas about the exposition and development of themes were ignored or minimized. The music became more chromatic, dissonant, and tonally colorful, with tensions (with respect to accepted norms of the older forms) about key signatures increasing. The art song (or Lied) came to maturity in this era, as did the epic scales of grand opera, ultimately transcended by Richard Wagner's Ring cycle.
In the 19th century, musical institutions emerged from the control of wealthy patrons, as composers and musicians could construct lives independent of the nobility. Increasing interest in music by the growing middle classes throughout western Europe spurred the creation of organizations for the teaching, performance, and preservation of music. The piano, which achieved its modern construction in this era (in part due to industrial advances in metallurgy) became widely popular with the middle class, whose demands for the instrument spurred a large number of piano builders. Many symphony orchestras date their founding to this era. Some musicians and composers were the stars of the day; some, like Franz Liszt and Niccolò Paganini, fulfilled both roles.
The family of instruments used, especially in orchestras, grew. A wider array of percussion instruments began to appear. Brass instruments took on larger roles, as the introduction of rotary valves made it possible for them to play a wider range of notes. The size of the orchestra (typically around 40 in the Classical era) grew to be over 100. Gustav Mahler's 1906 Symphony No. 8, for example, has been performed with over 150 instrumentalists and choirs of over 400.
European cultural ideas and institutions began to follow colonial expansion into other parts of the world. There was also a rise, especially toward the end of the era, of nationalism in music (echoing, in some cases, political sentiments of the time), as composers such as Edvard Grieg, Nikolai Rimsky-Korsakov, and Antonín Dvořák echoed traditional music of their homelands in their compositions.
Encompassing a wide variety of post-Romantic styles composed through the year 2000, 20th century classical music includes late romantic, modern, high-modern, and postmodern styles of composition. Modernism (1890–1930) marked an era when many composers rejected certain values of the common practice period, such as traditional tonality, melody, instrumentation, and structure. The high-modern era saw the emergence of neo-classical and serial music. A few authorities have claimed high-modernism as the beginning of postmodern music from about 1930. Others have more or less equated postmodern music with the "contemporary music" composed from the late 20th century through to the early 21st century.
Almost all of the composers who are described in music textbooks on classical music and whose works are widely performed as part of the standard concert repertoire are male composers, even though there has been a large number of women composers throughout the classical music period. Musicologist Marcia Citron has asked "[w]hy is music composed by women so marginal to the standard 'classical' repertoire?" Citron "examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works." She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers. In the "...Concise Oxford History of Music, Clara Shumann  [sic] is one of the only  [sic] female composers mentioned." Abbey Philips states that "[d]uring the 20th century the women who were composing/playing gained far less attention than their male counterparts."
The modernist views hold that classical music is considered primarily a written musical tradition, preserved in music notation, as opposed to being transmitted orally, by rote, or by recordings of particular performances.[citation needed] While there are differences between particular performances of a classical work, a piece of classical music is generally held to transcend any interpretation of it. The use of musical notation is an effective method for transmitting classical music, since the written music contains the technical instructions for performing the work.
In 1996–1997, a research study was conducted on a large population of middle age students in the Cherry Creek School District in Denver, Colorado, USA. The study showed that students who actively listen to classical music before studying had higher academic scores. The research further indicated that students who listened to the music prior to an examination also had positively elevated achievement scores. Students who listened to rock-and-roll or country had moderately lower scores. The study further indicated that students who used classical during the course of study had a significant leap in their academic performance; whereas, those who listened to other types of music had significantly lowered academic scores. The research was conducted over several schools within the Cherry Creek School District and was conducted through University of Colorado. This study is reflective of several recent studies (i.e. Mike Manthei and Steve N. Kelly of the University of Nebraska at Omaha; Donald A. Hodges and Debra S. O'Connell of the University of North Carolina at Greensboro; etc.) and others who had significant results through the discourse of their work.
During the 1990s, several research papers and popular books wrote on what came to be called the "Mozart effect": an observed temporary, small elevation of scores on certain tests as a result of listening to Mozart's works. The approach has been popularized in a book by Don Campbell, and is based on an experiment published in Nature suggesting that listening to Mozart temporarily boosted students' IQ by 8 to 9 points. This popularized version of the theory was expressed succinctly by the New York Times music columnist Alex Ross: "researchers... have determined that listening to Mozart actually makes you smarter." Promoters marketed CDs claimed to induce the effect. Florida passed a law requiring toddlers in state-run schools to listen to classical music every day, and in 1998 the governor of Georgia budgeted $105,000 per year to provide every child born in Georgia with a tape or CD of classical music. One of the co-authors of the original studies of the Mozart effect commented "I don't think it can hurt. I'm all for exposing children to wonderful cultural experiences. But I do think the money could be better spent on music education programs."
Shawn Vancour argues that the commercialization of classical music in the early 20th century served to harm the music industry through inadequate representation.
Several works from the Golden Age of Animation matched the action to classical music. Notable examples are Walt Disney's Fantasia, Tom and Jerry's Johann Mouse, and Warner Bros.' Rabbit of Seville and What's Opera, Doc?.
Similarly, movies and television often revert to standard, clichéd snatches of classical music to convey refinement or opulence: some of the most-often heard pieces in this category include Bach´s Cello Suite No. 1, Mozart's Eine kleine Nachtmusik, Vivaldi's Four Seasons, Mussorgsky's Night on Bald Mountain (as orchestrated by Rimsky-Korsakov), and Rossini's William Tell Overture.
The written score, however, does not usually contain explicit instructions as to how to interpret the piece in terms of production or performance, apart from directions for dynamics, tempo and expression (to a certain extent). This is left to the discretion of the performers, who are guided by their personal experience and musical education, their knowledge of the work's idiom, their personal artistic tastes, and the accumulated body of historic performance practices.
Some critics express the opinion that it is only from the mid-19th century, and especially in the 20th century, that the score began to hold such a high significance. Previously, improvisation (in preludes, cadenzas and ornaments), rhythmic flexibility (e.g., tempo rubato), improvisatory deviation from the score and oral tradition of playing was integral to the style. Yet in the 20th century, this oral tradition and passing on of stylistic features within classical music disappeared. Instead, musicians tend to use just the score to play music. Yet, even with the score providing the key elements of the music, there is considerable controversy about how to perform the works. Some of this controversy relates to the fact that this score-centric approach has led to performances that emphasize metrically strict block-rhythms (just as the music is notated in the score).
Improvisation once played an important role in classical music. A remnant of this improvisatory tradition in classical music can be heard in the cadenza, a passage found mostly in concertos and solo works, designed to allow skilled performers to exhibit their virtuoso skills on the instrument. Traditionally this was improvised by the performer; however, it is often written for (or occasionally by) the performer beforehand. Improvisation is also an important aspect in authentic performances of operas of Baroque era and of bel canto (especially operas of Vincenzo Bellini), and is best exemplified by the da capo aria, a form by which famous singers typically perform variations of the thematic matter of the aria in the recapitulation section ('B section' / the 'da capo' part). An example is Beverly Sills' complex, albeit pre-written, variation of Da tempeste il legno infranto from Händel's Giulio Cesare.
Certain staples of classical music are often used commercially (either in advertising or in movie soundtracks). In television commercials, several passages have become clichéd, particularly the opening of Richard Strauss' Also sprach Zarathustra (made famous in the film 2001: A Space Odyssey) and the opening section "O Fortuna" of Carl Orff's Carmina Burana, often used in the horror genre; other examples include the Dies Irae from the Verdi Requiem, Edvard Grieg's In the Hall of the Mountain King from Peer Gynt, the opening bars of Beethoven's Symphony No. 5, Wagner's Ride of the Valkyries from Die Walküre, Rimsky-Korsakov's Flight of the Bumblebee, and excerpts of Aaron Copland's Rodeo.
Composers of classical music have often made use of folk music (music created by musicians who are commonly not classically trained, often from a purely oral tradition). Some composers, like Dvořák and Smetana, have used folk themes to impart a nationalist flavor to their work, while others like Bartók have used specific themes lifted whole from their folk-music origins.
Its written transmission, along with the veneration bestowed on certain classical works, has led to the expectation that performers will play a work in a way that realizes in detail the original intentions of the composer. During the 19th century the details that composers put in their scores generally increased. Yet the opposite trend—admiration of performers for new "interpretations" of the composer's work—can be seen, and it is not unknown for a composer to praise a performer for achieving a better realization of the original intent than the composer was able to imagine. Thus, classical performers often achieve high reputations for their musicianship, even if they do not compose themselves. Generally however, it is the composers who are remembered more than the performers.
The primacy of the composer's written score has also led, today, to a relatively minor role played by improvisation in classical music, in sharp contrast to the practice of musicians who lived during the baroque, classical and romantic era. Improvisation in classical music performance was common during both the Baroque and early romantic eras, yet lessened strongly during the second half of the 19th and in the 20th centuries. During the classical era, Mozart and Beethoven often improvised the cadenzas to their piano concertos (and thereby encouraged others to do so), but they also provided written cadenzas for use by other soloists. In opera, the practice of singing strictly by the score, i.e. come scritto, was famously propagated by soprano Maria Callas, who called this practice 'straitjacketing' and implied that it allows the intention of the composer to be understood better, especially during studying the music for the first time.
Classical music has often incorporated elements or material from popular music of the composer's time. Examples include occasional music such as Brahms' use of student drinking songs in his Academic Festival Overture, genres exemplified by Kurt Weill's The Threepenny Opera, and the influence of jazz on early- and mid-20th-century composers including Maurice Ravel, exemplified by the movement entitled "Blues" in his sonata for violin and piano. Certain postmodern, minimalist and postminimalist classical composers acknowledge a debt to popular music.
Numerous examples show influence in the opposite direction, including popular songs based on classical music, the use to which Pachelbel's Canon has been put since the 1970s, and the musical crossover phenomenon, where classical musicians have achieved success in the popular music arena. In heavy metal, a number of lead guitarists (playing electric guitar) modeled their playing styles on Baroque or Classical era instrumental music, including Ritchie Blackmore and Randy Rhoads.
Beginning in 1689, the colonies became involved in a series of wars between Great Britain and France for control of North America, the most important of which were Queen Anne's War, in which the British conquered French colony Acadia, and the final French and Indian War (1754–63) when Britain was victorious over all the French colonies in North America. This final war was to give thousands of colonists, including Virginia colonel George Washington, military experience which they put to use during the American Revolutionary War.
By far the largest military action in which the United States engaged during this era was the War of 1812. With Britain locked in a major war with Napoleon's France, its policy was to block American shipments to France. The United States sought to remain neutral while pursuing overseas trade. Britain cut the trade and impressed seamen on American ships into the Royal Navy, despite intense protests. Britain supported an Indian insurrection in the American Midwest, with the goal of creating an Indian state there that would block American expansion. The United States finally declared war on the United Kingdom in 1812, the first time the U.S. had officially declared war. Not hopeful of defeating the Royal Navy, the U.S. attacked the British Empire by invading British Canada, hoping to use captured territory as a bargaining chip. The invasion of Canada was a debacle, though concurrent wars with Native Americans on the western front (Tecumseh's War and the Creek War) were more successful. After defeating Napoleon in 1814, Britain sent large veteran armies to invade New York, raid Washington and capture the key control of the Mississippi River at New Orleans. The New York invasion was a fiasco after the much larger British army retreated to Canada. The raiders succeeded in the burning of Washington on 25 August 1814, but were repulsed in their Chesapeake Bay Campaign at the Battle of Baltimore and the British commander killed. The major invasion in Louisiana was stopped by a one-sided military battle that killed the top three British generals and thousands of soldiers. The winners were the commanding general of the Battle of New Orleans, Major General Andrew Jackson, who became president and the Americans who basked in a victory over a much more powerful nation. The peace treaty proved successful, and the U.S. and Britain never again went to war. The losers were the Indians, who never gained the independent territory in the Midwest promised by Britain.
Secretary of War Elihu Root (1899–1904) led the modernization of the Army. His goal of a uniformed chief of staff as general manager and a European-type general staff for planning was stymied by General Nelson A. Miles but did succeed in enlarging West Point and establishing the U.S. Army War College as well as the General Staff. Root changed the procedures for promotions and organized schools for the special branches of the service. He also devised the principle of rotating officers from staff to line. Root was concerned about the Army's role in governing the new territories acquired in 1898 and worked out the procedures for turning Cuba over to the Cubans, and wrote the charter of government for the Philippines.
The United States originally wished to remain neutral when World War I broke out in August 1914. However, it insisted on its right as a neutral party to immunity from German submarine attack, even though its ships carried food and raw materials to Britain. In 1917 the Germans resumed submarine attacks, knowing that it would lead to American entry. When the U.S declared war, the U.S. army was still small by European standards and mobilization would take a year. Meanwhile, the U.S. continued to provide supplies and money to Britain and France, and initiated the first peacetime draft. Industrial mobilization took longer than expected, so divisions were sent to Europe without equipment, relying instead on the British and French to supply them.
Memories and lessons from the war are still a major factor in American politics. One side views the war as a necessary part of the Containment policy, which allowed the enemy to choose the time and place of warfare. Others note the U.S. made major strategic gains as the Communists were defeated in Indonesia, and by 1972 both Moscow and Beijing were competing for American support, at the expense of their allies in Hanoi. Critics see the conflict as a "quagmire"—an endless waste of American blood and treasure in a conflict that did not concern US interests. Fears of another quagmire have been major factors in foreign policy debates ever since. The draft became extremely unpopular, and President Nixon ended it in 1973, forcing the military (the Army especially) to rely entirely upon volunteers. That raised the issue of how well the professional military reflected overall American society and values; the soldiers typically took the position that their service represented the highest and best American values.
The Persian Gulf War was a conflict between Iraq and a coalition force of 34 nations led by the United States. The lead up to the war began with the Iraqi invasion of Kuwait in August 1990 which was met with immediate economic sanctions by the United Nations against Iraq. The coalition commenced hostilities in January 1991, resulting in a decisive victory for the U.S. led coalition forces, which drove Iraqi forces out of Kuwait with minimal coalition deaths. Despite the low death toll, over 180,000 US veterans would later be classified as "permanently disabled" according to the US Department of Veterans Affairs (see Gulf War Syndrome). The main battles were aerial and ground combat within Iraq, Kuwait and bordering areas of Saudi Arabia. Land combat did not expand outside of the immediate Iraq/Kuwait/Saudi border region, although the coalition bombed cities and strategic targets across Iraq, and Iraq fired missiles on Israeli and Saudi cities.
US troops participated in a UN peacekeeping mission in Somalia beginning in 1992. By 1993 the US troops were augmented with Rangers and special forces with the aim of capturing warlord Mohamed Farrah Aidid, whose forces had massacred peacekeepers from Pakistan. During a raid in downtown Mogadishu, US troops became trapped overnight by a general uprising in the Battle of Mogadishu. Eighteen American soldiers were killed, and a US television crew filmed graphic images of the body of one soldier being dragged through the streets by an angry mob. Somali guerrillas paid a staggering toll at an estimated 1,000–5,000 total casualties during the conflict. After much public disapproval, American forces were quickly withdrawn by President Bill Clinton. The incident profoundly affected US thinking about peacekeeping and intervention. The book Black Hawk Down was written about the battle, and was the basis for the later movie of the same name.
In January 2002, the U.S. sent more than 1,200 troops (later raised to 2,000) to assist the Armed Forces of the Philippines in combating terrorist groups linked to al-Qaida, such as Abu Sayyaf, under Operation Enduring Freedom - Philippines. Operations have taken place mostly in the Sulu Archipelago, where terrorists and other groups are active. The majority of troops provide logistics. However, there are special forces troops that are training and assisting in combat operations against the terrorist groups.
The British, for their part, lacked both a unified command and a clear strategy for winning. With the use of the Royal Navy, the British were able to capture coastal cities, but control of the countryside eluded them. A British sortie from Canada in 1777 ended with the disastrous surrender of a British army at Saratoga. With the coming in 1777 of General von Steuben, the training and discipline along Prussian lines began, and the Continental Army began to evolve into a modern force. France and Spain then entered the war against Great Britain as Allies of the US, ending its naval advantage and escalating the conflict into a world war. The Netherlands later joined France, and the British were outnumbered on land and sea in a world war, as they had no major allies apart from Indian tribes.
When revolutionary France declared war on Great Britain in 1793, the United States sought to remain neutral, but the Jay Treaty, which was favorable to Great Britain, angered the French government, which viewed it as a violation of the 1778 Treaty of Alliance. French privateers began to seize U.S. vessels, which led to an undeclared "Quasi-War" between the two nations. Fought at sea from 1798 to 1800, the United States won a string of victories in the Caribbean. George Washington was called out of retirement to head a "provisional army" in case of invasion by France, but President John Adams managed to negotiate a truce, in which France agreed to terminate the prior alliance and cease its attacks.
In the Treaty of Paris after the Revolution, the British had ceded the lands between the Appalachian Mountains and the Mississippi River to the United States, without consulting the Shawnee, Cherokee, Choctaw and other smaller tribes who lived there. Because many of the tribes had fought as allies of the British, the United States compelled tribal leaders to sign away lands in postwar treaties, and began dividing these lands for settlement. This provoked a war in the Northwest Territory in which the U.S. forces performed poorly; the Battle of the Wabash in 1791 was the most severe defeat ever suffered by the United States at the hands of American Indians. President Washington dispatched a newly trained army to the region, which decisively defeated the Indian confederacy at the Battle of Fallen Timbers in 1794.
Sectional tensions had long existed between the states located north of the Mason–Dixon line and those south of it, primarily centered on the "peculiar institution" of slavery and the ability of states to overrule the decisions of the national government. During the 1840s and 1850s, conflicts between the two sides became progressively more violent. After the election of Abraham Lincoln in 1860 (who southerners thought would work to end slavery) states in the South seceded from the United States, beginning with South Carolina in late 1860. On April 12, 1861, forces of the South (known as the Confederate States of America or simply the Confederacy) opened fire on Fort Sumter, whose garrison was loyal to the Union.
The Spanish–American War was a short decisive war marked by quick, overwhelming American victories at sea and on land against Spain. The Navy was well-prepared and won laurels, even as politicians tried (and failed) to have it redeployed to defend East Coast cities against potential threats from the feeble Spanish fleet. The Army performed well in combat in Cuba. However, it was too oriented to small posts in the West and not as well-prepared for an overseas conflict. It relied on volunteers and state militia units, which faced logistical, training and food problems in the staging areas in Florida. The United States freed Cuba (after an occupation by the U.S. Army). By the peace treaty Spain ceded to the United States its colonies of Puerto Rico, Guam, and the Philippines. The Navy set up coaling stations there and in Hawaii (which voluntarily joined the U.S. in 1898). The U.S. Navy now had a major forward presence across the Pacific and (with the lease of Guantánamo Bay Naval Base in Cuba) a major base in the Caribbean guarding the approaches to the Gulf Coast and the Panama Canal.
The Philippine–American War (1899–1902) was an armed conflict between a group of Filipino revolutionaries and the American forces following the ceding of the Philippines to the United States after the defeat of Spanish forces in the Battle of Manila. The Army sent in 100,000 soldiers (mostly from the National Guard) under General Elwell Otis. Defeated in the field and losing its capital in March 1899, the poorly armed and poorly led rebels broke into armed bands. The insurgency collapsed in March 1901 when the leader Emilio Aguinaldo was captured by General Frederick Funston and his Macabebe allies. Casualties included 1,037 Americans killed in action and 3,340 who died from disease; 20,000 rebels were killed.
The loss of eight battleships and 2,403 Americans at Pearl Harbor forced the U.S. to rely on its remaining aircraft carriers, which won a major victory over Japan at Midway just six months into the war, and on its growing submarine fleet. The Navy and Marine Corps followed this up with an island hopping campaign across the central and south Pacific in 1943–45, reaching the outskirts of Japan in the Battle of Okinawa. During 1942 and 1943, the U.S. deployed millions of men and thousands of planes and tanks to the UK, beginning with the strategic bombing of Nazi Germany and occupied Europe and leading up to the Allied invasions of occupied North Africa in November 1942, Sicily and Italy in 1943, France in 1944, and the invasion of Germany in 1945, parallel with the Soviet invasion from the east. That led to the surrender of Nazi Germany in May 1945. In the Pacific, the U.S. experienced much success in naval campaigns during 1944, but bloody battles at Iwo Jima and Okinawa in 1945 led the U.S. to look for a way to end the war with minimal loss of American lives. The U.S. used atomic bombs on Hiroshima and Nagasaki to destroy the Japanese war effort and to shock the Japanese leadership, which quickly caused the surrender of Japan.
The Korean War was a conflict between the United States and its United Nations allies and the communist powers under influence of the Soviet Union (also a UN member nation) and the People's Republic of China (which later also gained UN membership). The principal combatants were North and South Korea. Principal allies of South Korea included the United States, Canada, Australia, the United Kingdom, although many other nations sent troops under the aegis of the United Nations. Allies of North Korea included the People's Republic of China, which supplied military forces, and the Soviet Union, which supplied combat advisors and aircraft pilots, as well as arms, for the Chinese and North Korean troops.
The war started badly for the US and UN. North Korean forces struck massively in the summer of 1950 and nearly drove the outnumbered US and ROK defenders into the sea. However the United Nations intervened, naming Douglas MacArthur commander of its forces, and UN-US-ROK forces held a perimeter around Pusan, gaining time for reinforcement. MacArthur, in a bold but risky move, ordered an amphibious invasion well behind the front lines at Inchon, cutting off and routing the North Koreans and quickly crossing the 38th Parallel into North Korea. As UN forces continued to advance toward the Yalu River on the border with Communist China, the Chinese crossed the Yalu River in October and launched a series of surprise attacks that sent the UN forces reeling back across the 38th Parallel. Truman originally wanted a Rollback strategy to unify Korea; after the Chinese successes he settled for a Containment policy to split the country. MacArthur argued for rollback but was fired by President Harry Truman after disputes over the conduct of the war. Peace negotiations dragged on for two years until President Dwight D. Eisenhower threatened China with nuclear weapons; an armistice was quickly reached with the two Koreas remaining divided at the 38th parallel. North and South Korea are still today in a state of war, having never signed a peace treaty, and American forces remain stationed in South Korea as part of American foreign policy.
Fighting on one side was a coalition of forces including the Republic of Vietnam (South Vietnam or the "RVN"), the United States, supplemented by South Korea, Thailand, Australia, New Zealand, and the Philippines. The allies fought against the North Vietnamese Army (NVA) as well as the National Liberation Front (NLF, also known as Viet communists Viet Cong), or "VC", a guerrilla force within South Vietnam. The NVA received substantial military and economic aid from the Soviet Union and China, turning Vietnam into a proxy war.
The military history of the American side of the war involved different strategies over the years. The bombing campaigns of the Air Force were tightly controlled by the White House for political reasons, and until 1972 avoided the main Northern cities of Hanoi and Haiphong and concentrated on bombing jungle supply trails, especially the Ho Chi Minh Trail. The most controversial Army commander was William Westmoreland whose strategy involved systematic defeat of all enemy forces in the field, despite heavy American casualties that alienated public opinion back home.
In 1983 fighting between Palestinian refugees and Lebanese factions reignited that nation's long-running civil war. A UN agreement brought an international force of peacekeepers to occupy Beirut and guarantee security. US Marines landed in August 1982 along with Italian and French forces. On October 23, 1983, a suicide bomber driving a truck filled with 6 tons of TNT crashed through a fence and destroyed the Marine barracks, killing 241 Marines; seconds later, a second bomber leveled a French barracks, killing 58. Subsequently the US Navy engaged in bombing of militia positions inside Lebanon. While US President Ronald Reagan was initially defiant, political pressure at home eventually forced the withdrawal of the Marines in February 1984.
However, the battle was one-sided almost from the beginning. The reasons for this are the subject of continuing study by military strategists and academics. There is general agreement that US technological superiority was a crucial factor but the speed and scale of the Iraqi collapse has also been attributed to poor strategic and tactical leadership and low morale among Iraqi troops, which resulted from a history of incompetent leadership. After devastating initial strikes against Iraqi air defenses and command and control facilities on 17 January 1991, coalition forces achieved total air superiority almost immediately. The Iraqi air force was destroyed within a few days, with some planes fleeing to Iran, where they were interned for the duration of the conflict. The overwhelming technological advantages of the US, such as stealth aircraft and infrared sights, quickly turned the air war into a "turkey shoot". The heat signature of any tank which started its engine made an easy target. Air defense radars were quickly destroyed by radar-seeking missiles fired from wild weasel aircraft. Grainy video clips, shot from the nose cameras of missiles as they aimed at impossibly small targets, were a staple of US news coverage and revealed to the world a new kind of war, compared by some to a video game. Over 6 weeks of relentless pounding by planes and helicopters, the Iraqi army was almost completely beaten but did not retreat, under orders from Iraqi President Saddam Hussein, and by the time the ground forces invaded on 24 February, many Iraqi troops quickly surrendered to forces much smaller than their own; in one instance, Iraqi forces attempted to surrender to a television camera crew that was advancing with coalition forces.
After just 100 hours of ground combat, and with all of Kuwait and much of southern Iraq under coalition control, US President George H. W. Bush ordered a cease-fire and negotiations began resulting in an agreement for cessation of hostilities. Some US politicians were disappointed by this move, believing Bush should have pressed on to Baghdad and removed Hussein from power; there is little doubt that coalition forces could have accomplished this if they had desired. Still, the political ramifications of removing Hussein would have broadened the scope of the conflict greatly, and many coalition nations refused to participate in such an action, believing it would create a power vacuum and destabilize the region.
The War on Terrorism is a global effort by the governments of several countries (primarily the United States and its principal allies) to neutralize international terrorist groups (primarily Islamic Extremist terrorist groups, including al-Qaeda) and ensure that countries considered by the US and some of its allies to be Rogue Nations no longer support terrorist activities. It has been adopted primarily as a response to the September 11, 2001 attacks on the United States. Since 2001, terrorist motivated attacks upon service members have occurred in Arkansas and Texas.
After the lengthy Iraq disarmament crisis culminated with an American demand that Iraqi President Saddam Hussein leave Iraq, which was refused, a coalition led by the United States and the United Kingdom fought the Iraqi army in the 2003 invasion of Iraq. Approximately 250,000 United States troops, with support from 45,000 British, 2,000 Australian and 200 Polish combat forces, entered Iraq primarily through their staging area in Kuwait. (Turkey had refused to permit its territory to be used for an invasion from the north.) Coalition forces also supported Iraqi Kurdish militia, estimated to number upwards of 50,000. After approximately three weeks of fighting, Hussein and the Ba'ath Party were forcibly removed, followed by 9 years of military presence by the United States and the coalition fighting alongside the newly elected Iraqi government against various insurgent groups.
As a result of the Libyan Civil War, the United Nations enacted United Nations Security Council Resolution 1973, which imposed a no-fly zone over Libya, and the protection of civilians from the forces of Muammar Gaddafi. The United States, along with Britain, France and several other nations, committed a coalition force against Gaddafi's forces. On 19 March, the first U.S. action was taken when 114 Tomahawk missiles launched by US and UK warships destroyed shoreline air defenses of the Gaddafi regime. The U.S. continued to play a major role in Operation Unified Protector, the NATO-directed mission that eventually incorporated all of the military coalition's actions in the theater. Throughout the conflict however, the U.S. maintained it was playing a supporting role only and was following the UN mandate to protect civilians, while the real conflict was between Gaddafi's loyalists and Libyan rebels fighting to depose him. During the conflict, American drones were also deployed.
General George Washington (1732–99) proved an excellent organizer and administrator, who worked successfully with Congress and the state governors, selecting and mentoring his senior officers, supporting and training his troops, and maintaining an idealistic Republican Army. His biggest challenge was logistics, since neither Congress nor the states had the funding to provide adequately for the equipment, munitions, clothing, paychecks, or even the food supply of the soldiers. As a battlefield tactician Washington was often outmaneuvered by his British counterparts. As a strategist, however, he had a better idea of how to win the war than they did. The British sent four invasion armies. Washington's strategy forced the first army out of Boston in 1776, and was responsible for the surrender of the second and third armies at Saratoga (1777) and Yorktown (1781). He limited the British control to New York and a few places while keeping Patriot control of the great majority of the population. The Loyalists, on whom the British had relied too heavily, comprised about 20% of the population but never were well organized. As the war ended, Washington watched proudly as the final British army quietly sailed out of New York City in November 1783, taking the Loyalist leadership with them. Washington astonished the world when, instead of seizing power, he retired quietly to his farm in Virginia.
The Berbers along the Barbary Coast (modern day Libya) sent pirates to capture merchant ships and hold the crews for ransom. The U.S. paid protection money until 1801, when President Thomas Jefferson refused to pay and sent in the Navy to challenge the Barbary States, the First Barbary War followed. After the U.S.S. Philadelphia was captured in 1803, Lieutenant Stephen Decatur led a raid which successfully burned the captured ship, preventing Tripoli from using or selling it. In 1805, after William Eaton captured the city of Derna, Tripoli agreed to a peace treaty. The other Barbary states continued to raid U.S. shipping, until the Second Barbary War in 1815 ended the practice.
The American Civil War caught both sides unprepared. The Confederacy hoped to win by getting Britain and France to intervene, or else by wearing down the North's willingness to fight. The U.S. sought a quick victory focused on capturing the Confederate capital at Richmond, Virginia. The Confederates under Robert E. Lee tenaciously defended their capital until the very end. The war spilled across the continent, and even to the high seas. Most of the material and personnel of the South were used up, while the North prospered.
The Navy was modernized in the 1880s, and by the 1890s had adopted the naval power strategy of Captain Alfred Thayer Mahan—as indeed did every major navy. The old sailing ships were replaced by modern steel battleships, bringing them in line with the navies of Britain and Germany. In 1907, most of the Navy's battleships, with several support vessels, dubbed the Great White Fleet, were featured in a 14-month circumnavigation of the world. Ordered by President Theodore Roosevelt, it was a mission designed to demonstrate the Navy's capability to extend to the global theater.
The Mexican Revolution involved a civil war with hundreds of thousands of deaths and large numbers fleeing combat zones. Tens of thousands fled to the U.S. President Wilson sent U.S. forces to occupy the Mexican city of Veracruz for six months in 1914. It was designed to show the U.S. was keenly interested in the civil war and would not tolerate attacks on Americans, especially the April 9, 1914, "Tampico Affair", which involved the arrest of American sailors by soldiers of the regime of Mexican President Victoriano Huerta. In early 1916 Pancho Villa a Mexican general ordered 500 soldiers on a murderous raid on the American city of Columbus New Mexico, with the goal of robbing banks to fund his army. The German Secret Service encouraged Pancho Villa in his attacks to involve the United States in an intervention in Mexico which would distract the United States from its growing involvement in the war and divert aid from Europe to support the intervention. Wilson called up the state militias (National Guard) and sent them and the U.S. Army under General John J. Pershing to punish Villa in the Pancho Villa Expedition. Villa fled, with the Americans in pursuit deep into Mexico, thereby arousing Mexican nationalism. By early 1917 President Venustiano Carranza had contained Villa and secured the border, so Wilson ordered Pershing to withdraw.
After the costly U.S. involvement in World War I, isolationism grew within the nation. Congress refused membership in the League of Nations, and in response to the growing turmoil in Europe and Asia, the gradually more restrictive Neutrality Acts were passed, which were intended to prevent the U.S. from supporting either side in a war. President Franklin D. Roosevelt sought to support Britain, however, and in 1940 signed the Lend-Lease Act, which permitted an expansion of the "cash and carry" arms trade to develop with Britain, which controlled the Atlantic sea lanes.
World War II holds a special place in the American psyche as the country's greatest triumph, and the U.S. military personnel of World War II are frequently referred to as "the Greatest Generation." Over 16 million served (about 11% of the population), and over 400,000 died during the war. The U.S. emerged as one of the two undisputed superpowers along with the Soviet Union, and unlike the Soviet Union, the U.S. homeland was virtually untouched by the ravages of war. During and following World War II, the United States and Britain developed an increasingly strong defense and intelligence relationship. Manifestations of this include extensive basing of U.S. forces in the UK, shared intelligence, shared military technology (e.g. nuclear technology), and shared procurement.
The U.S. framed the war as part of its policy of containment of Communism in south Asia, but American forces were frustrated by an inability to engage the enemy in decisive battles, corruption and incompetence in the Army of the Republic of Vietnam, and ever increasing protests at home. The Tet Offensive in 1968, although a major military defeat for the NLF with half their forces eliminated, marked the psychological turning point in the war. With President Richard M. Nixon opposed to containment and more interested in achieving détente with both the Soviet Union and China, American policy shifted to "Vietnamization," – providing very large supplies of arms and letting the Vietnamese fight it out themselves. After more than 57,000 dead and many more wounded, American forces withdrew in 1973 with no clear victory, and in 1975 South Vietnam was finally conquered by communist North Vietnam and unified.
Ongoing political tensions between Great Britain and the thirteen colonies reached a crisis in 1774 when the British placed the province of Massachusetts under martial law after the Patriots protested taxes they regarded as a violation of their constitutional rights as Englishmen. When shooting began at Lexington and Concord in April 1775, militia units from across New England rushed to Boston and bottled up the British in the city. The Continental Congress appointed George Washington as commander-in-chief of the newly created Continental Army, which was augmented throughout the war by colonial militia. He drove the British out of Boston but in late summer 1776 they returned to New York and nearly captured Washington's army. Meanwhile, the revolutionaries expelled British officials from the 13 states, and declared themselves an independent nation on July 4, 1776.
Following the American Revolutionary War, the United States faced potential military conflict on the high seas as well as on the western frontier. The United States was a minor military power during this time, having only a modest army, Marine corps, and navy. A traditional distrust of standing armies, combined with faith in the abilities of local militia, precluded the development of well-trained units and a professional officer corps. Jeffersonian leaders preferred a small army and navy, fearing that a large military establishment would involve the United States in excessive foreign wars, and potentially allow a domestic tyrant to seize power.
After the Civil War, population expansion, railroad construction, and the disappearance of the buffalo herds heightened military tensions on the Great Plains. Several tribes, especially the Sioux and Comanche, fiercely resisted confinement to reservations. The main role of the Army was to keep indigenous peoples on reservations and to end their wars against settlers and each other, William Tecumseh Sherman and Philip Sheridan were in charge. A famous victory for the Plains Nations was the Battle of the Little Big Horn in 1876, when Col. George Armstrong Custer and two hundred plus members of the 7th Cavalry were killed by a force consisting of Native Americans from the Lakota, Northern Cheyenne, and Arapaho nations. The last significant conflict came in 1891.
Rear Admiral Bradley A. Fiske was at the vanguard of new technology in naval guns and gunnery, thanks to his innovations in fire control 1890–1910. He immediately grasped the potential for air power, and called for the development of a torpedo plane. Fiske, as aide for operations in 1913–15 to Assistant Secretary Franklin D. Roosevelt, proposed a radical reorganization of the Navy to make it a war-fighting instrument. Fiske wanted to centralize authority in a chief of naval operations and an expert staff that would develop new strategies, oversee the construction of a larger fleet, coordinate war planning including force structure, mobilization plans, and industrial base, and ensure that the US Navy possessed the best possible war machines. Eventually, the Navy adopted his reforms and by 1915 started to reorganize for possible involvement in the World War then underway.
By summer 1918, a million American soldiers, or "doughboys" as they were often called, of the American Expeditionary Forces were in Europe under the command of John J. Pershing, with 25,000 more arriving every week. The failure of Germany's spring offensive exhausted its reserves and they were unable to launch new offensives. The German Navy and home front then revolted and a new German government signed a conditional surrender, the Armistice, ending the war against the western front on November 11, 1918.
Starting in 1940 (18 months before Pearl Harbor), the nation mobilized, giving high priority to air power. American involvement in World War II in 1940–41 was limited to providing war material and financial support to Britain, the Soviet Union, and the Republic of China. The U.S. entered officially on 8 December 1941 following the Japanese attack on Pearl Harbor, Hawaii. Japanese forces soon seized American, Dutch, and British possessions across the Pacific and Southeast Asia, except for Australia, which became a main American forward base along with Hawaii.
The Vietnam War was a war fought between 1959 and 1975 on the ground in South Vietnam and bordering areas of Cambodia and Laos (see Secret War) and in the strategic bombing (see Operation Rolling Thunder) of North Vietnam. American advisors came in the late 1950s to help the RVN (Republic of Vietnam) combat Communist insurgents known as "Viet Cong." Major American military involvement began in 1964, after Congress provided President Lyndon B. Johnson with blanket approval for presidential use of force in the Gulf of Tonkin Resolution.
Before the war, many observers believed the US and its allies could win but might suffer substantial casualties (certainly more than any conflict since Vietnam), and that the tank battles across the harsh desert might rival those of North Africa during World War II. After nearly 50 years of proxy wars, and constant fears of another war in Europe between NATO and the Warsaw Pact, some thought the Persian Gulf War might finally answer the question of which military philosophy would have reigned supreme. Iraqi forces were battle-hardened after 8 years of war with Iran, and they were well equipped with late model Soviet tanks and jet fighters, but the antiaircraft weapons were crippled; in comparison, the US had no large-scale combat experience since its withdrawal from Vietnam nearly 20 years earlier, and major changes in US doctrine, equipment and technology since then had never been tested under fire.
With the emergence of ISIL and its capture of large areas of Iraq and Syria, a number of crises resulted that sparked international attention. ISIL had perpetrated sectarian killings and war crimes in both Iraq and Syria. Gains made in the Iraq war were rolled back as Iraqi army units abandoned their posts. Cities were taken over by the terrorist group which enforced its brand of Sharia law. The kidnapping and decapitation of numerous Western journalists and aid-workers also garnered interest and outrage among Western powers. The US intervened with airstrikes in Iraq over ISIL held territories and assets in August, and in September a coalition of US and Middle Eastern powers initiated a bombing campaign in Syria aimed at degrading and destroying ISIL and Al-Nusra-held territory.
Computer security, also known as cybersecurity or IT security, is the protection of information systems from theft or damage to the hardware, the software, and to the information on them, as well as from disruption or misdirection of the services they provide. It includes controlling physical access to the hardware, as well as protecting against harm that may come via network access, data and code injection, and due to malpractice by operators, whether intentional, accidental, or due to them being tricked into deviating from secure procedures.
Denial of service attacks are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of Distributed denial of service (DDoS) attacks are possible, where the attack comes from a large number of points – and defending is much more difficult. Such attacks can originate from the zombie computers of a botnet, but a range of other techniques are possible including reflection and amplification attacks, where innocent systems are fooled into sending traffic to the victim.
If access is gained to a car's internal controller area network, it is possible to disable the brakes and turn the steering wheel. Computerized engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver assistance systems make these disruptions possible, and self-driving cars go even further. Connected cars may use wifi and bluetooth to communicate with onboard consumer devices, and the cell phone network to contact concierge and emergency assistance services or get navigational or entertainment information; each of these networks is a potential entry point for malware or an attacker. Researchers in 2011 were even able to use a malicious compact disc in a car's stereo system as a successful attack vector, and cars with built-in voice recognition or remote assistance features have onboard microphones which could be used for eavesdropping.
However, relatively few organisations maintain computer systems with effective detection systems, and fewer still have organised response mechanisms in place. As result, as Reuters points out: "Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets". The primary obstacle to effective eradication of cyber crime could be traced to excessive reliance on firewalls and other automated "detection" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.
One use of the term "computer security" refers to technology that is used to implement secure operating systems. In the 1980s the United States Department of Defense (DoD) used the "Orange Book" standards, but the current international standard ISO/IEC 15408, "Common Criteria" defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being "Methodically Designed, Tested and Reviewed", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 ("Semiformally Verified Design and Tested") system is Integrity-178B, which is used in the Airbus A380 and several military jets.
China's network security and information technology leadership team was established February 27, 2014. The leadership team is tasked with national security and long-term development and co-ordination of major issues related to network security and information technology. Economic, political, cultural, social and military fields as related to network security and information technology strategy, planning and major macroeconomic policy are being researched. The promotion of national network security and information technology law are constantly under study for enhanced national security capabilities.
Eavesdropping is the act of surreptitiously listening to a private conversation, typically between hosts on a network. For instance, programs such as Carnivore and NarusInsight have been used by the FBI and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon via monitoring the faint electro-magnetic transmissions generated by the hardware; TEMPEST is a specification by the NSA referring to these attacks.
Desktop computers and laptops are commonly infected with malware either to gather passwords or financial account information, or to construct a botnet to attack another target. Smart phones, tablet computers, smart watches, and other mobile devices such as Quantified Self devices like activity trackers have also become targets and many of these have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. Wifi, Bluetooth, and cell phone network on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.
Within computer systems, two of many security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.[citation needed]
In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.
In July of 2015, a hacker group known as "The Impact Team" successfully breached the extramarital relationship website Ashley Madison. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. With this initial data release, the group stated “Avid Life Media has been instructed to take Ashley Madison and Established Men offline permanently in all forms, or we will release all customer records, including profiles with all the customers' secret sexual fantasies and matching credit card transactions, real names and addresses, and employee documents and emails. The other websites may stay online.”  When Avid Life Media, the parent company that created the Ashley Madison website, did not take the site offline, The Impact Group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned, but the website remained functional.
The question of whether the government should intervene or not in the regulation of the cyberspace is a very polemical one. Indeed, for as long as it has existed and by definition, the cyberspace is a virtual space free of any government intervention. Where everyone agree that an improvement on cybersecurity is more than vital, is the government the best actor to solve this issue? Many government officials and experts think that the government should step in and that there is a crucial need for regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the "industry only responds when you threaten regulation. If industry doesn't respond (to the threat), you have to follow through." On the other hand, executives from the private sector agree that improvements are necessary, but think that the government intervention would affect their ability to innovate efficiently.
On October 3, 2010, Public Safety Canada unveiled Canada’s Cyber Security Strategy, following a Speech from the Throne commitment to boost the security of Canadian cyberspace. The aim of the strategy is to strengthen Canada’s "cyber systems and critical infrastructure sectors, support economic growth and protect Canadians as they connect to each other and to the world." Three main pillars define the strategy: securing government systems, partnering to secure vital cyber systems outside the federal government, and helping Canadians to be secure online. The strategy involves multiple departments and agencies across the Government of Canada. The Cyber Incident Management Framework for Canada outlines these responsibilities, and provides a plan for coordinated response between government and other partners in the event of a cyber incident. The Action Plan 2010–2015 for Canada's Cyber Security Strategy outlines the ongoing implementation of the strategy.
Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable to physical damage caused by malicious commands sent to industrial equipment (in that case uranium enrichment centrifuges) which are infected via removable media. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.
Today, computer security comprises mainly "preventive" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real time filtering and blocking. Another implementation is a so-called physical firewall which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.
Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. "Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal."
While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.
Public Safety Canada’s Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada’s critical infrastructure and cyber systems. The CCIRC provides support to mitigate cyber threats, technical support to respond and recover from targeted cyber attacks, and provides online tools for members of Canada’s critical infrastructure sectors. The CCIRC posts regular cyber security bulletins on the Public Safety Canada website. The CCIRC also operates an online reporting tool where individuals and organizations can report a cyber incident. Canada's Cyber Security Strategy is part of a larger, integrated approach to critical infrastructure protection, and functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure.
This has led to new terms such as cyberwarfare and cyberterrorism. More and more critical infrastructure is being controlled via computer programs that, while increasing efficiency, exposes new vulnerabilities. The test will be to see if governments and corporations that control critical systems such as energy, communications and other information will be able to prevent attacks before they occur. As Jay Cross, the chief scientist of the Internet Time Group, remarked, "Connectedness begets vulnerability."
On September 27, 2010, Public Safety Canada partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations dedicated to informing the general public on how to protect themselves online. On February 4, 2014, the Government of Canada launched the Cyber Security Cooperation Program. The program is a $1.5 million five-year initiative aimed at improving Canada’s cyber systems through grants and contributions to projects in support of this objective. Public Safety Canada aims to begin an evaluation of Canada's Cyber Security Strategy in early 2015. Public Safety Canada administers and routinely updates the GetCyberSafe portal for Canadian citizens, and carries out Cyber Security Awareness Month during October.
An unauthorized user gaining physical access to a computer is most likely able to directly download data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, or covert listening devices. Even when the system is protected by standard security measures, these may be able to be by passed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.
Clickjacking, also known as "UI redress attack or User Interface redress attack", is a malicious technique in which an attacker tricks a user into clicking on a button or link on another webpage while the user intended to click on the top level page. This is done using multiple transparent or opaque layers. The attacker is basically "hijacking" the clicks meant for the top level page and routing them to some other irrelevant page, most likely owned by someone else. A similar technique can be used to hijack keystrokes. Carefully drafting a combination of stylesheets, iframes, buttons and text boxes, a user can be led into believing that they are typing the password or other information on some authentic webpage while it is being channeled into an invisible frame controlled by the attacker.
In 1988, only 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On November 2, 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet "computer worm". The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris, Jr. who said 'he wanted to count how many machines were connected to the Internet'.
In 2013 and 2014, a Russian/Ukrainian hacking ring known as "Rescator" broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. "The malware utilized is absolutely unsophisticated and uninteresting," says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.
Berlin starts National Cyber Defense Initiative: On June 16, 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt für Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt für den Militärischen Abschirmdienst and other national organisations in Germany taking care of national security aspects. According to the Minister the primary task of the new organisation founded on February 23, 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet.
In psychology, memory is the process in which information is encoded, stored, and retrieved. Encoding allows information from the outside world to be sensed in the form of chemical and physical stimuli. In the first stage the information must be changed so that it may be put into the encoding process. Storage is the second memory stage or process. This entails that information is maintained over short periods of time. Finally the third process is the retrieval of information that has been stored. Such information must be located and returned to the consciousness. Some retrieval attempts may be effortless due to the type of information, and other attempts to remember stored information may be more demanding for various reasons.
Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent a visual code. Conrad (1964) found that test subjects had more difficulty recalling collections of letters that were acoustically similar (e.g. E, P, D). Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad's (1964) study, however, deals with the encoding of written text; thus, while memory of written language may rely on acoustic components, generalisations to all forms of memory cannot be made.
Short-term memory is also known as working memory. Short-term memory allows recall for a period of several seconds to a minute without rehearsal. Its capacity is also very limited: George A. Miller (1956), when working at Bell Laboratories, conducted experiments showing that the store of short-term memory was 7±2 items (the title of his famous paper, "The magical number 7±2"). Modern estimates of the capacity of short-term memory are lower, typically of the order of 4–5 items; however, memory capacity can be increased through a process called chunking. For example, in recalling a ten-digit telephone number, a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456) and lastly a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This may be reflected in some countries in the tendency to display telephone numbers as several chunks of two to four numbers.
The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration, which means that information is not retained indefinitely. By contrast, long-term memory can store much larger quantities of information for potentially unlimited duration (sometimes a whole life span). Its capacity is immeasurable. For example, given a random seven-digit number we may remember it for only a few seconds before forgetting, suggesting it was stored in our short-term memory. On the other hand, we can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory.
The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient 'KF' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was brain damaged, displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Interestingly, visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory.
Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. Without the hippocampus, new memories are unable to be stored into long-term memory, as learned from patient Henry Molaison after removal of both his hippocampi, and there will be a very short attention span. Furthermore, it may be involved in changing neural connections for a period of three months or more after the initial learning.
Sensory memory holds sensory information less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is the example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to "see" more than they can actually report. The first experiments exploring this form of sensory memory were conducted by George Sperling (1963) using the "partial report paradigm". Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments,Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the "whole report" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal.
While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966) discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, "which attempts to capture information such as 'what', 'when' and 'where'". With episodic memory, individuals are able to recall specific events such as birthday parties and weddings.
Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children’s memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants’ recognition memory and their recall memory. Habituation and operant conditioning techniques have been used to assess infants’ recognition memory and the deferred and elicited imitation techniques have been used to assess infants’ recall memory.
Another major way to distinguish different memory functions is whether the content to be remembered is in the past, retrospective memory, or in the future, prospective memory. Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or remembering to remember (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory.
Hebb distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of cortisol or epinephrine help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause memory loss and problems with memory storage. This memory loss includes, retrograde amnesia which is the loss of memory for events that occurred shortly before the time of brain damage.
One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved.
Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal representation implies that such definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called engram or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007).
In contrast, procedural memory (or implicit memory) is not based on the conscious recall of information, but on implicit learning. It can best be summarized as remember how to do something. Procedural memory is primarily employed in learning motor skills and should be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition - no new explicit memories have been formed, but one is unconsciously accessing aspects of those previous experiences. Procedural memory involved in motor learning depends on the cerebellum and basal ganglia.
The working memory model explains many practical observations, such as why it is easier to do two different tasks (one verbal and one visual) than two similar tasks (e.g., two visual), and the aforementioned word-length effect. However, the concept of a central executive as noted here has been criticised as inadequate and vague.[citation needed] Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics.
One of the key concerns of older adults is the experience of memory loss, especially as it is one of the hallmark symptoms of Alzheimer's disease. However, memory loss is qualitatively different in normal aging from the kind of memory loss associated with a diagnosis of Alzheimer's (Budson & Price, 2005). Research has revealed that individuals’ performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information; source memory tasks that require them to remember the specific circumstances or context in which they learned information; and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example.
It should be noted that although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order - that is, recalling step 1 and then step 2. In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence. Researchers have suggested that these age differences are probably due to the fact that the dentate gyrus of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months.
Declarative memory can be further sub-divided into semantic memory, concerning principles and facts taken independent of context; and episodic memory, concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract knowledge about the world, such as "Paris is the capital of France". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal associations of a particular place or time. Episodic memories often reflect the "firsts" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one's life that can be remembered clearly. Autobiographical memory - memory for particular events within one's own life - is generally viewed as either equivalent to, or a subset of, episodic memory. Visual memory is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a mental image. Visual memory can result in priming and it is assumed some kind of perceptual representational system underlies this phenomenon.[citation needed]
Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats. An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or ‘Socially Evaluated Cold Pressor Test’) for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process.
Interference can hamper memorization and retrieval. There is retroactive interference, when learning new information makes it harder to recall old information and proactive interference, where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer.
Up until the middle of the 1980s it was assumed that infants could not encode, retain, and retrieve information. A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay. Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months. In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it.
Brain areas involved in the neuroanatomy of memory such as the hippocampus, the amygdala, the striatum, or the mammillary bodies are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and declarative learning, while the amygdala is thought to be involved in emotional memory. Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway traveling through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart, learning, as solely dependent on specific brain regions. Learning and memory are attributed to changes in neuronal synapses, thought to be mediated by long-term potentiation and long-term depression.
The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they have often difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life.
Stressful life experiences may be a cause of memory loss as a person ages. Glucocorticoids that are released during stress damage neurons that are located in the hippocampal region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The CA1 neurons found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of glutamate. This high level of extracellular glutamate allow calcium to enter NMDA receptors which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind. This directly relates to traumatic events in one's past such as kidnappings, being prisoners of war or sexual abuse as a child.
Sleep does not affect acquisition or recall while one is awake. Therefore, sleep has the greatest effect on memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain’s abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). This process implicates that memories are reactivated during sleep, but that the process doesn’t enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. When you are sleeping, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When you do not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. Therefore, it is important to get the proper amount of sleep so that memory can function at the highest level. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, suggesting that new memories may be solidified through such rehearsal.
A UCLA research study published in the June 2006 issue of the American Journal of Geriatric Psychiatry found that people can improve cognitive function and brain efficiency through simple lifestyle changes such as incorporating memory exercises, healthy eating, physical fitness and stress reduction into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance. Eight subjects were asked to follow a "brain healthy" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long term follow up was conducted, it is therefore unclear if this intervention has lasting effects on memory.
Much of the current knowledge of memory has come from studying memory disorders, particularly amnesia. Loss of memory is known as amnesia. Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus. There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain's memory systems, and thus hypothesize their function in the normally working brain. Other neurological disorders such as Alzheimer's disease and Parkinson's disease can also affect memory and cognition. Hyperthymesia, or hyperthymesic syndrome, is a disorder that affects an individual's autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored. Korsakoff's syndrome, also known as Korsakoff's psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex.
Physical exercise, particularly continuous aerobic exercises such as running, cycling and swimming, has many cognitive benefits and effects on the brain. Influences on the brain include increases in neurotransmitter levels, improved oxygen and nutrient delivery, and increased neurogenesis in the hippocampus. The effects of exercise on memory have important implications for improving children's academic performance, maintaining mental abilities in old age, and the prevention and potential cure of neurological diseases.
However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated. Seventy-two healthy female and male university students, randomly assigned to the SECPT stress test or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game "Concentration" or "Memory". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar.
Interestingly, research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996) asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events.
Although people often think that memory operates like recording equipment, it is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime. In fact, research has revealed that our memories are constructed. People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974) in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, "How fast were the cars going when they smashed into each other?" gave higher estimates than those who were asked, "How fast were the cars going when they hit each other?" Furthermore, when asked a week later whether they have seen broken glass in the film, those who had been asked the question with smashed were twice more likely to report that they have seen broken glass than those who had been asked the question with hit. There was no broken glass depicted in the film. Thus, the wording of the questions distorted viewers’ memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with smashed recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.
Memorization is a method of learning that allows an individual to recall information verbatim. Rote learning is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as Cosmos Rossellius using visual alphabets. The spacing effect shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is cramming: an intensive memorization in a short period of time. Also relevant is the Zeigarnik effect which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called Method of loci uses spatial memory to memorize non-spatial information.
New Delhi (i/ˌnjuː ˈdɛli/) is a municipality and district in Delhi which serves as the capital and seat of government of India. In addition, it also serves as the seat of Government of Delhi.
The foundation stone of the city was laid by George V, Emperor of India during the Delhi Durbar of 1911. It was designed by British architects, Sir Edwin Lutyens and Sir Herbert Baker. The new capital was inaugurated on 13 February 1931, by India's Viceroy Lord Irwin.
Although colloquially Delhi and New Delhi as names are used interchangeably to refer to the jurisdiction of NCT of Delhi, these are two distinct entities, and the latter is a small part of the former.
New Delhi has been selected as one of the hundred Indian cities to be developed as a smart city under PM Narendra Modi's flagship Smart Cities Mission.
Calcutta (now Kolkata) was the capital of India during the British Raj until December 1911. However, Delhi had served as the political and financial centre of several empires of ancient India and the Delhi Sultanate, most notably of the Mughal Empire from 1649 to 1857. During the early 1900s, a proposal was made to the British administration to shift the capital of the British Indian Empire (as it was officially called) from Calcutta to Delhi. Unlike Calcutta, which was located on the eastern coast of India, Delhi was at the centre of northern India and the Government of British India felt that it would be logistically easier to administer India from the latter rather than the former.
On 12 December 1911, during the Delhi Durbar, George V, then Emperor of India, along with Queen Mary, his Consort, made the announcement that the capital of the Raj was to be shifted from Calcutta to Delhi, while laying the foundation stone for the Viceroy's residence in the Coronation Park, Kingsway Camp. The foundation stone of New Delhi was laid by King George V and Queen Mary at the site of Delhi Durbar of 1911 at Kingsway Camp on 15 December 1911, during their imperial visit. Large parts of New Delhi were planned by Edwin Lutyens (Sir Edwin from 1918), who first visited Delhi in 1912, and Herbert Baker (Sir Herbert from 1926), both leading 20th-century British architects. The contract was given to Sobha Singh (later Sir Sobha Singh). Construction really began after World War I and was completed by 1931. The city that was later dubbed "Lutyens' Delhi" was inaugurated in ceremonies beginning on 10 February 1931 by Lord Irwin, the Viceroy. Lutyens designed the central administrative area of the city as a testament to Britain's imperial aspirations.
Soon Lutyens started considering other places. Indeed, the Delhi Town Planning Committee, set up to plan the new imperial capital, with George Swinton as chairman and John A. Brodie and Lutyens as members, submitted reports for both North and South sites. However, it was rejected by the Viceroy when the cost of acquiring the necessary properties was found to be too high. The central axis of New Delhi, which today faces east at India Gate, was previously meant to be a north-south axis linking the Viceroy's House at one end with Paharganj at the other. During the project's early years, many tourists believed it was a gate from Earth to Heaven itself. Eventually, owing to space constraints and the presence of a large number of heritage sites in the North side, the committee settled on the South site. A site atop the Raisina Hill, formerly Raisina Village, a Meo village, was chosen for the Rashtrapati Bhawan, then known as the Viceroy's House. The reason for this choice was that the hill lay directly opposite the Dinapanah citadel, which was also considered the site of Indraprastha, the ancient region of Delhi. Subsequently, the foundation stone was shifted from the site of Delhi Durbar of 1911–1912, where the Coronation Pillar stood, and embedded in the walls of the forecourt of the Secretariat. The Rajpath, also known as King's Way, stretched from the India Gate to the Rashtrapati Bhawan. The Secretariat building, the two blocks of which flank the Rashtrapati Bhawan and houses ministries of the Government of India, and the Parliament House, both designed by Herbert Baker, are located at the Sansad Marg and run parallel to the Rajpath.
In the south, land up to Safdarjung's Tomb was acquired in order to create what is today known as Lutyens' Bungalow Zone. Before construction could begin on the rocky ridge of Raisina Hill, a circular railway line around the Council House (now Parliament House), called the Imperial Delhi Railway, was built to transport construction material and workers for the next twenty years. The last stumbling block was the Agra-Delhi railway line that cut right through the site earmarked for the hexagonal All-India War Memorial (India Gate) and Kingsway (Rajpath), which was a problem because the Old Delhi Railway Station served the entire city at that time. The line was shifted to run along the Yamuna river, and it began operating in 1924. The New Delhi Railway Station opened in 1926 with a single platform at Ajmeri Gate near Paharganj and was completed in time for the city's inauguration in 1931. As construction of the Viceroy's House (the present Rashtrapati Bhavan), Central Secretariat, Parliament House, and All-India War Memorial (India Gate) was winding down, the building of a shopping district and a new plaza, Connaught Place, began in 1929, and was completed by 1933. Named after Prince Arthur, 1st Duke of Connaught (1850–1942), it was designed by Robert Tor Russell, chief architect to the Public Works Department (PWD).
After the capital of India moved to Delhi, a temporary secretariat building was constructed in a few months in 1912 in North Delhi. Most of the government offices of the new capital moved here from the 'Old secretariat' in Old Delhi (the building now houses the Delhi Legislative Assembly), a decade before the new capital was inaugurated in 1931. Many employees were brought into the new capital from distant parts of India, including the Bengal Presidency and Madras Presidency. Subsequently housing for them was developed around Gole Market area in the 1920s. Built in the 1940s, to house government employees, with bungalows for senior officials in the nearby Lodhi Estate area, Lodhi colony near historic Lodhi Gardens, was the last residential areas built by the British Raj.
After India gained independence in 1947, a limited autonomy was conferred to New Delhi and was administered by a Chief Commissioner appointed by the Government of India. In 1956, Delhi was converted into a union territory and eventually the Chief Commissioner was replaced by a Lieutenant Governor. The Constitution (Sixty-ninth Amendment) Act, 1991 declared the Union Territory of Delhi to be formally known as National Capital Territory of Delhi. A system was introduced under which the elected Government was given wide powers, excluding law and order which remained with the Central Government. The actual enforcement of the legislation came in 1993.
The first major extension of New Delhi outside of Lutyens' Delhi came in the 1950s when the Central Public Works Department (CPWD) developed a large area of land southwest of Lutyens' Delhi to create the diplomatic enclave of Chanakyapuri, where land was allotted for embassies, chanceries, high commissions and residences of ambassadors, around wide central vista, Shanti Path.
With a total area of 42.7 km2 (16.5 sq mi), New Delhi forms a small part of the Delhi metropolitan area. Because the city is located on the Indo-Gangetic Plain, there is little difference in elevation across the city. New Delhi and surrounding areas were once a part of the Aravalli Range; all that is left of those mountains is the Delhi Ridge, which is also called the Lungs of Delhi. While New Delhi lies on the floodplains of the Yamuna River, it is essentially a landlocked city. East of the river is the urban area of Shahdara. New Delhi falls under the seismic zone-IV, making it vulnerable to earthquakes.
New Delhi lies on several fault lines and thus experiences frequent earthquakes, most of them of mild intensity. There has, however, been a spike in the number of earthquakes in the last six years, most notable being a 5.4 magnitude earthquake in 2015 with its epicentre in Nepal, a 4.7-magnitude earthquake on 25 November 2007, a 4.2-magnitude earthquake on 7 September 2011, a 5.2-magnitude earthquake on 5 March 2012, and a swarm of twelve earthquakes, including four of magnitudes 2.5, 2.8, 3.1, and 3.3, on 12 November 2013.
The climate of New Delhi is a monsoon-influenced humid subtropical climate (Köppen Cwa) with high variation between summer and winter in terms of both temperature and rainfall. The temperature varies from 46 °C (115 °F) in summers to around 0 °C (32 °F) in winters. The area's version of a humid subtropical climate is noticeably different from many other cities with this climate classification in that it features long and very hot summers, relatively dry and mild winters, a monsoonal period, and dust storms. Summers are long, extending from early April to October, with the monsoon season occurring in the middle of the summer. Winter starts in November and peaks in January. The annual mean temperature is around 25 °C (77 °F); monthly daily mean temperatures range from approximately 14 to 34 °C (57 to 93 °F). New Delhi's highest temperature ever recorded is 49.1 °C (120.4 °F) while the lowest temperature ever recorded is −3.2 °C (26.2 °F). Those for Delhi metropolis stand at 49.9 °C (121.8 °F) and −3.2 °C (26.2 °F) respectively. The average annual rainfall is 784 millimetres (30.9 in), most of which is during the monsoons in July and August.
In recent Mercer’s 2015 annual quality-of-living survey, New Delhi ranks at number 154 out of 230 cities due to bad air quality and pollution. The World Health Organization ranked New Delhi as the world’s worst polluted city in 2014 among about 1,600 cities the organization tracked around the world.
In an attempt to curb air pollution in New Delhi, which gets worst during the winter, a temporary alternate-day travel scheme for cars using the odd- and even-numbered license plates system was announced by Delhi government in December 2015. In addition, trucks will be allowed to enter India's capital only after 11 p.m., two hours later than the existing restriction. The driving restriction scheme is planned to be implemented as a trial from January 1, 2016 for an initial period of 15 days. The restriction will be in force between 8 a.m. and 8 p.m., and traffic will not be restricted on Sundays. Public transportation service will be increased during the restriction period.
On December 16, 2015, the Supreme Court of India mandated several restrictions on Delhi's transportation system to curb pollution. Among the measures, the court ordered to stop registrations of diesel cars and sport utility vehicles with an engine capacity of 2,000 cc and over until March 31, 2016. The court also ordered all taxis in the Delhi region to switch to compressed natural gas by March 1, 2016. Transportation vehicles that are more than 10 years old were banned from entering the capital.
The national capital of India, New Delhi is jointly administered by both the Central Government of India and the local Government of Delhi, it is also the capital of the National Capital Territory (NCT) of Delhi.
As of 2015, the government structure of the New Delhi Municipal Council includes a chairperson, three members of New Delhi's Legislative Assembly, two members nominated by the Chief Minister of the NCT of Delhi and five members nominated by the central government.
The head of state of Delhi is the Lieutenant Governor of the Union Territory of Delhi, appointed by the President of India on the advice of the Central government and the post is largely ceremonial, as the Chief Minister of the Union Territory of Delhi is the head of government and is vested with most of the executive powers. According to the Indian constitution, if a law passed by Delhi's legislative assembly is repugnant to any law passed by the Parliament of India, then the law enacted by the parliament will prevail over the law enacted by the assembly.
New Delhi is governed through a municipal government, known as the New Delhi Municipal Council (NDMC). Other urban areas of the metropolis of Delhi are administered by the Municipal Corporation of Delhi (MCD). However, the entire metropolis of Delhi is commonly known as New Delhi in contrast to Old Delhi.
Much of New Delhi, planned by the leading 20th-century British architect Edwin Lutyens, was laid out to be the central administrative area of the city as a testament to Britain's imperial pretensions. New Delhi is structured around two central promenades called the Rajpath and the Janpath. The Rajpath, or King's Way, stretches from the Rashtrapati Bhavan to the India Gate. The Janpath (Hindi: "Path of the People"), formerly Queen's Way, begins at Connaught Circus and cuts the Rajpath at right angles. 19 foreign embassies are located on the nearby Shantipath (Hindi: "Path of Peace"), making it the largest diplomatic enclave in India.
At the heart of the city is the magnificent Rashtrapati Bhavan (formerly known as Viceroy's House) which sits atop Raisina Hill. The Secretariat, which houses ministries of the Government of India, flanks out of the Rashtrapati Bhavan. The Parliament House, designed by Herbert Baker, is located at the Sansad Marg, which runs parallel to the Rajpath. Connaught Place is a large, circular commercial area in New Delhi, modelled after the Royal Crescent in England. Twelve separate roads lead out of the outer ring of Connaught Place, one of them being the Janpath.
Indira Gandhi International Airport, situated to the southwest of Delhi, is the main gateway for the city's domestic and international civilian air traffic. In 2012-13, the airport was used by more than 35 million passengers, making it one of the busiest airports in South Asia. Terminal 3, which cost ₹96.8 billion (US$1.4 billion) to construct between 2007 and 2010, handles an additional 37 million passengers annually.
The Delhi Flying Club, established in 1928 with two de Havilland Moth aircraft named Delhi and Roshanara, was based at Safdarjung Airport which started operations in 1929, when it was the Delhi's only airport and the second in India. The airport functioned until 2001, however in January 2002 the government closed the airport for flying activities because of security concerns following the New York attacks in September 2001. Since then, the club only carries out aircraft maintenance courses, and is used for helicopter rides to Indira Gandhi International Airport for VIP including the president and the prime minister.
In 2010, Indira Gandhi International Airport (IGIA) was conferred the fourth best airport award in the world in the 15–25 million category, and Best Improved Airport in the Asia-Pacific Region by Airports Council International. The airport was rated as the Best airport in the world in the 25–40 million passengers category in 2015, by Airports Council International.[not in citation given][better source needed] Delhi Airport also bags two awards for The Best Airport in Central Asia/India and Best Airport Staff in Central Asia/India at the Skytrax World Airport Awards 2015.
New Delhi has one of India's largest bus transport systems. Buses are operated by the state-owned Delhi Transport Corporation (DTC), which owns largest fleet of compressed natural gas (CNG)-fueled buses in the world. Personal vehicles especially cars also form a major chunk of vehicles plying on New Delhi roads. New Delhi has the highest number of registered cars compared to any other metropolitan city in India. Taxis and Auto Rickshaws also ply on New Delhi roads in large numbers. New Delhi has one of the highest road density in India.
New Delhi is a major junction in the Indian railway network and is the headquarters of the Northern Railway. The five main railway stations are New Delhi railway station, Old Delhi, Nizamuddin Railway Station, Anand Vihar Railway Terminal and Sarai Rohilla. The Delhi Metro, a mass rapid transit system built and operated by Delhi Metro Rail Corporation (DMRC), serves many parts of Delhi and the neighbouring cities Faridabad, Gurgaon, Noida and Ghaziabad. As of August 2011, the metro consists of six operational lines with a total length of 189 km (117 mi) and 146 stations, and several other lines are under construction. It carries millions of passengers every day. In addition to the Delhi Metro, a suburban railway, the Delhi Suburban Railway exists.
The Delhi Metro is a rapid transit system serving New Delhi, Delhi, Gurgaon, Faridabad, Noida, and Ghaziabad in the National Capital Region of India. Delhi Metro is the world's 12th largest metro system in terms of length. Delhi Metro was India's first modern public transportation system, which had revolutionised travel by providing a fast, reliable, safe, and comfortable means of transport. The network consists of six lines with a total length of 189.63 kilometres (117.83 miles) with 142 stations, of which 35 are underground, five are at-grade, and the remainder are elevated. All stations have escalators, elevators, and tactile tiles to guide the visually impaired from station entrances to trains. It has a combination of elevated, at-grade, and underground lines, and uses both broad gauge and standard gauge rolling stock. Four types of rolling stock are used: Mitsubishi-ROTEM Broad gauge, Bombardier MOVIA, Mitsubishi-ROTEM Standard gauge, and CAF Beasain Standard gauge.
Delhi Metro is being built and operated by the Delhi Metro Rail Corporation Limited (DMRC), a state-owned company with equal equity participation from Government of India and Government of National Capital Territory of Delhi. However, the organisation is under administrative control of Ministry of Urban Development, Government of India. Besides construction and operation of Delhi metro, DMRC is also involved in the planning and implementation of metro rail, monorail and high-speed rail projects in India and providing consultancy services to other metro projects in the country as well as abroad. The Delhi Metro project was spearheaded by Padma Vibhushan E. Sreedharan, the Managing Director of DMRC and popularly known as the "Metro Man" of India. He famously resigned from DMRC, taking moral responsibility for a metro bridge collapse which took five lives. Sreedharan was awarded with the prestigious Legion of Honour by the French Government for his contribution to Delhi Metro.
New Delhi has a population of 249,998. Hindi and Punjabi are the most widely spoken languages in New Delhi and the lingua franca of the city. English is primarily used as the formal language by business and government institutes. New Delhi has a literacy rate of 89.38% according to 2011 census, which is highest in Delhi.
Hinduism is the religion of 79.8% of New Delhi's population. There are also communities of Muslims (12.9%), Sikhs (5.4%), Jains (1.1%) and Christians (0.9%) in Delhi. Other religious groups (2.5%) include Parsis, Buddhists and Jews.
New Delhi is a cosmopolitan city due to the multi-ethnic and multi-cultural presence of the vast Indian bureaucracy and political system. The city's capital status has amplified the importance of national events and holidays. National events such as Republic Day, Independence Day and Gandhi Jayanti (Gandhi's birthday) are celebrated with great enthusiasm in New Delhi and the rest of India. On India's Independence Day (15 August) the Prime Minister of India addresses the nation from the Red Fort. Most Delhiites celebrate the day by flying kites, which are considered a symbol of freedom. The Republic Day Parade is a large cultural and military parade showcasing India's cultural diversity and military might.
Religious festivals include Diwali (the festival of light), Maha Shivaratri, Teej, Guru Nanak Jayanti, Baisakhi, Durga Puja, Holi, Lohri, Eid ul-Fitr, Eid ul-Adha, Christmas, Chhath Puja and Mahavir Jayanti. The Qutub Festival is a cultural event during which performances of musicians and dancers from all over India are showcased at night, with the Qutub Minar as the chosen backdrop of the event. Other events such as Kite Flying Festival, International Mango Festival and Vasant Panchami (the Spring Festival) are held every year in Delhi.
In 2007, the Japanese Buddhist organisation Nipponzan Myohoji decided to build a Peace Pagoda in the city containing Buddha relics. It was inaugurated by the current Dalai Lama.
The New Delhi town plan, like its architecture, was chosen with one single chief consideration: to be a symbol of British power and supremacy. All other decisions were subordinate to this, and it was this framework that dictated the choice and application of symbology and influences from both Hindu and Islamic architecture.
It took about 20 years to build the city from 1911. Many elements of New Delhi architecture borrow from indigenous sources; however, they fit into a British Classical/Palladian tradition. The fact that there were any indigenous features in the design were due to the persistence and urging of both the Viceroy Lord Hardinge and historians like E.B. Havell.
New Delhi is home to several historic sites and museums. The National Museum which began with an exhibition of Indian art and artefacts at the Royal Academy in London in the winter of 1947–48 was later at the end was shown at the Rashtrapati Bhawan in 1949. Later it was to form a permanent National Museum. On 15 August 1949, the National Museum was formally inaugurated and currently has 200,000 works of art, both of Indian and foreign origin, covering over 5,000 years.
The India Gate built in 1931 was inspired by the Arc de Triomphe in Paris. It is the national monument of India commemorating the 90,000 soldiers of the Indian Army who lost their lives while fighting for the British Raj in World War I and the Third Anglo-Afghan War.
The Rajpath which was built similar to the Champs-Élysées in Paris is the ceremonial boulevard for the Republic of India located in New Delhi. The annual Republic Day parade takes place here on 26 January.
Gandhi Smriti in New Delhi is the location where Mahatma Gandhi spent the last 144 days of his life and was assassinated on 30 January 1948. Rajghat is the place where Mahatma Gandhi was cremated on 31 January 1948 after his assassination and his ashes were buried and make it a final resting place beside the sanctity of the Yamuna River. The Raj Ghat in the shape of large square platform with black marble was designed by architect Vanu Bhuta.
Jantar Mantar located in Connaught Place was built by Maharaja Jai Singh II of Jaipur. It consists of 13 architectural astronomy instruments. The primary purpose of the observatory was to compile astronomical tables, and to predict the times and movements of the sun, moon and planets.
New Delhi is home to Indira Gandhi Memorial Museum, National Gallery of Modern Art, National Museum of Natural History, National Rail Museum, National Handicrafts and Handlooms Museum, National Philatelic Museum, Nehru Planetarium, Shankar's International Dolls Museum. and Supreme Court of India Museum.
New Delhi is particularly renowned for its beautifully landscaped gardens that can look quite stunning in spring. The largest of these include Buddha Jayanti Park and the historic Lodi Gardens. In addition, there are the gardens in the Presidential Estate, the gardens along the Rajpath and India Gate, the gardens along Shanti Path, the Rose Garden, Nehru Park and the Railway Garden in Chanakya Puri. Also of note is the garden adjacent to the Jangpura Metro Station near the Defence Colony Flyover, as are the roundabout and neighbourhood gardens throughout the city.
The city hosted the 2010 Commonwealth Games and annually hosts Delhi Half Marathon foot-race. The city has previously hosted the 1951 Asian Games and the 1982 Asian Games. New Delhi was interested  in bidding for the 2019 Asian Games but was turned down by the government on 2 August 2010 amid allegations of corruption in 2010 Commonwealth Games .
Major sporting venues in New Delhi include the Jawaharlal Nehru Stadium, Ambedkar Stadium, Indira Gandhi Indoor Stadium, Feroz Shah Kotla Ground, R.K. Khanna Tennis Complex, Dhyan Chand National Stadium and Siri Fort Sports Complex.
New Delhi is the largest commercial city in northern India. It has an estimated net State Domestic Product (FY 2010) of ₹1595 billion (US$23 billion) in nominal terms and ~₹6800 billion (US$100 billion) in PPP terms. As of 2013, the per capita income of Delhi was Rs. 230000, second highest in India after Goa. GSDP in Delhi at the current prices for 2012-13 is estimated at Rs 3.88 trillion (short scale) against Rs 3.11 trillion (short scale) in 2011-12.
Connaught Place, one of North India's largest commercial and financial centres, is located in the northern part of New Delhi. Adjoining areas such as Barakhamba Road, ITO are also major commercial centres. Government and quasi government sector was the primary employer in New Delhi. The city's service sector has expanded due in part to the large skilled English-speaking workforce that has attracted many multinational companies. Key service industries include information technology, telecommunications, hotels, banking, media and tourism.
The 2011 World Wealth Report ranks economic activity in New Delhi at 39, but overall the capital is ranked at 37, above cities like Jakarta and Johannesburg. New Delhi with Beijing shares the top position as the most targeted emerging markets retail destination among Asia-Pacific markets.
The Government of National Capital Territory of Delhi does not release any economic figures specifically for New Delhi but publishes an official economic report on the whole of Delhi annually. According to the Economic Survey of Delhi, the metropolis has a net State Domestic Product (SDP) of Rs. 83,085 crores (for the year 2004–05) and a per capita income of Rs. 53,976($1,200). In the year 2008–09 New Delhi had a Per Capita Income of Rs.1,16,886 ($2,595).It grew by 16.2% to reach Rs.1,35,814 ($3,018) in 2009–10 fiscal. New Delhi's Per Capita GDP (at PPP) was at $6,860 during 2009–10 fiscal, making it one of the richest cities in India. The tertiary sector contributes 78.4% of Delhi's gross SDP followed by secondary and primary sectors with 20.2% and 1.4% contribution respectively.
The gross state domestic product (GSDP) of Delhi at current prices for the year 2011-12 has been estimated at Rs 3.13 lakh crore, which is an increase of 18.7 per cent over the previous fiscal.
The city is home to numerous international organisations. The Asian and Pacific Centre for Transfer of Technology of the UNESCAP servicing the Asia-Pacific region is headquartered in New Delhi. New Delhi is home to most UN regional offices in India namely the UNDP, UNODC, UNESCO, UNICEF, WFP, UNV, UNCTAD, FAO, UNFPA, WHO, World Bank, ILO, IMF, UNIFEM, IFC and UNAIDS.
According to the apocryphal Gospel of James, Mary was the daughter of Saint Joachim and Saint Anne. Before Mary's conception, Anne had been barren and was far advanced in years. Mary was given to service as a consecrated virgin in the Temple in Jerusalem when she was three years old, much like Hannah took Samuel to the Tabernacle as recorded in the Old Testament. Some apocryphal accounts state that at the time of her betrothal to Joseph, Mary was 12–14 years old, and he was thirty years old, but such accounts are unreliable.
The Gospel of Luke begins its account of Mary's life with the Annunciation, when the angel Gabriel appeared to her and announced her divine selection to be the mother of Jesus. According to gospel accounts, Mary was present at the Crucifixion of Jesus and is depicted as a member of the early Christian community in Jerusalem. According to Apocryphal writings, at some time soon after her death, her incorrupt body was assumed directly into Heaven, to be reunited with her soul, and the apostles thereupon found the tomb empty; this is known in Christian teaching as the Assumption.
The doctrines of the Assumption or Dormition of Mary relate to her death and bodily assumption to Heaven. The Roman Catholic Church has dogmaically defined the doctrine of the Assumption, which was done in 1950 by Pope Pius XII in Munificentissimus Deus. Whether the Virgin Mary died or not is not defined dogmatically, however, although a reference to the death of Mary are made in Munificentissimus Deus. In the Eastern Orthodox Church, the Assumption of the Virgin Mary is believed, and celebrated with her Dormition, where they believe she died.
After Mary continued in the "blood of her purifying" another 33 days for a total of 40 days, she brought her burnt offering and sin offering to the Temple in Jerusalem,[Luke 2:22] so the priest could make atonement for her sins, being cleansed from her blood.[Leviticus 12:1-8] They also presented Jesus –  "As it is written in the law of the Lord, Every male that openeth the womb shall be called holy to the Lord" (Luke 2:23other verses). After the prophecies of Simeon and the prophetess Anna in Luke 2:25-38 concluded, Joseph and Mary took Jesus and "returned into Galilee, to their own city Nazareth".[Luke 2:39]
According to the writer of Luke, Mary was a relative of Elizabeth, wife of the priest Zechariah of the priestly division of Abijah, who was herself part of the lineage of Aaron and so of the tribe of Levi.[Luke 1:5;1:36] Some of those who consider that the relationship with Elizabeth was on the maternal side, consider that Mary, like Joseph, to whom she was betrothed, was of the House of David and so of the Tribe of Judah, and that the genealogy of Jesus presented in Luke 3 from Nathan, third son of David and Bathsheba, is in fact the genealogy of Mary,[need quotation to verify] while the genealogy from Solomon given in Matthew 1 is that of Joseph. (Aaron's wife Elisheba was of the tribe of Judah, so all their descendants are from both Levi and Judah.)[Num.1:7 & Ex.6:23]
Mary is also depicted as being present among the women at the crucifixion during the crucifixion standing near "the disciple whom Jesus loved" along with Mary of Clopas and Mary Magdalene,[Jn 19:25-26] to which list Matthew 27:56 adds "the mother of the sons of Zebedee", presumably the Salome mentioned in Mark 15:40. This representation is called a Stabat Mater. While not recorded in the Gospel accounts, Mary cradling the dead body of her son is a common motif in art, called a "pietà" or "pity".
The adoption of the mother of Jesus as a virtual goddess may represent a reintroduction of aspects of the worship of Isis. "When looking at images of the Egyptian goddess Isis and those of the Virgin Mary, one may initially observe iconographic similarities. These parallels have led many scholars to suggest that there is a distinct iconographic relationship between Isis and Mary. In fact, some scholars have gone even further, and have suggested, on the basis of this relationship, a direct link between the cult of Mary and that of Isis." 
In the 19th century, a house near Ephesus in Turkey was found, based on the visions of Anne Catherine Emmerich, an Augustinian nun in Germany. It has since been visited as the House of the Virgin Mary by Roman Catholic pilgrims who consider it the place where Mary lived until her assumption. The Gospel of John states that Mary went to live with the Disciple whom Jesus loved,[Jn 19:27] identified as John the Evangelist.[citation needed] Irenaeus and Eusebius of Caesarea wrote in their histories that John later went to Ephesus, which may provide the basis for the early belief that Mary also lived in Ephesus with John.
Devotions to artistic depictions of Mary vary among Christian traditions. There is a long tradition of Roman Catholic Marian art and no image permeates Catholic art as does the image of Madonna and Child. The icon of the Virgin Theotokos with Christ is without doubt the most venerated icon in the Orthodox Church. Both Roman Catholic and Orthodox Christians venerate images and icons of Mary, given that the Second Council of Nicaea in 787 permitted their veneration with the understanding that those who venerate the image are venerating the reality of the person it represents, and the 842 Synod of Constantinople confirming the same. According to Orthodox piety and traditional practice, however, believers ought to pray before and venerate only flat, two-dimensional icons, and not three-dimensional statues.
Ephesus is a cultic centre of Mary, the site of the first Church dedicated to her and the rumoured place of her death. Ephesus was previously a centre for worship of Artemis a virgin goddess. The Temple of Artemis at Ephesus being regarded as one of the Seven Wonders of the Ancient World The cult of Mary was furthered by Queen Theodora in the 6th Century. According to William E. Phipps, in the book Survivals of Roman Religion "Gordon Laing argues convincingly that the worship of Artemis as both virgin and mother at the grand Ephesian temple contributed to the veneration of Mary."
Some titles have a Biblical basis, for instance the title Queen Mother has been given to Mary since she was the mother of Jesus, who was sometimes referred to as the "King of Kings" due to his lineage of King David. The biblical basis for the term Queen can be seen in the Gospel of Luke 1:32 and the Book of Isaiah 9:6, and Queen Mother from 1 Kings 2:19-20 and Jeremiah 13:18-19. Other titles have arisen from reported miracles, special appeals or occasions for calling on Mary, e.g., Our Lady of Good Counsel, Our Lady of Navigators or Our Lady of Ransom who protects captives.
Despite Martin Luther's harsh polemics against his Roman Catholic opponents over issues concerning Mary and the saints, theologians appear to agree that Luther adhered to the Marian decrees of the ecumenical councils and dogmas of the church. He held fast to the belief that Mary was a perpetual virgin and the Theotokos or Mother of God. Special attention is given to the assertion that Luther, some three-hundred years before the dogmatization of the Immaculate Conception by Pope Pius IX in 1854, was a firm adherent of that view. Others maintain that Luther in later years changed his position on the Immaculate Conception, which, at that time was undefined in the Church, maintaining however the sinlessness of Mary throughout her life. For Luther, early in his life, the Assumption of Mary was an understood fact, although he later stated that the Bible did not say anything about it and stopped celebrating its feast. Important to him was the belief that Mary and the saints do live on after death. "Throughout his career as a priest-professor-reformer, Luther preached, taught, and argued about the veneration of Mary with a verbosity that ranged from childlike piety to sophisticated polemics. His views are intimately linked to his Christocentric theology and its consequences for liturgy and piety." Luther, while revering Mary, came to criticize the "Papists" for blurring the line, between high admiration of the grace of God wherever it is seen in a human being, and religious service given to another creature. He considered the Roman Catholic practice of celebrating saints' days and making intercessory requests addressed especially to Mary and other departed saints to be idolatry. His final thoughts on Marian devotion and veneration are preserved in a sermon preached at Wittenberg only a month before his death:
Differences in feasts may also originate from doctrinal issues—the Feast of the Assumption is such an example. Given that there is no agreement among all Christians on the circumstances of the death, Dormition or Assumption of Mary, the feast of assumption is celebrated among some denominations and not others.  While the Catholic Church celebrates the Feast of the Assumption on August 15, some Eastern Catholics celebrate it as Dormition of the Theotokos, and may do so on August 28, if they follow the Julian calendar. The Eastern Orthodox also celebrate it as the Dormition of the Theotokos, one of their 12 Great Feasts. Protestants do not celebrate this, or any other Marian feasts.
In paintings, Mary is traditionally portrayed in blue. This tradition can trace its origin to the Byzantine Empire, from c.500 AD, where blue was "the colour of an empress". A more practical explanation for the use of this colour is that in Medieval and Renaissance Europe, the blue pigment was derived from the rock lapis lazuli, a stone imported from Afghanistan of greater value than gold. Beyond a painter's retainer, patrons were expected to purchase any gold or lapis lazuli to be used in the painting. Hence, it was an expression of devotion and glorification to swathe the Virgin in gowns of blue.
Nontrinitarians, such as Unitarians, Christadelphians and Jehovah's Witnesses also acknowledge Mary as the biological mother of Jesus Christ, but do not recognise Marian titles such as "Mother of God" as these groups generally reject Christ's divinity. Since Nontrinitarian churches are typically also mortalist, the issue of praying to Mary, whom they would consider "asleep", awaiting resurrection, does not arise. Emanuel Swedenborg says God as he is in himself could not directly approach evil spirits to redeem those spirits without destroying them (Exodus 33:20, John 1:18), so God impregnated Mary, who gave Jesus Christ access to the evil heredity of the human race, which he could approach, redeem and save.
The Qur'an relates detailed narrative accounts of Maryam (Mary) in two places, Qur'an 3:35–47 and 19:16–34. These state beliefs in both the Immaculate Conception of Mary and the Virgin birth of Jesus. The account given in Sura 19 is nearly identical with that in the Gospel according to Luke, and both of these (Luke, Sura 19) begin with an account of the visitation of an angel upon Zakariya (Zecharias) and Good News of the birth of Yahya (John), followed by the account of the annunciation. It mentions how Mary was informed by an angel that she would become the mother of Jesus through the actions of God alone.
The Perpetual Virginity of Mary asserts Mary's real and perpetual virginity even in the act of giving birth to the Son of God made Man. The term Ever-Virgin (Greek ἀειπάρθενος) is applied in this case, stating that Mary remained a virgin for the remainder of her life, making Jesus her biological and only son, whose conception and birth are held to be miraculous. While the Orthodox Churches hold the position articulated in the Protoevangelium of James that Jesus' brothers and sisters are older children of Joseph the Betrothed, step-siblings from an earlier marriage that left him widowed, Roman Catholic teaching follows the Latin father Jerome in considering them Jesus' cousins.
Orthodox Christianity includes a large number of traditions regarding the Ever Virgin Mary, the Theotokos. The Orthodox believe that she was and remained a virgin before and after Christ's birth. The Theotokia (i.e., hymns to the Theotokos) are an essential part of the Divine Services in the Eastern Church and their positioning within the liturgical sequence effectively places the Theotokos in the most prominent place after Christ. Within the Orthodox tradition, the order of the saints begins with: The Theotokos, Angels, Prophets, Apostles, Fathers, Martyrs, etc. giving the Virgin Mary precedence over the angels. She is also proclaimed as the "Lady of the Angels".
The multiple churches that form the Anglican Communion and the Continuing Anglican movement have different views on Marian doctrines and venerative practices given that there is no single church with universal authority within the Communion and that the mother church (the Church of England) understands itself to be both "catholic" and "Reformed". Thus unlike the Protestant churches at large, the Anglican Communion (which includes the Episcopal Church in the United States) includes segments which still retain some veneration of Mary.
Although Calvin and Huldrych Zwingli honored Mary as the Mother of God in the 16th century, they did so less than Martin Luther. Thus the idea of respect and high honor for Mary was not rejected by the first Protestants; but, they came to criticize the Roman Catholics for venerating Mary. Following the Council of Trent in the 16th century, as Marian veneration became associated with Catholics, Protestant interest in Mary decreased. During the Age of the Enlightenment any residual interest in Mary within Protestant churches almost disappeared, although Anglicans and Lutherans continued to honor her.
 In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary "continued a pure and unspotted virgin", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.
She is the only woman directly named in the Qur'an; declared (uniquely along with Jesus) to be a Sign of God to humanity; as one who "guarded her chastity"; an obedient one; chosen of her mother and dedicated to Allah whilst still in the womb; uniquely (amongst women) Accepted into service by God; cared for by (one of the prophets as per Islam) Zakariya (Zacharias); that in her childhood she resided in the Temple and uniquely had access to Al-Mihrab (understood to be the Holy of Holies), and was provided with heavenly "provisions" by God.
From the early stages of Christianity, belief in the virginity of Mary and the virgin conception of Jesus, as stated in the gospels, holy and supernatural, was used by detractors, both political and religious, as a topic for discussions, debates and writings, specifically aimed to challenge the divinity of Jesus and thus Christians and Christianity alike. In the 2nd century, as part of the earliest anti-Christian polemics, Celsus suggested that Jesus was the illegitimate son of a Roman soldier named Panthera. The views of Celsus drew responses from Origen, the Church Father in Alexandria, Egypt, who considered it a fabricated story. How far Celsus sourced his view from Jewish sources remains a subject of discussion.
Mary had been venerated since Early Christianity, and is considered by millions to be the most meritorious saint of the religion. The Eastern and Oriental Orthodox, Roman Catholic, Anglican, and Lutheran Churches believe that Mary, as Mother of Jesus, is the Mother of God and the Theotokos, literally "Giver of birth to God". There is significant diversity in the Marian beliefs and devotional practices of major Christian traditions. The Roman Catholic Church holds distinctive Marian dogmas; namely her status as the mother of God; her Immaculate Conception; her perpetual virginity; and her Assumption into heaven. Many Protestants minimize Mary's role within Christianity, based on the argued brevity of biblical references. Mary (Maryam) also has a revered position in Islam, where a whole chapter of the Qur'an is devoted to her, also describing the birth of Jesus.
Mary resided in "her own house"[Lk.1:56] in Nazareth in Galilee, possibly with her parents, and during her betrothal — the first stage of a Jewish marriage — the angel Gabriel announced to her that she was to be the mother of the promised Messiah by conceiving him through the Holy Spirit, and she responded, "I am the handmaid of the Lord. Let it be done unto me according to your word." After a number of months, when Joseph was told of her conception in a dream by "an angel of the Lord", he planned to divorce her; but the angel told him to not hesitate to take her as his wife, which Joseph did, thereby formally completing the wedding rites.[Mt 1:18-25]
The Virgin birth of Jesus was an almost universally held belief among Christians from the 2nd until the 19th century. It is included in the two most widely used Christian creeds, which state that Jesus "was incarnate of the Holy Spirit and the Virgin Mary" (the Nicene Creed in what is now its familiar form) and the Apostles' Creed. The Gospel of Matthew describes Mary as a virgin who fulfilled the prophecy of Isaiah 7:14, mistranslating the Hebrew word alma ("young woman") in Isaiah 7:14 as "virgin", though.[citation needed] The authors of the Gospels of Matthew and Luke consider Jesus' conception not the result of intercourse and assert that Mary had "no relations with man" before Jesus' birth.[Mt 1:18] [Mt 1:25] [Lk 1:34] This alludes to the belief that Mary conceived Jesus through the action of God the Holy Spirit, and not through intercourse with Joseph or anyone else.

In the Catholic Church, Mary is accorded the title "Blessed", (from Latin beatus, blessed, via Greek μακάριος, makarios and Latin facere, make) in recognition of her assumption to Heaven and her capacity to intercede on behalf of those who pray to her. Catholic teachings make clear that Mary is not considered divine and prayers to her are not answered by her, they are answered by God. The four Catholic dogmas regarding Mary are: Mother of God, Perpetual virginity of Mary, Immaculate Conception (of Mary) and Assumption of Mary.
The views of the Church Fathers still play an important role in the shaping of Orthodox Marian perspective. However, the Orthodox views on Mary are mostly doxological, rather than academic: they are expressed in hymns, praise, liturgical poetry and the veneration of icons. One of the most loved Orthodox Akathists (i.e. standing hymns) is devoted to Mary and it is often simply called the Akathist Hymn. Five of the twelve Great Feasts in Orthodoxy are dedicated to Mary. The Sunday of Orthodoxy directly links the Virgin Mary's identity as Mother of God with icon veneration. A number of Orthodox feasts are connected with the miraculous icons of the Theotokos.
Mary's special position within God's purpose of salvation as "God-bearer" (Theotokos) is recognised in a number of ways by some Anglican Christians. All the member churches of the Anglican Communion affirm in the historic creeds that Jesus was born of the Virgin Mary, and celebrates the feast days of the Presentation of Christ in the Temple. This feast is called in older prayer books the Purification of the Blessed Virgin Mary on February 2. The Annunciation of our Lord to the Blessed Virgin on March 25 was from before the time of Bede until the 18th century New Year's Day in England. The Annunciation is called the "Annunciation of our Lady" in the 1662 Book of Common Prayer. Anglicans also celebrate in the Visitation of the Blessed Virgin on 31 May, though in some provinces the traditional date of July 2 is kept. The feast of the St. Mary the Virgin is observed on the traditional day of the Assumption, August 15. The Nativity of the Blessed Virgin is kept on September 8.
Protestants in general reject the veneration and invocation of the Saints.:1174 Protestants typically hold that Mary was the mother of Jesus, but was an ordinary woman devoted to God. Therefore, there is virtually no Marian veneration, Marian feasts, Marian pilgrimages, Marian art, Marian music or Marian spirituality in today's Protestant communities. Within these views, Roman Catholic beliefs and practices are at times rejected, e.g., theologian Karl Barth wrote that "the heresy of the Catholic Church is its Mariology".
The statement that Joseph "knew her not till she brought forth her first born son" (Matthew 1:25 DouayRheims) has been debated among scholars, with some saying that she did not remain a virgin and some saying that she was a perpetual virgin. Other scholars contend that the Greek word heos (i.e., until) denotes a state up to a point, but does not mean that the state ended after that point, and that Matthew 1:25 does not confirm or deny the virginity of Mary after the birth of Jesus. According to Biblical scholar Bart Ehrman the Hebrew word almah, meaning young woman of childbearing age, was translated into Greek as parthenos, which only means virgin, in Isaiah 7:14, which is commonly believed by Christians to be the prophecy of the Virgin Mary referred to in Matthew 1:23. While Matthew and Luke give differing versions of the virgin birth, John quotes the uninitiated Philip and the disbelieving Jews gathered at Galilee referring to Joseph as Jesus's father.
The hagiography of Mary and the Holy Family can be contrasted with other material in the Gospels. These references include an incident which can be interpreted as Jesus rejecting his family in the New Testament: "And his mother and his brothers arrived, and standing outside, they sent in a message asking for him ... And looking at those who sat in a circle around him, Jesus said, 'These are my mother and my brothers. Whoever does the will of God is my brother, and sister, and mother'."[3:31-35] Other verses suggest a conflict between Jesus and his family, including an attempt to have Jesus restrained because "he is out of his mind", and the famous quote: "A prophet is not without honor except in his own town, among his relatives and in his own home." A leading Biblical scholar commented: "there are clear signs not only that Jesus's family rejected his message during his public ministry but that he in turn spurned them publicly".
Although the Catholics and the Orthodox may honor and venerate Mary, they do not view her as divine, nor do they worship her. Roman Catholics view Mary as subordinate to Christ, but uniquely so, in that she is seen as above all other creatures. Similarly Theologian Sergei Bulgakov wrote that the Orthodox view Mary as "superior to all created beings" and "ceaselessly pray for her intercession". However, she is not considered a "substitute for the One Mediator" who is Christ. "Let Mary be in honor, but let worship be given to the Lord", he wrote. Similarly, Catholics do not worship Mary as a divine being, but rather "hyper-venerate" her. In Roman Catholic theology, the term hyperdulia is reserved for Marian veneration, latria for the worship of God, and dulia for the veneration of other saints and angels. The definition of the three level hierarchy of latria, hyperdulia and dulia goes back to the Second Council of Nicaea in 787.
Mary is referred to by the Eastern Orthodox Church, Oriental Orthodoxy, the Anglican Church, and all Eastern Catholic Churches as Theotokos, a title recognized at the Third Ecumenical Council (held at Ephesus to address the teachings of Nestorius, in 431). Theotokos (and its Latin equivalents, "Deipara" and "Dei genetrix") literally means "Godbearer". The equivalent phrase "Mater Dei" (Mother of God) is more common in Latin and so also in the other languages used in the Western Catholic Church, but this same phrase in Greek (Μήτηρ Θεοῦ), in the abbreviated form of the first and last letter of the two words (ΜΡ ΘΥ), is the indication attached to her image in Byzantine icons. The Council stated that the Church Fathers "did not hesitate to speak of the holy Virgin as the Mother of God".
Roman Catholics believe in the Immaculate Conception of Mary, as proclaimed Ex Cathedra by Pope Pius IX in 1854, namely that she was filled with grace from the very moment of her conception in her mother's womb and preserved from the stain of original sin. The Latin Rite of the Roman Catholic Church has a liturgical feast by that name, kept on December 8. Orthodox Christians reject the Immaculate Conception dogma principally because their understanding of ancestral sin (the Greek term corresponding to the Latin "original sin") differs from the Augustinian interpretation and that of the Roman Catholic Church.
The Protoevangelium of James, an extra-canonical book, has been the source of many Orthodox beliefs on Mary. The account of Mary's life presented includes her consecration as a virgin at the temple at age three. The High Priest Zachariah blessed Mary and informed her that God had magnified her name among many generations. Zachariah placed Mary on the third step of the altar, whereby God gave her grace. While in the temple, Mary was miraculously fed by an angel, until she was twelve years old. At that point an angel told Zachariah to betroth Mary to a widower in Israel, who would be indicated. This story provides the theme of many hymns for the Feast of Presentation of Mary, and icons of the feast depict the story. The Orthodox believe that Mary was instrumental in the growth of Christianity during the life of Jesus, and after his Crucifixion, and Orthodox Theologian Sergei Bulgakov wrote: "The Virgin Mary is the center, invisible, but real, of the Apostolic Church."
In the Islamic tradition, Mary and Jesus were the only children who could not be touched by Satan at the moment of their birth, for God imposed a veil between them and Satan. According to author Shabbir Akhtar, the Islamic perspective on Mary's Immaculate Conception is compatible with the Catholic doctrine of the same topic. "O People of the Book! Do not go beyond the bounds in your religion, and do not say anything of Allah but the truth. The Messiah, Jesus son of Mary, was but a Messenger of God, and a Word of His (Power) which He conveyed to Mary, and a spirit from Him. So believe in Allah (as the One, Unique God), and His Messengers (including Jesus, as Messenger); and do not say: (Allah is one of) a trinity. Give up (this assertion) â€" (it is) for your own good (to do so). Allah is but One Allah ; All-Glorified He is in that He is absolutely above having a son. To Him belongs whatever is in the heavens and whatever is on the earth. And Allah suffices as the One to be relied on, to Whom affairs should be referred." Quran 4/171
The issue of the parentage of Jesus in the Talmud affects also the view of his mother. However the Talmud does not mention Mary by name and is considerate rather than only polemic. The story about Panthera is also found in the Toledot Yeshu, the literary origins of which can not be traced with any certainty and given that it is unlikely to go before the 4th century, it is far too late to include authentic remembrances of Jesus. The Blackwell Companion to Jesus states that the Toledot Yeshu has no historical facts as such, and was perhaps created as a tool for warding off conversions to Christianity. The name Panthera may be a distortion of the term parthenos (virgin) and Raymond E. Brown considers the story of Panthera a fanciful explanation of the birth of Jesus which includes very little historical evidence. Robert Van Voorst states that given that Toledot Yeshu is a medieval document and due to its lack of a fixed form and orientation towards a popular audience, it is "most unlikely" to have reliable historical information.
In the Roman era, copper was principally mined on Cyprus, the origin of the name of the metal from aes сyprium (metal of Cyprus), later corrupted to сuprum, from which the words copper (English), cuivre (French), Koper (Dutch) and Kupfer (German) are all derived. Its compounds are commonly encountered as copper(II) salts, which often impart blue or green colors to minerals such as azurite, malachite and turquoise and have been widely used historically as pigments. Architectural structures built with copper corrode to give green verdigris (or patina). Decorative art prominently features copper, both by itself and in the form of pigments.
Copper occurs naturally as native copper and was known to some of the oldest civilizations on record. It has a history of use that is at least 10,000 years old, and estimates of its discovery place it at 9000 BC in the Middle East; a copper pendant was found in northern Iraq that dates to 8700 BC. There is evidence that gold and meteoric iron (but not iron smelting) were the only metals used by humans before copper. The history of copper metallurgy is thought to have followed the following sequence: 1) cold working of native copper, 2) annealing, 3) smelting, and 4) the lost wax method. In southeastern Anatolia, all four of these metallurgical techniques appears more or less simultaneously at the beginning of the Neolithic c. 7500 BC. However, just as agriculture was independently invented in several parts of the world, copper smelting was invented locally in several different places. It was probably discovered independently in China before 2800 BC, in Central America perhaps around 600 AD, and in West Africa about the 9th or 10th century AD. Investment casting was invented in 4500–4000 BC in Southeast Asia and carbon dating has established mining at Alderley Edge in Cheshire, UK at 2280 to 1890 BC. Ötzi the Iceman, a male dated from 3300–3200 BC, was found with an axe with a copper head 99.7% pure; high levels of arsenic in his hair suggest his involvement in copper smelting. Experience with copper has assisted the development of other metals; in particular, copper smelting led to the discovery of iron smelting. Production in the Old Copper Complex in Michigan and Wisconsin is dated between 6000 and 3000 BC. Natural bronze, a type of copper made from ores rich in silicon, arsenic, and (rarely) tin, came into general use in the Balkans around 5500 BC.[citation needed]
The gates of the Temple of Jerusalem used Corinthian bronze made by depletion gilding. It was most prevalent in Alexandria, where alchemy is thought to have begun. In ancient India, copper was used in the holistic medical science Ayurveda for surgical instruments and other medical equipment. Ancient Egyptians (~2400 BC) used copper for sterilizing wounds and drinking water, and later on for headaches, burns, and itching. The Baghdad Battery, with copper cylinders soldered to lead, dates back to 248 BC to AD 226 and resembles a galvanic cell, leading people to believe this was the first battery; the claim has not been verified.
Despite competition from other materials, copper remains the preferred electrical conductor in nearly all categories of electrical wiring with the major exception being overhead electric power transmission where aluminium is often preferred. Copper wire is used in power generation, power transmission, power distribution, telecommunications, electronics circuitry, and countless types of electrical equipment. Electrical wiring is the most important market for the copper industry. This includes building wire, communications cable, power distribution cable, appliance wire, automotive wire and cable, and magnet wire. Roughly half of all copper mined is used to manufacture electrical wire and cable conductors. Many electrical devices rely on copper wiring because of its multitude of inherent beneficial properties, such as its high electrical conductivity, tensile strength, ductility, creep (deformation) resistance, corrosion resistance, low thermal expansion, high thermal conductivity, solderability, and ease of installation.
Copper is biostatic, meaning bacteria will not grow on it. For this reason it has long been used to line parts of ships to protect against barnacles and mussels. It was originally used pure, but has since been superseded by Muntz metal. Similarly, as discussed in copper alloys in aquaculture, copper alloys have become important netting materials in the aquaculture industry because they are antimicrobial and prevent biofouling, even in extreme conditions and have strong structural and corrosion-resistant properties in marine environments.
Copper, silver and gold are in group 11 of the periodic table, and they share certain attributes: they have one s-orbital electron on top of a filled d-electron shell and are characterized by high ductility and electrical conductivity. The filled d-shells in these elements do not contribute much to the interatomic interactions, which are dominated by the s-electrons through metallic bonds. Unlike metals with incomplete d-shells, metallic bonds in copper are lacking a covalent character and are relatively weak. This explains the low hardness and high ductility of single crystals of copper. At the macroscopic scale, introduction of extended defects to the crystal lattice, such as grain boundaries, hinders flow of the material under applied stress, thereby increasing its hardness. For this reason, copper is usually supplied in a fine-grained polycrystalline form, which has greater strength than monocrystalline forms.
Copper is synthesized in massive stars and is present in the Earth's crust at a concentration of about 50 parts per million (ppm), where it occurs as native copper or in minerals such as the copper sulfides chalcopyrite and chalcocite, the copper carbonates azurite and malachite, and the copper(I) oxide mineral cuprite. The largest mass of elemental copper discovered weighed 420 tonnes and was found in 1857 on the Keweenaw Peninsula in Michigan, US. Native copper is a polycrystal, with the largest described single crystal measuring 4.4×3.2×3.2 cm.
In Greece, copper was known by the name chalkos (χαλκός). It was an important resource for the Romans, Greeks and other ancient peoples. In Roman times, it was known as aes Cyprium, aes being the generic Latin term for copper alloys and Cyprium from Cyprus, where much copper was mined. The phrase was simplified to cuprum, hence the English copper. Aphrodite and Venus represented copper in mythology and alchemy, because of its lustrous beauty, its ancient use in producing mirrors, and its association with Cyprus, which was sacred to the goddess. The seven heavenly bodies known to the ancients were associated with the seven metals known in antiquity, and Venus was assigned to copper.
Compounds that contain a carbon-copper bond are known as organocopper compounds. They are very reactive towards oxygen to form copper(I) oxide and have many uses in chemistry. They are synthesized by treating copper(I) compounds with Grignard reagents, terminal alkynes or organolithium reagents; in particular, the last reaction described produces a Gilman reagent. These can undergo substitution with alkyl halides to form coupling products; as such, they are important in the field of organic synthesis. Copper(I) acetylide is highly shock-sensitive but is an intermediate in reactions such as the Cadiot-Chodkiewicz coupling and the Sonogashira coupling. Conjugate addition to enones and carbocupration of alkynes can also be achieved with organocopper compounds. Copper(I) forms a variety of weak complexes with alkenes and carbon monoxide, especially in the presence of amine ligands.
The uses of copper in art were not limited to currency: it was used by Renaissance sculptors, in photographic technology known as the daguerreotype, and the Statue of Liberty. Copper plating and copper sheathing for ships' hulls was widespread; the ships of Christopher Columbus were among the earliest to have this feature. The Norddeutsche Affinerie in Hamburg was the first modern electroplating plant starting its production in 1876. The German scientist Gottfried Osann invented powder metallurgy in 1830 while determining the metal's atomic mass; around then it was discovered that the amount and type of alloying element (e.g., tin) to copper would affect bell tones. Flash smelting was developed by Outokumpu in Finland and first applied at Harjavalta in 1949; the energy-efficient process accounts for 50% of the world's primary copper production.
Copper's greater conductivity versus other metals enhances the electrical energy efficiency of motors. This is important because motors and motor-driven systems account for 43%-46% of all global electricity consumption and 69% of all electricity used by industry. Increasing the mass and cross section of copper in a coil increases the electrical energy efficiency of the motor. Copper motor rotors, a new technology designed for motor applications where energy savings are prime design objectives, are enabling general-purpose induction motors to meet and exceed National Electrical Manufacturers Association (NEMA) premium efficiency standards.
Chromobacterium violaceum and Pseudomonas fluorescens can both mobilize solid copper, as a cyanide compound. The ericoid mycorrhizal fungi associated with Calluna, Erica and Vaccinium can grow in copper metalliferous soils. The ectomycorrhizal fungus Suillus luteus protects young pine trees from copper toxicity. A sample of the fungus Aspergillus niger was found growing from gold mining solution; and was found to contain cyano metal complexes; such as gold, silver, copper iron and zinc. The fungus also plays a role in the solubilization of heavy metal sulfides.
Copper-alloy touch surfaces have natural intrinsic properties to destroy a wide range of microorganisms (e.g., E. coli O157:H7, methicillin-resistant Staphylococcus aureus (MRSA), Staphylococcus, Clostridium difficile, influenza A virus, adenovirus, and fungi). Some 355 copper alloys were proven to kill more than 99.9% of disease-causing bacteria within just two hours when cleaned regularly. The United States Environmental Protection Agency (EPA) has approved the registrations of these copper alloys as "antimicrobial materials with public health benefits," which allows manufacturers to legally make claims as to the positive public health benefits of products made with registered antimicrobial copper alloys. In addition, the EPA has approved a long list of antimicrobial copper products made from these alloys, such as bedrails, handrails, over-bed tables, sinks, faucets, door knobs, toilet hardware, computer keyboards, health club equipment, shopping cart handles, etc. (for a comprehensive list of products, see: Antimicrobial copper-alloy touch surfaces#Approved products). Copper doorknobs are used by hospitals to reduce the transfer of disease, and Legionnaires' disease is suppressed by copper tubing in plumbing systems. Antimicrobial copper alloy products are now being installed in healthcare facilities in the U.K., Ireland, Japan, Korea, France, Denmark, and Brazil[citation needed] and in the subway transit system in Santiago, Chile, where copper-zinc alloy handrails will be installed in some 30 stations between 2011–2014.
Copper compounds in liquid form are used as a wood preservative, particularly in treating original portion of structures during restoration of damage due to dry rot. Together with zinc, copper wires may be placed over non-conductive roofing materials to discourage the growth of moss.[citation needed] Textile fibers use copper to create antimicrobial protective fabrics, as do ceramic glazes, stained glass and musical instruments. Electroplating commonly uses copper as a base for other metals such as nickel.
Copper has been in use at least 10,000 years, but more than 95% of all copper ever mined and smelted has been extracted since 1900, and more than half was extracted in only the last 24 years. As with many natural resources, the total amount of copper on Earth is vast (around 1014 tons just in the top kilometer of Earth's crust, or about 5 million years' worth at the current rate of extraction). However, only a tiny fraction of these reserves is economically viable, given present-day prices and technologies. Various estimates of existing copper reserves available for mining vary from 25 years to 60 years, depending on core assumptions such as the growth rate. Recycling is a major source of copper in the modern world. Because of these and other factors, the future of copper production and supply is the subject of much debate, including the concept of peak copper, analogous to peak oil.
The cultural role of copper has been important, particularly in currency. Romans in the 6th through 3rd centuries BC used copper lumps as money. At first, the copper itself was valued, but gradually the shape and look of the copper became more important. Julius Caesar had his own coins made from brass, while Octavianus Augustus Caesar's coins were made from Cu-Pb-Sn alloys. With an estimated annual output of around 15,000 t, Roman copper mining and smelting activities reached a scale unsurpassed until the time of the Industrial Revolution; the provinces most intensely mined were those of Hispania, Cyprus and in Central Europe.
The major applications of copper are in electrical wires (60%), roofing and plumbing (20%) and industrial machinery (15%). Copper is mostly used as a pure metal, but when a higher hardness is required it is combined with other elements to make an alloy (5% of total use) such as brass and bronze. A small part of copper supply is used in production of compounds for nutritional supplements and fungicides in agriculture. Machining of copper is possible, although it is usually necessary to use an alloy for intricate parts to get good machinability characteristics.
The softness of copper partly explains its high electrical conductivity (59.6×106 S/m) and thus also high thermal conductivity, which are the second highest (to silver) among pure metals at room temperature. This is because the resistivity to electron transport in metals at room temperature mostly originates from scattering of electrons on thermal vibrations of the lattice, which are relatively weak for a soft metal. The maximum permissible current density of copper in open air is approximately 3.1×106 A/m2 of cross-sectional area, above which it begins to heat excessively. As with other metals, if copper is placed against another metal, galvanic corrosion will occur.
Most copper is mined or extracted as copper sulfides from large open pit mines in porphyry copper deposits that contain 0.4 to 1.0% copper. Examples include Chuquicamata in Chile, Bingham Canyon Mine in Utah, United States and El Chino Mine in New Mexico, United States. According to the British Geological Survey, in 2005, Chile was the top mine producer of copper with at least one-third world share followed by the United States, Indonesia and Peru. Copper can also be recovered through the in-situ leach process. Several sites in the state of Arizona are considered prime candidates for this method. The amount of copper in use is increasing and the quantity available is barely sufficient to allow all countries to reach developed world levels of usage.
Like aluminium, copper is 100% recyclable without any loss of quality, regardless of whether it is in a raw state or contained in a manufactured product. In volume, copper is the third most recycled metal after iron and aluminium. It is estimated that 80% of the copper ever mined is still in use today. According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of copper in use in society is 35–55 kg. Much of this is in more-developed countries (140–300 kg per capita) rather than less-developed countries (30–40 kg per capita).
The metal's distinctive natural green patina has long been coveted by architects and designers. The final patina is a particularly durable layer that is highly resistant to atmospheric corrosion, thereby protecting the underlying metal against further weathering. It can be a mixture of carbonate and sulfate compounds in various amounts, depending upon environmental conditions such as sulfur-containing acid rain. Architectural copper and its alloys can also be 'finished' to embark a particular look, feel, and/or color. Finishes include mechanical surface treatments, chemical coloring, and coatings.
Gram quantities of various copper salts have been taken in suicide attempts and produced acute copper toxicity in humans, possibly due to redox cycling and the generation of reactive oxygen species that damage DNA. Corresponding amounts of copper salts (30 mg/kg) are toxic in animals. A minimum dietary value for healthy growth in rabbits has been reported to be at least 3 ppm in the diet. However, higher concentrations of copper (100 ppm, 200 ppm, or 500 ppm) in the diet of rabbits may favorably influence feed conversion efficiency, growth rates, and carcass dressing percentages.
Britain's first use of brass occurred around the 3rd–2nd century BC. In North America, copper mining began with marginal workings by Native Americans. Native copper is known to have been extracted from sites on Isle Royale with primitive stone tools between 800 and 1600. Copper metallurgy was flourishing in South America, particularly in Peru around 1000 AD; it proceeded at a much slower rate on other continents. Copper burial ornamentals from the 15th century have been uncovered, but the metal's commercial production did not start until the early 20th century.
There are 29 isotopes of copper. 63Cu and 65Cu are stable, with 63Cu comprising approximately 69% of naturally occurring copper; they both have a spin of 3⁄2. The other isotopes are radioactive, with the most stable being 67Cu with a half-life of 61.83 hours. Seven metastable isotopes have been characterized, with 68mCu the longest-lived with a half-life of 3.8 minutes. Isotopes with a mass number above 64 decay by β−, whereas those with a mass number below 64 decay by β+. 64Cu, which has a half-life of 12.7 hours, decays both ways.
The alloy of copper and nickel, called cupronickel, is used in low-denomination coins, often for the outer cladding. The US 5-cent coin called a nickel consists of 75% copper and 25% nickel and has a homogeneous composition. The alloy consisting of 90% copper and 10% nickel is remarkable for its resistance to corrosion and is used in various parts that are exposed to seawater. Alloys of copper with aluminium (about 7%) have a pleasant golden color and are used in decorations. Some lead-free solders consist of tin alloyed with a small proportion of copper and other metals.
Polyols, compounds containing more than one alcohol functional group, generally interact with cupric salts. For example, copper salts are used to test for reducing sugars. Specifically, using Benedict's reagent and Fehling's solution the presence of the sugar is signaled by a color change from blue Cu(II) to reddish copper(I) oxide. Schweizer's reagent and related complexes with ethylenediamine and other amines dissolve cellulose. Amino acids form very stable chelate complexes with copper(II). Many wet-chemical tests for copper ions exist, one involving potassium ferrocyanide, which gives a brown precipitate with copper(II) salts.
Alloying copper with tin to make bronze was first practiced about 4000 years after the discovery of copper smelting, and about 2000 years after "natural bronze" had come into general use[citation needed]. Bronze artifacts from the Vinča culture date to 4500 BC. Sumerian and Egyptian artifacts of copper and bronze alloys date to 3000 BC. The Bronze Age began in Southeastern Europe around 3700–3300 BC, in Northwestern Europe about 2500 BC. It ended with the beginning of the Iron Age, 2000–1000 BC in the Near East, 600 BC in Northern Europe. The transition between the Neolithic period and the Bronze Age was formerly termed the Chalcolithic period (copper-stone), with copper tools being used with stone tools. This term has gradually fallen out of favor because in some parts of the world the Chalcolithic and Neolithic are coterminous at both ends. Brass, an alloy of copper and zinc, is of much more recent origin. It was known to the Greeks, but became a significant supplement to bronze during the Roman Empire.
Copper is an essential trace element in plants and animals, but not some microorganisms. The human body contains copper at a level of about 1.4 to 2.1 mg per kg of body mass. Stated differently, the RDA for copper in normal healthy adults is quoted as 0.97 mg/day and as 3.0 mg/day. Copper is absorbed in the gut, then transported to the liver bound to albumin. After processing in the liver, copper is distributed to other tissues in a second phase. Copper transport here involves the protein ceruloplasmin, which carries the majority of copper in blood. Ceruloplasmin also carries copper that is excreted in milk, and is particularly well-absorbed as a copper source. Copper in the body normally undergoes enterohepatic circulation (about 5 mg a day, vs. about 1 mg per day absorbed in the diet and excreted from the body), and the body is able to excrete some excess copper, if needed, via bile, which carries some copper out of the liver that is not then reabsorbed by the intestine.
The concentration of copper in ores averages only 0.6%, and most commercial ores are sulfides, especially chalcopyrite (CuFeS2) and to a lesser extent chalcocite (Cu2S). These minerals are concentrated from crushed ores to the level of 10–15% copper by froth flotation or bioleaching. Heating this material with silica in flash smelting removes much of the iron as slag. The process exploits the greater ease of converting iron sulfides into its oxides, which in turn react with the silica to form the silicate slag, which floats on top of the heated mass. The resulting copper matte consisting of Cu2S is then roasted to convert all sulfides into oxides:
Together with caesium and gold (both yellow), and osmium (bluish), copper is one of only four elemental metals with a natural color other than gray or silver. Pure copper is orange-red and acquires a reddish tarnish when exposed to air. The characteristic color of copper results from the electronic transitions between the filled 3d and half-empty 4s atomic shells – the energy difference between these shells is such that it corresponds to orange light. The same mechanism accounts for the yellow color of gold and caesium.
Copper has been used since ancient times as a durable, corrosion resistant, and weatherproof architectural material. Roofs, flashings, rain gutters, downspouts, domes, spires, vaults, and doors have been made from copper for hundreds or thousands of years. Copper's architectural use has been expanded in modern times to include interior and exterior wall cladding, building expansion joints, radio frequency shielding, and antimicrobial indoor products, such as attractive handrails, bathroom fixtures, and counter tops. Some of copper's other important benefits as an architectural material include its low thermal movement, light weight, lightning protection, and its recyclability.
Westminster Abbey, formally titled the Collegiate Church of St Peter at Westminster, is a large, mainly Gothic abbey church in the City of Westminster, London, located just to the west of the Palace of Westminster. It is one of the most notable religious buildings in the United Kingdom and has been the traditional place of coronation and burial site for English and, later, British monarchs. Between 1540 and 1556 the abbey had the status of a cathedral. Since 1560, however, the building is no longer an abbey nor a cathedral, having instead the status of a Church of England "Royal Peculiar"—a church responsible directly to the sovereign. The building itself is the original abbey church.
According to a tradition first reported by Sulcard in about 1080, a church was founded at the site (then known as Thorn Ey (Thorn Island)) in the 7th century, at the time of Mellitus, a Bishop of London. Construction of the present church began in 1245, on the orders of King Henry III.
The first reports of the abbey are based on a late tradition claiming that a young fisherman called Aldrich on the River Thames saw a vision of Saint Peter near the site. This seems to be quoted to justify the gifts of salmon from Thames fishermen that the abbey received in later years. In the present era, the Fishmonger's Company still gives a salmon every year. The proven origins are that in the 960s or early 970s, Saint Dunstan, assisted by King Edgar, installed a community of Benedictine monks here.
Between 1042 and 1052 King Edward the Confessor began rebuilding St Peter's Abbey to provide himself with a royal burial church. It was the first church in England built in the Romanesque style. The building was not completed until around 1090 but was consecrated on 28 December 1065, only a week before Edward's death on 5 January 1066. A week later he was buried in the church, and nine years later his wife Edith was buried alongside him. His successor, Harold II, was probably crowned in the abbey, although the first documented coronation is that of William the Conqueror later the same year.
Since 1066, when Harold Godwinson and William the Conqueror were crowned, the coronations of English and British monarchs have been held there. There have been at least 16 royal weddings at the abbey since 1100. Two were of reigning monarchs (Henry I and Richard II), although, before 1919, there had been none for some 500 years.
The only extant depiction of Edward's abbey, together with the adjacent Palace of Westminster, is in the Bayeux Tapestry. Some of the lower parts of the monastic dormitory, an extension of the South Transept, survive in the Norman undercroft of the Great School, including a door said to come from the previous Saxon abbey. Increased endowments supported a community increased from a dozen monks in Dunstan's original foundation, up to a maximum about eighty monks, although there was also a large community of lay brothers who supported the monastery's extensive property and activities.
The abbot and monks, in proximity to the royal Palace of Westminster, the seat of government from the later 12th century, became a powerful force in the centuries after the Norman Conquest. The abbot often was employed on royal service and in due course took his place in the House of Lords as of right. Released from the burdens of spiritual leadership, which passed to the reformed Cluniac movement after the mid-10th century, and occupied with the administration of great landed properties, some of which lay far from Westminster, "the Benedictines achieved a remarkable degree of identification with the secular life of their times, and particularly with upper-class life", Barbara Harvey concludes, to the extent that her depiction of daily life provides a wider view of the concerns of the English gentry in the High and Late Middle Ages.[citation needed]
The proximity of the Palace of Westminster did not extend to providing monks or abbots with high royal connections; in social origin the Benedictines of Westminster were as modest as most of the order. The abbot remained Lord of the Manor of Westminster as a town of two to three thousand persons grew around it: as a consumer and employer on a grand scale the monastery helped fuel the town economy, and relations with the town remained unusually cordial, but no enfranchising charter was issued during the Middle Ages. The abbey built shops and dwellings on the west side, encroaching upon the sanctuary.[citation needed]
The abbey became the coronation site of Norman kings. None were buried there until Henry III, intensely devoted to the cult of the Confessor, rebuilt the abbey in Anglo-French Gothic style as a shrine to venerate King Edward the Confessor and as a suitably regal setting for Henry's own tomb, under the highest Gothic nave in England. The Confessor's shrine subsequently played a great part in his canonisation. The work continued between 1245 and 1517 and was largely finished by the architect Henry Yevele in the reign of Richard II. Henry III also commissioned unique Cosmati pavement in front of the High Altar (the pavement has recently undergone a major cleaning and conservation programme and was re-dedicated by the Dean at a service on 21 May 2010).
Henry VII added a Perpendicular style chapel dedicated to the Blessed Virgin Mary in 1503 (known as the Henry VII Chapel or the "Lady Chapel"). Much of the stone came from Caen, in France (Caen stone), the Isle of Portland (Portland stone) and the Loire Valley region of France (tuffeau limestone).[citation needed]
In 1535, the abbey's annual income of £2400–2800[citation needed] (£1,310,000 to £1,530,000 as of 2016), during the assessment attendant on the Dissolution of the Monasteries rendered it second in wealth only to Glastonbury Abbey.
Henry VIII assumed direct royal control in 1539 and granted the abbey the status of a cathedral by charter in 1540, simultaneously issuing letters patent establishing the Diocese of Westminster. By granting the abbey cathedral status Henry VIII gained an excuse to spare it from the destruction or dissolution which he inflicted on most English abbeys during this period.
Westminster diocese was dissolved in 1550, but the abbey was recognised (in 1552, retroactively to 1550) as a second cathedral of the Diocese of London until 1556. The already-old expression "robbing Peter to pay Paul" may have been given a new lease of life when money meant for the abbey, which is dedicated to Saint Peter, was diverted to the treasury of St Paul's Cathedral.
The abbey was restored to the Benedictines under the Catholic Mary I of England, but they were again ejected under Elizabeth I in 1559. In 1560, Elizabeth re-established Westminster as a "Royal Peculiar" – a church of the Church of England responsible directly to the Sovereign, rather than to a diocesan bishop – and made it the Collegiate Church of St Peter (that is, a non-cathedral church with an attached chapter of canons, headed by a dean.) The last of Mary's abbots was made the first dean.
It suffered damage during the turbulent 1640s, when it was attacked by Puritan iconoclasts, but was again protected by its close ties to the state during the Commonwealth period. Oliver Cromwell was given an elaborate funeral there in 1658, only to be disinterred in January 1661 and posthumously hanged from a gibbet at Tyburn.
The abbey's two western towers were built between 1722 and 1745 by Nicholas Hawksmoor, constructed from Portland stone to an early example of a Gothic Revival design. Purbeck marble was used for the walls and the floors of Westminster Abbey, even though the various tombstones are made of different types of marble. Further rebuilding and restoration occurred in the 19th century under Sir George Gilbert Scott.
A narthex (a portico or entrance hall) for the west front was designed by Sir Edwin Lutyens in the mid-20th century but was not built. Images of the abbey prior to the construction of the towers are scarce, though the abbey's official website states that the building was without towers following Yevele's renovation, with just the lower segments beneath the roof level of the Nave completed.
Until the 19th century, Westminster was the third seat of learning in England, after Oxford and Cambridge. It was here that the first third of the King James Bible Old Testament and the last half of the New Testament were translated. The New English Bible was also put together here in the 20th century. Westminster suffered minor damage during the Blitz on 15 November 1940.
In the 1990s two icons by the Russian icon painter Sergei Fyodorov were hung in the abbey. On 6 September 1997 the funeral of Diana, Princess of Wales, was held at the Abbey. On 17 September 2010 Pope Benedict XVI became the first pope to set foot in the abbey.
Since the coronations in 1066 of both King Harold and William the Conqueror, coronations of English and British monarchs were held in the abbey. In 1216, Henry III was unable to be crowned in London when he first came to the throne, because the French prince Louis had taken control of the city, and so the king was crowned in Gloucester Cathedral. This coronation was deemed by the Pope to be improper, and a further coronation was held in the abbey on 17 May 1220. The Archbishop of Canterbury is the traditional cleric in the coronation ceremony.[citation needed]
King Edward's Chair (or St Edward's Chair), the throne on which English and British sovereigns have been seated at the moment of coronation, is housed within the abbey and has been used at every coronation since 1308. From 1301 to 1996 (except for a short time in 1950 when it was temporarily stolen by Scottish nationalists), the chair also housed the Stone of Scone upon which the kings of Scots are crowned. Although the Stone is now kept in Scotland, in Edinburgh Castle, at future coronations it is intended that the Stone will be returned to St Edward's Chair for use during the coronation ceremony.[citation needed]
Westminster Abbey is a collegiate church governed by the Dean and Chapter of Westminster, as established by Royal charter of Queen Elizabeth I in 1560, which created it as the Collegiate Church of St Peter Westminster and a Royal Peculiar under the personal jurisdiction of the Sovereign. The members of the Chapter are the Dean and four canons residentiary, assisted by the Receiver General and Chapter Clerk. One of the canons is also Rector of St Margaret's Church, Westminster, and often holds also the post of Chaplain to the Speaker of the House of Commons.
In addition to the Dean and canons, there are at present two full-time minor canons, one is precentor, and the other is sacrist. The office of Priest Vicar was created in the 1970s for those who assist the minor canons. Together with the clergy and Receiver General and Chapter Clerk, various lay officers constitute the college, including the Organist and Master of the Choristers, the Registrar, the Auditor, the Legal Secretary, the Surveyor of the Fabric, the Head Master of the choir school, the Keeper of the Muniments and the Clerk of the Works, as well as 12 lay vicars, 10 choristers and the High Steward and High Bailiff.
Henry III rebuilt the abbey in honour of a royal saint, Edward the Confessor, whose relics were placed in a shrine in the sanctuary. Henry III himself was interred nearby, as were many of the Plantagenet kings of England, their wives and other relatives. Until the death of George II of Great Britain in 1760, most kings and queens were buried in the abbey, some notable exceptions being Henry VI, Edward IV, Henry VIII and Charles I who are buried in St George's Chapel at Windsor Castle. Other exceptions include Richard III, now buried at Leicester Cathedral, and the de facto queen Lady Jane Grey, buried in the chapel of St Peter ad Vincula in the Tower of London. Most monarchs and royals who died after 1760 are buried either in St George's Chapel or at Frogmore to the east of Windsor Castle.[citation needed]
From the Middle Ages, aristocrats were buried inside chapels, while monks and other people associated with the abbey were buried in the cloisters and other areas. One of these was Geoffrey Chaucer, who was buried here as he had apartments in the abbey where he was employed as master of the King's Works. Other poets, writers and musicians were buried or memorialised around Chaucer in what became known as Poets' Corner. Abbey musicians such as Henry Purcell were also buried in their place of work.[citation needed]
Subsequently, it became one of Britain's most significant honours to be buried or commemorated in the abbey. The practice of burying national figures in the abbey began under Oliver Cromwell with the burial of Admiral Robert Blake in 1657. The practice spread to include generals, admirals, politicians, doctors and scientists such as Isaac Newton, buried on 4 April 1727, and Charles Darwin, buried 26 April 1882. Another was William Wilberforce who led the movement to abolish slavery in the United Kingdom and the Plantations, buried on 3 August 1833. Wilberforce was buried in the north transept, close to his friend, the former Prime Minister, William Pitt.[citation needed]
During the early 20th century it became increasingly common to bury cremated remains rather than coffins in the abbey. In 1905 the actor Sir Henry Irving was cremated and his ashes buried in Westminster Abbey, thereby becoming the first person ever to be cremated prior to interment at the abbey. The majority of interments at the Abbey are of cremated remains, but some burials still take place - Frances Challen, wife of the Rev Sebastian Charles, Canon of Westminster, was buried alongside her husband in the south choir aisle in 2014. Members of the Percy Family have a family vault, The Northumberland Vault, in St Nicholas's chapel within the abbey.
In the floor, just inside the great west door, in the centre of the nave, is the tomb of The Unknown Warrior, an unidentified British soldier killed on a European battlefield during the First World War. He was buried in the abbey on 11 November 1920. This grave is the only one in the abbey on which it is forbidden to walk.[citation needed]
At the east end of the Lady Chapel is a memorial chapel to the airmen of the RAF who were killed in the Second World War. It incorporates a memorial window to the Battle of Britain, which replaces an earlier Tudor stained glass window destroyed in the war.
On Saturday September 6, 1997 the formal, though not "state" Funeral of Diana, Princess of Wales, was held. It was a royal ceremonial funeral including royal pageantry and Anglican funeral liturgy. A Second Public service was held on Sunday at the demand of the people. The burial occurred privately later the same day. Diana's former husband, sons, mother, siblings, a close friend, and a clergyman were present. Diana's body was clothed in a black long-sleeved dress designed by Catherine Walker, which she had chosen some weeks before. A set of rosary beads was placed in her hands, a gift she had received from Mother Teresa. Her grave is on the grounds of her family estate, Althorp, on a private island.[citation needed]
Westminster School and Westminster Abbey Choir School are also in the precincts of the abbey. It was natural for the learned and literate monks to be entrusted with education, and Benedictine monks were required by the Pope to maintain a charity school in 1179. The Choir School educates and trains the choirboys who sing for services in the Abbey.
The organ was built by Harrison & Harrison in 1937, then with four manuals and 84 speaking stops, and was used for the first time at the coronation of King George VI. Some pipework from the previous Hill organ of 1848 was revoiced and incorporated in the new scheme. The two organ cases, designed in the late 19th century by John Loughborough Pearson, were re-instated and coloured in 1959.
In 1982 and 1987, Harrison and Harrison enlarged the organ under the direction of the then abbey organist Simon Preston to include an additional Lower Choir Organ and a Bombarde Organ: the current instrument now has five manuals and 109 speaking stops. In 2006, the console of the organ was refurbished by Harrison and Harrison, and space was prepared for two additional 16 ft stops on the Lower Choir Organ and the Bombarde Organ. One part of the instrument, the Celestial Organ, is currently not connected or playable.
The bells at the abbey were overhauled in 1971. The ring is now made up of ten bells, hung for change ringing, cast in 1971, by the Whitechapel Bell Foundry, tuned to the notes: F#, E, D, C#, B, A, G, F#, E and D. The Tenor bell in D (588.5 Hz) has a weight of 30 cwt, 1 qtr, 15 lb (3403 lb or 1544 kg).
In addition there are two service bells, cast by Robert Mot, in 1585 and 1598 respectively, a Sanctus bell cast in 1738 by Richard Phelps and Thomas Lester and two unused bells—one cast about 1320, by the successor to R de Wymbish, and a second cast in 1742, by Thomas Lester. The two service bells and the 1320 bell, along with a fourth small silver "dish bell", kept in the refectory, have been noted as being of historical importance by the Church Buildings Council of the Church of England.
The chapter house was built concurrently with the east parts of the abbey under Henry III, between about 1245 and 1253. It was restored by Sir George Gilbert Scott in 1872. The entrance is approached from the east cloister walk and includes a double doorway with a large tympanum above.
Inner and outer vestibules lead to the octagonal chapter house, which is of exceptional architectural purity. It is built in a Geometrical Gothic style with an octagonal crypt below. A pier of eight shafts carries the vaulted ceiling. To the sides are blind arcading, remains of 14th-century paintings and numerous stone benches above which are innovatory large 4-light quatre-foiled windows. These are virtually contemporary with the Sainte-Chapelle, Paris.
The chapter house has an original mid-13th-century tiled pavement. A door within the vestibule dates from around 1050 and is believed to be the oldest in England.[citation needed] The exterior includes flying buttresses added in the 14th century and a leaded tent-lantern roof on an iron frame designed by Scott. The Chapter house was originally used in the 13th century by Benedictine monks for daily meetings. It later became a meeting place of the King's Great Council and the Commons, predecessors of Parliament.
The Pyx Chamber formed the undercroft of the monks' dormitory. It dates to the late 11th century and was used as a monastic and royal treasury. The outer walls and circular piers are of 11th-century date, several of the capitals were enriched in the 12th century and the stone altar added in the 13th century. The term pyx refers to the boxwood chest in which coins were held and presented to a jury during the Trial of the Pyx, in which newly minted coins were presented to ensure they conformed to the required standards.
The chapter house and Pyx Chamber at Westminster Abbey are in the guardianship of English Heritage, but under the care and management of the Dean and Chapter of Westminster. English Heritage have funded a major programme of work on the chapter house, comprising repairs to the roof, gutters, stonework on the elevations and flying buttresses as well as repairs to the lead light.
The Westminster Abbey Museum is located in the 11th-century vaulted undercroft beneath the former monks' dormitory in Westminster Abbey. This is one of the oldest areas of the abbey, dating back almost to the foundation of the church by Edward the Confessor in 1065. This space has been used as a museum since 1908.
The exhibits include a collection of royal and other funeral effigies (funeral saddle, helm and shield of Henry V), together with other treasures, including some panels of mediaeval glass, 12th-century sculpture fragments, Mary II's coronation chair and replicas of the coronation regalia, and historic effigies of Edward III, Henry VII and his queen, Elizabeth of York, Charles II, William III, Mary II and Queen Anne.
Later wax effigies include a likeness of Horatio, Viscount Nelson, wearing some of his own clothes and another of Prime Minister William Pitt, Earl of Chatham, modelled by the American-born sculptor Patience Wright.[citation needed] During recent conservation of Elizabeth I's effigy, a unique corset dating from 1603 was found on the figure and is now displayed separately.[citation needed]
A recent addition to the exhibition is the late 13th-century Westminster Retable, England's oldest altarpiece, which was most probably designed for the high altar of the abbey. Although it has been damaged in past centuries, the panel has been expertly cleaned and conserved.
In June 2009 the first major building work at the abbey for 250 years was proposed. A corona—a crown-like architectural feature—was intended to be built around the lantern over the central crossing, replacing an existing pyramidal structure dating from the 1950s. This was part of a wider £23m development of the abbey expected to be completed in 2013. On 4 August 2010 the Dean and Chapter announced that, "[a]fter a considerable amount of preliminary and exploratory work", efforts toward the construction of a corona would not be continued. In 2012, architects Panter Hudspith completed refurbishment of the 14th-century food-store originally used by the abbey's monks, converting it into a restaurant with English Oak furniture by Covent Garden-based furniture makers Luke Hughes and Company.
A project that is proceeding is the creation of The Queen's Diamond Jubilee Galleries in the medieval triforium of the abbey. The aim is to create a new display area for the abbey's treasures in the galleries high up around the abbey's nave. To this end a new Gothic access tower with lift has been designed by the abbey architect and Surveyor of the Fabric, Ptolemy Dean. It is planned that the new galleries will open in 2018.
London i/ˈlʌndən/ is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south eastern part of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium. London's ancient core, the City of London, largely retains its 1.12-square-mile (2.9 km2) medieval boundaries and in 2011 had a resident population of 7,375, making it the smallest city in England. Since at least the 19th century, the term London has also referred to the metropolis developed around this core. The bulk of this conurbation forms Greater London,[note 1] a region of England governed by the Mayor of London and the London Assembly.[note 2] The conurbation also covers two English counties: the small district of the City of London and the county of Greater London. The latter constitutes the vast majority of London, though historically it was split between Middlesex (a now abolished county), Essex, Surrey, Kent and Hertfordshire.
London is a leading global city, with strengths in the arts, commerce, education, entertainment, fashion, finance, healthcare, media, professional services, research and development, tourism, and transport all contributing to its prominence. It is one of the world's leading financial centres and has the fifth-or sixth-largest metropolitan area GDP in the world depending on measurement.[note 3] London is a world cultural capital. It is the world's most-visited city as measured by international arrivals and has the world's largest city airport system measured by passenger traffic. London is one of the world's leading investment destinations, hosting more international retailers and ultra high-net-worth individuals than any other city. London's 43 universities form the largest concentration of higher education institutes in Europe, and a 2014 report placed it first in the world university rankings. According to the report London also ranks first in the world in software, multimedia development and design, and shares first position in technology readiness. In 2012, London became the first city to host the modern Summer Olympic Games three times.
London has a diverse range of peoples and cultures, and more than 300 languages are spoken within Greater London. The Office for National Statistics estimated its mid-2014 population to be 8,538,689, the largest of any municipality in the European Union, and accounting for 12.5 percent of the UK population. London's urban area is the second most populous in the EU, after Paris, with 9,787,426 inhabitants according to the 2011 census. The city's metropolitan area is one of the most populous in Europe with 13,879,757 inhabitants,[note 4] while the Greater London Authority states the population of the city-region (covering a large part of the south east) as 22.7 million. London was the world's most populous city from around 1831 to 1925.
London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT). Other famous landmarks include Buckingham Palace, the London Eye, Piccadilly Circus, St Paul's Cathedral, Tower Bridge, Trafalgar Square, and The Shard. London is home to numerous museums, galleries, libraries, sporting events and other cultural institutions, including the British Museum, National Gallery, Tate Modern, British Library and 40 West End theatres. The London Underground is the oldest underground railway network in the world.
From 1898, it was commonly accepted that the name was of Celtic origin and meant place belonging to a man called *Londinos; this explanation has since been rejected. Richard Coates put forward an explanation in 1998 that it is derived from the pre-Celtic Old European *(p)lowonida, meaning 'river too wide to ford', and suggested that this was a name given to the part of the River Thames which flows through London; from this, the settlement gained the Celtic form of its name, *Lowonidonjon; this requires quite a serious amendment however. The ultimate difficulty lies in reconciling the Latin form Londinium with the modern Welsh Llundain, which should demand a form *(h)lōndinion (as opposed to *londīnion), from earlier *loundiniom. The possibility cannot be ruled out that the Welsh name was borrowed back in from English at a later date, and thus cannot be used as a basis from which to reconstruct the original name.
Two recent discoveries indicate probable very early settlements near the Thames in the London area. In 1999, the remains of a Bronze Age bridge were found on the foreshore north of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500 BC. In 2010 the foundations of a large timber structure, dated to 4500 BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. Both structures are on South Bank, at a natural crossing point where the River Effra flows into the River Thames.
Although there is evidence of scattered Brythonic settlements in the area, the first major settlement was founded by the Romans after the invasion of 43 AD. This lasted only until around 61, when the Iceni tribe led by Queen Boudica stormed it, burning it to the ground. The next, heavily planned, incarnation of Londinium prospered, and it superseded Colchester as the capital of the Roman province of Britannia in 100. At its height in the 2nd century, Roman London had a population of around 60,000.
With the collapse of Roman rule in the early 5th century, London ceased to be a capital and the walled city of Londinium was effectively abandoned, although Roman civilisation continued in the St Martin-in-the-Fields area until around 450. From around 500, an Anglo-Saxon settlement known as Lundenwic developed in the same area, slightly to the west of the old Roman city. By about 680, it had revived sufficiently to become a major port, although there is little evidence of large-scale production of goods. From the 820s the town declined because of repeated Viking invasions. There are three recorded Viking assaults on London; two of which were successful in 851 and 886 AD, although they were defeated during the attack of 994 AD.
The Vikings established Danelaw over much of the eastern and northern part of England with its boundary roughly stretching from London to Chester. It was an area of political and geographical control imposed by the Viking incursions which was formally agreed to by the Danish warlord, Guthrum and west-Saxon king, Alfred the Great in 886 AD. The Anglo-Saxon Chronicle recorded that London was "refounded" by Alfred the Great in 886. Archaeological research shows that this involved abandonment of Lundenwic and a revival of life and trade within the old Roman walls. London then grew slowly until about 950, after which activity increased dramatically.
By the 11th century, London was beyond all comparison the largest town in England. Westminster Abbey, rebuilt in the Romanesque style by King Edward the Confessor, was one of the grandest churches in Europe. Winchester had previously been the capital of Anglo-Saxon England, but from this time on, London became the main forum for foreign traders and the base for defence in time of war. In the view of Frank Stenton: "It had the resources, and it was rapidly developing the dignity and the political self-consciousness appropriate to a national capital."
Following his victory in the Battle of Hastings, William, Duke of Normandy, was crowned King of England in the newly finished Westminster Abbey on Christmas Day 1066. William constructed the Tower of London, the first of the many Norman castles in England to be rebuilt in stone, in the southeastern corner of the city, to intimidate the native inhabitants. In 1097, William II began the building of Westminster Hall, close by the abbey of the same name. The hall became the basis of a new Palace of Westminster.
During the 12th century, the institutions of central government, which had hitherto accompanied the royal English court as it moved around the country, grew in size and sophistication and became increasingly fixed in one place. In most cases this was Westminster, although the royal treasury, having been moved from Winchester, came to rest in the Tower. While the City of Westminster developed into a true capital in governmental terms, its distinct neighbour, the City of London, remained England's largest city and principal commercial centre, and it flourished under its own unique administration, the Corporation of London. In 1100, its population was around 18,000; by 1300 it had grown to nearly 100,000.
During the Tudor period the Reformation produced a gradual shift to Protestantism, much of London passing from church to private ownership. The traffic in woollen cloths shipped undyed and undressed from London to the nearby shores of the Low Countries, where it was considered indispensable. But the tentacles of English maritime enterprise hardly extended beyond the seas of north-west Europe. The commercial route to Italy and the Mediterranean Sea normally lay through Antwerp and over the Alps; any ships passing through the Strait of Gibraltar to or from England were likely to be Italian or Ragusan. Upon the re-opening of the Netherlands to English shipping in January 1565, there ensued a strong outburst of commercial activity. The Royal Exchange was founded. Mercantilism grew, and monopoly trading companies such as the East India Company were established, with trade expanding to the New World. London became the principal North Sea port, with migrants arriving from England and abroad. The population rose from an estimated 50,000 in 1530 to about 225,000 in 1605.
During the English Civil War the majority of Londoners supported the Parliamentary cause. After an initial advance by the Royalists in 1642 culminating in the battles of Brentford and Turnham Green, London was surrounded by defensive perimeter wall known as the Lines of Communication. The lines were built by an up to 20,000 people, and were completed in under two months. The fortifications failed their only test when the New Model Army entered London in 1647, and they were levelled by Parliament the same year.
In 1762, George III acquired Buckingham House and it was enlarged over the next 75 years. During the 18th century, London was dogged by crime, and the Bow Street Runners were established in 1750 as a professional police force. In total, more than 200 offences were punishable by death, including petty theft. Most children born in the city died before reaching their third birthday. The coffeehouse became a popular place to debate ideas, with growing literacy and the development of the printing press making news widely available; and Fleet Street became the centre of the British press.
London was the world's largest city from about 1831 to 1925. London's overcrowded conditions led to cholera epidemics, claiming 14,000 lives in 1848, and 6,000 in 1866. Rising traffic congestion led to the creation of the world's first local urban rail network. The Metropolitan Board of Works oversaw infrastructure expansion in the capital and some of the surrounding counties; it was abolished in 1889 when the London County Council was created out of those areas of the counties surrounding the capital. London was bombed by the Germans during the First World War while during the Second World War, the Blitz and other bombings by the German Luftwaffe, killed over 30,000 Londoners and destroyed large tracts of housing and other buildings across the city. Immediately after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when London had barely recovered from the war.
Primarily starting in the mid-1960s, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture associated with the King's Road, Chelsea and Carnaby Street. The role of trendsetter was revived during the punk era. In 1965 London's political boundaries were expanded to take into account the growth of the urban area and a new Greater London Council was created. During The Troubles in Northern Ireland, London was subjected to bombing attacks by the Provisional IRA. Racial inequality was highlighted by the 1981 Brixton riot.
Greater London's population declined steadily in the decades after the Second World War, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. The principal ports for London moved downstream to Felixstowe and Tilbury, with the London Docklands area becoming a focus for regeneration, including the Canary Wharf development. This was borne out of London's ever-increasing role as a major international financial centre during the 1980s. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea.
The Greater London Council was abolished in 1986, which left London as the only large metropolis in the world without a central administration. In 2000, London-wide government was restored, with the creation of the Greater London Authority. To celebrate the start of the 21st century, the Millennium Dome, London Eye and Millennium Bridge were constructed. On 6 July 2005 London was awarded the 2012 Summer Olympics, making London the first city to stage the Olympic Games three times. In January 2015, Greater London's population was estimated to be 8.63 million, the highest level since 1939.
The administration of London is formed of two tiers—a city-wide, strategic tier and a local tier. City-wide administration is coordinated by the Greater London Authority (GLA), while local administration is carried out by 33 smaller authorities. The GLA consists of two elected components; the Mayor of London, who has executive powers, and the London Assembly, which scrutinises the mayor's decisions and can accept or reject the mayor's budget proposals each year. The headquarters of the GLA is City Hall, Southwark; the mayor is Boris Johnson. The mayor's statutory planning strategy is published as the London Plan, which was most recently revised in 2011. The local authorities are the councils of the 32 London boroughs and the City of London Corporation. They are responsible for most local services, such as local planning, schools, social services, local roads and refuse collection. Certain functions, such as waste management, are provided through joint arrangements. In 2009–2010 the combined revenue expenditure by London councils and the GLA amounted to just over £22 billion (£14.7 billion for the boroughs and £7.4 billion for the GLA).
The London Fire Brigade is the statutory fire and rescue service for Greater London. It is run by the London Fire and Emergency Planning Authority and is the third largest fire service in the world. National Health Service ambulance services are provided by the London Ambulance Service (LAS) NHS Trust, the largest free-at-the-point-of-use emergency ambulance service in the world. The London Air Ambulance charity operates in conjunction with the LAS where required. Her Majesty's Coastguard and the Royal National Lifeboat Institution operate on the River Thames, which is under the jurisdiction of the Port of London Authority from Teddington Lock to the sea.
London is the seat of the Government of the United Kingdom. Many government departments are based close to the Palace of Westminster, particularly along Whitehall, including the Prime Minister's residence at 10 Downing Street. The British Parliament is often referred to as the "Mother of Parliaments" (although this sobriquet was first applied to England itself by John Bright) because it has been the model for most other parliamentary systems. There are 73 Members of Parliament (MPs) from London, who correspond to local parliamentary constituencies in the national Parliament. As of May 2015, 45 are from the Labour Party, 27 are Conservatives, and one is a Liberal Democrat.
Policing in Greater London, with the exception of the City of London, is provided by the Metropolitan Police Service, overseen by the Mayor through the Mayor's Office for Policing and Crime (MOPAC). The City of London has its own police force – the City of London Police. The British Transport Police are responsible for police services on National Rail, London Underground, Docklands Light Railway and Tramlink services. A fourth police force in London, the Ministry of Defence Police, do not generally become involved with policing the general public.
Outward urban expansion is now prevented by the Metropolitan Green Belt, although the built-up area extends beyond the boundary in places, resulting in a separately defined Greater London Urban Area. Beyond this is the vast London commuter belt. Greater London is split for some purposes into Inner London and Outer London. The city is split by the River Thames into North and South, with an informal central London area in its interior. The coordinates of the nominal centre of London, traditionally considered to be the original Eleanor Cross at Charing Cross near the junction of Trafalgar Square and Whitehall, are approximately 51°30′26″N 00°07′39″W﻿ / ﻿51.50722°N 0.12750°W﻿ / 51.50722; -0.12750.
Within London, both the City of London and the City of Westminster have city status and both the City of London and the remainder of Greater London are counties for the purposes of lieutenancies. The area of Greater London has incorporated areas that were once part of the historic counties of Middlesex, Kent, Surrey, Essex and Hertfordshire. London's status as the capital of England, and later the United Kingdom, has never been granted or confirmed officially—by statute or in written form.[note 6]
Greater London encompasses a total area of 1,583 square kilometres (611 sq mi), an area which had a population of 7,172,036 in 2001 and a population density of 4,542 inhabitants per square kilometre (11,760/sq mi). The extended area known as the London Metropolitan Region or the London Metropolitan Agglomeration, comprises a total area of 8,382 square kilometres (3,236 sq mi) has a population of 13,709,000 and a population density of 1,510 inhabitants per square kilometre (3,900/sq mi). Modern London stands on the Thames, its primary geographical feature, a navigable river which crosses the city from the south-west to the east. The Thames Valley is a floodplain surrounded by gently rolling hills including Parliament Hill, Addington Hills, and Primrose Hill. The Thames was once a much broader, shallower river with extensive marshlands; at high tide, its shores reached five times their present width.
Summers are generally warm and sometimes hot. London's average July high is 24 °C (75.2 °F). On average London will see 31 days above 25 °C (77.0 °F) each year, and 4.2 days above 30.0 °C (86.0 °F) every year. During the 2003 European heat wave there were 14 consecutive days above 30 °C (86.0 °F) and 2 consecutive days where temperatures reached 38 °C (100.4 °F), leading to hundreds of heat related deaths. Winters are generally cool and damp with little temperature variation. Snowfall does occur from time to time, and can cause travel disruption when this happens. Spring and autumn are mixed seasons and can be pleasant. As a large city, London has a considerable urban heat island effect, making the centre of London at times 5 °C (9 °F) warmer than the suburbs and outskirts. The effect of this can be seen below when comparing London Heathrow, 15 miles west of London, with the London Weather Centre, in the city centre.
London's buildings are too diverse to be characterised by any particular architectural style, partly because of their varying ages. Many grand houses and public buildings, such as the National Gallery, are constructed from Portland stone. Some areas of the city, particularly those just west of the centre, are characterised by white stucco or whitewashed buildings. Few structures in central London pre-date the Great Fire of 1666, these being a few trace Roman remains, the Tower of London and a few scattered Tudor survivors in the City. Further out is, for example, the Tudor period Hampton Court Palace, England's oldest surviving Tudor palace, built by Cardinal Thomas Wolsey c.1515.
The Monument in the City of London provides views of the surrounding area while commemorating the Great Fire of London, which originated nearby. Marble Arch and Wellington Arch, at the north and south ends of Park Lane respectively, have royal connections, as do the Albert Memorial and Royal Albert Hall in Kensington. Nelson's Column is a nationally recognised monument in Trafalgar Square, one of the focal points of central London. Older buildings are mainly brick built, most commonly the yellow London stock brick or a warm orange-red variety, often decorated with carvings and white plaster mouldings.
In the dense areas, most of the concentration is via medium- and high-rise buildings. London's skyscrapers such as 30 St Mary Axe, Tower 42, the Broadgate Tower and One Canada Square are mostly in the two financial districts, the City of London and Canary Wharf. High-rise development is restricted at certain sites if it would obstruct protected views of St Paul's Cathedral and other historic buildings. Nevertheless, there are a number of very tall skyscrapers in central London (see Tall buildings in London), including the 95-storey Shard London Bridge, the tallest building in the European Union.
The London Natural History Society suggest that London is "one of the World's Greenest Cities" with more than 40 percent green space or open water. They indicate that 2000 species of flowering plant have been found growing there and that the tidal Thames supports 120 species of fish. They also state that over 60 species of bird nest in central London and that their members have recorded 47 species of butterfly, 1173 moths and more than 270 kinds of spider around London. London's wetland areas support nationally important populations of many water birds. London has 38 Sites of Special Scientific Interest (SSSIs), two National Nature Reserves and 76 Local Nature Reserves.
Among other inhabitants of London are 10,000 foxes, so that there are now 16 foxes for every square mile (2.6 square kilometres) of London. These urban foxes are noticeably bolder than their country cousins, sharing the pavement with pedestrians and raising cubs in people's backyards. Foxes have even sneaked into the Houses of Parliament, where one was found asleep on a filing cabinet. Another broke into the grounds of Buckingham Palace, reportedly killing some of Queen Elizabeth II's prized pink flamingos. Generally, however, foxes and city folk appear to get along. A survey in 2001 by the London-based Mammal Society found that 80 percent of 3,779 respondents who volunteered to keep a diary of garden mammal visits liked having them around. This sample cannot be taken to represent Londoners as a whole.
Other mammals found in Greater London are hedgehogs, rats, mice, rabbit, shrew, vole, and squirrels, In wilder areas of Outer London, such as Epping Forest, a wide variety of mammals are found including hare, badger, field, bank and water vole, wood mouse, yellow-necked mouse, mole, shrew, and weasel, in addition to fox, squirrel and hedgehog. A dead otter was found at The Highway, in Wapping, about a mile from the Tower Bridge, which would suggest that they have begun to move back after being absent a hundred years from the city. Ten of England's eighteen species of bats have been recorded in Epping Forest: soprano, nathusius and common pipistrelles, noctule, serotine, barbastelle, daubenton's, brown Long-eared, natterer's and leisler's.
Herds of red and fallow deer also roam freely within much of Richmond and Bushy Park. A cull takes place each November and February to ensure numbers can be sustained. Epping Forest is also known for its fallow deer, which can frequently be seen in herds to the north of the Forest. A rare population of melanistic, black fallow deer is also maintained at the Deer Sanctuary near Theydon Bois. Muntjac deer, which escaped from deer parks at the turn of the twentieth century, are also found in the forest. While Londoners are accustomed to wildlife such as birds and foxes sharing the city, more recently urban deer have started becoming a regular feature, and whole herds of fallow and white-tailed deer come into residential areas at night to take advantage of the London's green spaces.
The 2011 census recorded that 2,998,264 people or 36.7% of London's population are foreign-born making London the city with the second largest immigrant population, behind New York City, in terms of absolute numbers. The table to the right shows the most common countries of birth of London residents. Note that some of the German-born population, in 18th position, are British citizens from birth born to parents serving in the British Armed Forces in Germany. With increasing industrialisation, London's population grew rapidly throughout the 19th and early 20th centuries, and it was for some time in the late 19th and early 20th centuries the most populous city in the world. Its population peaked at 8,615,245 in 1939 immediately before the outbreak of the Second World War, but had declined to 7,192,091 at the 2001 Census. However, the population then grew by just over a million between the 2001 and 2011 Censuses, to reach 8,173,941 in the latter enumeration.
The region covers an area of 1,579 square kilometres (610 sq mi). The population density is 5,177 inhabitants per square kilometre (13,410/sq mi), more than ten times that of any other British region. In terms of population, London is the 19th largest city and the 18th largest metropolitan region in the world. As of 2014[update], London has the largest number of billionaires (British Pound Sterling) in the world, with 72 residing in the city. London ranks as one of the most expensive cities in the world, alongside Tokyo and Moscow.
Across London, Black and Asian children outnumber White British children by about six to four in state schools. Altogether at the 2011 census, of London's 1,624,768 population aged 0 to 15, 46.4 per cent were White, 19.8 per cent were Asian, 19 per cent were Black, 10.8 per cent were Mixed and 4 per cent represented another ethnic group. In January 2005, a survey of London's ethnic and religious diversity claimed that there were more than 300 languages spoken in London and more than 50 non-indigenous communities with a population of more than 10,000. Figures from the Office for National Statistics show that, in 2010[update], London's foreign-born population was 2,650,000 (33 per cent), up from 1,630,000 in 1997.
The 2011 census showed that 36.7 per cent of Greater London's population were born outside the UK. The table to the right shows the 30 most common countries of birth of London residents in 2011, the date of the last published UK Census. A portion of the German-born population are likely to be British nationals born to parents serving in the British Armed Forces in Germany. Estimates produced by the Office for National Statistics indicate that the five largest foreign-born groups living in London in the period July 2009 to June 2010 were those born in India, Poland, the Republic of Ireland, Bangladesh and Nigeria.
London is also home to sizeable Muslim, Hindu, Sikh, and Jewish communities. Notable mosques include the East London Mosque in Tower Hamlets, London Central Mosque on the edge of Regent's Park and the Baitul Futuh Mosque of the Ahmadiyya Muslim Community. Following the oil boom, increasing numbers of wealthy Hindus and Middle-Eastern Muslims have based themselves around Mayfair and Knightsbridge in West London. There are large Muslim communities in the eastern boroughs of Tower Hamlets and Newham. Large Hindu communities are in the north-western boroughs of Harrow and Brent, the latter of which is home to Europe's largest Hindu temple, Neasden Temple. London is also home to 42 Hindu temples. There are Sikh communities in East and West London, particularly in Southall, home to one of the largest Sikh populations and the largest Sikh temple outside India.
The majority of British Jews live in London, with significant Jewish communities in Stamford Hill, Stanmore, Golders Green, Finchley, Hampstead, Hendon and Edgware in North London. Bevis Marks Synagogue in the City of London is affiliated to London's historic Sephardic Jewish community. It is the only synagogue in Europe which has held regular services continuously for over 300 years. Stanmore and Canons Park Synagogue has the largest membership of any single Orthodox synagogue in the whole of Europe, overtaking Ilford synagogue (also in London) in 1998. The community set up the London Jewish Forum in 2006 in response to the growing significance of devolved London Government.
There are many accents that are traditionally thought of as London accents. The most well known of the London accents long ago acquired the Cockney label, which is heard both in London itself, and across the wider South East England region more generally. The accent of a 21st-century 'Londoner' varies widely; what is becoming more and more common amongst the under-30s however is some fusion of Cockney with a whole array of 'ethnic' accents, in particular Caribbean, which form an accent labelled Multicultural London English (MLE). The other widely heard and spoken accent is RP (Received Pronunciation) in various forms, which can often be heard in the media and many of other traditional professions and beyond, although this accent is not limited to London and South East England, and can also be heard selectively throughout the whole UK amongst certain social groupings.
London's largest industry is finance, and its financial exports make it a large contributor to the UK's balance of payments. Around 325,000 people were employed in financial services in London until mid-2007. London has over 480 overseas banks, more than any other city in the world. Over 85 percent (3.2 million) of the employed population of greater London works in the services industries. Because of its prominent global role, London's economy had been affected by the Late-2000s financial crisis. However, by 2010 the City has recovered; put in place new regulatory powers, proceeded to regain lost ground and re-established London's economic dominance. The City of London is home to the Bank of England, London Stock Exchange, and Lloyd's of London insurance market.
Along with professional services, media companies are concentrated in London and the media distribution industry is London's second most competitive sector. The BBC is a significant employer, while other broadcasters also have headquarters around the City. Many national newspapers are edited in London. London is a major retail centre and in 2010 had the highest non-food retail sales of any city in the world, with a total spend of around £64.2 billion. The Port of London is the second-largest in the United Kingdom, handling 45 million tonnes of cargo each year.
London is one of the leading tourist destinations in the world and in 2015 was ranked as the most visited city in the world with over 65 million visits. It is also the top city in the world by visitor cross-border spending, estimated at US$20.23 billion in 2015 Tourism is one of London's prime industries, employing the equivalent of 350,000 full-time workers in 2003, and the city accounts for 54% of all inbound visitor spend in UK. As of 2016 London is rated as the world top ranked city destination by TripAdvisor users.
Transport is one of the four main areas of policy administered by the Mayor of London, however the mayor's financial control does not extend to the longer distance rail network that enters London. In 2007 he assumed responsibility for some local lines, which now form the London Overground network, adding to the existing responsibility for the London Underground, trams and buses. The public transport network is administered by Transport for London (TfL) and is one of the most extensive in the world.
London is a major international air transport hub with the busiest city airspace in the world. Eight airports use the word London in their name, but most traffic passes through six of these. London Heathrow Airport, in Hillingdon, West London, is the busiest airport in the world for international traffic, and is the major hub of the nation's flag carrier, British Airways. In March 2008 its fifth terminal was opened. There were plans for a third runway and a sixth terminal; however, these were cancelled by the Coalition Government on 12 May 2010.
Stansted Airport, north east of London in Essex, is a local UK hub and Luton Airport to the north of London in Bedfordshire, caters mostly for cheap short-haul flights. London City Airport, the smallest and most central airport, in Newham, East London, is focused on business travellers, with a mixture of full service short-haul scheduled flights and considerable business jet traffic. London Southend Airport, east of London in Essex, is a smaller, regional airport that mainly caters for cheap short-haul flights.
There are 366 railway stations in the London Travelcard Zones on an extensive above-ground suburban railway network. South London, particularly, has a high concentration of railways as it has fewer Underground lines. Most rail lines terminate around the centre of London, running into eighteen terminal stations, with the exception of the Thameslink trains connecting Bedford in the north and Brighton in the south via Luton and Gatwick airports. London has Britain's busiest station by number of passengers – Waterloo, with over 184 million people using the interchange station complex (which includes Waterloo East station) each year. Clapham Junction is the busiest station in Europe by the number of trains passing.
Some international railway services to Continental Europe were operated during the 20th century as boat trains, such as the Admiraal de Ruijter to Amsterdam and the Night Ferry to Paris and Brussels. The opening of the Channel Tunnel in 1994 connected London directly to the continental rail network, allowing Eurostar services to begin. Since 2007, high-speed trains link St. Pancras International with Lille, Paris, Brussels and European tourist destinations via the High Speed 1 rail link and the Channel Tunnel. The first high-speed domestic trains started in June 2009 linking Kent to London. There are plans for a second high speed line linking London to the Midlands, North West England, and Yorkshire.
London's bus network is one of the largest in the world, running 24 hours a day, with about 8,500 buses, more than 700 bus routes and around 19,500 bus stops. In 2013, the network had more than 2 billion commuter trips per annum, more than the Underground. Around £850 million is taken in revenue each year. London has the largest wheelchair accessible network in the world and, from the 3rd quarter of 2007, became more accessible to hearing and visually impaired passengers as audio-visual announcements were introduced. The distinctive red double-decker buses are an internationally recognised trademark of London transport along with black cabs and the Tube.
London's first and only cable car, known as the Emirates Air Line, opened in June 2012. Crossing the River Thames, linking Greenwich Peninsula and the Royal Docks in the east of the city, the cable car is integrated with London's Oyster Card ticketing system, although special fares are charged. Costing £60 million to build, it carries over 3,500 passengers every day, although this is very much lower than its capacity. Similar to the Santander Cycles bike hire scheme, the cable car is sponsored in a 10-year deal by the airline Emirates.
Although the majority of journeys involving central London are made by public transport, car travel is common in the suburbs. The inner ring road (around the city centre), the North and South Circular roads (in the suburbs), and the outer orbital motorway (the M25, outside the built-up area) encircle the city and are intersected by a number of busy radial routes—but very few motorways penetrate into inner London. A plan for a comprehensive network of motorways throughout the city (the Ringways Plan) was prepared in the 1960s but was mostly cancelled in the early 1970s. The M25 is the longest ring-road motorway in the world at 121.5 mi (195.5 km) long. The A1 and M1 connect London to Leeds, and Newcastle and Edinburgh.
In 2003, a congestion charge was introduced to reduce traffic volumes in the city centre. With a few exceptions, motorists are required to pay £10 per day to drive within a defined zone encompassing much of central London. Motorists who are residents of the defined zone can buy a greatly reduced season pass. London government initially expected the Congestion Charge Zone to increase daily peak period Underground and bus users by 20,000 people, reduce road traffic by 10 to 15 per cent, increase traffic speeds by 10 to 15 per cent, and reduce queues by 20 to 30 per cent. Over the course of several years, the average number of cars entering the centre of London on a weekday was reduced from 195,000 to 125,000 cars – a 35-per-cent reduction of vehicles driven per day.
London is a major global centre of higher education teaching and research and its 43 universities form the largest concentration of higher education institutes in Europe. According to the QS World University Rankings 2015/16, London has the greatest concentration of top class universities in the world and the international student population around 110,000 which is also more than any other city in the world. A 2014 PricewaterhouseCoopers report termed London as the global capital of higher education
A number of world-leading education institutions are based in London. In the 2014/15 QS World University Rankings, Imperial College London is ranked joint 2nd in the world (alongside The University of Cambridge), University College London (UCL) is ranked 5th, and King's College London (KCL) is ranked 16th. The London School of Economics has been described as the world's leading social science institution for both teaching and research. The London Business School is considered one of the world's leading business schools and in 2015 its MBA programme was ranked second best in the world by the Financial Times.
With 120,000 students in London, the federal University of London is the largest contact teaching university in the UK. It includes four large multi-faculty universities – King's College London, Queen Mary, Royal Holloway and UCL – and a number of smaller and more specialised institutions including Birkbeck, the Courtauld Institute of Art, Goldsmiths, Guildhall School of Music and Drama, the Institute of Education, the London Business School, the London School of Economics, the London School of Hygiene & Tropical Medicine, the Royal Academy of Music, the Central School of Speech and Drama, the Royal Veterinary College and the School of Oriental and African Studies. Members of the University of London have their own admissions procedures, and some award their own degrees.
A number of universities in London are outside the University of London system, including Brunel University, City University London, Imperial College London, Kingston University, London Metropolitan University, Middlesex University, University of East London, University of West London and University of Westminster, (with over 34,000 students, the largest unitary university in London), London South Bank University, Middlesex University, University of the Arts London (the largest university of art, design, fashion, communication and the performing arts in Europe), University of East London, the University of West London and the University of Westminster. In addition there are three international universities in London – Regent's University London, Richmond, The American International University in London and Schiller International University.
London is home to five major medical schools – Barts and The London School of Medicine and Dentistry (part of Queen Mary), King's College London School of Medicine (the largest medical school in Europe), Imperial College School of Medicine, UCL Medical School and St George's, University of London – and has a large number of affiliated teaching hospitals. It is also a major centre for biomedical research, and three of the UK's five academic health science centres are based in the city – Imperial College Healthcare, King's Health Partners and UCL Partners (the largest such centre in Europe).
There are a number of business schools in London, including the London School of Business and Finance, Cass Business School (part of City University London), Hult International Business School, ESCP Europe, European Business School London, Imperial College Business School and the London Business School. London is also home to many specialist arts education institutions, including the Academy of Live and Recorded Arts, Central School of Ballet, LAMDA, London College of Contemporary Arts (LCCA), London Contemporary Dance School, National Centre for Circus Arts, RADA, Rambert School of Ballet and Contemporary Dance, the Royal College of Art, the Royal College of Music and Trinity Laban.
The majority of primary and secondary schools and further-education colleges in London are controlled by the London boroughs or otherwise state-funded; leading examples include City and Islington College, Ealing, Hammersmith and West London College, Leyton Sixth Form College, Tower Hamlets College and Bethnal Green Academy. There are also a number of private schools and colleges in London, some old and famous, such as City of London School, Harrow, St Paul's School, Haberdashers' Aske's Boys' School, University College School, The John Lyon School, Highgate School and Westminster School.
Within the City of Westminster in London the entertainment district of the West End has its focus around Leicester Square, where London and world film premieres are held, and Piccadilly Circus, with its giant electronic advertisements. London's theatre district is here, as are many cinemas, bars, clubs and restaurants, including the city's Chinatown district (in Soho), and just to the east is Covent Garden, an area housing speciality shops. The city is the home of Andrew Lloyd Webber, whose musicals have dominated the West End theatre since the late 20th century. The United Kingdom's Royal Ballet, English National Ballet, Royal Opera and English National Opera are based in London and perform at the Royal Opera House, the London Coliseum, Sadler's Wells Theatre and the Royal Albert Hall as well as touring the country.
Islington's 1 mile (1.6 km) long Upper Street, extending northwards from Angel, has more bars and restaurants than any other street in the United Kingdom. Europe's busiest shopping area is Oxford Street, a shopping street nearly 1 mile (1.6 km) long, making it the longest shopping street in the United Kingdom. Oxford Street is home to vast numbers of retailers and department stores, including the world-famous Selfridges flagship store. Knightsbridge, home to the equally renowned Harrods department store, lies to the south-west.
There is a variety of annual events, beginning with the relatively new New Year's Day Parade, fireworks display at the London Eye, the world's second largest street party, the Notting Hill Carnival is held during the late August Bank Holiday each year. Traditional parades include November's Lord Mayor's Show, a centuries-old event celebrating the annual appointment of a new Lord Mayor of the City of London with a procession along the streets of the City, and June's Trooping the Colour, a formal military pageant performed by regiments of the Commonwealth and British armies to celebrate the Queen's Official Birthday.
London has been the setting for many works of literature. The literary centres of London have traditionally been hilly Hampstead and (since the early 20th century) Bloomsbury. Writers closely associated with the city are the diarist Samuel Pepys, noted for his eyewitness account of the Great Fire, Charles Dickens, whose representation of a foggy, snowy, grimy London of street sweepers and pickpockets has been a major influence on people's vision of early Victorian London, and Virginia Woolf, regarded as one of the foremost modernist literary figures of the 20th century.
The pilgrims in Geoffrey Chaucer's late 14th-century Canterbury Tales set out for Canterbury from London – specifically, from the Tabard inn, Southwark. William Shakespeare spent a large part of his life living and working in London; his contemporary Ben Jonson was also based there, and some of his work—most notably his play The Alchemist—was set in the city. A Journal of the Plague Year (1722) by Daniel Defoe is a fictionalisation of the events of the 1665 Great Plague. Later important depictions of London from the 19th and early 20th centuries are Dickens' novels, and Arthur Conan Doyle's Sherlock Holmes stories. Modern writers pervasively influenced by the city include Peter Ackroyd, author of a "biography" of London, and Iain Sinclair, who writes in the genre of psychogeography.
London has played a significant role in the film industry, and has major studios at Ealing and a special effects and post-production community centred in Soho. Working Title Films has its headquarters in London. London has been the setting for films including Oliver Twist (1948), Scrooge (1951), Peter Pan (1953), The 101 Dalmatians (1961), My Fair Lady (1964), Mary Poppins (1964), Blowup (1966), The Long Good Friday (1980), Notting Hill (1999), Love Actually (2003), V For Vendetta (2005), Sweeney Todd: The Demon Barber Of Fleet Street (2008) and The King's Speech (2010). Notable actors and filmmakers from London include; Charlie Chaplin, Alfred Hitchcock, Michael Caine, Helen Mirren, Gary Oldman, Christopher Nolan, Jude Law, Tom Hardy, Keira Knightley and Daniel Day-Lewis. As of 2008[update], the British Academy Film Awards have taken place at the Royal Opera House. London is a major centre for television production, with studios including BBC Television Centre, The Fountain Studios and The London Studios. Many television programmes have been set in London, including the popular television soap opera EastEnders, broadcast by the BBC since 1985.
London is home to many museums, galleries, and other institutions, many of which are free of admission charges and are major tourist attractions as well as playing a research role. The first of these to be established was the British Museum in Bloomsbury, in 1753. Originally containing antiquities, natural history specimens and the national library, the museum now has 7 million artefacts from around the globe. In 1824 the National Gallery was founded to house the British national collection of Western paintings; this now occupies a prominent position in Trafalgar Square.
In the latter half of the 19th century the locale of South Kensington was developed as "Albertopolis", a cultural and scientific quarter. Three major national museums are there: the Victoria and Albert Museum (for the applied arts), the Natural History Museum and the Science Museum. The National Portrait Gallery was founded in 1856 to house depictions of figures from British history; its holdings now comprise the world's most extensive collection of portraits. The national gallery of British art is at Tate Britain, originally established as an annexe of the National Gallery in 1897. The Tate Gallery, as it was formerly known, also became a major centre for modern art; in 2000 this collection moved to Tate Modern, a new gallery housed in the former Bankside Power Station.
London is one of the major classical and popular music capitals of the world and is home to major music corporations, such as EMI and Warner Music Group as well as countless bands, musicians and industry professionals. The city is also home to many orchestras and concert halls, such as the Barbican Arts Centre (principal base of the London Symphony Orchestra and the London Symphony Chorus), Cadogan Hall (Royal Philharmonic Orchestra) and the Royal Albert Hall (The Proms). London's two main opera houses are the Royal Opera House and the London Coliseum. The UK's largest pipe organ is at the Royal Albert Hall. Other significant instruments are at the cathedrals and major churches. Several conservatoires are within the city: Royal Academy of Music, Royal College of Music, Guildhall School of Music and Drama and Trinity Laban.
London has numerous venues for rock and pop concerts, including the world's busiest arena the o2 arena and other large arenas such as Earls Court, Wembley Arena, as well as many mid-sized venues, such as Brixton Academy, the Hammersmith Apollo and the Shepherd's Bush Empire. Several music festivals, including the Wireless Festival, South West Four, Lovebox, and Hyde Park's British Summer Time are all held in London. The city is home to the first and original Hard Rock Cafe and the Abbey Road Studios where The Beatles recorded many of their hits. In the 1960s, 1970s and 1980s, musicians and groups like Elton John, Pink Floyd, David Bowie, Queen, The Kinks, The Rolling Stones, The Who, Eric Clapton, Led Zeppelin, The Small Faces, Iron Maiden, Fleetwood Mac, Elvis Costello, Cat Stevens, The Police, The Cure, Madness, The Jam, Dusty Springfield, Phil Collins, Rod Stewart and Sade, derived their sound from the streets and rhythms vibrating through London.
London was instrumental in the development of punk music, with figures such as the Sex Pistols, The Clash, and Vivienne Westwood all based in the city. More recent artists to emerge from the London music scene include George Michael, Kate Bush, Seal, Siouxsie and the Banshees, Bush, the Spice Girls, Jamiroquai, Blur, The Prodigy, Gorillaz, Mumford & Sons, Coldplay, Amy Winehouse, Adele, Ed Sheeran and One Direction. London is also a centre for urban music. In particular the genres UK garage, drum and bass, dubstep and grime evolved in the city from the foreign genres of hip hop and reggae, alongside local drum and bass. Black music station BBC Radio 1Xtra was set up to support the rise of home-grown urban music both in London and in the rest of the UK.
The largest parks in the central area of London are three of the eight Royal Parks, namely Hyde Park and its neighbour Kensington Gardens in the west, and Regent's Park to the north. Hyde Park in particular is popular for sports and sometimes hosts open-air concerts. Regent's Park contains London Zoo, the world's oldest scientific zoo, and is near the tourist attraction of Madame Tussauds Wax Museum. Primrose Hill in the northern part of Regent's Park at 256 feet (78 m) is a popular spot to view the city skyline.
Close to Richmond Park is Kew Gardens which has the world's largest collection of living plants. In 2003, the gardens were put on the UNESCO list of World Heritage Sites. There are also numerous parks administered by London's borough Councils, including Victoria Park in the East End and Battersea Park in the centre. Some more informal, semi-natural open spaces also exist, including the 320-hectare (790-acre) Hampstead Heath of North London, and Epping Forest, which covers 2,476 hectares (6,118.32 acres) in the east. Both are controlled by the City of London Corporation. Hampstead Heath incorporates Kenwood House, the former stately home and a popular location in the summer months where classical musical concerts are held by the lake, attracting thousands of people every weekend to enjoy the music, scenery and fireworks. Epping Forest is a popular venue for various outdoor activities, including mountain biking, walking, horse riding, golf, angling, and orienteering.
Walking is a popular recreational activity in London. Areas that provide for walks include Wimbledon Common, Epping Forest, Hampton Court Park, Hampstead Heath, the eight Royal Parks, canals and disused railway tracks. Access to canals and rivers has improved recently, including the creation of the Thames Path, some 28 miles (45 km) of which is within Greater London, and The Wandle Trail; this runs 12 miles (19 km) through South London along the River Wandle, a tributary of the River Thames. Other long distance paths, linking green spaces, have also been created, including the Capital Ring, the Green Chain Walk, London Outer Orbital Path ("Loop"), Jubilee Walkway, Lea Valley Walk, and the Diana, Princess of Wales Memorial Walk.
London's most popular sport is football and it has fourteen League football clubs, including five in the Premier League: Arsenal, Chelsea, Crystal Palace, Tottenham Hotspur, and West Ham United. Among other professional teams based in London include Fulham, Queens Park Rangers, Millwall and Charlton Athletic. In May 2012, Chelsea became the first London club to win the UEFA Champions League. Aside from Arsenal, Chelsea and Tottenham, none of the other London clubs have ever won the national league title.
Three Aviva Premiership rugby union teams are based in London, (London Irish, Saracens, and Harlequins), although currently only Harlequins and Saracens play their home games within Greater London. London Scottish and London Welsh play in the RFU Championship club and other rugby union clubs in the city include Richmond F.C., Rosslyn Park F.C., Westcombe Park R.F.C. and Blackheath F.C.. Twickenham Stadium in south-west London is the national rugby union stadium, and has a capacity of 82,000 now that the new south stand has been completed.
London is the world's most expensive office market for the last three years according to world property journal (2015) report. As of 2015[update] the residential property in London is worth $2.2 trillion - same value as that of Brazil annual GDP. The city has the highest property prices of any European city according to the Office for National Statistics and the European Office of Statistics. On average the price per square metre in central London is €24,252 (April 2014). This is higher than the property prices in other G8 European capital cities; Berlin €3,306, Rome €6,188 and Paris €11,229.
Guam (i/ˈɡwɑːm/ or /ˈɡwɒm/; Chamorro: Guåhån;[needs IPA] formally the Territory of Guam) is an unincorporated and organized territory of the United States. Located in the northwestern Pacific Ocean, Guam is one of five American territories with an established civilian government. The capital city is Hagåtña, and the most populous city is Dededo. In 2015, 161,785 people resided on Guam. Guamanians are American citizens by birth. Guam has an area of 544 km2 (210 sq mi) and a density of 297/km² (770/sq mi). It is the largest and southernmost of the Mariana Islands, and the largest island in Micronesia. Among its municipalities, Mongmong-Toto-Maite has the highest density at 1,425/km² (3,691/sq mi), whereas Inarajan and Umatac have the lowest density at 47/km² (119/sq mi). The highest point is Mount Lamlam at 406 meters (1,332 ft) above sea level.
The Chamorros, Guam's indigenous people, settled the island approximately 4,000 years ago. Portuguese explorer Ferdinand Magellan was the first European to visit the island on March 6, 1521. Guam was colonized in 1668 with settlers, like Diego Luis de San Vitores, a Catholic missionary. Between the 1500s and the 1700s, Guam was an important stopover for the Spanish Manila Galleons. During the Spanish–American War, the United States captured Guam on June 21, 1898. Under the Treaty of Paris, Spain ceded Guam to the United States on December 10, 1898. Guam is amongst the seventeen Non-Self-Governing Territories of the United Nations.
Before World War II, Guam and three other territories – American Samoa, Hawaii, and the Philippines – were the only American jurisdictions in the Pacific Ocean. On December 7, 1941, hours after the attack on Pearl Harbor, Guam was captured by the Japanese, and was occupied for thirty months. During the occupation, Guamanians were subjected to culture alignment, forced labor, beheadings, rape, and torture. Guam endured hostilities when American forces recaptured the island on July 21, 1944; Liberation Day commemorates the victory. Since the 1960s, the economy is supported by two industries: tourism and the United States Armed Forces.
The ancient-Chamorro society had four classes: chamorri (chiefs), matua (upper class), achaot (middle class), and mana'chang (lower class).:20–21 The matua were located in the coastal villages, which meant they had the best access to fishing grounds, whereas the mana'chang were located in the interior of the island. Matua and mana'chang rarely communicated with each other, and matua often used achaot as intermediaries. There were also "makåhna" (similar to shamans), skilled in healing and medicine. Belief in spirits of ancient Chamorros called "Taotao mo'na" still persists as a remnant of pre-European culture. Their society was organized along matrilineal clans.:21
The first European to discover Guam was Portuguese navigator Ferdinand Magellan, sailing for the King of Spain, when he sighted the island on March 6, 1521 during his fleet's circumnavigation of the globe.:41–42 When Magellan arrived on Guam, he was greeted by hundreds of small outrigger canoes that appeared to be flying over the water, due to their considerable speed. These outrigger canoes were called Proas, and resulted in Magellan naming Guam Islas de las Velas Latinas ("Islands of the Lateen sails"). Antonio Pigafetta, one of Magellan's original 18 the name "Island of Sails", but he also writes that the inhabitants "entered the ships and stole whatever they could lay their hands on", including "the small boat that was fastened to the poop of the flagship.":129 "Those people are poor, but ingenious and very thievish, on account of which we called those three islands Islas de los Ladrones ("Islands of thieves").":131
Despite Magellan's visit, Guam was not officially claimed by Spain until January 26, 1565 by General Miguel López de Legazpi.:46 From 1565 to 1815, Guam and the Northern Mariana Islands, the only Spanish outpost in the Pacific Ocean east of the Philippines, was an important resting stop for the Manila galleons, a fleet that covered the Pacific trade route between Acapulco and Manila.:51 To protect these Pacific fleets, Spain built several defensive structures which are still standing today, such as Fort Nuestra Señora de la Soledad in Umatac. It is the biggest single segment of Micronesia, the largest islands between the island of Kyushu (Japan), New Guinea, the Philippines, and the Hawaiian Islands.
Spanish colonization commenced on June 15, 1668 with the arrival of Diego Luis de San Vitores and Pedro Calungsod, who established the first Catholic church.:64 The islands were part of the Spanish East Indies governed from the Philippines, which were in turn part of the Viceroyalty of New Spain based in Mexico City. Other reminders of colonial times include the old Governor's Palace in Plaza de España and the Spanish Bridge, both in Hagatña. Guam's Cathedral Dulce Nombre de Maria was formally opened on February 2, 1669, as was the Royal College of San Juan de Letran.:68 Guam, along with the rest of the Mariana and Caroline Islands, were treated as part of Spain's colony in the Philippines. While Guam's Chamorro culture has indigenous roots, the cultures of both Guam and the Northern Marianas have many similarities with Spanish and Mexican culture due to three centuries of Spanish rule.
Intermittent warfare lasting from July 23, 1670 until July 1695, plus the typhoons of 1671 and 1693, and in particular the smallpox epidemic of 1688, reduced the Chamorro population from 50,000 to 10,000 to less than 5,000.:86 Precipitated by the death of Quipuha, and the murder of Father San Vitores and Pedro Calungsod by local rebel chief Matapang, tensions led to a number of conflicts. Captain Juan de Santiago started a campaign to pacify the island, which was continued by the successive commanders of the Spanish forces.:68–74
After his arrival in 1674, Captain Damian de Esplana ordered the arrest of rebels who attacked the population of certain towns. Hostilities eventually led to the destruction of villages such as Chochogo, Pepura, Tumon, Sidia-Aty, Sagua, Nagan and Ninca.:74–75 Starting in June 1676, the first Spanish Governor of Guam, Capt. Francisco de Irrisarri y Vinar controlled internal affairs more strictly than his predecessors in order to curb tensions. He also ordered the construction of schools, roads and other infrastructure.:75–76 Later, Capt. Jose de Quiroga arrived in 1680 and continued some of the development projects started by his predecessors. He also continued the search for the rebels who had assassinated Father San Vitores, resulting in campaigns against the rebels which were hiding out in some islands, eventually leading to the death of Matapang, Hurao and Aguarin.:77–78 Quiroga brought some natives from the northern islands to Guam, ordering the population to live in a few large villages.:78–79 These included Jinapsan, Umatac, Pago, Agat and Inarajan, where he built a number of churches.:79 By July 1695, Quiroga had completed the pacification process in Guam, Rota, Tinian and Aguigan.:85
The United States took control of the island in the 1898 Spanish–American War, as part of the Treaty of Paris. Guam was transferred to U.S. Navy control on 23 December 1898 by Executive Order 108-A. Guam came to serve as a station for American ships traveling to and from the Philippines, while the Northern Mariana Islands passed to Germany, and then to Japan. A U.S. Navy yard was established at Piti in 1899, and a marine barracks at Sumay in 1901.:13 Following the Philippine–American War, Emilio Aguinaldo and Apolinario Mabini were exiled on Guam in 1901.:vi
The Northern Mariana Islands had become a Japanese protectorate before the war. It was the Chamorros from the Northern Marianas who were brought to Guam to serve as interpreters and in other capacities for the occupying Japanese force. The Guamanian Chamorros were treated as an occupied enemy by the Japanese military. After the war, this would cause resentment between the Guamanian Chamorros and the Chamorros of the Northern Marianas. Guam's Chamorros believed their northern brethren should have been compassionate towards them, whereas having been occupied for over 30 years, the Northern Mariana Chamorros were loyal to Japan.
After World War II, the Guam Organic Act of 1950 established Guam as an unincorporated organized territory of the United States, provided for the structure of the island's civilian government, and granted the people U.S. citizenship. The Governor of Guam was federally appointed until 1968, when the Guam Elective Governor Act provided for the office's popular election.:242 Since Guam is not a U.S. state, U.S. citizens residing on Guam are not allowed to vote for president and their congressional representative is a non-voting member.
Guam lies between 13.2°N and 13.7°N and between 144.6°E and 145.0°E, and has an area of 212 square miles (549 km2), making it the 32nd largest island of the United States. It is the southernmost and largest island in the Mariana island chain and is also the largest island in Micronesia. This island chain was created by the colliding Pacific and Philippine Sea tectonic plates. Guam is the closest land mass to the Mariana Trench, a deep subduction zone, that lies beside the island chain to the east. Challenger Deep, the deepest surveyed point in the Oceans, is southwest of Guam at 35,797 feet (10,911 meters) deep. The highest point in Guam is Mount Lamlam at an elevation of 1,334 feet (407 meters).
The island of Guam is 30 miles (50 km) long and 4 to 12 miles (6 to 19 km) wide, 3⁄4 the size of Singapore. The island experiences occasional earthquakes due to its location on the western edge of the Pacific Plate and near the Philippine Sea Plate. In recent years, earthquakes with epicenters near Guam have had magnitudes ranging from 5.0 to 8.7. Unlike the Anatahan volcano in the Northern Mariana Islands, Guam is not volcanically active. However, due to its proximity to Anatahan, vog (i.e. volcanic smog) does occasionally affect Guam.
Guam's climate is characterized as tropical marine moderated by seasonal northeast trade winds. The weather is generally very warm and humid with little seasonal temperature variation. The mean high temperature is 86 °F (30 °C) and mean low is 76 °F (24 °C) with an average annual rainfall of 96 inches (2,180 mm). The dry season runs from December to June. The remaining months (July to November) constitute the rainy season. The months of January and February are considered the coolest months of the year with overnight low temperatures of 70–75 °F (21–24 °C) and low humidity levels. The highest temperature ever recorded in Guam was 96 °F (36 °C) on April 18, 1971 and April 1, 1990, and the lowest temperature ever recorded was 65 °F (18 °C) on February 8, 1973.
Post-European-contact Chamorro culture is a combination of American, Spanish, Filipino, other Micronesian Islander and Mexican traditions, with few remaining indigenous pre-Hispanic customs. These influences are manifested in the local language, music, dance, sea navigation, cuisine, fishing, games (such as batu, chonka, estuleks, and bayogu), songs and fashion. During Spanish colonial rule (1668–1898) the majority of the population was converted to Roman Catholicism and religious festivities such as Easter and Christmas became widespread. Post-contact Chamorro cuisine is largely based on corn, and includes tortillas, tamales, atole and chilaquiles, which are a clear influence from Spanish trade between Mesoamerica and Asia. The modern Chamorro language is a Malayo-Polynesian language with much Spanish and Filipino influence. Many Chamorros also have Spanish surnames because of their conversion to Roman Catholic Christianity and the adoption of names from the Catálogo alfabético de apellidos, a phenomenon also common to the Philippines.
Two aspects of indigenous pre-Hispanic culture that withstood time are chenchule' and inafa'maolek. Chenchule' is the intricate system of reciprocity at the heart of Chamorro society. It is rooted in the core value of inafa'maolek. Historian Lawrence Cunningham in 1992 wrote, "In a Chamorro sense, the land and its produce belong to everyone. Inafa'maolek, or interdependence, is the key, or central value, in Chamorro culture ... Inafa'maolek depends on a spirit of cooperation and sharing. This is the armature, or core, that everything in Chamorro culture revolves around. It is a powerful concern for mutuality rather than individualism and private property rights."
The core culture or Pengngan Chamorro is based on complex social protocol centered upon respect: From sniffing over the hands of the elders (called mangnginge in Chamorro), the passing down of legends, chants, and courtship rituals, to a person asking for permission from spiritual ancestors before entering a jungle or ancient battle grounds. Other practices predating Spanish conquest include galaide' canoe-making, making of the belembaotuyan (a string musical instrument made from a gourd), fashioning of åcho' atupat slings and slingstones, tool manufacture, Måtan Guma' burial rituals, and preparation of herbal medicines by Suruhanu.
The cosmopolitan and multicultural nature of modern Guam poses challenges for Chamorros struggling to preserve their culture and identity amidst forces of acculturation. The increasing numbers of Chamorros, especially Chamorro youth, relocating to the U.S. Mainland has further complicated both definition and preservation of Chamorro identity.[citation needed] While only a few masters exist to continue traditional art forms, the resurgence of interest among the Chamorros to preserve the language and culture has resulted in a growing number of young Chamorros who seek to continue the ancient ways of the Chamorro people.
Guam is governed by a popularly elected governor and a unicameral 15-member legislature, whose members are known as senators. Guam elects one non-voting delegate, currently Democrat Madeleine Z. Bordallo, to the United States House of Representatives. U.S. citizens in Guam vote in a straw poll for their choice in the U.S. Presidential general election, but since Guam has no votes in the Electoral College, the poll has no real effect. However, in sending delegates to the Republican and Democratic national conventions, Guam does have influence in the national presidential race. These delegates are elected by local party conventions.
In the 1980s and early 1990s, there was a significant movement in favor of the territory becoming a commonwealth, which would give it a level of self-government similar to Puerto Rico and the Northern Mariana Islands. However, the federal government rejected the version of a commonwealth that the government of Guam proposed, due to it having clauses incompatible with the Territorial Clause (Art. IV, Sec. 3, cl. 2) of the U.S. Constitution. Other movements advocate U.S. statehood for Guam, union with the state of Hawaii, union with the Northern Mariana Islands as a single territory, or independence.
The U.S. military has proposed building a new aircraft carrier berth on Guam and moving 8,600 Marines, and 9,000 of their dependents, to Guam from Okinawa, Japan. Including the required construction workers, this buildup would increase Guam's population by 45%. In a February 2010 letter, the United States Environmental Protection Agency sharply criticized these plans because of a water shortfall, sewage problems and the impact on coral reefs. By 2012, these plans had been cut to only have a maximum of 4,800 Marines stationed on the island, two thirds of which would be there on a rotational basis without their dependents.
Lying in the western Pacific, Guam is a popular destination for Japanese tourists. Its tourist hub, Tumon, features over 20 large hotels, a Duty Free Shoppers Galleria, Pleasure Island district, indoor aquarium, Sandcastle Las Vegas–styled shows and other shopping and entertainment venues. It is a relatively short flight from Asia or Australia compared to Hawaii, with hotels and seven public golf courses accommodating over a million tourists per year. Although 75% of the tourists are Japanese, Guam receives a sizable number of tourists from South Korea, the U.S., the Philippines, and Taiwan. Significant sources of revenue include duty-free designer shopping outlets, and the American-style malls: Micronesia Mall, Guam Premier Outlets, the Agana Shopping Center, and the world's largest Kmart.[citation needed]
The Compacts of Free Association between the United States, the Federated States of Micronesia, the Republic of the Marshall Islands and the Republic of Palau accorded the former entities of the Trust Territory of the Pacific Islands a political status of "free association" with the United States. The Compacts give citizens of these island nations generally no restrictions to reside in the United States (also its territories), and many were attracted to Guam due to its proximity, environmental, and cultural familiarity. Over the years, it was claimed by some in Guam that the territory has had to bear the brunt of this agreement in the form of public assistance programs and public education for those from the regions involved, and the federal government should compensate the states and territories affected by this type of migration.[citation needed] Over the years, Congress had appropriated "Compact Impact" aids to Guam, the Northern Mariana Islands and Hawaii, and eventually this appropriation was written into each renewed Compact. Some, however, continue to claim the compensation is not enough or that the distribution of actual compensation received is significantly disproportionate.[citation needed]
In 1899, the local postage stamps were overprinted "Guam" as was done for the other former Spanish colonies, but this was discontinued shortly thereafter and regular U.S. postage stamps have been used ever since. Because Guam is also part of the U.S. Postal System (postal abbreviation: GU, ZIP code range: 96910–96932), mail to Guam from the U.S. mainland is considered domestic and no additional charges are required. Private shipping companies, such as FedEx, UPS, and DHL, however, have no obligation to do so, and do not regard Guam as domestic.
The speed of mail traveling between Guam and the states varies depending on size and time of year. Light, first-class items generally take less than a week to or from the mainland, but larger first-class or Priority items can take a week or two. Fourth-class mail, such as magazines, are transported by sea after reaching Hawaii. Most residents use post office boxes or private mail boxes, although residential delivery is becoming increasingly available. Incoming mail not from the Americas should be addressed to "Guam" instead of "USA" to avoid being routed the long way through the U.S. mainland and possibly charged a higher rate (especially from Asia).
The Commercial Port of Guam is the island's lifeline because most products must be shipped into Guam for consumers. It receives the weekly calls of the Hawaii-based shipping line Matson, Inc. whose container ships connect Guam with Honolulu, Hawaii, Los Angeles, California, Oakland, California and Seattle, Washington. The port is also the regional transhipment hub for over 500,000 customers throughout the Micronesian region. The port is the shipping and receiving point for containers designated for the island's U.S. Department of Defense installations, Andersen Air Force Base and Commander, Naval Forces Marianas and eventually the Third Marine Expeditionary Force.
Guam is served by the Antonio B. Won Pat International Airport, which is a hub for United Airlines. The island is outside the United States customs zone so Guam is responsible for establishing and operating its own customs and quarantine agency and jurisdiction. Therefore, the U.S. Customs and Border Protection only carries immigration (but not customs) functions. Since Guam is under federal immigration jurisdiction, passengers arriving directly from the United States skip immigration and proceed directly to Guam Customs and Quarantine.
Believed to be a stowaway on a U.S. military transport near the end of World War II, the brown tree snake (Boiga irregularis) was accidentally introduced to Guam, that previously had no native species of snake. It nearly eliminated the native bird population. The problem was exacerbated because the reptile has no natural predators on the island. The brown tree snake, known locally as the kulebla, is native to northern and eastern coasts of Australia, Papua New Guinea, and the Solomon Islands. While slightly venomous, the snake is relatively harmless to human beings. Although some studies have suggested a high density of these serpents on Guam, residents rarely see the nocturnal creatures. The United States Department of Agriculture has trained detector dogs to keep the snakes out of the island's cargo flow. The United States Geological Survey also has dogs capable of detecting snakes in forested environments around the region's islands.
Before the introduction of the brown tree snake, Guam was home to several endemic bird species. Among them were the Guam rail (or ko'ko' bird in Chamorro) and the Guam flycatcher, both common throughout the island. Today the flycatcher is entirely extinct while the Guam rail is extinct in the wild but bred in captivity by the Division of Aquatic and Wildlife Resources. The devastation caused by the snake has been significant over the past several decades. As many as twelve bird species are believed to have been driven to extinction. According to many elders, ko'ko' birds were common in Guam before World War II.
An infestation of the coconut rhinoceros beetle (CRB), Oryctes rhinoceros, was detected on Guam on September 12, 2007. CRB is not known to occur in the United States except in American Samoa. Delimiting surveys performed September 13–25, 2007 indicated that the infestation was limited to Tumon Bay and Faifai Beach, an area of approximately 900 acres (3.6 km2). Guam Department of Agriculture (GDA) placed quarantine on all properties within the Tumon area on October 5 and later expanded the quarantine to about 2,500 acres (10 km2) on October 25; approximately 0.5 miles (800 m) radius in all directions from all known locations of CRB infestation. CRB is native to Southern Asia and distributed throughout Asia and the Western Pacific including Sri Lanka, Upolu, Samoa, American Samoa, Palau, New Britain, West Irian, New Ireland, Pak Island and Manus Island (New Guinea), Fiji, Cocos (Keeling) Islands, Mauritius, and Reunion.
Wildfires plague the forested areas of Guam every dry season despite the island's humid climate. Most fires are man-caused with 80% resulting from arson. Poachers often start fires to attract deer to the new growth. Invasive grass species that rely on fire as part of their natural life cycle grow in many regularly burned areas. Grasslands and "barrens" have replaced previously forested areas leading to greater soil erosion. During the rainy season sediment is carried by the heavy rains into the Fena Lake Reservoir and Ugum River, leading to water quality problems for southern Guam. Eroded silt also destroys the marine life in reefs around the island. Soil stabilization efforts by volunteers and forestry workers (planting trees) have had little success in preserving natural habitats.
Efforts have been made to protect Guam's coral reef habitats from pollution, eroded silt and overfishing, problems that have led to decreased fish populations. (Since Guam is a significant vacation spot for scuba divers, this is important.) In recent years, the Department of Agriculture, Division of Aquatic and Wildlife Resources has established several new marine preserves where fish populations are monitored by biologists. Before adopting U.S. Environmental Protection Agency standards, portions of Tumon Bay were dredged by the hotel chains to provide a better experience for hotel guests. Tumon Bay has since been made into a preserve. A federal Guam National Wildlife Refuge in northern Guam protects the decimated sea turtle population in addition to a small colony of Mariana fruit bats.
The University of Guam (UOG) and Guam Community College, both fully accredited by the Western Association of Schools and Colleges, offer courses in higher education. UOG is a member of the exclusive group of only 76 U.S. land-grant institutions in the entire United States. Pacific Islands University is a small Christian liberal arts institution nationally accredited by the Transnational Association of Christian Colleges and Schools. They offer courses at both the undergraduate and graduate levels.
The Guam Department of Education serves the entire island of Guam. In 2000, 32,000 students attended Guam's public schools. Guam Public Schools have struggled with problems such as high dropout rates and poor test scores. Guam's educational system has always faced unique challenges as a small community located 6,000 miles (9,700 km) from the U.S. mainland with a very diverse student body including many students who come from backgrounds without traditional American education. An economic downturn in Guam since the mid-1990s has compounded the problems in schools.
The Government of Guam maintains the island's main health care facility, Guam Memorial Hospital, in Tamuning. U.S. board certified doctors and dentists practice in all specialties. In addition, the U.S. Naval Hospital in Agana Heights serves active-duty members and dependents of the military community. There is one subscriber-based air ambulance located on the island, CareJet, which provides emergency patient transportation across Guam and surrounding islands. A private hospital, the Guam Regional Medical City opened its doors in early 2016.
If a defendant is sentenced to death at the trial level, the case then goes into a direct review. The direct review process is a typical legal appeal. An appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether the decision was legally sound or not. Direct review of a capital sentencing hearing will result in one of three outcomes. If the appellate court finds that no significant legal errors occurred in the capital sentencing hearing, the appellate court will affirm the judgment, or let the sentence stand. If the appellate court finds that significant legal errors did occur, then it will reverse the judgment, or nullify the sentence and order a new capital sentencing hearing. Lastly, if the appellate court finds that no reasonable juror could find the defendant eligible for the death penalty, a rarity, then it will order the defendant acquitted, or not guilty, of the crime for which he/she was given the death penalty, and order him sentenced to the next most severe punishment for which the offense is eligible. About 60 percent survive the process of direct review intact.
Under the Antiterrorism and Effective Death Penalty Act of 1996, a state prisoner is ordinarily only allowed one suit for habeas corpus in federal court. If the federal courts refuse to issue a writ of habeas corpus, an execution date may be set. In recent times, however, prisoners have postponed execution through a final round of federal litigation using the Civil Rights Act of 1871 — codified at 42 U.S.C. § 1983 — which allows people to bring lawsuits against state actors to protect their federal constitutional and statutory rights.
The moratorium ended on January 17, 1977 with the shooting of Gary Gilmore by firing squad in Utah. The first use of the electric chair after the moratorium was the electrocution of John Spenkelink in Florida on May 25, 1979. The first use of the gas chamber after the moratorium was the gassing of Jesse Bishop in Nevada on October 22, 1979. The first use of the gallows after the moratorium was the hanging of Westley Allan Dodd in Washington on January 5, 1993. The first use of lethal injection was on December 7, 1982, when Charles Brooks, Jr., was executed in Texas.
Electrocution was the preferred method of execution during the 20th century. Electric chairs have commonly been nicknamed Old Sparky; however, Alabama's electric chair became known as the "Yellow Mama" due to its unique color. Some, particularly in Florida, were noted for malfunctions, which caused discussion of their cruelty and resulted in a shift to lethal injection as the preferred method of execution. Although lethal injection dominates as a method of execution, some states allow prisoners on death row to choose the method used to execute them.
Other states with long histories of no death penalty include Wisconsin (the only state with only one execution), Rhode Island (although later reintroduced, it was unused and abolished again), Maine, North Dakota, Minnesota, West Virginia, Iowa, and Vermont. The District of Columbia has also abolished the death penalty; it was last used in 1957. Oregon abolished the death penalty through an overwhelming majority in a 1964 public referendum but reinstated it in a 1984 joint death penalty/life imprisonment referendum by an even higher margin after a similar 1978 referendum succeeded but was not implemented due to judicial rulings.
Within the context of the overall murder rate, the death penalty cannot be said to be widely or routinely used in the United States; in recent years the average has been about one execution for about every 700 murders committed, or 1 execution for about every 325 murder convictions. However, 32 of the 50 states still execute people. Among them, Alabama has the highest per capita rate of death sentences. This is due to judges overriding life imprisonment sentences and imposing the death penalty. No other states allow this.
Puerto Rico's constitution expressly forbids capital punishment, stating "The death penalty shall not exist", setting it apart from all U.S. states and territories other than Michigan, which also has a constitutional prohibition (eleven other states and the District of Columbia have abolished capital punishment through statutory law). However, capital punishment is still applicable to offenses committed in Puerto Rico, if they fall under the jurisdiction of the federal government, though federal death penalty prosecutions there have generated significant controversy.
Capital punishment was suspended in the United States from 1972 through 1976 primarily as a result of the Supreme Court's decision in Furman v. Georgia. The last pre-Furman execution was that of Luis Monge on June 2, 1967. In this case, the court found that the death penalty was being imposed in an unconstitutional manner, on the grounds of cruel and unusual punishment in violation of the Eighth Amendment to the United States Constitution. The Supreme Court has never ruled the death penalty to be per se unconstitutional.
Present-day statutes from across the nation use the same words and phrases, requiring modern executions to take place within a wall or enclosure to exclude public view. Connecticut General Statute § 54–100 requires death sentences to be conducted in an "enclosure" which "shall be so constructed as to exclude public view." Kentucky Revised Statute 431.220 and Missouri Revised Statute § 546.730 contain substantially identical language. New Mexico's former death penalty, since repealed, see N.M. Stat. § 31-14-12, required executions be conducted in a "room or place enclosed from public view." Similarly, a dormant Massachusetts law, see Mass. Gen. Law ch. 279 § 60, required executions to take place "within an enclosure or building." North Carolina General Statute § 15-188 requires death sentences to be executed "within the walls" of the penitentiary, as do Oklahoma Statute Title 22 § 1015 and Montana Code § 46-19-103. Ohio Revised Code § 2949.22 requires that "[t]he enclosure shall exclude public view." Similarly, Tennessee Code § 40-23-116 requires "an enclosure" for "strict seclusion and privacy." United States Code Title 18 § 3596 and the Code of Federal Regulations 28 CFR 26.4 limit the witnesses permitted at federal executions.
In 1976, contemporaneously with Woodson and Roberts, the Court decided Gregg v. Georgia and upheld a procedure in which the trial of capital crimes was bifurcated into guilt-innocence and sentencing phases. At the first proceeding, the jury decides the defendant's guilt; if the defendant is innocent or otherwise not convicted of first-degree murder, the death penalty will not be imposed. At the second hearing, the jury determines whether certain statutory aggravating factors exist, whether any mitigating factors exist, and, in many jurisdictions, weigh the aggravating and mitigating factors in assessing the ultimate penalty – either death or life in prison, either with or without parole.
Possibly in part due to expedited federal habeas corpus procedures embodied in the Antiterrorism and Effective Death Penalty Act of 1996, the pace of executions picked up, reaching a peak of 98 in 1999 and then they declined gradually to 28 in 2015. Since the death penalty was reauthorized in 1976, 1,411 people have been executed, almost exclusively by the states, with most occurring after 1990. Texas has accounted for over one-third of modern executions (although only two death sentences were imposed in Texas during 2015, with the courts preferring to issue sentences of life without parole instead) and over four times as many as Oklahoma, the state with the second-highest number. California has the greatest number of prisoners on death row, has issued the highest number of death sentences but has held relatively few executions.
As of November 2008, there is only one person on death row facing capital punishment who has not been convicted of murder. Demarcus Sears remains under a death sentence in Georgia for the crime of "kidnapping with bodily injury." Sears was convicted in 1986 for the kidnapping and bodily injury of victim Gloria Ann Wilbur. Wilbur was kidnapped and beaten in Georgia, raped in Tennessee, and murdered in Kentucky. Sears was never charged with the murder of Wilbur in Kentucky, but was sentenced to death by a jury in Georgia for "kidnapping with bodily injury."
At times when a death sentence is affirmed on direct review, it is considered final. Yet, supplemental methods to attack the judgment, though less familiar than a typical appeal, do remain. These supplemental remedies are considered collateral review, that is, an avenue for upsetting judgments that have become otherwise final. Where the prisoner received his death sentence in a state-level trial, as is usually the case, the first step in collateral review is state collateral review. (If the case is a federal death penalty case, it proceeds immediately from direct review to federal habeas corpus.) Although all states have some type of collateral review, the process varies widely from state to state. Generally, the purpose of these collateral proceedings is to permit the prisoner to challenge his sentence on grounds that could not have been raised reasonably at trial or on direct review. Most often these are claims, such as ineffective assistance of counsel, which requires the court to consider new evidence outside the original trial record, something courts may not do in an ordinary appeal. State collateral review, though an important step in that it helps define the scope of subsequent review through federal habeas corpus, is rarely successful in and of itself. Only around 6 percent of death sentences are overturned on state collateral review. In 2010, the death sentences of 53 inmates were overturned as a result of legal appeals or high court reversals.
Traditionally, Section 1983 was of limited use for a state prisoner under sentence of death because the Supreme Court has held that habeas corpus, not Section 1983, is the only vehicle by which a state prisoner can challenge his judgment of death. In the 2006 Hill v. McDonough case, however, the United States Supreme Court approved the use of Section 1983 as a vehicle for challenging a state's method of execution as cruel and unusual punishment in violation of the Eighth Amendment. The theory is that a prisoner bringing such a challenge is not attacking directly his judgment of death, but rather the means by which that the judgment will be carried out. Therefore, the Supreme Court held in the Hill case that a prisoner can use Section 1983 rather than habeas corpus to bring the lawsuit. Yet, as Clarence Hill's own case shows, lower federal courts have often refused to hear suits challenging methods of execution on the ground that the prisoner brought the claim too late and only for the purposes of delay. Further, the Court's decision in Baze v. Rees, upholding a lethal injection method used by many states, has drastically narrowed the opportunity for relief through Section 1983.
The largest single execution in United States history was the hanging of 38 American Indians convicted of murder and rape during the Dakota War of 1862. They were executed simultaneously on December 26, 1862, in Mankato, Minnesota. A single blow from an axe cut the rope that held the large four-sided platform, and the prisoners (except for one whose rope had broken and who had to be re-hanged) fell to their deaths. The second-largest mass execution was also a hanging: the execution of 13 African-American soldiers for taking part in the Houston Riot of 1917. The largest non-military mass execution occurred in one of the original thirteen colonies in 1723, when 26 convicted pirates were hanged in Newport, Rhode Island by order of the Admiralty Court.
After a death sentence is affirmed in state collateral review, the prisoner may file for federal habeas corpus, which is a unique type of lawsuit that can be brought in federal courts. Federal habeas corpus is a species of collateral review, and it is the only way that state prisoners may attack a death sentence in federal court (other than petitions for certiorari to the United States Supreme Court after both direct review and state collateral review). The scope of federal habeas corpus is governed by the Antiterrorism and Effective Death Penalty Act of 1996, which restricted significantly its previous scope. The purpose of federal habeas corpus is to ensure that state courts, through the process of direct review and state collateral review, have done at least a reasonable job in protecting the prisoner's federal constitutional rights. Prisoners may also use federal habeas corpus suits to bring forth new evidence that they are innocent of the crime, though to be a valid defense at this late stage in the process, evidence of innocence must be truly compelling.
In New Jersey and Illinois, all death row inmates had their sentences commuted to life in prison without parole when the death penalty repeal bills were signed into law. In Maryland, Governor Martin O'Malley commuted the state's four remaining death sentences to life in prison without parole in January 2015. While the bill repealing capital punishment in Connecticut was not retroactive, the Connecticut Supreme Court ruled in 2015 in State v. Santiago that the legislature's decision to prospectively abolish capital punishment rendered it an offense to "evolving standards of decency," thus commuting the sentences of the 11 men remaining on death row to life in prison without parole. New Mexico may yet execute two condemned inmates sentenced prior to abolition, and Nebraska has ten death row inmates who may still be executed despite abolition.
The United States Supreme Court in Penry v. Lynaugh and the United States Court of Appeals for the Fifth Circuit in Bigby v. Dretke have been clear in their decisions that jury instructions in death penalty cases that do not ask about mitigating factors regarding the defendant's mental health violate the defendant's Eighth Amendment rights, saying that the jury is to be instructed to consider mitigating factors when answering unrelated questions. This ruling suggests that specific explanations to the jury are necessary to weigh mitigating factors.
Several states have never had capital punishment, the first being Michigan, which abolished it shortly after entering the Union. (However, the United States government executed Tony Chebatoris at the Federal Correctional Institution in Milan, Michigan in 1938.) Article 4, Section 46 of Michigan's fourth Constitution (ratified in 1963; effective in 1964) prohibits any law providing for the penalty of death. Attempts to change the provision have failed. In 2004, a constitutional amendment proposed to allow capital punishment in some circumstances failed to make it on the November ballot after a resolution failed in the legislature and a public initiative failed to gather enough signatures.
The method of execution of federal prisoners for offenses under the Violent Crime Control and Law Enforcement Act of 1994 is that of the state in which the conviction took place. If the state has no death penalty, the judge must choose a state with the death penalty for carrying out the execution. For offenses under the Drug Kingpin Act of 1988, the method of execution is lethal injection. The Federal Correctional Complex in Terre Haute, Indiana is currently the home of the only death chamber for federal death penalty recipients in the United States, where inmates are put to death by lethal injection. The complex has so far been the only location used for federal executions post-Gregg. Timothy McVeigh and Juan Garza were put to death in June 2001, and Louis Jones, Jr. was put to death on March 18, 2003.
In October 2009, the American Law Institute voted to disavow the framework for capital punishment that it had created in 1962, as part of the Model Penal Code, "in light of the current intractable institutional and structural obstacles to ensuring a minimally adequate system for administering capital punishment." A study commissioned by the institute had said that experience had proved that the goal of individualized decisions about who should be executed and the goal of systemic fairness for minorities and others could not be reconciled.
In 1977, the Supreme Court's Coker v. Georgia decision barred the death penalty for rape of an adult woman, and implied that the death penalty was inappropriate for any offense against another person other than murder. Prior to the decision, the death penalty for rape of an adult had been gradually phased out in the United States, and at the time of the decision, the State of Georgia and the U.S. Federal government were the only two jurisdictions to still retain the death penalty for that offense. However, three states maintained the death penalty for child rape, as the Coker decision only imposed a ban on executions for the rape of an adult woman. In 2008, the Kennedy v. Louisiana decision barred the death penalty for child rape. The result of these two decisions means that the death penalty in the United States is largely restricted to cases where the defendant took the life of another human being. The current federal kidnapping statute, however, may be exempt because the death penalty applies if the victim dies in the perpetrator's custody, not necessarily by his hand, thus stipulating a resulting death, which was the wording of the objection. In addition, the Federal government retains the death penalty for non-murder offenses that are considered crimes against the state, including treason, espionage, and crimes under military jurisdiction.
Four states in the modern era, Nebraska in 2008, New York and Kansas in 2004, and Massachusetts in 1984, had their statutes ruled unconstitutional by state courts. The death rows of New York and Massachusetts were disestablished, and attempts to restore the death penalty were unsuccessful. Kansas successfully appealed State v. Kleypas, the Kansas Supreme Court decision that declared the state's death penalty statute unconstitutional, to the United States Supreme Court. Nebraska's death penalty statute was rendered ineffective on February 8, 2008 when the required method, electrocution, was ruled unconstitutional by the Nebraska Supreme Court. In 2009, Nebraska enacted a bill that changed its method of execution to lethal injection.
In the 2010s, American jurisdictions have experienced a shortage of lethal injection drugs, due to anti-death penalty advocacy and low production volume. Hospira, the only U.S. manufacturer of sodium thiopental, stopped making the drug in 2011. The European Union has outlawed the export of any product that could be used in an execution; this has prevented executioners from using EU-manufactured anesthetics like propofol which are needed for general medical purposes. Another alternative, pentobarbital, is also only manufactured in the European Union, which has caused the Danish producer to restrict distribution to U.S. government customers.
As noted in the introduction to this article, the American public has maintained its position of support for capital punishment for murder. However, when given a choice between the death penalty and life imprisonment without parole, support has traditionally been significantly lower than polling which has only mentioned the death penalty as a punishment. In 2010, for instance, one poll showed 49 percent favoring the death penalty and 46 percent favoring life imprisonment while in another 61% said they preferred another punishment to the death penalty. The highest level of support for the death penalty recorded overall was 80 percent in 1994 (16 percent opposed), and the lowest recorded was 42 percent in 1966 (47 percent opposed). On the question of the death penalty vs. life without parole, the strongest preference for the death penalty was 61 percent in 1997 (29 percent favoring life), and the lowest preference for the death penalty was 47 percent in 2006 (48 percent favoring life).
Other capital crimes include: the use of a weapon of mass destruction resulting in death, espionage, terrorism, certain violations of the Geneva Conventions that result in the death of one or more persons, and treason at the federal level; aggravated rape in Louisiana, Florida, and Oklahoma; extortionate kidnapping in Oklahoma; aggravated kidnapping in Georgia, Idaho, Kentucky and South Carolina; aircraft hijacking in Alabama and Mississippi; assault by an escaping capital felon in Colorado; armed robbery in Georgia; drug trafficking resulting in a person's death in Florida; train wrecking which leads to a person's death, and perjury which leads to a person's death in California, Colorado, Idaho and Nebraska.
In a five-to-four decision, the Supreme Court struck down the impositions of the death penalty in each of the consolidated cases as unconstitutional. The five justices in the majority did not produce a common opinion or rationale for their decision, however, and agreed only on a short statement announcing the result. The narrowest opinions, those of Byron White and Potter Stewart, expressed generalized concerns about the inconsistent application of the death penalty across a variety of cases but did not exclude the possibility of a constitutional death penalty law. Stewart and William O. Douglas worried explicitly about racial discrimination in enforcement of the death penalty. Thurgood Marshall and William J. Brennan, Jr. expressed the opinion that the death penalty was proscribed absolutely by the Eighth Amendment as "cruel and unusual" punishment.
In total, 156 prisoners have been either acquitted, or received pardons or commutations on the basis of possible innocence, between 1973 to 2015. Death penalty opponents often argue that this statistic shows how perilously close states have come to undertaking wrongful executions; proponents point out that the statistic refers only to those exonerated in law, and that the truly innocent may be a smaller number. Statistics likely understate the actual problem of wrongful convictions because once an execution has occurred there is often insufficient motivation and finance to keep a case open, and it becomes unlikely at that point that the miscarriage of justice will ever be exposed.
The death penalty is sought and applied more often in some jurisdictions, not only between states but within states. A 2004 Cornell University study showed that while 2.5 percent of murderers convicted nationwide were sentenced to the death penalty, in Nevada 6 percent were given the death penalty. Texas gave 2 percent of murderers a death sentence, less than the national average. Texas, however, executed 40 percent of those sentenced, which was about four times higher than the national average. California had executed only 1 percent of those sentenced.
Congress acted defiantly toward the Supreme Court by passing the Drug Kingpin Act of 1988 and the Federal Death Penalty Act of 1994 that made roughly fifty crimes punishable by death, including crimes that do not always involve the death of someone. Such non-death capital offenses include treason, espionage (spying for another country), and high-level drug trafficking. Since no one has yet been sentenced to death for such non-death capital offenses, the Supreme Court has not ruled on their constitutionality.
Executions resumed on January 17, 1977, when Gary Gilmore went before a firing squad in Utah. But the pace was quite slow due to the use of litigation tactics which involved filing repeated writs of habeas corpus, which succeeded for many in delaying their actual execution for many years. Although hundreds of individuals were sentenced to death in the United States during the 1970s and early 1980s, only ten people besides Gilmore (who had waived all of his appeal rights) were actually executed prior to 1984.
After the September 2011 execution of Troy Davis, believed by many to be innocent, Richard Dieter, the director of the Death Penalty Information Center, said this case was a clear wake-up call to politicians across the United States. He said: "They weren't expecting such passion from people in opposition to the death penalty. There's a widely held perception that all Americans are united in favor of executions, but this message came across loud and clear that many people are not happy with it." Brian Evans of Amnesty International, which led the campaign to spare Davis's life, said that there was a groundswell in America of people "who are tired of a justice system that is inhumane and inflexible and allows executions where there is clear doubts about guilt". He predicted the debate would now be conducted with renewed energy.
Various methods have been used in the history of the American colonies and the United States but only five methods are currently used. Historically, burning, crushing, breaking on wheel, and bludgeoning were used for a small number of executions, while hanging was the most common method. The last person burned at the stake was a black slave in South Carolina in August 1825. The last person to be hanged in chains was a murderer named John Marshall in West Virginia on April 4, 1913. Although beheading was a legal method in Utah from 1851 to 1888, it was never used.
African Americans made up 41 percent of death row inmates while making up only 12.6 percent of the general population. (They have made up 34 percent of those actually executed since 1976.) However, that number is lower than that of prison inmates, which is 47 percent. According to the US Department of Justice, African Americans accounted for 52.5% of homicide offenders from 1980 to 2008, with whites 45.3% and Native Americans and Asians 2.2%. This means African Americans are less likely to be executed on a per capita basis. However, according to a 2003 Amnesty International report, blacks and whites were the victims of murder in almost equal numbers, yet 80 percent of the people executed since 1977 were convicted of murders involving white victims. 13.5% of death row inmates are of Hispanic or Latino descent, while they make up 17.4% of the general population.
The legal administration of the death penalty in the United States is complex. Typically, it involves four critical steps: (1) sentencing, (2) direct review, (3) state collateral review, and (4) federal habeas corpus. Recently, a narrow and final fifth level of process – (5) the Section 1983 challenge – has become increasingly important. (Clemency or pardon, through which the Governor or President of the jurisdiction can unilaterally reduce or abrogate a death sentence, is an executive rather than judicial process.) The number of new death sentences handed down peaked in 1995–1996 (309). There were 73 new death sentences handed down in 2014, the lowest number since 1973 (44).
All of the executions which have taken place since the 1936 hanging of Bethea in Owensboro have been conducted within a wall or enclosure. For example, Fred Adams was legally hanged in Kennett, Missouri, on April 2, 1937, within a 10-foot (3 m) wooden stockade. Roscoe "Red" Jackson was hanged within a stockade in Galena, Missouri, on May 26, 1937. Two Kentucky hangings were conducted after Galena in which numerous persons were present within a wooden stockade, that of John "Peter" Montjoy in Covington, Kentucky on December 17, 1937, and that of Harold Van Venison in Covington on June 3, 1938. An estimated 400 witnesses were present for the hanging of Lee Simpson in Ryegate, Montana, on December 30, 1939. The execution of Timothy McVeigh on June 11, 2001 was witnessed by some 300 people, some by closed-circuit television.
James Liebman, a professor of law at Columbia Law School, stated in 1996 that his study found that when habeas corpus petitions in death penalty cases were traced from conviction to completion of the case that there was "a 40 percent success rate in all capital cases from 1978 to 1995." Similarly, a study by Ronald Tabak in a law review article puts the success rate in habeas corpus cases involving death row inmates even higher, finding that between "1976 and 1991, approximately 47 percent of the habeas petitions filed by death row inmates were granted." The different numbers are largely definitional, rather than substantive. Freedam's statistics looks at the percentage of all death penalty cases reversed, while the others look only at cases not reversed prior to habeas corpus review.
The last use of the firing squad between 1608 and the moratorium on judicial executions between 1967 and 1977 was when Utah shot James W. Rodgers on March 30, 1960. The last use of the gallows between 1608 and the moratorium was when Kansas hanged George York on June 22, 1965. The last use of the electric chair between the first electrocution on August 6, 1890 and the moratorium was when Oklahoma electrocuted James French on August 10, 1966. The last use of the gas chamber between the first gassing on February 8, 1924 and the moratorium was when Colorado gassed Luis Monge on June 2, 1967.
Since 1642 (in the 13 colonies, the United States under the Articles of Confederation, and the current United States) an estimated 364 juvenile offenders have been put to death by the states and the federal government. The earliest known execution of a prisoner for crimes committed as a juvenile was Thomas Graunger in 1642. Twenty-two of the executions occurred after 1976, in seven states. Due to the slow process of appeals, it was highly unusual for a condemned person to be under 18 at the time of execution. The youngest person to be executed in the 20th century was George Stinney, who was electrocuted in South Carolina at the age of 14 on June 16, 1944. The last execution of a juvenile may have been Leonard Shockley, who died in the Maryland gas chamber on April 10, 1959, at the age of 17. No one has been under age 19 at time of execution since at least 1964. Since the reinstatement of the death penalty in 1976, 22 people have been executed for crimes committed under the age of 18. Twenty-one were 17 at the time of the crime. The last person to be executed for a crime committed as a juvenile was Scott Hain on April 3, 2003 in Oklahoma.
In May 2014, Oklahoma Director of Corrections, Robert Patton, recommended an indefinite hold on executions in the state after the botched execution of African-American Clayton Lockett. The prisoner had to be tasered to restrain him prior to the execution, and the lethal injection missed a vein in his groin, resulting in Lockett regaining consciousness, trying to get up, and to speak, before dying of a heart attack 43 minutes later, after the attempted execution had been called off. In 2015, the state approved nitrogen asphyxiation as a method of execution.
Opponents argue that the death penalty is not an effective means of deterring crime, risks the execution of the innocent, is unnecessarily barbaric in nature, cheapens human life, and puts a government on the same base moral level as those criminals involved in murder. Furthermore, some opponents argue that the arbitrariness with which it is administered and the systemic influence of racial, socio-economic, geographic, and gender bias on determinations of desert make the current practice of capital punishment immoral and illegitimate.
Sixteen was held to be the minimum permissible age in the 1988 Supreme Court decision of Thompson v. Oklahoma. The Court, considering the case Roper v. Simmons in March 2005, found the execution of juvenile offenders unconstitutional by a 5–4 margin, effectively raising the minimum permissible age to 18. State laws have not been updated to conform with this decision. In the American legal system, unconstitutional laws do not need to be repealed; instead, they are held to be unenforceable. (See also List of juvenile offenders executed in the United States)
Around 1890, a political movement developed in the United States to mandate private executions. Several states enacted laws which required executions to be conducted within a "wall" or "enclosure" to "exclude public view." For example, in 1919, the Missouri legislature adopted a statute (L.1919, p. 781) which required, "the sentence of death should be executed within the county jail, if convenient, and otherwise within an enclosure near the jail." The Missouri law permitted the local sheriff to distribute passes to individuals (usually local citizens) whom he believed should witness the hanging, but the sheriffs – for various reasons – sometimes denied passes to individuals who wanted to watch. Missouri executions conducted after 1919 were not "public" because they were conducted behind closed walls, and the general public was not permitted to attend.
Previous post-Furman mass clemencies took place in 1986 in New Mexico, when Governor Toney Anaya commuted all death sentences because of his personal opposition to the death penalty. In 1991, outgoing Ohio Governor Dick Celeste commuted the sentences of eight prisoners, among them all four women on the state's death row. And during his two terms (1979–1987) as Florida's Governor, Bob Graham, although a strong death penalty supporter who had overseen the first post-Furman involuntary execution as well as 15 others, agreed to commute the sentences of six people on the grounds of "possible innocence" or "disproportionality."
In 2010, bills to abolish the death penalty in Kansas and in South Dakota (which had a de facto moratorium at the time) were rejected. Idaho ended its de facto moratorium, during which only one volunteer had been executed, on November 18, 2011 by executing Paul Ezra Rhoades; South Dakota executed Donald Moeller on October 30, 2012, ending a de facto moratorium during which only two volunteers had been executed. Of the 12 prisoners whom Nevada has executed since 1976, 11 waived their rights to appeal. Kentucky and Montana have executed two prisoners against their will (KY: 1997 and 1999, MT: 1995 and 1998) and one volunteer, respectively (KY: 2008, MT: 2006). Colorado (in 1997) and Wyoming (in 1992) have executed only one prisoner, respectively.
Pharmaceutical companies whose products are used in the three-drug cocktails for lethal injections are predominantly European, and they have strenuously objected to the use of their drugs for executions and taken steps to prevent their use. For example, Hospira, the sole American manufacturer of sodium thiopental, the critical anesthetic in the three-drug cocktail, announced in 2011 that it would no longer manufacture the drug for the American market, in part for ethical reasons and in part because its transfer of sodium thiopental manufacturing to Italy would subject it to the European Union's Torture Regulation, which forbids the use of any product manufactured within the Union for torture (as execution by lethal injection is considered by the Regulation). Since the drug manufacturers began taking these steps and the EU regulation ended the importation of drugs produced in Europe, the resulting shortage of execution drugs has led to or influenced decisions to impose moratoria in Arkansas, California, Kentucky, Louisiana, Mississippi, Montana, Nevada, North Carolina, and Tennessee.
With an estimated population of 1,381,069 as of July 1, 2014, San Diego is the eighth-largest city in the United States and second-largest in California. It is part of the San Diego–Tijuana conurbation, the second-largest transborder agglomeration between the US and a bordering country after Detroit–Windsor, with a population of 4,922,723 people. San Diego is the birthplace of California and is known for its mild year-round climate, natural deep-water harbor, extensive beaches, long association with the United States Navy and recent emergence as a healthcare and biotechnology development center.
Historically home to the Kumeyaay people, San Diego was the first site visited by Europeans on what is now the West Coast of the United States. Upon landing in San Diego Bay in 1542, Juan Rodríguez Cabrillo claimed the entire area for Spain, forming the basis for the settlement of Alta California 200 years later. The Presidio and Mission San Diego de Alcalá, founded in 1769, formed the first European settlement in what is now California. In 1821, San Diego became part of the newly-independent Mexico, which reformed as the First Mexican Republic two years later. In 1850, it became part of the United States following the Mexican–American War and the admission of California to the union.
The first European to visit the region was Portuguese-born explorer Juan Rodríguez Cabrillo sailing under the flag of Castile. Sailing his flagship San Salvador from Navidad, New Spain, Cabrillo claimed the bay for the Spanish Empire in 1542, and named the site 'San Miguel'. In November 1602, Sebastián Vizcaíno was sent to map the California coast. Arriving on his flagship San Diego, Vizcaíno surveyed the harbor and what are now Mission Bay and Point Loma and named the area for the Catholic Saint Didacus, a Spaniard more commonly known as San Diego de Alcalá. On November 12, 1602, the first Christian religious service of record in Alta California was conducted by Friar Antonio de la Ascensión, a member of Vizcaíno's expedition, to celebrate the feast day of San Diego.
In May 1769, Gaspar de Portolà established the Fort Presidio of San Diego on a hill near the San Diego River. It was the first settlement by Europeans in what is now the state of California. In July of the same year, Mission San Diego de Alcalá was founded by Franciscan friars under Junípero Serra. By 1797, the mission boasted the largest native population in Alta California, with over 1,400 neophytes living in and around the mission proper. Mission San Diego was the southern anchor in California of the historic mission trail El Camino Real. Both the Presidio and the Mission are National Historic Landmarks.
In 1821, Mexico won its independence from Spain, and San Diego became part of the Mexican territory of Alta California. In 1822, Mexico began attempting to extend its authority over the coastal territory of Alta California. The fort on Presidio Hill was gradually abandoned, while the town of San Diego grew up on the level land below Presidio Hill. The Mission was secularized by the Mexican government in 1833, and most of the Mission lands were sold to wealthy Californio settlers. The 432 residents of the town petitioned the governor to form a pueblo, and Juan María Osuna was elected the first alcalde ("municipal magistrate"), defeating Pío Pico in the vote. (See, List of pre-statehood mayors of San Diego.) However, San Diego had been losing population throughout the 1830s and in 1838 the town lost its pueblo status because its size dropped to an estimated 100 to 150 residents. Beyond town Mexican land grants expanded the number of California ranchos that modestly added to the local economy.
In 1846, the United States went to war against Mexico and sent a naval and land expedition to conquer Alta California. At first they had an easy time of it capturing the major ports including San Diego, but the Californios in southern Alta California struck back. Following the successful revolt in Los Angeles, the American garrison at San Diego was driven out without firing a shot in early October 1846. Mexican partisans held San Diego for three weeks until October 24, 1846, when the Americans recaptured it. For the next several months the Americans were blockaded inside the pueblo. Skirmishes occurred daily and snipers shot into the town every night. The Californios drove cattle away from the pueblo hoping to starve the Americans and their Californio supporters out. On December 1 the Americans garrison learned that the dragoons of General Stephen W. Kearney were at Warner's Ranch. Commodore Robert F. Stockton sent a mounted force of fifty under Captain Archibald Gillespie to march north to meet him. Their joint command of 150 men, returning to San Diego, encountered about 93 Californios under Andrés Pico. In the ensuing Battle of San Pasqual, fought in the San Pasqual Valley which is now part of the city of San Diego, the Americans suffered their worst losses in the campaign. Subsequently a column led by Lieutenant Gray arrived from San Diego, rescuing Kearny's battered and blockaded command.
Stockton and Kearny went on to recover Los Angeles and force the capitulation of Alta California with the "Treaty of Cahuenga" on January 13, 1847. As a result of the Mexican–American War of 1846–48, the territory of Alta California, including San Diego, was ceded to the United States by Mexico, under the terms of the Treaty of Guadalupe Hidalgo in 1848. The Mexican negotiators of that treaty tried to retain San Diego as part of Mexico, but the Americans insisted that San Diego was "for every commercial purpose of nearly equal importance to us with that of San Francisco," and the Mexican-American border was eventually established to be one league south of the southernmost point of San Diego Bay, so as to include the entire bay within the United States.
The state of California was admitted to the United States in 1850. That same year San Diego was designated the seat of the newly established San Diego County and was incorporated as a city. Joshua H. Bean, the last alcalde of San Diego, was elected the first mayor. Two years later the city was bankrupt; the California legislature revoked the city's charter and placed it under control of a board of trustees, where it remained until 1889. A city charter was re-established in 1889 and today's city charter was adopted in 1931.
The original town of San Diego was located at the foot of Presidio Hill, in the area which is now Old Town San Diego State Historic Park. The location was not ideal, being several miles away from navigable water. In 1850, William Heath Davis promoted a new development by the Bay shore called "New San Diego", several miles south of the original settlement; however, for several decades the new development consisted only a few houses, a pier and an Army depot. In the late 1860s, Alonzo Horton promoted a move to the bayside area, which he called "New Town" and which became Downtown San Diego. Horton promoted the area heavily, and people and businesses began to relocate to New Town because of its location on San Diego Bay convenient to shipping. New Town soon eclipsed the original settlement, known to this day as Old Town, and became the economic and governmental heart of the city. Still, San Diego remained a relative backwater town until the arrival of a railroad connection in 1878.
In the early part of the 20th century, San Diego hosted two World's Fairs: the Panama-California Exposition in 1915 and the California Pacific International Exposition in 1935. Both expositions were held in Balboa Park, and many of the Spanish/Baroque-style buildings that were built for those expositions remain to this day as central features of the park. The buildings were intended to be temporary structures, but most remained in continuous use until they progressively fell into disrepair. Most were eventually rebuilt, using castings of the original façades to retain the architectural style. The menagerie of exotic animals featured at the 1915 exposition provided the basis for the San Diego Zoo. During the 1950s there was a citywide festival called Fiesta del Pacifico highlighting the area's Spanish and Mexican past. In the 2010s there was a proposal for a large-scale celebration of the 100th anniversary of Balboa Park, but the plans were abandoned when the organization tasked with putting on the celebration went out of business.
The southern portion of the Point Loma peninsula was set aside for military purposes as early as 1852. Over the next several decades the Army set up a series of coastal artillery batteries and named the area Fort Rosecrans. Significant U.S. Navy presence began in 1901 with the establishment of the Navy Coaling Station in Point Loma, and expanded greatly during the 1920s. By 1930, the city was host to Naval Base San Diego, Naval Training Center San Diego, San Diego Naval Hospital, Camp Matthews, and Camp Kearny (now Marine Corps Air Station Miramar). The city was also an early center for aviation: as early as World War I, San Diego was proclaiming itself "The Air Capital of the West". The city was home to important airplane developers and manufacturers like Ryan Airlines (later Ryan Aeronautical), founded in 1925, and Consolidated Aircraft (later Convair), founded in 1923. Charles A. Lindbergh's plane The Spirit of St. Louis was built in San Diego in 1927 by Ryan Airlines.
During World War II, San Diego became a major hub of military and defense activity, due to the presence of so many military installations and defense manufacturers. The city's population grew rapidly during and after World War II, more than doubling between 1930 (147,995) and 1950 (333,865). During the final months of the war, the Japanese had a plan to target multiple U.S. cities for biological attack, starting with San Diego. The plan was called "Operation Cherry Blossoms at Night" and called for kamikaze planes filled with fleas infected with plague (Yersinia pestis) to crash into civilian population centers in the city, hoping to spread plague in the city and effectively kill tens of thousands of civilians. The plan was scheduled to launch on September 22, 1945, but was not carried out because Japan surrendered five weeks earlier.
From the start of the 20th century through the 1970s, the American tuna fishing fleet and tuna canning industry were based in San Diego, "the tuna capital of the world". San Diego's first tuna cannery was founded in 1911, and by the mid-1930s the canneries employed more than 1,000 people. A large fishing fleet supported the canneries, mostly staffed by immigrant fishermen from Japan, and later from the Portuguese Azores and Italy whose influence is still felt in neighborhoods like Little Italy and Point Loma. Due to rising costs and foreign competition, the last of the canneries closed in the early 1980s.
The city lies on approximately 200 deep canyons and hills separating its mesas, creating small pockets of natural open space scattered throughout the city and giving it a hilly geography. Traditionally, San Diegans have built their homes and businesses on the mesas, while leaving the urban canyons relatively wild. Thus, the canyons give parts of the city a segmented feel, creating gaps between otherwise proximate neighborhoods and contributing to a low-density, car-centered environment. The San Diego River runs through the middle of San Diego from east to west, creating a river valley which serves to divide the city into northern and southern segments. The river used to flow into San Diego Bay and its fresh water was the focus of the earliest Spanish explorers.[citation needed] Several reservoirs and Mission Trails Regional Park also lie between and separate developed areas of the city.
Downtown San Diego is located on San Diego Bay. Balboa Park encompasses several mesas and canyons to the northeast, surrounded by older, dense urban communities including Hillcrest and North Park. To the east and southeast lie City Heights, the College Area, and Southeast San Diego. To the north lies Mission Valley and Interstate 8. The communities north of the valley and freeway, and south of Marine Corps Air Station Miramar, include Clairemont, Kearny Mesa, Tierrasanta, and Navajo. Stretching north from Miramar are the northern suburbs of Mira Mesa, Scripps Ranch, Rancho Peñasquitos, and Rancho Bernardo. The far northeast portion of the city encompasses Lake Hodges and the San Pasqual Valley, which holds an agricultural preserve. Carmel Valley and Del Mar Heights occupy the northwest corner of the city. To their south are Torrey Pines State Reserve and the business center of the Golden Triangle. Further south are the beach and coastal communities of La Jolla, Pacific Beach, Mission Beach, and Ocean Beach. Point Loma occupies the peninsula across San Diego Bay from downtown. The communities of South San Diego, such as San Ysidro and Otay Mesa, are located next to the Mexico–United States border, and are physically separated from the rest of the city by the cities of National City and Chula Vista. A narrow strip of land at the bottom of San Diego Bay connects these southern neighborhoods with the rest of the city.
The development of skyscrapers over 300 feet (91 m) in San Diego is attributed to the construction of the El Cortez Hotel in 1927, the tallest building in the city from 1927 to 1963. As time went on multiple buildings claimed the title of San Diego's tallest skyscraper, including the Union Bank of California Building and Symphony Towers. Currently the tallest building in San Diego is One America Plaza, standing 500 feet (150 m) tall, which was completed in 1991. The downtown skyline contains no super-talls, as a regulation put in place by the Federal Aviation Administration in the 1970s set a 500 feet (152 m) limit on the height of buildings due to the proximity of San Diego International Airport. An iconic description of the skyline includes its skyscrapers being compared to the tools of a toolbox.
San Diego is one of the top-ten best climates in the Farmers' Almanac and is one of the two best summer climates in America as scored by The Weather Channel. Under the Köppen–Geiger climate classification system, the San Diego area has been variously categorized as having either a semi-arid climate (BSh in the original classification and BSkn in modified Köppen classification) or a Mediterranean climate (Csa and Csb). San Diego's climate is characterized by warm, dry summers and mild winters with most of the annual precipitation falling between December and March. The city has a mild climate year-round, with an average of 201 days above 70 °F (21 °C) and low rainfall (9–13 inches [230–330 mm] annually). Dewpoints in the summer months range from 57.0 °F (13.9 °C) to 62.4 °F (16.9 °C).
The climate in San Diego, like most of Southern California, often varies significantly over short geographical distances resulting in microclimates. In San Diego, this is mostly because of the city's topography (the Bay, and the numerous hills, mountains, and canyons). Frequently, particularly during the "May gray/June gloom" period, a thick "marine layer" cloud cover will keep the air cool and damp within a few miles of the coast, but will yield to bright cloudless sunshine approximately 5–10 miles (8.0–16.1 km) inland. Sometimes the June gloom can last into July, causing cloudy skies over most of San Diego for the entire day. Even in the absence of June gloom, inland areas tend to experience much more significant temperature variations than coastal areas, where the ocean serves as a moderating influence. Thus, for example, downtown San Diego averages January lows of 50 °F (10 °C) and August highs of 78 °F (26 °C). The city of El Cajon, just 10 miles (16 km) inland from downtown San Diego, averages January lows of 42 °F (6 °C) and August highs of 88 °F (31 °C).
Rainfall along the coast averages about 10 inches (250 mm) of precipitation annually. The average (mean) rainfall is 10.65 inches (271 mm) and the median is 9.6 inches (240 mm). Most of the rainfall occurs during the cooler months. The months of December through March supply most of the rain, with February the only month averaging 2 inches (51 mm) or more of rain. The months of May through September tend to be almost completely dry. Though there are few wet days per month during the rainy period, rainfall can be heavy when it does fall. Rainfall is usually greater in the higher elevations of San Diego; some of the higher elevation areas of San Diego can receive 11–15 inches (280–380 mm) of rain a year. Variability of rainfall can be extreme: in the wettest years of 1883/1884 and 1940/1941 more than 24 inches (610 mm) fell in the city, whilst in the driest years as little as 3.2 inches (80 mm) has fallen for a full year. The wettest month on record has been December 1921 with 9.21 inches (234 mm).
Like most of southern California, the majority of San Diego's current area was originally occupied by chaparral, a plant community made up mostly of drought-resistant shrubs. The endangered Torrey pine has the bulk of its population in San Diego in a stretch of protected chaparral along the coast. The steep and varied topography and proximity to the ocean create a number of different habitats within the city limits, including tidal marsh and canyons. The chaparral and coastal sage scrub habitats in low elevations along the coast are prone to wildfire, and the rates of fire have increased in the 20th century, due primarily to fires starting near the borders of urban and wild areas.
San Diego County has one of the highest counts of animal and plant species that appear on the endangered species list among counties in the United States. Because of its diversity of habitat and its position on the Pacific Flyway, San Diego County has recorded the presence of 492 bird species, more than any other region in the country. San Diego always scores very high in the number of bird species observed in the annual Christmas Bird Count, sponsored by the Audubon Society, and it is known as one of the "birdiest" areas in the United States.
San Diego and its backcountry are subject to periodic wildfires. In October 2003, San Diego was the site of the Cedar Fire, which has been called the largest wildfire in California over the past century. The fire burned 280,000 acres (1,100 km2), killed 15 people, and destroyed more than 2,200 homes. In addition to damage caused by the fire, smoke resulted in a significant increase in emergency room visits due to asthma, respiratory problems, eye irritation, and smoke inhalation; the poor air quality caused San Diego County schools to close for a week. Wildfires four years later destroyed some areas, particularly within the communities of Rancho Bernardo, Rancho Santa Fe, and Ramona.
The city had a population of 1,307,402 according to the 2010 census, distributed over a land area of 372.1 square miles (963.7 km2). The urban area of San Diego extends beyond the administrative city limits and had a total population of 2,956,746, making it the third-largest urban area in the state, after that of the Los Angeles metropolitan area and San Francisco metropolitan area. They, along with the Riverside–San Bernardino, form those metropolitan areas in California larger than the San Diego metropolitan area, with a total population of 3,095,313 at the 2010 census.
As of the Census of 2010, there were 1,307,402 people living in the city of San Diego. That represents a population increase of just under 7% from the 1,223,400 people, 450,691 households, and 271,315 families reported in 2000. The estimated city population in 2009 was 1,306,300. The population density was 3,771.9 people per square mile (1,456.4/km2). The racial makeup of San Diego was 45.1% White, 6.7% African American, 0.6% Native American, 15.9% Asian (5.9% Filipino, 2.7% Chinese, 2.5% Vietnamese, 1.3% Indian, 1.0% Korean, 0.7% Japanese, 0.4% Laotian, 0.3% Cambodian, 0.1% Thai). 0.5% Pacific Islander (0.2% Guamanian, 0.1% Samoan, 0.1% Native Hawaiian), 12.3% from other races, and 5.1% from two or more races. The ethnic makeup of the city was 28.8% Hispanic or Latino (of any race); 24.9% of the total population were Mexican American, and 0.6% were Puerto Rican.
As of January 1, 2008 estimates by the San Diego Association of Governments revealed that the household median income for San Diego rose to $66,715, up from $45,733, and that the city population rose to 1,336,865, up 9.3% from 2000. The population was 45.3% non-Hispanic whites, down from 78.9% in 1970, 27.7% Hispanics, 15.6% Asians/Pacific Islanders, 7.1% blacks, 0.4% American Indians, and 3.9% from other races. Median age of Hispanics was 27.5 years, compared to 35.1 years overall and 41.6 years among non-Hispanic whites; Hispanics were the largest group in all ages under 18, and non-Hispanic whites constituted 63.1% of population 55 and older.
The U.S. Census Bureau reported that in 2000, 24.0% of San Diego residents were under 18, and 10.5% were 65 and over. As of 2011[update] the median age was 35.6; more than a quarter of residents were under age 20 and 11% were over age 65. Millennials (ages 18 through 34) constitute 27.1% of San Diego's population, the second-highest percentage in a major U.S. city. The San Diego County regional planning agency, SANDAG, provides tables and graphs breaking down the city population into 5-year age groups.
In 2000, the median income for a household in the city was $45,733, and the median income for a family was $53,060. Males had a median income of $36,984 versus $31,076 for females. The per capita income for the city was $23,609. According to Forbes in 2005, San Diego was the fifth wealthiest U.S. city but about 10.6% of families and 14.6% of the population were below the poverty line, including 20.0% of those under age 18 and 7.6% of those age 65 or over. Nonetheless, San Diego was rated the fifth-best place to live in the United States in 2006 by Money magazine.
Tourism is a major industry owing to the city's climate, its beaches, and numerous tourist attractions such as Balboa Park, Belmont amusement park, San Diego Zoo, San Diego Zoo Safari Park, and SeaWorld San Diego. San Diego's Spanish and Mexican heritage is reflected in the many historic sites across the city, such as Mission San Diego de Alcala and Old Town San Diego State Historic Park. Also, the local craft brewing industry attracts an increasing number of visitors for "beer tours" and the annual San Diego Beer Week in November; San Diego has been called "America's Craft Beer Capital."
The city shares a 15-mile (24 km) border with Mexico that includes two border crossings. San Diego hosts the busiest international border crossing in the world, in the San Ysidro neighborhood at the San Ysidro Port of Entry. A second, primarily commercial border crossing operates in the Otay Mesa area; it is the largest commercial crossing on the California-Baja California border and handles the third-highest volume of trucks and dollar value of trade among all United States-Mexico land crossings.
San Diego hosts several major producers of wireless cellular technology. Qualcomm was founded and is headquartered in San Diego, and is one of the largest private-sector employers in San Diego. Other wireless industry manufacturers headquartered here include Nokia, LG Electronics, Kyocera International., Cricket Communications and Novatel Wireless. The largest software company in San Diego is security software company Websense Inc. San Diego also has the U.S. headquarters for the Slovakian security company ESET. San Diego has been designated as an iHub Innovation Center for collaboration potentially between wireless and life sciences.
The presence of the University of California, San Diego and other research institutions has helped to fuel biotechnology growth. In 2013, San Diego has the second-largest biotech cluster in the United States, below the Boston area and above the San Francisco Bay Area. There are more than 400 biotechnology companies in the area. In particular, the La Jolla and nearby Sorrento Valley areas are home to offices and research facilities for numerous biotechnology companies. Major biotechnology companies like Illumina and Neurocrine Biosciences are headquartered in San Diego, while many biotech and pharmaceutical companies have offices or research facilities in San Diego. San Diego is also home to more than 140 contract research organizations (CROs) that provide a variety of contract services for pharmaceutical and biotechnology companies.
Many popular museums, such as the San Diego Museum of Art, the San Diego Natural History Museum, the San Diego Museum of Man, the Museum of Photographic Arts, and the San Diego Air & Space Museum are located in Balboa Park, which is also the location of the San Diego Zoo. The Museum of Contemporary Art San Diego (MCASD) is located in La Jolla and has a branch located at the Santa Fe Depot downtown. The downtown branch consists of two building on two opposite streets. The Columbia district downtown is home to historic ship exhibits belonging to the San Diego Maritime Museum, headlined by the Star of India, as well as the unrelated San Diego Aircraft Carrier Museum featuring the USS Midway aircraft carrier.
The San Diego Symphony at Symphony Towers performs on a regular basis and is directed by Jahja Ling. The San Diego Opera at Civic Center Plaza, directed by Ian Campbell, was ranked by Opera America as one of the top 10 opera companies in the United States. Old Globe Theatre at Balboa Park produces about 15 plays and musicals annually. The La Jolla Playhouse at UCSD is directed by Christopher Ashley. Both the Old Globe Theatre and the La Jolla Playhouse have produced the world premieres of plays and musicals that have gone on to win Tony Awards or nominations on Broadway. The Joan B. Kroc Theatre at Kroc Center's Performing Arts Center is a 600-seat state-of-the-art theatre that hosts music, dance, and theatre performances. The San Diego Repertory Theatre at the Lyceum Theatres in Horton Plaza produces a variety of plays and musicals. Hundreds of movies and a dozen TV shows have been filmed in San Diego, a tradition going back as far as 1898.
The San Diego Surf of the American Basketball Association is located in the city. The annual Farmers Insurance Open golf tournament (formerly the Buick Invitational) on the PGA Tour occurs at Torrey Pines Golf Course. This course was also the site of the 2008 U.S. Open Golf Championship. The San Diego Yacht Club hosted the America's Cup yacht races three times during the period 1988 to 1995. The amateur beach sport Over-the-line was invented in San Diego, and the annual world Over-the-line championships are held at Mission Bay every year.
The city is governed by a mayor and a 9-member city council. In 2006, the city's form of government changed from a council–manager government to a strong mayor government. The change was brought about by a citywide vote in 2004. The mayor is in effect the chief executive officer of the city, while the council is the legislative body. The City of San Diego is responsible for police, public safety, streets, water and sewer service, planning and zoning, and similar services within its borders. San Diego is a sanctuary city, however, San Diego County is a participant of the Secure Communities program. As of 2011[update], the city had one employee for every 137 residents, with a payroll greater than $733 million.
The members of the city council are each elected from single member districts within the city. The mayor and city attorney are elected directly by the voters of the entire city. The mayor, city attorney, and council members are elected to four-year terms, with a two-term limit. Elections are held on a non-partisan basis per California state law; nevertheless, most officeholders do identify themselves as either Democrats or Republicans. In 2007, registered Democrats outnumbered Republicans by about 7 to 6 in the city, and Democrats currently (as of 2015[update]) hold a 5-4 majority in the city council. The current mayor, Kevin Faulconer, is a Republican.
In 2005 two city council members, Ralph Inzunza and Deputy Mayor Michael Zucchet – who briefly took over as acting mayor when Murphy resigned – were convicted of extortion, wire fraud, and conspiracy to commit wire fraud for taking campaign contributions from a strip club owner and his associates, allegedly in exchange for trying to repeal the city's "no touch" laws at strip clubs. Both subsequently resigned. Inzunza was sentenced to 21 months in prison. In 2009, a judge acquitted Zucchet on seven out of the nine counts against him, and granted his petition for a new trial on the other two charges; the remaining charges were eventually dropped.
In July 2013, three former supporters of Mayor Bob Filner asked him to resign because of allegations of repeated sexual harassment. Over the ensuing six weeks, 18 women came forward to publicly claim that Filner had sexually harassed them, and multiple individuals and groups called for him to resign. On August 19 Filner and city representatives entered a mediation process, as a result of which Filner agreed to resign, effective August 30, 2013, while the city agreed to limit his legal and financial exposure. Filner subsequently pleaded guilty to one felony count of false imprisonment and two misdemeanor battery charges, and was sentenced to house arrest and probation.
San Diego was ranked as the 20th-safest city in America in 2013 by Business Insider. According to Forbes magazine, San Diego was the ninth-safest city in the top 10 list of safest cities in the U.S. in 2010. Like most major cities, San Diego had a declining crime rate from 1990 to 2000. Crime in San Diego increased in the early 2000s. In 2004, San Diego had the sixth lowest crime rate of any U.S. city with over half a million residents. From 2002 to 2006, the crime rate overall dropped 0.8%, though not evenly by category. While violent crime decreased 12.4% during this period, property crime increased 1.1%. Total property crimes per 100,000 people were lower than the national average in 2008.
San Diego's first television station was KFMB, which began broadcasting on May 16, 1949. Since the Federal Communications Commission (FCC) licensed seven television stations in Los Angeles, two VHF channels were available for San Diego because of its relative proximity to the larger city. In 1952, however, the FCC began licensing UHF channels, making it possible for cities such as San Diego to acquire more stations. Stations based in Mexico (with ITU prefixes of XE and XH) also serve the San Diego market. Television stations today include XHTJB 3 (Once TV), XETV 6 (CW), KFMB 8 (CBS), KGTV 10 (ABC), XEWT 12 (Televisa Regional), KPBS 15 (PBS), KBNT-CD 17 (Univision), XHTIT-TDT 21 (Azteca 7), XHJK-TDT 27 (Azteca 13), XHAS 33 (Telemundo), K35DG-D 35 (UCSD-TV), KDTF-LD 51 (Telefutura), KNSD 39 (NBC), KZSD-LP 41 (Azteca America), KSEX-CD 42 (Infomercials), XHBJ-TDT 45 (Gala TV), XHDTV 49 (MNTV), KUSI 51 (Independent), XHUAA-TDT 57 (Canal de las Estrellas), and KSWB-TV 69 (Fox). San Diego has an 80.6 percent cable penetration rate.
Due to the ratio of U.S. and Mexican-licensed stations, San Diego is the largest media market in the United States that is legally unable to support a television station duopoly between two full-power stations under FCC regulations, which disallow duopolies in metropolitan areas with fewer than nine full-power television stations and require that there must be eight unique station owners that remain once a duopoly is formed (there are only seven full-power stations on the California side of the San Diego-Tijuana market).[citation needed] Though the E. W. Scripps Company owns KGTV and KZSD-LP, they are not considered a duopoly under the FCC's legal definition as common ownership between full-power and low-power television stations in the same market is permitted regardless to the number of stations licensed to the area. As a whole, the Mexico side of the San Diego-Tijuana market has two duopolies and one triopoly (Entravision Communications owns both XHAS-TV and XHDTV-TV, Azteca owns XHJK-TV and XHTIT-TV, and Grupo Televisa owns XHUAA-TV and XHWT-TV along with being the license holder for XETV-TV, which is run by California-based subsidiary Bay City Television).
The radio stations in San Diego include nationwide broadcaster, Clear Channel Communications; CBS Radio, Midwest Television, Lincoln Financial Media, Finest City Broadcasting, and many other smaller stations and networks. Stations include: KOGO AM 600, KFMB AM 760, KCEO AM 1000, KCBQ AM 1170, K-Praise, KLSD AM 1360 Air America, KFSD 1450 AM, KPBS-FM 89.5, Channel 933, Star 94.1, FM 94/9, FM News and Talk 95.7, Q96 96.1, KyXy 96.5, Free Radio San Diego (AKA Pirate Radio San Diego) 96.9FM FRSD, KSON 97.3/92.1, KXSN 98.1, Jack-FM 100.7, 101.5 KGB-FM, KLVJ 102.1, Rock 105.3, and another Pirate Radio station at 106.9FM, as well as a number of local Spanish-language radio stations.
With the automobile being the primary means of transportation for over 80 percent of its residents, San Diego is served by a network of freeways and highways. This includes Interstate 5, which runs south to Tijuana and north to Los Angeles; Interstate 8, which runs east to Imperial County and the Arizona Sun Corridor; Interstate 15, which runs northeast through the Inland Empire to Las Vegas and Salt Lake City; and Interstate 805, which splits from I-5 near the Mexican border and rejoins I-5 at Sorrento Valley.
Major state highways include SR 94, which connects downtown with I-805, I-15 and East County; SR 163, which connects downtown with the northeast part of the city, intersects I-805 and merges with I-15 at Miramar; SR 52, which connects La Jolla with East County through Santee and SR 125; SR 56, which connects I-5 with I-15 through Carmel Valley and Rancho Peñasquitos; SR 75, which spans San Diego Bay as the San Diego-Coronado Bridge, and also passes through South San Diego as Palm Avenue; and SR 905, which connects I-5 and I-805 to the Otay Mesa Port of Entry.
San Diego's roadway system provides an extensive network of routes for travel by bicycle. The dry and mild climate of San Diego makes cycling a convenient and pleasant year-round option. At the same time, the city's hilly, canyon-like terrain and significantly long average trip distances—brought about by strict low-density zoning laws—somewhat restrict cycling for utilitarian purposes. Older and denser neighborhoods around the downtown tend to be utility cycling oriented. This is partly because of the grid street patterns now absent in newer developments farther from the urban core, where suburban style arterial roads are much more common. As a result, a vast majority of cycling-related activities are recreational. Testament to San Diego's cycling efforts, in 2006, San Diego was rated as the best city for cycling for U.S. cities with a population over 1 million.
San Diego is served by the San Diego Trolley light rail system, by the SDMTS bus system, and by Coaster and Amtrak Pacific Surfliner commuter rail; northern San Diego county is also served by the Sprinter light rail line. The Trolley primarily serves downtown and surrounding urban communities, Mission Valley, east county, and coastal south bay. A planned Mid-Coast extension of the Trolley will operate from Old Town to University City and the University of California, San Diego along the I-5 Freeway, with planned operation by 2018. The Amtrak and Coaster trains currently run along the coastline and connect San Diego with Los Angeles, Orange County, Riverside, San Bernardino, and Ventura via Metrolink and the Pacific Surfliner. There are two Amtrak stations in San Diego, in Old Town and the Santa Fe Depot downtown. San Diego transit information about public transportation and commuting is available on the Web and by dialing "511" from any phone in the area.
The city's primary commercial airport is the San Diego International Airport (SAN), also known as Lindbergh Field. It is the busiest single-runway airport in the United States. It served over 17 million passengers in 2005, and is dealing with an increasingly larger number every year. It is located on San Diego Bay three miles (4.8 km) from downtown. San Diego International Airport maintains scheduled flights to the rest of the United States including Hawaii, as well as to Mexico, Canada, Japan, and the United Kingdom. It is operated by an independent agency, the San Diego Regional Airport Authority. In addition, the city itself operates two general-aviation airports, Montgomery Field (MYF) and Brown Field (SDM). By 2015, the Tijuana Cross-border Terminal in Otay Mesa will give direct access to Tijuana International Airport, with passengers walking across the U.S.–Mexico border on a footbridge to catch their flight on the Mexican side.
Numerous regional transportation projects have occurred in recent years to mitigate congestion in San Diego. Notable efforts are improvements to San Diego freeways, expansion of San Diego Airport, and doubling the capacity of the cruise ship terminal of the port. Freeway projects included expansion of Interstates 5 and 805 around "The Merge," a rush-hour spot where the two freeways meet. Also, an expansion of Interstate 15 through the North County is underway with the addition of high-occupancy-vehicle (HOV) "managed lanes". There is a tollway (The South Bay Expressway) connecting SR 54 and Otay Mesa, near the Mexican border. According to a 2007 assessment, 37 percent of streets in San Diego were in acceptable driving condition. The proposed budget fell $84.6 million short of bringing the city's streets to an acceptable level. Port expansions included a second cruise terminal on Broadway Pier which opened in 2010. Airport projects include expansion of Terminal 2, currently under construction and slated for completion in summer 2013.
Black people is a term used in certain countries, often in socially based systems of racial classification or of ethnicity, to describe persons who are perceived to be dark-skinned compared to other given populations. As such, the meaning of the expression varies widely both between and within societies, and depends significantly on context. For many other individuals, communities and countries, "black" is also perceived as a derogatory, outdated, reductive or otherwise unrepresentative label, and as a result is neither used nor defined.
Different societies apply differing criteria regarding who is classified as "black", and these social constructs have also changed over time. In a number of countries, societal variables affect classification as much as skin color, and the social criteria for "blackness" vary. For example, in North America the term black people is not necessarily an indicator of skin color or majority ethnic ancestry, but it is instead a socially based racial classification related to being African American, with a family history associated with institutionalized slavery. In South Africa and Latin America, for instance, mixed-race people are generally not classified as "black." In South Pacific regions such as Australia and Melanesia, European colonists applied the term "black" or it was used by populations with different histories and ethnic origin.
The Romans interacted with and later conquered parts of Mauretania, an early state that covered modern Morocco, western Algeria, and the Spanish cities Ceuta and Melilla during the classical period. The people of the region were noted in Classical literature as Mauri, which was subsequently rendered as Moors in English.
Numerous communities of dark-skinned peoples are present in North Africa, some dating from prehistoric communities. Others are descendants of the historical Trans-Saharan trade in peoples and/or, and after the Arab invasions of North Africa in the 7th century, descendants of slaves from the Arab Slave Trade in North Africa.
In the 18th century, the Moroccan Sultan Moulay Ismail "the Bloodthirsty" (1672–1727) raised a corps of 150,000 black slaves, called his Black Guard, who coerced the country into submission.
According to Dr. Carlos Moore, resident scholar at Brazil's University of the State of Bahia, in the 21st century Afro-multiracials in the Arab world, including Arabs in North Africa, self-identify in ways that resemble multi-racials in Latin America. He claims that black-looking Arabs, much like black-looking Latin Americans, consider themselves white because they have some distant white ancestry.
Egyptian President Anwar Sadat had a mother who was a dark-skinned Nubian Sudanese woman and a father who was a lighter-skinned Egyptian. In response to an advertisement for an acting position, as a young man he said, "I am not white but I am not exactly black either. My blackness is tending to reddish".
Due to the patriarchal nature of Arab society, Arab men, including during the slave trade in North Africa, enslaved more black women than men. They used more black female slaves in domestic service and agriculture than males. The men interpreted the Qur'an to permit sexual relations between a male master and his female slave outside of marriage (see Ma malakat aymanukum and sex), leading to many mixed-race children. When an enslaved woman became pregnant with her Arab master's child, she was considered as umm walad or "mother of a child", a status that granted her privileged rights. The child was given rights of inheritance to the father's property, so mixed-race children could share in any wealth of the father. Because the society was patrilineal, the children took their fathers' social status at birth and were born free.
Some succeeded their fathers as rulers, such as Sultan Ahmad al-Mansur, who ruled Morocco from 1578 to 1608. He was not technically considered as a mixed-race child of a slave; his mother was Fulani and a concubine of his father. Such tolerance for black persons, even when technically "free", was not so common in Morocco. The long association of sub-Saharan peoples as slaves is shown in the term abd (Arabic: عبد‎,) (meaning "slave"); it is still frequently used in the Arabic-speaking world as a term for black people.
In early 1991, non-Arabs of the Zaghawa tribe of Sudan attested that they were victims of an intensifying Arab apartheid campaign, segregating Arabs and non-Arabs (specifically people of sub-Saharan African descent). Sudanese Arabs, who controlled the government, were widely referred to as practicing apartheid against Sudan's non-Arab citizens. The government was accused of "deftly manipulat(ing) Arab solidarity" to carry out policies of apartheid and ethnic cleansing.
American University economist George Ayittey accused the Arab government of Sudan of practicing acts of racism against black citizens. According to Ayittey, "In Sudan... the Arabs monopolized power and excluded blacks – Arab apartheid." Many African commentators joined Ayittey in accusing Sudan of practising Arab apartheid.
Alan Dershowitz described Sudan as an example of a government that "actually deserve(s)" the appellation "apartheid." Former Canadian Minister of Justice Irwin Cotler echoed the accusation.
In South Africa, the period of colonization resulted in many unions and marriages between European men and African women from various tribes, resulting in mixed-race children. As the Europeans acquired territory and imposed rule over the Africans, they generally pushed mixed-race and Africans into second-class status. During the first half of the 20th century, the Afrikaaner-dominated government classified the population according to four main racial groups: Black, White, Asian (mostly Indian), and Coloured. The Coloured group included people of mixed Bantu, Khoisan, and European descent (with some Malay ancestry, especially in the Western Cape). The Coloured definition occupied an intermediary political position between the Black and White definitions in South Africa. It imposed a system of legal racial segregation, a complex of laws known as apartheid.
The apartheid bureaucracy devised complex (and often arbitrary) criteria in the Population Registration Act of 1945 to determine who belonged in which group. Minor officials administered tests to enforce the classifications. When it was unclear from a person's physical appearance whether the individual should be considered Coloured or Black, the "pencil test" was used. A pencil was inserted into a person's hair to determine if the hair was kinky enough to hold the pencil, rather than having it pass through, as it would with smoother hair. If so, the person was classified as Black. Such classifications sometimes divided families.
Sandra Laing is a South African woman who was classified as Coloured by authorities during the apartheid era, due to her skin colour and hair texture, although her parents could prove at least three generations of European ancestors. At age 10, she was expelled from her all-white school. The officials' decisions based on her anomalous appearance disrupted her family and adult life. She was the subject of the 2008 biographical dramatic film Skin, which won numerous awards.
During the apartheid era, those classed as "Coloured" were oppressed and discriminated against. But, they had limited rights and overall had slightly better socioeconomic conditions than those classed as "Black". The government required that Blacks and Coloureds live in areas separate from Whites, creating large townships located away from the cities as areas for Blacks.
In the post-apartheid era, the Constitution of South Africa has declared the country to be a "Non-racial democracy". In an effort to redress past injustices, the ANC government has introduced laws in support of affirmative action policies for Blacks; under these they define "Black" people to include "Africans", "Coloureds" and "Asians". Some affirmative action policies favor "Africans" over "Coloureds" in terms of qualifying for certain benefits. Some South Africans categorized as "African Black" say that "Coloureds" did not suffer as much as they did during apartheid. "Coloured" South Africans are known to discuss their dilemma by saying, "we were not white enough under apartheid, and we are not black enough under the ANC (African National Congress)".[citation needed]
In 2008, the High Court in South Africa ruled that Chinese South Africans who were residents during the apartheid era (and their descendants) are to be reclassified as "Black people," solely for the purposes of accessing affirmative action benefits, because they were also "disadvantaged" by racial discrimination. Chinese people who arrived in the country after the end of apartheid do not qualify for such benefits.
Other than by appearance, "Coloureds" can usually be distinguished from "Blacks" by language. Most speak Afrikaans or English as a first language, as opposed to Bantu languages such as Zulu or Xhosa. They also tend to have more European-sounding names than Bantu names.
Historians estimate that between the advent of Islam in 650CE and the abolition of slavery in the Arabian Peninsula in the mid-20th century, 10 to 18 million sub-Saharan Black Africans were enslaved by Arab slave traders and transported to the Arabian Peninsula and neighboring countries. This number far exceeded the number of slaves who were taken to the Americas. Several factors affected the visibility of descendants of this diaspora in 21st-century Arab societies: The traders shipped more female slaves than males, as there was a demand for them to serve as concubines in harems in the Arabian Peninsula and neighboring countries. Male slaves were castrated in order to serve as harem guards. The death toll of Black African slaves from forced labor was high. The mixed-race children of female slaves and Arab owners were assimilated into the Arab owners' families under the patrilineal kinship system. As a result, few distinctive Afro-Arab black communities have survived in the Arabian Peninsula and neighboring countries.
Genetic studies have found significant African female-mediated gene flow in Arab communities in the Arabian Peninsula and neighboring countries, with an average of 38% of maternal lineages in Yemen are of direct African descent, 16% in Oman-Qatar, and 10% in Saudi Arabia-United Arab Emirates.
Distinctive and self-identified black communities have been reported in countries such as Iraq, with a reported 1.2 million black people, and they attest to a history of discrimination. African-Iraquis have sought minority status from the government, which would reserve some seats in Parliament for representatives of their population. According to Alamin M. Mazrui et al., generally in the Arabian Peninsula and neighboring countries, most of those of visible African descent are still classified and identify as Arab, not black.
About 150,000 East African and black people live in Israel, amounting to just over 2% of the nation's population. The vast majority of these, some 120,000, are Beta Israel, most of whom are recent immigrants who came during the 1980s and 1990s from Ethiopia. In addition, Israel is home to over 5,000 members of the African Hebrew Israelites of Jerusalem movement that are descendants of African Americans who emigrated to Israel in the 20th century, and who reside mainly in a distinct neighborhood in the Negev town of Dimona. Unknown numbers of black converts to Judaism reside in Israel, most of them converts from the United Kingdom, Canada, and the United States.
Additionally, there are around 60,000 non-Jewish African immigrants in Israel, some of whom have sought asylum. Most of the migrants are from communities in Sudan and Eritrea, particularly the Niger-Congo-speaking Nuba groups of the southern Nuba Mountains; some are illegal immigrants.
Beginning several centuries ago, during the period of the Ottoman Empire, tens of thousands of Black Africans were brought by slave traders to plantations and agricultural areas situated between Antalya and Istanbul in present-day Turkey. Some of their descendants remained in situ, and many migrated to larger cities and towns. Other blacks slaves were transported to Crete, from where they or their descendants later reached the İzmir area through the population exchange between Greece and Turkey in 1923, or indirectly from Ayvalık in pursuit of work.
The Siddi are an ethnic group inhabiting India and Pakistan whose members are descended from Bantu peoples from Southeast Africa that were brought to the Indian subcontinent as slaves by Arab and Portuguese merchants. Although it is commonly believed locally that "Siddi" derives from a word meaning "black", the term is actually derived from "Sayyid", the title borne by the captains of the Arab vessels that first brought Siddi settlers to the area. In the Makran strip of the Sindh and Balochistan provinces in southwestern Pakistan, these Bantu descendants are known as the Makrani. There was a brief "Black Power" movement in Sindh in the 1960s and many Siddi are proud of and celebrate their African ancestry.
The Negritos are believed to be the first inhabitants of Southeast Asia. Once inhabiting Taiwan, Vietnam, and various other parts of Asia, they are now confined primarily to Thailand, the Malay Archipelago, and the Andaman and Nicobar Islands. Negrito means "little black people" in Spanish (negrito is the Spanish diminutive of negro, i.e., "little black person"); it is what the Spaniards called the short-statured, hunter-gatherer autochthones that they encountered in the Philippines. Despite this, Negritos are never referred to as black today, and doing so would cause offense. The term Negrito itself has come under criticism in countries like Malaysia, where it is now interchangeable with the more acceptable Semang, although this term actually refers to a specific group. The common Thai word for Negritos literally means "frizzy hair".
The term "Moors" has been used in Europe in a broader, somewhat derogatory sense to refer to Muslims, especially those of Arab or Berber descent, whether living in North Africa or Iberia. Moors were not a distinct or self-defined people. Medieval and early modern Europeans applied the name to Muslim Arabs, Berbers, Black Africans and Europeans alike.
Isidore of Seville, writing in the 7th century, claimed that the Latin word Maurus was derived from the Greek mauron, μαύρον, which is the Greek word for black. Indeed, by the time Isidore of Seville came to write his Etymologies, the word Maurus or "Moor" had become an adjective in Latin, "for the Greeks call black, mauron". "In Isidore’s day, Moors were black by definition…"
Afro-Spaniards are Spanish nationals of West/Central African descent. They today mainly come from Angola, Brazil, Cameroon, Cape Verde, Equatorial Guinea, Ghana, Gambia, Guinea-Bissau, Mali, Nigeria and Senegal. Additionally, many Afro-Spaniards born in Spain are from the former Spanish colony Equatorial Guinea. Today, there are an estimated 683,000 Afro-Spaniards in Spain.
According to the Office for National Statistics, at the 2001 census there were over a million black people in the United Kingdom; 1% of the total population described themselves as "Black Caribbean", 0.8% as "Black African", and 0.2% as "Black other". Britain encouraged the immigration of workers from the Caribbean after World War II; the first symbolic movement was those who came on the ship the Empire Windrush. The preferred official umbrella term is "black and minority ethnic" (BME), but sometimes the term "black" is used on its own, to express unified opposition to racism, as in the Southall Black Sisters, which started with a mainly British Asian constituency, and the National Black Police Association, which has a membership of "African, African-Caribbean and Asian origin".
As African states became independent in the 1960s, the Soviet Union offered many of their citizens the chance to study in Russia. Over a period of 40 years, about 400,000 African students from various countries moved to Russia to pursue higher studies, including many Black Africans. This extended beyond the Soviet Union to many countries of the Eastern bloc.
Due to the Ottoman slave trade that had flourished in the Balkans, the coastal town of Ulcinj in Montenegro had its own black community. As a consequence of the slave trade and privateer activity, it is told how until 1878 in Ulcinj 100 black people lived. The Ottoman Army also deployed an estimated 30,000 Black African troops and cavalrymen to its expedition in Hungary during the Austro-Turkish War of 1716–18.
Indigenous Australians have been referred to as "black people" in Australia since the early days of European settlement. While originally related to skin colour, the term is used to today to indicate Aboriginal or Torres Strait Islander ancestry in general and can refer to people of any skin pigmentation.
Being identified as either "black" or "white" in Australia during the 19th and early 20th centuries was critical in one's employment and social prospects. Various state-based Aboriginal Protection Boards were established which had virtually complete control over the lives of Indigenous Australians – where they lived, their employment, marriage, education and included the power to separate children from their parents. Aborigines were not allowed to vote and were often confined to reserves and forced into low paid or effectively slave labour. The social position of mixed-race or "half-caste" individuals varied over time. A 1913 report by Sir Baldwin Spencer states that:
After the First World War, however, it became apparent that the number of mixed-race people was growing at a faster rate than the white population, and by 1930 fear of the "half-caste menace" undermining the White Australia ideal from within was being taken as a serious concern. Dr. Cecil Cook, the Northern Territory Protector of Natives, noted that:
The official policy became one of biological and cultural assimilation: "Eliminate the full-blood and permit the white admixture to half-castes and eventually the race will become white". This led to different treatment for "black" and "half-caste" individuals, with lighter-skinned individuals targeted for removal from their families to be raised as "white" people, restricted from speaking their native language and practising traditional customs, a process now known as the Stolen Generation.
The second half of the 20th century to the present has seen a gradual shift towards improved human rights for Aboriginal people. In a 1967 referendum over 90% of the Australian population voted to end constitutional discrimination and to include Aborigines in the national census. During this period many Aboriginal activists began to embrace the term "black" and use their ancestry as a source of pride. Activist Bob Maza said:
In 1978 Aboriginal writer Kevin Gilbert received the National Book Council award for his book Living Black: Blacks Talk to Kevin Gilbert, a collection of Aboriginal people's stories, and in 1998 was awarded (but refused to accept) the Human Rights Award for Literature for Inside Black Australia, a poetry anthology and exhibition of Aboriginal photography. In contrast to previous definitions based solely on the degree of Aboriginal ancestry, in 1990 the Government changed the legal definition of Aboriginal to include any:
This nationwide acceptance and recognition of Aboriginal people led to a significant increase in the number of people self-identifying as Aboriginal or Torres Strait Islander. The reappropriation of the term "black" with a positive and more inclusive meaning has resulted in its widespread use in mainstream Australian culture, including public media outlets, government agencies, and private companies. In 2012, a number of high-profile cases highlighted the legal and community attitude that identifying as Aboriginal or Torres Strait Islander is not dependent on skin colour, with a well-known boxer Anthony Mundine being widely criticised for questioning the "blackness" of another boxer and journalist Andrew Bolt being successfully sued for publishing discriminatory comments about Aboriginals with light skin.
In the Colonial America of 1619, John Rolfe used negars in describing the slaves who were captured from West Africa and then shipped to the Virginia colony. Later American English spellings, neger and neggar, prevailed in a northern colony, New York under the Dutch, and in metropolitan Philadelphia's Moravian and Pennsylvania Dutch communities; the African Burial Ground in New York City originally was known by the Dutch name "Begraafplaats van de Neger" (Cemetery of the Negro); an early US occurrence of neger in Rhode Island, dates from 1625. Thomas Jefferson also used the term "black" in his Notes on the State of Virginia in allusion to the slave populations.
By the 1900s, nigger had become a pejorative word in the United States. In its stead, the term colored became the mainstream alternative to negro and its derived terms. After the African-American Civil rights movement, the terms colored and negro gave way to "black". Negro had superseded colored as the most polite word for African Americans at a time when black was considered more offensive. This term was accepted as normal, including by people classified as Negroes, until the later Civil Rights movement in the late 1960s. One well-known example is the identification by Reverend Martin Luther King, Jr. of his own race as "Negro" in his famous speech of 1963, I Have a Dream. During the American Civil Rights movement of the 1950s and 1960s, some African-American leaders in the United States, notably Malcolm X, objected to the word Negro because they associated it with the long history of slavery, segregation, and discrimination that treated African Americans as second-class citizens, or worse. Malcolm X preferred Black to Negro, but later gradually abandoned that as well for Afro-American after leaving the Nation of Islam.
In the first 200 years that black people were in the United States, they primarily identified themselves by their specific ethnic group (closely allied to language) and not by skin color. Individuals identified themselves, for example, as Ashanti, Igbo, Bakongo, or Wolof. However, when the first captives were brought to the Americas, they were often combined with other groups from West Africa, and individual ethnic affiliations were not generally acknowledged by English colonists. In areas of the Upper South, different ethnic groups were brought together. This is significant as the captives came from a vast geographic region: the West African coastline stretching from Senegal to Angola and in some cases from the south-east coast such as Mozambique. A new African-American identity and culture was born that incorporated elements of the various ethnic groups and of European cultural heritage, resulting in fusions such as the Black church and Black English. This new identity was based on provenance and slave status rather than membership in any one ethnic group.[citation needed] By contrast, slave records from Louisiana show that the French and Spanish colonists recorded more complete identities of the West Africans, including ethnicities and given tribal names.
The US racial or ethnic classification "black" refers to people with all possible kinds of skin pigmentation, from the darkest through to the very lightest skin colors, including albinos, if they are believed by others to have West African ancestry (in any discernible percentage), or to exhibit cultural traits associated with being "African American". As a result, in the United States the term "black people" is not an indicator of skin color or ethnic origin but is instead a socially based racial classification related to being African American, with a family history associated with institutionalized slavery. Relatively dark-skinned people can be classified as white if they fulfill other social criteria of "whiteness", and relatively light-skinned people can be classified as black if they fulfill the social criteria for "blackness" in a particular setting.
By that time, the majority of black people in the United States were native-born, so the use of the term "African" became problematic. Though initially a source of pride, many blacks feared that the use of African as an identity would be a hindrance to their fight for full citizenship in the US. They also felt that it would give ammunition to those who were advocating repatriating black people back to Africa. In 1835, black leaders called upon Black Americans to remove the title of "African" from their institutions and replace it with "Negro" or "Colored American". A few institutions chose to keep their historic names, such as the African Methodist Episcopal Church. African Americans popularly used the terms "Negro" or "colored" for themselves until the late 1960s.
In 1988, the civil rights leader Jesse Jackson urged Americans to use instead the term "African American" because it had a historical cultural base and was a construction similar to terms used by European descendants, such as German American, Italian American, etc. Since then, African American and black have often had parallel status. However, controversy continues over which if any of the two terms is more appropriate. Maulana Karenga argues that the term African-American is more appropriate because it accurately articulates their geographical and historical origin.[citation needed] Others have argued that "black" is a better term because "African" suggests foreignness, although Black Americans helped found the United States. Still others believe that the term black is inaccurate because African Americans have a variety of skin tones. Some surveys suggest that the majority of Black Americans have no preference for "African American" or "Black", although they have a slight preference for "black" in personal settings and "African American" in more formal settings.
The U.S. census race definitions says a "black" is a person having origins in any of the black (sub-Saharan) racial groups of Africa. It includes people who indicate their race as "Black, African Am., or Negro" or who provide written entries such as African American, Afro-American, Kenyan, Nigerian, or Haitian. The Census Bureau notes that these classifications are socio-political constructs and should not be interpreted as scientific or anthropological. Most African Americans also have European ancestry in varying amounts; a lesser proportion have some Native American ancestry. For instance, genetic studies of African Americans show an ancestry that is on average 17–18% European.
From the late 19th century, the South used a colloquial term, the one-drop rule, to classify as black a person of any known African ancestry. This practice of hypodescent was not put into law until the early 20th century. Legally the definition varied from state to state. Racial definition was more flexible in the 18th and 19th centuries before the American Civil War. For instance, President Thomas Jefferson held persons who were legally white (less than 25% black) according to Virginia law at the time, but, because they were born to slave mothers, they were born into slavery, according to the principle of partus sequitur ventrem, which Virginia adopted into law in 1662.
The concept of blackness in the United States has been described as the degree to which one associates themselves with mainstream African-American culture, politics, and values. To a certain extent, this concept is not so much about race but more about political orientation, culture and behavior. Blackness can be contrasted with "acting white", where black Americans are said to behave with assumed characteristics of stereotypical white Americans with regard to fashion, dialect, taste in music, and possibly, from the perspective of a significant number of black youth, academic achievement.
Due to the often political and cultural contours of blackness in the United States, the notion of blackness can also be extended to non-black people. Toni Morrison once described Bill Clinton as the first black President of the United States, because, as she put it, he displayed "almost every trope of blackness". Christopher Hitchens was offended by the notion of Clinton as the first black president, noting, "Mr Clinton, according to Toni Morrison, the Nobel Prize-winning novelist, is our first black President, the first to come from the broken home, the alcoholic mother, the under-the-bridge shadows of our ranking systems. Thus, we may have lost the mystical power to divine diabolism, but we can still divine blackness by the following symptoms: broken homes, alcoholic mothers, under-the-bridge habits and (presumable from the rest of [Arthur] Miller's senescent musings) the tendency to sexual predation and to shameless perjury about same." Some black activists were also offended, claiming that Clinton used his knowledge of black culture to exploit black people for political gain as no other president had before, while not serving black interests. They cite the lack of action during the Rwandan Genocide and his welfare reform, which Larry Roberts said had led to the worst child poverty since the 1960s. Others cited that the number of black people in jail increased during his administration.
In July 2012, Ancestry.com reported on historic and DNA research by its staff that discovered that Obama is likely a descendant through his mother of John Punch, considered by some historians to be the first African slave in the Virginia colony. An indentured servant, he was "bound for life" in 1640 after trying to escape. The story of him and his descendants is that of multi-racial America since it appeared he and his sons married or had unions with white women, likely indentured servants and working-class like them. Their multi-racial children were free because they were born to free English women. Over time, Obama's line of the Bunch family (as they became known) were property owners and continued to "marry white"; they became part of white society, likely by the early to mid-18th century.
Approximately 12 million Africans were shipped to the Americas during the Atlantic slave trade from 1492 to 1888, with 11.5 million of those shipped to South America and the Caribbean. Brazil was the largest importer in the Americas, with 5.5 million African slaves imported, followed by the British Caribbean with 2.76 million, the Spanish Caribbean and Spanish Mainland with 1.59 million Africans, and the French Caribbean with 1.32 million. Today their descendants number approximately 150 million in South America and the Caribbean. In addition to skin color, other physical characteristics such as facial features and hair texture are often variously used in classifying peoples as black in South America and the Caribbean. In South America and the Caribbean, classification as black is also closely tied to social status and socioeconomic variables, especially in light of social conceptions of "blanqueamiento" (racial whitening) and related concepts.
The concept of race in Brazil is complex. A Brazilian child was never automatically identified with the racial type of one or both of his or her parents, nor were there only two categories to choose from. Between an individual of unmixed West African descent and a very light mulatto individual, more than a dozen racial categories were acknowledged, based on various combinations of hair color, hair texture, eye color, and skin color. These types grade into each other like the colors of the spectrum, and no one category stands significantly isolated from the rest. In Brazil, people are classified by appearance, not heredity.
Scholars disagree over the effects of social status on racial classifications in Brazil. It is generally believed that achieving upward mobility and education results in individuals being classified as a category of lighter skin. The popular claim is that in Brazil, poor whites are considered black and wealthy blacks are considered white. Some scholars disagree, arguing that "whitening" of one's social status may be open to people of mixed race, a large part of the population known as pardo, but a person perceived as preto (black) will continue to be classified as black regardless of wealth or social status.
From the years 1500 to 1850, an estimated 3.5 million captives were forcibly shipped from West/Central Africa to Brazil; the territory received the highest number of slaves of any country in the Americas. Scholars estimate that more than half of the Brazilian population is at least in part descended from these individuals. Brazil has the largest population of Afro-descendants outside of Africa. In contrast to the US, during the slavery period and after, the Portuguese colonial government and later Brazilian government did not pass formal anti-miscegenation or segregation laws. As in other Latin countries, intermarriage was prevalent during the colonial period and continued afterward. In addition, people of mixed race (pardo) often tended to marry white, and their descendants became accepted as white. As a result, some of the European descended population also has West African or Amerindian blood. According to the last census of the 20th century, in which Brazilians could choose from five color/ethnic categories with which they identified, 54% of individuals identified as white, 6.2% identified as black, and 39.5% identified as pardo (brown) — a broad multi-racial category, including tri-racial persons.
By the 2000 census, demographic changes including the end to slavery, immigration from Europe and Asia, assimilation of multiracial persons, and other factors resulted in a population in which 6.2% of the population identified as black, 40% as pardo, and 55% as white. Essentially most of the black population was absorbed into the multi-racial category by intermixing. A 2007 genetic study found that at least 29% of the middle-class, white Brazilian population had some recent (since 1822 and the end of the colonial period) African ancestry.
Because of the acceptance of miscegenation, Brazil has avoided the binary polarization of society into black and white. In addition, it abolished slavery without a civil war. The bitter and sometimes violent racial tensions that have divided the US are notably absent in Brazil. According to the 2010 census, 6.7% of Brazilians said they were black, compared with 6.2% in 2000, and 43.1% said they were racially mixed, up from 38.5%. In 2010, Elio Ferreira de Araujo, Brazil's minister for racial equality, attributed the increases to growing pride among his country's black and indigenous communities.
In the US, African Americans, who include multiracial people, earn 75% of what white people earn. In Brazil, people of color earn less than 50% of what whites earn. Some have posited that the facts of lower socioeconomic status for people of color suggest that Brazil practices a kind of one-drop rule, or discrimination against people who are not visibly European in ancestry. The gap in income between blacks and other non-whites is relatively small compared to the large gap between whites and all people of color. Other social factors, such as illiteracy and education levels, show the same patterns of disadvantage for people of color. Some commentators observe that the United States practice of segregation and white supremacy in the South, and discrimination in many areas outside that region, forced many African Americans to unite in the civil rights struggle. They suggest that the fluid nature of race in Brazil has divided individuals of African descent, between those with more or less ancestry. As a result, they have not united for a stronger civil rights movement.[citation needed]
Though Brazilians of at least partial African heritage make up a large percentage of the population, few blacks have been elected as politicians. The city of Salvador, Bahia, for instance, is 80% people of color, but voters have not elected a mayor of color. Journalists like to say that US cities with black majorities, such as Detroit and New Orleans, have not elected white mayors since after the civil rights movement, when the Voting Rights Act of 1965 protected the franchise for minorities, and blacks in the South regained the power to vote for the first time since the turn of the 20th century. New Orleans elected its first black mayor in the 1970s. New Orleans elected a white mayor after the widescale disruption and damage of Hurricane Katrina in 2005.
Critics note that people of color have limited media visibility. The Brazilian media has been accused of hiding or overlooking the nation's Black, Indigenous, Multiracial and East Asian populations. For example, the telenovelas or soaps are criticized for featuring actors who resemble northern Europeans rather than actors of the more prevalent Southern European features) and light-skinned mulatto and mestizo appearance. (Pardos may achieve "white" status if they have attained the middle-class or higher social status).
These patterns of discrimination against non-whites have led some academic and other activists to advocate for use of the Portuguese term negro to encompass all African-descended people, in order to stimulate a "black" consciousness and identity. This proposal has been criticized since the term pardo is considered to include a wide range of multiracial people, such as caboclos (mestizos), assimilated Amerindians and tri-racials, not only people of partial African and European descent. Trying to identify this entire group as "black" would be a false imposition of a different identity from outside the culture and deny people their other, equally valid, ancestries and cultures. It seems a one-drop rule in reverse.
Greece is a developed country with an economy based on the service (82.8%) and industrial sectors (13.3%). The agricultural sector contributed 3.9% of national economic output in 2015. Important Greek industries include tourism and shipping. With 18 million international tourists in 2013, Greece was the 7th most visited country in the European Union and 16th in the world. The Greek Merchant Navy is the largest in the world, with Greek-owned vessels accounting for 15% of global deadweight tonnage as of 2013. The increased demand for international maritime transportation between Greece and Asia has resulted in unprecedented investment in the shipping industry.
The country is a significant agricultural producer within the EU. Greece has the largest economy in the Balkans and is as an important regional investor. Greece was the largest foreign investor in Albania in 2013, the third in Bulgaria, in the top-three in Romania and Serbia and the most important trading partner and largest foreign investor in the former Yugoslav Republic of Macedonia. The Greek telecommunications company OTE has become a strong investor in former Yugoslavia and in other Balkan countries.
Greece is classified as an advanced, high-income economy, and was a founding member of the Organisation for Economic Co-operation and Development (OECD) and of the Organization of the Black Sea Economic Cooperation (BSEC). The country joined what is now the European Union in 1981. In 2001 Greece adopted the euro as its currency, replacing the Greek drachma at an exchange rate of 340.75 drachmae per euro. Greece is a member of the International Monetary Fund and of the World Trade Organization, and ranked 34th on Ernst & Young's Globalization Index 2011.
World War II (1939-1945) devastated the country's economy, but the high levels of economic growth that followed from 1950 to 1980 have been called the Greek economic miracle. From 2000 Greece saw high levels of GDP growth above the Eurozone average, peaking at 5.8% in 2003 and 5.7% in 2006. The subsequent Great Recession and Greek government-debt crisis, a central focus of the wider European debt crisis, plunged the economy into a sharp downturn, with real GDP growth rates of −0.3% in 2008, −4.3% in 2009, −5.5% in 2010, −9.1% in 2011, −7.3% in 2012 and −3.2% in 2013. In 2011, the country's public debt reached €356 billion (172% of nominal GDP). After negotiating the biggest debt restructuring in history with the private sector, Greece reduced its sovereign debt burden to €280 billion (137% of GDP) in the first quarter of 2012. Greece achieved a real GDP growth rate of 0.7% in 2014 after 6 years of economic decline, but fell back into recession in 2015.
The evolution of the Greek economy during the 19th century (a period that transformed a large part of the world because of the Industrial Revolution) has been little researched. Recent research from 2006 examines the gradual development of industry and further development of shipping in a predominantly agricultural economy, calculating an average rate of per capita GDP growth between 1833 and 1911 that was only slightly lower than that of the other Western European nations. Industrial activity, (including heavy industry like shipbuilding) was evident, mainly in Ermoupolis and Piraeus. Nonetheless, Greece faced economic hardships and defaulted on its external loans in 1826, 1843, 1860 and 1894.
After fourteen consecutive years of economic growth, Greece went into recession in 2008. By the end of 2009, the Greek economy faced the highest budget deficit and government debt-to-GDP ratios in the EU. After several upward revisions, the 2009 budget deficit is now estimated at 15.7% of GDP. This, combined with rapidly rising debt levels (127.9% of GDP in 2009) led to a precipitous increase in borrowing costs, effectively shutting Greece out of the global financial markets and resulting in a severe economic crisis.
Greece was accused of trying to cover up the extent of its massive budget deficit in the wake of the global financial crisis. The allegation was prompted by the massive revision of the 2009 budget deficit forecast by the new PASOK government elected in October 2009, from "6–8%" (estimated by the previous New Democracy government) to 12.7% (later revised to 15.7%). However, the accuracy of the revised figures has also been questioned, and in February 2012 the Hellenic Parliament voted in favor of an official investigation following accusations by a former member of the Hellenic Statistical Authority that the deficit had been artificially inflated in order to justify harsher austerity measures.
Most of the differences in the revised budget deficit numbers were due to a temporary change of accounting practices by the new government, i.e., recording expenses when military material was ordered rather than received. However, it was the retroactive application of ESA95 methodology (applied since 2000) by Eurostat, that finally raised the reference year (1999) budget deficit to 3.38% of GDP, thus exceeding the 3% limit. This led to claims that Greece (similar claims have been made about other European countries like Italy) had not actually met all five accession criteria, and the common perception that Greece entered the Eurozone through "falsified" deficit numbers.
In the 2005 OECD report for Greece, it was clearly stated that "the impact of new accounting rules on the fiscal figures for the years 1997 to 1999 ranged from 0.7 to 1 percentage point of GDP; this retroactive change of methodology was responsible for the revised deficit exceeding 3% in 1999, the year of [Greece's] EMU membership qualification". The above led the Greek minister of finance to clarify that the 1999 budget deficit was below the prescribed 3% limit when calculated with the ESA79 methodology in force at the time of Greece's application, and thus the criteria had been met.
An error sometimes made is the confusion of discussion regarding Greece’s Eurozone entry with the controversy regarding usage of derivatives’ deals with U.S. Banks by Greece and other Eurozone countries to artificially reduce their reported budget deficits. A currency swap arranged with Goldman Sachs allowed Greece to "hide" 2.8 billion Euros of debt, however, this affected deficit values after 2001 (when Greece had already been admitted into the Eurozone) and is not related to Greece’s Eurozone entry.
According to Der Spiegel, credits given to European governments were disguised as "swaps" and consequently did not get registered as debt because Eurostat at the time ignored statistics involving financial derivatives. A German derivatives dealer had commented to Der Spiegel that "The Maastricht rules can be circumvented quite legally through swaps," and "In previous years, Italy used a similar trick to mask its true debt with the help of a different US bank." These conditions had enabled Greek as well as many other European governments to spend beyond their means, while meeting the deficit targets of the European Union and the monetary union guidelines. In May 2010, the Greek government deficit was again revised and estimated to be 13.6% which was the second highest in the world relative to GDP with Iceland in first place at 15.7% and Great Britain third with 12.6%. Public debt was forecast, according to some estimates, to hit 120% of GDP during 2010.
As a consequence, there was a crisis in international confidence in Greece's ability to repay its sovereign debt, as reflected by the rise of the country's borrowing rates (although their slow rise – the 10-year government bond yield only exceeded 7% in April 2010 – coinciding with a large number of negative articles, has led to arguments about the role of international news media in the evolution of the crisis). In order to avert a default (as high borrowing rates effectively prohibited access to the markets), in May 2010 the other Eurozone countries, and the IMF, agreed to a "rescue package" which involved giving Greece an immediate €45 billion in bail-out loans, with more funds to follow, totaling €110 billion. In order to secure the funding, Greece was required to adopt harsh austerity measures to bring its deficit under control. Their implementation will be monitored and evaluated by the European Commission, the European Central Bank and the IMF.
Between 2005 and 2011, Greece has had the highest percentage increase in industrial output compared to 2005 levels out of all European Union members, with an increase of 6%. Eurostat statistics show that the industrial sector was hit by the Greek financial crisis throughout 2009 and 2010, with domestic output decreasing by 5.8% and industrial production in general by 13.4%. Currently, Greece is ranked third in the European Union in the production of marble (over 920,000 tons), after Italy and Spain.
Greece has the largest merchant navy in the world, accounting for more than 15% of the world's total deadweight tonnage (dwt) according to the United Nations Conference on Trade and Development. The Greek merchant navy's total dwt of nearly 245 million is comparable only to Japan's, which is ranked second with almost 224 million. Additionally, Greece represents 39.52% of all of the European Union's dwt. However, today's fleet roster is smaller than an all-time high of 5,000 ships in the late 1970s.
In terms of ship categories, Greek companies have 22.6% of the world's tankers and 16.1% of the world's bulk carriers (in dwt). An additional equivalent of 27.45% of the world's tanker dwt is on order, with another 12.7% of bulk carriers also on order. Shipping accounts for an estimated 6% of Greek GDP, employs about 160,000 people (4% of the workforce), and represents 1/3 of the country's trade deficit. Earnings from shipping amounted to €14.1 billion in 2011, while between 2000 and 2010 Greek shipping contributed a total of €140 billion (half of the country's public debt in 2009 and 3.5 times the receipts from the European Union in the period 2000–2013). The 2011 ECSA report showed that there are approximately 750 Greek shipping companies in operation.
Counting shipping as quasi-exports and in terms of monetary value, Greece ranked 4th globally in 2011 having "exported" shipping services worth 17,704.132 million $; only Denmark, Germany and South Korea ranked higher during that year. Similarly counting shipping services provided to Greece by other countries as quasi-imports and the difference between "exports" and "imports" as a "trade balance", Greece in 2011 ranked in the latter second behind Germany, having "imported" shipping services worth 7,076.605 million US$ and having run a "trade surplus" of 10,712.342 million US$.
Between 1949 and the 1980s, telephone communications in Greece were a state monopoly by the Hellenic Telecommunications Organization, better known by its acronym, OTE. Despite the liberalization of telephone communications in the country in the 1980s, OTE still dominates the Greek market in its field and has emerged as one of the largest telecommunications companies in Southeast Europe. Since 2011, the company's major shareholder is Deutsche Telekom with a 40% stake, while the Greek state continues to own 10% of the company's shares. OTE owns several subsidiaries across the Balkans, including Cosmote, Greece's top mobile telecommunications provider, Cosmote Romania and Albanian Mobile Communications.
Greece has tended to lag behind its European Union partners in terms of Internet use, with the gap closing rapidly in recent years. The percentage of households with access to the Internet more than doubled between 2006 and 2013, from 23% to 56% respectively (compared with an EU average of 49% and 79%). At the same time, there has been a massive increase in the proportion of households with a broadband connection, from 4% in 2006 to 55% in 2013 (compared with an EU average of 30% and 76%). However, Greece also has the EU's third highest percentage of people who have never used the Internet: 36% in 2013, down from 65% in 2006 (compared with an EU average of 21% and 42%).
Greece attracts more than 16 million tourists each year, thus contributing 18.2% to the nation's GDP in 2008 according to an OECD report. The same survey showed that the average tourist expenditure while in Greece was $1,073, ranking Greece 10th in the world. The number of jobs directly or indirectly related to the tourism sector were 840,000 in 2008 and represented 19% of the country's total labor force. In 2009, Greece welcomed over 19.3 million tourists, a major increase from the 17.7 million tourists the country welcomed in 2008.
In recent years a number of well-known tourism-related organizations have placed Greek destinations in the top of their lists. In 2009 Lonely Planet ranked Thessaloniki, the country's second-largest city, the world's fifth best "Ultimate Party Town", alongside cities such as Montreal and Dubai, while in 2011 the island of Santorini was voted as the best island in the world by Travel + Leisure. The neighbouring island of Mykonos was ranked as the 5th best island Europe. Thessaloniki was the European Youth Capital in 2014.
Between 1975 and 2009, Olympic Airways (known after 2003 as Olympic Airlines) was the country’s state-owned flag carrier, but financial problems led to its privatization and relaunch as Olympic Air in 2009. Both Aegean Airlines and Olympic Air have won awards for their services; in 2009 and 2011, Aegean Airlines was awarded the "Best regional airline in Europe" award by Skytrax, and also has two gold and one silver awards by the ERA, while Olympic Air holds one silver ERA award for "Airline of the Year" as well as a "Condé Nast Traveller 2011 Readers Choice Awards: Top Domestic Airline" award.
Greece's rail network is estimated to be at 2,548 km. Rail transport in Greece is operated by TrainOSE, a subsidiary of the Hellenic Railways Organization (OSE). Most of the country's network is standard gauge (1,565 km), while the country also has 983 km of narrow gauge. A total of 764 km of rail are electrified. Greece has rail connections with Bulgaria, the Republic of Macedonia and Turkey. A total of three suburban railway systems (Proastiakos) are in operation (in Athens, Thessaloniki and Patras), while one metro system is operational in Athens with another under construction.
According to Eurostat, Greece's largest port by tons of goods transported in 2010 is the port of Aghioi Theodoroi, with 17.38 million tons. The Port of Thessaloniki comes second with 15.8 million tons, followed by the Port of Piraeus, with 13.2 million tons, and the port of Eleusis, with 12.37 million tons. The total number of goods transported through Greece in 2010 amounted to 124.38 million tons, a considerable drop from the 164.3 million tons transported through the country in 2007. Since then, Piraeus has grown to become the Mediterranean's third-largest port thanks to heavy investment by Chinese logistics giant COSCO. In 2013, Piraeus was declared the fastest-growing port in the world.
In 2010 Piraeus handled 513,319 TEUs, followed by Thessaloniki, which handled 273,282 TEUs. In the same year, 83.9 million people passed through Greece's ports, 12.7 million through the port of Paloukia in Salamis, another 12.7 through the port of Perama, 9.5 million through Piraeus and 2.7 million through Igoumenitsa. In 2013, Piraeus handled a record 3.16 million TEUs, the third-largest figure in the Mediterranean, of which 2.52 million were transported through Pier II, owned by COSCO and 644,000 were transported through Pier I, owned by the Greek state.
Energy production in Greece is dominated by the Public Power Corporation (known mostly by its acronym ΔΕΗ, or in English DEI). In 2009 DEI supplied for 85.6% of all energy demand in Greece, while the number fell to 77.3% in 2010. Almost half (48%) of DEI's power output is generated using lignite, a drop from the 51.6% in 2009. Another 12% comes from Hydroelectric power plants and another 20% from natural gas. Between 2009 and 2010, independent companies' energy production increased by 56%, from 2,709 Gigawatt hour in 2009 to 4,232 GWh in 2010.
In 2008 renewable energy accounted for 8% of the country's total energy consumption, a rise from the 7.2% it accounted for in 2006, but still below the EU average of 10% in 2008. 10% of the country's renewable energy comes from solar power, while most comes from biomass and waste recycling. In line with the European Commission's Directive on Renewable Energy, Greece aims to get 18% of its energy from renewable sources by 2020. In 2013 and for several months, Greece produced more than 20% of its electricity from renewable energy sources and hydroelectric power plants. Greece currently does not have any nuclear power plants in operation, however in 2009 the Academy of Athens suggested that research in the possibility of Greek nuclear power plants begin.
In addition to the above, Greece is also to start oil and gas exploration in other locations in the Ionian Sea, as well as the Libyan Sea, within the Greek exclusive economic zone, south of Crete. The Ministry of the Environment, Energy and Climate Change announced that there was interest from various countries (including Norway and the United States) in exploration, and the first results regarding the amount of oil and gas in these locations were expected in the summer of 2012. In November 2012, a report published by Deutsche Bank estimated the value of natural gas reserves south of Crete at €427 billion.
Between 1832 and 2002 the currency of Greece was the drachma. After signing the Maastricht Treaty, Greece applied to join the eurozone. The two main convergence criteria were a maximum budget deficit of 3% of GDP and a declining public debt if it stood above 60% of GDP. Greece met the criteria as shown in its 1999 annual public account. On 1 January 2001, Greece joined the eurozone, with the adoption of the euro at the fixed exchange rate ₯340.75 to €1. However, in 2001 the euro only existed electronically, so the physical exchange from drachma to euro only took place on 1 January 2002. This was followed by a ten-year period for eligible exchange of drachma to euro, which ended on 1 March 2012.
IMF's forecast said that Greece's unemployment rate would hit the highest 14.8 percent in 2012 and decrease to 14.1 in 2014.  But in fact, the Greek economy suffered a prolonged high unemployemnt. The unemployment figure was between 9 per cent and 11 per cent in 2009, and it soared to 28 per cent in 2013. In 2015, Greece's jobless rate is around 24 per cent. It is thought that Greece's potential output has been eroded by this prolonged massive unemployment due to the associated hysteresis effects.
The original Latin word "universitas" refers in general to "a number of persons associated into one body, a society, company, community, guild, corporation, etc." At the time of the emergence of urban town life and medieval guilds, specialised "associations of students and teachers with collective legal rights usually guaranteed by charters issued by princes, prelates, or the towns in which they were located" came to be denominated by this general term. Like other guilds, they were self-regulating and determined the qualifications of their members.
An important idea in the definition of a university is the notion of academic freedom. The first documentary evidence of this comes from early in the life of the first university. The University of Bologna adopted an academic charter, the Constitutio Habita, in 1158 or 1155, which guaranteed the right of a traveling scholar to unhindered passage in the interests of education. Today this is claimed as the origin of "academic freedom". This is now widely recognised internationally - on 18 September 1988, 430 university rectors signed the Magna Charta Universitatum, marking the 900th anniversary of Bologna's foundation. The number of universities signing the Magna Charta Universitatum continues to grow, drawing from all parts of the world.
European higher education took place for hundreds of years in Christian cathedral schools or monastic schools (scholae monasticae), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century. The earliest universities were developed under the aegis of the Latin Church by papal bull as studia generalia and perhaps from cathedral schools. It is possible, however, that the development of cathedral schools into universities was quite rare, with the University of Paris being an exception. Later they were also founded by Kings (University of Naples Federico II, Charles University in Prague, Jagiellonian University in Kraków) or municipal administrations (University of Cologne, University of Erfurt). In the early medieval period, most new universities were founded from pre-existing schools, usually when these schools were deemed to have become primarily sites of higher education. Many historians state that universities and cathedral schools were a continuation of the interest in learning promoted by monasteries.
All over Europe rulers and city governments began to create universities to satisfy a European thirst for knowledge, and the belief that society would benefit from the scholarly expertise generated from these institutions. Princes and leaders of city governments perceived the potential benefits of having a scholarly expertise develop with the ability to address difficult problems and achieve desired ends. The emergence of humanism was essential to this understanding of the possible utility of universities as well as the revival of interest in knowledge gained from ancient Greek texts.
The rediscovery of Aristotle's works–more than 3000 pages of it would eventually be translated –fuelled a spirit of inquiry into natural processes that had already begun to emerge in the 12th century. Some scholars believe that these works represented one of the most important document discoveries in Western intellectual history. Richard Dales, for instance, calls the discovery of Aristotle's works "a turning point in the history of Western thought." After Aristotle re-emerged, a community of scholars, primarily communicating in Latin, accelerated the process and practice of attempting to reconcile the thoughts of Greek antiquity, and especially ideas related to understanding the natural world, with those of the church. The efforts of this "scholasticism" were focused on applying Aristotelian logic and thoughts about natural processes to biblical passages and attempting to prove the viability of those passages through reason. This became the primary mission of lecturers, and the expectation of students.
The university culture developed differently in northern Europe than it did in the south, although the northern (primarily Germany, France and Great Britain) and southern universities (primarily Italy) did have many elements in common. Latin was the language of the university, used for all texts, lectures, disputations and examinations. Professors lectured on the books of Aristotle for logic, natural philosophy, and metaphysics; while Hippocrates, Galen, and Avicenna were used for medicine. Outside of these commonalities, great differences separated north and south, primarily in subject matter. Italian universities focused on law and medicine, while the northern universities focused on the arts and theology. There were distinct differences in the quality of instruction in these areas which were congruent with their focus, so scholars would travel north or south based on their interests and means. There was also a difference in the types of degrees awarded at these universities. English, French and German universities usually awarded bachelor's degrees, with the exception of degrees in theology, for which the doctorate was more common. Italian universities awarded primarily doctorates. The distinction can be attributed to the intent of the degree holder after graduation – in the north the focus tended to be on acquiring teaching positions, while in the south students often went on to professional positions. The structure of northern universities tended to be modeled after the system of faculty governance developed at the University of Paris. Southern universities tended to be patterned after the student-controlled model begun at the University of Bologna. Among the southern universities, a further distinction has been noted between those of northern Italy, which followed the pattern of Bologna as a "self-regulating, independent corporation of scholars" and those of southern Italy and Iberia, which were "founded by royal and imperial charter to serve the needs of government."
Their endowment by a prince or monarch and their role in training government officials made these Mediterranean universities similar to Islamic madrasas, although madrasas were generally smaller and individual teachers, rather than the madrasa itself, granted the license or degree. Scholars like Arnold H. Green and Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madrasahs became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Other scholars regard the university as uniquely European in origin and characteristics.
Many scholars (including Makdisi) have argued that early medieval universities were influenced by the religious madrasahs in Al-Andalus, the Emirate of Sicily, and the Middle East (during the Crusades). Other scholars see this argument as overstated. Lowe and Yasuhara have recently drawn on the well-documented influences of scholarship from the Islamic world on the universities of Western Europe to call for a reconsideration of the development of higher education, turning away from a concern with local institutional structures to a broader consideration within a global context.
During the Early Modern period (approximately late 15th century to 1800), the universities of Europe would see a tremendous amount of growth, productivity and innovative research. At the end of the Middle Ages, about 400 years after the first university was founded, there were twenty-nine universities spread throughout Europe. In the 15th century, twenty-eight new ones were created, with another eighteen added between 1500 and 1625. This pace continued until by the end of the 18th century there were approximately 143 universities in Europe and Eastern Europe, with the highest concentrations in the German Empire (34), Italian countries (26), France (25), and Spain (23) – this was close to a 500% increase over the number of universities toward the end of the Middle Ages. This number does not include the numerous universities that disappeared, or institutions that merged with other universities during this time. It should be noted that the identification of a university was not necessarily obvious during the Early Modern period, as the term is applied to a burgeoning number of institutions. In fact, the term "university" was not always used to designate a higher education institution. In Mediterranean countries, the term studium generale was still often used, while "Academy" was common in Northern European countries.
The propagation of universities was not necessarily a steady progression, as the 17th century was rife with events that adversely affected university expansion. Many wars, and especially the Thirty Years' War, disrupted the university landscape throughout Europe at different times. War, plague, famine, regicide, and changes in religious power and structure often adversely affected the societies that provided support for universities. Internal strife within the universities themselves, such as student brawling and absentee professors, acted to destabilize these institutions as well. Universities were also reluctant to give up older curricula, and the continued reliance on the works of Aristotle defied contemporary advancements in science and the arts. This era was also affected by the rise of the nation-state. As universities increasingly came under state control, or formed under the auspices of the state, the faculty governance model (begun by the University of Paris) became more and more prominent. Although the older student-controlled universities still existed, they slowly started to move toward this structural organization. Control of universities still tended to be independent, although university leadership was increasingly appointed by the state.
Although the structural model provided by the University of Paris, where student members are controlled by faculty "masters," provided a standard for universities, the application of this model took at least three different forms. There were universities that had a system of faculties whose teaching addressed a very specific curriculum; this model tended to train specialists. There was a collegiate or tutorial model based on the system at University of Oxford where teaching and organization was decentralized and knowledge was more of a generalist nature. There were also universities that combined these models, using the collegiate model but having a centralized organization.
Early Modern universities initially continued the curriculum and research of the Middle Ages: natural philosophy, logic, medicine, theology, mathematics, astronomy (and astrology), law, grammar and rhetoric. Aristotle was prevalent throughout the curriculum, while medicine also depended on Galen and Arabic scholarship. The importance of humanism for changing this state-of-affairs cannot be underestimated. Once humanist professors joined the university faculty, they began to transform the study of grammar and rhetoric through the studia humanitatis. Humanist professors focused on the ability of students to write and speak with distinction, to translate and interpret classical texts, and to live honorable lives. Other scholars within the university were affected by the humanist approaches to learning and their linguistic expertise in relation to ancient texts, as well as the ideology that advocated the ultimate importance of those texts. Professors of medicine such as Niccolò Leoniceno, Thomas Linacre and William Cop were often trained in and taught from a humanist perspective as well as translated important ancient medical texts. The critical mindset imparted by humanism was imperative for changes in universities and scholarship. For instance, Andreas Vesalius was educated in a humanist fashion before producing a translation of Galen, whose ideas he verified through his own dissections. In law, Andreas Alciatus infused the Corpus Juris with a humanist perspective, while Jacques Cujas humanist writings were paramount to his reputation as a jurist. Philipp Melanchthon cited the works of Erasmus as a highly influential guide for connecting theology back to original texts, which was important for the reform at Protestant universities. Galileo Galilei, who taught at the Universities of Pisa and Padua, and Martin Luther, who taught at the University of Wittenberg (as did Melanchthon), also had humanist training. The task of the humanists was to slowly permeate the university; to increase the humanist presence in professorships and chairs, syllabi and textbooks so that published works would demonstrate the humanistic ideal of science and scholarship.
Although the initial focus of the humanist scholars in the university was the discovery, exposition and insertion of ancient texts and languages into the university, and the ideas of those texts into society generally, their influence was ultimately quite progressive. The emergence of classical texts brought new ideas and led to a more creative university climate (as the notable list of scholars above attests to). A focus on knowledge coming from self, from the human, has a direct implication for new forms of scholarship and instruction, and was the foundation for what is commonly known as the humanities. This disposition toward knowledge manifested in not simply the translation and propagation of ancient texts, but also their adaptation and expansion. For instance, Vesalius was imperative for advocating the use of Galen, but he also invigorated this text with experimentation, disagreements and further research. The propagation of these texts, especially within the universities, was greatly aided by the emergence of the printing press and the beginning of the use of the vernacular, which allowed for the printing of relatively large texts at reasonable prices.
There are several major exceptions on tuition fees. In many European countries, it is possible to study without tuition fees. Public universities in Nordic countries were entirely without tuition fees until around 2005. Denmark, Sweden and Finland then moved to put in place tuition fees for foreign students. Citizens of EU and EEA member states and citizens from Switzerland remain exempted from tuition fees, and the amounts of public grants granted to promising foreign students were increased to offset some of the impact.
Colloquially, the term university may be used to describe a phase in one's life: "When I was at university..." (in the United States and Ireland, college is often used instead: "When I was in college..."). In Australia, Canada, New Zealand, the United Kingdom, Nigeria, the Netherlands, Spain and the German-speaking countries university is often contracted to uni. In Ghana, New Zealand and in South Africa it is sometimes called "varsity" (although this has become uncommon in New Zealand in recent years). "Varsity" was also common usage in the UK in the 19th century.[citation needed] "Varsity" is still in common usage in Scotland.
In Canada, "college" generally refers to a two-year, non-degree-granting institution, while "university" connotes a four-year, degree-granting institution. Universities may be sub-classified (as in the Macleans rankings) into large research universities with many PhD granting programs and medical schools (for example, McGill University); "comprehensive" universities that have some PhDs but aren't geared toward research (such as Waterloo); and smaller, primarily undergraduate universities (such as St. Francis Xavier).
Although each institution is organized differently, nearly all universities have a board of trustees; a president, chancellor, or rector; at least one vice president, vice-chancellor, or vice-rector; and deans of various divisions. Universities are generally divided into a number of academic departments, schools or faculties. Public university systems are ruled over by government-run higher education boards. They review financial requests and budget proposals and then allocate funds for each university in the system. They also approve new programs of instruction and cancel or make changes in existing programs. In addition, they plan for the further coordinated growth and development of the various institutions of higher education in the state or country. However, many public universities in the world have a considerable degree of financial, research and pedagogical autonomy. Private universities are privately funded and generally have broader independence from state policies. However, they may have less independence from business corporations depending on the source of their finances.
The funding and organization of universities varies widely between different countries around the world. In some countries universities are predominantly funded by the state, while in others funding may come from donors or from fees which students attending the university must pay. In some countries the vast majority of students attend university in their local town, while in other countries universities attract students from all over the world, and may provide university accommodation for their students.
Universities created by bilateral or multilateral treaties between states are intergovernmental. An example is the Academy of European Law, which offers training in European law to lawyers, judges, barristers, solicitors, in-house counsel and academics. EUCLID (Pôle Universitaire Euclide, Euclid University) is chartered as a university and umbrella organisation dedicated to sustainable development in signatory countries, and the United Nations University engages in efforts to resolve the pressing global problems that are of concern to the United Nations, its peoples and member states. The European University Institute, a post-graduate university specialised in the social sciences, is officially an intergovernmental organisation, set up by the member states of the European Union.
A national university is generally a university created or run by a national state but at the same time represents a state autonomic institution which functions as a completely independent body inside of the same state. Some national universities are closely associated with national cultural or political aspirations, for instance the National University of Ireland in the early days of Irish independence collected a large amount of information on the Irish language and Irish culture. Reforms in Argentina were the result of the University Revolution of 1918 and its posterior reforms by incorporating values that sought for a more equal and laic higher education system.
In 1963, the Robbins Report on universities in the United Kingdom concluded that such institutions should have four main "objectives essential to any properly balanced system: instruction in skills; the promotion of the general powers of the mind so as to produce not mere specialists but rather cultivated men and women; to maintain research in balance with teaching, since teaching should not be separated from the advancement of learning and the search for truth; and to transmit a common culture and common standards of citizenship."
Until the 19th century, religion played a significant role in university curriculum; however, the role of religion in research universities decreased in the 19th century, and by the end of the 19th century, the German university model had spread around the world. Universities concentrated on science in the 19th and 20th centuries and became increasingly accessible to the masses. In Britain, the move from Industrial Revolution to modernity saw the arrival of new civic universities with an emphasis on science and engineering, a movement initiated in 1960 by Sir Keith Murray (chairman of the University Grants Committee) and Sir Samuel Curran, with the formation of the University of Strathclyde. The British also established universities worldwide, and higher education became available to the masses not only in Europe.
By the end of the early modern period, the structure and orientation of higher education had changed in ways that are eminently recognizable for the modern context. Aristotle was no longer a force providing the epistemological and methodological focus for universities and a more mechanistic orientation was emerging. The hierarchical place of theological knowledge had for the most part been displaced and the humanities had become a fixture, and a new openness was beginning to take hold in the construction and dissemination of knowledge that were to become imperative for the formation of the modern state.
The epistemological tensions between scientists and universities were also heightened by the economic realities of research during this time, as individual scientists, associations and universities were vying for limited resources. There was also competition from the formation of new colleges funded by private benefactors and designed to provide free education to the public, or established by local governments to provide a knowledge hungry populace with an alternative to traditional universities. Even when universities supported new scientific endeavors, and the university provided foundational training and authority for the research and conclusions, they could not compete with the resources available through private benefactors.
Other historians find incongruity in the proposition that the very place where the vast number of the scholars that influenced the scientific revolution received their education should also be the place that inhibits their research and the advancement of science. In fact, more than 80% of the European scientists between 1450–1650 included in the Dictionary of Scientific Biography were university trained, of which approximately 45% held university posts. It was the case that the academic foundations remaining from the Middle Ages were stable, and they did provide for an environment that fostered considerable growth and development. There was considerable reluctance on the part of universities to relinquish the symmetry and comprehensiveness provided by the Aristotelian system, which was effective as a coherent system for understanding and interpreting the world. However, university professors still utilized some autonomy, at least in the sciences, to choose epistemological foundations and methods. For instance, Melanchthon and his disciples at University of Wittenberg were instrumental for integrating Copernican mathematical constructs into astronomical debate and instruction. Another example was the short-lived but fairly rapid adoption of Cartesian epistemology and methodology in European universities, and the debates surrounding that adoption, which led to more mechanistic approaches to scientific problems as well as demonstrated an openness to change. There are many examples which belie the commonly perceived intransigence of universities. Although universities may have been slow to accept new sciences and methodologies as they emerged, when they did accept new ideas it helped to convey legitimacy and respectability, and supported the scientific changes through providing a stable environment for instruction and material resources.
Regardless of the way the tension between universities, individual scientists, and the scientific revolution itself is perceived, there was a discernible impact on the way that university education was constructed. Aristotelian epistemology provided a coherent framework not simply for knowledge and knowledge construction, but also for the training of scholars within the higher education setting. The creation of new scientific constructs during the scientific revolution, and the epistemological challenges that were inherent within this creation, initiated the idea of both the autonomy of science and the hierarchy of the disciplines. Instead of entering higher education to become a "general scholar" immersed in becoming proficient in the entire curriculum, there emerged a type of scholar that put science first and viewed it as a vocation in itself. The divergence between those focused on science and those still entrenched in the idea of a general scholar exacerbated the epistemological tensions that were already beginning to emerge.
Examining the influence of humanism on scholars in medicine, mathematics, astronomy and physics may suggest that humanism and universities were a strong impetus for the scientific revolution. Although the connection between humanism and the scientific discovery may very well have begun within the confines of the university, the connection has been commonly perceived as having been severed by the changing nature of science during the scientific revolution. Historians such as Richard S. Westfall have argued that the overt traditionalism of universities inhibited attempts to re-conceptualize nature and knowledge and caused an indelible tension between universities and scientists. This resistance to changes in science may have been a significant factor in driving many scientists away from the university and toward private benefactors, usually in princely courts, and associations with newly forming scientific societies.
An alloy is a mixture of metals or a mixture of a metal and another element. Alloys are defined by metallic bonding character. An alloy may be a solid solution of metal elements (a single phase) or a mixture of metallic phases (two or more solutions). Intermetallic compounds are alloys with a defined stoichiometry and crystal structure. Zintl phases are also sometimes considered alloys depending on bond types (see also: Van Arkel-Ketelaar triangle for information on classifying bonding in binary compounds).
An alloy is a mixture of either pure or fairly pure chemical elements, which forms an impure substance (admixture) that retains the characteristics of a metal. An alloy is distinct from an impure metal, such as wrought iron, in that, with an alloy, the added impurities are usually desirable and will typically have some useful benefit. Alloys are made by mixing two or more elements; at least one of which being a metal. This is usually called the primary metal or the base metal, and the name of this metal may also be the name of the alloy. The other constituents may or may not be metals but, when mixed with the molten base, they will be soluble, dissolving into the mixture.
When the alloy cools and solidifies (crystallizes), its mechanical properties will often be quite different from those of its individual constituents. A metal that is normally very soft and malleable, such as aluminium, can be altered by alloying it with another soft metal, like copper. Although both metals are very soft and ductile, the resulting aluminium alloy will be much harder and stronger. Adding a small amount of non-metallic carbon to iron produces an alloy called steel. Due to its very-high strength and toughness (which is much higher than pure iron), and its ability to be greatly altered by heat treatment, steel is one of the most common alloys in modern use. By adding chromium to steel, its resistance to corrosion can be enhanced, creating stainless steel, while adding silicon will alter its electrical characteristics, producing silicon steel.
Although the elements usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If the mixture cools and the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. These alloys are called intermetallic alloys because, if cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys separate within the crystals, forming intermetallic phases that serve to reinforce the crystals internally.
Some alloys occur naturally, such as electrum, which is an alloy that is native to Earth, consisting of silver and gold. Meteorites are sometimes made of naturally occurring alloys of iron and nickel, but are not native to the Earth. One of the first alloys made by humans was bronze, which is made by mixing the metals tin and copper. Bronze was an extremely useful alloy to the ancients, because it is much stronger and harder than either of its components. Steel was another common alloy. However, in ancient times, it could only be created as an accidental byproduct from the heating of iron ore in fires (smelting) during the manufacture of iron. Other ancient alloys include pewter, brass and pig iron. In the modern age, steel can be created in many forms. Carbon steel can be made by varying only the carbon content, producing soft alloys like mild steel or hard alloys like spring steel. Alloy steels can be made by adding other elements, such as molybdenum, vanadium or nickel, resulting in alloys such as high-speed steel or tool steel. Small amounts of manganese are usually alloyed with most modern-steels because of its ability to remove unwanted impurities, like phosphorus, sulfur and oxygen, which can have detrimental effects on the alloy. However, most alloys were not created until the 1900s, such as various aluminium, titanium, nickel, and magnesium alloys. Some modern superalloys, such as incoloy, inconel, and hastelloy, may consist of a multitude of different components.
The term alloy is used to describe a mixture of atoms in which the primary constituent is a metal. The primary metal is called the base, the matrix, or the solvent. The secondary constituents are often called solutes. If there is a mixture of only two types of atoms, not counting impurities, such as a copper-nickel alloy, then it is called a binary alloy. If there are three types of atoms forming the mixture, such as iron, nickel and chromium, then it is called a ternary alloy. An alloy with four constituents is a quaternary alloy, while a five-part alloy is termed a quinary alloy. Because the percentage of each constituent can be varied, with any mixture the entire range of possible variations is called a system. In this respect, all of the various forms of an alloy containing only two constituents, like iron and carbon, is called a binary system, while all of the alloy combinations possible with a ternary alloy, such as alloys of iron, carbon and chromium, is called a ternary system.
Although an alloy is technically an impure metal, when referring to alloys, the term "impurities" usually denotes those elements which are not desired. These impurities are often found in the base metals or the solutes, but they may also be introduced during the alloying process. For instance, sulfur is a common impurity in steel. Sulfur combines readily with iron to form iron sulfide, which is very brittle, creating weak spots in the steel. Lithium, sodium and calcium are common impurities in aluminium alloys, which can have adverse effects on the structural integrity of castings. Conversely, otherwise pure-metals that simply contain unwanted impurities are often called "impure metals" and are not usually referred to as alloys. Oxygen, present in the air, readily combines with most metals to form metal oxides; especially at higher temperatures encountered during alloying. Great care is often taken during the alloying process to remove excess impurities, using fluxes, chemical additives, or other methods of extractive metallurgy.
The term "alloy" is sometimes used in everyday speech as a synonym for a particular alloy. For example, automobile wheels made of an aluminium alloy are commonly referred to as simply "alloy wheels", although in point of fact steels and most other metals in practical use are also alloys. Steel is such a common alloy that many items made from it, like wheels, barrels, or girders, are simply referred to by the name of the item, assuming it is made of steel. When made from other materials, they are typically specified as such, (i.e.: "bronze wheel," "plastic barrel," or "wood girder").
Alloying a metal is done by combining it with one or more other metals or non-metals that often enhance its properties. For example, steel is stronger than iron, its primary element. The electrical and thermal conductivity of alloys is usually lower than that of the pure metals. The physical properties, such as density, reactivity, Young's modulus of an alloy may not differ greatly from those of its elements, but engineering properties such as tensile strength and shear strength may be substantially different from those of the constituent materials. This is sometimes a result of the sizes of the atoms in the alloy, because larger atoms exert a compressive force on neighboring atoms, and smaller atoms exert a tensile force on their neighbors, helping the alloy resist deformation. Sometimes alloys may exhibit marked differences in behavior even when small amounts of one element are present. For example, impurities in semiconducting ferromagnetic alloys lead to different properties, as first predicted by White, Hogan, Suhl, Tian Abrie and Nakamura. Some alloys are made by melting and mixing two or more metals. Bronze, an alloy of copper and tin, was the first alloy discovered, during the prehistoric period now known as the bronze age; it was harder than pure copper and originally used to make tools and weapons, but was later superseded by metals and alloys with better properties. In later times bronze has been used for ornaments, bells, statues, and bearings. Brass is an alloy made from copper and zinc.
Alloys are often made to alter the mechanical properties of the base metal, to induce hardness, toughness, ductility, or other desired properties. Most metals and alloys can be work hardened by creating defects in their crystal structure. These defects are created during plastic deformation, such as hammering or bending, and are permanent unless the metal is recrystallized. However, some alloys can also have their properties altered by heat treatment. Nearly all metals can be softened by annealing, which recrystallizes the alloy and repairs the defects, but not as many can be hardened by controlled heating and cooling. Many alloys of aluminium, copper, magnesium, titanium, and nickel can be strengthened to some degree by some method of heat treatment, but few respond to this to the same degree that steel does.
At a certain temperature, (usually between 1,500 °F (820 °C) and 1,600 °F (870 °C), depending on carbon content), the base metal of steel undergoes a change in the arrangement of the atoms in its crystal matrix, called allotropy. This allows the small carbon atoms to enter the interstices of the iron crystal, diffusing into the iron matrix. When this happens, the carbon atoms are said to be in solution, or mixed with the iron, forming a single, homogeneous, crystalline phase called austenite. If the steel is cooled slowly, the iron will gradually change into its low temperature allotrope. When this happens the carbon atoms will no longer be soluble with the iron, and will be forced to precipitate out of solution, nucleating into the spaces between the crystals. The steel then becomes heterogeneous, being formed of two phases; the carbon (carbide) phase cementite, and ferrite. This type of heat treatment produces steel that is rather soft and bendable. However, if the steel is cooled quickly the carbon atoms will not have time to precipitate. When rapidly cooled, a diffusionless (martensite) transformation occurs, in which the carbon atoms become trapped in solution. This causes the iron crystals to deform intrinsically when the crystal structure tries to change to its low temperature state, making it very hard and brittle.
Conversely, most heat-treatable alloys are precipitation hardening alloys, which produce the opposite effects that steel does. When heated to form a solution and then cooled quickly, these alloys become much softer than normal, during the diffusionless transformation, and then harden as they age. The solutes in these alloys will precipitate over time, forming intermetallic phases, which are difficult to discern from the base metal. Unlike steel, in which the solid solution separates to form different crystal phases, precipitation hardening alloys separate to form different phases within the same crystal. These intermetallic alloys appear homogeneous in crystal structure, but tend to behave heterogeneous, becoming hard and somewhat brittle.
When a molten metal is mixed with another substance, there are two mechanisms that can cause an alloy to form, called atom exchange and the interstitial mechanism. The relative size of each element in the mix plays a primary role in determining which mechanism will occur. When the atoms are relatively similar in size, the atom exchange method usually happens, where some of the atoms composing the metallic crystals are substituted with atoms of the other constituent. This is called a substitutional alloy. Examples of substitutional alloys include bronze and brass, in which some of the copper atoms are substituted with either tin or zinc atoms. With the interstitial mechanism, one atom is usually much smaller than the other, so cannot successfully replace an atom in the crystals of the base metal. The smaller atoms become trapped in the spaces between the atoms in the crystal matrix, called the interstices. This is referred to as an interstitial alloy. Steel is an example of an interstitial alloy, because the very small carbon atoms fit into interstices of the iron matrix. Stainless steel is an example of a combination of interstitial and substitutional alloys, because the carbon atoms fit into the interstices, but some of the iron atoms are replaced with nickel and chromium atoms.
The use of alloys by humans started with the use of meteoric iron, a naturally occurring alloy of nickel and iron. It is the main constituent of iron meteorites which occasionally fall down on Earth from outer space. As no metallurgic processes were used to separate iron from nickel, the alloy was used as it was. Meteoric iron could be forged from a red heat to make objects such as tools, weapons, and nails. In many cultures it was shaped by cold hammering into knives and arrowheads. They were often used as anvils. Meteoric iron was very rare and valuable, and difficult for ancient people to work.
Iron is usually found as iron ore on Earth, except for one deposit of native iron in Greenland, which was used by the Inuit people. Native copper, however, was found worldwide, along with silver, gold and platinum, which were also used to make tools, jewelry, and other objects since Neolithic times. Copper was the hardest of these metals, and the most widely distributed. It became one of the most important metals to the ancients. Eventually, humans learned to smelt metals such as copper and tin from ore, and, around 2500 BC, began alloying the two metals to form bronze, which is much harder than its ingredients. Tin was rare, however, being found mostly in Great Britain. In the Middle East, people began alloying copper with zinc to form brass. Ancient civilizations took into account the mixture and the various properties it produced, such as hardness, toughness and melting point, under various conditions of temperature and work hardening, developing much of the information contained in modern alloy phase diagrams. Arrowheads from the Chinese Qin dynasty (around 200 BC) were often constructed with a hard bronze-head, but a softer bronze-tang, combining the alloys to prevent both dulling and breaking during use.
Mercury has been smelted from cinnabar for thousands of years. Mercury dissolves many metals, such as gold, silver, and tin, to form amalgams (an alloy in a soft paste, or liquid form at ambient temperature). Amalgams have been used since 200 BC in China for plating objects with precious metals, called gilding, such as armor and mirrors. The ancient Romans often used mercury-tin amalgams for gilding their armor. The amalgam was applied as a paste and then heated until the mercury vaporized, leaving the gold, silver, or tin behind. Mercury was often used in mining, to extract precious metals like gold and silver from their ores.
Many ancient civilizations alloyed metals for purely aesthetic purposes. In ancient Egypt and Mycenae, gold was often alloyed with copper to produce red-gold, or iron to produce a bright burgundy-gold. Gold was often found alloyed with silver or other metals to produce various types of colored gold. These metals were also used to strengthen each other, for more practical purposes. Copper was often added to silver to make sterling silver, increasing its strength for use in dishes, silverware, and other practical items. Quite often, precious metals were alloyed with less valuable substances as a means to deceive buyers. Around 250 BC, Archimedes was commissioned by the king to find a way to check the purity of the gold in a crown, leading to the famous bath-house shouting of "Eureka!" upon the discovery of Archimedes' principle.
The term pewter covers a variety of alloys consisting primarily of tin. As a pure metal, tin was much too soft to be used for any practical purpose. However, in the Bronze age, tin was a rare metal and, in many parts of Europe and the Mediterranean, was often valued higher than gold. To make jewelry, forks and spoons, or other objects from tin, it was usually alloyed with other metals to increase its strength and hardness. These metals were typically lead, antimony, bismuth or copper. These solutes sometimes were added individually in varying amounts, or added together, making a wide variety of things, ranging from practical items, like dishes, surgical tools, candlesticks or funnels, to decorative items such as ear rings and hair clips.
The first known smelting of iron began in Anatolia, around 1800 BC. Called the bloomery process, it produced very soft but ductile wrought iron. By 800 BC, iron-making technology had spread to Europe, arriving in Japan around 700 AD. Pig iron, a very hard but brittle alloy of iron and carbon, was being produced in China as early as 1200 BC, but did not arrive in Europe until the Middle Ages. Pig iron has a lower melting point than iron, and was used for making cast-iron. However, these metals found little practical use until the introduction of crucible steel around 300 BC. These steels were of poor quality, and the introduction of pattern welding, around the 1st century AD, sought to balance the extreme properties of the alloys by laminating them, to create a tougher metal. Around 700 AD, the Japanese began folding bloomery-steel and cast-iron in alternating layers to increase the strength of their swords, using clay fluxes to remove slag and impurities. This method of Japanese swordsmithing produced one of the purest steel-alloys of the early Middle Ages.
While the use of iron started to become more widespread around 1200 BC, mainly because of interruptions in the trade routes for tin, the metal is much softer than bronze. However, very small amounts of steel, (an alloy of iron and around 1% carbon), was always a byproduct of the bloomery process. The ability to modify the hardness of steel by heat treatment had been known since 1100 BC, and the rare material was valued for the manufacture of tools and weapons. Because the ancients could not produce temperatures high enough to melt iron fully, the production of steel in decent quantities did not occur until the introduction of blister steel during the Middle Ages. This method introduced carbon by heating wrought iron in charcoal for long periods of time, but the penetration of carbon was not very deep, so the alloy was not homogeneous. In 1740, Benjamin Huntsman began melting blister steel in a crucible to even out the carbon content, creating the first process for the mass production of tool steel. Huntsman's process was used for manufacturing tool steel until the early 1900s.
With the introduction of the blast furnace to Europe in the Middle Ages, pig iron was able to be produced in much higher volumes than wrought iron. Because pig iron could be melted, people began to develop processes of reducing the carbon in the liquid pig iron to create steel. Puddling was introduced during the 1700s, where molten pig iron was stirred while exposed to the air, to remove the carbon by oxidation. In 1858, Sir Henry Bessemer developed a process of steel-making by blowing hot air through liquid pig iron to reduce the carbon content. The Bessemer process was able to produce the first large scale manufacture of steel. Once the Bessemer process began to gain widespread use, other alloys of steel began to follow. Mangalloy, an alloy of steel and manganese exhibiting extreme hardness and toughness, was one of the first alloy steels, and was created by Robert Hadfield in 1882.
In 1906, precipitation hardening alloys were discovered by Alfred Wilm. Precipitation hardening alloys, such as certain alloys of aluminium, titanium, and copper, are heat-treatable alloys that soften when quenched (cooled quickly), and then harden over time. After quenching a ternary alloy of aluminium, copper, and magnesium, Wilm discovered that the alloy increased in hardness when left to age at room temperature. Although an explanation for the phenomenon was not provided until 1919, duralumin was one of the first "age hardening" alloys to be used, and was soon followed by many others. Because they often exhibit a combination of high strength and low weight, these alloys became widely used in many forms of industry, including the construction of modern aircraft.
The Alps (/ælps/; Italian: Alpi [ˈalpi]; French: Alpes [alp]; German: Alpen [ˈʔalpm̩]; Slovene: Alpe [ˈáːlpɛ]) are the highest and most extensive mountain range system that lies entirely in Europe, stretching approximately 1,200 kilometres (750 mi) across eight Alpine countries: Austria, France, Germany, Italy, Liechtenstein, Monaco, Slovenia, and Switzerland. The Caucasus Mountains are higher, and the Urals longer, but both lie partly in Asia. The mountains were formed over tens of millions of years as the African and Eurasian tectonic plates collided. Extreme shortening caused by the event resulted in marine sedimentary rocks rising by thrusting and folding into high mountain peaks such as Mont Blanc and the Matterhorn. Mont Blanc spans the French–Italian border, and at 4,810 m (15,781 ft) is the highest mountain in the Alps. The Alpine region area contains about a hundred peaks higher than 4,000 m (13,123 ft), known as the "four-thousanders".
The altitude and size of the range affects the climate in Europe; in the mountains precipitation levels vary greatly and climatic conditions consist of distinct zones. Wildlife such as ibex live in the higher peaks to elevations of 3,400 m (11,155 ft), and plants such as Edelweiss grow in rocky areas in lower elevations as well as in higher elevations. Evidence of human habitation in the Alps goes back to the Paleolithic era.
A mummified man, determined to be 5,000 years old, was discovered on a glacier at the Austrian–Italian border in 1991. By the 6th century BC, the Celtic La Tène culture was well established. Hannibal famously crossed the Alps with a herd of elephants, and the Romans had settlements in the region. In 1800 Napoleon crossed one of the mountain passes with an army of 40,000. The 18th and 19th centuries saw an influx of naturalists, writers, and artists, in particular the Romantics, followed by the golden age of alpinism as mountaineers began to ascend the peaks. In World War II, Adolf Hitler kept a base of operation in the Bavarian Alps throughout the war.
The Alpine region has a strong cultural identity. The traditional culture of farming, cheesemaking, and woodworking still exists in Alpine villages, although the tourist industry began to grow early in the 20th century and expanded greatly after World War II to become the dominant industry by the end of the century. The Winter Olympic Games have been hosted in the Swiss, French, Italian, Austrian and German Alps. At present the region is home to 14 million people and has 120 million annual visitors.
The English word Alps derives from the Latin Alpes (through French). Maurus Servius Honoratus, an ancient commentator of Virgil, says in his commentary (A. X 13) that all high mountains are called Alpes by Celts. The term may be common to Italo-Celtic, because the Celtic languages have terms for high mountains derived from alp.
This may be consistent with the theory that in Greek Alpes is a name of non-Indo-European origin (which is common for prominent mountains and mountain ranges in the Mediterranean region). According to the Old English Dictionary, the Latin Alpes might possibly derive from a pre-Indo-European word *alb "hill"; "Albania" is a related derivation. Albania, a name not native to the region known as the country of Albania, has been used as a name for a number of mountainous areas across Europe. In Roman times, "Albania" was a name for the eastern Caucasus, while in the English language "Albania" (or "Albany") was occasionally used as a name for Scotland.
It's likely[weasel words] that alb ("white") and albus have common origins deriving from the association of the tops of tall mountains or steep hills with snow.
In modern languages the term alp, alm, albe or alpe refers to a grazing pastures in the alpine regions below the glaciers, not the peaks. An alp refers to a high mountain pasture where cows are taken to be grazed during the summer months and where hay barns can be found, and the term "the Alps", referring to the mountains, is a misnomer. The term for the mountain peaks varies by nation and language: words such as horn, kogel, gipfel, spitz, and berg are used in German speaking regions: mont, pic, dent and aiguille in French speaking regions; and monte, picco or cima in Italian speaking regions.
The Alps are a crescent shaped geographic feature of central Europe that ranges in a 800 km (500 mi) arc from east to west and is 200 km (120 mi) in width. The mean height of the mountain peaks is 2.5 km (1.6 mi). The range stretches from the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland. The range continues toward Vienna in Austria, and east to the Adriatic Sea and into Slovenia. To the south it dips into northern Italy and to the north extends to the south border of Bavaria in Germany. In areas like Chiasso, Switzerland, and Neuschwanstein, Bavaria, the demarcation between the mountain range and the flatlands are clear; in other places such as Geneva, the demarcation is less clear. The countries with the greatest alpine territory are Switzerland, France, Austria and Italy.
The highest portion of the range is divided by the glacial trough of the Rhone valley, with the Pennine Alps from Mont Blanc to the Matterhorn and Monte Rosa on the southern side, and the Bernese Alps on the northern. The peaks in the easterly portion of the range, in Austria and Slovenia, are smaller than those in the central and western portions.
The variances in nomenclature in the region spanned by the Alps makes classification of the mountains and subregions difficult, but a general classification is that of the Eastern Alps and Western Alps with the divide between the two occurring in eastern Switzerland according to geologist Stefan Schmid, near the Splügen Pass.
The highest peaks of the Western Alps and Eastern Alps, respectively, are Mont Blanc, at 4,810 m (15,780 ft) and Piz Bernina at 4,049 metres (13,284 ft). The second-highest major peaks are Monte Rosa at 4,634 m (15,200 ft) and Ortler at 3,905 m (12,810 ft), respectively
Series of lower mountain ranges run parallel to the main chain of the Alps, including the French Prealps in France and the Jura Mountains in Switzerland and France. The secondary chain of the Alps follows the watershed from the Mediterranean Sea to the Wienerwald, passing over many of the highest and most well-known peaks in the Alps. From the Colle di Cadibona to Col de Tende it runs westwards, before turning to the northwest and then, near the Colle della Maddalena, to the north. Upon reaching the Swiss border, the line of the main chain heads approximately east-northeast, a heading it follows until its end near Vienna.
The Alps have been crossed for war and commerce, and by pilgrims, students and tourists. Crossing routes by road, train or foot are known as passes, and usually consist of depressions in the mountains in which a valley leads from the plains and hilly pre-mountainous zones. In the medieval period hospices were established by religious orders at the summits of many of the main passes. The most important passes are the Col de l'Iseran (the highest), the Brenner Pass, the Mont-Cenis, the Great St. Bernard Pass, the Col de Tende, the Gotthard Pass, the Semmering Pass, and the Stelvio Pass.
Crossing the Italian-Austrian border, the Brenner Pass separates the Ötztal Alps and Zillertal Alps and has been in use as a trading route since the 14th century. The lowest of the Alpine passes at 985 m (3,232 ft), the Semmering crosses from Lower Austria to Styria; since the 12th century when a hospice was built there it has seen continuous use. A railroad with a tunnel 1 mile (1.6 km) long was built along the route of the pass in the mid-19th century. With a summit of 2,469 m (8,100 ft), the Great St. Bernard Pass is one of the highest in the Alps, crossing the Italian-Swiss border east of the Pennine Alps along the flanks of Mont Blanc. The pass was used by Napoleon Bonaparte to cross 40,000 troops in 1800. The Saint Gotthard Pass crosses from Central Switzerland to Ticino; in the late 19th century the 14 km (9 mi) long Saint Gotthard Tunnel was built connecting Lucerne in Switzerland, with Milan in Italy. The Mont Cenis pass has been a major commercial road between Western Europe and Italy. Now the pass has been supplanted by the Fréjus Road and Rail tunnel. At 2,756 m (9,042 ft), the Stelvio Pass in northern Italy is one of the highest of the Alpine passes; the road was built in the 1820s. The highest pass in the alps is the col de l'Iseran in Savoy (France) at 2,770 m (9,088 ft).
Important geological concepts were established as naturalists began studying the rock formations of the Alps in the 18th century. In the mid-19th century the now defunct theory of geosynclines was used to explain the presence of "folded" mountain chains but by the mid-20th century the theory of plate tectonics became widely accepted.
The formation of the Alps (the Alpine orogeny) was an episodic process that began about 300 million years ago. In the Paleozoic Era the Pangaean supercontinent consisted of a single tectonic plate; it broke into separate plates during the Mesozoic Era and the Tethys sea developed between Laurasia and Gondwana during the Jurassic Period. The Tethys was later squeezed between colliding plates causing the formation of mountain ranges called the Alpide belt, from Gibraltar through the Himalayas to Indonesia—a process that began at the end of the Mesozoic and continues into the present. The formation of the Alps was a segment of this orogenic process, caused by the collision between the African and the Eurasian plates that began in the late Cretaceous Period.
Under extreme compressive stresses and pressure, marine sedimentary rocks were uplifted, creating characteristic recumbent folds, or nappes, and thrust faults. As the rising peaks underwent erosion, a layer of marine flysch sediments was deposited in the foreland basin, and the sediments became involved in younger nappes (folds) as the orogeny progressed. Coarse sediments from the continual uplift and erosion were later deposited in foreland areas as molasse. The molasse regions in Switzerland and Bavaria were well-developed and saw further upthrusting of flysch.
The Alpine orogeny occurred in ongoing cycles through to the Paleogene causing differences in nappe structures, with a late-stage orogeny causing the development of the Jura Mountains. A series of tectonic events in the Triassic, Jurassic and Cretaceous periods caused different paleogeographic regions. The Alps are subdivided by different lithology (rock composition) and nappe structure according to the orogenic events that affected them. The geological subdivision differentiates the Western, Eastern Alps and Southern Alps: the Helveticum in the north, the Penninicum and Austroalpine system in the center and, south of the Periadriatic Seam, the Southern Alpine system.
According to geologist Stefan Schmid, because the Western Alps underwent a metamorphic event in the Cenozoic Era while the Austroalpine peaks underwent an event in the Cretaceous Period, the two areas show distinct differences in nappe formations. Flysch deposits in the Southern Alps of Lombardy probably occurred in the Cretaceous or later.
Peaks in France, Italy and Switzerland lie in the "Houillière zone", which consists of basement with sediments from the Mesozoic Era. High "massifs" with external sedimentary cover are more common in the Western Alps and were affected by Neogene Period thin-skinned thrusting whereas the Eastern Alps have comparatively few high peaked massifs. Similarly the peaks in Switzerland extending to western Austria (Helvetic nappes) consist of thin-skinned sedimentary folding that detached from former basement rock.
In simple terms the structure of the Alps consists of layers of rock of European, African and oceanic (Tethyan) origin. The bottom nappe structure is of continental European origin, above which are stacked marine sediment nappes, topped off by nappes derived from the African plate. The Matterhorn is an example of the ongoing orogeny and shows evidence of great folding. The tip of the mountain consists of gneisses from the African plate; the base of the peak, below the glaciated area, consists of European basement rock. The sequence of Tethyan marine sediments and their oceanic basement is sandwiched between rock derived from the African and European plates.
The core regions of the Alpine orogenic belt have been folded and fractured in such a manner that erosion created the characteristic steep vertical peaks of the Swiss Alps that rise seemingly straight out of the foreland areas. Peaks such as Mont Blanc, the Matterhorn, and high peaks in the Pennine Alps, the Briançonnais, and Hohe Tauern consist of layers of rock from the various orogenies including exposures of basement rock.
The Union Internationale des Associations d'Alpinisme (UIAA) has defined a list of 82 "official" Alpine summits that reach at least 4,000 m (13,123 ft). The list includes not only mountains, but also subpeaks with little prominence that are considered important mountaineering objectives. Below are listed the 22 "four-thousanders" with at least 500 m (1,640 ft) of prominence.
While Mont Blanc was first climbed in 1786, most of the Alpine four-thousanders were climbed during the first half of the 19th century; the ascent of the Matterhorn in 1865 marked the end of the golden age of alpinism. Karl Blodig (1859–1956) was among the first to successfully climb all the major 4,000 m peaks. He completed his series of ascents in 1911.
The first British Mont Blanc ascent was in 1788; the first female ascent in 1819. By the mid-1850s Swiss mountaineers had ascended most of the peaks and were eagerly sought as mountain guides. Edward Whymper reached the top of the Matterhorn in 1865 (after seven attempts), and in 1938 the last of the six great north faces of the Alps was climbed with the first ascent of the Eiger Nordwand (north face of the Eiger).
The Alps are a source of minerals that have been mined for thousands of years. In the 8th to 6th centuries BC during the Hallstatt culture, Celtic tribes mined copper; later the Romans mined gold for coins in the Bad Gastein area. Erzberg in Styria furnishes high-quality iron ore for the steel industry. Crystals are found throughout much of the Alpine region such as cinnabar, amethyst, and quartz. The cinnabar deposits in Slovenia are a notable source of cinnabar pigments.
Alpine crystals have been studied and collected for hundreds of years, and began to be classified in the 18th century. Leonhard Euler studied the shapes of crystals, and by the 19th century crystal hunting was common in Alpine regions. David Friedrich Wiser amassed a collection of 8000 crystals that he studied and documented. In the 20th century Robert Parker wrote a well-known work about the rock crystals of the Swiss Alps; at the same period a commission was established to control and standardize the naming of Alpine minerals.
In the Miocene Epoch the mountains underwent severe erosion because of glaciation, which was noted in the mid-19th century by naturalist Louis Agassiz who presented a paper proclaiming the Alps were covered in ice at various intervals—a theory he formed when studying rocks near his Neuchâtel home which he believed originated to the west in the Bernese Oberland. Because of his work he came to be known as the "father of the ice-age concept" although other naturalists before him put forth similar ideas.
Agassiz studied glacier movement in the 1840s at the Unteraar Glacier where he found the glacier moved 100 m (328 ft) per year, more rapidly in the middle than at the edges. His work was continued by other scientists and now a permanent laboratory exists inside a glacier under the Jungfraujoch, devoted exclusively to the study of Alpine glaciers.
Glaciers pick up rocks and sediment with them as they flow. This causes erosion and the formation of valleys over time. The Inn valley is an example of a valley carved by glaciers during the ice ages with a typical terraced structure caused by erosion. Eroded rocks from the most recent ice age lie at the bottom of the valley while the top of the valley consists of erosion from earlier ice ages. Glacial valleys have characteristically steep walls (reliefs); valleys with lower reliefs and talus slopes are remnants of glacial troughs or previously infilled valleys. Moraines, piles of rock picked up during the movement of the glacier, accumulate at edges, center and the terminus of glaciers.
Alpine glaciers can be straight rivers of ice, long sweeping rivers, spread in a fan-like shape (Piedmont glaciers), and curtains of ice that hang from vertical slopes of the mountain peaks. The stress of the movement causes the ice to break and crack loudly, perhaps explaining why the mountains were believed to be home to dragons in the medieval period. The cracking creates unpredictable and dangerous crevasses, often invisible under new snowfall, which cause the greatest danger to mountaineers.
Glaciers end in ice caves (the Rhone Glacier), by trailing into a lake or river, or by shedding snowmelt on a meadow. Sometimes a piece of glacier will detach or break resulting in flooding, property damage and loss of life. In the 17th century about 2500 people were killed by an avalanche in a village on the French-Italian border; in the 19th century 120 homes in a village near Zermatt were destroyed by an avalanche.
High levels of precipitation cause the glaciers to descend to permafrost levels in some areas whereas in other, more arid regions, glaciers remain above about the 3,500 m (11,483 ft) level. The 1,817 square kilometres (702 sq mi) of the Alps covered by glaciers in 1876 had shrunk to 1,342 km2 (518 sq mi) by 1973, resulting in decreased river run-off levels. Forty percent of the glaciation in Austria has disappeared since 1850, and 30% of that in Switzerland.
The Alps provide lowland Europe with drinking water, irrigation, and hydroelectric power. Although the area is only about 11 percent of the surface area of Europe, the Alps provide up to 90 percent of water to lowland Europe, particularly to arid areas and during the summer months. Cities such as Milan depend on 80 percent of water from Alpine runoff. Water from the rivers is used in over 500 hydroelectricity power plants, generating as much as 2900 kilowatts of electricity.
Major European rivers flow from Switzerland, such as the Rhine, the Rhone, the Inn, the Ticino and the Po, all of which have headwaters in the Alps and flow into neighbouring countries, finally emptying into the North Sea, the Mediterranean Sea, the Adriatic Sea and the Black Sea. Other rivers such as the Danube have major tributaries flowing into them that originate in the Alps. The Rhone is second to the Nile as a freshwater source to the Mediterranean Sea; the river begins as glacial meltwater, flows into Lake Geneva, and from there to France where one of its uses is to cool nuclear power plants. The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country. Tributary valleys, some of which are complicated, channel water to the main valleys which can experience flooding during the snow melt season when rapid runoff causes debris torrents and swollen rivers.
The rivers form lakes, such as Lake Geneva, a crescent shaped lake crossing the Swiss border with Lausanne on the Swiss side and the town of Evian-les-Bains on the French side. In Germany, the medieval St. Bartholomew's chapel was built on the south side of the Königssee, accessible only by boat or by climbing over the abutting peaks.
Scientists have been studying the impact of climate change and water use. For example, each year more water is diverted from rivers for snowmaking in the ski resorts, the effect of which is yet unknown. Furthermore, the decrease of glaciated areas combined with a succession of winters with lower-than-expected precipitation may have a future impact on the rivers in the Alps as well as an effect on the water availability to the lowlands.
The Alps are a classic example of what happens when a temperate area at lower altitude gives way to higher-elevation terrain. Elevations around the world that have cold climates similar to those of the polar regions have been called Alpine. A rise from sea level into the upper regions of the atmosphere causes the temperature to decrease (see adiabatic lapse rate). The effect of mountain chains on prevailing winds is to carry warm air belonging to the lower region into an upper zone, where it expands in volume at the cost of a proportionate loss of temperature, often accompanied by precipitation in the form of snow or rain. The height of the Alps is sufficient to divide the weather patterns in Europe into a wet north and a dry south because moisture is sucked from the air as it flows over the high peaks.
The severe weather in the Alps has been studied since the 18th century; particularly the weather patterns such as the seasonal foehn wind. Numerous weather stations were placed in the mountains early in the early 20th century, providing continuous data for climatologists. Some of the valleys are quite arid such as the Aosta valley in Italy, the Maurienne in France, the Valais in Switzerland, and northern Tyrol.
The areas that are not arid and receive high precipitation experience periodic flooding from rapid snowmelt and runoff. The mean precipitation in the Alps ranges from a low of 2,600 mm (100 in) per year to 3,600 mm (140 in) per year, with the higher levels occurring at high altitudes. At altitudes between 1,000 and 3,000 m (3,281 and 9,843 ft), snowfall begins in November and accumulates through to April or May when the melt begins. Snow lines vary from 2,400 to 3,000 m (7,874 to 9,843 ft), above which the snow is permanent and the temperatures hover around the freezing point even July and August. High-water levels in streams and rivers peak in June and July when the snow is still melting at the higher altitudes.
The Alps are split into five climatic zones, each with different vegetation. The climate, plant life and animal life vary among the different sections or zones of the mountains. The lowest zone is the colline zone, which exists between 500 and 1,000 m (1,640 and 3,281 ft), depending on the location. The montane zone extends from 800 to 1,700 m (2,625 to 5,577 ft), followed by the sub-Alpine zone from 1,600 to 2,400 m (5,249 to 7,874 ft). The Alpine zone, extending from tree line to snow line, is followed by the glacial zone, which covers the glaciated areas of the mountain. Climatic conditions show variances within the same zones; for example, weather conditions at the head of a mountain valley, extending directly from the peaks, are colder and more severe than those at the mouth of a valley which tend to be less severe and receive less snowfall.
Various models of climate change have been projected into the 22nd century for the Alps, with an expectation that a trend toward increased temperatures will have an effect on snowfall, snowpack, glaciation, and river runoff.
Thirteen thousand species of plants have been identified in the Alpine regions. Alpine plants are grouped by habitat and soil type which can be limestone or non-calcerous. The habitats range from meadows, bogs, woodland (deciduous and coniferous) areas to soilless scree and moraines, and rock faces and ridges. A natural vegetation limit with altitude is given by the presence of the chief deciduous trees—oak, beech, ash and sycamore maple. These do not reach exactly to the same elevation, nor are they often found growing together; but their upper limit corresponds accurately enough to the change from a temperate to a colder climate that is further proved by a change in the presence of wild herbaceous vegetation. This limit usually lies about 1,200 m (3,940 ft) above the sea on the north side of the Alps, but on the southern slopes it often rises to 1,500 m (4,920 ft), sometimes even to 1,700 m (5,580 ft).
Above the forestry, there is often a band of short pine trees (Pinus mugo), which is in turn superseded by Alpenrosen, dwarf shrubs, typically Rhododendron ferrugineum (on acid soils) or Rhododendron hirsutum (on alkaline soils). Although the Alpenrose prefers acidic soil, the plants are found throughout the region. Above the tree line is the area defined as "alpine" where in the alpine meadow plants are found that have adapted well to harsh conditions of cold temperatures, aridity, and high altitudes. The alpine area fluctuates greatly because of regional fluctuations in tree lines.
Alpine plants such the Alpine gentian grow in abundance in areas such as the meadows above the Lauterbrunnental. Gentians are named after the Illyrian king Gentius, and 40 species of the early-spring blooming flower grow in the Alps, in a range of 1,500 to 2,400 m (4,921 to 7,874 ft). Writing about the gentians in Switzerland D. H. Lawrence described them as "darkening the day-time, torch-like with the smoking blueness of Pluto's gloom." Gentians tend to "appear" repeatedly as the spring blooming takes place at progressively later dates, moving from the lower altitude to the higher altitude meadows where the snow melts much later than in the valleys. On the highest rocky ledges the spring flowers bloom in the summer.
At these higher altitudes, the plants tend to form isolated cushions. In the Alps, several species of flowering plants have been recorded above 4,000 m (13,120 ft), including Ranunculus glacialis, Androsace alpina and Saxifraga biflora. The Eritrichium nanum, commonly known as the King of the Alps, is the most elusive of the alpine flowers, growing on rocky ridges at 2,600 to 3,750 m (8,530 to 12,303 ft). Perhaps the best known of the alpine plants is the Edelweiss which grows in rocky areas and can be found at altitudes as low as 1,400 m (4,593 ft) and as high as 3,400 m (11,155 ft). The plants that grow at the highest altitudes have adapted to conditions by specialization such as growing in rock screes that give protection from winds.
The extreme and stressful climatic conditions give way to the growth of plant species with secondary metabolites important for medicinal purposes. Origanum vulgare, Prunella vulgaris, Solanum nigrum and Urtica dioica are some of the more useful medicinal species found in the Alps.
Human interference has nearly exterminated the trees in many areas, and, except for the beech forests of the Austrian Alps, forests of deciduous trees are rarely found after the extreme deforestation between the 17th and 19th centuries. The vegetation has changed since the second half of the 20th century, as the high alpine meadows cease to be harvested for hay or used for grazing which eventually might result in a regrowth of forest. In some areas the modern practice of building ski runs by mechanical means has destroyed the underlying tundra from which the plant life cannot recover during the non-skiing months, whereas areas that still practice a natural piste type of ski slope building preserve the fragile underlayers.
The Alps are a habitat for 30,000 species of wildlife, ranging from the tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.
The largest mammal to live in the highest altitudes are the alpine ibex, which have been sighted as high as 3,000 m (9,843 ft). The ibex live in caves and descend to eat the succulent alpine grasses. Classified as antelopes, chamois are smaller than ibex and found throughout the Alps, living above the tree line and are common in the entire alpine range. Areas of the eastern Alps are still home to brown bears. In Switzerland the canton of Bern was named for the bears but the last bear is recorded as having been killed in 1792 above Kleine Scheidegg by three hunters from Grindelwald.
Many rodents such as voles live underground. Marmots live almost exclusively above the tree line as high as 2,700 m (8,858 ft). They hibernate in large groups to provide warmth, and can be found in all areas of the Alps, in large colonies they build beneath the alpine pastures. Golden eagles and bearded vultures are the largest birds to be found in the Alps; they nest high on rocky ledges and can be found at altitudes of 2,400 m (7,874 ft). The most common bird is the alpine chough which can be found scavenging at climber's huts or at the Jungfraujoch, a high altitude tourist destination.
Reptiles such as adders and vipers live up to the snow line; because they cannot bear the cold temperatures they hibernate underground and soak up the warmth on rocky ledges. The high-altitude Alpine salamanders have adapted to living above the snow line by giving birth to fully developed young rather than laying eggs. Brown trout can be found in the streams up to the snow line. Molluscs such as the wood snail live up the snow line. Popularly gathered as food, the snails are now protected.
A number of species of moths live in the Alps, some of which are believed to have evolved in the same habitat up to 120 million years ago, long before the Alps were created. Blue moths can commonly be seen drinking from the snow melt; some species of blue moths fly as high as 1,800 m (5,906 ft). The butterflies tend to be large, such as those from the swallowtail Parnassius family, with a habitat that ranges to 1,800 m (5,906 ft). Twelve species of beetles have habitats up to the snow line; the most beautiful and formerly collected for its colours but now protected is the Rosalia alpina. Spiders, such as the large wolf spider, live above the snow line and can be seen as high as 400 m (1,312 ft). Scorpions can be found in the Italian Alps.
Some of the species of moths and insects show evidence of having been indigenous to the area from as long ago as the Alpine orogeny. In Emosson in Valais, Switzerland, dinosaur tracks were found in the 1970s, dating probably from the Triassic Period.
About 10,000 years ago, when the ice melted after the last glacial period, late Paleolithic communities were established along the lake shores and in cave systems. Evidence of human habitation has been found in caves near Vercors, close to Grenoble; in Austria the Mondsee culture shows evidence of houses built on piles to keep them dry. Standing stones have been found in Alpine areas of France and Italy. The rock drawings in Valcamonica are more than 5000 years old; more than 200,000 drawings and etchings have been identified at the site.
In 1991 a mummy of a neolithic body, known as Ötzi the Iceman, was discovered by hikers on the Similaun glacier. His clothing and gear indicate that he lived in an alpine farming community, while the location and manner of his death - an arrowhead was discovered in his shoulder - suggests he was travelling from one place to another. Analysis of the mitochondrial DNA of Ötzi, has shown that he belongs to the K1 subclade which cannot be categorized into any of the three modern branches of that subclade. The new subclade has provisionally been named K1ö for Ötzi.
Celtic tribes settled in Switzerland between 1000 to 1500 BC. The Raetians lived in the eastern regions, while the west was occupied by the Helvetii and the Allobrogi settled in the Rhone valley and in Savoy. Among the many substances Celtic tribes mined was salt in areas such as Salzburg in Austria where evidence of the Hallstatt culture was found by a mine manager in the 19th century. By the 6th century BC the La Tène culture was well established in the region, and became known for high quality decorated weapons and jewelry. The Celts were the most widespread of the mountain tribes—they had warriors that were strong, tall and fair skinned skilled with iron weapons, which gave them an advantage in warfare.
During the Second Punic War in 218 BC, the Carthaginian general Hannibal probably crossed the Alps with an army numbering 38,000 infantry, 8,000 cavalry, and 37 war elephants. This was one of the most celebrated achievements of any military force in ancient warfare, although no evidence exists of the actual crossing or the place of crossing. The Romans, however, had built roads along the mountain passes, which continued to be used through the medieval period to cross the mountains and Roman road markers can still be found on the mountain passes.
The Roman expansion brought the defeat of the Allobrogi in 121 BC and during the Gallic Wars in 58 BC Julius Caesar overcame the Helvetii. The Rhaetians continued to resist but were eventually conquered when the Romans turned northward to the Danube valley in Austria and defeated the Brigantes. The Romans built settlements in the Alps; towns such as Aosta (named for Augustus) in Italy, Martigny and Lausanne in Switzerland, and Partenkirchen in Bavaria show remains of Roman baths, villas, arenas and temples. Much of the Alpine region was gradually settled by Germanic tribes, (Lombards, Alemanni, Bavarii, and Franks) from the 6th to the 13th centuries mixing with the local Celtic tribes.
Christianity was established in the region by the Romans, and saw the establishment of monasteries and churches in the high regions. The Frankish expansion of the Carolingian Empire and the Bavarian expansion in the eastern Alps introduced feudalism and the building of castles to support the growing number of dukedoms and kingdoms. Castello del Buonconsiglio in Trento, Italy, still has intricate frescoes, excellent examples of Gothic art, in a tower room. In Switzerland, Château de Chillon is preserved as an example of medieval architecture.
Much of the medieval period was a time of power struggles between competing dynasties such as the House of Savoy, the Visconti in northern Italy and the House of Habsburg in Austria and Slovenia. In 1291 to protect themselves from incursions by the Habsburgs, four cantons in the middle of Switzerland drew up a charter that is considered to be a declaration of independence from neighboring kingdoms. After a series of battles fought in the 13th, 14th and 15th centuries, more cantons joined the confederacy and by the 16th century Switzerland was well-established as a separate state.
During the Napoleonic Wars in the late 18th century and early 19th century, Napoleon annexed territory formerly controlled by the Habsburgs and Savoys. In 1798 he established the Helvetic Republic in Switzerland; two years later he led an army across the St. Bernard pass and conquered almost all of the Alpine regions.
After the fall of Napoléon, many alpine countries developed heavy protections to prevent any new invasion. Thus, Savoy built a series of fortifications in the Maurienne valley in order to protect the major alpine passes, such as the col du Mont-Cenis that was even crossed by, Charlemagne and his father to defeat the Lombarts. The later indeed became very popular after the construction of a paved road ordered by Napoléon Bonaparte. The Barrière de l'Esseillon is a serie of forts with heavy batteries, built on a cliff with a perfect view on the valley, a gorge on one side and steep mountains on the other side.
In the 19th century, the monasteries built in the high Alps during the medieval period to shelter travelers and as places of pilgrimage, became tourist destinations. The Benedictines had built monasteries in Lucerne, Switzerland, and Oberammergau; the Cistercians in the Tyrol and at Lake Constance; and the Augustinians had abbeys in the Savoy and one in the center of Interlaken, Switzerland. The Great St Bernard Hospice, built in the 9th or 10th centuries, at the summit of the Great Saint Bernard Pass was shelter for travelers and place for pilgrims since its inception; by the 19th century it became a tourist attraction with notable visitors such as author Charles Dickens and mountaineer Edward Whymper.
Radiocarbon dated charcoal placed around 50,000 years ago was found in the Drachloch (Dragon's Hole) cave above the village of Vattis in the canton of St. Gallen, proving that the high peaks were visited by prehistoric people. Seven bear skulls from the cave may have been buried by the same prehistoric people. The peaks, however, were mostly ignored except for a few notable examples, and long left to the exclusive attention of the people of the adjoining valleys. The mountain peaks were seen as terrifying, the abode of dragons and demons, to the point that people blindfolded themselves to cross the Alpine passes. The glaciers remained a mystery and many still believed the highest areas to be inhabited by dragons.
Charles VII of France ordered his chamberlain to climb Mont Aiguille in 1356. The knight reached the summit of Rocciamelone where he left a bronze triptych of three crosses, a feat which he conducted with the use of ladders to traverse the ice. In 1492 Antoine de Ville climbed Mont Aiguille, without reaching the summit, an experience he described as "horrifying and terrifying." Leonardo da Vinci was fascinated by variations of light in the higher altitudes, and climbed a mountain—scholars are uncertain which one; some believe it may have been Monte Rosa. From his description of a "blue like that of a gentian" sky it is thought that he reached a significantly high altitude. In the 18th century four Chamonix man almost made the summit of Mont Blanc but were overcome by altitude sickness and snowblindness.
Conrad Gessner was the first naturalist to ascend the mountains in the 16th century, to study them, writing that in the mountains he found the "theatre of the Lord". By the 19th century more naturalists began to arrive to explore, study and conquer the high peaks; they were followed by artists, writers and painters. Two men who first explored the regions of ice and snow were Horace-Bénédict de Saussure (1740–1799) in the Pennine Alps, and the Benedictine monk of Disentis Placidus a Spescha (1752–1833). Born in Geneva, Saussure was enamored with the mountains from an early age; he left a law career to become a naturalist and spent many years trekking through the Bernese Oberland, the Savoy, the Piedmont and Valais, studying the glaciers and the geology, as he became an early proponent of the theory of rock upheaval. Saussure, in 1787, was a member of the third ascent of Mont Blanc—today the summits of all the peaks have been climbed.
Jean-Jacques Rousseau was the first of many to present the Alps as a place of allure and beauty, banishing the prevalent conception of the mountains as a hellish wasteland inhabited by demons. Rousseau's conception of alpine purity was later emphasized with the publication of Albrecht von Haller's poem Die Alpen that described the mountains as an area of mythical purity. Late in the 18th century the first wave of Romantics such as Goethe and Turner came to admire the scenery; Wordsworth visited the area in 1790, writing of his experiences in The Prelude. Schiller later wrote the play William Tell romanticising Swiss independence. After the end of the Napoleonic Wars, the Alpine countries began to see an influx of poets, artists, and musicians, as visitors came to experience the sublime effects of monumental nature.
In 1816 Byron, Percy Bysshe Shelley and his wife Mary Shelley visited Geneva and all three were inspired by the scenery in their writings. During these visits Shelley wrote the poem "Mont Blanc", Byron wrote "The Prisoner of Chillon" and the dramatic poem Manfred, and Mary Shelley, who found the scenery overwhelming, conceived the idea for the novel Frankenstein in her villa on the shores of Lake Geneva in the midst of a thunderstorm. When Coleridge travelled to Chamonix, he declaimed, in defiance of Shelley, who had signed himself "Atheos" in the guestbook of the Hotel de Londres near Montenvers, "Who would be, who could be an atheist in this valley of wonders". By the mid-19th century scientists began to arrive en masse to study the geology and ecology of the region.
Austrian-born Adolf Hitler had a lifelong romantic fascination with the Alps and by the 1930s established a home in the Obersalzberg region outside of Berchtesgaden. His first visit to the area was in 1923 and he maintained a strong tie there until the end of his life. At the end of World War II the US Army occupied Obersalzberg, to prevent Hitler from retreating with the Wehrmacht into the mountains.
By 1940 the Third Reich had occupied many of the Alpine countries. Austria underwent a political coup that made it part of the Third Reich; France had been invaded and Italy was a fascist regime. Switzerland was the only country to luckily avoid invasion. The Swiss Confederate mobilized its troops—the country follows the doctrine of "armed neutrality" with all males required to have military training—a number that General Eisenhower estimated to be about 850,000. The Swiss commanders wired the infrastructure leading into the country, and threatening to destroy bridges, railway tunnels and passes in the event of a Nazi invasion, and then they retreated to the heart of the mountain peaks where conditions were harsher and a military invasion would involve difficult and protracted battles.
Ski troops were trained for the war, and battles were waged in mountainous areas such as the battle at Riva Ridge in Italy, where the American 10th Mountain Division encountered heavy resistance in February 1945. At the end of the war, a substantial amount of Nazi plunder was found stored in Austria, where Hitler had hoped to retreat as the war drew to a close. The salt mines surrounding the Altaussee area, where American troops found 75 kilos of gold coins stored in a single mine, were used to store looted art, jewels, and currency; vast quantities of looted art were found and returned to the owners.
The population of the region is 14 million spread across eight countries. On the rim of the mountains, on the plateaus and the plains the economy consists of manufacturing and service jobs whereas in the higher altitudes and in the mountains farming is still essential to the economy. Farming and forestry continue to be mainstays of Alpine culture, industries that provide for export to the cities and maintain the mountain ecology.
Much of the Alpine culture is unchanged since the medieval period when skills that guaranteed survival in the mountain valleys and in the highest villages became mainstays, leading to strong traditions of carpentry, woodcarving, baking and pastry-making, and cheesemaking.
Farming had been a traditional occupation for centuries, although it became less dominant in the 20th century with the advent of tourism. Grazing and pasture land are limited because of the steep and rocky topography of the Alps. In mid-June cows are moved to the highest pastures close to the snowline, where they are watched by herdsmen who stay in the high altitudes often living in stone huts or wooden barns during the summers. Villagers celebrate the day the cows are herded up to the pastures and again when they return in mid-September. The Alpanschluss or Désalpes ("coming down from the alps") is celebrated by decorating the cows with garlands and enormous cowbells while the farmers dress in traditional costumes.
Cheesemaking is an ancient tradition in most Alpine countries. A wheel of cheese from the Emmental in Switzerland can weigh up to 45 kg (100 lb), and the Beaufort in Savoy can weight up to 70 kilograms (150 lb). Owners of the cows traditionally receive from the cheesemakers a portion in relation to the proportion of the cows' milk from the summer months in the high alps. Haymaking is an important farming activity in mountain villages which has become somewhat mechanized in recent years, although the slopes are so steep that usually scythes are necessary to cut the grass. Hay is normally brought in twice a year, often also on festival days. Alpine festivals vary from country to country and often include the display of local costumes such as dirndl and trachten, the playing of Alpenhorns, wrestling matches, some pagan traditions such as Walpurgis Night and, in many areas, Carnival is celebrated before Lent.
In the high villages people live in homes built according to medieval designs that withstand cold winters. The kitchen is separated from the living area (called the stube, the area of the home heated by a stove), and second-floor bedrooms benefit from rising heat. The typical Swiss chalet originated in the Bernese Oberland. Chalets often face south or downhill, and are built of solid wood, with a steeply gabled roof to allow accumulated snow to slide off easily. Stairs leading to upper levels are sometimes built on the outside, and balconies are sometimes enclosed.
Food is passed from the kitchen to the stube, where the dining room table is placed. Some meals are communal, such as fondue, where a pot is set in the middle of the table for each person to dip into. Other meals are still served in a traditional manner on carved wooden plates. Furniture has been traditionally elaborately carved and in many Alpine countries carpentry skills are passed from generation to generation.
Roofs are traditionally constructed from Alpine rocks such as pieces of schist, gneiss or slate. Such chalets are typically found in the higher parts of the valleys, as in the Maurienne valley in Savoy, where the amount of snow during the cold months is important. The inclination of the roof cannot exceed 40%, allowing the snow to stay on top, thereby functioning as insulation from the cold. In the lower areas where the forests are widespread, wooden tiles are traditionally used. Commonly made of Norway spruce, they are called "tavaillon". The Alpine regions are multicultural and linguistically diverse. Dialects are common, and vary from valley to valley and region to region. In the Slavic Alps alone 19 dialects have been identified. Some of the French dialects spoken in the French, Swiss and Italian alps of Aosta Valley derive from Arpitan, while the southern part of the western range is related to Old Provençal; the German dialects derive from Germanic tribal languages. Romansh, spoken by two percent of the population in southeast Switzerland, is an ancient Rhaeto-Romanic language derived from Latin, remnants of ancient Celtic languages and perhaps Etruscan.
At present the Alps are one of the more popular tourist destinations in the world with many resorts such Oberstdorf, in Bavaria, Saalbach in Austria, Davos in Switzerland, Chamonix in France, and Cortina d'Ampezzo in Italy recording more than a million annual visitors. With over 120 million visitors a year tourism is integral to the Alpine economy with much it coming from winter sports although summer visitors are an important component of the tourism industry.
The tourism industry began in the early 19th century when foreigners visited the Alps, traveled to the bases of the mountains to enjoy the scenery, and stayed at the spa-resorts. Large hotels were built during the Belle Époque; cog-railways, built early in the 20th century, brought tourists to ever higher elevations, with the Jungfraubahn terminating at the Jungfraujoch, well above the eternal snow-line, after going through a tunnel in Eiger. During this period winter sports were slowly introduced: in 1882 the first figure skating championship was held in St. Moritz, and downhill skiing became a popular sport with English visitors early in the 20th century, as the first ski-lift was installed in 1908 above Grindelwald.
In the first half of the 20th century the Olympic Winter Games were held three times in Alpine venues: the 1924 Winter Olympics in Chamonix, France; the 1928 Winter Olympics in St. Moritz, Switzerland; and the 1936 Winter Olympics in Garmisch-Partenkirchen, Germany. During World War II the winter games were canceled but after that time the Winter Games have been held in St. Moritz (1948), Cortina d'Ampezzo (1956), Innsbruck, Austria (1964 and 1976), Grenoble, France, (1968), Albertville, France, (1992), and Torino (2006). In 1930 the Lauberhorn Rennen (Lauberhorn Race), was run for the first time on the Lauberhorn above Wengen; the equally demanding Hahnenkamm was first run in the same year in Kitzbühl, Austria. Both races continue to be held each January on successive weekends. The Lauberhorn is the more strenuous downhill race at 4.5 km (2.8 mi) and poses danger to racers who reach 130 km/h (81 mph) within seconds of leaving the start gate.
During the post-World War I period ski-lifts were built in Swiss and Austrian towns to accommodate winter visitors, but summer tourism continued to be important; by the mid-20th century the popularity of downhill skiing increased greatly as it became more accessible and in the 1970s several new villages were built in France devoted almost exclusively to skiing, such as Les Menuires. Until this point Austria and Switzerland had been the traditional and more popular destinations for winter sports, but by the end of the 20th century and into the early 21st century, France, Italy and the Tyrol began to see increases in winter visitors. From 1980 to the present, ski-lifts have been modernized and snow-making machines installed at many resorts, leading to concerns regarding the loss of traditional Alpine culture and questions regarding sustainable development as the winter ski industry continues to develop quickly and the number of summer tourists decline.
The region is serviced by 4,200 km (2,600 mi) of roads used by 6 million vehicles. Train travel is well established in the Alps, with, for instance 120 km (75 mi) of track for every 1,000 km2 (390 sq mi) in a country such as Switzerland. Most of Europe's highest railways are located there. Moreover, plans are underway to build a 57 km (35 mi)-long sub-alpine tunnel connecting the older Lötschberg and Gotthard tunnels built in the 19th century.
Some high mountain villages, such as Avoriaz (in France), Wengen, and Zermatt (in Switzerland) are accessible only by cable car or cog-rail trains, and are car free. Other villages in the Alps are considering becoming car free zones or limiting the number of cars for reasons of sustainability of the fragile Alpine terrain.
The lower regions and larger towns of the Alps are well-served by motorways and main roads, but higher mountain passes and byroads, which are amongst the highest in Europe, can be treacherous even in summer due to steep slopes. Many passes are closed in winter. A multitude of airports around the Alps (and some within), as well as long-distance rail links from all neighbouring countries, afford large numbers of travellers easy access from abroad.
The term Muslim world, also known as Islamic world and the Ummah (Arabic: أمة‎, meaning "nation" or "community") has different meanings. In a religious sense, the Islamic Ummah refers to those who adhere to the teachings of Islam, referred to as Muslims. In a cultural sense, the Muslim Ummah refers to Islamic civilization, exclusive of non-Muslims living in that civilization. In a modern geopolitical sense, the term "Islamic Nation" usually refers collectively to Muslim-majority countries, states, districts, or towns.
The Islamic Golden Age coincided with the Middle Ages in the Muslim world, starting with the rise of Islam and establishment of the first Islamic state in 622. The end of the age is variously given as 1258 with the Mongolian Sack of Baghdad, or 1492 with the completion of the Christian Reconquista of the Emirate of Granada in Al-Andalus, Iberian Peninsula. During the reign of the Abbasid caliph Harun ar-Rashid (786 to 809), the legendary House of Wisdom was inaugurated in Baghdad where scholars from various parts of the world sought to translate and gather all the known world's knowledge into Arabic. The Abbasids were influenced by the Quranic injunctions and hadiths, such as "the ink of a scholar is more holy than the blood of a martyr," that stressed the value of knowledge. The major Islamic capital cities of Baghdad, Cairo, and Córdoba became the main intellectual centers for science, philosophy, medicine, and education. During this period, the Muslim world was a collection of cultures; they drew together and advanced the knowledge gained from the ancient Greek, Roman, Persian, Chinese, Indian, Egyptian, and Phoenician civilizations.
Between the 8th and 18th centuries, the use of glazed ceramics was prevalent in Islamic art, usually assuming the form of elaborate pottery. Tin-opacified glazing was one of the earliest new technologies developed by the Islamic potters. The first Islamic opaque glazes can be found as blue-painted ware in Basra, dating to around the 8th century. Another contribution was the development of stone-paste ceramics, originating from 9th century Iraq. Other centers for innovative ceramic pottery in the Old world included Fustat (from 975 to 1075), Damascus (from 1100 to around 1600) and Tabriz (from 1470 to 1550).
The best known work of fiction from the Islamic world is One Thousand and One Nights (In Persian: hezār-o-yek šab > Arabic: ʔalf-layl-at-wa-l’-layla= One thousand Night and (one) Night) or *Arabian Nights, a name invented by early Western translators, which is a compilation of folk tales from Sanskrit, Persian, and later Arabian fables. The original concept is derived from a pre-Islamic Persian prototype Hezār Afsān (Thousand Fables) that relied on particular Indian elements. It reached its final form by the 14th century; the number and type of tales have varied from one manuscript to another. All Arabian fantasy tales tend to be called Arabian Nights stories when translated into English, regardless of whether they appear in The Book of One Thousand and One Nights or not. This work has been very influential in the West since it was translated in the 18th century, first by Antoine Galland. Imitations were written, especially in France. Various characters from this epic have themselves become cultural icons in Western culture, such as Aladdin, Sinbad the Sailor and Ali Baba.
A famous example of Arabic poetry and Persian poetry on romance (love) is Layla and Majnun, dating back to the Umayyad era in the 7th century. It is a tragic story of undying love much like the later Romeo and Juliet, which was itself said to have been inspired by a Latin version of Layla and Majnun to an extent. Ferdowsi's Shahnameh, the national epic of Iran, is a mythical and heroic retelling of Persian history. Amir Arsalan was also a popular mythical Persian story, which has influenced some modern works of fantasy fiction, such as The Heroic Legend of Arslan.
Ibn Tufail (Abubacer) and Ibn al-Nafis were pioneers of the philosophical novel. Ibn Tufail wrote the first Arabic novel Hayy ibn Yaqdhan (Philosophus Autodidactus) as a response to Al-Ghazali's The Incoherence of the Philosophers, and then Ibn al-Nafis also wrote a novel Theologus Autodidactus as a response to Ibn Tufail's Philosophus Autodidactus. Both of these narratives had protagonists (Hayy in Philosophus Autodidactus and Kamil in Theologus Autodidactus) who were autodidactic feral children living in seclusion on a desert island, both being the earliest examples of a desert island story. However, while Hayy lives alone with animals on the desert island for the rest of the story in Philosophus Autodidactus, the story of Kamil extends beyond the desert island setting in Theologus Autodidactus, developing into the earliest known coming of age plot and eventually becoming the first example of a science fiction novel.
Theologus Autodidactus, written by the Arabian polymath Ibn al-Nafis (1213–1288), is the first example of a science fiction novel. It deals with various science fiction elements such as spontaneous generation, futurology, the end of the world and doomsday, resurrection, and the afterlife. Rather than giving supernatural or mythological explanations for these events, Ibn al-Nafis attempted to explain these plot elements using the scientific knowledge of biology, astronomy, cosmology and geology known in his time. Ibn al-Nafis' fiction explained Islamic religious teachings via science and Islamic philosophy.
A Latin translation of Ibn Tufail's work, Philosophus Autodidactus, first appeared in 1671, prepared by Edward Pococke the Younger, followed by an English translation by Simon Ockley in 1708, as well as German and Dutch translations. These translations might have later inspired Daniel Defoe to write Robinson Crusoe, regarded as the first novel in English. Philosophus Autodidactus, continuing the thoughts of philosophers such as Aristotle from earlier ages, inspired Robert Boyle to write his own philosophical novel set on an island, The Aspiring Naturalist.
Dante Alighieri's Divine Comedy, derived features of and episodes about Bolgia from Arabic works on Islamic eschatology: the Hadith and the Kitab al-Miraj (translated into Latin in 1264 or shortly before as Liber Scale Machometi) concerning the ascension to Heaven of Muhammad, and the spiritual writings of Ibn Arabi. The Moors also had a noticeable influence on the works of George Peele and William Shakespeare. Some of their works featured Moorish characters, such as Peele's The Battle of Alcazar and Shakespeare's The Merchant of Venice, Titus Andronicus and Othello, which featured a Moorish Othello as its title character. These works are said to have been inspired by several Moorish delegations from Morocco to Elizabethan England at the beginning of the 17th century.
One of the common definitions for "Islamic philosophy" is "the style of philosophy produced within the framework of Islamic culture." Islamic philosophy, in this definition is neither necessarily concerned with religious issues, nor is exclusively produced by Muslims. The Persian scholar Ibn Sina (Avicenna) (980–1037) had more than 450 books attributed to him. His writings were concerned with various subjects, most notably philosophy and medicine. His medical textbook The Canon of Medicine was used as the standard text in European universities for centuries. He also wrote The Book of Healing, an influential scientific and philosophical encyclopedia.
Yet another influential philosopher who had an influence on modern philosophy was Ibn Tufail. His philosophical novel, Hayy ibn Yaqdha, translated into Latin as Philosophus Autodidactus in 1671, developed the themes of empiricism, tabula rasa, nature versus nurture, condition of possibility, materialism, and Molyneux's problem. European scholars and writers influenced by this novel include John Locke, Gottfried Leibniz, Melchisédech Thévenot, John Wallis, Christiaan Huygens, George Keith, Robert Barclay, the Quakers, and Samuel Hartlib.
Other influential Muslim philosophers include al-Jahiz, a pioneer in evolutionary thought; Ibn al-Haytham (Alhazen), a pioneer of phenomenology and the philosophy of science and a critic of Aristotelian natural philosophy and Aristotle's concept of place (topos); Al-Biruni, a critic of Aristotelian natural philosophy; Ibn Tufail and Ibn al-Nafis, pioneers of the philosophical novel; Shahab al-Din Suhrawardi, founder of Illuminationist philosophy; Fakhr al-Din al-Razi, a critic of Aristotelian logic and a pioneer of inductive logic; and Ibn Khaldun, a pioneer in the philosophy of history.
Muslim scientists contributed to advances in the sciences. They placed far greater emphasis on experiment than had the Greeks. This led to an early scientific method being developed in the Muslim world, where progress in methodology was made, beginning with the experiments of Ibn al-Haytham (Alhazen) on optics from circa 1000, in his Book of Optics. The most important development of the scientific method was the use of experiments to distinguish between competing scientific theories set within a generally empirical orientation, which began among Muslim scientists. Ibn al-Haytham is also regarded as the father of optics, especially for his empirical proof of the intromission theory of light. Some have also described Ibn al-Haytham as the "first scientist." al-Khwarzimi's invented the log base systems that are being used today, he also contributed theorems in trigonometry as well as limits. Recent studies show that it is very likely that the Medieval Muslim artists were aware of advanced decagonal quasicrystal geometry (discovered half a millennium later in the 1970s and 1980s in the West) and used it in intricate decorative tilework in the architecture.
Muslim physicians contributed to the field of medicine, including the subjects of anatomy and physiology: such as in the 15th century Persian work by Mansur ibn Muhammad ibn al-Faqih Ilyas entitled Tashrih al-badan (Anatomy of the body) which contained comprehensive diagrams of the body's structural, nervous and circulatory systems; or in the work of the Egyptian physician Ibn al-Nafis, who proposed the theory of pulmonary circulation. Avicenna's The Canon of Medicine remained an authoritative medical textbook in Europe until the 18th century. Abu al-Qasim al-Zahrawi (also known as Abulcasis) contributed to the discipline of medical surgery with his Kitab al-Tasrif ("Book of Concessions"), a medical encyclopedia which was later translated to Latin and used in European and Muslim medical schools for centuries. Other medical advancements came in the fields of pharmacology and pharmacy.
In astronomy, Muḥammad ibn Jābir al-Ḥarrānī al-Battānī improved the precision of the measurement of the precession of the Earth's axis. The corrections made to the geocentric model by al-Battani, Averroes, Nasir al-Din al-Tusi, Mu'ayyad al-Din al-'Urdi and Ibn al-Shatir were later incorporated into the Copernican heliocentric model. Heliocentric theories were also discussed by several other Muslim astronomers such as Al-Biruni, Al-Sijzi, Qotb al-Din Shirazi, and Najm al-Dīn al-Qazwīnī al-Kātibī. The astrolabe, though originally developed by the Greeks, was perfected by Islamic astronomers and engineers, and was subsequently brought to Europe.
Advances were made in irrigation and farming, using new technology such as the windmill. Crops such as almonds and citrus fruit were brought to Europe through al-Andalus, and sugar cultivation was gradually adopted by the Europeans. Arab merchants dominated trade in the Indian Ocean until the arrival of the Portuguese in the 16th century. Hormuz was an important center for this trade. There was also a dense network of trade routes in the Mediterranean, along which Muslim countries traded with each other and with European powers such as Venice, Genoa and Catalonia. The Silk Road crossing Central Asia passed through Muslim states between China and Europe.
Muslim engineers in the Islamic world made a number of innovative industrial uses of hydropower, and early industrial uses of tidal power and wind power, fossil fuels such as petroleum, and early large factory complexes (tiraz in Arabic). The industrial uses of watermills in the Islamic world date back to the 7th century, while horizontal-wheeled and vertical-wheeled water mills were both in widespread use since at least the 9th century. A variety of industrial mills were being employed in the Islamic world, including early fulling mills, gristmills, hullers, sawmills, ship mills, stamp mills, steel mills, sugar mills, tide mills and windmills. By the 11th century, every province throughout the Islamic world had these industrial mills in operation, from al-Andalus and North Africa to the Middle East and Central Asia. Muslim engineers also invented crankshafts and water turbines, employed gears in mills and water-raising machines, and pioneered the use of dams as a source of water power, used to provide additional power to watermills and water-raising machines. Such advances made it possible for industrial tasks that were previously driven by manual labour in ancient times to be mechanized and driven by machinery instead in the medieval Islamic world. The transfer of these technologies to medieval Europe had an influence on the Industrial Revolution.
More than 20% of the world's population is Muslim. Current estimates conclude that the number of Muslims in the world is around 1,5 billion. Muslims are the majority in 49 countries, they speak hundreds of languages and come from diverse ethnic backgrounds. Major languages spoken by Muslims include Arabic, Urdu, Bengali, Punjabi, Malay, Javanese, Sundanese, Swahili, Hausa, Fula, Berber, Tuareg, Somali, Albanian, Bosnian, Russian, Turkish, Azeri, Kazakh, Uzbek, Tatar, Persian, Kurdish, Pashto, Balochi, Sindhi and Kashmiri, among many others.
The two main denominations of Islam are the Sunni and Shia sects. They differ primarily upon of how the life of the ummah ("faithful") should be governed, and the role of the imam. These two main differences stem from the understanding of which hadith are to interpret the Quran. Sunnis believe the true political successor of the Prophet in Sunnah is based on ٍShura (consultation) at the Saqifah which selected Abu Bakr, father of the Prophet's favourite wife, 'A'ishah, to lead the Islamic community while the religious succession ceased to exist on account of finality of Prophethood. Shia on the other hand believe that the true political as well as religious successor is Ali ibn Abi Talib, husband of the Prophet's daughter Fatimah (designated by the Prophet).
Literacy rate in the Muslim world varies. Some members such as Kuwait, Kazakhstan, Tajikistan and Turkmenistan have over 97% literacy rates, whereas literacy rates are the lowest in Mali, Afghanistan, Chad and parts of Africa. In 2015, the International Islamic News Agency reported that nearly 37% of the population of the Muslim world is unable to read or write, basing that figure on reports from the Organisation of Islamic Cooperation and the Islamic Educational, Scientific and Cultural Organization.
Several Muslim countries like Turkey and Iran exhibit high scientific publication. Some countries have tried to encourage scientific research. In Pakistan, establishment of the Higher Education Commission in 2002, resulted in a 5-fold increase in the number of PhDs and a 10-fold increase in the number of scientific research papers in 10 years with the total number of universities increasing from 115 in 2001 to over 400 in 2012.[citation needed] Saudi Arabia has established the King Abdullah University of Science and Technology. United Arab Emirates has invested in Zayed University, United Arab Emirates University, and Masdar Institute of Science and Technology[clarification needed]
Encompasses both secular and religious styles, the design and style made by Muslims and their construction of buildings and structures in Islamic culture included the architectural types: the Mosque, the Tomb, the Palace and the Fort. Perhaps the most important expression of Islamic art is architecture, particularly that of the mosque. Through Islamic architecture, effects of varying cultures within Islamic civilization can be illustrated. Generally, the use of Islamic geometric patterns and foliage based arabesques were striking. There was also the use of decorative calligraphy instead of pictures which were haram (forbidden) in mosque architecture. Note that in secular architecture, human and animal representation was indeed present.
No Islamic visual images or depictions of God are meant to exist because it is believed that such artistic depictions may lead to idolatry. Moreover, Muslims believe that God is incorporeal, making any two- or three- dimensional depictions impossible. Instead, Muslims describe God by the names and attributes that, according to Islam, he revealed to his creation. All but one sura of the Quran begins with the phrase "In the name of God, the Beneficent, the Merciful". Images of Mohammed are likewise prohibited. Such aniconism and iconoclasm can also be found in Jewish and some Christian theology.
Islamic art frequently adopts the use of geometrical floral or vegetal designs in a repetition known as arabesque. Such designs are highly nonrepresentational, as Islam forbids representational depictions as found in pre-Islamic pagan religions. Despite this, there is a presence of depictional art in some Muslim societies, notably the miniature style made famous in Persia and under the Ottoman Empire which featured paintings of people and animals, and also depictions of Quranic stories and Islamic traditional narratives. Another reason why Islamic art is usually abstract is to symbolize the transcendence, indivisible and infinite nature of God, an objective achieved by arabesque. Islamic calligraphy is an omnipresent decoration in Islamic art, and is usually expressed in the form of Quranic verses. Two of the main scripts involved are the symbolic kufic and naskh scripts, which can be found adorning the walls and domes of mosques, the sides of minbars, and so on.
Distinguishing motifs of Islamic architecture have always been ordered repetition, radiating structures, and rhythmic, metric patterns. In this respect, fractal geometry has been a key utility, especially for mosques and palaces. Other features employed as motifs include columns, piers and arches, organized and interwoven with alternating sequences of niches and colonnettes. The role of domes in Islamic architecture has been considerable. Its usage spans centuries, first appearing in 691 with the construction of the Dome of the Rock mosque, and recurring even up until the 17th century with the Taj Mahal. And as late as the 19th century, Islamic domes had been incorporated into European architecture.
The Solar Hijri calendar, also called the Shamsi Hijri calendar, and abbreviated as SH, is the official calendar of Iran and Afghanistan. It begins on the vernal equinox. Each of the twelve months corresponds with a zodiac sign. The first six months have 31 days, the next five have 30 days, and the last month has 29 days in usual years but 30 days in leap years. The year of Prophet Muhammad's migration to Medina (622 CE) is fixed as the first year of the calendar, and the New Year's Day always falls on the March equinox.
In a small minority of Muslim countries, the law requires women to cover either just legs, shoulders and head or the whole body apart from the face. In strictest forms, the face as well must be covered leaving just a mesh to see through. These rules for dressing cause tensions, concerning particularly Muslims living in Western countries, where restrictions are considered both sexist and oppressive. Some Muslims oppose this charge, and instead declare that the media in these countries presses on women to reveal too much in order to be deemed attractive, and that this is itself sexist and oppressive.
The Canadian Armed Forces (CAF; French: Forces armées canadiennes, FAC), or Canadian Forces (CF) (French: les Forces canadiennes, FC), is the unified armed force of Canada, as constituted by the National Defence Act, which states: "The Canadian Forces are the armed forces of Her Majesty raised by Canada and consist of one Service called the Canadian Armed Forces."
This unified institution consists of sea, land, and air elements referred to as the Royal Canadian Navy (RCN), Canadian Army, and Royal Canadian Air Force (RCAF). Personnel may belong to either the Regular Force or the Reserve Force, which has four sub-components: the Primary Reserve, Supplementary Reserve, Cadet Organizations Administration and Training Service, and the Canadian Rangers. Under the National Defence Act, the Canadian Armed Forces are an entity separate and distinct from the Department of National Defence (the federal government department responsible for administration and formation of defence policy), which also exists as the civilian support system for the Forces.
The Commander-in-Chief of the Canadian Armed Forces is the reigning Canadian monarch, Queen Elizabeth II, who is represented by the Governor General of Canada. The Canadian Armed Forces is led by the Chief of the Defence Staff, who is advised and assisted by the Armed Forces Council.
During the Cold War, a principal focus of Canadian defence policy was contributing to the security of Europe in the face of the Soviet military threat. Toward that end, Canadian ground and air forces were based in Europe from the early 1950s until the early 1990s.
However, since the end of the Cold War, as the North Atlantic Treaty Organization (NATO) has moved much of its defence focus "out of area", the Canadian military has also become more deeply engaged in international security operations in various other parts of the world – most notably in Afghanistan since 2002.
Canadian defence policy today is based on the Canada First Defence Strategy, introduced in 2008. Based on that strategy, the Canadian military is oriented and being equipped to carry out six core missions within Canada, in North America and globally. Specifically, the Canadian Armed Forces are tasked with having the capacity to:
Consistent with the missions and priorities outlined above, the Canadian Armed Forces also contribute to the conduct of Canadian defence diplomacy through a range of activities, including the deployment of Canadian Defence Attachés, participation in bilateral and multilateral military forums (e.g. the System of Cooperation Among the American Air Forces), ship and aircraft visits, military training and cooperation, and other such outreach and relationship-building efforts.
Prior to Confederation in 1867, residents of the colonies in what is now Canada served as regular members of French and British forces and in local militia groups. The latter aided in the defence of their respective territories against attacks by other European powers, Aboriginal peoples, and later American forces during the American Revolutionary War and War of 1812, as well as in the Fenian raids, Red River Rebellion, and North-West Rebellion. Consequently, the lineages of some Canadian army units stretch back to the early 19th century, when militia units were formed to assist in the defence of British North America against invasion by the United States.
The responsibility for military command remained with the British Crown-in-Council, with a commander-in-chief for North America stationed at Halifax until the final withdrawal of British Army and Royal Navy units from that city in 1906. Thereafter, the Royal Canadian Navy was formed, and, with the advent of military aviation, the Royal Canadian Air Force. These forces were organised under the Department of Militia and Defence, and split into the Permanent and Non-Permanent Active Militias—frequently shortened to simply The Militia. By 1923, the department was merged into the Department of National Defence, but land forces in Canada were not referred to as the Canadian Army until November 1940.
The first overseas deployment of Canadian military forces occurred during the Second Boer War, when several units were raised to serve under British command. Similarly, when the United Kingdom entered into conflict with Germany in the First World War, Canadian troops were called to participate in European theatres. The Canadian Crown-in-Council then decided to send its forces into the Second World War, as well as the Korean War.
Since 1947, Canadian military units have participated in more than 200 operations worldwide, and completed 72 international operations. Canadian soldiers, sailors, and aviators came to be considered world-class professionals through conspicuous service during these conflicts and the country's integral participation in NATO during the Cold War, First Gulf War, Kosovo War, and in United Nations Peacekeeping operations, such as the Suez Crisis, Golan Heights, Cyprus, Croatia, Bosnia, Afghanistan, and Libya. Canada maintained an aircraft carrier from 1957 to 1970 during the Cold War, which never saw combat but participated in patrols during the Cuban Missile Crisis.
Battles which are particularly notable to the Canadian military include the Battle of Vimy Ridge, the Dieppe Raid, the Battle of Ortona, the Battle of Passchendaele, the Normandy Landings, the Battle for Caen, the Battle of the Scheldt, the Battle of Britain, the Battle of the Atlantic, the strategic bombing of German cities, and more recently the Battle of Medak Pocket, in Croatia.
At the end of the Second World War, Canada possessed the fourth-largest air force and fifth-largest naval surface fleet in the world, as well as the largest volunteer army ever fielded. Conscription for overseas service was introduced only near the end of the war, and only 2,400 conscripts actually made it into battle. Originally, Canada was thought to have had the third-largest navy in the world, but with the fall of the Soviet Union, new data based on Japanese and Soviet sources found that to be incorrect.
The current iteration of the Canadian Armed Forces dates from 1 February 1968, when the Royal Canadian Navy, Canadian Army, and Royal Canadian Air Force were merged into a unified structure and superseded by elemental commands. Its roots, however, lie in colonial militia groups that served alongside garrisons of the French and British armies and navies; a structure that remained in place until the early 20th century. Thereafter, a distinctly Canadian army and navy was established, followed by an air force, that, because of the constitutional arrangements at the time, remained effectively under the control of the British government until Canada gained legislative independence from the United Kingdom in 1931, in part due to the distinguished achievement and sacrifice of the Canadian Corps in the First World War.
After the 1980s, the use of the "Canadian Armed Forces" name gave way to "Canadian Forces";[citation needed] The "Canadian Armed Forces" name returned in 2013.
Land Forces during this period also deployed in support of peacekeeping operations within United Nations sanctioned conflicts. The nature of the Canadian Forces has continued to evolve. They have been deployed in Afghanistan until 2011, under the NATO-led United Nations International Security Assistance Force (ISAF), at the request of the Government of Afghanistan.
The Armed Forces are today funded by approximately $20.1 billion annually and are presently ranked 74th in size compared to the world's other armed forces by number of total personnel, and 58th in terms of active personnel, standing at a strength of roughly 68,000, plus 27,000 reservists, 5000 Rangers, and 19,000 supplementary reserves, bringing the total force to approximately 119,000. The number of primary reserve personnel is expected to go up to 30,000 by 2020, and the number of active to at least 70,000. In addition, 5000 rangers and 19,000 supplementary personnel will be serving. If this happens the total strength would be around 124,000. These individuals serve on numerous CF bases located in all regions of the country, and are governed by the Queen's Regulations and Orders and the National Defence Act.
In 2008 the Government of Canada made efforts, through the Canada First Defence Strategy, to modernize the Canadian Armed Forces, through the purchase of new equipment, improved training and readiness, as well as the establishment of the Canadian Special Operations Regiment. More funds were also put towards recruitment, which had been dwindling throughout the 1980s and '90s, possibly because the Canadian populace had come to perceive the CAF as peacekeepers rather than as soldiers, as shown in a 2008 survey conducted for the Department of National Defence. The poll found that nearly two thirds of Canadians agreed with the country's participation in the invasion of Afghanistan, and that the military should be stronger, but also that the purpose of the forces should be different, such as more focused on responding to natural disasters. Then CDS, Walter Natynczyk, said later that year that while recruiting has become more successful, the CF was facing a problem with its rate of loss of existing members, which increased between 2006 and 2008 from 6% to 9.2% annually.
The 2006 renewal and re-equipment effort has resulted in the acquisition of specific equipment (main battle tanks, artillery, unmanned air vehicles and other systems) to support the mission in Afghanistan. It has also encompassed initiatives to renew certain so-called "core capabilities" (such as the air force's medium range transport aircraft fleet – the C-130 Hercules – and the army's truck and armoured vehicle fleets). In addition, new systems (such as C-17 Globemaster III strategic transport aircraft and CH-47 Chinook heavy-lift helicopters) have also been acquired for the Armed Forces. Although the viability of the Canada First Defence Strategy continues to suffer setbacks from challenging and evolving fiscal and other factors, it originally aimed to:
In the 1950s, the recruitment of women was open to roles in medicine, communication, logistics, and administration. The roles of women in the CAF began to expand in 1971, after the Department reviewed the recommendations of the Royal Commission on the Status of Women, at which time it lifted the ceiling of 1,500 women personnel, and gradually expanded employment opportunities into the non-traditional areas—vehicle drivers and mechanics, aircraft mechanics, air-traffic controllers, military police, and firefighters. The Department further reviewed personnel policies in 1978 and 1985, after Parliament passed the Canadian Human Rights Act and the Canadian Charter of Rights and Freedoms. As a result of these reviews, the Department changed its policies to permit women to serve at sea in replenishment ships and in a diving tender, with the army service battalions, in military police platoons and field ambulance units, and in most air squadrons.
In 1987, occupations and units with the primary role of preparing for direct involvement in combat on the ground or at sea were still closed to women: infantry, armoured corps, field artillery, air-defence artillery, signals, field engineers, and naval operations. On 5 February 1987, the Minister of National Defence created an office to study the impact of employing men and women in combat units. These trials were called Combat-Related Employment of Women.
All military occupations were open to women in 1989, with the exception of submarine service, which opened in 2000. Throughout the 1990s, the introduction of women into the combat arms increased the potential recruiting pool by about 100 percent. It also provided opportunities for all persons to serve their country to the best of their abilities. Women were fully integrated in all occupations and roles by the government of Jean Chretien, and by 8 March 2000, even allowed to serve on submarines.
All equipment must be suitable for a mixed-gender force. Combat helmets, rucksacks, combat boots, and flak jackets are designed to ensure women have the same level of protection and comfort as their male colleagues. The women's uniform is similar in design to the men's uniform, but conforms to the female figure, and is functional and practical. Women are also provided with an annual financial entitlement for the purchase of brassiere undergarments.
The following is the hierarchy of the Canadian Armed Forces. It begins at the top with the most senior-ranking personnel and works its way into lower organizations.
The Canadian constitution determines that the Commander-in-Chief of the Canadian Armed Forces is the country's sovereign, who, since 1904, has authorized his or her viceroy, the governor general, to exercise the duties ascribed to the post of Commander-in-Chief and to hold the associated title since 1905. All troop deployment and disposition orders, including declarations of war, fall within the royal prerogative and are issued as Orders in Council, which must be signed by either the monarch or governor general. Under the Westminster system's parliamentary customs and practices, however, the monarch and viceroy must generally follow the advice of his or her ministers in Cabinet, including the prime minister and minister of national defence, who are accountable to the elected House of Commons.
The Armed Forces' 115,349 personnel are divided into a hierarchy of numerous ranks of officers and non-commissioned members. The governor general appoints, on the advice of the prime minister, the Chief of the Defence Staff (CDS) as the highest ranking commissioned officer in the Armed Forces and who, as head of the Armed Forces Council, is in command of the Canadian Forces. The Armed Forces Council generally operates from National Defence Headquarters (NDHQ) in Ottawa, Ontario. On the Armed Forces Council sit the heads of Canadian Joint Operations Command and Canadian Special Operations Forces Command, the Vice Chief of the Defence Staff, and the heads of the Royal Canadian Navy, the Canadian Army, the Royal Canadian Air Force and other key Level 1 organizations. The sovereign and most other members of the Canadian Royal Family also act as colonels-in-chief, honorary air commodores, air commodores-in-chief, admirals, and captains-general of Canadian Forces units, though these positions are ceremonial.
Canada's Armed forces operate out of 27 Canadian Forces bases (CFB) across the country, including NDHQ. This number has been gradually reduced since the 1970s with bases either being closed or merged. Both officers and non-commissioned members receive their basic training at the Canadian Forces Leadership and Recruit School in Saint-Jean-sur-Richelieu. Officers will generally either directly enter the Canadian Armed Forces with a degree from a civilian university, or receive their commission upon graduation from the Royal Military College of Canada. Specific element and trade training is conducted at a variety of institutions throughout Canada, and to a lesser extent, the world.
The Royal Canadian Navy (RCN), headed by the Commander of the Royal Canadian Navy, includes 33 warships and submarines deployed in two fleets: Maritime Forces Pacific (MARPAC) at CFB Esquimalt on the west coast, and Maritime Forces Atlantic (MARLANT) at Her Majesty's Canadian Dockyard in Halifax on the east coast, as well as one formation: the Naval Reserve Headquarters (NAVRESHQ) at Quebec City, Quebec. The fleet is augmented by various aircraft and supply vessels. The RCN participates in NATO exercises and operations, and ships are deployed all over the world in support of multinational deployments.
The Canadian Army is headed by the Commander of the Canadian Army and administered through four divisions—the 2nd Canadian Division, the 3rd Canadian Division, the 4th Canadian Division and the 5th Canadian Division—the Canadian Army Doctrine and Training System and the Canadian Army Headquarters.
Currently, the Regular Force component of the Army consists of three field-ready brigade groups: 1 Canadian Mechanized Brigade Group, at CFB Edmonton and CFB Shilo; 2 Canadian Mechanized Brigade Group, at CFB Petawawa and CFB Gagetown; and 5 Canadian Mechanized Brigade Group, at CFB Valcartier and Quebec City. Each contains one regiment each of artillery, armour, and combat engineers, three battalions of infantry (all scaled in the British fashion), one battalion for logistics, a squadron for headquarters/signals, and several smaller support organizations. A tactical helicopter squadron and a field ambulance are co-located with each brigade, but do not form part of the brigade's command structure.
The 2nd, 3rd and 4th Canadian Divisions each has a Regular Force brigade group, and each division except the 1st has two to three Reserve Force brigades groups. In total, there are ten Reserve Force brigade groups. The 5th Canadian Division and the 2nd Canadian Division each have two Reserve Force brigade groups, while the 4th Canadian Division and the 3rd Canadian Division each have three Reserve Force brigade groups. Major training and support establishments exist at CFB Gagetown, CFB Montreal and CFB Wainwright.
The Royal Canadian Air Force (RCAF) is headed by the Commander of the Royal Canadian Air Force. The commander of 1 Canadian Air Division and Canadian NORAD Region, based in Winnipeg, is responsible for the operational command and control of Air Force activities throughout Canada and worldwide. 1 Canadian Air Division operations are carried out through eleven wings located across Canada. The commander of 2 Canadian Air Division is responsible for training and support functions. 2 Canadian Air Division operations are carried out at two wings. Wings represent the grouping of various squadrons, both operational and support, under a single tactical commander reporting to the operational commander and vary in size from several hundred personnel to several thousand.
Major air bases are located in British Columbia, Alberta, Saskatchewan, Manitoba, Ontario, Quebec, Nova Scotia, and Newfoundland and Labrador, while administrative and command and control facilities are located in Winnipeg and North Bay. A Canadian component of the NATO Airborne Early Warning Force is also based at NATO Air Base Geilenkirchen near Geilenkirchen, Germany.
The RCAF and Joint Task Force (North) (JTFN) also maintain at various points throughout Canada's northern region a chain of forward operating locations, each capable of supporting fighter operations. Elements of CF-18 squadrons periodically deploy to these airports for short training exercises or Arctic sovereignty patrols.
The Canadian Joint Operations Command is an operational element established in October 2012 with the merger of Canada Command, the Canadian Expeditionary Force Command and the Canadian Operational Support Command. The new command, created as a response to the cost-cutting measures in the 2012 federal budget, combines the resources, roles and responsibilities of the three former commands under a single headquarters.
The Canadian Special Operations Forces Command (CANSOFCOM) is a formation capable of operating independently but primarily focused on generating special operations forces (SOF) elements to support CJOC. The command includes Joint Task Force 2 (JTF2), the Canadian Joint Incident Response Unit (CJIRU) based at CFB Trenton, as well as the Canadian Special Operations Regiment (CSOR) and 427 Special Operations Aviation Squadron (SOAS) based at CFB Petawawa.
Among other things, the Information Management Group is responsible for the conduct of electronic warfare and the protection of the Armed Forces' communications and computer networks. Within the group, this operational role is fulfilled by the Canadian Forces Information Operations Group, headquartered at CFS Leitrim in Ottawa, which operates the following units: the Canadian Forces Information Operations Group Headquarters (CFIOGHQ), the Canadian Forces Electronic Warfare Centre (CFEWC), the Canadian Forces Network Operation Centre (CFNOC), the Canadian Forces Signals Intelligence Operations Centre (CFSOC), the Canadian Forces Station (CFS) Leitrim, and the 764 Communications Squadron. In June 2011 the Canadian Armed Forces Chief of Force Development announced the establishment of a new organization, the Directorate of Cybernetics, headed by a Brigadier General, the Director General Cyber (DG Cyber). Within that directorate the newly established CAF Cyber Task Force, has been tasked to design and build cyber warfare capabilities for the Canadian Armed Forces.
The Health Services Group is a joint formation that includes over 120 general or specialized units and detachments providing health services to the Canadian Armed Forces. With few exceptions, all elements are under command of the Surgeon General for domestic support and force generation, or temporarily assigned under command of a deployed Joint Task Force through Canadian Joint Operations Command.
The Canadian Armed Forces have a total reserve force of approximately 50,000 primary and supplementary that can be called upon in times of national emergency or threat. For the components and sub-components of the Canadian Armed Forces Reserve Force, the order of precedence follows:
Approximately 26,000 citizen soldiers, sailors, and airmen and women, trained to the level of and interchangeable with their Regular Force counterparts, and posted to CAF operations or duties on a casual or ongoing basis, make up the Primary Reserve. This group is represented, though not commanded, at NDHQ by the Chief of Reserves and Cadets, who is usually a major general or rear admiral, and is divided into four components that are each operationally and administratively responsible to its corresponding environmental command in the Regular Force – the Naval Reserve (NAVRES), Land Force Reserve (LFR), and Air Reserve (AIRRES) – in addition to one force that does not fall under an environmental command, the Health Services Reserve under the Canadian Forces Health Services Group.
The Cadet Organizations Administration and Training Service (COATS) consists of officers and non-commissioned members who conduct training, safety, supervision and administration of nearly 60,000 cadets aged 12 to 18 years in the Canadian Cadet Movement. The majority of members in COATS are officers of the Cadet Instructors Cadre (CIC) branch of the CAF. Members of the Reserve Force Sub-Component COATS who are not employed part-time (Class A) or full-time (Class B) may be held on the "Cadet Instructor Supplementary Staff List" (CISS List) in anticipation of employment in the same manner as other reservists are held as members of the Supplementary Reserve.
The Canadian Rangers, who provide surveillance and patrol services in Canada's arctic and other remote areas, are an essential reserve force component used for Canada's exercise of sovereignty over its northern territory.
Only service dress is suitable for CAF members to wear on any occasion, barring "dirty work" or combat. With gloves, swords, and medals (No. 1 or 1A), it is suitable for ceremonial occasions and "dressed down" (No. 3 or lower), it is suitable for daily wear. Generally, after the elimination of base dress (although still defined for the Air Force uniform), operational dress is now the daily uniform worn by most members of the CF, unless service dress is prescribed (such as at the NDHQ, on parades, at public events, etc.). Approved parkas are authorized for winter wear in cold climates and a light casual jacket is also authorized for cooler days. The navy, most army, and some other units have, for very specific occasions, a ceremonial/regimental full dress, such as the naval "high-collar" white uniform, kilted Highland, Scottish, and Irish regiments, and the scarlet uniforms of the Royal Military Colleges.
Authorized headdress for the Canadian Armed Forces are the: beret, wedge cap, ballcap, Yukon cap, and tuque (toque). Each is coloured according to the distinctive uniform worn: navy (white or navy blue), army (rifle green or "regimental" colour), air force (light blue). Adherents of the Sikh faith may wear uniform turbans (dastar) (or patka, when operational) and Muslim women may wear uniform tucked hijabs under their authorized headdress. Jews may wear yarmulke under their authorized headdress and when bareheaded. The beret is probably the most widely worn headgear and is worn with almost all orders of dress (with the exception of the more formal orders of Navy and Air Force dress), and the colour of which is determined by the wearer's environment, branch, or mission. Naval personnel, however, seldom wear berets, preferring either service cap or authorized ballcaps (shipboard operational dress), which only the Navy wear. Air Force personnel, particularly officers, prefer the wedge cap to any other form of headdress. There is no naval variant of the wedge cap. The Yukon cap and tuque are worn only with winter dress, although clearance and combat divers may wear tuques year-round as a watch cap. Soldiers in Highland, Scottish, and Irish regiments generally wear alternative headdress, including the glengarry, balmoral, tam o'shanter, and caubeen instead of the beret. The officer cadets of both Royal Military Colleges wear gold-braided "pillbox" (cavalry) caps with their ceremonial dress and have a unique fur "Astrakhan" for winter wear. The Canadian Army wears the CG634 helmet.
The Constitution of Canada gives the federal government exclusive responsibility for national defence, and expenditures are thus outlined in the federal budget. For the 2008–2009 fiscal year, the amount allocated for defence spending was CAD$18.9 billion. This regular funding was augmented in 2005 with an additional CAD$12.5 billion over five years, as well as a commitment to increasing regular force troop levels by 5,000 persons, and the primary reserve by 3,000 over the same period. In 2006, a further CAD$5.3 billion over five years was provided to allow for 13,000 more regular force members, and 10,000 more primary reserve personnel, as well as CAD$17.1 billion for the purchase of new trucks for the Canadian Army, transport aircraft and helicopters for the Royal Canadian Air Force, and joint support ships for the Royal Canadian Navy.
Bill & Melinda Gates Foundation (or the Gates Foundation, abbreviated as BMGF) is the largest private foundation in the world, founded by Bill and Melinda Gates. It was launched in 2000 and is said to be the largest transparently operated private foundation in the world. The primary aims of the foundation are, globally, to enhance healthcare and reduce extreme poverty, and in America, to expand educational opportunities and access to information technology. The foundation, based in Seattle, Washington, is controlled by its three trustees: Bill Gates, Melinda Gates and Warren Buffett. Other principal officers include Co-Chair William H. Gates, Sr. and Chief Executive Officer Susan Desmond-Hellmann.
On June 25, 2006, Warren Buffett (then the world's richest person, estimated worth of US$62 billion as of April 16, 2008) pledged to give the foundation approximately 10 million Berkshire Hathaway Class B shares spread over multiple years through annual contributions, with the first year's donation of 500,000 shares being worth approximately US$1.5 billion. Buffett set conditions so that these contributions do not simply increase the foundation's endowment, but effectively work as a matching contribution, doubling the Foundation's annual giving: "Buffett's gift came with three conditions for the Gates foundation: Bill or Melinda Gates must be alive and active in its administration; it must continue to qualify as a charity; and each year it must give away an amount equal to the previous year's Berkshire gift, plus an additional amount equal to 5 percent of net assets. Buffett gave the foundation two years to abide by the third requirement." The Gates Foundation received 5% (500,000) of the shares in July 2006 and will receive 5% of the remaining earmarked shares in the July of each following year (475,000 in 2007, 451,250 in 2008). In July 2013, Buffet announced another donation of his company's Class B, this time in the amount worth $2 billion, is going to the Bill and Melinda Gates Foundation.
In November 2014, the Bill and Melinda Gates Foundation announced that they are adopting an open access (OA) policy for publications and data, "to enable the unrestricted access and reuse of all peer-reviewed published research funded by the foundation, including any underlying data sets". This move has been widely applauded by those who are working in the area of capacity building and knowledge sharing.[citation needed] Its terms have been called the most stringent among similar OA policies. As of January 1, 2015 their Open Access policy is effective for all new agreements.
The foundation explains on its website that its trustees divided the organization into two entities: the Bill & Melinda Gates Foundation (foundation) and the Bill & Melinda Gates Foundation Trust (trust). The foundation section, based in Seattle, US, "focuses on improving health and alleviating extreme poverty," and its trustees are Bill and Melinda Gates and Warren Buffett. The trust section manages "the investment assets and transfer proceeds to the foundation as necessary to achieve the foundation's charitable goals"—it holds the assets of Bill and Melinda Gates, who are the sole trustees, and receives contributions from Buffett.
The foundation trust invests undistributed assets, with the exclusive goal of maximizing the return on investment. As a result, its investments include companies that have been criticized for worsening poverty in the same developing countries where the foundation is attempting to relieve poverty. These include companies that pollute heavily and pharmaceutical companies that do not sell into the developing world. In response to press criticism, the foundation announced in 2007 a review of its investments to assess social responsibility. It subsequently cancelled the review and stood by its policy of investing for maximum return, while using voting rights to influence company practices.
In March 2006, the foundation announced a US$5 million grant for the International Justice Mission (IJM), a human rights organization based in Washington, D.C., US to work in the area of sex trafficking. The official announcement explained that the grant would allow the IJM to "create a replicable model for combating sex trafficking and slavery" that would involve the opening of an office in a region with high rates of sex trafficking, following research. The office was opened for three years for the following purposes: "conducting undercover investigations, training law enforcement, rescuing victims, ensuring appropriate aftercare, and seeking perpetrator accountability".
The IJM used the grant money to found "Project Lantern" and established an office in the Philippines city of Cebu. In 2010 the results of the project were published, in which the IJM stated that Project Lantern had led to "an increase in law enforcement activity in sex trafficking cases, an increase in commitment to resolving sex trafficking cases among law enforcement officers trained through the project, and an increase in services – like shelter, counseling and career training – provided to trafficking survivors". At the time that the results were released, the IJM was exploring opportunities to replicate the model in other regions.
The Water, Sanitation and Hygiene (WSH) program of the Gates Foundation was launched in mid-2005 as a "Learning Initiative," and became a full-fledged program under the Global Development Division in early 2010. The Foundation has since 2005 undertaken a wide range of efforts in the WASH sector involving research, experimentation, reflection, advocacy, and field implementation. In 2009, the Foundation decided to refocus its WASH effort mainly on sustainable sanitation services for the poor, using non-piped sanitation services (i.e. without the use of sewers), and less on water supply. This was because the sanitation sector was generally receiving less attention from other donors and from governments, and because the Foundation believed it had the potential to make a real difference through strategic investments.
In mid 2011, the Foundation announced in its new "Water, Sanitation, Hygiene Strategy Overview" that its funding now focuses primarily on sanitation, particularly in sub-Saharan Africa and South Asia, because access to improved sanitation is lowest in those regions. Their grant-making focus has been since 2011 on sanitation science and technology ("transformative technologies"), delivery models at scale, urban sanitation markets, building demand for sanitation, measurement and evaluation as well as policy, advocacy and communications.
Improved sanitation in the developing world is a global need, but a neglected priority as shown by the data collected by the Joint Monitoring Programme for Water Supply and Sanitation (JMP) of UNICEF and WHO. This program is tasked to monitor progress towards the Millennium Development Goal (MDG) relating to drinking water and sanitation. About one billion people have no sanitation facility whatsoever and continue to defecate in gutters, behind bushes or in open water bodies, with no dignity or privacy - which is called open defecation and which poses significant health risks. India is the country with the highest number of people practicing open defecation: around 600 million people. India has also become a focus country for the foundation's sanitation activities which has become evident since the "Reinvent the Toilet Fair" in Delhi, India in March 2014.
In 2011, the foundation launched a program called "Reinvent the Toilet Challenge" with the aim to promote the development of innovations in toilet design to benefit the 2.5 billion people that do not have access to safe and effective sanitation. This program has generated significant interest of the mainstream media. It was complemented by a program called "Grand Challenges Explorations" (2011 to 2013 with some follow-up grants reaching until 2015) which involved grants of US$100,000 each in the first round. Both funding schemes explicitly excluded project ideas that relied on centralized sewerage systems or are not compatible with development country contexts.
Since the launch of the "Reinvent the Toilet Challenge", more than a dozen research teams, mainly at universities in the U.S., Europe, India, China and South Africa, have received grants to develop innovative on-site and off-site waste treatment solutions for the urban poor. The grants were in the order of 400,000 USD for their first phase, followed by typically 1-3 million USD for their second phase; many of them investigated resource recovery or processing technologies for excreta or fecal sludge.
The Reinvent the Toilet Challenge is a long-term research and development effort to develop a hygienic, stand-alone toilet. This challenge is being complemented by another investment program to develop new technologies for improved pit latrine emptying (called by the foundation the "Omni-Ingestor") and fecal sludge processing (called "Omni-Processor"). The aim of the "Omni Processor" is to convert excreta (for example fecal sludge) into beneficial products such as energy and soil nutrients with the potential to develop local business and revenue.
The foundation has donated billions of dollars to help sufferers of AIDS, tuberculosis and malaria, protecting millions of children from death at the hands of preventable diseases. However, a 2007 investigation by The Los Angeles Times claimed there are three major unintended consequences with the foundation's allocation of aid. First, sub-Saharan Africa already suffered from a shortage of primary doctors before the arrival of the Gates Foundation, but "by pouring most contributions into the fight against such high-profile killers as AIDS, Gates grantees have increased the demand for specially trained, higher-paid clinicians, diverting staff from basic care" in sub-Saharan Africa. This "brain drain" adds to the existing doctor shortage and pulls away additional trained staff from children and those suffering from other common killers. Second, "the focus on a few diseases has shortchanged basic needs such as nutrition and transportation". Third, "Gates-funded vaccination programs have instructed caregivers to ignore – even discourage patients from discussing – ailments that the vaccinations cannot prevent".
Melinda Gates has stated that the foundation "has decided not to fund abortion". In response to questions about this decision, Gates stated in a June 2014 blog post that she "struggle[s] with the issue" and that "the emotional and personal debate about abortion is threatening to get in the way of the lifesaving consensus regarding basic family planning". Up to 2013, the Bill & Melinda Gates Foundation provided $71 million to Planned Parenthood, the primary U.S. abortion provider, and affiliated organizations.
In 1997, the charity introduced a U.S. Libraries initiative with a goal of "ensuring that if you can get to a public library, you can reach the internet". Only 35% of the world's population has access to the Internet. The foundation has given grants, installed computers and software, and provided training and technical support in partnership with public libraries nationwide in an effort to increase access and knowledge. Helping provide access and training for these resources, this foundation helps move public libraries into the digital age.
A key aspect of the Gates Foundation's U.S. efforts involves an overhaul of the country's education policies at both the K-12 and college levels, including support for teacher evaluations and charter schools and opposition to seniority-based layoffs and other aspects of the education system that are typically backed by teachers' unions. It spent $373 million on education in 2009. It has also donated to the two largest national teachers' unions. The foundation was the biggest early backer of the Common Core State Standards Initiative.
One of the foundation's goals is to lower poverty by increasing the number of college graduates in the United States, and the organization has funded "Reimagining Aid Design and Delivery" grants to think tanks and advocacy organizations to produce white papers on ideas for changing the current system of federal financial aid for college students, with a goal of increasing graduation rates. One of the ways the foundation has sought to increase the number of college graduates is to get them through college faster, but that idea has received some pushback from organizations of universities and colleges.
As part of its education-related initiatives, the foundation has funded journalists, think tanks, lobbying organizations and governments. Millions of dollars of grants to news organizations have funded reporting on education and higher education, including more than $1.4 million to the Education Writers Association to fund training for journalists who cover education. While some critics have feared the foundation for directing the conversation on education or pushing its point of view through news coverage, the foundation has said it lists all its grants publicly and does not enforce any rules for content among its grantees, who have editorial independence. Union activists in Chicago have accused Gates Foundation grantee Teach Plus, which was founded by new teachers and advocates against seniority-based layoffs, of "astroturfing".
The K-12 and higher education reform programs of the Gates Foundation have been criticized by some education professionals, parents, and researchers because they have driven the conversation on education reform to such an extent that they may marginalize researchers who do not support Gates' predetermined policy preferences. Several Gates-backed policies such as small schools, charter schools, and increasing class sizes have been expensive and disruptive, but some studies indicate they have not improved educational outcomes and may have caused harm. Peer reviewed scientific studies at Stanford find that Charter Schools do not systematically improve student performance
In October 2006, the Bill & Melinda Gates Foundation was split into two entities: the Bill & Melinda Gates Foundation Trust, which manages the endowment assets and the Bill & Melinda Gates Foundation, which "... conducts all operations and grantmaking work, and it is the entity from which all grants are made". Also announced was the decision to "... spend all of [the Trust's] resources within 20 years after Bill's and Melinda's deaths". This would close the Bill & Melinda Gates Foundation Trust and effectively end the Bill & Melinda Gates Foundation. In the same announcement it was reiterated that Warren Buffett "... has stipulated that the proceeds from the Berkshire Hathaway shares he still owns at death are to be used for philanthropic purposes within 10 years after his estate has been settled".
Tuvalu (i/tuːˈvɑːluː/ too-VAH-loo or /ˈtuːvəluː/ TOO-və-loo), formerly known as the Ellice Islands, is a Polynesian island nation located in the Pacific Ocean, midway between Hawaii and Australia. It comprises three reef islands and six true atolls spread out between the latitude of 5° to 10° south and longitude of 176° to 180°, west of the International Date Line. Its nearest neighbours are Kiribati, Nauru, Samoa and Fiji. Tuvalu has a population of 10,640 (2012 census). The total land area of the islands of Tuvalu is 26 square kilometres (10 sq mi).
In 1568, Spanish navigator Álvaro de Mendaña was the first European to sail through the archipelago, sighting the island of Nui during his expedition in search of Terra Australis. In 1819 the island of Funafuti was named Ellice's Island; the name Ellice was applied to all nine islands after the work of English hydrographer Alexander George Findlay. The islands came under Britain's sphere of influence in the late 19th century, when each of the Ellice Islands was declared a British Protectorate by Captain Gibson of HMS Curacoa between 9 and 16 October 1892. The Ellice Islands were administered as British protectorate by a Resident Commissioner from 1892 to 1916 as part of the British Western Pacific Territories (BWPT), and then as part of the Gilbert and Ellice Islands colony from 1916 to 1974.
The origins of the people of Tuvalu are addressed in the theories regarding migration into the Pacific that began about 3000 years ago. During pre-European-contact times there was frequent canoe voyaging between the nearer islands including Samoa and Tonga. Eight of the nine islands of Tuvalu were inhabited; thus the name, Tuvalu, means "eight standing together" in Tuvaluan (compare to *walo meaning "eight" in Proto-Austronesian). Possible evidence of fire in the Caves of Nanumanga may indicate human occupation for thousands of years.
An important creation myth of the islands of Tuvalu is the story of the te Pusi mo te Ali (the Eel and the Flounder) who created the islands of Tuvalu; te Ali (the flounder) is believed to be the origin of the flat atolls of Tuvalu and the te Pusin (the Eel) is the model for the coconut palms that are important in the lives of Tuvaluans. The stories as to the ancestors of the Tuvaluans vary from island to island. On Niutao, Funafuti and Vaitupu the founding ancestor is described as being from Samoa; whereas on Nanumea the founding ancestor is described as being from Tonga.
Captain John Byron passed through the islands of Tuvalu in 1764 during his circumnavigation of the globe as captain of the Dolphin (1751). Byron charted the atolls as Lagoon Islands. Keith S. Chambers and Doug Munro (1980) identified Niutao as the island that Francisco Mourelle de la Rúa sailed past on 5 May 1781, thus solving what Europeans had called The Mystery of Gran Cocal. Mourelle's map and journal named the island El Gran Cocal ('The Great Coconut Plantation'); however, the latitude and longitude was uncertain. Longitude could only be reckoned crudely as accurate chronometers were unavailable until the late 18th century.
The next European to visit was Arent Schuyler de Peyster, of New York, captain of the armed brigantine or privateer Rebecca, sailing under British colours, which passed through the southern Tuvaluan waters in May 1819; de Peyster sighted Nukufetau and Funafuti, which he named Ellice's Island after an English Politician, Edward Ellice, the Member of Parliament for Coventry and the owner of the Rebecca's cargo. The name Ellice was applied to all nine islands after the work of English hydrographer Alexander George Findlay.
For less than a year between 1862–63, Peruvian ships, engaged in what became to be called the "blackbirding" trade, combed the smaller islands of Polynesia from Easter Island in the eastern Pacific to Tuvalu and the southern atolls of the Gilbert Islands (now Kiribati), seeking recruits to fill the extreme labour shortage in Peru. While some islanders were voluntary recruits the "blackbirders" were notorious for enticing islanders on to ships with tricks, such as pretending to be Christian missionaries, as well as kidnapping islanders at gun point. The Rev. A. W. Murray, the earliest European missionary in Tuvalu, reported that in 1863 about 170 people were taken from Funafuti and about 250 were taken from Nukulaelae, as there were fewer than 100 of the 300 recorded in 1861 as living on Nukulaelae.
Christianity came to Tuvalu in 1861 when Elekana, a deacon of a Congregational church in Manihiki, Cook Islands became caught in a storm and drifted for 8 weeks before landing at Nukulaelae on 10 May 1861. Elekana began proselytising Christianity. He was trained at Malua Theological College, a London Missionary Society (LMS) school in Samoa, before beginning his work in establishing the Church of Tuvalu. In 1865 the Rev. A. W. Murray of the LMS – a Protestant congregationalist missionary society – arrived as the first European missionary where he too proselytised among the inhabitants of Tuvalu. By 1878 Protestantism was well established with preachers on each island. In the later 19th and early 20th centuries the ministers of what became the Church of Tuvalu (Te Ekalesia Kelisiano Tuvalu) were predominantly Samoans, who influenced the development of the Tuvaluan language and the music of Tuvalu.
Trading companies became active in Tuvalu in the mid-19th century; the trading companies engaged palagi traders who lived on the islands. John (also known as Jack) O'Brien was the first European to settle in Tuvalu, he became a trader on Funafuti in the 1850s. He married Salai, the daughter of the paramount chief of Funafuti. Louis Becke, who later found success as a writer, was a trader on Nanumanga from April 1880 until the trading-station was destroyed later that year in a cyclone. He then became a trader on Nukufetau.
In 1892 Captain Davis of the HMS Royalist reported on trading activities and traders on each of the islands visited. Captain Davis identified the following traders in the Ellice Group: Edmund Duffy (Nanumea); Jack Buckland (Niutao); Harry Nitz (Vaitupu); John (also known as Jack) O'Brien (Funafuti); Alfred Restieaux and Emile Fenisot (Nukufetau); and Martin Kleis (Nui). During this time, the greatest number of palagi traders lived on the atolls, acting as agents for the trading companies. Some islands would have competing traders while dryer islands might only have a single trader.
In the later 1890s and into first decade of the 20th century, structural changes occurred in the operation of the Pacific trading companies; they moved from a practice of having traders resident on each island to instead becoming a business operation where the supercargo (the cargo manager of a trading ship) would deal directly with the islanders when a ship visited an island. From 1900 the numbers of palagi traders in Tuvalu declined and the last of the palagi traders were Fred Whibley on Niutao, Alfred Restieaux on Nukufetau, and Martin Kleis on Nui. By 1909 there were no more resident palagi traders representing the trading companies, although both Whibley and Restieaux remained in the islands until their deaths.
In 1890 Robert Louis Stevenson, his wife Fanny Vandegrift Stevenson and her son Lloyd Osbourne sailed on the Janet Nicoll, a trading steamer owned by Henderson and Macfarlane of Auckland, New Zealand, which operated between Sydney and Auckland and into the central Pacific. The Janet Nicoll visited three of the Ellice Islands; while Fanny records that they made landfall at Funafuti, Niutao and Nanumea, Jane Resture suggests that it was more likely they landed at Nukufetau rather than Funafuti. An account of this voyage was written by Fanny Stevenson and published under the title The Cruise of the Janet Nichol, together with photographs taken by Robert Louis Stevenson and Lloyd Osbourne.
The boreholes on Funafuti, at the site now called Darwin's Drill, are the result of drilling conducted by the Royal Society of London for the purpose of investigating the formation of coral reefs to determine whether traces of shallow water organisms could be found at depth in the coral of Pacific atolls. This investigation followed the work on The Structure and Distribution of Coral Reefs conducted by Charles Darwin in the Pacific. Drilling occurred in 1896, 1897 and 1898. Professor Edgeworth David of the University of Sydney was a member of the 1896 "Funafuti Coral Reef Boring Expedition of the Royal Society", under Professor William Sollas and lead the expedition in 1897. Photographers on these trips recorded people, communities, and scenes at Funafuti.
Charles Hedley, a naturalist at the Australian Museum, accompanied the 1896 expedition and during his stay on Funafuti collected invertebrate and ethnological objects. The descriptions of these were published in Memoir III of the Australian Museum Sydney between 1896 and 1900. Hedley also wrote the General Account of the Atoll of Funafuti, The Ethnology of Funafuti, and The Mollusca of Funafuti. Edgar Waite was also part of the 1896 expedition and published an account of The mammals, reptiles, and fishes of Funafuti. William Rainbow described the spiders and insects collected at Funafuti in The insect fauna of Funafuti.
During the Pacific War Funafuti was used as a base to prepare for the subsequent seaborn attacks on the Gilbert Islands (Kiribati) that were occupied by Japanese forces. The United States Marine Corps landed on Funafuti on 2 October 1942 and on Nanumea and Nukufetau in August 1943. The Japanese had already occupied Tarawa and other islands in what is now Kiribati, but were delayed by the losses at the Battle of the Coral Sea. The islanders assisted the American forces to build airfields on Funafuti, Nanumea and Nukufetau and to unload supplies from ships. On Funafuti the islanders shifted to the smaller islets so as to allow the American forces to build the airfield and to build naval bases and port facilities on Fongafale. A Naval Construction Battalion (Seabees) built a sea plane ramp on the lagoon side of Fongafale islet for seaplane operations by both short and long range seaplanes and a compacted coral runway was also constructed on Fongafale, with runways also constructed to create Nanumea Airfield and Nukufetau Airfield. USN Patrol Torpedo Boats (PTs) were based at Funafuti from 2 November 1942 to 11 May 1944.
In 1974 ministerial government was introduced to the Gilbert and Ellice Islands Colony through a change to the Constitution. In that year a general election was held; and a referendum was held in December 1974 to determine whether the Gilbert Islands and Ellice Islands should each have their own administration. As a consequence of the referendum, separation occurred in two stages. The Tuvaluan Order 1975, which took effect on 1 October 1975, recognised Tuvalu as a separate British dependency with its own government. The second stage occurred on 1 January 1976 when separate administrations were created out of the civil service of the Gilbert and Ellice Islands Colony.
From 1974 (the creation of the British colony of Tuvalu) until independence, the legislative body of Tuvalu was called the House of the Assembly or Fale I Fono. Following independence in October 1978 the House of the Assembly was renamed the Parliament of Tuvalu or Palamene o Tuvalu. The unicameral Parliament has 15 members with elections held every four years. The members of parliament select the Prime Minister (who is the head of government) and the Speaker of Parliament. The ministers that form the Cabinet are appointed by the Governor General on the advice of the Prime Minister.
There are eight Island Courts and Lands Courts; appeals in relation to land disputes are made to the Lands Courts Appeal Panel. Appeals from the Island Courts and the Lands Courts Appeal Panel are made to the Magistrates Court, which has jurisdiction to hear civil cases involving up to $T10,000. The superior court is the High Court of Tuvalu as it has unlimited original jurisdiction to determine the Law of Tuvalu and to hear appeals from the lower courts. Sir Gordon Ward is the current Chief Justice of Tuvalu. Rulings of the High Court can be appealed to the Court of Appeal of Tuvalu. From the Court of Appeal there is a right of appeal to Her Majesty in Council, i.e., the Privy Council in London.
Tuvalu participates in the work of Secretariat of the Pacific Community, or SPC (sometimes Pacific Community) and is a member of the Pacific Islands Forum, the Commonwealth of Nations and the United Nations. Tuvalu has maintained a mission at the United Nations in New York City since 2000. Tuvalu is a member of the World Bank and the Asian Development Bank. On 18 February 2016 Tuvalu signed the Pacific Islands Development Forum Charter and formally joined the Pacific Islands Development Forum (PIDF).
A major international priority for Tuvalu in the UN, at the 2002 Earth Summit in Johannesburg, South Africa and in other international fora, is promoting concern about global warming and the possible sea level rising. Tuvalu advocates ratification and implementation of the Kyoto Protocol. In December 2009 the islands stalled talks on climate change at the United Nations Climate Change Conference in Copenhagen, fearing some other developing countries were not committing fully to binding deals on a reduction in carbon emissions. Their chief negotiator stated, "Tuvalu is one of the most vulnerable countries in the world to climate change and our future rests on the outcome of this meeting."
Tuvalu participates in the Alliance of Small Island States (AOSIS), which is a coalition of small island and low-lying coastal countries that have concerns about their vulnerability to the adverse effects of global climate change. Under the Majuro Declaration, which was signed on 5 September 2013, Tuvalu has commitment to implement power generation of 100% renewable energy (between 2013 and 2020), which is proposed to be implemented using Solar PV (95% of demand) and biodiesel (5% of demand). The feasibility of wind power generation will be considered. Tuvalu participates in the operations of the Pacific Islands Applied Geoscience Commission (SOPAC) and the Secretariat of the Pacific Regional Environment Programme (SPREP).
Tuvalu participates in the operations of the Pacific Island Forum Fisheries Agency (FFA) and the Western and Central Pacific Fisheries Commission (WCPFC). The Tuvaluan government, the US government, and the governments of other Pacific islands, are parties to the South Pacific Tuna Treaty (SPTT), which entered into force in 1988. Tuvalu is also a member of the Nauru Agreement which addresses the management of tuna purse seine fishing in the tropical western Pacific. In May 2013 representatives from the United States and the Pacific Islands countries agreed to sign interim arrangement documents to extend the Multilateral Fisheries Treaty (which encompasses the South Pacific Tuna Treaty) to confirm access to the fisheries in the Western and Central Pacific for US tuna boats for 18 months. Tuvalu and the other members of the Pacific Island Forum Fisheries Agency (FFA) and the United States have settled a tuna fishing deal for 2015; a longer term deal will be negotiated. The treaty is an extension of the Nauru Agreement and provides for US flagged purse seine vessels to fish 8,300 days in the region in return for a payment of US$90 million made up by tuna fishing industry and US-Government contributions. In 2015 Tuvalu has refused to sell fishing days to certain nations and fleets that have blocked Tuvaluan initiatives to develop and sustain their own fishery.
In July 2013 Tuvalu signed the Memorandum of Understanding (MOU) to establish the Pacific Regional Trade and Development Facility, which Facility originated in 2006, in the context of negotiations for an Economic Partnership Agreement (EPA) between Pacific ACP States and the European Union. The rationale for the creation of the Facility being to improve the delivery of aid to Pacific island countries in support of the Aid-for-Trade (AfT) requirements. The Pacific ACP States are the countries in the Pacific that are signatories to the Cotonou Agreement with the European Union.
Each island has its own high-chief, or ulu-aliki, and several sub-chiefs (alikis). The community council is the Falekaupule (the traditional assembly of elders) or te sina o fenua (literally: "grey-hairs of the land"). In the past, another caste, the priests (tofuga), were also amongst the decision-makers. The ulu-aliki and aliki exercise informal authority at the local level. Ulu-aliki are always chosen based on ancestry. Under the Falekaupule Act (1997), the powers and functions of the Falekaupule are now shared with the pule o kaupule (elected village presidents; one on each atoll).
In 2014 attention was drawn to an appeal to the New Zealand Immigration and Protection Tribunal against the deportation of a Tuvaluan family on the basis that they were "climate change refugees", who would suffer hardship resulting from the environmental degradation of Tuvalu. However the subsequent grant of residence permits to the family was made on grounds unrelated to the refugee claim. The family was successful in their appeal because, under the relevant immigration legislation, there were "exceptional circumstances of a humanitarian nature" that justified the grant of resident permits as the family was integrated into New Zealand society with a sizeable extended family which had effectively relocated to New Zealand. Indeed, in 2013 a claim of a Kiribati man of being a "climate change refugee" under the Convention relating to the Status of Refugees (1951) was determined by the New Zealand High Court to be untenable as there was no persecution or serious harm related to any of the five stipulated Refugee Convention grounds. Permanent migration to Australia and New Zealand, such as for family reunification, requires compliance with the immigration legislation of those countries.
New Zealand has an annual quota of 75 Tuvaluans granted work permits under the Pacific Access Category, as announced in 2001. The applicants register for the Pacific Access Category (PAC) ballots; the primary criteria is that the principal applicant must have a job offer from a New Zealand employer. Tuvaluans also have access to seasonal employment in the horticulture and viticulture industries in New Zealand under the Recognised Seasonal Employer (RSE) Work Policy introduced in 2007 allowing for employment of up to 5,000 workers from Tuvalu and other Pacific islands. Tuvaluans can participate in the Australian Pacific Seasonal Worker Program, which allows Pacific Islanders to obtain seasonal employment in the Australian agriculture industry, in particular cotton and cane operations; fishing industry, in particular aquaculture; and with accommodation providers in the tourism industry.
The Tuvaluan language and English are the national languages of Tuvalu. Tuvaluan is of the Ellicean group of Polynesian languages, distantly related to all other Polynesian languages such as Hawaiian, Māori, Tahitian, Samoan and Tongan. It is most closely related to the languages spoken on the Polynesian outliers in Micronesia and northern and central Melanesia. The language has borrowed from the Samoan language, as a consequence of Christian missionaries in the late 19th and early 20th centuries being predominantly Samoan.
The Princess Margaret Hospital on Funafuti is the only hospital in Tuvalu. The Tuvaluan medical staff at PMH in 2011 comprised the Director of Health & Surgeon, the Chief Medical Officer Public Health, an anaesthetist, a paediatric medical officer and an obstetrics and gynaecology medical officer. Allied health staff include two radiographers, two pharmacists, three laboratory technicians, two dieticians and 13 nurses with specialised training in fields including surgical nursing, anaesthesia nursing/ICU, paediatric nursing and midwifery. PMH also employs a dentist. The Department of Health also employs nine or ten nurses on the outer islands to provide general nursing and midwifery services.
Fetuvalu offers the Cambridge syllabus. Motufoua offers the Fiji Junior Certificate (FJC) at year 10, Tuvaluan Certificate at Year 11 and the Pacific Senior Secondary Certificate (PSSC) at Year 12, set by the Fiji-based exam board SPBEA. Sixth form students who pass their PSSC go on to the Augmented Foundation Programme, funded by the government of Tuvalu. This program is required for tertiary education programmes outside of Tuvalu and is available at the University of the South Pacific (USP) Extension Centre in Funafuti.
Required attendance at school is 10 years for males and 11 years for females (2001). The adult literacy rate is 99.0% (2002). In 2010, there were 1,918 students who were taught by 109 teachers (98 certified and 11 uncertified). The teacher-pupil ratio for primary schools in Tuvalu is around 1:18 for all schools with the exception of Nauti School, which has a teacher-student ratio of 1:27. Nauti School on Funafuti is the largest primary in Tuvalu with more than 900 students (45 percent of the total primary school enrolment). The pupil-teacher ratio for Tuvalu is low compared to the Pacific region (ratio of 1:29).
Community Training Centres (CTCs) have been established within the primary schools on each atoll. The CTCs provide vocational training to students who do not progress beyond Class 8 because they failed the entry qualifications for secondary education. The CTCs offer training in basic carpentry, gardening and farming, sewing and cooking. At the end of their studies the graduates can apply to continue studies either at Motufoua Secondary School or the Tuvalu Maritime Training Institute (TMTI). Adults can also attend courses at the CTCs.
The traditional buildings of Tuvalu used plants and trees from the native broadleaf forest, including timber from: Pouka, (Hernandia peltata); Ngia or Ingia bush, (Pemphis acidula); Miro, (Thespesia populnea); Tonga, (Rhizophora mucronata); Fau or Fo fafini, or woman's fibre tree (Hibiscus tiliaceus). and fibre from: coconut; Ferra, native fig (Ficus aspem); Fala, screw pine or Pandanus. The buildings were constructed without nails and were lashed and tied together with a plaited sennit rope that was handmade from dried coconut fibre.
The women of Tuvalu use cowrie and other shells in traditional handicrafts. The artistic traditions of Tuvalu have traditionally been expressed in the design of clothing and traditional handicrafts such as the decoration of mats and fans. Crochet (kolose) is one of the art forms practiced by Tuvaluan women. The material culture of Tuvalu uses traditional design elements in artefacts used in everyday life such as the design of canoes and fish hooks made from traditional materials. The design of women's skirts (titi), tops (teuga saka), headbands, armbands, and wristbands, which continue to be used in performances of the traditional dance songs of Tuvalu, represents contemporary Tuvaluan art and design.
The cuisine of Tuvalu is based on the staple of coconut and the many species of fish found in the ocean and lagoons of the atolls. Desserts made on the islands include coconut and coconut milk, instead of animal milk. The traditional foods eaten in Tuvalu are pulaka, taro, bananas, breadfruit and coconut. Tuvaluans also eat seafood, including coconut crab and fish from the lagoon and ocean. A traditional food source is seabirds (taketake or black noddy and akiaki or white tern), with pork being eaten mostly at fateles (or parties with dancing to celebrate events).
Another important building is the falekaupule or maneapa the traditional island meeting hall, where important matters are discussed and which is also used for wedding celebrations and community activities such as a fatele involving music, singing and dancing. Falekaupule is also used as the name of the council of elders – the traditional decision making body on each island. Under the Falekaupule Act, Falekaupule means "traditional assembly in each island...composed in accordance with the Aganu of each island". Aganu means traditional customs and culture.
A traditional sport played in Tuvalu is kilikiti, which is similar to cricket. A popular sport specific to Tuvalu is Ano, which is played with two round balls of 12 cm (5 in) diameter. Ano is a localised version of volleyball, in which the two hard balls made from pandanus leaves are volleyed at great speed with the team members trying to stop the Ano hitting the ground. Traditional sports in the late 19th century were foot racing, lance throwing, quarterstaff fencing and wrestling, although the Christian missionaries disapproved of these activities.
The popular sports in Tuvalu include kilikiti, Ano, football, futsal, volleyball, handball, basketball and rugby union. Tuvalu has sports organisations for athletics, badminton, tennis, table tennis, volleyball, football, basketball, rugby union, weightlifting and powerlifting. At the 2013 Pacific Mini Games, Tuau Lapua Lapua won Tuvalu's first gold medal in an international competition in the weightlifting 62 kilogram male snatch. (He also won bronze in the clean and jerk, and obtained the silver medal overall for the combined event.) In 2015 Telupe Iosefa received the first gold medal won by Tuvalu at the Pacific Games in the powerlifting 120 kg male division.
A major sporting event is the "Independence Day Sports Festival" held annually on 1 October. The most important sports event within the country is arguably the Tuvalu Games, which are held yearly since 2008. Tuvalu first participated in the Pacific Games in 1978 and in the Commonwealth Games in 1998, when a weightlifter attended the games held at Kuala Lumpur, Malaysia. Two table tennis players attended the 2002 Commonwealth Games in Manchester, England; Tuvalu entered competitors in shooting, table tennis and weightlifting at the 2006 Commonwealth Games in Melbourne, Australia; three athletes participated in the 2010 Commonwealth Games in Delhi, India, entering the discus, shot put and weightlifting events; and a team of 3 weightlifters and 2 table tennis players attended the 2014 Commonwealth Games in Glasgow. Tuvaluan athletes have also participated in the men's and women's 100 metre sprints at the World Championships in Athletics from 2009.
From 1996 to 2002, Tuvalu was one of the best-performing Pacific Island economies and achieved an average real gross domestic product (GDP) growth rate of 5.6% per annum. Since 2002 economic growth has slowed, with GDP growth of 1.5% in 2008. Tuvalu was exposed to rapid rises in world prices of fuel and food in 2008, with the level of inflation peaking at 13.4%. The International Monetary Fund 2010 Report on Tuvalu estimates that Tuvalu experienced zero growth in its 2010 GDP, after the economy contracted by about 2% in 2009. On 5 August 2012, the Executive Board of the International Monetary Fund (IMF) concluded the Article IV consultation with Tuvalu, and assessed the economy of Tuvalu: "A slow recovery is underway in Tuvalu, but there are important risks. GDP grew in 2011 for the first time since the global financial crisis, led by the private retail sector and education spending. We expect growth to rise slowly". The IMF 2014 Country Report noted that real GDP growth in Tuvalu had been volatile averaging only 1 percent in the past decade. The 2014 Country Report describes economic growth prospects as generally positive as the result of large revenues from fishing licenses, together with substantial foreign aid.
Banking services are provided by the National Bank of Tuvalu. Public sector workers make up about 65% of those formally employed. Remittances from Tuvaluans living in Australia and New Zealand, and remittances from Tuvaluan sailors employed on overseas ships are important sources of income for Tuvaluans. Approximately 15% of adult males work as seamen on foreign-flagged merchant ships. Agriculture in Tuvalu is focused on coconut trees and growing pulaka in large pits of composted soil below the water table. Tuvaluans are otherwise involved in traditional subsistence agriculture and fishing.
Tuvaluans are well known for their seafaring skills, with the Tuvalu Maritime Training Institute on Amatuku motu (island), Funafuti, providing training to approximately 120 marine cadets each year so that they have the skills necessary for employment as seafarers on merchant shipping. The Tuvalu Overseas Seamen's Union (TOSU) is the only registered trade union in Tuvalu. It represents workers on foreign ships. The Asian Development Bank (ADB) estimates that 800 Tuvaluan men are trained, certified and active as seafarers. The ADB estimates that, at any one time, about 15% of the adult male population works abroad as seafarers. Job opportunities also exist as observers on tuna boats where the role is to monitor compliance with the boat's tuna fishing licence.
Government revenues largely come from sales of fishing licenses, income from the Tuvalu Trust Fund, and from the lease of its highly fortuitous .tv Internet Top Level Domain (TLD). In 1998, Tuvalu began deriving revenue from the use of its area code for premium-rate telephone numbers and from the commercialisation of its ".tv" Internet domain name, which is now managed by Verisign until 2021. The ".tv" domain name generates around $2.2 million each year from royalties, which is about ten per cent of the government's total revenue. Domain name income paid most of the cost of paving the streets of Funafuti and installing street lighting in mid-2002. Tuvalu also generates income from stamps by the Tuvalu Philatelic Bureau and income from the Tuvalu Ship Registry.
The United Nations designates Tuvalu as a least developed country (LDC) because of its limited potential for economic development, absence of exploitable resources and its small size and vulnerability to external economic and environmental shocks. Tuvalu participates in the Enhanced Integrated Framework for Trade-Related Technical Assistance to Least Developed Countries (EIF), which was established in October 1997 under the auspices of the World Trade Organisation. In 2013 Tuvalu deferred its graduation from least developed country (LDC) status to a developing country to 2015. Prime Minister Enele Sopoaga said that this deferral was necessary to maintain access by Tuvalu to the funds provided by the United Nations's National Adaptation Programme of Action (NAPA), as "Once Tuvalu graduates to a developed country, it will not be considered for funding assistance for climate change adaptation programmes like NAPA, which only goes to LDCs". Tuvalu had met targets so that Tuvalu was to graduate from LDC status. Prime minister, Enele Sopoaga wants the United Nations to reconsider its criteria for graduation from LDC status as not enough weight is given to the environmental plight of small island states like Tuvalu in the application of the Environmental Vulnerability Index (EVI).
The Tuvalu Media Department of the Government of Tuvalu operates Radio Tuvalu which broadcasts from Funafuti. In 2011 the Japanese government provided financial support to construct a new AM broadcast studio. The installation of upgraded transmission equipment allows Radio Tuvalu to be heard on all nine islands of Tuvalu. The new AM radio transmitter on Funafuti replaced the FM radio service to the outer islands and freed up satellite bandwidth for mobile services. Fenui – news from Tuvalu is a free digital publication of the Tuvalu Media Department that is emailed to subscribers and operates a Facebook page, which publishes news about government activities and news about Tuvaluan events, such as a special edition covering the results of the 2015 general election.
Funafuti is the only port but there is a deep-water berth in the harbour at Nukufetau. The merchant marine fleet consists of two passenger/cargo ships Nivaga III and Manu Folau. These ships carry cargo and passengers between the main atolls and travel between Suva, Fiji and Funafuti 3 to 4 times a year. The Nivaga III and Manu Folau provide round trip visits to the outer islands every three or four weeks. The Manu Folau is a 50-metre vessel that was a gift from Japan to the people of Tuvalu. In 2015 the United Nations Development Program (UNDP) assisted the government of Tuvalu to acquire MV Talamoana, a 30-metre vessel that will be used to implement Tuvalu's National Adaptation Programme of Action (NAPA) to transport government officials and project personnel to the outer islands. In 2015 the Nivaga III was donated by the government of Japan; it replaced the Nivaga II, which had serviced Tuvalu from 1989.
Tuvalu consists of three reef islands and six true atolls. Its small, scattered group of atolls have poor soil and a total land area of only about 26 square kilometres (10 square miles) making it the fourth smallest country in the world. The islets that form the atolls are very low lying. Nanumanga, Niutao, Niulakita are reef islands and the six true atolls are Funafuti, Nanumea, Nui, Nukufetau, Nukulaelae and Vaitupu. Tuvalu's Exclusive Economic Zone (EEZ) covers an oceanic area of approximately 900,000 km2.
Funafuti is the largest atoll of the nine low reef islands and atolls that form the Tuvalu volcanic island chain. It comprises numerous islets around a central lagoon that is approximately 25.1 kilometres (15.6 miles) (N–S) by 18.4 kilometres (11.4 miles) (W-E), centred on 179°7'E and 8°30'S. On the atolls, an annular reef rim surrounds the lagoon with several natural reef channels. Surveys were carried out in May 2010 of the reef habitats of Nanumea, Nukulaelae and Funafuti and a total of 317 fish species were recorded during this Tuvalu Marine Life study. The surveys identified 66 species that had not previously been recorded in Tuvalu, which brings the total number of identified species to 607.
Tuvalu experiences the effects of El Niño and La Niña caused by changes in ocean temperatures in the equatorial and central Pacific. El Niño effects increase the chances of tropical storms and cyclones, while La Niña effects increase the chances of drought. Typically the islands of Tuvalu receive between 200 to 400 mm (8 to 16 in) of rainfall per month. However, in 2011 a weak La Niña effect caused a drought by cooling the surface of the sea around Tuvalu. A state of emergency was declared on 28 September 2011; with rationing of fresh-water on the islands of Funafuti and Nukulaelae. Households on Funafuti and Nukulaelae were restricted to two buckets of fresh water per day (40 litres).
The governments of Australia and New Zealand responded to the 2011 fresh-water crisis by supplying temporary desalination plants, and assisted in the repair of the existing desalination unit that was donated by Japan in 2006. In response to the 2011 drought, Japan funded the purchase of a 100 m3/d desalination plant and two portable 10 m3/d plants as part of its Pacific Environment Community (PEC) program. Aid programs from the European Union and Australia also provided water tanks as part of the longer term solution for the storage of available fresh water.
The eastern shoreline of Funafuti Lagoon was modified during World War II when the airfield (what is now Funafuti International Airport) was constructed. The coral base of the atoll was used as fill to create the runway. The resulting borrow pits impacted the fresh-water aquifer. In the low areas of Funafuti the sea water can be seen bubbling up through the porous coral rock to form pools with each high tide. Since 1994 a project has been in development to assess the environmental impact of transporting sand from the lagoon to fill all the borrow pits and low-lying areas on Fongafale. In 2014 the Tuvalu Borrow Pits Remediation (BPR) project was approved in order to fill 10 borrow pits, leaving Tafua Pond, which is a natural pond. The New Zealand Government funded the BPR project. The project was carried out in 2015 with 365,000 sqm of sand being dredged from the lagoon to fill the holes and improve living conditions on the island. This project increase the usable land space on Fongafale by eight per cent.
The reefs at Funafuti have suffered damage, with 80 per cent of the coral becoming bleached as a consequence of the increase in ocean temperatures and ocean acidification. The coral bleaching, which includes staghorn corals, is attributed to the increase in water temperature that occurred during the El Niños that occurred from 1998–2000 and from 2000–2001. A reef restoration project has investigated reef restoration techniques; and researchers from Japan have investigated rebuilding the coral reefs through the introduction of foraminifera. The project of the Japan International Cooperation Agency is designed to increase the resilience of the Tuvalu coast against sea level rise through ecosystem rehabilitation and regeneration and through support for sand production.
The rising population has resulted in an increased demand on fish stocks, which are under stress; although the creation of the Funafuti Conservation Area has provided a fishing exclusion area to help sustain the fish population across the Funafuti lagoon. Population pressure on the resources of Funafuti and inadequate sanitation systems have resulted in pollution. The Waste Operations and Services Act of 2009 provides the legal framework for waste management and pollution control projects funded by the European Union directed at organic waste composting in eco-sanitation systems. The Environment Protection (Litter and Waste Control) Regulation 2013 is intended to improve the management of the importation of non-biodegradable materials. In Tuvalu plastic waste is a problem as much imported food and other commodities are supplied in plastic containers or packaging.
Reverse osmosis (R/O) desalination units supplement rainwater harvesting on Funafuti. The 65 m3 desalination plant operates at a real production level of around 40 m3 per day. R/O water is only intended to be produced when storage falls below 30%, however demand to replenish household storage supplies with tanker-delivered water means that the R/O desalination units are continually operating. Water is delivered at a cost of A$3.50 per m3. Cost of production and delivery has been estimated at A$6 per m3, with the difference subsidised by the government.
In July 2012 a United Nations Special Rapporteur called on the Tuvalu Government to develop a national water strategy to improve access to safe drinking water and sanitation. In 2012, Tuvalu developed a National Water Resources Policy under the Integrated Water Resource Management (IWRM) Project and the Pacific Adaptation to Climate Change (PACC) Project, which are sponsored by the Global Environment Fund/SOPAC. Government water planning has established a target of between 50 and 100L of water per person per day accounting for drinking water, cleaning, community and cultural activities.
Because of the low elevation, the islands that make up this nation are vulnerable to the effects of tropical cyclones and by the threat of current and future sea level rise. The highest elevation is 4.6 metres (15 ft) above sea level on Niulakita, which gives Tuvalu the second-lowest maximum elevation of any country (after the Maldives). The highest elevations are typically in narrow storm dunes on the ocean side of the islands which are prone to overtopping in tropical cyclones, as occurred with Cyclone Bebe, which was a very early-season storm that passed through the Tuvaluan atolls in October 1972. Cyclone Bebe submerged Funafuti, eliminating 90% of structures on the island. Sources of drinking water were contaminated as a result of the system's storm surge and the flooding of the sources of fresh water.
Cyclone Bebe in 1972 caused severe damage to Funafuti. Funafuti's Tepuka Vili Vili islet was devastated by Cyclone Meli in 1979, with all its vegetation and most of its sand swept away during the cyclone. Along with a tropical depression that affected the islands a few days later, Severe Tropical Cyclone Ofa had a major impact on Tuvalu with most islands reporting damage to vegetation and crops. Cyclone Gavin was first identified during 2 March 1997, and was the first of three tropical cyclones to affect Tuvalu during the 1996–97 cyclone season with Cyclones Hina and Keli following later in the season.
In March 2015, the winds and storm surge created by Cyclone Pam resulted in waves of 3 metres (9.8 ft) to 5 metres (16 ft) breaking over the reef of the outer islands caused damage to houses, crops and infrastructure. On Nui the sources of fresh water were destroyed or contaminated. The flooding in Nui and Nukufetau caused many families to shelter in evacuation centres or with other families. Nui suffered the most damage of the three central islands (Nui, Nukufetau and Vaitupu); with both Nui and Nukufetau suffering the loss of 90% of the crops. Of the three northern islands (Nanumanga, Niutao, Nanumea), Nanumanga suffered the most damage, with 60-100 houses flooded, with the waves also causing damage to the health facility. Vasafua islet, part of the Funafuti Conservation Area, was severely damaged by Cyclone Pam. The coconut palms were washed away, leaving the islet as a sand bar.
The Tuvalu Government carried out assessments of the damage caused by Cyclone Pam to the islands and has provided medical aid, food as well as assistance for the cleaning-up of storm debris. Government and Non-Government Organisations provided assistance technical, funding and material support to Tuvalu to assist with recovery, including WHO, UNICEF, UNDP, OCHA, World Bank, DFAT, New Zealand Red Cross & IFRC, Fiji National University and governments of New Zealand, Netherlands, UAE, Taiwan and the United States.
Whether there are measurable changes in the sea level relative to the islands of Tuvalu is a contentious issue. There were problems associated with the pre-1993 sea level records from Funafuti which resulted in improvements in the recording technology to provide more reliable data for analysis. The degree of uncertainty as to estimates of sea level change relative to the islands of Tuvalu was reflected in the conclusions made in 2002 from the available data. The 2011 report of the Pacific Climate Change Science Program published by the Australian Government, concludes: "The sea-level rise near Tuvalu measured by satellite altimeters since 1993 is about 5 mm (0.2 in) per year."
The atolls have shown resilience to gradual sea-level rise, with atolls and reef islands being able to grow under current climate conditions by generating sufficient sand and coral debris that accumulates and gets dumped on the islands during cyclones. Gradual sea-level rise also allows for coral polyp activity to increase the reefs. However, if the increase in sea level occurs at faster rate as compared to coral growth, or if polyp activity is damaged by ocean acidification, then the resilience of the atolls and reef islands is less certain. The 2011 report of Pacific Climate Change Science Program of Australia concludes, in relation to Tuvalu, states the conclusions that over the course of the 21st century:
While some commentators have called for the relocation of Tuvalu's population to Australia, New Zealand or Kioa in Fiji, in 2006 Maatia Toafa (Prime Minister from 2004–2006) said his government did not regard rising sea levels as such a threat that the entire population would need to be evacuated. In 2013 Enele Sopoaga, the prime minister of Tuvalu, said that relocating Tuvaluans to avoid the impact of sea level rise "should never be an option because it is self defeating in itself. For Tuvalu I think we really need to mobilise public opinion in the Pacific as well as in the [rest of] world to really talk to their lawmakers to please have some sort of moral obligation and things like that to do the right thing."
Unlike the Spanish milled dollar the U.S. dollar is based upon a decimal system of values. In addition to the dollar the coinage act officially established monetary units of mill or one-thousandth of a dollar (symbol ₥), cent or one-hundredth of a dollar (symbol ¢), dime or one-tenth of a dollar, and eagle or ten dollars, with prescribed weights and composition of gold, silver, or copper for each. It was proposed in the mid-1800s that one hundred dollars be known as a union, but no union coins were ever struck and only patterns for the $50 half union exist. However, only cents are in everyday use as divisions of the dollar; "dime" is used solely as the name of the coin with the value of 10¢, while "eagle" and "mill" are largely unknown to the general public, though mills are sometimes used in matters of tax levies, and gasoline prices are usually in the form of $X.XX9 per gallon, e.g., $3.599, sometimes written as $3.599⁄10. When currently issued in circulating form, denominations equal to or less than a dollar are emitted as U.S. coins while denominations equal to or greater than a dollar are emitted as Federal Reserve notes (with the exception of gold, silver and platinum coins valued up to $100 as legal tender, but worth far more as bullion). Both one-dollar coins and notes are produced today, although the note form is significantly more common. In the past, "paper money" was occasionally issued in denominations less than a dollar (fractional currency) and gold coins were issued for circulation up to the value of $20 (known as the "double eagle", discontinued in the 1930s). The term eagle was used in the Coinage Act of 1792 for the denomination of ten dollars, and subsequently was used in naming gold coins. Paper currency less than one dollar in denomination, known as "fractional currency", was also sometimes pejoratively referred to as "shinplasters". In 1854, James Guthrie, then Secretary of the Treasury, proposed creating $100, $50 and $25 gold coins, which were referred to as a "Union", "Half Union", and "Quarter Union", thus implying a denomination of 1 Union = $100.
The symbol $, usually written before the numerical amount, is used for the U.S. dollar (as well as for many other currencies). The sign was the result of a late 18th-century evolution of the scribal abbreviation "ps" for the peso, the common name for the Spanish dollars that were in wide circulation in the New World from the 16th to the 19th centuries. These Spanish pesos or dollars were minted in Spanish America, namely in Mexico City, Potosí, Bolivia; and Lima, Peru. The p and the s eventually came to be written over each other giving rise to $.
Though still predominantly green, post-2004 series incorporate other colors to better distinguish different denominations. As a result of a 2008 decision in an accessibility lawsuit filed by the American Council of the Blind, the Bureau of Engraving and Printing is planning to implement a raised tactile feature in the next redesign of each note, except the $1 and the version of the $100 bill already in process. It also plans larger, higher-contrast numerals, more color differences, and distribution of currency readers to assist the visually impaired during the transition period.
The Constitution of the United States of America provides that the United States Congress has the power "To coin money". Laws implementing this power are currently codified at 31 U.S.C. § 5112. Section 5112 prescribes the forms, in which the United States dollars should be issued. These coins are both designated in Section 5112 as "legal tender" in payment of debts. The Sacagawea dollar is one example of the copper alloy dollar. The pure silver dollar is known as the American Silver Eagle. Section 5112 also provides for the minting and issuance of other coins, which have values ranging from one cent to 50 dollars. These other coins are more fully described in Coins of the United States dollar.
In the 16th century, Count Hieronymus Schlick of Bohemia began minting coins known as Joachimstalers (from German thal, or nowadays usually Tal, "valley", cognate with "dale" in English), named for Joachimstal, the valley where the silver was mined (St. Joachim's Valley, now Jáchymov; then part of the Kingdom of Bohemia, now part of the Czech Republic). Joachimstaler was later shortened to the German Taler, a word that eventually found its way into Danish and Swedish as daler, Norwegian as dalar and daler, Dutch as daler or daalder, Ethiopian as ታላሪ (talari), Hungarian as tallér, Italian as tallero, and English as dollar. Alternatively, thaler is said to come from the German coin Guldengroschen ("great guilder", being of silver but equal in value to a gold guilder), minted from the silver from Joachimsthal.
The early currency of the United States did not exhibit faces of presidents, as is the custom now; although today, by law, only the portrait of a deceased individual may appear on United States currency. In fact, the newly formed government was against having portraits of leaders on the currency, a practice compared to the policies of European monarchs. The currency as we know it today did not get the faces they currently have until after the early 20th century; before that "heads" side of coinage used profile faces and striding, seated, and standing figures from Greek and Roman mythology and composite Native Americans. The last coins to be converted to profiles of historic Americans were the dime (1946) and the Dollar (1971).
In 1862, paper money was issued without the backing of precious metals, due to the Civil War. Silver and gold coins continued to be issued and in 1878 the link between paper money and coins was reinstated. This disconnection from gold and silver backing also occurred during the War of 1812. The use of paper money not backed by precious metals had also occurred under the Articles of Confederation from 1777 to 1788. With no solid backing and being easily counterfeited, the continentals quickly lost their value, giving rise to the phrase "not worth a continental". This was a primary reason for the "No state shall... make any thing but gold and silver coin a tender in payment of debts" clause in article 1, section 10 of the United States Constitution.
In February 2007, the U.S. Mint, under the Presidential $1 Coin Act of 2005, introduced a new $1 U.S. Presidential dollar coin. Based on the success of the "50 State Quarters" series, the new coin features a sequence of presidents in order of their inaugurations, starting with George Washington, on the obverse side. The reverse side features the Statue of Liberty. To allow for larger, more detailed portraits, the traditional inscriptions of "E Pluribus Unum", "In God We Trust", the year of minting or issuance, and the mint mark will be inscribed on the edge of the coin instead of the face. This feature, similar to the edge inscriptions seen on the British £1 coin, is not usually associated with U.S. coin designs. The inscription "Liberty" has been eliminated, with the Statue of Liberty serving as a sufficient replacement. In addition, due to the nature of U.S. coins, this will be the first time there will be circulating U.S. coins of different denominations with the same president featured on the obverse (heads) side (Lincoln/penny, Jefferson/nickel, Franklin D. Roosevelt/dime, Washington/quarter, Kennedy/half dollar, and Eisenhower/dollar). Another unusual fact about the new $1 coin is Grover Cleveland will have two coins with his portrait issued due to the fact he was the only U.S. President to be elected to two non-consecutive terms.
When the Federal Reserve makes a purchase, it credits the seller's reserve account (with the Federal Reserve). This money is not transferred from any existing funds—it is at this point that the Federal Reserve has created new high-powered money. Commercial banks can freely withdraw in cash any excess reserves from their reserve account at the Federal Reserve. To fulfill those requests, the Federal Reserve places an order for printed money from the U.S. Treasury Department. The Treasury Department in turn sends these requests to the Bureau of Engraving and Printing (to print new dollar bills) and the Bureau of the Mint (to stamp the coins).
The value of the U.S. dollar declined significantly during wartime, especially during the American Civil War, World War I, and World War II. The Federal Reserve, which was established in 1913, was designed to furnish an "elastic" currency subject to "substantial changes of quantity over short periods", which differed significantly from previous forms of high-powered money such as gold, national bank notes, and silver coins. Over the very long run, the prior gold standard kept prices stable—for instance, the price level and the value of the U.S. dollar in 1914 was not very different from the price level in the 1880s. The Federal Reserve initially succeeded in maintaining the value of the U.S. dollar and price stability, reversing the inflation caused by the First World War and stabilizing the value of the dollar during the 1920s, before presiding over a 30% deflation in U.S. prices in the 1930s.
There is ongoing debate about whether central banks should target zero inflation (which would mean a constant value for the U.S. dollar over time) or low, stable inflation (which would mean a continuously but slowly declining value of the dollar over time, as is the case now). Although some economists are in favor of a zero inflation policy and therefore a constant value for the U.S. dollar, others contend that such a policy limits the ability of the central bank to control interest rates and stimulate the economy when needed.
The word "dollar" is one of the words in the first paragraph of Section 9 of Article 1 of the U.S. Constitution. In that context, "dollars" is a reference to the Spanish milled dollar, a coin that had a monetary value of 8 Spanish units of currency, or reales. In 1792 the U.S. Congress adopted legislation titled An act establishing a mint, and regulating the Coins of the United States. Section 9 of that act authorized the production of various coins, including "DOLLARS OR UNITS—each to be of the value of a Spanish milled dollar as the same is now current, and to contain three hundred and seventy-one grains and four sixteenth parts of a grain of pure, or four hundred and sixteen grains of standard silver". Section 20 of the act provided, "That the money of account of the United States shall be expressed in dollars, or units... and that all accounts in the public offices and all proceedings in the courts of the United States shall be kept and had in conformity to this regulation". In other words, this act designated the United States dollar as the unit of currency of the United States.
A "grand", sometimes shortened to simply "G", is a common term for the amount of $1,000. The suffix "K" or "k" (from "kilo-") is also commonly used to denote this amount (such as "$10k" to mean $10,000). However, the $1,000 note is no longer in general use. A "large" or "stack", it is usually a reference to a multiple of $1,000 (such as "fifty large" meaning $50,000). The $100 note is nicknamed "Benjamin", "Benji", "Ben", or "Franklin" (after Benjamin Franklin), "C-note" (C being the Roman numeral for 100), "Century note" or "bill" (e.g. "two bills" being $200). The $50 note is occasionally called a "yardstick" or a "grant" (after President Ulysses S. Grant, pictured on the obverse). The $20 note is referred to as a "double sawbuck", "Jackson" (after Andrew Jackson), or "double eagle". The $10 note is referred to as a "sawbuck", "ten-spot" or "Hamilton" (after Alexander Hamilton). The $5 note as "Lincoln", "fin", "fiver" or "five-spot". The infrequently-used $2 note is sometimes called "deuce", "Tom", or "Jefferson" (after Thomas Jefferson). The $1 note as a "single" or "buck". The dollar has also been, referred to as a "bone" and "bones" in plural (e.g. "twenty bones" is equal to $20). The newer designs, with portraits displayed in the main body of the obverse rather than in cameo insets upon paper color-coded by denomination, are sometimes referred to as "bigface" notes or "Monopoly money".
The U.S. dollar was created by the Constitution and defined by the Coinage Act of 1792. It specified a "dollar" to be based in the Spanish milled dollar and of 371 grains and 4 sixteenths part of a grain of pure or 416 grains (27.0 g) of standard silver and an "eagle" to be 247 and 4 eighths of a grain or 270 grains (17 g) of gold (again depending on purity). The choice of the value 371 grains arose from Alexander Hamilton's decision to base the new American unit on the average weight of a selection of worn Spanish dollars. Hamilton got the treasury to weigh a sample of Spanish dollars and the average weight came out to be 371 grains. A new Spanish dollar was usually about 377 grains in weight, and so the new U.S. dollar was at a slight discount in relation to the Spanish dollar.
The United States Mint produces Proof Sets specifically for collectors and speculators. Silver Proofs tend to be the standard designs but with the dime, quarter, and half dollar containing 90% silver. Starting in 1983 and ending in 1997, the Mint also produced proof sets containing the year's commemorative coins alongside the regular coins. Another type of proof set is the Presidential Dollar Proof Set where four special $1 coins are minted each year featuring a president. Because of budget constraints and increasing stockpiles of these relatively unpopular coins, the production of new Presidential dollar coins for circulation was suspended on December 13, 2011, by U.S. Treasury Secretary Timothy F. Geithner. Future minting of such coins will be made solely for collectors.
The Constitution provides that "a regular Statement and Account of the Receipts and Expenditures of all public Money shall be published from time to time". That provision of the Constitution is made specific by Section 331 of Title 31 of the United States Code. The sums of money reported in the "Statements" are currently being expressed in U.S. dollars (for example, see the 2009 Financial Report of the United States Government). The U.S. dollar may therefore be described as the unit of account of the United States.
Currently printed denominations are $1, $2, $5, $10, $20, $50, and $100. Notes above the $100 denomination stopped being printed in 1946 and were officially withdrawn from circulation in 1969. These notes were used primarily in inter-bank transactions or by organized crime; it was the latter usage that prompted President Richard Nixon to issue an executive order in 1969 halting their use. With the advent of electronic banking, they became less necessary. Notes in denominations of $500, $1,000, $5,000, $10,000 and $100,000 were all produced at one time; see large denomination bills in U.S. currency for details. These notes are now collectors' items and are worth more than their face value to collectors.
The colloquialism "buck"(s) (much like the British word "quid"(s, pl) for the pound sterling) is often used to refer to dollars of various nations, including the U.S. dollar. This term, dating to the 18th century, may have originated with the colonial leather trade. It may also have originated from a poker term. "Greenback" is another nickname originally applied specifically to the 19th century Demand Note dollars created by Abraham Lincoln to finance the costs of the Civil War for the North. The original note was printed in black and green on the back side. It is still used to refer to the U.S. dollar (but not to the dollars of other countries). Other well-known names of the dollar as a whole in denominations include "greenmail", "green" and "dead presidents" (the last because deceased presidents are pictured on most bills).
The value of the U.S. dollar was therefore no longer anchored to gold, and it fell upon the Federal Reserve to maintain the value of the U.S. currency. The Federal Reserve, however, continued to increase the money supply, resulting in stagflation and a rapidly declining value of the U.S. dollar in the 1970s. This was largely due to the prevailing economic view at the time that inflation and real economic growth were linked (the Phillips curve), and so inflation was regarded as relatively benign. Between 1965 and 1981, the U.S. dollar lost two thirds of its value.
The dollar was first based on the value and look of the Spanish dollar, used widely in Spanish America from the 16th to the 19th centuries. The first dollar coins issued by the United States Mint (founded 1792) were similar in size and composition to the Spanish dollar, minted in Mexico and Peru. The Spanish, U.S. silver dollars, and later, Mexican silver pesos circulated side by side in the United States, and the Spanish dollar and Mexican peso remained legal tender until the Coinage Act of 1857. The coinage of various English colonies also circulated. The lion dollar was popular in the Dutch New Netherland Colony (New York), but the lion dollar also circulated throughout the English colonies during the 17th century and early 18th century. Examples circulating in the colonies were usually worn so that the design was not fully distinguishable, thus they were sometimes referred to as "dog dollars".
The Gold Standard Act of 1900 abandoned the bimetallic standard and defined the dollar as 23.22 grains (1.505 g) of gold, equivalent to setting the price of 1 troy ounce of gold at $20.67. Silver coins continued to be issued for circulation until 1964, when all silver was removed from dimes and quarters, and the half dollar was reduced to 40% silver. Silver half dollars were last issued for circulation in 1970. Gold coins were confiscated by Executive Order 6102 issued in 1933 by Franklin Roosevelt. The gold standard was changed to 13.71 grains (0.888 g), equivalent to setting the price of 1 troy ounce of gold at $35. This standard persisted until 1968.
Early releases of the Washington coin included error coins shipped primarily from the Philadelphia mint to Florida and Tennessee banks. Highly sought after by collectors, and trading for as much as $850 each within a week of discovery, the error coins were identified by the absence of the edge impressions "E PLURIBUS UNUM IN GOD WE TRUST 2007 P". The mint of origin is generally accepted to be mostly Philadelphia, although identifying the source mint is impossible without opening a mint pack also containing marked units. Edge lettering is minted in both orientations with respect to "heads", some amateur collectors were initially duped into buying "upside down lettering error" coins. Some cynics also erroneously point out that the Federal Reserve makes more profit from dollar bills than dollar coins because they wear out in a few years, whereas coins are more permanent. The fallacy of this argument arises because new notes printed to replace worn out notes, which have been withdrawn from circulation, bring in no net revenue to the government to offset the costs of printing new notes and destroying the old ones. As most vending machines are incapable of making change in banknotes, they commonly accept only $1 bills, though a few will give change in dollar coins.
The U.S. Constitution provides that Congress shall have the power to "borrow money on the credit of the United States". Congress has exercised that power by authorizing Federal Reserve Banks to issue Federal Reserve Notes. Those notes are "obligations of the United States" and "shall be redeemed in lawful money on demand at the Treasury Department of the United States, in the city of Washington, District of Columbia, or at any Federal Reserve bank". Federal Reserve Notes are designated by law as "legal tender" for the payment of debts. Congress has also authorized the issuance of more than 10 other types of banknotes, including the United States Note and the Federal Reserve Bank Note. The Federal Reserve Note is the only type that remains in circulation since the 1970s.
Usually, the short-term goal of open market operations is to achieve a specific short-term interest rate target. In other instances, monetary policy might instead entail the targeting of a specific exchange rate relative to some foreign currency or else relative to gold. For example, in the case of the United States the Federal Reserve targets the federal funds rate, the rate at which member banks lend to one another overnight. The other primary means of conducting monetary policy include: (i) Discount window lending (as lender of last resort); (ii) Fractional deposit lending (changes in the reserve requirement); (iii) Moral suasion (cajoling certain market players to achieve specified outcomes); (iv) "Open mouth operations" (talking monetary policy with the market).
Under the Bretton Woods system established after World War II, the value of gold was fixed to $35 per ounce, and the value of the U.S. dollar was thus anchored to the value of gold. Rising government spending in the 1960s, however, led to doubts about the ability of the United States to maintain this convertibility, gold stocks dwindled as banks and international investors began to convert dollars to gold, and as a result the value of the dollar began to decline. Facing an emerging currency crisis and the imminent danger that the United States would no longer be able to redeem dollars for gold, gold convertibility was finally terminated in 1971 by President Nixon, resulting in the "Nixon shock".
The U.S. dollar is fiat money. It is the currency most used in international transactions and is the world's most dominant reserve currency. Several countries use it as their official currency, and in many others it is the de facto currency. Besides the United States, it is also used as the sole currency in two British Overseas Territories in the Caribbean: the British Virgin Islands and the Turks and Caicos islands. A few countries use only the U.S. Dollar for paper money, while the country mints its own coins, or also accepts U.S. coins that can be used as payment in U.S. dollars, such as the Susan B. Anthony dollar.
Today, USD notes are made from cotton fiber paper, unlike most common paper, which is made of wood fiber. U.S. coins are produced by the United States Mint. U.S. dollar banknotes are printed by the Bureau of Engraving and Printing and, since 1914, have been issued by the Federal Reserve. The "large-sized notes" issued before 1928 measured 7.42 inches (188 mm) by 3.125 inches (79.4 mm); small-sized notes, introduced that year, measure 6.14 inches (156 mm) by 2.61 inches (66 mm) by 0.0043 inches (0.11 mm). When the current, smaller sized U.S. currency was introduced it was referred to as Philippine-sized currency because the Philippines had previously adopted the same size for its legal currency.
From 1792, when the Mint Act was passed, the dollar was defined as 371.25 grains (24.056 g) of silver. Many historians[who?] erroneously assume gold was standardized at a fixed rate in parity with silver; however, there is no evidence of Congress making this law. This has to do with Alexander Hamilton's suggestion to Congress of a fixed 15:1 ratio of silver to gold, respectively. The gold coins that were minted however, were not given any denomination whatsoever and traded for a market value relative to the Congressional standard of the silver dollar. 1834 saw a shift in the gold standard to 23.2 grains (1.50 g), followed by a slight adjustment to 23.22 grains (1.505 g) in 1837 (16:1 ratio).[citation needed]
Technically, all these coins are still legal tender at face value, though some are far more valuable today for their numismatic value, and for gold and silver coins, their precious metal value. From 1965 to 1970 the Kennedy half dollar was the only circulating coin with any silver content, which was removed in 1971 and replaced with cupronickel. However, since 1992, the U.S. Mint has produced special Silver Proof Sets in addition to the regular yearly proof sets with silver dimes, quarters, and half dollars in place of the standard copper-nickel versions. In addition, an experimental $4.00 (Stella) coin was also minted in 1879, but never placed into circulation, and is properly considered to be a pattern rather than an actual coin denomination.
Dollar coins have not been very popular in the United States. Silver dollars were minted intermittently from 1794 through 1935; a copper-nickel dollar of the same large size, featuring President Dwight D. Eisenhower, was minted from 1971 through 1978. Gold dollars were also minted in the 19th century. The Susan B. Anthony dollar coin was introduced in 1979; these proved to be unpopular because they were often mistaken for quarters, due to their nearly equal size, their milled edge, and their similar color. Minting of these dollars for circulation was suspended in 1980 (collectors' pieces were struck in 1981), but, as with all past U.S. coins, they remain legal tender. As the number of Anthony dollars held by the Federal Reserve and dispensed primarily to make change in postal and transit vending machines had been virtually exhausted, additional Anthony dollars were struck in 1999. In 2000, a new $1 coin, featuring Sacagawea, (the Sacagawea dollar) was introduced, which corrected some of the problems of the Anthony dollar by having a smooth edge and a gold color, without requiring changes to vending machines that accept the Anthony dollar. However, this new coin has failed to achieve the popularity of the still-existing $1 bill and is rarely used in daily transactions. The failure to simultaneously withdraw the dollar bill and weak publicity efforts have been cited by coin proponents as primary reasons for the failure of the dollar coin to gain popular support.
The monetary base consists of coins and Federal Reserve Notes in circulation outside the Federal Reserve Banks and the U.S. Treasury, plus deposits held by depository institutions at Federal Reserve Banks. The adjusted monetary base has increased from approximately 400 billion dollars in 1994, to 800 billion in 2005, and over 3000 billion in 2013. The amount of cash in circulation is increased (or decreased) by the actions of the Federal Reserve System. Eight times a year, the 12-person Federal Open Market Committee meet to determine U.S. monetary policy. Every business day, the Federal Reserve System engages in Open market operations to carry out that monetary policy. If the Federal Reserve desires to increase the money supply, it will buy securities (such as U.S. Treasury Bonds) anonymously from banks in exchange for dollars. Conversely, it will sell securities to the banks in exchange for dollars, to take dollars out of circulation.
The decline in the value of the U.S. dollar corresponds to price inflation, which is a rise in the general level of prices of goods and services in an economy over a period of time. A consumer price index (CPI) is a measure estimating the average price of consumer goods and services purchased by households. The United States Consumer Price Index, published by the Bureau of Labor Statistics, is a measure estimating the average price of consumer goods and services in the United States. It reflects inflation as experienced by consumers in their day-to-day living expenses. A graph showing the U.S. CPI relative to 1982–1984 and the annual year-over-year change in CPI is shown at right.
The word "animal" comes from the Latin animalis, meaning having breath, having soul or living being. In everyday non-scientific usage the word excludes humans – that is, "animal" is often used to refer only to non-human members of the kingdom Animalia; often, only closer relatives of humans such as mammals, or mammals and other vertebrates, are meant. The biological definition of the word refers to all members of the kingdom Animalia, encompassing creatures as diverse as sponges, jellyfish, insects, and humans.
All animals have eukaryotic cells, surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules. During development, it forms a relatively flexible framework upon which cells can move about and be reorganized, making complex structures possible. In contrast, other multicellular organisms, like plants and fungi, have cells held in place by cell walls, and so develop by progressive growth. Also, unique to animal cells are the following intercellular junctions: tight junctions, gap junctions, and desmosomes.
Predation is a biological interaction where a predator (a heterotroph that is hunting) feeds on its prey (the organism that is attacked). Predators may or may not kill their prey prior to feeding on them, but the act of predation almost always results in the death of the prey. The other main category of consumption is detritivory, the consumption of dead organic matter. It can at times be difficult to separate the two feeding behaviours, for example, where parasitic species prey on a host organism and then lay their eggs on it for their offspring to feed on its decaying corpse. Selective pressures imposed on one another has led to an evolutionary arms race between prey and predator, resulting in various antipredator adaptations.
Among the other phyla, the Ctenophora and the Cnidaria, which includes sea anemones, corals, and jellyfish, are radially symmetric and have digestive chambers with a single opening, which serves as both the mouth and the anus. Both have distinct tissues, but they are not organized into organs. There are only two main germ layers, the ectoderm and endoderm, with only scattered cells between them. As such, these animals are sometimes called diploblastic. The tiny placozoans are similar, but they do not have a permanent digestive chamber.
Animals have several characteristics that set them apart from other living things. Animals are eukaryotic and multicellular, which separates them from bacteria and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking rigid cell walls. All animals are motile, if only at certain life stages. In most animals, embryos pass through a blastula stage, which is a characteristic exclusive to animals.
A zygote initially develops into a hollow sphere, called a blastula, which undergoes rearrangement and differentiation. In sponges, blastula larvae swim to a new location and develop into a new sponge. In most other groups, the blastula undergoes more complicated rearrangement. It first invaginates to form a gastrula with a digestive chamber, and two separate germ layers — an external ectoderm and an internal endoderm. In most cases, a mesoderm also develops between them. These germ layers then differentiate to form tissues and organs.
Some paleontologists suggest that animals appeared much earlier than the Cambrian explosion, possibly as early as 1 billion years ago. Trace fossils such as tracks and burrows found in the Tonian period indicate the presence of triploblastic worms, like metazoans, roughly as large (about 5 mm wide) and complex as earthworms. During the beginning of the Tonian period around 1 billion years ago, there was a decrease in Stromatolite diversity, which may indicate the appearance of grazing animals, since stromatolite diversity increased when grazing animals went extinct at the End Permian and End Ordovician extinction events, and decreased shortly after the grazer populations recovered. However the discovery that tracks very similar to these early trace fossils are produced today by the giant single-celled protist Gromia sphaerica casts doubt on their interpretation as evidence of early animal evolution.
Animals are generally considered to have evolved from a flagellated eukaryote. Their closest known living relatives are the choanoflagellates, collared flagellates that have a morphology similar to the choanocytes of certain sponges. Molecular studies place animals in a supergroup called the opisthokonts, which also include the choanoflagellates, fungi and a few small parasitic protists. The name comes from the posterior location of the flagellum in motile cells, such as most animal spermatozoa, whereas other eukaryotes tend to have anterior flagella.
The remaining animals form a monophyletic group called the Bilateria. For the most part, they are bilaterally symmetric, and often have a specialized head with feeding and sensory organs. The body is triploblastic, i.e. all three germ layers are well-developed, and tissues form distinct organs. The digestive chamber has two openings, a mouth and an anus, and there is also an internal body cavity called a coelom or pseudocoelom. There are exceptions to each of these characteristics, however — for instance adult echinoderms are radially symmetric, and certain parasitic worms have extremely simplified body structures.
Traditional morphological and modern molecular phylogenetic analysis have both recognized a major evolutionary transition from "non-bilaterian" animals, which are those lacking a bilaterally symmetric body plan (Porifera, Ctenophora, Cnidaria and Placozoa), to "bilaterian" animals (Bilateria) whose body plans display bilateral symmetry. The latter are further classified based on a major division between Deuterostomes and Protostomes. The relationships among non-bilaterian animals are disputed, but all bilaterian animals are thought to form a monophyletic group. Current understanding of the relationships among the major groups of animals is summarized by the following cladogram:
The Ecdysozoa are protostomes, named after the common trait of growth by moulting or ecdysis. The largest animal phylum belongs here, the Arthropoda, including insects, spiders, crabs, and their kin. All these organisms have a body divided into repeating segments, typically with paired appendages. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The ecdysozoans also include the Nematoda or roundworms, perhaps the second largest animal phylum. Roundworms are typically microscopic, and occur in nearly every environment where there is water. A number are important parasites. Smaller phyla related to them are the Nematomorpha or horsehair worms, and the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom.
Because of the great diversity found in animals, it is more economical for scientists to study a small number of chosen species so that connections can be drawn from their work and conclusions extrapolated about how animals function in general. Because they are easy to keep and breed, the fruit fly Drosophila melanogaster and the nematode Caenorhabditis elegans have long been the most intensively studied metazoan model organisms, and were among the first life-forms to be genetically sequenced. This was facilitated by the severely reduced state of their genomes, but as many genes, introns, and linkages lost, these ecdysozoans can teach us little about the origins of animals in general. The extent of this type of evolution within the superphylum will be revealed by the crustacean, annelid, and molluscan genome projects currently in progress. Analysis of the starlet sea anemone genome has emphasised the importance of sponges, placozoans, and choanoflagellates, also being sequenced, in explaining the arrival of 1500 ancestral genes unique to the Eumetazoa.
The Lophotrochozoa, evolved within Protostomia, include two of the most successful animal phyla, the Mollusca and Annelida. The former, which is the second-largest animal phylum by number of described species, includes animals such as snails, clams, and squids, and the latter comprises the segmented worms, such as earthworms and leeches. These two groups have long been considered close relatives because of the common presence of trochophore larvae, but the annelids were considered closer to the arthropods because they are both segmented. Now, this is generally considered convergent evolution, owing to many morphological and genetic differences between the two phyla. The Lophotrochozoa also include the Nemertea or ribbon worms, the Sipuncula, and several phyla that have a ring of ciliated tentacles around the mouth, called a lophophore. These were traditionally grouped together as the lophophorates. but it now appears that the lophophorate group may be paraphyletic, with some closer to the nemerteans and some to the molluscs and annelids. They include the Brachiopoda or lamp shells, which are prominent in the fossil record, the Entoprocta, the Phoronida, and possibly the Bryozoa or moss animals.
Several animal phyla are recognized for their lack of bilateral symmetry, and are thought to have diverged from other animals early in evolution. Among these, the sponges (Porifera) were long thought to have diverged first, representing the oldest animal phylum. They lack the complex organization found in most other phyla. Their cells are differentiated, but in most cases not organized into distinct tissues. Sponges typically feed by drawing in water through pores. However, a series of phylogenomic studies from 2008-2015 have found support for Ctenophora, or comb jellies, as the basal lineage of animals. This result has been controversial, since it would imply that that sponges may not be so primitive, but may instead be secondarily simplified. Other researchers have argued that the placement of Ctenophora as the earliest-diverging animal phylum is a statistical anomaly caused by the high rate of evolution in ctenophore genomes.
Deuterostomes differ from protostomes in several ways. Animals from both groups possess a complete digestive tract. However, in protostomes, the first opening of the gut to appear in embryological development (the archenteron) develops into the mouth, with the anus forming secondarily. In deuterostomes the anus forms first, with the mouth developing secondarily. In most protostomes, cells simply fill in the interior of the gastrula to form the mesoderm, called schizocoelous development, but in deuterostomes, it forms through invagination of the endoderm, called enterocoelic pouching. Deuterostome embryos undergo radial cleavage during cell division, while protostomes undergo spiral cleavage.
The Platyzoa include the phylum Platyhelminthes, the flatworms. These were originally considered some of the most primitive Bilateria, but it now appears they developed from more complex ancestors. A number of parasites are included in this group, such as the flukes and tapeworms. Flatworms are acoelomates, lacking a body cavity, as are their closest relatives, the microscopic Gastrotricha. The other platyzoan phyla are mostly microscopic and pseudocoelomate. The most prominent are the Rotifera or rotifers, which are common in aqueous environments. They also include the Acanthocephala or spiny-headed worms, the Gnathostomulida, Micrognathozoa, and possibly the Cycliophora. These groups share the presence of complex jaws, from which they are called the Gnathifera.
Most animals indirectly use the energy of sunlight by eating plants or plant-eating animals. Most plants use light to convert inorganic molecules in their environment into carbohydrates, fats, proteins and other biomolecules, characteristically containing reduced carbon in the form of carbon-hydrogen bonds. Starting with carbon dioxide (CO2) and water (H2O), photosynthesis converts the energy of sunlight into chemical energy in the form of simple sugars (e.g., glucose), with the release of molecular oxygen. These sugars are then used as the building blocks for plant growth, including the production of other biomolecules. When an animal eats plants (or eats other animals which have eaten plants), the reduced carbon compounds in the food become a source of energy and building materials for the animal. They are either used directly to help the animal grow, or broken down, releasing stored solar energy, and giving the animal the energy required for motion.
Jehovah's Witnesses is a millenarian restorationist Christian denomination with nontrinitarian beliefs distinct from mainstream Christianity. The group claims a worldwide membership of more than 8.2 million adherents involved in evangelism, convention attendance figures of more than 15 million, and an annual Memorial attendance of more than 19.9 million. Jehovah's Witnesses are directed by the Governing Body of Jehovah's Witnesses, a group of elders in Brooklyn, New York, which establishes all doctrines based on its interpretations of the Bible. They prefer to use their own translation, the New World Translation of the Holy Scriptures, although their literature occasionally quotes and cites other translations. They believe that the destruction of the present world system at Armageddon is imminent, and that the establishment of God's kingdom over the earth is the only solution for all problems faced by humanity.
Jehovah's Witnesses are best known for their door-to-door preaching, distributing literature such as The Watchtower and Awake!, and refusing military service and blood transfusions. They consider use of the name Jehovah vital for proper worship. They reject Trinitarianism, inherent immortality of the soul, and hellfire, which they consider to be unscriptural doctrines. They do not observe Christmas, Easter, birthdays or other holidays and customs they consider to have pagan origins incompatible with Christianity. Adherents commonly refer to their body of beliefs as "the truth" and consider themselves to be "in the truth". They consider secular society to be morally corrupt and under the influence of Satan, and most limit their social interaction with non-Witnesses. Congregational disciplinary actions include disfellowshipping, their term for formal expulsion and shunning. Baptized individuals who formally leave are considered disassociated and are also shunned. Disfellowshipped and disassociated individuals may eventually be reinstated if deemed repentant.
In 1870, Charles Taze Russell and others formed a group in Pittsburgh, Pennsylvania, to study the Bible. During the course of his ministry, Russell disputed many beliefs of mainstream Christianity including immortality of the soul, hellfire, predestination, the fleshly return of Jesus Christ, the Trinity, and the burning up of the world. In 1876, Russell met Nelson H. Barbour; later that year they jointly produced the book Three Worlds, which combined restitutionist views with end time prophecy. The book taught that God's dealings with humanity were divided dispensationally, each ending with a "harvest," that Christ had returned as an invisible spirit being in 1874 inaugurating the "harvest of the Gospel age," and that 1914 would mark the end of a 2520-year period called "the Gentile Times," at which time world society would be replaced by the full establishment of God's kingdom on earth. Beginning in 1878 Russell and Barbour jointly edited a religious journal, Herald of the Morning. In June 1879 the two split over doctrinal differences, and in July, Russell began publishing the magazine Zion's Watch Tower and Herald of Christ's Presence, stating that its purpose was to demonstrate that the world was in "the last days," and that a new age of earthly and human restitution under the reign of Christ was imminent.
From 1879, Watch Tower supporters gathered as autonomous congregations to study the Bible topically. Thirty congregations were founded, and during 1879 and 1880, Russell visited each to provide the format he recommended for conducting meetings. As congregations continued to form during Russell's ministry, they each remained self-administrative, functioning under the congregationalist style of church governance. In 1881, Zion's Watch Tower Tract Society was presided over by William Henry Conley, and in 1884, Charles Taze Russell incorporated the society as a non-profit business to distribute tracts and Bibles. By about 1900, Russell had organized thousands of part- and full-time colporteurs, and was appointing foreign missionaries and establishing branch offices. By the 1910s, Russell's organization maintained nearly a hundred "pilgrims," or traveling preachers. Russell engaged in significant global publishing efforts during his ministry, and by 1912, he was the most distributed Christian author in the United States.
Russell moved the Watch Tower Society's headquarters to Brooklyn, New York, in 1909, combining printing and corporate offices with a house of worship; volunteers were housed in a nearby residence he named Bethel. He identified the religious movement as "Bible Students," and more formally as the International Bible Students Association. By 1910, about 50,000 people worldwide were associated with the movement and congregations re-elected him annually as their "pastor." Russell died October 31, 1916, at the age of 64 while returning from a ministerial speaking tour.
In January 1917, the Watch Tower Society's legal representative, Joseph Franklin Rutherford, was elected as its next president. His election was disputed, and members of the Board of Directors accused him of acting in an autocratic and secretive manner. The divisions between his supporters and opponents triggered a major turnover of members over the next decade. In June 1917, he released The Finished Mystery as a seventh volume of Russell's Studies in the Scriptures series. The book, published as the posthumous work of Russell, was a compilation of his commentaries on the Bible books of Ezekiel and Revelation, plus numerous additions by Bible Students Clayton Woodworth and George Fisher. It strongly criticized Catholic and Protestant clergy and Christian involvement in the Great War. As a result, Watch Tower Society directors were jailed for sedition under the Espionage Act in 1918 and members were subjected to mob violence; charges against the directors were dropped in 1920.
Rutherford centralized organizational control of the Watch Tower Society. In 1919, he instituted the appointment of a director in each congregation, and a year later all members were instructed to report their weekly preaching activity to the Brooklyn headquarters. At an international convention held at Cedar Point, Ohio, in September 1922, a new emphasis was made on house-to-house preaching. Significant changes in doctrine and administration were regularly introduced during Rutherford's twenty-five years as president, including the 1920 announcement that the Jewish patriarchs (such as Abraham and Isaac) would be resurrected in 1925, marking the beginning of Christ's thousand-year Kingdom. Disappointed by the changes, tens of thousands of defections occurred during the first half of Rutherford's tenure, leading to the formation of several Bible Student organizations independent of the Watch Tower Society, most of which still exist. By mid-1919, as many as one in seven of Russell-era Bible Students had ceased their association with the Society, and as many as two-thirds by the end of the 1920s.
On July 26, 1931, at a convention in Columbus, Ohio, Rutherford introduced the new name—Jehovah's witnesses—based on Isaiah 43:10: "Ye are my witnesses, saith Jehovah, and my servant whom I have chosen"—which was adopted by resolution. The name was chosen to distinguish his group of Bible Students from other independent groups that had severed ties with the Society, as well as symbolize the instigation of new outlooks and the promotion of fresh evangelizing methods. In 1932, Rutherford eliminated the system of locally elected elders and in 1938, introduced what he called a "theocratic" (literally, God-ruled) organizational system, under which appointments in congregations worldwide were made from the Brooklyn headquarters.
From 1932, it was taught that the "little flock" of 144,000 would not be the only people to survive Armageddon. Rutherford explained that in addition to the 144,000 "anointed" who would be resurrected—or transferred at death—to live in heaven to rule over earth with Christ, a separate class of members, the "great multitude," would live in a paradise restored on earth; from 1935, new converts to the movement were considered part of that class. By the mid-1930s, the timing of the beginning of Christ's presence (Greek: parousía), his enthronement as king, and the start of the "last days" were each moved to 1914.
Nathan Knorr was appointed as third president of the Watch Tower Bible and Tract Society in 1942. Knorr commissioned a new translation of the Bible, the New World Translation of the Holy Scriptures, the full version of which was released in 1961. He organized large international assemblies, instituted new training programs for members, and expanded missionary activity and branch offices throughout the world. Knorr's presidency was also marked by an increasing use of explicit instructions guiding Witnesses in their lifestyle and conduct, and a greater use of congregational judicial procedures to enforce a strict moral code.
From 1966, Witness publications and convention talks built anticipation of the possibility that Christ's thousand-year reign might begin in late 1975 or shortly thereafter. The number of baptisms increased significantly, from about 59,000 in 1966 to more than 297,000 in 1974. By 1975, the number of active members exceeded two million. Membership declined during the late 1970s after expectations for 1975 were proved wrong. Watch Tower Society literature did not state dogmatically that 1975 would definitely mark the end, but in 1980 the Watch Tower Society admitted its responsibility in building up hope regarding that year.
The offices of elder and ministerial servant were restored to Witness congregations in 1972, with appointments made from headquarters (and later, also by branch committees). It was announced that, starting in September 2014, appointments would be made by traveling overseers. In a major organizational overhaul in 1976, the power of the Watch Tower Society president was diminished, with authority for doctrinal and organizational decisions passed to the Governing Body. Since Knorr's death in 1977, the position of president has been occupied by Frederick Franz (1977–1992) and Milton Henschel (1992–2000), both members of the Governing Body, and since 2000 by Don A. Adams, not a member of the Governing Body. In 1995, Jehovah's Witnesses abandoned the idea that Armageddon must occur during the lives of the generation that was alive in 1914 and in 2013 changed their teaching on the "generation".
Jehovah's Witnesses are organized hierarchically, in what the leadership calls a "theocratic organization", reflecting their belief that it is God's "visible organization" on earth. The organization is led by the Governing Body—an all-male group that varies in size, but since early 2014 has comprised seven members,[note 1] all of whom profess to be of the "anointed" class with a hope of heavenly life—based in the Watch Tower Society's Brooklyn headquarters. There is no election for membership; new members are selected by the existing body. Until late 2012, the Governing Body described itself as the representative and "spokesman" for God's "faithful and discreet slave class" (approximately 10,000 self-professed "anointed" Jehovah's Witnesses). At the 2012 Annual Meeting of the Watch Tower Society, the "faithful and discreet slave" was defined as referring to the Governing Body only. The Governing Body directs several committees that are responsible for administrative functions, including publishing, assembly programs and evangelizing activities. It appoints all branch committee members and traveling overseers, after they have been recommended by local branches, with traveling overseers supervising circuits of congregations within their jurisdictions. Traveling overseers appoint local elders and ministerial servants, and while branch offices may appoint regional committees for matters such as Kingdom Hall construction or disaster relief.
Each congregation has a body of appointed unpaid male elders and ministerial servants. Elders maintain general responsibility for congregational governance, setting meeting times, selecting speakers and conducting meetings, directing the public preaching work, and creating "judicial committees" to investigate and decide disciplinary action for cases involving sexual misconduct or doctrinal breaches. New elders are appointed by a traveling overseer after recommendation by the existing body of elders. Ministerial servants—appointed in a similar manner to elders—fulfill clerical and attendant duties, but may also teach and conduct meetings. Witnesses do not use elder as a title to signify a formal clergy-laity division, though elders may employ ecclesiastical privilege such as confession of sins.
Baptism is a requirement for being considered a member of Jehovah's Witnesses. Jehovah's Witnesses do not practice infant baptism, and previous baptisms performed by other denominations are not considered valid. Individuals undergoing baptism must affirm publicly that dedication and baptism identify them "as one of Jehovah's Witnesses in association with God's spirit-directed organization," though Witness publications say baptism symbolizes personal dedication to God and not "to a man, work or organization." Their literature emphasizes the need for members to be obedient and loyal to Jehovah and to "his organization,"[note 2] stating that individuals must remain part of it to receive God's favor and to survive Armageddon.
Jehovah's Witnesses believe their religion is a restoration of first-century Christianity. Doctrines of Jehovah's Witnesses are established by the Governing Body, which assumes responsibility for interpreting and applying scripture. The Governing Body does not issue any single, comprehensive "statement of faith", but prefers to express its doctrinal position in a variety of ways through publications published by the Watch Tower Society. Their publications teach that doctrinal changes and refinements result from a process of progressive revelation, in which God gradually reveals his will and purpose, and that such enlightenment or "new light" results from the application of reason and study, the guidance of the holy spirit, and direction from Jesus Christ and angels. The Society also teaches that members of the Governing Body are helped by the holy spirit to discern "deep truths", which are then considered by the entire Governing Body before it makes doctrinal decisions. The religion's leadership, while disclaiming divine inspiration and infallibility, is said to provide "divine guidance" through its teachings described as "based on God's Word thus ... not from men, but from Jehovah."
The entire Protestant canon of scripture is considered the inspired, inerrant word of God. Jehovah's Witnesses consider the Bible to be scientifically and historically accurate and reliable and interpret much of it literally, but accept parts of it as symbolic. They consider the Bible to be the final authority for all their beliefs, although sociologist Andrew Holden's ethnographic study of the religion concluded that pronouncements of the Governing Body, through Watch Tower Society publications, carry almost as much weight as the Bible. Regular personal Bible reading is frequently recommended; Witnesses are discouraged from formulating doctrines and "private ideas" reached through Bible research independent of Watch Tower Society publications, and are cautioned against reading other religious literature. Adherents are told to have "complete confidence" in the leadership, avoid skepticism about what is taught in the Watch Tower Society's literature, and "not advocate or insist on personal opinions or harbor private ideas when it comes to Bible understanding." The religion makes no provision for members to criticize or contribute to official teachings and all Witnesses must abide by its doctrines and organizational requirements.
Jehovah's Witnesses believe that Jesus is God's only direct creation, that everything else was created by means of Christ, and that the initial unassisted act of creation uniquely identifies Jesus as God's "only-begotten Son". Jesus served as a redeemer and a ransom sacrifice to pay for the sins of humanity. They believe Jesus died on a single upright post rather than the traditional cross. They believe that references in the Bible to the Archangel Michael, Abaddon (Apollyon), and the Word all refer to Jesus. Jesus is considered to be the only intercessor and high priest between God and humanity, and appointed by God as the king and judge of his kingdom. His role as a mediator (referred to in 1 Timothy 2:5) is applied to the 'anointed' class, though the 'other sheep' are said to also benefit from the arrangement.
Witnesses believe that a "little flock" go to heaven, but that the hope for life after death for the majority of "other sheep" involves being resurrected by God to a cleansed earth after Armageddon. They interpret Revelation 14:1–5 to mean that the number of Christians going to heaven is limited to exactly 144,000, who will rule with Jesus as kings and priests over earth. Jehovah's Witnesses teach that only they meet scriptural requirements for surviving Armageddon, but that God is the final judge. During Christ's millennial reign, most people who died prior to Armageddon will be resurrected with the prospect of living forever; they will be taught the proper way to worship God to prepare them for their final test at the end of the millennium.
Jehovah's Witnesses believe that God's kingdom is a literal government in heaven, ruled by Jesus Christ and 144,000 "spirit-anointed" Christians drawn from the earth, which they associate with Jesus' reference to a "new covenant". The kingdom is viewed as the means by which God will accomplish his original purpose for the earth, transforming it into a paradise without sickness or death. It is said to have been the focal point of Jesus' ministry on earth. They believe the kingdom was established in heaven in 1914, and that Jehovah's Witnesses serve as representatives of the kingdom on earth.
A central teaching of Jehovah's Witnesses is that the current world era, or "system of things", entered the "last days" in 1914 and faces imminent destruction through intervention by God and Jesus Christ, leading to deliverance for those who worship God acceptably. They consider all other present-day religions to be false, identifying them with "Babylon the Great", or the "harlot", of Revelation 17, and believe that they will soon be destroyed by the United Nations, which they believe is represented in scripture by the scarlet-colored wild beast of Revelation chapter 17. This development will mark the beginning of the "great tribulation". Satan will subsequently attack Jehovah's Witnesses, an action that will prompt God to begin the war of Armageddon, during which all forms of government and all people not counted as Christ's "sheep", or true followers, will be destroyed. After Armageddon, God will extend his heavenly kingdom to include earth, which will be transformed into a paradise similar to the Garden of Eden. After Armageddon, most of those who had died before God's intervention will gradually be resurrected during "judgment day" lasting for one thousand years. This judgment will be based on their actions after resurrection rather than past deeds. At the end of the thousand years, Christ will hand all authority back to God. Then a final test will take place when Satan is released to mislead perfect mankind. Those who fail will be destroyed, along with Satan and his demons. The end result will be a fully tested, glorified human race.
Jehovah's Witnesses believe that Jesus Christ began to rule in heaven as king of God's kingdom in October 1914, and that Satan was subsequently ousted from heaven to the earth, resulting in "woe" to humanity. They believe that Jesus rules invisibly, from heaven, perceived only as a series of "signs". They base this belief on a rendering of the Greek word parousia—usually translated as "coming" when referring to Christ—as "presence". They believe Jesus' presence includes an unknown period beginning with his inauguration as king in heaven in 1914, and ending when he comes to bring a final judgment against humans on earth. They thus depart from the mainstream Christian belief that the "second coming" of Matthew 24 refers to a single moment of arrival on earth to judge humans.
Meetings for worship and study are held at Kingdom Halls, which are typically functional in character, and do not contain religious symbols. Witnesses are assigned to a congregation in whose "territory" they usually reside and attend weekly services they refer to as "meetings" as scheduled by congregation elders. The meetings are largely devoted to study of Watch Tower Society literature and the Bible. The format of the meetings is established by the religion's headquarters, and the subject matter for most meetings is the same worldwide. Congregations meet for two sessions each week comprising five distinct meetings that total about three-and-a-half hours, typically gathering mid-week (three meetings) and on the weekend (two meetings). Prior to 2009, congregations met three times each week; these meetings were condensed, with the intention that members dedicate an evening for "family worship". Gatherings are opened and closed with kingdom songs (hymns) and brief prayers. Twice each year, Witnesses from a number of congregations that form a "circuit" gather for a one-day assembly. Larger groups of congregations meet once a year for a three-day "regional convention", usually at rented stadiums or auditoriums. Their most important and solemn event is the commemoration of the "Lord's Evening Meal", or "Memorial of Christ's Death" on the date of the Jewish Passover.
Jehovah's Witnesses are perhaps best known for their efforts to spread their beliefs, most notably by visiting people from house to house, distributing literature published by the Watch Tower Society in 700 languages. The objective is to start a regular "Bible study" with any person who is not already a member, with the intention that the student be baptized as a member of the group; Witnesses are advised to consider discontinuing Bible studies with students who show no interest in becoming members. Witnesses are taught they are under a biblical command to engage in public preaching. They are instructed to devote as much time as possible to their ministry and are required to submit an individual monthly "Field Service Report". Baptized members who fail to report a month of preaching are termed "irregular" and may be counseled by elders; those who do not submit reports for six consecutive months are termed "inactive".
Divorce is discouraged, and remarriage is forbidden unless a divorce is obtained on the grounds of adultery, which they refer to as "a scriptural divorce". If a divorce is obtained for any other reason, remarriage is considered adulterous unless the prior spouse has died or is since considered to have committed sexual immorality. Extreme physical abuse, willful non-support of one's family, and what the religion terms "absolute endangerment of spirituality" are considered grounds for legal separation.
Formal discipline is administered by congregation elders. When a baptized member is accused of committing a serious sin—usually cases of sexual misconduct or charges of apostasy for disputing Jehovah's Witness doctrines—a judicial committee is formed to determine guilt, provide help and possibly administer discipline. Disfellowshipping, a form of shunning, is the strongest form of discipline, administered to an offender deemed unrepentant. Contact with disfellowshipped individuals is limited to direct family members living in the same home, and with congregation elders who may invite disfellowshipped persons to apply for reinstatement; formal business dealings may continue if contractually or financially obliged. Witnesses are taught that avoiding social and spiritual interaction with disfellowshipped individuals keeps the congregation free from immoral influence and that "losing precious fellowship with loved ones may help [the shunned individual] to come 'to his senses,' see the seriousness of his wrong, and take steps to return to Jehovah." The practice of shunning may also serve to deter other members from dissident behavior. Members who disassociate (formally resign) are described in Watch Tower Society literature as wicked and are also shunned. Expelled individuals may eventually be reinstated to the congregation if deemed repentant by elders in the congregation in which the disfellowshipping was enforced. Reproof is a lesser form of discipline given formally by a judicial committee to a baptized Witness who is considered repentant of serious sin; the reproved person temporarily loses conspicuous privileges of service, but suffers no restriction of social or spiritual fellowship. Marking, a curtailing of social but not spiritual fellowship, is practiced if a baptized member persists in a course of action regarded as a violation of Bible principles but not a serious sin.[note 4]
Jehovah's Witnesses believe that the Bible condemns the mixing of religions, on the basis that there can only be one truth from God, and therefore reject interfaith and ecumenical movements. They believe that only their religion represents true Christianity, and that other religions fail to meet all the requirements set by God and will soon be destroyed. Jehovah's Witnesses are taught that it is vital to remain "separate from the world." The Witnesses' literature defines the "world" as "the mass of mankind apart from Jehovah's approved servants" and teach that it is morally contaminated and ruled by Satan. Witnesses are taught that association with "worldly" people presents a "danger" to their faith, and are instructed to minimize social contact with non-members to better maintain their own standards of morality.
Jehovah's Witnesses believe their highest allegiance belongs to God's kingdom, which is viewed as an actual government in heaven, with Christ as king. They remain politically neutral, do not seek public office, and are discouraged from voting, though individual members may participate in uncontroversial community improvement issues. Although they do not take part in politics, they respect the authority of the governments under which they live. They do not celebrate religious holidays such as Christmas and Easter, nor do they observe birthdays, nationalistic holidays, or other celebrations they consider to honor people other than Jesus. They feel that these and many other customs have pagan origins or reflect a nationalistic or political spirit. Their position is that these traditional holidays reflect Satan's control over the world. Witnesses are told that spontaneous giving at other times can help their children to not feel deprived of birthdays or other celebrations.
They do not work in industries associated with the military, do not serve in the armed services, and refuse national military service, which in some countries may result in their arrest and imprisonment. They do not salute or pledge allegiance to flags or sing national anthems or patriotic songs. Jehovah's Witnesses see themselves as a worldwide brotherhood that transcends national boundaries and ethnic loyalties. Sociologist Ronald Lawson has suggested the religion's intellectual and organizational isolation, coupled with the intense indoctrination of adherents, rigid internal discipline and considerable persecution, has contributed to the consistency of its sense of urgency in its apocalyptic message.
Jehovah's Witnesses refuse blood transfusions, which they consider a violation of God's law based on their interpretation of Acts 15:28, 29 and other scriptures. Since 1961 the willing acceptance of a blood transfusion by an unrepentant member has been grounds for expulsion from the religion. Members are directed to refuse blood transfusions, even in "a life-or-death situation". Jehovah's Witnesses accept non-blood alternatives and other medical procedures in lieu of blood transfusions, and their literature provides information about non-blood medical procedures.
Though Jehovah's Witnesses do not accept blood transfusions of whole blood, they may accept some blood plasma fractions at their own discretion. The Watch Tower Society provides pre-formatted durable power of attorney documents prohibiting major blood components, in which members can specify which allowable fractions and treatments they will personally accept. Jehovah's Witnesses have established Hospital Liaison Committees as a cooperative arrangement between individual Jehovah's Witnesses and medical professionals and hospitals.
As of August 2015, Jehovah's Witnesses report an average of 8.2 million publishers—the term they use for members actively involved in preaching—in 118,016 congregations. In 2015, these reports indicated over 1.93 billion hours spent in preaching and "Bible study" activity. Since the mid-1990s, the number of peak publishers has increased from 4.5 million to 8.2 million. In the same year, they conducted "Bible studies" with over 9.7 million individuals, including those conducted by Witness parents with their children. Jehovah's Witnesses estimate their current worldwide growth rate to be 1.5% per year.
The official published membership statistics, such as those mentioned above, include only those who submit reports for their personal ministry; official statistics do not include inactive and disfellowshipped individuals or others who might attend their meetings. As a result, only about half of those who self-identified as Jehovah's Witnesses in independent demographic studies are considered active by the faith itself. The 2008 US Pew Forum on Religion & Public Life survey found a low retention rate among members of the religion: about 37% of people raised in the religion continued to identify themselves as Jehovah's Witnesses.
Sociologist James A. Beckford, in his 1975 study of Jehovah's Witnesses, classified the religion's organizational structure as Totalizing, characterized by an assertive leadership, specific and narrow objectives, control over competing demands on members' time and energy, and control over the quality of new members. Other characteristics of the classification include likelihood of friction with secular authorities, reluctance to co-operate with other religious organizations, a high rate of membership turnover, a low rate of doctrinal change, and strict uniformity of beliefs among members. Beckford identified the religion's chief characteristics as historicism (identifying historical events as relating to the outworking of God's purpose), absolutism (conviction that Jehovah's Witness leaders dispense absolute truth), activism (capacity to motivate members to perform missionary tasks), rationalism (conviction that Witness doctrines have a rational basis devoid of mystery), authoritarianism (rigid presentation of regulations without the opportunity for criticism) and world indifference (rejection of certain secular requirements and medical treatments).
A sociological comparative study by the Pew Research Center found that Jehovah's Witnesses in the United States ranked highest in statistics for getting no further than high school graduation, belief in God, importance of religion in one's life, frequency of religious attendance, frequency of prayers, frequency of Bible reading outside of religious services, belief their prayers are answered, belief that their religion can only be interpreted one way, belief that theirs is the only one true faith leading to eternal life, opposition to abortion, and opposition to homosexuality. In the study, Jehovah's Witnesses ranked lowest in statistics for having earned a graduate degree and interest in politics.
Political and religious animosity against Jehovah's Witnesses has at times led to mob action and government oppression in various countries. Their doctrine of political neutrality and their refusal to serve in the military has led to imprisonment of members who refused conscription during World War II and at other times where national service has been compulsory. In 1933, there were approximately 20,000 Jehovah's Witnesses in Germany, of whom about 10,000 were later imprisoned. Of those, 2000 were sent to Nazi concentration camps, where they were identified by purple triangles; as many as 1200 died, including 250 who were executed. In Canada, Jehovah's Witnesses were interned in camps along with political dissidents and people of Chinese and Japanese descent. In the former Soviet Union, about 9,300 Jehovah's Witnesses were deported to Siberia as part of Operation North in April 1951. Their religious activities are currently banned or restricted in some countries, including China, Vietnam and some Islamic states.
Authors including William Whalen, Shawn Francis Peters and former Witnesses Barbara Grizzuti Harrison, Alan Rogerson and William Schnell have claimed the arrests and mob violence in the United States in the 1930s and 1940s were the consequence of what appeared to be a deliberate course of provocation of authorities and other religions by Jehovah’s Witnesses. Whalen, Harrison and Schnell have suggested Rutherford invited and cultivated opposition for publicity purposes in a bid to attract dispossessed members of society, and to convince members that persecution from the outside world was evidence of the truth of their struggle to serve God. Watch Tower Society literature of the period directed that Witnesses should "never seek a controversy" nor resist arrest, but also advised members not to co-operate with police officers or courts that ordered them to stop preaching, and to prefer jail rather than pay fines.
In the United States, their persistent legal challenges prompted a series of state and federal court rulings that reinforced judicial protections for civil liberties. Among the rights strengthened by Witness court victories in the United States are the protection of religious conduct from federal and state interference, the right to abstain from patriotic rituals and military service, the right of patients to refuse medical treatment, and the right to engage in public discourse. Similar cases in their favor have been heard in Canada.
Doctrines of Jehovah's Witnesses are established by the Governing Body. The religion does not tolerate dissent over doctrines and practices; members who openly disagree with the religion's teachings are expelled and shunned. Witness publications strongly discourage followers from questioning doctrine and counsel received from the Governing Body, reasoning that it is to be trusted as part of "God's organization". It also warns members to "avoid independent thinking", claiming such thinking "was introduced by Satan the Devil" and would "cause division". Those who openly disagree with official teachings are condemned as "apostates" who are "mentally diseased".
Former members Heather and Gary Botting compare the cultural paradigms of the religion to George Orwell's Nineteen Eighty-four, and Alan Rogerson describes the religion's leadership as totalitarian. Other critics charge that by disparaging individual decision-making, the religion's leaders cultivate a system of unquestioning obedience in which Witnesses abrogate all responsibility and rights over their personal lives. Critics also accuse the religion's leaders of exercising "intellectual dominance" over Witnesses, controlling information and creating "mental isolation", which former Governing Body member Raymond Franz argued were all elements of mind control.
Sociologist Rodney Stark states that Jehovah's Witness leaders are "not always very democratic" and that members "are expected to conform to rather strict standards," but adds that "enforcement tends to be very informal, sustained by the close bonds of friendship within the group", and that Jehovah's Witnesses see themselves as "part of the power structure rather than subject to it." Sociologist Andrew Holden states that most members who join millenarian movements such as Jehovah's Witnesses have made an informed choice. However, he also states that defectors "are seldom allowed a dignified exit", and describes the administration as autocratic.
On the other hand, in his study on nine of "the Bibles most widely in use in the English-speaking world", Bible scholar Jason BeDuhn, Professor of Religious Studies at the Northern Arizona University, wrote: “The NW [New World Translation] emerges as the most accurate of the translations compared.” Although the general public and many Bible scholars assume that the differences in the New World Translation are the result of religious bias on the part of its translators, BeDuhn stated: “Most of the differences are due to the greater accuracy of the NW as a literal, conservative translation of the original expressions of the New Testament writers.” He added however that the insertion of the name Jehovah in the New Testament "violate[s] accuracy in favor of denominationally preferred expressions for God".
Watch Tower Society publications have claimed that God has used Jehovah's Witnesses (and formerly, the International Bible Students) to declare his will and has provided advance knowledge about Armageddon and the establishment of God's kingdom. Some publications also claimed that God has used Jehovah's Witnesses and the International Bible Students as a modern-day prophet.[note 5] Jehovah's Witnesses' publications have made various predictions about world events they believe were prophesied in the Bible. Failed predictions have led to the alteration or abandonment of some doctrines. Some failed predictions had been presented as "beyond doubt" or "approved by God".
The Watch Tower Society rejects accusations that it is a false prophet, stating that its teachings are not inspired or infallible, and that it has not claimed its predictions were "the words of Jehovah." George D. Chryssides has suggested that with the exception of statements about 1914, 1925 and 1975, the changing views and dates of the Jehovah's Witnesses are largely attributable to changed understandings of biblical chronology than to failed predictions. Chryssides further states, "it is therefore simplistic and naïve to view the Witnesses as a group that continues to set a single end-date that fails and then devise a new one, as many counter-cultists do." However, sociologist Andrew Holden states that since the foundation of the movement around 140 years ago, "Witnesses have maintained that we are living on the precipice of the end of time."
Jehovah's Witnesses have been accused of having policies and culture that help to conceal cases of sexual abuse within the organization. The religion has been criticized for its "two witness rule" for church discipline, based on its application of scriptures at Deuteronomy 19:15 and Matthew 18:15-17, which requires sexual abuse to be substantiated by secondary evidence if the accused person denies any wrongdoing. In cases where corroboration is lacking, the Watch Tower Society's instruction is that "the elders will leave the matter in Jehovah's hands". A former member of the church’s headquarters staff, Barbara Anderson, says the policy effectively requires that there be another witness to an act of molestation, "which is an impossibility". Anderson says the policies "protect pedophiles rather than protect the children." Jehovah's Witnesses maintain that they have a strong policy to protect children, adding that the best way to protect children is by educating parents; they also state that they do not sponsor activities that separate children from parents.
The religion's failure to report abuse allegations to authorities has also been criticized. The Watch Tower Society's policy is that elders inform authorities when required by law to do so, but otherwise leave that action up to the victim and his or her family. The Australian Royal Commission into Institutional Responses to Child Sexual Abuse found that of 1006 alleged perpetrators of child sexual abuse identified by the Jehovah's Witnesses within their organization since 1950, "not one was reported by the church to secular authorities." William Bowen, a former Jehovah's Witness elder who established the Silentlambs organization to assist sex abuse victims within the religion, has claimed Witness leaders discourage followers from reporting incidents of sexual misconduct to authorities, and other critics claim the organization is reluctant to alert authorities in order to protect its "crime-free" reputation. In court cases in the United Kingdom and the United States the Watch Tower Society has been found to have been negligent in its failure to protect children from known sex offenders within the congregation and the Society has settled other child abuse lawsuits out of court, reportedly paying as much as $780,000 to one plaintiff without admitting wrongdoing.
Relations between Grand Lodges are determined by the concept of Recognition. Each Grand Lodge maintains a list of other Grand Lodges that it recognises. When two Grand Lodges recognise and are in Masonic communication with each other, they are said to be in amity, and the brethren of each may visit each other's Lodges and interact Masonically. When two Grand Lodges are not in amity, inter-visitation is not allowed. There are many reasons why one Grand Lodge will withhold or withdraw recognition from another, but the two most common are Exclusive Jurisdiction and Regularity.
Since the middle of the 19th century, Masonic historians have sought the origins of the movement in a series of similar documents known as the Old Charges, dating from the Regius Poem in about 1425 to the beginning of the 18th century. Alluding to the membership of a lodge of operative masons, they relate a mythologised history of the craft, the duties of its grades, and the manner in which oaths of fidelity are to be taken on joining. The fifteenth century also sees the first evidence of ceremonial regalia.
A dispute during the Lausanne Congress of Supreme Councils of 1875 prompted the Grand Orient de France to commission a report by a Protestant pastor which concluded that, as Freemasonry was not a religion, it should not require a religious belief. The new constitutions read, "Its principles are absolute liberty of conscience and human solidarity", the existence of God and the immortality of the soul being struck out. It is possible that the immediate objections of the United Grand Lodge of England were at least partly motivated by the political tension between France and Britain at the time. The result was the withdrawal of recognition of the Grand Orient of France by the United Grand Lodge of England, a situation that continues today.
At the dawn of the Grand Lodge era, during the 1720s, James Anderson composed the first printed constitutions for Freemasons, the basis for most subsequent constitutions, which specifically excluded women from Freemasonry. As Freemasonry spread, continental masons began to include their ladies in Lodges of Adoption, which worked three degrees with the same names as the men's but different content. The French officially abandoned the experiment in the early 19th century. Later organisations with a similar aim emerged in the United States, but distinguished the names of the degrees from those of male masonry.
In contrast to Catholic allegations of rationalism and naturalism, Protestant objections are more likely to be based on allegations of mysticism, occultism, and even Satanism. Masonic scholar Albert Pike is often quoted (in some cases misquoted) by Protestant anti-Masons as an authority for the position of Masonry on these issues. However, Pike, although undoubtedly learned, was not a spokesman for Freemasonry and was also controversial among Freemasons in general. His writings represented his personal opinion only, and furthermore an opinion grounded in the attitudes and understandings of late 19th century Southern Freemasonry of the USA. Notably, his book carries in the preface a form of disclaimer from his own Grand Lodge. No one voice has ever spoken for the whole of Freemasonry.
In 1799, English Freemasonry almost came to a halt due to Parliamentary proclamation. In the wake of the French Revolution, the Unlawful Societies Act 1799 banned any meetings of groups that required their members to take an oath or obligation. The Grand Masters of both the Moderns and the Antients Grand Lodges called on Prime Minister William Pitt (who was not a Freemason) and explained to him that Freemasonry was a supporter of the law and lawfully constituted authority and was much involved in charitable work. As a result, Freemasonry was specifically exempted from the terms of the Act, provided that each private lodge's Secretary placed with the local "Clerk of the Peace" a list of the members of his lodge once a year. This continued until 1967 when the obligation of the provision was rescinded by Parliament.
In some countries anti-Masonry is often related to antisemitism and anti-Zionism. For example, In 1980, the Iraqi legal and penal code was changed by Saddam Hussein's ruling Ba'ath Party, making it a felony to "promote or acclaim Zionist principles, including Freemasonry, or who associate [themselves] with Zionist organisations". Professor Andrew Prescott of the University of Sheffield writes: "Since at least the time of the Protocols of the Elders of Zion, antisemitism has gone hand in hand with anti-masonry, so it is not surprising that allegations that 11 September was a Zionist plot have been accompanied by suggestions that the attacks were inspired by a masonic world order".
The bulk of Masonic ritual consists of degree ceremonies. Candidates for Freemasonry are progressively initiated into Freemasonry, first in the degree of Entered Apprentice. Some time later, in a separate ceremony, they will be passed to the degree of Fellowcraft, and finally they will be raised to the degree of Master Mason. In all of these ceremonies, the candidate is entrusted with passwords, signs and grips peculiar to his new rank. Another ceremony is the annual installation of the Master and officers of the Lodge. In some jurisdictions Installed Master is valued as a separate rank, with its own secrets to distinguish its members. In other jurisdictions, the grade is not recognised, and no inner ceremony conveys new secrets during the installation of a new Master of the Lodge.
English Freemasonry spread to France in the 1720s, first as lodges of expatriates and exiled Jacobites, and then as distinctively French lodges which still follow the ritual of the Moderns. From France and England, Freemasonry spread to most of Continental Europe during the course of the 18th century. The Grande Loge de France formed under the Grand Mastership of the Duke of Clermont, who exercised only nominal authority. His successor, the Duke of Orléans, reconstituted the central body as the Grand Orient de France in 1773. Briefly eclipsed during the French Revolution, French Freemasonry continued to grow in the next century.
The majority of Freemasonry considers the Liberal (Continental) strand to be Irregular, and thus withhold recognition. For the Continental lodges, however, having a different approach to Freemasonry was not a reason for severing masonic ties. In 1961, an umbrella organisation, Centre de Liaison et d'Information des Puissances maçonniques Signataires de l'Appel de Strasbourg (CLIPSAS) was set up, which today provides a forum for most of these Grand Lodges and Grand Orients worldwide. Included in the list of over 70 Grand Lodges and Grand Orients are representatives of all three of the above categories, including mixed and women's organisations. The United Grand Lodge of England does not communicate with any of these jurisdictions, and expects its allies to follow suit. This creates the distinction between Anglo-American and Continental Freemasonry.
The denomination with the longest history of objection to Freemasonry is the Roman Catholic Church. The objections raised by the Roman Catholic Church are based on the allegation that Masonry teaches a naturalistic deistic religion which is in conflict with Church doctrine. A number of Papal pronouncements have been issued against Freemasonry. The first was Pope Clement XII's In eminenti apostolatus, 28 April 1738; the most recent was Pope Leo XIII's Ab apostolici, 15 October 1890. The 1917 Code of Canon Law explicitly declared that joining Freemasonry entailed automatic excommunication, and banned books favouring Freemasonry.
In 1933, the Orthodox Church of Greece officially declared that being a Freemason constitutes an act of apostasy and thus, until he repents, the person involved with Freemasonry cannot partake of the Eucharist. This has been generally affirmed throughout the whole Eastern Orthodox Church. The Orthodox critique of Freemasonry agrees with both the Roman Catholic and Protestant versions: "Freemasonry cannot be at all compatible with Christianity as far as it is a secret organisation, acting and teaching in mystery and secret and deifying rationalism."
In addition, most Grand Lodges require the candidate to declare a belief in a Supreme Being. In a few cases, the candidate may be required to be of a specific religion. The form of Freemasonry most common in Scandinavia (known as the Swedish Rite), for example, accepts only Christians. At the other end of the spectrum, "Liberal" or Continental Freemasonry, exemplified by the Grand Orient de France, does not require a declaration of belief in any deity, and accepts atheists (a cause of discord with the rest of Freemasonry).
Exclusive Jurisdiction is a concept whereby only one Grand Lodge will be recognised in any geographical area. If two Grand Lodges claim jurisdiction over the same area, the other Grand Lodges will have to choose between them, and they may not all decide to recognise the same one. (In 1849, for example, the Grand Lodge of New York split into two rival factions, each claiming to be the legitimate Grand Lodge. Other Grand Lodges had to choose between them until the schism was healed.) Exclusive Jurisdiction can be waived when the two over-lapping Grand Lodges are themselves in Amity and agree to share jurisdiction (for example, since the Grand Lodge of Connecticut is in Amity with the Prince Hall Grand Lodge of Connecticut, the principle of Exclusive Jurisdiction does not apply, and other Grand Lodges may recognise both).
There is no clear mechanism by which these local trade organisations became today's Masonic Lodges, but the earliest rituals and passwords known, from operative lodges around the turn of the 17th–18th centuries, show continuity with the rituals developed in the later 18th century by accepted or speculative Masons, as those members who did not practice the physical craft came to be known. The minutes of the Lodge of Edinburgh (Mary's Chapel) No. 1 in Scotland show a continuity from an operative lodge in 1598 to a modern speculative Lodge. It is reputed to be the oldest Masonic Lodge in the world.
Prince Hall Freemasonry exists because of the refusal of early American lodges to admit African-Americans. In 1775, an African-American named Prince Hall, along with fourteen other African-Americans, was initiated into a British military lodge with a warrant from the Grand Lodge of Ireland, having failed to obtain admission from the other lodges in Boston. When the military Lodge left North America, those fifteen men were given the authority to meet as a Lodge, but not to initiate Masons. In 1784, these individuals obtained a Warrant from the Premier Grand Lodge of England (GLE) and formed African Lodge, Number 459. When the UGLE was formed in 1813, all U.S.-based Lodges were stricken from their rolls – due largely to the War of 1812. Thus, separated from both UGLE and any concordantly recognised U.S. Grand Lodge, African Lodge re-titled itself as the African Lodge, Number 1 – and became a de facto "Grand Lodge" (this Lodge is not to be confused with the various Grand Lodges on the Continent of Africa). As with the rest of U.S. Freemasonry, Prince Hall Freemasonry soon grew and organised on a Grand Lodge system for each state.
Maria Deraismes was initiated into Freemasonry in 1882, then resigned to allow her lodge to rejoin their Grand Lodge. Having failed to achieve acceptance from any masonic governing body, she and Georges Martin started a mixed masonic lodge that actually worked masonic ritual. Annie Besant spread the phenomenon to the English speaking world. Disagreements over ritual led to the formation of exclusively female bodies of Freemasons in England, which spread to other countries. Meanwhile, the French had re-invented Adoption as an all-female lodge in 1901, only to cast it aside again in 1935. The lodges, however, continued to meet, which gave rise, in 1959, to a body of women practising continental Freemasonry.
Many Islamic anti-Masonic arguments are closely tied to both antisemitism and Anti-Zionism, though other criticisms are made such as linking Freemasonry to al-Masih ad-Dajjal (the false Messiah). Some Muslim anti-Masons argue that Freemasonry promotes the interests of the Jews around the world and that one of its aims is to destroy the Al-Aqsa Mosque in order to rebuild the Temple of Solomon in Jerusalem. In article 28 of its Covenant, Hamas states that Freemasonry, Rotary, and other similar groups "work in the interest of Zionism and according to its instructions ..."
The preserved records of the Reichssicherheitshauptamt (the Reich Security Main Office) show the persecution of Freemasons during the Holocaust. RSHA Amt VII (Written Records) was overseen by Professor Franz Six and was responsible for "ideological" tasks, by which was meant the creation of antisemitic and anti-Masonic propaganda. While the number is not accurately known, it is estimated that between 80,000 and 200,000 Freemasons were killed under the Nazi regime. Masonic concentration camp inmates were graded as political prisoners and wore an inverted red triangle.
Freemasonry consists of fraternal organisations that trace their origins to the local fraternities of stonemasons, which from the end of the fourteenth century regulated the qualifications of stonemasons and their interaction with authorities and clients. The degrees of freemasonry retain the three grades of medieval craft guilds, those of Apprentice, Journeyman or fellow (now called Fellowcraft), and Master Mason. These are the degrees offered by Craft (or Blue Lodge) Freemasonry. Members of these organisations are known as Freemasons or Masons. There are additional degrees, which vary with locality and jurisdiction, and are usually administered by different bodies than the craft degrees.
Candidates for Freemasonry will have met most active members of the Lodge they are joining before they are initiated. The process varies between jurisdictions, but the candidate will typically have been introduced by a friend at a Lodge social function, or at some form of open evening in the Lodge. In modern times, interested people often track down a local Lodge through the Internet. The onus is on candidates to ask to join; while candidates may be encouraged to ask, they are never invited. Once the initial inquiry is made, an interview usually follows to determine the candidate's suitability. If the candidate decides to proceed from here, the Lodge ballots on the application before he (or she, depending on the Masonic Jurisdiction) can be accepted.
Freemasonry, as it exists in various forms all over the world, has a membership estimated by the United Grand Lodge of England at around six million worldwide. The fraternity is administratively organised into independent Grand Lodges (or sometimes Grand Orients), each of which governs its own Masonic jurisdiction, which consists of subordinate (or constituent) Lodges. The largest single jurisdiction, in terms of membership, is the United Grand Lodge of England (with a membership estimated at around a quarter million). The Grand Lodge of Scotland and Grand Lodge of Ireland (taken together) have approximately 150,000 members. In the United States total membership is just under two million.
The idea of Masonic brotherhood probably descends from a 16th-century legal definition of a brother as one who has taken an oath of mutual support to another. Accordingly, Masons swear at each degree to keep the contents of that degree secret, and to support and protect their brethren unless they have broken the law. In most Lodges the oath or obligation is taken on a Volume of Sacred Law, whichever book of divine revelation is appropriate to the religious beliefs of the individual brother (usually the Bible in the Anglo-American tradition). In Progressive continental Freemasonry, books other than scripture are permissible, a cause of rupture between Grand Lodges.
The earliest known American lodges were in Pennsylvania. The Collector for the port of Pennsylvania, John Moore, wrote of attending lodges there in 1715, two years before the formation of the first Grand Lodge in London. The Premier Grand Lodge of England appointed a Provincial Grand Master for North America in 1731, based in Pennsylvania. Other lodges in the colony obtained authorisations from the later Antient Grand Lodge of England, the Grand Lodge of Scotland, and the Grand Lodge of Ireland, which was particularly well represented in the travelling lodges of the British Army. Many lodges came into existence with no warrant from any Grand Lodge, applying and paying for their authorisation only after they were confident of their own survival.
Masonic lodges existed in Iraq as early as 1917, when the first lodge under the United Grand Lodge of England (UGLE) was opened. Nine lodges under UGLE existed by the 1950s, and a Scottish lodge was formed in 1923. However, the position changed following the revolution, and all lodges were forced to close in 1965. This position was later reinforced under Saddam Hussein; the death penalty was "prescribed" for those who "promote or acclaim Zionist principles, including freemasonry, or who associate [themselves] with Zionist organisations."
The ritual form on which the Grand Orient of France was based was abolished in England in the events leading to the formation of the United Grand Lodge of England in 1813. However the two jurisdictions continued in amity (mutual recognition) until events of the 1860s and 1870s drove a seemingly permanent wedge between them. In 1868 the Supreme Council of the Ancient and Accepted Scottish Rite of the State of Louisiana appeared in the jurisdiction of the Grand Lodge of Louisiana, recognised by the Grand Orient de France, but regarded by the older body as an invasion of their jurisdiction. The new Scottish rite body admitted blacks, and the resolution of the Grand Orient the following year that neither colour, race, nor religion could disqualify a man from Masonry prompted the Grand Lodge to withdraw recognition, and it persuaded other American Grand Lodges to do the same.
In 1983, the Church issued a new code of canon law. Unlike its predecessor, the 1983 Code of Canon Law did not explicitly name Masonic orders among the secret societies it condemns. It states: "A person who joins an association which plots against the Church is to be punished with a just penalty; one who promotes or takes office in such an association is to be punished with an interdict." This named omission of Masonic orders caused both Catholics and Freemasons to believe that the ban on Catholics becoming Freemasons may have been lifted, especially after the perceived liberalisation of Vatican II. However, the matter was clarified when Cardinal Joseph Ratzinger (later Pope Benedict XVI), as the Prefect of the Congregation for the Doctrine of the Faith, issued a Declaration on Masonic Associations, which states: "... the Church's negative judgment in regard to Masonic association remains unchanged since their principles have always been considered irreconcilable with the doctrine of the Church and therefore membership in them remains forbidden. The faithful who enroll in Masonic associations are in a state of grave sin and may not receive Holy Communion." For its part, Freemasonry has never objected to Catholics joining their fraternity. Those Grand Lodges in amity with UGLE deny the Church's claims. The UGLE now states that "Freemasonry does not seek to replace a Mason's religion or provide a substitute for it."
Even in modern democracies, Freemasonry is sometimes viewed with distrust. In the UK, Masons working in the justice system, such as judges and police officers, were from 1999 to 2009 required to disclose their membership. While a parliamentary inquiry found that there has been no evidence of wrongdoing, it was felt that any potential loyalties Masons might have, based on their vows to support fellow Masons, should be transparent to the public. The policy of requiring a declaration of masonic membership of applicants for judicial office (judges and magistrates) was ended in 2009 by Justice Secretary Jack Straw (who had initiated the requirement in the 1990s). Straw stated that the rule was considered disproportionate, since no impropriety or malpractice had been shown as a result of judges being Freemasons.
The Masonic Lodge is the basic organisational unit of Freemasonry. The Lodge meets regularly to conduct the usual formal business of any small organisation (pay bills, organise social and charitable events, elect new members, etc.). In addition to business, the meeting may perform a ceremony to confer a Masonic degree or receive a lecture, which is usually on some aspect of Masonic history or ritual. At the conclusion of the meeting, the Lodge might adjourn for a formal dinner, or festive board, sometimes involving toasting and song.
During the ceremony of initiation, the candidate is expected to swear (usually on a volume of sacred text appropriate to his personal religious faith) to fulfil certain obligations as a Mason. In the course of three degrees, new masons will promise to keep the secrets of their degree from lower degrees and outsiders, and to support a fellow Mason in distress (as far as practicality and the law permit). There is instruction as to the duties of a Freemason, but on the whole, Freemasons are left to explore the craft in the manner they find most satisfying. Some will further explore the ritual and symbolism of the craft, others will focus their involvement on the social side of the Lodge, while still others will concentrate on the charitable functions of the lodge.
Regularity is a concept based on adherence to Masonic Landmarks, the basic membership requirements, tenets and rituals of the craft. Each Grand Lodge sets its own definition of what these landmarks are, and thus what is Regular and what is Irregular (and the definitions do not necessarily agree between Grand Lodges). Essentially, every Grand Lodge will hold that its landmarks (its requirements, tenets and rituals) are Regular, and judge other Grand Lodges based on those. If the differences are significant, one Grand Lodge may declare the other "Irregular" and withdraw or withhold recognition.
All Freemasons begin their journey in the "craft" by being progressively initiated, passed and raised into the three degrees of Craft, or Blue Lodge Masonry. During these three rituals, the candidate is progressively taught the meanings of the Lodge symbols, and entrusted with grips, signs and words to signify to other Masons that he has been so initiated. The initiations are part allegory and part lecture, and revolve around the construction of the Temple of Solomon, and the artistry and death of his chief architect, Hiram Abiff. The degrees are those of Entered apprentice, Fellowcraft and Master Mason. While many different versions of these rituals exist, with at least two different lodge layouts and versions of the Hiram myth, each version is recognisable to any Freemason from any jurisdiction.
The first Grand Lodge, the Grand Lodge of London and Westminster (later called the Grand Lodge of England (GLE)), was founded on 24 June 1717, when four existing London Lodges met for a joint dinner. Many English Lodges joined the new regulatory body, which itself entered a period of self-publicity and expansion. However, many Lodges could not endorse changes which some Lodges of the GLE made to the ritual (they came to be known as the Moderns), and a few of these formed a rival Grand Lodge on 17 July 1751, which they called the "Antient Grand Lodge of England." These two Grand Lodges vied for supremacy until the Moderns promised to return to the ancient ritual. They united on 27 December 1813 to form the United Grand Lodge of England (UGLE).
Widespread segregation in 19th- and early 20th-century North America made it difficult for African-Americans to join Lodges outside of Prince Hall jurisdictions – and impossible for inter-jurisdiction recognition between the parallel U.S. Masonic authorities. By the 1980s, such discrimination was a thing of the past, and today most U.S. Grand Lodges recognise their Prince Hall counterparts, and the authorities of both traditions are working towards full recognition. The United Grand Lodge of England has no problem with recognising Prince Hall Grand Lodges. While celebrating their heritage as lodges of black Americans, Prince Hall is open to all men regardless of race or religion.
In general, Continental Freemasonry is sympathetic to Freemasonry amongst women, dating from the 1890s when French lodges assisted the emergent co-masonic movement by promoting enough of their members to the 33rd degree of the Ancient and Accepted Scottish Rite to allow them, in 1899, to form their own grand council, recognised by the other Continental Grand Councils of that Rite. The United Grand Lodge of England issued a statement in 1999 recognising the two women's grand lodges there to be regular in all but the participants. While they were not, therefore, recognised as regular, they were part of Freemasonry "in general". The attitude of most regular Anglo-American grand lodges remains that women Freemasons are not legitimate Masons.
Since the founding of Freemasonry, many Bishops of the Church of England have been Freemasons, such as Archbishop Geoffrey Fisher. In the past, few members of the Church of England would have seen any incongruity in concurrently adhering to Anglican Christianity and practicing Freemasonry. In recent decades, however, reservations about Freemasonry have increased within Anglicanism, perhaps due to the increasing prominence of the evangelical wing of the church. The former Archbishop of Canterbury, Dr Rowan Williams, appeared to harbour some reservations about Masonic ritual, whilst being anxious to avoid causing offence to Freemasons inside and outside the Church of England. In 2003 he felt it necessary to apologise to British Freemasons after he said that their beliefs were incompatible with Christianity and that he had barred the appointment of Freemasons to senior posts in his diocese when he was Bishop of Monmouth.
In Italy, Freemasonry has become linked to a scandal concerning the Propaganda Due lodge (a.k.a. P2). This lodge was chartered by the Grande Oriente d'Italia in 1877, as a lodge for visiting Masons unable to attend their own lodges. Under Licio Gelli's leadership, in the late 1970s, P2 became involved in the financial scandals that nearly bankrupted the Vatican Bank. However, by this time the lodge was operating independently and irregularly, as the Grand Orient had revoked its charter and expelled Gelli in 1976.
A pub /pʌb/, or public house is, despite its name, a private house, but is called a public house because it is licensed to sell alcohol to the general public. It is a drinking establishment in Britain, Ireland, New Zealand, Australia, Canada, Denmark and New England. In many places, especially in villages, a pub can be the focal point of the community. The writings of Samuel Pepys describe the pub as the heart of England.
The history of pubs can be traced back to Roman taverns, through the Anglo-Saxon alehouse to the development of the modern tied house system in the 19th century.
Historically, pubs have been socially and culturally distinct from cafés, bars and German beer halls. Most pubs offer a range of beers, wines, spirits, and soft drinks and snacks. Traditionally the windows of town pubs were of smoked or frosted glass to obscure the clientele from the street but from the 1990s onwards, there has been a move towards clear glass, in keeping with brighter interiors.
The owner, tenant or manager (licensee) of a pub is properly known as the "pub landlord". The term publican (in historical Roman usage a public contractor or tax farmer) has come into use since Victorian times to designate the pub landlord. Known as "locals" to regulars, pubs are typically chosen for their proximity to home or work, the availability of a particular beer, as a place to smoke (or avoid it), hosting a darts team, having a pool or snooker table, or appealing to friends.
Until the 1970s most of the larger pubs also featured an off-sales counter or attached shop for the sales of beers, wines and spirits for home consumption. In the 1970s the newly built supermarkets and high street chain stores or off-licences undercut the pub prices to such a degree that within ten years all but a handful of pubs had closed their off-sale counters, which had often been referred to colloquially as the jug and bottle.
The inhabitants of the British Isles have been drinking ale since the Bronze Age, but it was with the arrival of the Roman Empire in its shores in the 1st Century, and the construction of the Roman road networks that the first inns, called tabernae, in which travellers could obtain refreshment began to appear. After the departure of Roman authority in the 5th Century and the fall of the Romano-British kingdoms, the Anglo-Saxons established alehouses that grew out of domestic dwellings, the Anglo-Saxon alewife would put a green bush up on a pole to let people know her brew was ready. These alehouses quickly evolved into meeting houses for the folk to socially congregate, gossip and arrange mutual help within their communities. Herein lies the origin of the modern public house, or "Pub" as it is colloquially called in England. They rapidly spread across the Kingdom, becoming so commonplace that in 965 King Edgar decreed that there should be no more than one alehouse per village.
A traveller in the early Middle Ages could obtain overnight accommodation in monasteries, but later a demand for hostelries grew with the popularity of pilgrimages and travel. The Hostellers of London were granted guild status in 1446 and in 1514 the guild became the Worshipful Company of Innholders.
Inns are buildings where travellers can seek lodging and, usually, food and drink. They are typically located in the country or along a highway. In Europe, they possibly first sprang up when the Romans built a system of roads two millennia ago.[citation needed] Some inns in Europe are several centuries old. In addition to providing for the needs of travellers, inns traditionally acted as community gathering places.
In Europe, it is the provision of accommodation, if anything, that now distinguishes inns from taverns, alehouses and pubs. The latter tend to provide alcohol (and, in the UK, soft drinks and often food), but less commonly accommodation. Inns tend to be older and grander establishments: historically they provided not only food and lodging, but also stabling and fodder for the traveller's horse(s) and on some roads fresh horses for the mail coach. Famous London inns include The George, Southwark and The Tabard. There is however no longer a formal distinction between an inn and other kinds of establishment. Many pubs use "Inn" in their name, either because they are long established former coaching inns, or to summon up a particular kind of image, or in many cases simply as a pun on the word "in", as in "The Welcome Inn", the name of many pubs in Scotland.
The original services of an inn are now also available at other establishments, such as hotels, lodges, and motels, which focus more on lodging customers than on other services, although they usually provide meals; pubs, which are primarily alcohol-serving establishments; and restaurants and taverns, which serve food and drink. In North America, the lodging aspect of the word "inn" lives on in hotel brand names like Holiday Inn, and in some state laws that refer to lodging operators as innkeepers.
The Inns of Court and Inns of Chancery in London started as ordinary inns where barristers met to do business, but became institutions of the legal profession in England and Wales.
Traditional English ale was made solely from fermented malt. The practice of adding hops to produce beer was introduced from the Netherlands in the early 15th century. Alehouses would each brew their own distinctive ale, but independent breweries began to appear in the late 17th century. By the end of the century almost all beer was brewed by commercial breweries.
The 18th century saw a huge growth in the number of drinking establishments, primarily due to the introduction of gin. Gin was brought to England by the Dutch after the Glorious Revolution of 1688 and became very popular after the government created a market for "cuckoo grain" or "cuckoo malt" that was unfit to be used in brewing and distilling by allowing unlicensed gin and beer production, while imposing a heavy duty on all imported spirits. As thousands of gin-shops sprang up all over England, brewers fought back by increasing the number of alehouses. By 1740 the production of gin had increased to six times that of beer and because of its cheapness it became popular with the poor, leading to the so-called Gin Craze. Over half of the 15,000 drinking establishments in London were gin shops.
The drunkenness and lawlessness created by gin was seen to lead to ruination and degradation of the working classes. The distinction[clarification needed] was illustrated by William Hogarth in his engravings Beer Street and Gin Lane. The Gin Act 1736 imposed high taxes on retailers and led to riots in the streets. The prohibitive duty was gradually reduced and finally abolished in 1742. The Gin Act 1751 however was more successful. It forced distillers to sell only to licensed retailers and brought gin shops under the jurisdiction of local magistrates.
By the early 19th century, encouraged by lower duties on gin, the gin houses or "Gin Palaces" had spread from London to most cities and towns in Britain, with most of the new establishments illegal and unlicensed. These bawdy, loud and unruly drinking dens so often described by Charles Dickens in his Sketches by Boz (published 1835–1836) increasingly came to be held as unbridled cesspits of immorality or crime and the source of much ill-health and alcoholism among the working classes.
Under a banner of "reducing public drunkenness" the Beer Act of 1830 introduced a new lower tier of premises permitted to sell alcohol, the Beer Houses. At the time beer was viewed as harmless, nutritious and even healthy. Young children were often given what was described as small beer, which was brewed to have a low alcohol content, as the local water was often unsafe. Even the evangelical church and temperance movements of the day viewed the drinking of beer very much as a secondary evil and a normal accompaniment to a meal. The freely available beer was thus intended to wean the drinkers off the evils of gin, or so the thinking went.
Under the 1830 Act any householder who paid rates could apply, with a one-off payment of two guineas (roughly equal in value to £168 today), to sell beer or cider in his home (usually the front parlour) and even to brew his own on his premises. The permission did not extend to the sale of spirits and fortified wines, and any beer house discovered selling those items was closed down and the owner heavily fined. Beer houses were not permitted to open on Sundays. The beer was usually served in jugs or dispensed directly from tapped wooden barrels on a table in the corner of the room. Often profits were so high the owners were able to buy the house next door to live in, turning every room in their former home into bars and lounges for customers.
In the first year, 400 beer houses opened and within eight years there were 46,000 across the country, far outnumbering the combined total of long-established taverns, pubs, inns and hotels. Because it was so easy to obtain permission and the profits could be huge compared to the low cost of gaining permission, the number of beer houses was continuing to rise and in some towns nearly every other house in a street could be a beer house. Finally in 1869 the growth had to be checked by magisterial control and new licensing laws were introduced. Only then was it made harder to get a licence, and the licensing laws which operate today were formulated.
Although the new licensing laws prevented new beer houses from being created, those already in existence were allowed to continue and many did not close until nearly the end of the 19th century. A very small number remained into the 21st century. The vast majority of the beer houses applied for the new licences and became full pubs. These usually small establishments can still be identified in many towns, seemingly oddly located in the middle of otherwise terraced housing part way up a street, unlike purpose-built pubs that are usually found on corners or road junctions. Many of today's respected real ale micro-brewers in the UK started as home based Beer House brewers under the 1830 Act.
The beer houses tended to avoid the traditional pub names like The Crown, The Red Lion, The Royal Oak etc. and, if they did not simply name their place Smith's Beer House, they would apply topical pub names in an effort to reflect the mood of the times.
There was already regulation on public drinking spaces in the 17th and 18th centuries,[citation needed] and the income earned from licences was beneficial to the crown. Tavern owners were required to possess a licence to sell ale, and a separate licence for distilled spirits.
From the mid-19th century on the opening hours of licensed premises in the UK were restricted. However licensing was gradually liberalised after the 1960s, until contested licensing applications became very rare, and the remaining administrative function was transferred to Local Authorities in 2005.
The Wine and Beerhouse Act 1869 reintroduced the stricter controls of the previous century. The sale of beers, wines or spirits required a licence for the premises from the local magistrates. Further provisions regulated gaming, drunkenness, prostitution and undesirable conduct on licensed premises, enforceable by prosecution or more effectively by the landlord under threat of forfeiting his licence. Licences were only granted, transferred or renewed at special Licensing Sessions courts, and were limited to respectable individuals. Often these were ex-servicemen or ex-policemen; retiring to run a pub was popular amongst military officers at the end of their service. Licence conditions varied widely, according to local practice. They would specify permitted hours, which might require Sunday closing, or conversely permit all-night opening near a market. Typically they might require opening throughout the permitted hours, and the provision of food or lavatories. Once obtained, licences were jealously protected by the licensees (who were expected to be generally present, not an absentee owner or company), and even "Occasional Licences" to serve drinks at temporary premises such as fêtes would usually be granted only to existing licensees. Objections might be made by the police, rival landlords or anyone else on the grounds of infractions such as serving drunks, disorderly or dirty premises, or ignoring permitted hours.
Detailed licensing records were kept, giving the Public House, its address, owner, licensee and misdemeanours of the licensees, often going back for hundreds of years[citation needed]. Many of these records survive and can be viewed, for example, at the London Metropolitan Archives centre.
The restrictions were tightened by the Defence of the Realm Act of August 1914, which, along with the introduction of rationing and the censorship of the press for wartime purposes, restricted pubs' opening hours to 12 noon–2:30 pm and 6:30 pm–9:30 pm. Opening for the full licensed hours was compulsory, and closing time was equally firmly enforced by the police; a landlord might lose his licence for infractions. Pubs were closed under the Act and compensation paid, for example in Pembrokeshire.
There was a special case established under the State Management Scheme where the brewery and licensed premises were bought and run by the state until 1973, most notably in Carlisle. During the 20th century elsewhere, both the licensing laws and enforcement were progressively relaxed, and there were differences between parishes; in the 1960s, at closing time in Kensington at 10:30 pm, drinkers would rush over the parish boundary to be in good time for "Last Orders" in Knightsbridge before 11 pm, a practice observed in many pubs adjoining licensing area boundaries. Some Scottish and Welsh parishes remained officially "dry" on Sundays (although often this merely required knocking at the back door of the pub). These restricted opening hours led to the tradition of lock-ins.
However, closing times were increasingly disregarded in the country pubs. In England and Wales by 2000 pubs could legally open from 11 am (12 noon on Sundays) through to 11 pm (10:30 pm on Sundays). That year was also the first to allow continuous opening for 36 hours from 11 am on New Year's Eve to 11 pm on New Year's Day. In addition, many cities had by-laws to allow some pubs to extend opening hours to midnight or 1 am, whilst nightclubs had long been granted late licences to serve alcohol into the morning. Pubs near London's Smithfield market, Billingsgate fish market and Covent Garden fruit and flower market could stay open 24 hours a day since Victorian times to provide a service to the shift working employees of the markets.
Scotland's and Northern Ireland's licensing laws have long been more flexible, allowing local authorities to set pub opening and closing times. In Scotland, this stemmed out of[clarification needed] a late repeal of the wartime licensing laws, which stayed in force until 1976.
The Licensing Act 2003, which came into force on 24 November 2005, consolidated the many laws into a single Act. This allowed pubs in England and Wales to apply to the local council for the opening hours of their choice. It was argued that this would end the concentration of violence around 11.30 pm, when people had to leave the pub, making policing easier. In practice, alcohol-related hospital admissions rose following the change in the law, with alcohol involved in 207,800 admissions in 2006/7. Critics claimed that these laws would lead to "24-hour drinking". By the time the law came into effect, 60,326 establishments had applied for longer hours and 1,121 had applied for a licence to sell alcohol 24 hours a day. However nine months later many pubs had not changed their hours, although some stayed open longer at the weekend, but rarely beyond 1:00 am.
A "lock-in" is when a pub owner lets drinkers stay in the pub after the legal closing time, on the theory that once the doors are locked, it becomes a private party rather than a pub. Patrons may put money behind the bar before official closing time, and redeem their drinks during the lock-in so no drinks are technically sold after closing time. The origin of the British lock-in was a reaction to 1915 changes in the licensing laws in England and Wales, which curtailed opening hours to stop factory workers from turning up drunk and harming the war effort. Since 1915, the UK licensing laws had changed very little, with comparatively early closing times. The tradition of the lock-in therefore remained. Since the implementation of Licensing Act 2003, premises in England and Wales may apply to extend their opening hours beyond 11 pm, allowing round-the-clock drinking and removing much of the need for lock-ins. Since the smoking ban, some establishments operated a lock-in during which the remaining patrons could smoke without repercussions but, unlike drinking lock-ins, allowing smoking in a pub was still a prosecutable offence.
In March 2006, a law was introduced to forbid smoking in all enclosed public places in Scotland. Wales followed suit in April 2007, with England introducing the ban in July 2007. Pub landlords had raised concerns prior to the implementation of the law that a smoking ban would have a negative impact on sales. After two years, the impact of the ban was mixed; some pubs suffered declining sales, while others developed their food sales. The Wetherspoon pub chain reported in June 2009 that profits were at the top end of expectations; however, Scottish & Newcastle's takeover by Carlsberg and Heineken was reported in January 2008 as partly the result of its weakness following falling sales due to the ban. Similar bans are applied in Australian pubs with smoking only allowed in designated areas.
By the end of the 18th century a new room in the pub was established: the saloon.[citation needed] Beer establishments had always provided entertainment of some sort—singing, gaming or sport.[citation needed] Balls Pond Road in Islington was named after an establishment run by a Mr Ball that had a duck pond at the rear, where drinkers could, for a fee, go out and take a potshot at the ducks. More common, however, was a card room or a billiard room.[citation needed] The saloon was a room where for an admission fee or a higher price of drinks, singing, dancing, drama or comedy was performed and drinks would be served at the table.[citation needed] From this came the popular music hall form of entertainment—a show consisting of a variety of acts.[citation needed] A most famous London saloon was the Grecian Saloon in The Eagle, City Road, which is still famous because of a nursery rhyme: "Up and down the City Road / In and out The Eagle / That's the way the money goes / Pop goes the weasel." This meant that the customer had spent all his money at The Eagle, and needed to pawn his "weasel" to get some more. The meaning of the "weasel" is unclear but the two most likely definitions are: a flat iron used for finishing clothing; or rhyming slang for a coat (weasel and stoat).
A few pubs have stage performances such as serious drama, stand-up comedy, musical bands, cabaret or striptease; however juke boxes, karaoke and other forms of pre-recorded music have otherwise replaced the musical tradition of a piano or guitar and singing.[citation needed]
By the 20th century, the saloon, or lounge bar, had become a middle-class room[citation needed]—carpets on the floor, cushions on the seats, and a penny or two on the prices,[citation needed] while the public bar, or tap room, remained working class with bare boards, sometimes with sawdust to absorb the spitting and spillages (known as "spit and sawdust"), hard bench seats, and cheap beer[citation needed]. This bar was known as the four-ale bar from the days when the cheapest beer served there cost 4 pence (4d) a quart.[citation needed]
Later, the public bars gradually improved until sometimes almost the only difference was in the prices, so that customers could choose between economy and exclusivity (or youth and age, or a jukebox or dartboard).[citation needed] With the blurring of class divisions in the 1960s and 1970s,[citation needed] the distinction between the saloon and the public bar was often seen as archaic,[citation needed] and was frequently abolished, usually by the removal of the dividing wall or partition.[citation needed] While the names of saloon and public bar may still be seen on the doors of pubs, the prices (and often the standard of furnishings and decoration) are the same throughout the premises, and many pubs now comprise one large room. However the modern importance of dining in pubs encourages some establishments to maintain distinct rooms or areas.
The "snug", sometimes called the smoke room, was typically a small, very private room with access to the bar that had a frosted glass external window, set above head height. A higher price was paid for beer in the snug and nobody could look in and see the drinkers. It was not only the wealthy visitors who would use these rooms. The snug was for patrons who preferred not to be seen in the public bar. Ladies would often enjoy a private drink in the snug in a time when it was frowned upon for women to be in a pub. The local police officer might nip in for a quiet pint, the parish priest for his evening whisky, or lovers for a rendezvous.
CAMRA have surveyed the 50,000 pubs in Britain and they believe that there are very few pubs that still have classic snugs. These are on a historic interiors list in order that they can be preserved.
It was the pub that first introduced the concept of the bar counter being used to serve the beer. Until that time beer establishments used to bring the beer out to the table or benches, as remains the practice in (for example) beer gardens and other drinking establishments in Germany. A bar might be provided for the manager to do paperwork while keeping an eye on his or her customers, but the casks of ale were kept in a separate taproom. When the first pubs were built, the main room was the public room with a large serving bar copied from the gin houses, the idea being to serve the maximum number of people in the shortest possible time. It became known as the public bar[citation needed]. The other, more private, rooms had no serving bar—they had the beer brought to them from the public bar. There are a number of pubs in the Midlands or the North which still retain this set up but these days the beer is fetched by the customer from the taproom or public bar. One of these is The Vine, known locally as The Bull and Bladder, in Brierley Hill near Birmingham, another the Cock at Broom, Bedfordshire a series of small rooms served drinks and food by waiting staff. In the Manchester district the public bar was known as the "vault", other rooms being the lounge and snug as usual elsewhere. By the early 1970s there was a tendency to change to one large drinking room and breweries were eager to invest in interior design and theming.
Isambard Kingdom Brunel, the British engineer and railway builder, introduced the idea of a circular bar into the Swindon station pub in order that customers were served quickly and did not delay his trains. These island bars became popular as they also allowed staff to serve customers in several different rooms surrounding the bar.
A "beer engine" is a device for pumping beer, originally manually operated and typically used to dispense beer from a cask or container in a pub's basement or cellar.
The first beer pump known in England is believed to have been invented by John Lofting (b. Netherlands 1659-d. Great Marlow Buckinghamshire 1742) an inventor, manufacturer and merchant of London.
The London Gazette of 17 March 1691 published a patent in favour of John Lofting for a fire engine, but remarked upon and recommended another invention of his, for a beer pump:
"Whereas their Majesties have been Graciously Pleased to grant Letters patent to John Lofting of London Merchant for a New Invented Engine for Extinguishing Fires which said Engine have found every great encouragement. The said Patentee hath also projected a Very Useful Engine for starting of beer and other liquors which will deliver from 20 to 30 barrels an hour which are completely fixed with Brass Joints and Screws at Reasonable Rates. Any Person that hath occasion for the said Engines may apply themselves to the Patentee at his house near St Thomas Apostle London or to Mr. Nicholas Wall at the Workshoppe near Saddlers Wells at Islington or to Mr. William Tillcar, Turner, his agent at his house in Woodtree next door to the Sun Tavern London."
Strictly the term refers to the pump itself, which is normally manually operated, though electrically powered and gas powered pumps are occasionally used. When manually powered, the term "handpump" is often used to refer to both the pump and the associated handle.
After the development of the large London Porter breweries in the 18th century, the trend grew for pubs to become tied houses which could only sell beer from one brewery (a pub not tied in this way was called a Free house). The usual arrangement for a tied house was that the pub was owned by the brewery but rented out to a private individual (landlord) who ran it as a separate business (even though contracted to buy the beer from the brewery). Another very common arrangement was (and is) for the landlord to own the premises (whether freehold or leasehold) independently of the brewer, but then to take a mortgage loan from a brewery, either to finance the purchase of the pub initially, or to refurbish it, and be required as a term of the loan to observe the solus tie.
A trend in the late 20th century was for breweries to run their pubs directly, using managers rather than tenants. Most such breweries, such as the regional brewery Shepherd Neame in Kent and Young's and Fuller's in London, control hundreds of pubs in a particular region of the UK, while a few, such as Greene King, are spread nationally. The landlord of a tied pub may be an employee of the brewery—in which case he/she would be a manager of a managed house, or a self-employed tenant who has entered into a lease agreement with a brewery, a condition of which is the legal obligation (trade tie) only to purchase that brewery's beer. The beer selection is mainly limited to beers brewed by that particular company. The Beer Orders, passed in 1989, were aimed at getting tied houses to offer at least one alternative beer, known as a guest beer, from another brewery. This law has now been repealed but while in force it dramatically altered the industry. Some pubs still offer a regularly changing selection of guest beers.
Organisations such as Wetherspoons, Punch Taverns and O'Neill's were formed in the UK in the wake of the Beer Orders. A PubCo is a company involved in the retailing but not the manufacture of beverages, while a Pub chain may be run either by a PubCo or by a brewery.
Pubs within a chain will usually have items in common, such as fittings, promotions, ambience and range of food and drink on offer. A pub chain will position itself in the marketplace for a target audience. One company may run several pub chains aimed at different segments of the market. Pubs for use in a chain are bought and sold in large units, often from regional breweries which are then closed down. Newly acquired pubs are often renamed by the new owners, and many people resent the loss of traditional names, especially if their favourite regional beer disappears at the same time.
A brewery tap is the nearest outlet for a brewery's beers. This is usually a room or bar in the brewery itself, though the name may be applied to the nearest pub. The term is not applied to a brewpub which brews and sells its beer on the same premises.
A "country pub" by tradition is a rural public house. However, the distinctive culture surrounding country pubs, that of functioning as a social centre for a village and rural community, has been changing over the last thirty or so years. In the past, many rural pubs provided opportunities for country folk to meet and exchange (often local) news, while others—especially those away from village centres—existed for the general purpose, before the advent of motor transport, of serving travellers as coaching inns.
In more recent years, however, many country pubs have either closed down, or have been converted to establishments intent on providing seating facilities for the consumption of food, rather than a venue for members of the local community meeting and convivially drinking.
Pubs that cater for a niche clientele, such as sports fans or people of certain nationalities are known as theme pubs. Examples of theme pubs include sports bars, rock pubs, biker pubs, Goth pubs, strip pubs, gay bars, karaoke bars and Irish pubs.
In 1393 King Richard II compelled landlords to erect signs outside their premises. The legislation stated "Whosoever shall brew ale in the town with intention of selling it must hang out a sign, otherwise he shall forfeit his ale." This was to make alehouses easily visible to passing inspectors, borough ale tasters, who would decide the quality of the ale they provided. William Shakespeare's father, John Shakespeare, was one such inspector.
Another important factor was that during the Middle Ages a large proportion of the population would have been illiterate and so pictures on a sign were more useful than words as a means of identifying a public house. For this reason there was often no reason to write the establishment's name on the sign and inns opened without a formal written name, the name being derived later from the illustration on the pub's sign.
The earliest signs were often not painted but consisted, for example, of paraphernalia connected with the brewing process such as bunches of hops or brewing implements, which were suspended above the door of the pub. In some cases local nicknames, farming terms and puns were used. Local events were often commemorated in pub signs. Simple natural or religious symbols such as 'The Sun', 'The Star' and 'The Cross' were incorporated into pub signs, sometimes being adapted to incorporate elements of the heraldry (e.g. the coat of arms) of the local lords who owned the lands upon which the pub stood. Some pubs have Latin inscriptions.
Other subjects that lent themselves to visual depiction included the name of battles (e.g. Trafalgar), explorers, local notables, discoveries, sporting heroes and members of the royal family. Some pub signs are in the form of a pictorial pun or rebus. For example, a pub in Crowborough, East Sussex called The Crow and Gate has an image of a crow with gates as wings.
Most British pubs still have decorated signs hanging over their doors, and these retain their original function of enabling the identification of the pub. Today's pub signs almost always bear the name of the pub, both in words and in pictorial representation. The more remote country pubs often have stand-alone signs directing potential customers to their door.
Pub names are used to identify and differentiate each pub. Modern names are sometimes a marketing ploy or attempt to create "brand awareness", frequently using a comic theme thought to be memorable, Slug and Lettuce for a pub chain being an example. Interesting origins are not confined to old or traditional names, however. Names and their origins can be broken up into a relatively small number of categories.
As many pubs are centuries old, many of their early customers were unable to read, and pictorial signs could be readily recognised when lettering and words could not be read.
Pubs often have traditional names. A common name is the "Marquis of Granby". These pubs were named after John Manners, Marquess of Granby, who was the son of John Manners, 3rd Duke of Rutland and a general in the 18th century British Army. He showed a great concern for the welfare of his men, and on their retirement, provided funds for many of them to establish taverns, which were subsequently named after him. All pubs granted their licence in 1780 were called the Royal George[citation needed], after King George III, and the twentieth anniversary of his coronation.
Many names for pubs that appear nonsensical may have come from corruptions of old slogans or phrases, such as "The Bag o'Nails" (Bacchanals), "The Goat and Compasses" (God Encompasseth Us), "The Cat and the Fiddle" (Chaton Fidèle: Faithful Kitten) and "The Bull and Bush", which purportedly celebrates the victory of Henry VIII at "Boulogne Bouche" or Boulogne-sur-Mer Harbour.
Traditional games are played in pubs, ranging from the well-known darts, skittles, dominoes, cards and bar billiards, to the more obscure Aunt Sally, Nine Men's Morris and ringing the bull. In the UK betting is legally limited to certain games such as cribbage or dominoes, played for small stakes. In recent decades the game of pool (both the British and American versions) has increased in popularity as well as other table based games such as snooker or Table Football becoming common.
Increasingly, more modern games such as video games and slot machines are provided. Pubs hold special events, from tournaments of the aforementioned games to karaoke nights to pub quizzes. Some play pop music and hip-hop (dance bar), or show football and rugby union on big screen televisions (sports bar). Shove ha'penny and Bat and trap were also popular in pubs south of London.
Some pubs in the UK also have football teams composed of regular customers. Many of these teams are in leagues that play matches on Sundays, hence the term "Sunday League Football". Bowling is found in association with pubs in some parts of the country and the local team will play matches against teams invited from elsewhere on the pub's bowling green.
Pubs may be venues for pub songs and live music. During the 1970s pubs provided an outlet for a number of bands, such as Kilburn and the High Roads, Dr. Feelgood and The Kursaal Flyers, who formed a musical genre called Pub rock that was a precursor to Punk music.
Many pubs were drinking establishments, and little emphasis was placed on the serving of food, other than sandwiches and "bar snacks", such as pork scratchings, pickled eggs, salted crisps and peanuts which helped to increase beer sales. In South East England (especially London) it was common until recent times for vendors selling cockles, whelks, mussels, and other shellfish to sell to customers during the evening and at closing time. Many mobile shellfish stalls would set up near pubs, a practice that continues in London's East End. Otherwise, pickled cockles and mussels may be offered by the pub in jars or packets.
In the 1950s some British pubs would offer "a pie and a pint", with hot individual steak and ale pies made easily on the premises by the proprietor's wife during the lunchtime opening hours. The ploughman's lunch became popular in the late 1960s. In the late 1960s "chicken in a basket", a portion of roast chicken with chips, served on a napkin, in a wicker basket became popular due to its convenience.
Quality dropped but variety increased with the introduction of microwave ovens and freezer food. "Pub grub" expanded to include British food items such as steak and ale pie, shepherd's pie, fish and chips, bangers and mash, Sunday roast, ploughman's lunch, and pasties. In addition, dishes such as burgers, chicken wings, lasagne and chilli con carne are often served. Some pubs offer elaborate hot and cold snacks free to customers at Sunday lunchtimes, to prevent them getting hungry and leaving for their lunch at home.
Since the 1990s food has become a more important part of a pub's trade, and today most pubs serve lunches and dinners at the table in addition to (or instead of) snacks consumed at the bar. They may have a separate dining room. Some pubs serve meals to a higher standard, to match good restaurant standards; these are sometimes termed gastropubs.
A gastropub concentrates on quality food. The name is a portmanteau of pub and gastronomy and was coined in 1991 when David Eyre and Mike Belben took over The Eagle pub in Clerkenwell, London. The concept of a restaurant in a pub reinvigorated both pub culture and British dining, though has occasionally attracted criticism for potentially removing the character of traditional pubs.
CAMRA maintains a "National Inventory" of historical notability and of architecturally and decoratively notable pubs. The National Trust owns thirty-six public houses of historic interest including the George Inn, Southwark, London and The Crown Liquor Saloon, Belfast, Northern Ireland.
The highest pub in the United Kingdom is the Tan Hill Inn, Yorkshire, at 1,732 feet (528 m) above sea level. The remotest pub on the British mainland is The Old Forge in the village of Inverie, Lochaber, Scotland. There is no road access and it may only be reached by an 18-mile (29 km) walk over mountains, or a 7-mile (11 km) sea crossing. Likewise, The Berney Arms in Norfolk has no road access. It may be reached by foot or by boat, and by train as it is served by the nearby Berney Arms railway station, which likewise has no road access and serves no other settlement.
A number of pubs claim to be the oldest surviving establishment in the United Kingdom, although in several cases original buildings have been demolished and replaced on the same site. Others are ancient buildings that saw uses other than as a pub during their history. Ye Olde Fighting Cocks in St Albans, Hertfordshire, holds the Guinness World Record for the oldest pub in England, as it is an 11th-century structure on an 8th-century site. Ye Olde Trip to Jerusalem in Nottingham is claimed to be the "oldest inn in England". It has a claimed date of 1189, based on the fact it is constructed on the site of the Nottingham Castle brewhouse; the present building dates from around 1650. Likewise, The Nags Head in Burntwood, Staffordshire only dates back to the 16th century, but there has been a pub on the site since at least 1086, as it is mentioned in the Domesday Book.
There is archaeological evidence that parts of the foundations of The Old Ferryboat Inn in Holywell may date to AD 460, and there is evidence of ale being served as early as AD 560.
The Bingley Arms, Bardsey, Yorkshire, is claimed to date to 905 AD. Ye Olde Salutation Inn in Nottingham dates from 1240, although the building served as a tannery and a private residence before becoming an inn sometime before the English Civil War. The Adam and Eve in Norwich was first recorded in 1249, when it was an alehouse for the workers constructing nearby Norwich Cathedral. Ye Olde Man & Scythe in Bolton, Lancashire, is mentioned by name in a charter of 1251, but the current building is dated 1631. Its cellars are the only surviving part of the older structure.
The town of Stalybridge in Cheshire is thought to have the pubs with both the longest and shortest names in the United Kingdom — The Old 13th Cheshire Rifleman Corps Inn and the Q Inn.
The number of pubs in the UK has declined year on year, at least since 1982. Various reasons are put forward for this, such as the failure of some establishments to keep up with customer requirements. Others claim the smoking ban of 2007, intense competition from gastro-pubs, the availability of cheap alcohol in supermarkets or the general economic climate are either to blame, or are factors in the decline. Changes in demographics may be an additional factor.
The Lost Pubs Project listed 28,095 closed pubs on 21 April 2015, with photographs of many. In 2015 the rate of pub closures came under the scrutiny of Parliament in the UK, with a promise of legislation to improve relations between owners and tenants.
The highwayman Dick Turpin used the Swan Inn at Woughton-on-the-Green in Buckinghamshire as his base. In the 1920s John Fothergill (1876–1957) was the innkeeper of the Spread Eagle in Thame, Berkshire, and published his autobiography: An Innkeeper's Diary (London: Chatto & Windus, 1931). During his idiosyncratic occupancy many famous people came to stay, such as H. G. Wells. United States president George W. Bush fulfilled his lifetime ambition of visiting a 'genuine British pub' during his November 2003 state visit to the UK when he had lunch and a pint of non-alcoholic lager (Bush being a teetotaler) with British Prime Minister Tony Blair at the Dun Cow pub in Sedgefield, County Durham in Blair's home constituency. There were approximately 53,500 public houses in 2009 in the United Kingdom. This number has been declining every year, so that nearly half of the smaller villages no longer have a local pub.
Many of London's pubs are known to have been used by famous people, but in some cases, such as the association between Samuel Johnson and Ye Olde Cheshire Cheese, this is speculative, based on little more than the fact that the person is known to have lived nearby. However, Charles Dickens is known to have visited the Cheshire Cheese, the Prospect of Whitby, Ye Olde Cock Tavern and many others. Samuel Pepys is also associated with the Prospect of Whitby and the Cock Tavern.
The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name. It became famous (or according to others, infamous) during a period spanning the 1920s to the mid-1950s as a meeting place for many of London's artists, intellectuals and bohemians such as Dylan Thomas, Augustus John, and George Orwell. Several establishments in Soho, London, have associations with well-known, post-war literary and artistic figures, including the Pillars of Hercules, The Colony Room and the Coach and Horses. The Canonbury Tavern, Canonbury, was the prototype for Orwell's ideal English pub, The Moon Under Water.
The Red Lion in Parliament Square is close to the Palace of Westminster and is consequently used by political journalists and members of parliament. The pub is equipped with a Division bell that summons MPs back to the chamber when they are required to take part in a vote. The Punch Bowl, Mayfair was at one time jointly owned by Madonna and Guy Ritchie. The Coleherne public house in Earls Court was a well-known gay pub from the 1950s. It attracted many well-known patrons, such as Freddie Mercury, Kenny Everett and Rudolph Nureyev. It was used by the serial-killer Colin Ireland to pick up victims.
In 1966 The Blind Beggar in Whitechapel became infamous as the scene of a murder committed by gangster Ronnie Kray. The Ten Bells is associated with several of the victims of Jack the Ripper. In 1955, Ruth Ellis, the last woman executed in the United Kingdom, shot David Blakely as he emerged from The Magdala in South Hill Park, Hampstead, the bullet holes can still be seen in the walls outside. It is said that Vladimir Lenin and a young Joseph Stalin met in the Crown and Anchor pub (now known as The Crown Tavern) on Clerkenwell Green when the latter was visiting London in 1903.
The Angel, Islington was formerly a coaching inn, the first on the route northwards out of London, where Thomas Paine is believed to have written much of The Rights of Man. It was mentioned by Charles Dickens, became a Lyons Corner House, and is now a Co-operative Bank.
The Eagle and Child and the Lamb and Flag, Oxford, were regular meeting places of the Inklings, a writers' group which included J. R. R. Tolkien and C. S. Lewis. The Eagle in Cambridge is where Francis Crick interrupted patrons' lunchtime on 28 February 1953 to announce that he and James Watson had "discovered the secret of life" after they had come up with their proposal for the structure of DNA. The anecdote is related in Watson's book The Double Helix. and commemorated with a blue plaque on the outside wall.
The major soap operas on British television each feature a pub, and these pubs have become household names. The Rovers Return is the pub in Coronation Street, the British soap broadcast on ITV. The Queen Vic (short for the Queen Victoria) is the pub in EastEnders, the major soap on BBC One and the Woolpack in ITV's Emmerdale. The sets of each of the three major television soap operas have been visited by some of the members of the royal family, including Queen Elizabeth II. The centrepiece of each visit was a trip into the Rovers, the Queen Vic, or the Woolpack to be offered a drink. The Bull in the BBC Radio 4 soap opera The Archers is an important meeting point.
Although "British" pubs found outside of Britain and its former colonies are often themed bars owing little to the original British pub, a number of "true" pubs may be found around the world.
In Denmark—a country, like Britain, with a long tradition of brewing—a number of pubs have opened which eschew "theming", and which instead focus on the business of providing carefully conditioned beer, often independent of any particular brewery or chain, in an environment which would not be unfamiliar to a British pub-goer. Some import British cask ale, rather than beer in kegs, to provide the full British real ale experience to their customers. This newly established Danish interest in British cask beer and the British pub tradition is reflected by the fact that some 56 British cask beers were available at the 2008 European Beer Festival in Copenhagen, which was attended by more than 20,000 people.
In Ireland, pubs are known for their atmosphere or "craic". In Irish, a pub is referred to as teach tábhairne ("tavernhouse") or teach óil ("drinkinghouse"). Live music, either sessions of traditional Irish music or varieties of modern popular music, is frequently featured in the pubs of Ireland. Pubs in Northern Ireland are largely identical to their counterparts in the Republic of Ireland except for the lack of spirit grocers. A side effect of "The Troubles" was that the lack of a tourist industry meant that a higher proportion of traditional bars have survived the wholesale refitting of Irish pub interiors in the 'English style' in the 1950s and 1960s. New Zealand sports a number of Irish pubs.
The most popular term in English-speaking Canada used for a drinking establishment was "tavern", until the 1970s when the term "bar" became widespread as in the United States. In the 1800s the term used was "public house" as in England but "pub culture" did not spread to Canada. A fake "English looking" pub trend started in the 1990s, built into existing storefronts, like regular bars. Most universities in Canada have campus pubs which are central to student life, as it would be bad form just to serve alcohol to students without providing some type of basic food. Often these pubs are run by the student's union. The gastropub concept has caught on, as traditional British influences are to be found in many Canadian dishes. There are now pubs in the large cities of Canada that cater to anyone interested in a "pub" type drinking environment.[citation needed]
On 16 September 2001, at Camp David, President George W. Bush used the phrase war on terrorism in an unscripted and controversial comment when he said, "This crusade – this war on terrorism – is going to take a while, ... " Bush later apologized for this remark due to the negative connotations the term crusade has to people, e.g. of Muslim faith. The word crusade was not used again. On 20 September 2001, during a televised address to a joint session of congress, Bush stated that, "(o)ur 'war on terror' begins with al-Qaeda, but it does not end there. It will not end until every terrorist group of global reach has been found, stopped, and defeated."
U.S. President Barack Obama has rarely used the term, but in his inaugural address on 20 January 2009, he stated "Our nation is at war, against a far-reaching network of violence and hatred." In March 2009 the Defense Department officially changed the name of operations from "Global War on Terror" to "Overseas Contingency Operation" (OCO). In March 2009, the Obama administration requested that Pentagon staff members avoid use of the term, instead using "Overseas Contingency Operation". Basic objectives of the Bush administration "war on terror", such as targeting al Qaeda and building international counterterrorism alliances, remain in place. In December 2012, Jeh Johnson, the General Counsel of the Department of Defense, stated that the military fight will be replaced by a law enforcement operation when speaking at Oxford University, predicting that al Qaeda will be so weakened to be ineffective, and has been "effectively destroyed", and thus the conflict will not be an armed conflict under international law. In May 2013, Obama stated that the goal is "to dismantle specific networks of violent extremists that threaten America"; which coincided with the U.S. Office of Management and Budget having changed the wording from "Overseas Contingency Operations" to "Countering Violent Extremism" in 2010.
Because the actions involved in the "war on terrorism" are diffuse, and the criteria for inclusion are unclear, political theorist Richard Jackson has argued that "the 'war on terrorism' therefore, is simultaneously a set of actual practices—wars, covert operations, agencies, and institutions—and an accompanying series of assumptions, beliefs, justifications, and narratives—it is an entire language or discourse." Jackson cites among many examples a statement by John Ashcroft that "the attacks of September 11 drew a bright line of demarcation between the civil and the savage". Administration officials also described "terrorists" as hateful, treacherous, barbarous, mad, twisted, perverted, without faith, parasitical, inhuman, and, most commonly, evil. Americans, in contrast, were described as brave, loving, generous, strong, resourceful, heroic, and respectful of human rights.
The origins of al-Qaeda can be traced to the Soviet war in Afghanistan (December 1979 – February 1989). The United States, United Kingdom, Saudi Arabia, Pakistan, and the People's Republic of China supported the Islamist Afghan mujahadeen guerillas against the military forces of the Soviet Union and the Democratic Republic of Afghanistan. A small number of "Afghan Arab" volunteers joined the fight against the Soviets, including Osama bin Laden, but there is no evidence they received any external assistance. In May 1996 the group World Islamic Front for Jihad Against Jews and Crusaders (WIFJAJC), sponsored by bin Laden (and later re-formed as al-Qaeda), started forming a large base of operations in Afghanistan, where the Islamist extremist regime of the Taliban had seized power earlier in the year. In February 1998, Osama bin Laden signed a fatwā, as head of al-Qaeda, declaring war on the West and Israel, later in May of that same year al-Qaeda released a video declaring war on the U.S. and the West.
On 7 August 1998, al-Qaeda struck the U.S. embassies in Kenya and Tanzania, killing 224 people, including 12 Americans. In retaliation, U.S. President Bill Clinton launched Operation Infinite Reach, a bombing campaign in Sudan and Afghanistan against targets the U.S. asserted were associated with WIFJAJC, although others have questioned whether a pharmaceutical plant in Sudan was used as a chemical warfare plant. The plant produced much of the region's antimalarial drugs and around 50% of Sudan's pharmaceutical needs. The strikes failed to kill any leaders of WIFJAJC or the Taliban.
On the morning of 11 September 2001, 19 men affiliated with al-Qaeda hijacked four airliners all bound for California. Once the hijackers assumed control of the airliners, they told the passengers that they had the bomb on board and would spare the lives of passengers and crew once their demands were met – no passenger and crew actually suspected that they would use the airliners as suicide weapons since it had never happened before in history. The hijackers – members of al-Qaeda's Hamburg cell – intentionally crashed two airliners into the Twin Towers of the World Trade Center in New York City. Both buildings collapsed within two hours from fire damage related to the crashes, destroying nearby buildings and damaging others. The hijackers crashed a third airliner into the Pentagon in Arlington County, Virginia, just outside Washington D.C. The fourth plane crashed into a field near Shanksville, Pennsylvania, after some of its passengers and flight crew attempted to retake control of the plane, which the hijackers had redirected toward Washington D.C., to target the White House, or the U.S. Capitol. No flights had survivors. A total of 2,977 victims and the 19 hijackers perished in the attacks.
The Authorization for Use of Military Force Against Terrorists or "AUMF" was made law on 14 September 2001, to authorize the use of United States Armed Forces against those responsible for the attacks on 11 September 2001. It authorized the President to use all necessary and appropriate force against those nations, organizations, or persons he determines planned, authorized, committed, or aided the terrorist attacks that occurred on 11 September 2001, or harbored such organizations or persons, in order to prevent any future acts of international terrorism against the United States by such nations, organizations or persons. Congress declares this is intended to constitute specific statutory authorization within the meaning of section 5(b) of the War Powers Resolution of 1973.
Subsequently, in October 2001, U.S. forces (with UK and coalition allies) invaded Afghanistan to oust the Taliban regime. On 7 October 2001, the official invasion began with British and U.S. forces conducting airstrike campaigns over enemy targets. Kabul, the capital city of Afghanistan, fell by mid-November. The remaining al-Qaeda and Taliban remnants fell back to the rugged mountains of eastern Afghanistan, mainly Tora Bora. In December, Coalition forces (the U.S. and its allies) fought within that region. It is believed that Osama bin Laden escaped into Pakistan during the battle.
The Taliban regrouped in western Pakistan and began to unleash an insurgent-style offensive against Coalition forces in late 2002. Throughout southern and eastern Afghanistan, firefights broke out between the surging Taliban and Coalition forces. Coalition forces responded with a series of military offensives and an increase in the amount of troops in Afghanistan. In February 2010, Coalition forces launched Operation Moshtarak in southern Afghanistan along with other military offensives in the hopes that they would destroy the Taliban insurgency once and for all. Peace talks are also underway between Taliban affiliated fighters and Coalition forces. In September 2014, Afghanistan and the United States signed a security agreement, which permits United States and NATO forces to remain in Afghanistan until at least 2024. The United States and other NATO and non-NATO forces are planning to withdraw; with the Taliban claiming it has defeated the United States and NATO, and the Obama Administration viewing it as a victory. In December 2014, ISAF encasing its colors, and Resolute Support began as the NATO operation in Afghanistan. Continued United States operations within Afghanistan will continue under the name "Operation Freedom's Sentinel".
In January 2002, the United States Special Operations Command, Pacific deployed to the Philippines to advise and assist the Armed Forces of the Philippines in combating Filipino Islamist groups. The operations were mainly focused on removing the Abu Sayyaf group and Jemaah Islamiyah (JI) from their stronghold on the island of Basilan. The second portion of the operation was conducted as a humanitarian program called "Operation Smiles". The goal of the program was to provide medical care and services to the region of Basilan as part of a "Hearts and Minds" program. Joint Special Operations Task Force – Philippines disbanded in June 2014, ending a 14-year mission. After JSOTF-P disbanded, as late as November 2014, American forces continued to operate in the Philippines under the name "PACOM Augmentation Team".
On 14 September 2009, U.S. Special Forces killed two men and wounded and captured two others near the Somali village of Baarawe. Witnesses claim that helicopters used for the operation launched from French-flagged warships, but that could not be confirmed. A Somali-based al-Qaida affiliated group, the Al-Shabaab, has confirmed the death of "sheik commander" Saleh Ali Saleh Nabhan along with an unspecified number of militants. Nabhan, a Kenyan, was wanted in connection with the 2002 Mombasa attacks.
The conflict in northern Mali began in January 2012 with radical Islamists (affiliated to al-Qaeda) advancing into northern Mali. The Malian government had a hard time maintaining full control over their country. The fledgling government requested support from the international community on combating the Islamic militants. In January 2013, France intervened on behalf of the Malian government's request and deployed troops into the region. They launched Operation Serval on 11 January 2013, with the hopes of dislodging the al-Qaeda affiliated groups from northern Mali.
Following the ceasefire agreement that suspended hostilities (but not officially ended) in the 1991 Gulf War, the United States and its allies instituted and began patrolling Iraqi no-fly zones, to protect Iraq's Kurdish and Shi'a Arab population—both of which suffered attacks from the Hussein regime before and after the Gulf War—in Iraq's northern and southern regions, respectively. U.S. forces continued in combat zone deployments through November 1995 and launched Operation Desert Fox against Iraq in 1998 after it failed to meet U.S. demands of "unconditional cooperation" in weapons inspections.
The first ground attack came at the Battle of Umm Qasr on 21 March 2003 when a combined force of British, American and Polish forces seized control of the port city of Umm Qasr. Baghdad, Iraq's capital city, fell to American forces in April 2003 and Saddam Hussein's government quickly dissolved. On 1 May 2003, Bush announced that major combat operations in Iraq had ended. However, an insurgency arose against the U.S.-led coalition and the newly developing Iraqi military and post-Saddam government. The insurgency, which included al-Qaeda affiliated groups, led to far more coalition casualties than the invasion. Other elements of the insurgency were led by fugitive members of President Hussein's Ba'ath regime, which included Iraqi nationalists and pan-Arabists. Many insurgency leaders are Islamists and claim to be fighting a religious war to reestablish the Islamic Caliphate of centuries past. Iraq's former president, Saddam Hussein was captured by U.S. forces in December 2003. He was executed in 2006.
In a major split in the ranks of Al Qaeda's organization, the Iraqi franchise, known as Al Qaeda in Iraq covertly invaded Syria and the Levant and began participating in the ongoing Syrian Civil War, gaining enough support and strength to re-invade Iraq's western provinces under the name of the Islamic State of Iraq and the Levant (ISIS/ISIL), taking over much of the country in a blitzkrieg-like action and combining the Iraq insurgency and Syrian Civil War into a single conflict. Due to their extreme brutality and a complete change in their overall ideology, Al Qaeda's core organization in Central Asia eventually denounced ISIS and directed their affiliates to cut off all ties with this organization. Many analysts[who?] believe that because of this schism, Al Qaeda and ISIL are now in a competition to retain the title of the world's most powerful terrorist organization.
The Obama administration began to reengage in Iraq with a series of airstrikes aimed at ISIS beginning on 10 August 2014. On 9 September 2014 President Obama said that he had the authority he needed to take action to destroy the militant group known as the Islamic State of Iraq and the Levant, citing the 2001 Authorization for Use of Military Force Against Terrorists, and thus did not require additional approval from Congress. The following day on 10 September 2014 President Barack Obama made a televised speech about ISIL, which he stated "Our objective is clear: We will degrade, and ultimately destroy, ISIL through a comprehensive and sustained counter-terrorism strategy". Obama has authorized the deployment of additional U.S. Forces into Iraq, as well as authorizing direct military operations against ISIL within Syria. On the night of 21/22 September the United States, Saudi Arabia, Bahrain, the UAE, Jordan and Qatar started air attacks against ISIS in Syria.[citation needed]
Following the 11 September 2001 attacks, former President of Pakistan Pervez Musharraf sided with the U.S. against the Taliban government in Afghanistan after an ultimatum by then U.S. President George W. Bush. Musharraf agreed to give the U.S. the use of three airbases for Operation Enduring Freedom. United States Secretary of State Colin Powell and other U.S. administration officials met with Musharraf. On 19 September 2001, Musharraf addressed the people of Pakistan and stated that, while he opposed military tactics against the Taliban, Pakistan risked being endangered by an alliance of India and the U.S. if it did not cooperate. In 2006, Musharraf testified that this stance was pressured by threats from the U.S., and revealed in his memoirs that he had "war-gamed" the United States as an adversary and decided that it would end in a loss for Pakistan.
On 12 January 2002, Musharraf gave a speech against Islamic extremism. He unequivocally condemned all acts of terrorism and pledged to combat Islamic extremism and lawlessness within Pakistan itself. He stated that his government was committed to rooting out extremism and made it clear that the banned militant organizations would not be allowed to resurface under any new name. He said, "the recent decision to ban extremist groups promoting militancy was taken in the national interest after thorough consultations. It was not taken under any foreign influence".
In 2002, the Musharraf-led government took a firm stand against the jihadi organizations and groups promoting extremism, and arrested Maulana Masood Azhar, head of the Jaish-e-Mohammed, and Hafiz Muhammad Saeed, chief of the Lashkar-e-Taiba, and took dozens of activists into custody. An official ban was imposed on the groups on 12 January. Later that year, the Saudi born Zayn al-Abidn Muhammed Hasayn Abu Zubaydah was arrested by Pakistani officials during a series of joint U.S.-Pakistan raids. Zubaydah is said to have been a high-ranking al-Qaeda official with the title of operations chief and in charge of running al-Qaeda training camps. Other prominent al-Qaeda members were arrested in the following two years, namely Ramzi bin al-Shibh, who is known to have been a financial backer of al-Qaeda operations, and Khalid Sheikh Mohammed, who at the time of his capture was the third highest-ranking official in al-Qaeda and had been directly in charge of the planning for the 11 September attacks.
The use of drones by the Central Intelligence Agency in Pakistan to carry out operations associated with the Global War on Terror sparks debate over sovereignty and the laws of war. The U.S. Government uses the CIA rather than the U.S. Air Force for strikes in Pakistan in order to avoid breaching sovereignty through military invasion. The United States was criticized by[according to whom?] a report on drone warfare and aerial sovereignty for abusing the term 'Global War on Terror' to carry out military operations through government agencies without formally declaring war.
In a 'Letter to American People' written by Osama bin Laden in 2002, he stated that one of the reasons he was fighting America is because of its support of India on the Kashmir issue. While on a trip to Delhi in 2002, U.S. Secretary of Defense Donald Rumsfeld suggested that Al-Qaeda was active in Kashmir, though he did not have any hard evidence. An investigation in 2002 unearthed evidence that Al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence. A team of Special Air Service and Delta Force was sent into Indian-administered Kashmir in 2002 to hunt for Osama bin Laden after reports that he was being sheltered by the Kashmiri militant group Harkat-ul-Mujahideen. U.S. officials believed that Al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Fazlur Rehman Khalil, the leader of the Harkat-ul-Mujahideen, signed al-Qaeda's 1998 declaration of holy war, which called on Muslims to attack all Americans and their allies. Indian sources claimed that In 2006, Al-Qaeda claimed they had established a wing in Kashmir; this worried the Indian government. India also claimed that Al-Qaeda has strong ties with the Kashmir militant groups Lashkar-e-Taiba and Jaish-e-Mohammed in Pakistan. While on a visit to Pakistan in January 2010, U.S. Defense secretary Robert Gates stated that Al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan.
In September 2009, a U.S. Drone strike reportedly killed Ilyas Kashmiri, who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with Al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' Al-Qaeda member, while others described him as the head of military operations for Al-Qaeda. Waziristan had now become the new battlefield for Kashmiri militants, who were now fighting NATO in support of Al-Qaeda. On 8 July 2012, Al-Badar Mujahideen, a breakaway faction of Kashmir centric terror group Hizbul Mujahideen, on conclusion of their two-day Shuhada Conference called for mobilisation of resources for continuation of jihad in Kashmir.
In the following months, NATO took a wide range of measures to respond to the threat of terrorism. On 22 November 2002, the member states of the Euro-Atlantic Partnership Council (EAPC) decided on a Partnership Action Plan against Terrorism, which explicitly states, "EAPC States are committed to the protection and promotion of fundamental freedoms and human rights, as well as the rule of law, in combating terrorism." NATO started naval operations in the Mediterranean Sea designed to prevent the movement of terrorists or weapons of mass destruction as well as to enhance the security of shipping in general called Operation Active Endeavour.
Support for the U.S. cooled when America made clear its determination to invade Iraq in late 2002. Even so, many of the "coalition of the willing" countries that unconditionally supported the U.S.-led military action have sent troops to Afghanistan, particular neighboring Pakistan, which has disowned its earlier support for the Taliban and contributed tens of thousands of soldiers to the conflict. Pakistan was also engaged in the War in North-West Pakistan (Waziristan War). Supported by U.S. intelligence, Pakistan was attempting to remove the Taliban insurgency and al-Qaeda element from the northern tribal areas.
The British 16th Air Assault Brigade (later reinforced by Royal Marines) formed the core of the force in southern Afghanistan, along with troops and helicopters from Australia, Canada and the Netherlands. The initial force consisted of roughly 3,300 British, 2,000 Canadian, 1,400 from the Netherlands and 240 from Australia, along with special forces from Denmark and Estonia and small contingents from other nations. The monthly supply of cargo containers through Pakistani route to ISAF in Afghanistan is over 4,000 costing around 12 billion in Pakistani Rupees.
In addition to military efforts abroad, in the aftermath of 9/11 the Bush Administration increased domestic efforts to prevent future attacks. Various government bureaucracies that handled security and military functions were reorganized. A new cabinet-level agency called the United States Department of Homeland Security was created in November 2002 to lead and coordinate the largest reorganization of the U.S. federal government since the consolidation of the armed forces into the Department of Defense.[citation needed]
The USA PATRIOT Act of October 2001 dramatically reduces restrictions on law enforcement agencies' ability to search telephone, e-mail communications, medical, financial, and other records; eases restrictions on foreign intelligence gathering within the United States; expands the Secretary of the Treasury's authority to regulate financial transactions, particularly those involving foreign individuals and entities; and broadens the discretion of law enforcement and immigration authorities in detaining and deporting immigrants suspected of terrorism-related acts. The act also expanded the definition of terrorism to include domestic terrorism, thus enlarging the number of activities to which the USA PATRIOT Act's expanded law enforcement powers could be applied. A new Terrorist Finance Tracking Program monitored the movements of terrorists' financial resources (discontinued after being revealed by The New York Times). Global telecommunication usage, including those with no links to terrorism, is being collected and monitored through the NSA electronic surveillance program. The Patriot Act is still in effect.
Political interest groups have stated that these laws remove important restrictions on governmental authority, and are a dangerous encroachment on civil liberties, possible unconstitutional violations of the Fourth Amendment. On 30 July 2003, the American Civil Liberties Union (ACLU) filed the first legal challenge against Section 215 of the Patriot Act, claiming that it allows the FBI to violate a citizen's First Amendment rights, Fourth Amendment rights, and right to due process, by granting the government the right to search a person's business, bookstore, and library records in a terrorist investigation, without disclosing to the individual that records were being searched. Also, governing bodies in a number of communities have passed symbolic resolutions against the act.
In 2005, the UN Security Council adopted Resolution 1624 concerning incitement to commit acts of terrorism and the obligations of countries to comply with international human rights laws. Although both resolutions require mandatory annual reports on counter-terrorism activities by adopting nations, the United States and Israel have both declined to submit reports. In the same year, the United States Department of Defense and the Chairman of the Joint Chiefs of Staff issued a planning document, by the name "National Military Strategic Plan for the War on Terrorism", which stated that it constituted the "comprehensive military plan to prosecute the Global War on Terror for the Armed Forces of the United States...including the findings and recommendations of the 9/11 Commission and a rigorous examination with the Department of Defense".
Criticism of the War on Terror addresses the issues, morality, efficiency, economics, and other questions surrounding the War on Terror and made against the phrase itself, calling it a misnomer. The notion of a "war" against "terrorism" has proven highly contentious, with critics charging that it has been exploited by participating governments to pursue long-standing policy/military objectives, reduce civil liberties, and infringe upon human rights. It is argued that the term war is not appropriate in this context (as in War on Drugs), since there is no identifiable enemy, and that it is unlikely international terrorism can be brought to an end by military means.
Other critics, such as Francis Fukuyama, note that "terrorism" is not an enemy, but a tactic; calling it a "war on terror", obscures differences between conflicts such as anti-occupation insurgents and international mujahideen. With a military presence in Iraq and Afghanistan and its associated collateral damage Shirley Williams maintains this increases resentment and terrorist threats against the West. There is also perceived U.S. hypocrisy, media-induced hysteria, and that differences in foreign and security policy have damaged America's image in most of the world.
The Times is a British daily national newspaper based in London. It began in 1785 under the title The Daily Universal Register and became The Times on 1 January 1788. The Times and its sister paper The Sunday Times (founded in 1821) are published by Times Newspapers, since 1981 a subsidiary of News UK, itself wholly owned by the News Corp group headed by Rupert Murdoch. The Times and The Sunday Times do not share editorial staff, were founded independently and have only had common ownership since 1967.
The Times is the first newspaper to have borne that name, lending it to numerous other papers around the world, including The Times of India (founded in 1838), The Straits Times (Singapore) (1845), The New York Times (1851), The Irish Times (1859), Le Temps (France) (1861-1942), the Cape Times (South Africa) (1872), the Los Angeles Times (1881), The Seattle Times (1891), The Manila Times (1898), The Daily Times (Malawi) (1900), El Tiempo (Colombia) (1911), The Canberra Times (1926), and The Times (Malta) (1935). In these countries, the newspaper is often referred to as The London Times or The Times of London.
The Times is the originator of the widely used Times Roman typeface, originally developed by Stanley Morison of The Times in collaboration with the Monotype Corporation for its legibility in low-tech printing. In November 2006 The Times began printing headlines in a new font, Times Modern. The Times was printed in broadsheet format for 219 years, but switched to compact size in 2004 in an attempt to appeal more to younger readers and commuters using public transport. The Sunday Times remains a broadsheet.
Though traditionally a moderate newspaper and sometimes a supporter of the Conservative Party, it supported the Labour Party in the 2001 and 2005 general elections. In 2004, according to MORI, the voting intentions of its readership were 40% for the Conservative Party, 29% for the Liberal Democrats, and 26% for Labour. The Times had an average daily circulation of 394,448 in March 2014; in the same period, The Sunday Times had an average daily circulation of 839,077. An American edition of The Times has been published since 6 June 2006. It has been heavily used by scholars and researchers because of its widespread availability in libraries and its detailed index. A complete historical file of the digitized paper is online from Gage Cengage publisher.
The Times was founded by publisher John Walter on 1 January 1785 as The Daily Universal Register, with Walter in the role of editor. Walter had lost his job by the end of 1784 after the insurance company where he was working went bankrupt because of the complaints of a Jamaican hurricane. Being unemployed, Walter decided to set a new business up. It was in that time when Henry Johnson invented the logography, a new typography that was faster and more precise (three years later, it was proved that it was not as efficient as had been said). Walter bought the logography's patent and to use it, he decided to open a printing house, where he would daily produce an advertising sheet. The first publication of the newspaper The Daily Universal Register in Great Britain was 1 January 1785. Unhappy because people always omitted the word Universal, Ellias changed the title after 940 editions on 1 January 1788 to The Times. In 1803, Walter handed ownership and editorship to his son of the same name. Walter Sr had spent sixteen months in Newgate Prison for libel printed in The Times, but his pioneering efforts to obtain Continental news, especially from France, helped build the paper's reputation among policy makers and financiers.
The Times used contributions from significant figures in the fields of politics, science, literature, and the arts to build its reputation. For much of its early life, the profits of The Times were very large and the competition minimal, so it could pay far better than its rivals for information or writers. Beginning in 1814, the paper was printed on the new steam-driven cylinder press developed by Friedrich Koenig. In 1815, The Times had a circulation of 5,000.
Thomas Barnes was appointed general editor in 1817. In the same year, the paper's printer James Lawson, died and passed the business onto his son John Joseph Lawson(1802–1852). Under the editorship of Barnes and his successor in 1841, John Thadeus Delane, the influence of The Times rose to great heights, especially in politics and amongst the City of London. Peter Fraser and Edward Sterling were two noted journalists, and gained for The Times the pompous/satirical nickname 'The Thunderer' (from "We thundered out the other day an article on social and political reform."). The increased circulation and influence of the paper was based in part to its early adoption of the steam-driven rotary printing press. Distribution via steam trains to rapidly growing concentrations of urban populations helped ensure the profitability of the paper and its growing influence.
The Times was the first newspaper to send war correspondents to cover particular conflicts. W. H. Russell, the paper's correspondent with the army in the Crimean War, was immensely influential with his dispatches back to England.
In other events of the nineteenth century, The Times opposed the repeal of the Corn Laws until the number of demonstrations convinced the editorial board otherwise, and only reluctantly supported aid to victims of the Irish Potato Famine. It enthusiastically supported the Great Reform Bill of 1832, which reduced corruption and increased the electorate from 400,000 people to 800,000 people (still a small minority of the population). During the American Civil War, The Times represented the view of the wealthy classes, favouring the secessionists, but it was not a supporter of slavery.
The third John Walter, the founder's grandson, succeeded his father in 1847. The paper continued as more or less independent, but from the 1850s The Times was beginning to suffer from the rise in competition from the penny press, notably The Daily Telegraph and The Morning Post.
During the 19th century, it was not infrequent for the Foreign Office to approach The Times and ask for continental intelligence, which was often superior to that conveyed by official sources.[citation needed]
The Times faced financial extinction in 1890 under Arthur Fraser Walter, but it was rescued by an energetic editor, Charles Frederic Moberly Bell. During his tenure (1890–1911), The Times became associated with selling the Encyclopædia Britannica using aggressive American marketing methods introduced by Horace Everett Hooper and his advertising executive, Henry Haxton. Due to legal fights between the Britannica's two owners, Hooper and Walter Montgomery Jackson, The Times severed its connection in 1908 and was bought by pioneering newspaper magnate, Alfred Harmsworth, later Lord Northcliffe.
In editorials published on 29 and 31 July 1914, Wickham Steed, the Times's Chief Editor, argued that the British Empire should enter World War I. On 8 May 1920, also under the editorship of Steed, The Times in an editorial endorsed the anti-Semitic fabrication The Protocols of the Learned Elders of Zion as a genuine document, and called Jews the world's greatest danger. In the leader entitled "The Jewish Peril, a Disturbing Pamphlet: Call for Inquiry", Steed wrote about The Protocols of the Elders of Zion:
The following year, when Philip Graves, the Constantinople (modern Istanbul) correspondent of The Times, exposed The Protocols as a forgery, The Times retracted the editorial of the previous year.
In 1922, John Jacob Astor, son of the 1st Viscount Astor, bought The Times from the Northcliffe estate. The paper gained a measure of notoriety in the 1930s with its advocacy of German appeasement; then-editor Geoffrey Dawson was closely allied with those in the government who practised appeasement, most notably Neville Chamberlain.
Kim Philby, a Soviet double agent, was a correspondent for the newspaper in Spain during the Spanish Civil War of the late 1930s. Philby was admired for his courage in obtaining high-quality reporting from the front lines of the bloody conflict. He later joined MI6 during World War II, was promoted into senior positions after the war ended, then eventually defected to the Soviet Union in 1963.
Between 1941 and 1946, the left-wing British historian E.H. Carr was Assistant Editor. Carr was well known for the strongly pro-Soviet tone of his editorials. In December 1944, when fighting broke out in Athens between the Greek Communist ELAS and the British Army, Carr in a Times editorial sided with the Communists, leading Winston Churchill to condemn him and that leader in a speech to the House of Commons. As a result of Carr's editorial, The Times became popularly known during that stage of World War II as the threepenny Daily Worker (the price of the Daily Worker being one penny).
On 3 May 1966 it resumed printing news on the front page - previously the front page featured small advertisements, usually of interest to the moneyed classes in British society. In 1967, members of the Astor family sold the paper to Canadian publishing magnate Roy Thomson. His Thomson Corporation brought it under the same ownership as The Sunday Times to form Times Newspapers Limited.
The Thomson Corporation management were struggling to run the business due to the 1979 Energy Crisis and union demands. Management were left with no choice but to find a buyer who was in a position to guarantee the survival of both titles, and also one who had the resources and was committed to funding the introduction of modern printing methods.
Several suitors appeared, including Robert Maxwell, Tiny Rowland and Lord Rothermere; however, only one buyer was in a position to meet the full Thomson remit, Australian media magnate Rupert Murdoch. Robert Holmes à Court, another Australian magnate had previously tried to buy The Times in 1980.
In 1981, The Times and The Sunday Times were bought from Thomson by Rupert Murdoch's News International. The acquisition followed three weeks of intensive bargaining with the unions by company negotiators, John Collier and Bill O'Neill.
After 14 years as editor, William Rees-Mogg resigned the post upon completion of the change of ownership. Murdoch began to make his mark on the paper by appointing Harold Evans as his replacement. One of his most important changes was the introduction of new technology and efficiency measures. In March–May 1982, following agreement with print unions, the hot-metal Linotype printing process used to print The Times since the 19th century was phased out and replaced by computer input and photo-composition. This allowed print room staff at The Times and The Sunday Times to be reduced by half. However, direct input of text by journalists ("single stroke" input) was still not achieved, and this was to remain an interim measure until the Wapping dispute of 1986, when The Times moved from New Printing House Square in Gray's Inn Road (near Fleet Street) to new offices in Wapping.
Robert Fisk, seven times British International Journalist of the Year, resigned as foreign correspondent in 1988 over what he saw as "political censorship" of his article on the shooting-down of Iran Air Flight 655 in July 1988. He wrote in detail about his reasons for resigning from the paper due to meddling with his stories, and the paper's pro-Israel stance.
In June 1990, The Times ceased its policy of using courtesy titles ("Mr", "Mrs", or "Miss" prefixes) for living persons before full names on first reference, but it continues to use them before surnames on subsequent references. The more formal style is now confined to the "Court and Social" page, though "Ms" is now acceptable in that section, as well as before surnames in news sections.
In November 2003, News International began producing the newspaper in both broadsheet and tabloid sizes. On 13 September 2004, the weekday broadsheet was withdrawn from sale in Northern Ireland. Since 1 November 2004, the paper has been printed solely in tabloid format.
On 6 June 2005, The Times redesigned its Letters page, dropping the practice of printing correspondents' full postal addresses. Published letters were long regarded as one of the paper's key constituents. Author/solicitor David Green of Castle Morris Pembrokeshire has had more letters published on the main letters page than any other known contributor – 158 by 31 January 2008. According to its leading article, "From Our Own Correspondents", removal of full postal addresses was in order to fit more letters onto the page.
In a 2007 meeting with the House of Lords Select Committee on Communications, which was investigating media ownership and the news, Murdoch stated that the law and the independent board prevented him from exercising editorial control.
In May 2008 printing of The Times switched from Wapping to new plants at Broxbourne on the outskirts of London, and Merseyside and Glasgow, enabling the paper to be produced with full colour on every page for the first time.
On 26 July 2012, to coincide with the official start of the London 2012 Olympics and the issuing of a series of souvenir front covers, The Times added the suffix "of London" to its masthead.
The Times features news for the first half of the paper, the Opinion/Comment section begins after the first news section with world news normally following this. The business pages begin on the centre spread, and are followed by The Register, containing obituaries, Court & Social section, and related material. The sport section is at the end of the main paper. The Times current prices are £1.20 for the daily edition and £1.50 for the Saturday edition.
The Times's main supplement, every day, is the times2, featuring various lifestyle columns. It was discontinued on 1 March 2010 but reintroduced on 11 October 2010 after negative feedback. Its regular features include a puzzles section called Mind Games. Its previous incarnation began on 5 September 2005, before which it was called T2 and previously Times 2. Regular features include columns by a different columnist each weekday. There was a column by Marcus du Sautoy each Wednesday, for example. The back pages are devoted to puzzles and contain sudoku, "Killer Sudoku", "KenKen", word polygon puzzles, and a crossword simpler and more concise than the main "Times Crossword".
The Game is included in the newspaper on Mondays, and details all the weekend's football activity (Premier League and Football League Championship, League One and League Two.) The Scottish edition of The Game also includes results and analysis from Scottish Premier League games.
The Saturday edition of The Times contains a variety of supplements. These supplements were relaunched in January 2009 as: Sport, Weekend (including travel and lifestyle features), Saturday Review (arts, books, and ideas), The Times Magazine (columns on various topics), and Playlist (an entertainment listings guide).
The Times Magazine features columns touching on various subjects such as celebrities, fashion and beauty, food and drink, homes and gardens or simply writers' anecdotes. Notable contributors include Giles Coren, Food and Drink Writer of the Year in 2005 and Nadiya Hussain, winner of BBC's The Great British Bake Off.
The Times and The Sunday Times have had an online presence since March 1999, originally at the-times.co.uk and sunday-times.co.uk, and later at timesonline.co.uk. There are now two websites: thetimes.co.uk is aimed at daily readers, and the thesundaytimes.co.uk site at providing weekly magazine-like content. There are also iPad and Android editions of both newspapers. Since July 2010, News UK has required readers who do not subscribe to the print edition to pay £2 per week to read The Times and The Sunday Times online.
The Times Digital Archive (1785–2008) is freely accessible via Gale databases to readers affiliated with subscribing academic, public, and school libraries.
Visits to the websites have decreased by 87% since the paywall was introduced, from 21 million unique users per month to 2.7 million. In April 2009, the timesonline site had a readership of 750,000 readers per day. As of October 2011, there were around 111,000 subscribers to The Times' digital products.
At the time of Harold Evans' appointment as editor in 1981, The Times had an average daily sale of 282,000 copies in comparison to the 1.4 million daily sales of its traditional rival The Daily Telegraph. By November 2005 The Times sold an average of 691,283 copies per day, the second-highest of any British "quality" newspaper (after The Daily Telegraph, which had a circulation of 903,405 copies in the period), and the highest in terms of full-rate sales. By March 2014, average daily circulation of The Times had fallen to 394,448 copies, compared to The Daily Telegraph's 523,048, with the two retaining respectively the second-highest and highest circulations among British "quality" newspapers. In contrast The Sun, the highest-selling "tabloid" daily newspaper in the United Kingdom, sold an average of 2,069,809 copies in March 2014, and the Daily Mail, the highest-selling "middle market" British daily newspaper, sold an average of 1,708,006 copies in the period.
The Sunday Times has a significantly higher circulation than The Times, and sometimes outsells The Sunday Telegraph. As of January 2013, The Times has a circulation of 399,339 and The Sunday Times of 885,612.
In a 2009 national readership survey The Times was found to have the highest number of ABC1 25–44 readers and the largest numbers of readers in London of any of the "quality" papers.
The Times commissioned the serif typeface Times New Roman, created by Victor Lardent at the English branch of Monotype, in 1931. It was commissioned after Stanley Morison had written an article criticizing The Times for being badly printed and typographically antiquated. The font was supervised by Morison and drawn by Victor Lardent, an artist from the advertising department of The Times. Morison used an older font named Plantin as the basis for his design, but made revisions for legibility and economy of space. Times New Roman made its debut in the issue of 3 October 1932. After one year, the design was released for commercial sale. The Times stayed with Times New Roman for 40 years, but new production techniques and the format change from broadsheet to tabloid in 2004 have caused the newspaper to switch font five times since 1972. However, all the new fonts have been variants of the original New Roman font:
Historically, the paper was not overtly pro-Tory or Whig, but has been a long time bastion of the English Establishment and empire. The Times adopted a stance described as "peculiarly detached" at the 1945 general election; although it was increasingly critical of the Conservative Party's campaign, it did not advocate a vote for any one party. However, the newspaper reverted to the Tories for the next election five years later. It supported the Conservatives for the subsequent three elections, followed by support for both the Conservatives and the Liberal Party for the next five elections, expressly supporting a Con-Lib coalition in 1974. The paper then backed the Conservatives solidly until 1997, when it declined to make any party endorsement but supported individual (primarily Eurosceptic) candidates.
For the 2001 general election The Times declared its support for Tony Blair's Labour government, which was re-elected by a landslide. It supported Labour again in 2005, when Labour achieved a third successive win, though with a reduced majority. For the 2010 general election, however, the newspaper declared its support for the Tories once again; the election ended in the Tories taking the most votes and seats but having to form a coalition with the Liberal Democrats in order to form a government as they had failed to gain an overall majority.
This makes it the most varied newspaper in terms of political support in British history. Some columnists in The Times are connected to the Conservative Party such as Daniel Finkelstein, Tim Montgomerie, Matthew Parris and Matt Ridley, but there are also columnists connected to the Labour Party such as David Aaronovitch, Phil Collins, Oliver Kamm and Jenni Russell.
The Times occasionally makes endorsements for foreign elections. In November 2012, it endorsed a second term for Barack Obama although it also expressed reservations about his foreign policy.
The Times, along with the British Film Institute, sponsors the "The Times" bfi London Film Festival. It also sponsors the Cheltenham Literature Festival and the Asia House Festival of Asian Literature at Asia House, London.
The Times Literary Supplement (TLS) first appeared in 1902 as a supplement to The Times, becoming a separately paid-for weekly literature and society magazine in 1914. The Times and the TLS have continued to be co-owned, and as of 2012 the TLS is also published by News International and cooperates closely with The Times, with its online version hosted on The Times website, and its editorial offices based in Times House, Pennington Street, London.
Times Atlases have been produced since 1895. They are currently produced by the Collins Bartholomew imprint of HarperCollins Publishers. The flagship product is The Times Comprehensive Atlas of the World.
This 164-page monthly magazine is sold separately from the newspaper of record and is Britain's best-selling travel magazine. The first issue of The Sunday Times Travel Magazine was in 2003, and it includes news, features and insider guides.
In the dystopian future world of George Orwell's Nineteen Eighty-Four, The Times has been transformed into the organ of the totalitarian ruling party, its editorials—of which several are quoted in the book—reflecting Big Brother's pronouncements.
Rex Stout's fictional detective Nero Wolfe is described as fond of solving the London Times' crossword puzzle at his New York home, in preference to those of American papers.
In the James Bond series by Ian Fleming, James Bond, reads The Times. As described by Fleming in From Russia, with Love: "The Times was the only paper that Bond ever read."
In The Wombles, Uncle Bulgaria read The Times and asked for the other Wombles to bring him any copies that they found amongst the litter. The newspaper played a central role in the episode Very Behind the Times (Series 2, Episode 12).