Germany is a federal republic consisting of sixteen federal states (German: Bundesland, or Land).[a] Since today's Germany was formed from an earlier collection of several states, it has a federal constitution, and the constituent states retain a measure of sovereignty. With an emphasis on geographical conditions, Berlin and Hamburg are frequently called Stadtstaaten (city-states), as is the Free Hanseatic City of Bremen, which in fact includes the cities of Bremen and Bremerhaven. The remaining 13 states are called Flächenländer (literally: area states).
The creation of the Federal Republic of Germany in 1949 was through the unification of the western states (which were previously under American, British, and French administration) created in the aftermath of World War II. Initially, in 1949, the states of the Federal Republic were Baden, Bavaria (in German: Bayern), Bremen, Hamburg, Hesse (Hessen), Lower Saxony (Niedersachsen), North Rhine Westphalia (Nordrhein-Westfalen), Rhineland-Palatinate (Rheinland-Pfalz), Schleswig-Holstein, Württemberg-Baden, and Württemberg-Hohenzollern. West Berlin, while officially not part of the Federal Republic, was largely integrated and considered as a de facto state.
In 1952, following a referendum, Baden, Württemberg-Baden, and Württemberg-Hohenzollern merged into Baden-Württemberg. In 1957, the Saar Protectorate rejoined the Federal Republic as the Saarland. German reunification in 1990, in which the German Democratic Republic (East Germany) ascended into the Federal Republic, resulted in the addition of the re-established eastern states of Brandenburg, Mecklenburg-West Pomerania (in German Mecklenburg-Vorpommern), Saxony (Sachsen), Saxony-Anhalt (Sachsen-Anhalt), and Thuringia (Thüringen), as well as the reunification of West and East Berlin into Berlin and its establishment as a full and equal state. A regional referendum in 1996 to merge Berlin with surrounding Brandenburg as "Berlin-Brandenburg" failed to reach the necessary majority vote in Brandenburg, while a majority of Berliners voted in favour of the merger.
Federalism is one of the entrenched constitutional principles of Germany. According to the German constitution (called Grundgesetz or in English Basic Law), some topics, such as foreign affairs and defense, are the exclusive responsibility of the federation (i.e., the federal level), while others fall under the shared authority of the states and the federation; the states retain residual legislative authority for all other areas, including "culture", which in Germany includes not only topics such as financial promotion of arts and sciences, but also most forms of education and job training. Though international relations including international treaties are primarily the responsibility of the federal level, the constituent states have certain limited powers in this area: in matters that affect them directly, the states defend their interests at the federal level through the Bundesrat (literally Federal Council, the upper house of the German Federal Parliament) and in areas where they have legislative authority they have limited powers to conclude international treaties "with the consent of the federal government".
The use of the term Länder (Lands) dates back to the Weimar Constitution of 1919. Before this time, the constituent states of the German Empire were called Staaten (States). Today, it is very common to use the term Bundesland (Federal Land). However, this term is not used officially, neither by the constitution of 1919 nor by the Basic Law (Constitution) of 1949. Three Länder call themselves Freistaaten (Free States, which is the old-fashioned German expression for Republic), Bavaria (since 1919), Saxony (originally since 1919 and again since 1990), and Thuringia (since 1994). There is little continuity between the current states and their predecessors of the Weimar Republic with the exception of the three free states, and the two city-states of Hamburg and Bremen.
A new delimitation of the federal territory keeps being debated in Germany, though "Some scholars note that there are significant differences among the American states and regional governments in other federations without serious calls for territorial changes ...", as political scientist Arthur B. Gunlicks remarks. He summarizes the main arguments for boundary reform in Germany: "... the German system of dual federalism requires strong Länder that have the administrative and fiscal capacity to implement legislation and pay for it from own source revenues. Too many Länder also make coordination among them and with the federation more complicated ...". But several proposals have failed so far; territorial reform remains a controversial topic in German politics and public perception.
Federalism has a long tradition in German history. The Holy Roman Empire comprised many petty states numbering more than 300 around 1796. The number of territories was greatly reduced during the Napoleonic Wars (1796–1814). After the Congress of Vienna (1815), 39 states formed the German Confederation. The Confederation was dissolved after the Austro-Prussian War and replaced by a North German Federation under Prussian hegemony; this war left Prussia dominant in Germany, and German nationalism would compel the remaining independent states to ally with Prussia in the Franco-Prussian War of 1870–71, and then to accede to the crowning of King Wilhelm of Prussia as German Emperor. The new German Empire included 25 states (three of them, Hanseatic cities) and the imperial territory of Alsace-Lorraine. The empire was dominated by Prussia, which controlled 65% of the territory and 62% of the population. After the territorial losses of the Treaty of Versailles, the remaining states continued as republics of a new German federation. These states were gradually de facto abolished and reduced to provinces under the Nazi regime via the Gleichschaltung process, as the states administratively were largely superseded by the Nazi Gau system.
During the Allied occupation of Germany after World War II, internal borders were redrawn by the Allied military governments. No single state comprised more than 30% of either population or territory; this was intended to prevent any one state from being as dominant within Germany as Prussia had been in the past. Initially, only seven of the pre-War states remained: Baden (in part), Bavaria (reduced in size), Bremen, Hamburg, Hesse (enlarged), Saxony, and Thuringia. The states with hyphenated names, such as Rhineland-Palatinate, North Rhine-Westphalia, and Saxony-Anhalt, owed their existence to the occupation powers and were created out of mergers of former Prussian provinces and smaller states. Former German territory that lie east of the Oder-Neisse Line fell under either Polish or Soviet administration but attempts were made at least symbolically not to abandon sovereignty well into the 1960s. However, no attempts were made to establish new states in these territories as they lay outside the jurisdiction of West Germany at that time.
Upon its founding in 1949, West Germany had eleven states. These were reduced to nine in 1952 when three south-western states (South Baden, Württemberg-Hohenzollern, and Württemberg-Baden) merged to form Baden-Württemberg. From 1957, when the French-occupied Saar Protectorate was returned and formed into the Saarland, the Federal Republic consisted of ten states, which are referred to as the "Old States" today. West Berlin was under the sovereignty of the Western Allies and neither a Western German state nor part of one. However, it was in many ways de facto integrated with West Germany under a special status.
Later, the constitution was amended to state that the citizens of the 16 states had successfully achieved the unity of Germany in free self-determination and that the Basic Law thus applied to the entire German people. Article 23, which had allowed "any other parts of Germany" to join, was rephrased. It had been used in 1957 to reintegrate the Saar Protectorate as the Saarland into the Federal Republic, and this was used as a model for German reunification in 1990. The amended article now defines the participation of the Federal Council and the 16 German states in matters concerning the European Union.
A new delimitation of the federal territory has been discussed since the Federal Republic was founded in 1949 and even before. Committees and expert commissions advocated a reduction of the number of states; academics (Rutz, Miegel, Ottnad etc.) and politicians (Döring, Apel, and others) made proposals –  some of them far-reaching –  for redrawing boundaries but hardly anything came of these public discussions. Territorial reform is sometimes propagated by the richer states as a means to avoid or reduce fiscal transfers.
The debate on a new delimitation of the German territory started in 1919 as part of discussions about the new constitution. Hugo Preuss, the father of the Weimar Constitution, drafted a plan to divide the German Reich into 14 roughly equal-sized states. His proposal was turned down due to opposition of the states and concerns of the government. Article 18 of the constitution enabled a new delimitation of the German territory but set high hurdles: Three fifth of the votes handed in, and at least the majority of the population are necessary to decide on the alteration of territory. In fact, until 1933 there were only four changes in the configuration of the German states: The 7 Thuringian states were merged in 1920, whereby Coburg opted for Bavaria, Pyrmont joined Prussia in 1922, and Waldeck did so in 1929. Any later plans to break up the dominating Prussia into smaller states failed because political circumstances were not favorable to state reforms.
After the Nazi Party seized power in January 1933, the Länder increasingly lost importance. They became administrative regions of a centralised country. Three changes are of particular note: on January 1, 1934, Mecklenburg-Schwerin was united with the neighbouring Mecklenburg-Strelitz; and, by the Greater Hamburg Act (Groß-Hamburg-Gesetz), from April 1, 1937, the area of the city-state was extended, while Lübeck lost its independence and became part of the Prussian province of Schleswig-Holstein.
As the premiers did not come to an agreement on this question, the Parliamentary Council was supposed to address this issue. Its provisions are reflected in Article 29. There was a binding provision for a new delimitation of the federal territory: the Federal Territory must be revised ... (paragraph 1). Moreover, in territories or parts of territories whose affiliation with a Land had changed after 8 May 1945 without a referendum, people were allowed to petition for a revision of the current status within a year after the promulgation of the Basic Law (paragraph 2). If at least one tenth of those entitled to vote in Bundestag elections were in favour of a revision, the federal government had to include the proposal into its legislation. Then a referendum was required in each territory or part of a territory whose affiliation was to be changed (paragraph 3). The proposal should not take effect if within any of the affected territories a majority rejected the change. In this case, the bill had to be introduced again and after passing had to be confirmed by referendum in the Federal Republic as a whole (paragraph 4). The reorganization should be completed within three years after the Basic Law had come into force (paragraph 6).
In the Paris Agreements of 23 October 1954, France offered to establish an independent "Saarland", under the auspices of the Western European Union (WEU), but on 23 October 1955 in the Saar Statute referendum the Saar electorate rejected this plan by 67.7% to 32.3% (out of a 96.5% turnout: 423,434 against, 201,975 for) despite the public support of Federal German Chancellor Konrad Adenauer for the plan. The rejection of the plan by the Saarlanders was interpreted as support for the Saar to join the Federal Republic of Germany.
Paragraph 6 of Article 29 stated that if a petition was successful a referendum should be held within three years. Since the deadline passed on 5 May 1958 without anything happening the Hesse state government filed a constitutional complaint with the Federal Constitutional Court in October 1958. The complaint was dismissed in July 1961 on the grounds that Article 29 had made the new delimitation of the federal territory an exclusively federal matter. At the same time, the Court reaffirmed the requirement for a territorial revision as a binding order to the relevant constitutional bodies.
In his investiture address, given on 28 October 1969 in Bonn, Chancellor Willy Brandt proposed that the government would consider Article 29 of the Basic Law as a binding order. An expert commission was established, named after its chairman, the former Secretary of State Professor Werner Ernst. After two years of work, the experts delivered their report in 1973. It provided an alternative proposal for both northern Germany and central and southwestern Germany. In the north, either a single new state consisting of Schleswig-Holstein, Hamburg, Bremen and Lower Saxony should be created (solution A) or two new states, one in the northeast consisting of Schleswig-Holstein, Hamburg and the northern part of Lower Saxony (from Cuxhaven to Lüchow-Dannenberg) and one in the northwest consisting of Bremen and the rest of Lower Saxony (solution B). In the Center and South West either Rhineland-Palatinate (with the exception of the Germersheim district but including the Rhine-Neckar region) should be merged with Hesse and the Saarland (solution C), the district of Germersheim would then become part of Baden-Württemberg.
The Basic Law of the Federal Republic of Germany, the federal constitution, stipulates that the structure of each Federal State's government must "conform to the principles of republican, democratic, and social government, based on the rule of law" (Article 28). Most of the states are governed by a cabinet led by a Ministerpräsident (Minister-President), together with a unicameral legislative body known as the Landtag (State Diet). The states are parliamentary republics and the relationship between their legislative and executive branches mirrors that of the federal system: the legislatures are popularly elected for four or five years (depending on the state), and the Minister-President is then chosen by a majority vote among the Landtag's members. The Minister-President appoints a cabinet to run the state's agencies and to carry out the executive duties of the state's government.
The governments in Berlin, Bremen and Hamburg are designated by the term Senate. In the three free states of Bavaria, Saxony, and Thuringia the government is referred to as the State Government (Staatsregierung), and in the other ten states the term Land Government (Landesregierung) is used. Before January 1, 2000, Bavaria had a bicameral parliament, with a popularly elected Landtag, and a Senate made up of representatives of the state's major social and economic groups. The Senate was abolished following a referendum in 1998. The states of Berlin, Bremen, and Hamburg are governed slightly differently from the other states. In each of those cities, the executive branch consists of a Senate of approximately eight, selected by the state's parliament; the senators carry out duties equivalent to those of the ministers in the larger states. The equivalent of the Minister-President is the Senatspräsident (President of the Senate) in Bremen, the Erster Bürgermeister (First Mayor) in Hamburg, and the Regierender Bürgermeister (Governing Mayor) in Berlin. The parliament for Berlin is called the Abgeordnetenhaus (House of Representatives), while Bremen and Hamburg both have a Bürgerschaft. The parliaments in the remaining 13 states are referred to as Landtag (State Parliament).
The Districts of Germany (Kreise) are administrative districts, and every state except the city-states of Berlin, Hamburg, and Bremen consists of "rural districts" (Landkreise), District-free Towns/Cities (Kreisfreie Städte, in Baden-Württemberg also called "urban districts", or Stadtkreise), cities that are districts in their own right, or local associations of a special kind (Kommunalverbände besonderer Art), see below. The state Free Hanseatic City of Bremen consists of two urban districts, while Berlin and Hamburg are states and urban districts at the same time.
Local associations of a special kind are an amalgamation of one or more Landkreise with one or more Kreisfreie Städte to form a replacement of the aforementioned administrative entities at the district level. They are intended to implement simplification of administration at that level. Typically, a district-free city or town and its urban hinterland are grouped into such an association, or Kommunalverband besonderer Art. Such an organization requires the issuing of special laws by the governing state, since they are not covered by the normal administrative structure of the respective states.
Municipalities (Gemeinden): Every rural district and every Amt is subdivided into municipalities, while every urban district is a municipality in its own right. There are (as of 6 March 2009[update]) 12,141 municipalities, which are the smallest administrative units in Germany. Cities and towns are municipalities as well, also having city rights or town rights (Stadtrechte). Nowadays, this is mostly just the right to be called a city or town. However, in former times there were many other privileges, including the right to impose local taxes or to allow industry only within city limits.
The municipalities have two major policy responsibilities. First, they administer programs authorized by the federal or state government. Such programs typically relate to youth, schools, public health, and social assistance. Second, Article 28(2) of the Basic Law guarantees the municipalities "the right to regulate on their own responsibility all the affairs of the local community within the limits set by law." Under this broad statement of competence, local governments can justify a wide range of activities. For instance, many municipalities develop and expand the economic infrastructure of their communities through the development of industrial trading estates.
The stated clauses of the Nazi-Soviet non-aggression pact were a guarantee of non-belligerence by each party towards the other, and a written commitment that neither party would ally itself to, or aid, an enemy of the other party. In addition to stipulations of non-aggression, the treaty included a secret protocol that divided territories of Romania, Poland, Lithuania, Latvia, Estonia, and Finland into German and Soviet "spheres of influence", anticipating potential "territorial and political rearrangements" of these countries. Thereafter, Germany invaded Poland on 1 September 1939. After the Soviet–Japanese ceasefire agreement took effect on 16 September, Stalin ordered his own invasion of Poland on 17 September. Part of southeastern (Karelia) and Salla region in Finland were annexed by the Soviet Union after the Winter War. This was followed by Soviet annexations of Estonia, Latvia, Lithuania, and parts of Romania (Bessarabia, Northern Bukovina, and the Hertza region). Concern about ethnic Ukrainians and Belarusians had been proffered as justification for the Soviet invasion of Poland. Stalin's invasion of Bukovina in 1940 violated the pact, as it went beyond the Soviet sphere of influence agreed with the Axis.
Of the territories of Poland annexed by the Soviet Union between 1939 and 1940, the region around Białystok and a minor part of Galicia east of the San river around Przemyśl were returned to the Polish state at the end of World War II. Of all other territories annexed by the USSR in 1939–40, the ones detached from Finland (Karelia, Petsamo), Estonia (Ingrian area and Petseri County) and Latvia (Abrene) remained part of the Russian Federation, the successor state of the Soviet Union, after 1991. Northern Bukovina, Southern Bessarabia and Hertza remain part of Ukraine.
The outcome of the First World War was disastrous for both the German Reich and the Russian Soviet Federative Socialist Republic. During the war, the Bolsheviks struggled for survival, and Vladimir Lenin recognised the independence of Finland, Estonia, Latvia, Lithuania and Poland. Moreover, facing a German military advance, Lenin and Trotsky were forced to enter into the Treaty of Brest-Litovsk, which ceded massive western Russian territories to the German Empire. After Germany's collapse, a multinational Allied-led army intervened in the Russian Civil War (1917–22).
At the beginning of the 1930s, the Nazi Party's rise to power increased tensions between Germany and the Soviet Union along with other countries with ethnic Slavs, who were considered "Untermenschen" (inferior) according to Nazi racial ideology. Moreover, the anti-Semitic Nazis associated ethnic Jews with both communism and financial capitalism, both of which they opposed. Consequently, Nazi theory held that Slavs in the Soviet Union were being ruled by "Jewish Bolshevik" masters. In 1934, Hitler himself had spoken of an inescapable battle against both Pan-Slavism and Neo-Slavism, the victory in which would lead to "permanent mastery of the world", though he stated that they would "walk part of the road with the Russians, if that will help us." The resulting manifestation of German anti-Bolshevism and an increase in Soviet foreign debts caused German–Soviet trade to dramatically decline.[b] Imports of Soviet goods to Germany fell to 223 million Reichsmarks in 1934 as the more isolationist Stalinist regime asserted power and the abandonment of post–World War I Treaty of Versailles military controls decreased Germany's reliance on Soviet imports.[clarification needed]
Hitler's fierce anti-Soviet rhetoric was one of the reasons why the UK and France decided that Soviet participation in the 1938 Munich Conference regarding Czechoslovakia would be both dangerous and useless. The Munich Agreement that followed marked a partial German annexation of Czechoslovakia in late 1938 followed by its complete dissolution in March 1939, which as part of the appeasement of Germany conducted by Chamberlain's and Daladier's cabinets. This policy immediately raised the question of whether the Soviet Union could avoid being next on Hitler's list. The Soviet leadership believed that the West wanted to encourage German aggression in the East and that France and Britain might stay neutral in a war initiated by Germany, hoping that the warring states would wear each other out and put an end to both the Soviet Union and Nazi Germany.
For Germany, because an autarkic economic approach or an alliance with Britain were impossible, closer relations with the Soviet Union to obtain raw materials became necessary, if not just for economic reasons alone. Moreover, an expected British blockade in the event of war would create massive shortages for Germany in a number of key raw materials. After the Munich agreement, the resulting increase in German military supply needs and Soviet demands for military machinery, talks between the two countries occurred from late 1938 to March 1939. The third Soviet Five Year Plan required new infusions of technology and industrial equipment.[clarification needed] German war planners had estimated serious shortfalls of raw materials if Germany entered a war without Soviet supply.
The Soviet Union, which feared Western powers and the possibility of "capitalist encirclements", had little faith either that war could be avoided, or faith in the Polish army, and wanted nothing less than an ironclad military alliance with France and Britain that would provide a guaranteed support for a two-pronged attack on Germany; thus, Stalin's adherence to the collective security line was purely conditional. Britain and France believed that war could still be avoided, and that the Soviet Union, weakened by the Great Purge, could not be a main military participant, a point that many military sources were at variance with, especially Soviet victories over the Japanese Kwantung army on the Manchurian frontier. France was more anxious to find an agreement with the USSR than was Britain; as a continental power, it was more willing to make concessions, more fearful of the dangers of an agreement between the USSR and Germany. These contrasting attitudes partly explain why the USSR has often been charged with playing a double game in 1939: carrying on open negotiations for an alliance with Britain and France while secretly considering propositions from Germany.
By the end of May, drafts were formally presented. In mid-June, the main Tripartite negotiations started. The discussion was focused on potential guarantees to central and east European countries should a German aggression arise. The USSR proposed to consider that a political turn towards Germany by the Baltic states would constitute an "indirect aggression" towards the Soviet Union. Britain opposed such proposals, because they feared the Soviets' proposed language could justify a Soviet intervention in Finland and the Baltic states, or push those countries to seek closer relations with Germany. The discussion about a definition of "indirect aggression" became one of the sticking points between the parties, and by mid-July, the tripartite political negotiations effectively stalled, while the parties agreed to start negotiations on a military agreement, which the Soviets insisted must be entered into simultaneously with any political agreement.
From April–July, Soviet and German officials made statements regarding the potential for the beginning of political negotiations, while no actual negotiations took place during that time period. The ensuing discussion of a potential political deal between Germany and the Soviet Union had to be channeled into the framework of economic negotiations between the two countries, because close military and diplomatic connections, as was the case before the mid-1930s, had afterward been largely severed. In May, Stalin replaced his Foreign Minister Maxim Litvinov, who was regarded as pro-western and who was also Jewish, with Vyacheslav Molotov, allowing the Soviet Union more latitude in discussions with more parties, not only with Britain and France.
At the same time, British, French, and Soviet negotiators scheduled three-party talks on military matters to occur in Moscow in August 1939, aiming to define what the agreement would specify should be the reaction of the three powers to a German attack. The tripartite military talks, started in mid-August, hit a sticking point regarding the passage of Soviet troops through Poland if Germans attacked, and the parties waited as British and French officials overseas pressured Polish officials to agree to such terms. Polish officials refused to allow Soviet troops into Polish territory if Germany attacked; as Polish foreign minister Józef Beck pointed out, they feared that once the Red Army entered their territories, it might never leave.
On August 19, the 1939 German–Soviet Commercial Agreement was finally signed. On 21 August, the Soviets suspended Tripartite military talks, citing other reasons. That same day, Stalin received assurance that Germany would approve secret protocols to the proposed non-aggression pact that would place half of Poland (border along the Vistula river), Latvia, Estonia, Finland, and Bessarabia in the Soviets' sphere of influence. That night, Stalin replied that the Soviets were willing to sign the pact and that he would receive Ribbentrop on 23 August.
On 22 August, one day after the talks broke down with France and Britain, Moscow revealed that Ribbentrop would visit Stalin the next day. This happened while the Soviets were still negotiating with the British and French missions in Moscow. With the Western nations unwilling to accede to Soviet demands, Stalin instead entered a secret Nazi–Soviet pact. On 24 August a 10-year non-aggression pact was signed with provisions that included: consultation, arbitration if either party disagreed, neutrality if either went to war against a third power, no membership of a group "which is directly or indirectly aimed at the other".
Most notably, there was also a secret protocol to the pact, revealed only after Germany's defeat in 1945, although hints about its provisions were leaked much earlier, e.g., to influence Lithuania. According to said protocol Romania, Poland, Lithuania, Latvia, Estonia and Finland were divided into German and Soviet "spheres of influence". In the north, Finland, Estonia and Latvia were assigned to the Soviet sphere. Poland was to be partitioned in the event of its "political rearrangement"—the areas east of the Pisa, Narev, Vistula and San rivers going to the Soviet Union while Germany would occupy the west. Lithuania, adjacent to East Prussia, would be in the German sphere of influence, although a second secret protocol agreed to in September 1939 reassigned the majority of Lithuania to the USSR. According to the secret protocol, Lithuania would be granted the city of Vilnius – its historical capital, which was under Polish control during the inter-war period. Another clause of the treaty was that Germany would not interfere with the Soviet Union's actions towards Bessarabia, then part of Romania; as the result, Bessarabia was joined to the Moldovan ASSR, and become the Moldovan SSR under control of Moscow.
On 24 August, Pravda and Izvestia carried news of the non-secret portions of the Pact, complete with the now infamous front-page picture of Molotov signing the treaty, with a smiling Stalin looking on. The news was met with utter shock and surprise by government leaders and media worldwide, most of whom were aware only of the British–French–Soviet negotiations that had taken place for months. The Molotov–Ribbentrop Pact was received with shock by Nazi Germany's allies, notably Japan, by the Comintern and foreign communist parties, and by Jewish communities all around the world. So, that day, German diplomat Hans von Herwarth, whose grandmother was Jewish, informed Guido Relli, an Italian diplomat, and American chargé d'affaires Charles Bohlen on the secret protocol regarding vital interests in the countries' allotted "spheres of influence", without revealing the annexation rights for "territorial and political rearrangement".
Soviet propaganda and representatives went to great lengths to minimize the importance of the fact that they had opposed and fought against the Nazis in various ways for a decade prior to signing the Pact. Upon signing the pact, Molotov tried to reassure the Germans of his good intentions by commenting to journalists that "fascism is a matter of taste". For its part, Nazi Germany also did a public volte-face regarding its virulent opposition to the Soviet Union, though Hitler still viewed an attack on the Soviet Union as "inevitable".[citation needed]
The day after the Pact was signed, the French and British military negotiation delegation urgently requested a meeting with Soviet military negotiator Kliment Voroshilov. On August 25, Voroshilov told them "[i]n view of the changed political situation, no useful purpose can be served in continuing the conversation." That day, Hitler told the British ambassador to Berlin that the pact with the Soviets prevented Germany from facing a two front war, changing the strategic situation from that in World War I, and that Britain should accept his demands regarding Poland.
On 1 September, Germany invaded Poland from the west. Within the first few days of the invasion, Germany began conducting massacres of Polish and Jewish civilians and POWs. These executions took place in over 30 towns and villages in the first month of German occupation. The Luftwaffe also took part by strafing fleeing civilian refugees on roads and carrying out a bombing campaign. The Soviet Union assisted German air forces by allowing them to use signals broadcast by the Soviet radio station at Minsk allegedly "for urgent aeronautical experiments".
On 21 September, the Soviets and Germans signed a formal agreement coordinating military movements in Poland, including the "purging" of saboteurs. A joint German–Soviet parade was held in Lvov and Brest-Litovsk, while the countries commanders met in the latter location. Stalin had decided in August that he was going to liquidate the Polish state, and a German–Soviet meeting in September addressed the future structure of the "Polish region". Soviet authorities immediately started a campaign of Sovietization of the newly acquired areas. The Soviets organized staged elections, the result of which was to become a legitimization of Soviet annexation of eastern Poland.
Eleven days after the Soviet invasion of the Polish Kresy, the secret protocol of the Molotov–Ribbentrop Pact was modified by the German–Soviet Treaty of Friendship, Cooperation and Demarcation,) allotting Germany a larger part of Poland and transferring Lithuania's territory (with the exception of left bank of river Scheschupe, the "Lithuanian Strip") from the envisioned German sphere to the Soviets. On 28 September 1939, the Soviet Union and German Reich issued a joint declaration in which they declared:
After the Baltic states were forced to accept treaties, Stalin turned his sights on Finland, confident that Finnish capitulation could be attained without great effort. The Soviets demanded territories on the Karelian Isthmus, the islands of the Gulf of Finland and a military base near the Finnish capital Helsinki, which Finland rejected. The Soviets staged the shelling of Mainila and used it as a pretext to withdraw from the non-aggression pact. The Red Army attacked in November 1939. Simultaneously, Stalin set up a puppet government in the Finnish Democratic Republic.[clarification needed] The leader of the Leningrad Military District Andrei Zhdanov commissioned a celebratory piece from Dmitri Shostakovich, entitled "Suite on Finnish Themes" to be performed as the marching bands of the Red Army would be parading through Helsinki. After Finnish defenses surprisingly held out for over three months while inflicting stiff losses on Soviet forces, the Soviets settled for an interim peace. Finland ceded southeastern areas of Karelia (10% of Finnish territory), which resulted in approximately 422,000 Karelians (12% of Finland's population) losing their homes. Soviet official casualty counts in the war exceeded 200,000, although Soviet Premier Nikita Khrushchev later claimed the casualties may have been one million.
In mid-June 1940, when international attention was focused on the German invasion of France, Soviet NKVD troops raided border posts in Lithuania, Estonia and Latvia. State administrations were liquidated and replaced by Soviet cadres, in which 34,250 Latvians, 75,000 Lithuanians and almost 60,000 Estonians were deported or killed. Elections were held with single pro-Soviet candidates listed for many positions, with resulting peoples assemblies immediately requesting admission into the USSR, which was granted by the Soviet Union. The USSR annexed the whole of Lithuania, including the Scheschupe area, which was to be given to Germany.
Finally, on 26 June, four days after France sued for an armistice with the Third Reich, the Soviet Union issued an ultimatum demanding Bessarabia and, unexpectedly, Northern Bukovina from Romania. Two days later, the Romanians caved to the Soviet demands and the Soviets occupied the territory. The Hertza region was initially not requested by the USSR but was later occupied by force after the Romanians agreed to the initial Soviet demands. The subsequent waves of deportations began in Bessarabia and Northern Bukovina.
Elimination of Polish elites and intelligentia was part of Generalplan Ost. The Intelligenzaktion, a plan to eliminate the Polish intelligentsia, Poland's 'leadership class', took place soon after the German invasion of Poland, lasting from fall of 1939 till spring of 1940. As the result of this operation in 10 regional actions about 60,000 Polish nobles, teachers, social workers, priests, judges and political activists were killed. It was continued in May 1940 when Germany launched AB-Aktion, More than 16,000 members of the intelligentsia were murdered in Operation Tannenberg alone.
Although Germany used forced labourers in most occupied countries, Poles and other Slavs were viewed as inferior by Nazi propaganda, thus, better suited for such duties. Between 1 and 2.5 million Polish citizens were transported to the Reich for forced labour, against their will. All Polish males were required to perform forced labour. While ethnic Poles were subject to selective persecution, all ethnic Jews were targeted by the Reich. In the winter of 1939–40, about 100,000 Jews were thus deported to Poland. They were initially gathered into massive urban ghettos, such as 380,000 held in the Warsaw Ghetto, where large numbers died under the harsh conditions therein, including 43,000 in the Warsaw Ghetto alone. Poles and ethnic Jews were imprisoned in nearly every camp of the extensive concentration camp system in German-occupied Poland and the Reich. In Auschwitz, which began operating on 14 June 1940, 1.1 million people died.
On 10 January 1941, Germany and the Soviet Union signed an agreement settling several ongoing issues. Secret protocols in the new agreement modified the "Secret Additional Protocols" of the German–Soviet Boundary and Friendship Treaty, ceding the Lithuanian Strip to the Soviet Union in exchange for 7.5 million dollars (31.5 million Reichsmark). The agreement formally set the border between Germany and the Soviet Union between the Igorka river and the Baltic Sea. It also extended trade regulation of the 1940 German–Soviet Commercial Agreement until August 1, 1942, increased deliveries above the levels of year one of that agreement, settled trading rights in the Baltics and Bessarabia, calculated the compensation for German property interests in the Baltic States now occupied by the Soviets and other issues. It also covered the migration to Germany within two and a half months of ethnic Germans and German citizens in Soviet-held Baltic territories, and the migration to the Soviet Union of Baltic and "White Russian" "nationals" in German-held territories.
Before the pact's announcement, Communists in the West denied that such a treaty would be signed. Future member of the Hollywood Ten Herbert Biberman denounced rumors as "Fascist propaganda". Earl Browder, head of the Communist Party USA, stated that "there is as much chance of agreement as of Earl Browder being elected president of the Chamber of Commerce." Beginning in September 1939, the Soviet Comintern suspended all anti-Nazi and anti-fascist propaganda, explaining that the war in Europe was a matter of capitalist states attacking each other for imperialist purposes. Western Communists acted accordingly; while before they supported protecting collective security, now they denounced Britain and France going to war.
When anti-German demonstrations erupted in Prague, Czechoslovakia, the Comintern ordered the Czech Communist Party to employ all of its strength to paralyze "chauvinist elements." Moscow soon forced the Communist Parties of France and Great Britain to adopt an anti-war position. On 7 September, Stalin called Georgi Dimitrov,[clarification needed] and the latter sketched a new Comintern line on the war. The new line—which stated that the war was unjust and imperialist—was approved by the secretariat of the Communist International on 9 September. Thus, the various western Communist parties now had to oppose the war, and to vote against war credits. Although the French Communists had unanimously voted in Parliament for war credits on 2 September and on 19 September declared their "unshakeable will" to defend the country, on 27 September the Comintern formally instructed the party to condemn the war as imperialist. By 1 October the French Communists advocated listening to German peace proposals, and Communist leader Maurice Thorez deserted from the French Army on 4 October and fled to Russia. Other Communists also deserted from the army.
The Communist Party of Germany featured similar attitudes. In Die Welt, a communist newspaper published in Stockholm[e] the exiled communist leader Walter Ulbricht opposed the allies (Britain representing "the most reactionary force in the world") and argued: "The German government declared itself ready for friendly relations with the Soviet Union, whereas the English–French war bloc desires a war against the socialist Soviet Union. The Soviet people and the working people of Germany have an interest in preventing the English war plan."
When a joint German–Soviet peace initiative was rejected by Britain and France on 28 September 1939, Soviet foreign policy became critical of the Allies and more pro-German in turn. During the fifth session of the Supreme Soviet on 31 October 1939 Molotov analysed the international situation thus giving the direction for Communist propaganda. According to Molotov Germany had a legitimate interest in regaining its position as a great power and the Allies had started an aggressive war in order to maintain the Versailles system.
Molotov declared in his report entitled "On the Foreign Policy of the Soviet Union" (31 October 1939) held on the fifth (extraordinary) session of the Supreme Soviet, that the Western "ruling circles" disguise their intentions with the pretext of defending democracy against Hitlerism, declaring "their aim in war with Germany is nothing more, nothing less than extermination of Hitlerism. [...] There is absolutely no justification for this kind of war. The ideology of Hitlerism, just like any other ideological system, can be accepted or rejected, this is a matter of political views. But everyone grasps, that an ideology can not be exterminated by force, must not be finished off with a war."
Germany and the Soviet Union entered an intricate trade pact on February 11, 1940, that was over four times larger than the one the two countries had signed in August 1939. The trade pact helped Germany to surmount a British blockade of Germany. In the first year, Germany received one million tons of cereals, half a million tons of wheat, 900,000 tons of oil, 100,000 tons of cotton, 500,000 tons of phosphates and considerable amounts of other vital raw materials, along with the transit of one million tons of soybeans from Manchuria.[citation needed] These and other supplies were being transported through Soviet and occupied Polish territories. The Soviets were to receive a naval cruiser, the plans to the battleship Bismarck, heavy naval guns, other naval gear and thirty of Germany's latest warplanes, including the Me-109 and Me-110 fighters and Ju-88 bomber. The Soviets would also receive oil and electric equipment, locomotives, turbines, generators, diesel engines, ships, machine tools and samples of German artillery, tanks, explosives, chemical-warfare equipment and other items.
The Soviets also helped Germany to avoid British naval blockades by providing a submarine base, Basis Nord, in the northern Soviet Union near Murmansk. This also provided a refueling and maintenance location, and a takeoff point for raids and attacks on shipping. In addition, the Soviets provided Germany with access to the Northern Sea Route for both cargo ships and raiders (though only the commerce raider Komet used the route before the German invasion), which forced Britain to protect sea lanes in both the Atlantic and the Pacific.
The Finnish and Baltic invasions began a deterioration of relations between the Soviets and Germany. Stalin's invasions were a severe irritant to Berlin, as the intent to accomplish these was not communicated to the Germans beforehand, and prompted concern that Stalin was seeking to form an anti-German bloc. Molotov's reassurances to the Germans, and the Germans' mistrust, intensified. On June 16, as the Soviets invaded Lithuania, but before they had invaded Latvia and Estonia, Ribbentrop instructed his staff "to submit a report as soon as possible as to whether in the Baltic States a tendency to seek support from the Reich can be observed or whether an attempt was made to form a bloc."
In August 1940, the Soviet Union briefly suspended its deliveries under their commercial agreement after their relations were strained following disagreement over policy in Romania, the Soviet war with Finland, Germany falling behind in its deliveries of goods under the pact and with Stalin worried that Hitler's war with the West might end quickly after France signed an armistice. The suspension created significant resource problems for Germany. By the end of August, relations improved again as the countries had redrawn the Hungarian and Romanian borders, settled some Bulgarian claims and Stalin was again convinced that Germany would face a long war in the west with Britain's improvement in its air battle with Germany and the execution of an agreement between the United States and Britain regarding destroyers and bases. However, in late August, Germany arranged its own occupation of Romania, targeting oil fields. The move raised tensions with the Soviets, who responded that Germany was supposed to have consulted with the Soviet Union under Article III of the Molotov–Ribbentrop Pact.
After Germany entered a Tripartite Pact with Japan and Italy, Ribbentrop wrote to Stalin, inviting Molotov to Berlin for negotiations aimed to create a 'continental bloc' of Germany, Italy, Japan and the USSR that would oppose Britain and the USA. Stalin sent Molotov to Berlin to negotiate the terms for the Soviet Union to join the Axis and potentially enjoy the spoils of the pact. After negotiations during November 1940 on where to extend the USSR's sphere of influence, Hitler broke off talks and continued planning for the eventual attempts to invade the Soviet Union.
In an effort to demonstrate peaceful intentions toward Germany, on 13 April 1941, the Soviets signed a neutrality pact with Axis power Japan. While Stalin had little faith in Japan's commitment to neutrality, he felt that the pact was important for its political symbolism, to reinforce a public affection for Germany. Stalin felt that there was a growing split in German circles about whether Germany should initiate a war with the Soviet Union. Stalin did not know that Hitler had been secretly discussing an invasion of the Soviet Union since summer 1940, and that Hitler had ordered his military in late 1940 to prepare for war in the east regardless of the parties' talks of a potential Soviet entry as a fourth Axis Power.
Nazi Germany terminated the Molotov–Ribbentrop Pact at 03:15 on 22 June 1941 by launching a massive attack on the Soviet positions in eastern Poland which marked the beginning of the invasion of the Soviet Union known as Operation Barbarossa. Stalin had ignored several warnings that Germany was likely to invade, and ordered no 'full-scale' mobilization of forces although the mobilization was ongoing. After the launch of the invasion, the territories gained by the Soviet Union as a result of the Molotov–Ribbentrop Pact were lost in a matter of weeks. Within six months, the Soviet military had suffered 4.3 million casualties, and Germany had captured three million Soviet prisoners. The lucrative export of Soviet raw materials to Nazi Germany over the course of the Nazi–Soviet economic relations (1934–41) continued uninterrupted until the outbreak of hostilities. The Soviet exports in several key areas enabled Germany to maintain its stocks of rubber and grain from the first day of the invasion until October 1941.
The German original of the secret protocols was presumably destroyed in the bombing of Germany, but in late 1943, Ribbentrop had ordered that the most secret records of the German Foreign Office from 1933 on, amounting to some 9,800 pages, be microfilmed. When the various departments of the Foreign Office in Berlin were evacuated to Thuringia at the end of the war, Karl von Loesch, a civil servant who had worked for the chief interpreter Paul Otto Schmidt, was entrusted with these microfilm copies. He eventually received orders to destroy the secret documents but decided to bury the metal container with the microfilms as a personal insurance for his future well-being. In May 1945, von Loesch approached the British Lt. Col. Robert C. Thomson with the request to transmit a personal letter to Duncan Sandys, Churchill's son-in-law. In the letter, von Loesch revealed that he had knowledge of the documents' whereabouts but expected preferential treatment in return. Colonel Thomson and his American counterpart Ralph Collins agreed to transfer von Loesch to Marburg in the American zone if he would produce the microfilms. The microfilms contained a copy of the Non-Aggression Treaty as well as the Secret Protocol. Both documents were discovered as part of the microfilmed records in August 1945 by the State Department employee Wendell B. Blancke, head of a special unit called "Exploitation German Archives" (EGA).
The treaty was published in the United States for the first time by the St. Louis Post-Dispatch on May 22, 1946, in Britain by the Manchester Guardian. It was also part of an official State Department publication, Nazi–Soviet Relations 1939–1941, edited by Raymond J. Sontag and James S. Beddie in January 1948. The decision to publish the key documents on German–Soviet relations, including the treaty and protocol, had been taken already in spring 1947. Sontag and Beddie prepared the collection throughout the summer of 1947. In November 1947, President Truman personally approved the publication but it was held back in view of the Foreign Ministers Conference in London scheduled for December. Since negotiations at that conference did not prove constructive from an American point of view, the document edition was sent to press. The documents made headlines worldwide. State Department officials counted it as a success: "The Soviet Government was caught flat-footed in what was the first effective blow from our side in a clear-cut propaganda war."
In response to the publication of the secret protocols and other secret German–Soviet relations documents in the State Department edition Nazi–Soviet Relations (1948), Stalin published Falsifiers of History, which included the claim that, during the Pact's operation, Stalin rejected Hitler's claim to share in a division of the world, without mentioning the Soviet offer to join the Axis. That version persisted, without exception, in historical studies, official accounts, memoirs and textbooks published in the Soviet Union until the Soviet Union's dissolution.
For decades, it was the official policy of the Soviet Union to deny the existence of the secret protocol to the Soviet–German Pact. At the behest of Mikhail Gorbachev, Alexander Nikolaevich Yakovlev headed a commission investigating the existence of such a protocol. In December 1989, the commission concluded that the protocol had existed and revealed its findings to the Congress of People's Deputies of the Soviet Union. As a result, the Congress passed the declaration confirming the existence of the secret protocols, condemning and denouncing them. Both successor-states of the pact parties have declared the secret protocols to be invalid from the moment they were signed. The Federal Republic of Germany declared this on September 1, 1989 and the Soviet Union on December 24, 1989, following an examination of the microfilmed copy of the German originals.
Regarding the timing of German rapprochement, many historians agree that the dismissal of Maxim Litvinov, whose Jewish ethnicity was viewed unfavorably by Nazi Germany, removed an obstacle to negotiations with Germany. Stalin immediately directed Molotov to "purge the ministry of Jews." Given Litvinov's prior attempts to create an anti-fascist coalition, association with the doctrine of collective security with France and Britain, and pro-Western orientation by the standards of the Kremlin, his dismissal indicated the existence of a Soviet option of rapprochement with Germany.[f] Likewise, Molotov's appointment served as a signal to Germany that the USSR was open to offers. The dismissal also signaled to France and Britain the existence of a potential negotiation option with Germany. One British official wrote that Litvinov's disappearance also meant the loss of an admirable technician or shock-absorber, while Molotov's "modus operandi" was "more truly Bolshevik than diplomatic or cosmopolitan." Carr argued that the Soviet Union's replacement of Foreign Minister Litvinov with Molotov on May 3, 1939 indicated not an irrevocable shift towards alignment with Germany, but rather was Stalin's way of engaging in hard bargaining with the British and the French by appointing a proverbial hard man, namely Molotov, to the Foreign Commissariat. Historian Albert Resis stated that the Litvinov dismissal gave the Soviets freedom to pursue faster-paced German negotiations, but that they did not abandon British–French talks. Derek Watson argued that Molotov could get the best deal with Britain and France because he was not encumbered with the baggage of collective security and could negotiate with Germany. Geoffrey Roberts argued that Litvinov's dismissal helped the Soviets with British–French talks, because Litvinov doubted or maybe even opposed such discussions.
Edward Hallett Carr, a frequent defender of Soviet policy, stated: "In return for 'non-intervention' Stalin secured a breathing space of immunity from German attack."[page needed] According to Carr, the "bastion" created by means of the Pact, "was and could only be, a line of defense against potential German attack."[page needed] According to Carr, an important advantage was that "if Soviet Russia had eventually to fight Hitler, the Western Powers would already be involved."[page needed] However, during the last decades, this view has been disputed. Historian Werner Maser stated that "the claim that the Soviet Union was at the time threatened by Hitler, as Stalin supposed ... is a legend, to whose creators Stalin himself belonged. In Maser's view, "neither Germany nor Japan were in a situation [of] invading the USSR even with the least perspective  [sic] of success," and this could not have been unknown to Stalin. Carr further stated that, for a long time, the primary motive of Stalin's sudden change of course was assumed to be the fear of German aggressive intentions.
Some critics of Stalin's policy, such as the popular writer Viktor Suvorov, claim that Stalin's primary motive for signing the Soviet–German non-aggression treaty was his calculation that such a pact could result in a conflict between the capitalist countries of Western Europe.[citation needed] This idea is supported by Albert L. Weeks.[page needed] Claims by Suvorov that Stalin planned to invade Germany in 1941 are debated by historians with, for example, David Glantz opposing such claims, while Mikhail Meltyukhov supports them.[citation needed] The authors of The Black Book of Communism consider the pact a crime against peace and a "conspiracy to conduct war of aggression."
The modern English word green comes from the Middle English and Anglo-Saxon word grene, from the same Germanic root as the words "grass" and "grow". It is the color of living grass and leaves and as a result is the color most associated with springtime, growth and nature. By far the largest contributor to green in nature is chlorophyll, the chemical by which plants photosynthesize and convert sunlight into chemical energy. Many creatures have adapted to their green environments by taking on a green hue themselves as camouflage. Several minerals have a green color, including the emerald, which is colored green by its chromium content.
In surveys made in Europe and the United States, green is the color most commonly associated with nature, life, health, youth, spring, hope and envy. In Europe and the U.S. green is sometimes associated with death (green has several seemingly contrary associations), sickness, or the devil, but in China its associations are very positive, as the symbol of fertility and happiness. In the Middle Ages and Renaissance, when the color of clothing showed the owner's social status, green was worn by merchants, bankers and the gentry, while red was the color of the nobility. The Mona Lisa by Leonardo da Vinci wears green, showing she is not from a noble family; the benches in the British House of Commons are green, while those in the House of Lords are red. Green is also the traditional color of safety and permission; a green light means go ahead, a green card permits permanent residence in the United States. It is the most important color in Islam. It was the color of the banner of Muhammad, and is found in the flags of nearly all Islamic countries, and represents the lush vegetation of Paradise. It is also often associated with the culture of Gaelic Ireland, and is a color of the flag of Ireland. Because of its association with nature, it is the color of the environmental movement. Political groups advocating environmental protection and social justice describe themselves as part of the Green movement, some naming themselves Green parties. This has led to similar campaigns in advertising, as companies have sold green, or environmentally friendly, products.
Thus, the languages mentioned above (Germanic, Romance, Slavic, Greek) have old terms for "green" which are derived from words for fresh, sprouting vegetation. However, comparative linguistics makes clear that these terms were coined independently, over the past few millennia, and there is no identifiable single Proto-Indo-European or word for "green". For example, the Slavic zelenъ is cognate with Sanskrit hari "yellow, ochre, golden". The Turkic languages also have jašɨl "green" or "yellowish green", compared to a Mongolian word for "meadow".
In some languages, including old Chinese, Thai, old Japanese, and Vietnamese, the same word can mean either blue or green. The Chinese character 青 (pronounced qīng in Mandarin, ao in Japanese, and thanh in Sino-Vietnamese) has a meaning that covers both blue and green; blue and green are traditionally considered shades of "青". In more contemporary terms, they are 藍 (lán, in Mandarin) and 綠 (lǜ, in Mandarin) respectively. Japanese also has two terms that refer specifically to the color green, 緑 (midori, which is derived from the classical Japanese descriptive verb midoru "to be in leaf, to flourish" in reference to trees) and グリーン (guriin, which is derived from the English word "green"). However, in Japan, although the traffic lights have the same colors that other countries have, the green light is described using the same word as for blue, "aoi", because green is considered a shade of aoi; similarly, green variants of certain fruits and vegetables such as green apples, green shiso (as opposed to red apples and red shiso) will be described with the word "aoi". Vietnamese uses a single word for both blue and green, xanh, with variants such as xanh da trời (azure, lit. "sky blue"), lam (blue), and lục (green; also xanh lá cây, lit. "leaf green").
"Green" in modern European languages corresponds to about 520–570 nm, but many historical and non-European languages make other choices, e.g. using a term for the range of ca. 450–530 nm ("blue/green") and another for ca. 530–590 nm ("green/yellow").[citation needed] In the comparative study of color terms in the world's languages, green is only found as a separate category in languages with the fully developed range of six colors (white, black, red, green, yellow, and blue), or more rarely in systems with five colors (white, red, yellow, green, and black/blue). (See distinction of green from blue) These languages have introduced supplementary vocabulary to denote "green", but these terms are recognizable as recent adoptions that are not in origin color terms (much like the English adjective orange being in origin not a color term but the name of a fruit). Thus, the Thai word เขียว besides meaning "green" also means "rank" and "smelly" and holds other unpleasant associations.
In the subtractive color system, used in painting and color printing, green is created by a combination of yellow and blue, or yellow and cyan; in the RGB color model, used on television and computer screens, it is one of the additive primary colors, along with red and blue, which are mixed in different combinations to create all other colors. On the HSV color wheel, also known as the RGB color wheel, the complement of green is magenta; that is, a color corresponding to an equal mixture of red and blue light (one of the purples). On a traditional color wheel, based on subtractive color, the complementary color to green is considered to be red.
In additive color devices such as computer displays and televisions, one of the primary light sources is typically a narrow-spectrum yellowish-green of dominant wavelength ~550 nm; this "green" primary is combined with an orangish-red "red" primary and a purplish-blue "blue" primary to produce any color in between – the RGB color model. A unique green (green appearing neither yellowish nor bluish) is produced on such a device by mixing light from the green primary with some light from the blue primary.
Lasers emitting in the green part of the spectrum are widely available to the general public in a wide range of output powers. Green laser pointers outputting at 532 nm (563.5 THz) are relatively inexpensive compared to other wavelengths of the same power, and are very popular due to their good beam quality and very high apparent brightness. The most common green lasers use diode pumped solid state (DPSS) technology to create the green light. An infrared laser diode at 808 nm is used to pump a crystal of neodymium-doped yttrium vanadium oxide (Nd:YVO4) or neodymium-doped yttrium aluminium garnet (Nd:YAG) and induces it to emit 281.76 THz (1064 nm). This deeper infrared light is then passed through another crystal containing potassium, titanium and phosphorus (KTP), whose non-linear properties generate light at a frequency that is twice that of the incident beam (563.5 THz); in this case corresponding to the wavelength of 532 nm ("green"). Other green wavelengths are also available using DPSS technology ranging from 501 nm to 543 nm. Green wavelengths are also available from gas lasers, including the helium–neon laser (543 nm), the Argon-ion laser (514 nm) and the Krypton-ion laser (521 nm and 531 nm), as well as liquid dye lasers. Green lasers have a wide variety of applications, including pointing, illumination, surgery, laser light shows, spectroscopy, interferometry, fluorescence, holography, machine vision, non-lethal weapons and bird control.
Many minerals provide pigments which have been used in green paints and dyes over the centuries. Pigments, in this case, are minerals which reflect the color green, rather that emitting it through luminescent or phosphorescent qualities. The large number of green pigments makes it impossible to mention them all. Among the more notable green minerals, however is the emerald, which is colored green by trace amounts of chromium and sometimes vanadium. Chromium(III) oxide (Cr2O3), is called chrome green, also called viridian or institutional green when used as a pigment. For many years, the source of amazonite's color was a mystery. Widely thought to have been due to copper because copper compounds often have blue and green colors, the blue-green color is likely to be derived from small quantities of lead and water in the feldspar. Copper is the source of the green color in malachite pigments, chemically known as basic copper(II) carbonate.
Verdigris is made by placing a plate or blade of copper, brass or bronze, slightly warmed, into a vat of fermenting wine, leaving it there for several weeks, and then scraping off and drying the green powder that forms on the metal. The process of making verdigris was described in ancient times by Pliny. It was used by the Romans in the murals of Pompeii, and in Celtic medieval manuscripts as early as the 5th century AD. It produced a blue-green which no other pigment could imitate, but it had drawbacks; it was unstable, it could not resist dampness, it did not mix well with other colors, it could ruin other colors with which it came into contact., and it was toxic. Leonardo da Vinci, in his treatise on painting, warned artists not to use it. It was widely used in miniature paintings in Europe and Persia in the 16th and 17th centuries. Its use largely ended in the late 19th century, when it was replaced by the safer and more stable chrome green. Viridian, also called chrome green, is a pigment made with chromium oxide dihydrate, was patented in 1859. It became popular with painters, since, unlike other synthetic greens, it was stable and not toxic. Vincent van Gogh used it, along with Prussian blue, to create a dark blue sky with a greenish tint in his painting Cafe terrace at night.
There is no natural source for green food colorings which has been approved by the US Food and Drug Administration. Chlorophyll, the E numbers E140 and E141, is the most common green chemical found in nature, and only allowed in certain medicines and cosmetic materials. Quinoline Yellow (E104) is a commonly used coloring in the United Kingdom but is banned in Australia, Japan, Norway and the United States. Green S (E142) is prohibited in many countries, for it is known to cause hyperactivity, asthma, urticaria, and insomnia.
To create green sparks, fireworks use barium salts, such as barium chlorate, barium nitrate crystals, or barium chloride, also used for green fireplace logs. Copper salts typically burn blue, but cupric chloride (also known as "campfire blue") can also produce green flames. Green pyrotechnic flares can use a mix ratio 75:25 of boron and potassium nitrate. Smoke can be turned green by a mixture: solvent yellow 33, solvent green 3, lactose, magnesium carbonate plus sodium carbonate added to potassium chlorate.
Green is common in nature, as many plants are green because of a complex chemical known as chlorophyll, which is involved in photosynthesis. Chlorophyll absorbs the long wavelengths of light (red) and short wavelengths of light (blue) much more efficiently than the wavelengths that appear green to the human eye, so light reflected by plants is enriched in green. Chlorophyll absorbs green light poorly because it first arose in organisms living in oceans where purple halobacteria were already exploiting photosynthesis. Their purple color arose because they extracted energy in the green portion of the spectrum using bacteriorhodopsin. The new organisms that then later came to dominate the extraction of light were selected to exploit those portions of the spectrum not used by the halobacteria.
Animals typically use the color green as camouflage, blending in with the chlorophyll green of the surrounding environment. Green animals include, especially, amphibians, reptiles, and some fish, birds and insects. Most fish, reptiles, amphibians, and birds appear green because of a reflection of blue light coming through an over-layer of yellow pigment. Perception of color can also be affected by the surrounding environment. For example, broadleaf forests typically have a yellow-green light about them as the trees filter the light. Turacoverdin is one chemical which can cause a green hue in birds, especially. Invertebrates such as insects or mollusks often display green colors because of porphyrin pigments, sometimes caused by diet. This can causes their feces to look green as well. Other chemicals which generally contribute to greenness among organisms are flavins (lychochromes) and hemanovadin. Humans have imitated this by wearing green clothing as a camouflage in military and other fields. Substances that may impart a greenish hue to one's skin include biliverdin, the green pigment in bile, and ceruloplasmin, a protein that carries copper ions in chelation.
There is no green pigment in green eyes; like the color of blue eyes, it is an optical illusion; its appearance is caused by the combination of an amber or light brown pigmentation of the stroma, given by a low or moderate concentration of melanin, with the blue tone imparted by the Rayleigh scattering of the reflected light. Green eyes are most common in Northern and Central Europe. They can also be found in Southern Europe, West Asia, Central Asia, and South Asia. In Iceland, 89% of women and 87% of men have either blue or green eye color. A study of Icelandic and Dutch adults found green eyes to be much more prevalent in women than in men. Among European Americans, green eyes are most common among those of recent Celtic and Germanic ancestry, about 16%.
In Ancient Egypt green was the symbol of regeneration and rebirth, and of the crops made possible by the annual flooding of the Nile. For painting on the walls of tombs or on papyrus, Egyptian artists used finely-ground malachite, mined in the west Sinai and the eastern desert- A paintbox with malachite pigment was found inside the tomb of King Tutankhamun. They also used less expensive green earth pigment, or mixed yellow ochre and blue azurite. To dye fabrics green, they first colored them yellow with dye made from saffron and then soaked them in blue dye from the roots of the woad plant.
For the ancient Egyptians, green had very positive associations. The hieroglyph for green represented a growing papyrus sprout, showing the close connection between green, vegetation, vigor and growth. In wall paintings, the ruler of the underworld, Osiris, was typically portrayed with a green face, because green was the symbol of good health and rebirth. Palettes of green facial makeup, made with malachite, were found in tombs. It was worn by both the living and dead, particularly around the eyes, to protect them from evil. Tombs also often contained small green amulets in the shape of scarab beetles made of malachite, which would protect and give vigor to the deceased. It also symbolized the sea, which was called the "Very Green."
In Ancient Greece, green and blue were sometimes considered the same color, and the same word sometimes described the color of the sea and the color of trees. The philosopher Democritus described two different greens; cloron, or pale green, and prasinon, or leek green. Aristotle considered that green was located midway between black, symbolizing the earth, and white, symbolizing water. However, green was not counted among of the four classic colors of Greek painting; red, yellow, black and white, and is rarely found in Greek art.
The Romans had a greater appreciation for the color green; it was the color of Venus, the goddess of gardens, vegetables and vineyards.The Romans made a fine green earth pigment, which was widely used in the wall paintings of Pompeii, Herculaneum, Lyon, Vaison-la-Romaine, and other Roman cities. They also used the pigment verdigris, made by soaking copper plates in fermenting wine. By the Second Century AD, the Romans were using green in paintings, mosaics and glass, and there were ten different words in Latin for varieties of green.
Unfortunately for those who wanted or were required to wear green, there were no good vegetal green dyes which resisted washing and sunlight. Green dyes were made out of the fern, plantain, buckthorn berries, the juice of nettles and of leeks, the digitalis plant, the broom plant, the leaves of the fraxinus, or ash tree, and the bark of the alder tree, but they rapidly faded or changed color. Only in the 16th century was a good green dye produced, by first dyeing the cloth blue with woad, and then yellow with reseda luteola, also known as yellow-weed.
In the 18th and 19th century, green was associated with the romantic movement in literature and art. The French philosopher Jean-Jacques Rousseau celebrated the virtues of nature, The German poet and philosopher Goethe declared that green was the most restful color, suitable for decorating bedrooms. Painters such as John Constable and Jean-Baptiste-Camille Corot depicted the lush green of rural landscapes and forests. Green was contrasted to the smoky grays and blacks of the Industrial Revolution.
The late nineteenth century also brought the systematic study of color theory, and particularly the study of how complementary colors such as red and green reinforced each other when they were placed next to each other. These studies were avidly followed by artists such as Vincent van Gogh. Describing his painting, The Night Cafe, to his brother Theo in 1888, Van Gogh wrote: "I sought to express with red and green the terrible human passions. The hall is blood red and pale yellow, with a green billiard table in the center, and four lamps of lemon yellow, with rays of orange and green. Everywhere it is a battle and antithesis of the most different reds and greens."
Green can communicate safety to proceed, as in traffic lights. Green and red were standardized as the colors of international railroad signals in the 19th century. The first traffic light, using green and red gas lamps, was erected in 1868 in front of the Houses of Parliament in London. It exploded the following year, injuring the policeman who operated it. In 1912, the first modern electric traffic lights were put up in Salt Lake City, Utah. Red was chosen largely because of its high visibility, and its association with danger, while green was chosen largely because it could not be mistaken for red. Today green lights universally signal that a system is turned on and working as it should. In many video games, green signifies both health and completed objectives, opposite red.
Like other common colors, green has several completely opposite associations. While it is the color most associated by Europeans and Americans with good health, it is also the color most often associated with toxicity and poison. There was a solid foundation for this association; in the nineteenth century several popular paints and pigments, notably verdigris, vert de Schweinfurt and vert de Paris, were highly toxic, containing copper or arsenic.[d] The intoxicating drink absinthe was known as "the green fairy".
Many flags of the Islamic world are green, as the color is considered sacred in Islam (see below). The flag of Hamas, as well as the flag of Iran, is green, symbolizing their Islamist ideology. The 1977 flag of Libya consisted of a simple green field with no other characteristics. It was the only national flag in the world with just one color and no design, insignia, or other details. Some countries used green in their flags to represent their country's lush vegetation, as in the flag of Jamaica, and hope in the future, as in the flags of Portugal and Nigeria. The green cedar of Lebanon tree on the Flag of Lebanon officially represents steadiness and tolerance.
In the 1980s green became the color of a number of new European political parties organized around an agenda of environmentalism. Green was chosen for its association with nature, health, and growth. The largest green party in Europe is Alliance '90/The Greens (German: Bündnis 90/Die Grünen) in Germany, which was formed in 1993 from the merger of the German Green Party, founded in West Germany in 1980, and Alliance 90, founded during the Revolution of 1989–1990 in East Germany. In the 2009 federal elections, the party won 10.7% of the votes and 68 out of 622 seats in the Bundestag.
Roman Catholic and more traditional Protestant clergy wear green vestments at liturgical celebrations during Ordinary Time. In the Eastern Catholic Church, green is the color of Pentecost. Green is one of the Christmas colors as well, possibly dating back to pre-Christian times, when evergreens were worshiped for their ability to maintain their color through the winter season. Romans used green holly and evergreen as decorations for their winter solstice celebration called Saturnalia, which eventually evolved into a Christmas celebration. In Ireland and Scotland especially, green is used to represent Catholics, while orange is used to represent Protestantism. This is shown on the national flag of Ireland.
Napoléon Bonaparte (/nəˈpoʊliən, -ˈpoʊljən/; French: [napɔleɔ̃ bɔnapaʁt], born Napoleone di Buonaparte; 15 August 1769 – 5 May 1821) was a French military and political leader who rose to prominence during the French Revolution and led several successful campaigns during the Revolutionary Wars. As Napoleon I, he was Emperor of the French from 1804 until 1814, and again in 1815. Napoleon dominated European and global affairs for more than a decade while leading France against a series of coalitions in the Napoleonic Wars. He won most of these wars and the vast majority of his battles, building a large empire that ruled over continental Europe before its final collapse in 1815. Often considered one of the greatest commanders in history, his wars and campaigns are studied at military schools worldwide. He also remains one of the most celebrated and controversial political figures in Western history. In civil affairs, Napoleon had a major long-term impact by bringing liberal reforms to the territories that he conquered, especially the Low Countries, Switzerland, and large parts of modern Italy and Germany. He implemented fundamental liberal policies in France and throughout Western Europe.[note 1] His lasting legal achievement, the Napoleonic Code, has been adopted in various forms by a quarter of the world's legal systems, from Japan to Quebec.
Napoleon was born in Corsica to a relatively modest family of noble Tuscan ancestry. Napoleon supported the French Revolution from the outset in 1789 while serving in the French army, and he tried to spread its ideals to Corsica but was banished from the island in 1793. Two years later, he saved the French government from collapse by firing on the Parisian mobs with cannons. The Directory rewarded Napoleon by giving him command of the Army of Italy at age 26, when he began his first military campaign against the Austrians and their Italian allies, scoring a series of decisive victories that made him famous all across Europe. He followed the defeat of the Allies in Europe by commanding a military expedition to Egypt in 1798, invading and occupying the Ottoman province after defeating the Mamelukes and launching modern Egyptology through the discoveries made by his army.
After returning from Egypt, Napoleon engineered a coup in November 1799 and became First Consul of the Republic. Another victory over the Austrians at the Battle of Marengo in 1800 secured his political power. With the Concordat of 1801, Napoleon restored the religious privileges of the Catholic Church while keeping the lands seized by the Revolution. The state continued to nominate the bishops and to control church finances. He extended his political control over France until the Senate declared him Emperor of the French in 1804, launching the French Empire. Intractable differences with the British meant that the French were facing a Third Coalition by 1805. Napoleon shattered this coalition with decisive victories in the Ulm Campaign and a historic triumph at the Battle of Austerlitz, which led to the elimination of the Holy Roman Empire. In October 1805, however, a Franco-Spanish fleet was destroyed at the Battle of Trafalgar, allowing Britain to impose a naval blockade of the French coasts. In retaliation, Napoleon established the Continental System in 1806 to cut off continental trade with Britain. The Fourth Coalition took up arms against him the same year because Prussia became worried about growing French influence on the continent. Napoleon knocked out Prussia at the battles of Jena and Auerstedt, then turned his attention towards the Russians and annihilated them in June 1807 at Friedland, which forced the Russians to accept the Treaties of Tilsit.
Hoping to extend the Continental System, Napoleon invaded Iberia and declared his brother Joseph the King of Spain in 1808. The Spanish and the Portuguese revolted with British support. The Peninsular War lasted six years, noted for its brutal guerrilla warfare, and culminated in an Allied victory. Fighting also erupted in Central Europe, as the Austrians launched another attack against the French in 1809. Napoleon defeated them at the Battle of Wagram, dissolving the Fifth Coalition formed against France. By 1811, Napoleon ruled over 70 million people across an empire that had domination in Europe, which had not witnessed this level of political consolidation since the days of the Roman Empire. He maintained his strategic status through a series of alliances and family appointments. He created a new aristocracy in France while allowing the return of nobles who had been forced into exile by the Revolution.
Tensions over rising Polish nationalism and the economic effects of the Continental System led to renewed confrontation with Russia. To enforce his blockade, Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic collapse of the Grand Army, forcing the French to retreat, as well as leading to the widespread destruction of Russian lands and cities. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France. A chaotic military campaign in Central Europe eventually culminated in a large Allied army defeating Napoleon at the Battle of Leipzig in October. The next year, the Allies invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba. The Bourbons were restored to power and the French lost most of the territories that they had conquered since the Revolution. However, Napoleon escaped from Elba in February 1815 and took control of the government once again. The Allies responded by forming a Seventh Coalition, which ultimately defeated Napoleon at the Battle of Waterloo in June. The Royal Navy then thwarted his planned escape to the United States in July, so he surrendered to the British after running out of other options. The British exiled him to the remote island of Saint Helena in the South Atlantic. His death in 1821 at the age of 51 was received with shock and grief throughout Europe. In 1840, a million people witnessed his remains returning to Paris, where they still reside at Les Invalides.
Napoleon was born on 15 August 1769, to Carlo Maria di Buonaparte and Maria Letizia Ramolino, in his family's ancestral home Casa Buonaparte in Ajaccio, the capital of the island of Corsica. He was their fourth child and third son. This was a year after the island was transferred to France by the Republic of Genoa. He was christened Napoleone di Buonaparte, probably named after an uncle (an older brother who did not survive infancy was the first of the sons to be called Napoleone). In his 20s, he adopted the more French-sounding Napoléon Bonaparte.[note 2]
Napoleon's noble, moderately affluent background afforded him greater opportunities to study than were available to a typical Corsican of the time. In January 1779, he was enrolled at a religious school in Autun. In May, he was admitted to a military academy at Brienne-le-Château. His first language was Corsican, and he always spoke French with a marked Corsican accent and never learned to spell French properly. He was teased by other students for his accent and applied himself to reading. An examiner observed that Napoleon "has always been distinguished for his application in mathematics. He is fairly well acquainted with history and geography... This boy would make an excellent sailor."[note 3]
Upon graduating in September 1785, Bonaparte was commissioned a second lieutenant in La Fère artillery regiment.[note 4] He served in Valence and Auxonne until after the outbreak of the Revolution in 1789, and took nearly two years' leave in Corsica and Paris during this period. At this time, he was a fervent Corsican nationalist, and wrote to Corsican leader Pasquale Paoli in May 1789, "As the nation was perishing I was born. Thirty thousand Frenchmen were vomited on to our shores, drowning the throne of liberty in waves of blood. Such was the odious sight which was the first to strike me."
Some contemporaries alleged that Bonaparte was put under house arrest at Nice for his association with the Robespierres following their fall in the Thermidorian Reaction in July 1794, but Napoleon's secretary Bourrienne disputed the allegation in his memoirs. According to Bourrienne, jealousy was responsible, between the Army of the Alps and the Army of Italy (with whom Napoleon was seconded at the time). Bonaparte dispatched an impassioned defense in a letter to the commissar Salicetti, and he was subsequently acquitted of any wrongdoing.
By 1795, Bonaparte had become engaged to Désirée Clary, daughter of François Clary. Désirée's sister Julie Clary had married Bonaparte's elder brother Joseph. In April 1795, he was assigned to the Army of the West, which was engaged in the War in the Vendée—a civil war and royalist counter-revolution in Vendée, a region in west central France on the Atlantic Ocean. As an infantry command, it was a demotion from artillery general—for which the army already had a full quota—and he pleaded poor health to avoid the posting.
He was moved to the Bureau of Topography of the Committee of Public Safety and sought unsuccessfully to be transferred to Constantinople in order to offer his services to the Sultan. During this period, he wrote the romantic novella Clisson et Eugénie, about a soldier and his lover, in a clear parallel to Bonaparte's own relationship with Désirée. On 15 September, Bonaparte was removed from the list of generals in regular service for his refusal to serve in the Vendée campaign. He faced a difficult financial situation and reduced career prospects.
Two days after the marriage, Bonaparte left Paris to take command of the Army of Italy. He immediately went on the offensive, hoping to defeat the forces of Piedmont before their Austrian allies could intervene. In a series of rapid victories during the Montenotte Campaign, he knocked Piedmont out of the war in two weeks. The French then focused on the Austrians for the remainder of the war, the highlight of which became the protracted struggle for Mantua. The Austrians launched a series of offensives against the French to break the siege, but Napoleon defeated every relief effort, scoring notable victories at the battles of Castiglione, Bassano, Arcole, and Rivoli. The decisive French triumph at Rivoli in January 1797 led to the collapse of the Austrian position in Italy. At Rivoli, the Austrians lost up to 14,000 men while the French lost about 5,000.
The next phase of the campaign featured the French invasion of the Habsburg heartlands. French forces in Southern Germany had been defeated by the Archduke Charles in 1796, but the Archduke withdrew his forces to protect Vienna after learning about Napoleon's assault. In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797. The Austrians were alarmed by the French thrust that reached all the way to Leoben, about 100 km from Vienna, and finally decided to sue for peace. The Treaty of Leoben, followed by the more comprehensive Treaty of Campo Formio, gave France control of most of northern Italy and the Low Countries, and a secret clause promised the Republic of Venice to Austria. Bonaparte marched on Venice and forced its surrender, ending 1,100 years of independence. He also authorized the French to loot treasures such as the Horses of Saint Mark.
Bonaparte could win battles by concealment of troop deployments and concentration of his forces on the 'hinge' of an enemy's weakened front. If he could not use his favourite envelopment strategy, he would take up the central position and attack two co-operating forces at their hinge, swing round to fight one until it fled, then turn to face the other. In this Italian campaign, Bonaparte's army captured 150,000 prisoners, 540 cannons, and 170 standards. The French army fought 67 actions and won 18 pitched battles through superior artillery technology and Bonaparte's tactics.
During the campaign, Bonaparte became increasingly influential in French politics. He founded two newspapers: one for the troops in his army and another for circulation in France. The royalists attacked Bonaparte for looting Italy and warned that he might become a dictator. All told, Napoleon's forces extracted an estimated $45 million in funds from Italy during their campaign there, another $12 million in precious metals and jewels; atop that, his forces confiscated more than three-hundred priceless paintings and sculptures. Bonaparte sent General Pierre Augereau to Paris to lead a coup d'état and purge the royalists on 4 September—Coup of 18 Fructidor. This left Barras and his Republican allies in control again but dependent on Bonaparte, who proceeded to peace negotiations with Austria. These negotiations resulted in the Treaty of Campo Formio, and Bonaparte returned to Paris in December as a hero. He met Talleyrand, France's new Foreign Minister—who later served in the same capacity for Emperor Napoleon—and they began to prepare for an invasion of Britain.
General Bonaparte and his expedition eluded pursuit by the Royal Navy and landed at Alexandria on 1 July. He fought the Battle of Shubra Khit against the Mamluks, Egypt's ruling military caste. This helped the French practice their defensive tactic for the Battle of the Pyramids, fought on 21 July, about 24 km (15 mi) from the pyramids. General Bonaparte's forces of 25,000 roughly equalled those of the Mamluks' Egyptian cavalry. Twenty-nine French and approximately 2,000 Egyptians were killed. The victory boosted the morale of the French army.
On 1 August, the British fleet under Horatio Nelson captured or destroyed all but two French vessels in the Battle of the Nile, defeating Bonaparte's goal to strengthen the French position in the Mediterranean. His army had succeeded in a temporary increase of French power in Egypt, though it faced repeated uprisings. In early 1799, he moved an army into the Ottoman province of Damascus (Syria and Galilee). Bonaparte led these 13,000 French soldiers in the conquest of the coastal towns of Arish, Gaza, Jaffa, and Haifa. The attack on Jaffa was particularly brutal. Bonaparte discovered that many of the defenders were former prisoners of war, ostensibly on parole, so he ordered the garrison and 1,400 prisoners to be executed by bayonet or drowning to save bullets. Men, women, and children were robbed and murdered for three days.
Bonaparte began with an army of 13,000 men; 1,500 were reported missing, 1,200 died in combat, and thousands perished from disease—mostly bubonic plague. He failed to reduce the fortress of Acre, so he marched his army back to Egypt in May. To speed up the retreat, Bonaparte ordered plague-stricken men to be poisoned with opium; the number who died remains disputed, ranging from a low of 30 to a high of 580. He also brought out 1,000 wounded men. Back in Egypt on 25 July, Bonaparte defeated an Ottoman amphibious invasion at Abukir.
Despite the failures in Egypt, Napoleon returned to a hero's welcome. He drew together an alliance with director Emmanuel Joseph Sieyès, his brother Lucien, speaker of the Council of Five Hundred Roger Ducos, director Joseph Fouché, and Talleyrand, and they overthrew the Directory by a coup d'état on 9 November 1799 ("the 18th Brumaire" according to the revolutionary calendar), closing down the council of five hundred. Napoleon became "first consul" for ten years, with two consuls appointed by him who had consultative voices only. His power was confirmed by the new "Constitution of the Year VIII", originally devised by Sieyès to give Napoleon a minor role, but rewritten by Napoleon, and accepted by direct popular vote (3,000,000 in favor, 1,567 opposed). The constitution preserved the appearance of a republic but in reality established a dictatorship.
Napoleon established a political system that historian Martyn Lyons called "dictatorship by plebiscite." Worried by the democratic forces unleashed by the Revolution, but unwilling to ignore them entirely, Napoleon resorted to regular electoral consultations with the French people on his road to imperial power. He drafted the Constitution of the Year VIII and secured his own election as First Consul, taking up residence at the Tuileries. The constitution was approved in a rigged plebiscite held the following January, with 99.94 percent officially listed as voting "yes." Napoleon's brother, Lucien, had falsified the returns to show that 3 million people had participated in the plebiscite; the real number was 1.5 million. Political observers at the time assumed the eligible French voting public numbered about 5 million people, so the regime artificially doubled the participation rate to indicate popular enthusiasm for the Consulate. In the first few months of the Consulate, with war in Europe still raging and internal instability still plaguing the country, Napoleon's grip on power remained very tenuous.
In the spring of 1800, Napoleon and his troops crossed the Swiss Alps into Italy, aiming to surprise the Austrian armies that had reoccupied the peninsula when Napoleon was still in Egypt.[note 5] After a difficult crossing over the Alps, the French army entered the plains of Northern Italy virtually unopposed. While one French army approached from the north, the Austrians were busy with another stationed in Genoa, which was besieged by a substantial force. The fierce resistance of this French army, under André Masséna, gave the northern striking force precious time to carry out their operations with little interference. After spending several days looking for each other, the two armies finally collided at the Battle of Marengo on June 14. General Melas had a numerical advantage, fielding about 30,000 Austrian soldiers while Napoleon commanded 24,000 French troops. The battle began favorably for the Austrians as their initial attack surprised the French and gradually drove them back. Melas concluded that he'd won the battle and retired to his headquarters around 3 pm, leaving his subordinates in charge of pursuing the French. However, the French lines never broke during their tactical retreat; Napoleon constantly rode out among the troops urging them to stand and fight. Late in the afternoon, a full division under Desaix arrived on the field and dramatically reversed the tide of the battle. A series of artillery barrages and fortunate cavalry charges managed to decimate the Austrian army, which fled chaotically over the Bormida River back to Alessandria, leaving behind 14,000 casualties. The following day, the Austrian army agreed to abandon Northern Italy once more with the Convention of Alessandria, which granted them safe passage to friendly soil in exchange for their fortresses throughout the region.
Although critics have blamed Napoleon for several tactical mistakes preceding the battle, they have also praised his audacity for selecting a risky campaign strategy, choosing to invade the Italian peninsula from the north when the vast majority of French invasions came from the west, near or along the coastline. As Chandler points out, Napoleon spent almost a year getting the Austrians out of Italy in his first campaign; in 1800, it took him only a month to achieve the same goal. German strategist and field marshal Alfred von Schlieffen concluded that "Bonaparte did not annihilate his enemy but eliminated him and rendered him harmless" while "[attaining] the object of the campaign: the conquest of North Italy."
Napoleon's triumph at Marengo secured his political authority and boosted his popularity back home, but it did not lead to an immediate peace. Bonaparte's brother, Joseph, led the complex negotiations in Lunéville and reported that Austria, emboldened by British support, would not acknowledge the new territory that France had acquired. As negotiations became increasingly fractious, Bonaparte gave orders to his general Moreau to strike Austria once more. Moreau and the French swept through Bavaria and scored an overwhelming victory at Hohenlinden in December 1800. As a result, the Austrians capitulated and signed the Treaty of Lunéville in February 1801. The treaty reaffirmed and expanded earlier French gains at Campo Formio. Britain now remained the only nation that was still at war with France.
After a decade of constant warfare, France and Britain signed the Treaty of Amiens in March 1802, bringing the Revolutionary Wars to an end. Amiens called for the withdrawal of British troops from recently conquered colonial territories as well as for assurances to curtail the expansionary goals of the French Republic. With Europe at peace and the economy recovering, Napoleon's popularity soared to its highest levels under the Consulate, both domestically and abroad. In a new plebiscite during the spring of 1802, the French public came out in huge numbers to approve a constitution that made the Consulate permanent, essentially elevating Napoleon to dictator for life. Whereas the plebiscite two years earlier had brought out 1.5 million people to the polls, the new referendum enticed 3.6 million to go and vote (72% of all eligible voters). There was no secret ballot in 1802 and few people wanted to openly defy the regime; the constitution gained approval with over 99% of the vote. His broad powers were spelled out in the new constitution: Article 1. The French people name, and the Senate proclaims Napoleon-Bonaparte First Consul for Life. After 1802, he was generally referred to as Napoleon rather than Bonaparte.
The brief peace in Europe allowed Napoleon to focus on the French colonies abroad. Saint-Domingue had managed to acquire a high level of political autonomy during the Revolutionary Wars, with Toussaint Louverture installing himself as de facto dictator by 1801. Napoleon saw his chance to recuperate the formerly wealthy colony when he signed the Treaty of Amiens. During the Revolution, the National Convention voted to abolish slavery in February 1794. Under the terms of Amiens, however, Napoleon agreed to appease British demands by not abolishing slavery in any colonies where the 1794 decree had never been implemented. The resulting Law of 20 May never applied to colonies like Guadeloupe or Guyane, even though rogue generals and other officials used the pretext of peace as an opportunity to reinstate slavery in some of these places. The Law of 20 May officially restored the slave trade to the Caribbean colonies, not slavery itself. Napoleon sent an expedition under General Leclerc designed to reassert control over Sainte-Domingue. Although the French managed to capture Toussaint Louverture, the expedition failed when high rates of disease crippled the French army. In May 1803, the last 8000 French troops left the island and the slaves proclaimed an independent republic that they called Haïti in 1804. Seeing the failure of his colonial efforts, Napoleon decided in 1803 to sell the Louisiana Territory to the United States, instantly doubling the size of the U.S. The selling price in the Louisiana Purchase was less than three cents per acre, a total of $15 million.
During the Consulate, Napoleon faced several royalist and Jacobin assassination plots, including the Conspiration des poignards (Dagger plot) in October 1800 and the Plot of the Rue Saint-Nicaise (also known as the Infernal Machine) two months later. In January 1804, his police uncovered an assassination plot against him that involved Moreau and which was ostensibly sponsored by the Bourbon family, the former rulers of France. On the advice of Talleyrand, Napoleon ordered the kidnapping of the Duke of Enghien, violating the sovereignty of Baden. The Duke was quickly executed after a secret military trial, even though he had not been involved in the plot. Enghien's execution infuriated royal courts throughout Europe, become one of the contributing political factors for the outbreak of the Napoleonic Wars.
To expand his power, Napoleon used these assassination plots to justify the creation of an imperial system based on the Roman model. He believed that a Bourbon restoration would be more difficult if his family's succession was entrenched in the constitution. Launching yet another referendum, Napoleon was elected as Emperor of the French by a tally exceeding 99%. As with the Life Consulate two years earlier, this referendum produced heavy participation, bringing out almost 3.6 million voters to the polls.
Napoleon's coronation took place on December 2, 1804. Two separate crowns were brought for the ceremony: a golden laurel wreath recalling the Roman Empire and a replica of Charlemagne's crown. Napoleon entered the ceremony wearing the laurel wreath and kept it on his head throughout the proceedings. For the official coronation, he raised the Charlemagne crown over his own head in a symbolic gesture, but never placed it on top because he was already wearing the golden wreath. Instead he placed the crown on Josephine's head, the event commemorated in the officially sanctioned painting by Jacques-Louis David. Napoleon was also crowned King of Italy, with the Iron Crown of Lombardy, at the Cathedral of Milan on May 26, 1805. He created eighteen Marshals of the Empire from amongst his top generals to secure the allegiance of the army.
Before the formation of the Third Coalition, Napoleon had assembled an invasion force, the Armée d'Angleterre, around six camps at Boulogne in Northern France. He intended to use this invasion force to strike at England. They never invaded, but Napoleon's troops received careful and invaluable training for future military operations. The men at Boulogne formed the core for what Napoleon later called La Grande Armée. At the start, this French army had about 200,000 men organized into seven corps, which were large field units that contained 36 to 40 cannons each and were capable of independent action until other corps could come to the rescue. A single corps properly situated in a strong defensive position could survive at least a day without support, giving the Grande Armée countless strategic and tactical options on every campaign. On top of these forces, Napoleon created a cavalry reserve of 22,000 organized into two cuirassier divisions, four mounted dragoon divisions, one division of dismounted dragoons, and one of light cavalry, all supported by 24 artillery pieces. By 1805, the Grande Armée had grown to a force of 350,000 men, who were well equipped, well trained, and led by competent officers.
Napoleon knew that the French fleet could not defeat the Royal Navy in a head-to-head battle, so he planned to lure it away from the English Channel through diversionary tactics. The main strategic idea involved the French Navy escaping from the British blockades of Toulon and Brest and threatening to attack the West Indies. In the face of this attack, it was hoped, the British would weaken their defense of the Western Approaches by sending ships to the Caribbean, allowing a combined Franco-Spanish fleet to take control of the channel long enough for French armies to cross and invade. However, the plan unraveled after the British victory at the Battle of Cape Finisterre in July 1805. French Admiral Villeneuve then retreated to Cádiz instead of linking up with French naval forces at Brest for an attack on the English Channel.
By August 1805, Napoleon had realized that the strategic situation had changed fundamentally. Facing a potential invasion from his continental enemies, he decided to strike first and turned his army's sights from the English Channel to the Rhine. His basic objective was to destroy the isolated Austrian armies in Southern Germany before their Russian allies could arrive. On 25 September, after great secrecy and feverish marching, 200,000 French troops began to cross the Rhine on a front of 260 km (160 mi). Austrian commander Karl Mack had gathered the greater part of the Austrian army at the fortress of Ulm in Swabia. Napoleon swung his forces to the southeast and the Grande Armée performed an elaborate wheeling movement that outflanked the Austrian positions. The Ulm Maneuver completely surprised General Mack, who belatedly understood that his army had been cut off. After some minor engagements that culminated in the Battle of Ulm, Mack finally surrendered after realizing that there was no way to break out of the French encirclement. For just 2000 French casualties, Napoleon had managed to capture a total of 60,000 Austrian soldiers through his army's rapid marching. The Ulm Campaign is generally regarded as a strategic masterpiece and was influential in the development of the Schlieffen Plan in the late 19th century. For the French, this spectacular victory on land was soured by the decisive victory that the Royal Navy attained at the Battle of Trafalgar on 21 October. After Trafalgar, Britain had total domination of the seas for the duration of the Napoleonic Wars.
Following the Ulm Campaign, French forces managed to capture Vienna in November. The fall of Vienna provided the French a huge bounty as they captured 100,000 muskets, 500 cannons, and the intact bridges across the Danube. At this critical juncture, both Tsar Alexander I and Holy Roman Emperor Francis II decided to engage Napoleon in battle, despite reservations from some of their subordinates. Napoleon sent his army north in pursuit of the Allies, but then ordered his forces to retreat so that he could feign a grave weakness. Desperate to lure the Allies into battle, Napoleon gave every indication in the days preceding the engagement that the French army was in a pitiful state, even abandoning the dominant Pratzen Heights near the village of Austerlitz. At the Battle of Austerlitz, in Moravia on 2 December, he deployed the French army below the Pratzen Heights and deliberately weakened his right flank, enticing the Allies to launch a major assault there in the hopes of rolling up the whole French line. A forced march from Vienna by Marshal Davout and his III Corps plugged the gap left by Napoleon just in time. Meanwhile, the heavy Allied deployment against the French right weakened their center on the Pratzen Heights, which was viciously attacked by the IV Corps of Marshal Soult. With the Allied center demolished, the French swept through both enemy flanks and sent the Allies fleeing chaotically, capturing thousands of prisoners in the process. The battle is often seen as a tactical masterpiece because of the near-perfect execution of a calibrated but dangerous plan — of the same stature as Cannae, the celebrated triumph by Hannibal some 2000 years before.
The Allied disaster at Austerlitz significantly shook the faith of Emperor Francis in the British-led war effort. France and Austria agreed to an armistice immediately and the Treaty of Pressburg followed shortly after on 26 December. Pressburg took Austria out of both the war and the Coalition while reinforcing the earlier treaties of Campo Formio and of Lunéville between the two powers. The treaty confirmed the Austrian loss of lands to France in Italy and Bavaria, and lands in Germany to Napoleon's German allies. It also imposed an indemnity of 40 million francs on the defeated Habsburgs and allowed the fleeing Russian troops free passage through hostile territories and back to their home soil. Napoleon went on to say, "The battle of Austerlitz is the finest of all I have fought." Frank McLynn suggests that Napoleon was so successful at Austerlitz that he lost touch with reality, and what used to be French foreign policy became a "personal Napoleonic one". Vincent Cronin disagrees, stating that Napoleon was not overly ambitious for himself, "he embodied the ambitions of thirty million Frenchmen".
Napoleon continued to entertain a grand scheme to establish a French presence in the Middle East in order to put pressure on Britain and Russia, and perhaps form an alliance with the Ottoman Empire. In February 1806, Ottoman Emperor Selim III finally recognized Napoleon as Emperor. He also opted for an alliance with France, calling France "our sincere and natural ally." That decision brought the Ottoman Empire into a losing war against Russia and Britain. A Franco-Persian alliance was also formed between Napoleon and the Persian Empire of Fat′h-Ali Shah Qajar. It collapsed in 1807, when France and Russia themselves formed an unexpected alliance. In the end, Napoleon had made no effective alliances in the Middle East.
After Austerlitz, Napoleon established the Confederation of the Rhine in 1806. A collection of German states intended to serve as a buffer zone between France and Central Europe, the creation of the Confederation spelled the end of the Holy Roman Empire and significantly alarmed the Prussians. The brazen reorganization of German territory by the French risked threatening Prussian influence in the region, if not eliminating it outright. War fever in Berlin rose steadily throughout the summer of 1806. At the insistence of his court, especially his wife Queen Louise, Frederick William III decided to challenge the French domination of Central Europe by going to war.
The initial military maneuvers began in September 1806. In a notable letter to Marshal Soult detailing the plan for the campaign, Napoleon described the essential features of Napoleonic warfare and introduced the phrase le bataillon-carré ('square battalion'). In the bataillon-carré system, the various corps of the Grande Armée would march uniformly together in close supporting distance. If any single corps was attacked, the others could quickly spring into action and arrive to help. Napoleon invaded Prussia with 180,000 troops, rapidly marching on the right bank of the River Saale. As in previous campaigns, his fundamental objective was to destroy one opponent before reinforcements from another could tip the balance of the war. Upon learning the whereabouts of the Prussian army, the French swung westwards and crossed the Saale with overwhelming force. At the twin battles of Jena and Auerstedt, fought on 14 October, the French convincingly defeated the Prussians and inflicted heavy casualties. With several major commanders dead or incapacitated, the Prussian king proved incapable of effectively commanding the army, which began to quickly disintegrate. In a vaunted pursuit that epitomized the "peak of Napoleonic warfare," according to historian Richard Brooks, the French managed to capture 140,000 soldiers, over 2,000 cannons and hundreds of ammunition wagons, all in a single month. Historian David Chandler wrote of the Prussian forces: "Never has the morale of any army been more completely shattered." Despite their overwhelming defeat, the Prussians refused to negotiate with the French until the Russians had an opportunity to enter the fight.
Following his triumph, Napoleon imposed the first elements of the Continental System through the Berlin Decree issued in November 1806. The Continental System, which prohibited European nations from trading with Britain, was widely violated throughout his reign. In the next few months, Napoleon marched against the advancing Russian armies through Poland and was involved in the bloody stalemate at the Battle of Eylau in February 1807. After a period of rest and consolidation on both sides, the war restarted in June with an initial struggle at Heilsberg that proved indecisive. On 14 June, however, Napoleon finally obtained an overwhelming victory over the Russians at the Battle of Friedland, wiping out the majority of the Russian army in a very bloody struggle. The scale of their defeat convinced the Russians to make peace with the French. On 19 June, Czar Alexander sent an envoy to seek an armistice with Napoleon. The latter assured the envoy that the Vistula River represented the natural borders between French and Russian influence in Europe. On that basis, the two emperors began peace negotiations at the town of Tilsit after meeting on an iconic raft on the River Niemen. The very first thing Alexander said to Napoleon was probably well-calibrated: "I hate the English as much as you do."
Alexander faced pressure from his brother, Duke Constantine, to make peace with Napoleon. Given the victory he had just achieved, the French emperor offered the Russians relatively lenient terms–demanding that Russia join the Continental System, withdraw its forces from Wallachia and Moldavia, and hand over the Ionian Islands to France. By contrast, Napoleon dictated very harsh peace terms for Prussia, despite the ceaseless exhortations of Queen Louise. Wiping out half of Prussian territories from the map, Napoleon created a new kingdom of 1,100 square miles called Westphalia. He then appointed his young brother Jérôme as the new monarch of this kingdom. Prussia's humiliating treatment at Tilsit caused a deep and bitter antagonism which festered as the Napoleonic era progressed. Moreover, Alexander's pretensions at friendship with Napoleon led the latter to seriously misjudge the true intentions of his Russian counterpart, who would violate numerous provisions of the treaty in the next few years. Despite these problems, the Treaties of Tilsit at last gave Napoleon a respite from war and allowed him to return to France, which he had not seen in over 300 days.
The settlements at Tilsit gave Napoleon time to organize his empire. One of his major objectives became enforcing the Continental System against the British. He decided to focus his attention on the Kingdom of Portugal, which consistently violated his trade prohibitions. After defeat in the War of the Oranges in 1801, Portugal adopted a double-sided policy. At first, John VI agreed to close his ports to British trade. The situation changed dramatically after the Franco-Spanish defeat at Trafalgar; John grew bolder and officially resumed diplomatic and trade relations with Britain.
Unhappy with this change of policy by the Portuguese government, Napoleon sent an army to invade Portugal. On 17 October 1807, 24,000 French troops under General Junot crossed the Pyrenees with Spanish cooperation and headed towards Portugal to enforce Napoleon's orders. This attack was the first step in what would eventually become the Peninsular War, a six-year struggle that significantly sapped French strength. Throughout the winter of 1808, French agents became increasingly involved in Spanish internal affairs, attempting to incite discord between members of the Spanish royal family. On 16 February 1808, secret French machinations finally materialized when Napoleon announced that he would intervene to mediate between the rival political factions in the country. Marshal Murat led 120,000 troops into Spain and the French arrived in Madrid on 24 March, where wild riots against the occupation erupted just a few weeks later. Napoleon appointed his brother, Joseph Bonaparte, as the new King of Spain in the summer of 1808. The appointment enraged a heavily religious and conservative Spanish population. Resistance to French aggression soon spread throughout the country. The shocking French defeat at the Battle of Bailén in July gave hope to Napoleon's enemies and partly persuaded the French emperor to intervene in person.
Before going to Iberia, Napoleon decided to address several lingering issues with the Russians. At the Congress of Erfurt in October 1808, Napoleon hoped to keep Russia on his side during the upcoming struggle in Spain and during any potential conflict against Austria. The two sides reached an agreement, the Erfurt Convention, that called upon Britain to cease its war against France, that recognized the Russian conquest of Finland from Sweden, and that affirmed Russian support for France in a possible war against Austria "to the best of its ability." Napoleon then returned to France and prepared for war. The Grande Armée, under the Emperor's personal command, rapidly crossed the Ebro River in November 1808 and inflicted a series of crushing defeats against the Spanish forces. After clearing the last Spanish force guarding the capital at Somosierra, Napoleon entered Madrid on 4 December with 80,000 troops. He then unleashed his soldiers against Moore and the British forces. The British were swiftly driven to the coast, and they withdrew from Spain entirely after a last stand at the Battle of Corunna in January 1809.
Napoleon would end up leaving Iberia in order to deal with the Austrians in Central Europe, but the Peninsular War continued on long after his absence. He never returned to Spain after the 1808 campaign. Several months after Corunna, the British sent another army to the peninsula under the future Duke of Wellington. The war then settled into a complex and asymmetric strategic deadlock where all sides struggled to gain the upper hand. The highlight of the conflict became the brutal guerrilla warfare that engulfed much of the Spanish countryside. Both sides committed the worst atrocities of the Napoleonic Wars during this phase of the conflict. The vicious guerrilla fighting in Spain, largely absent from the French campaigns in Central Europe, severely disrupted the French lines of supply and communication. Although France maintained roughly 300,000 troops in Iberia during the Peninsular War, the vast majority were tied down to garrison duty and to intelligence operations. The French were never able to concentrate all of their forces effectively, prolonging the war until events elsewhere in Europe finally turned the tide in favor of the Allies. After the invasion of Russia in 1812, the number of French troops in Spain vastly declined as Napoleon needed reinforcements to conserve his strategic position in Europe. By 1814, after scores of battles and sieges throughout Iberia, the Allies had managed to push the French out of the peninsula.
After four years on the sidelines, Austria sought another war with France to avenge its recent defeats. Austria could not count on Russian support because the latter was at war with Britain, Sweden, and the Ottoman Empire in 1809. Frederick William of Prussia initially promised to help the Austrians, but reneged before conflict began. A report from the Austrian finance minister suggested that the treasury would run out of money by the middle of 1809 if the large army that the Austrians had formed since the Third Coalition remained mobilized. Although Archduke Charles warned that the Austrians were not ready for another showdown with Napoleon, a stance that landed him in the so-called "peace party," he did not want to see the army demobilized either. On 8 February 1809, the advocates for war finally succeeded when the Imperial Government secretly decided on another confrontation against the French.
In the early morning of 10 April, leading elements of the Austrian army crossed the Inn River and invaded Bavaria. The early Austrian attack surprised the French; Napoleon himself was still in Paris when he heard about the invasion. He arrived at Donauwörth on the 17th to find the Grande Armée in a dangerous position, with its two wings separated by 75 miles (121 km) and joined together by a thin cordon of Bavarian troops. Charles pressed the left wing of the French army and hurled his men towards the III Corps of Marshal Davout. In response, Napoleon came up with a plan to cut off the Austrians in the celebrated Landshut Maneuver. He realigned the axis of his army and marched his soldiers towards the town of Eckmühl. The French scored a convincing win in the resulting Battle of Eckmühl, forcing Charles to withdraw his forces over the Danube and into Bohemia. On 13 May, Vienna fell for the second time in four years, although the war continued since most of the Austrian army had survived the initial engagements in Southern Germany.
By 17 May, the main Austrian army under Charles had arrived on the Marchfeld. Charles kept the bulk of his troops several miles away from the river bank in hopes of concentrating them at the point where Napoleon decided to cross. On 21 May, the French made their first major effort to cross the Danube, precipitating the Battle of Aspern-Essling. The Austrians enjoyed a comfortable numerical superiority over the French throughout the battle; on the first day, Charles disposed of 110,000 soldiers against only 31,000 commanded by Napoleon. By the second day, reinforcements had boosted French numbers up to 70,000. The battle was characterized by a vicious back-and-forth struggle for the two villages of Aspern and Essling, the focal points of the French bridgehead. By the end of the fighting, the French had lost Aspern but still controlled Essling. A sustained Austrian artillery bombardment eventually convinced Napoleon to withdraw his forces back onto Lobau Island. Both sides inflicted about 23,000 casualties on each other. It was the first defeat Napoleon suffered in a major set-piece battle, and it caused excitement throughout many parts of Europe because it proved that he could be beaten on the battlefield.
After the setback at Aspern-Essling, Napoleon took more than six weeks in planning and preparing for contingencies before he made another attempt at crossing the Danube. From 30 June to the early days of July, the French recrossed the Danube in strength, with more than 180,000 troops marching across the Marchfeld towards the Austrians. Charles received the French with 150,000 of his own men. In the ensuing Battle of Wagram, which also lasted two days, Napoleon commanded his forces in what was the largest battle of his career up until then. Neither side made much progress on 5 July, but the 6th produced a definitive outcome. Both sides launched major assaults on their flanks. Austrian attacks against the French left wing looked dangerous initially, but they were all beaten back. Meanwhile, a steady French attack against the Austrian left wing eventually compromised the entire position for Charles. Napoleon finished off the battle with a concentrated central thrust that punctured a hole in the Austrian army and forced Charles to retreat. Austrian losses were very heavy, reaching well over 40,000 casualties. The French were too exhausted to pursue the Austrians immediately, but Napoleon eventually caught up with Charles at Znaim and the latter signed an armistice on 12 July.
In the Kingdom of Holland, the British launched the Walcheren Campaign to open up a second front in the war and to relieve the pressure on the Austrians. The British army only landed at Walcheren on 30 July, by which point the Austrians had already been defeated. The Walcheren Campaign was characterized by little fighting but heavy casualties thanks to the popularly dubbed "Walcheren Fever." Over 4000 British troops were lost in a bungled campaign, and the rest withdrew in December 1809. The main strategic result from the campaign became the delayed political settlement between the French and the Austrians. Emperor Francis wanted to wait and see how the British performed in their theater before entering into negotiations with Napoleon. Once it became apparent that the British were going nowhere, the Austrians agreed to peace talks.
The resulting Treaty of Schönbrunn in October 1809 was the harshest that France had imposed on Austria in recent memory. Metternich and Archduke Charles had the preservation of the Habsburg Empire as their fundamental goal, and to this end they succeeded by making Napoleon seek more modest goals in return for promises of friendship between the two powers. Nevertheless, while most of the hereditary lands remained a part of the Habsburg realm, France received Carinthia, Carniola, and the Adriatic ports, while Galicia was given to the Poles and the Salzburg area of the Tyrol went to the Bavarians. Austria lost over three million subjects, about one-fifth of her total population, as a result of these territorial changes. Although fighting in Iberia continued, the War of the Fifth Coalition would be the last major conflict on the European continent for the next three years.
Napoleon turned his focus to domestic affairs after the war. Empress Joséphine had still not given birth to a child from Napoleon, who became worried about the future of his empire following his death. Desperate for a legitimate heir, Napoleon divorced Joséphine in January 1810 and started looking for a new wife. Hoping to cement the recent alliance with Austria through a family connection, Napoleon married the Archduchess Marie Louise, who was 18 years old at the time. On 20 March 1811, Marie Louise gave birth to a baby boy, whom Napoleon made heir apparent and bestowed the title of King of Rome. His son never actually ruled the empire, but historians still refer to him as Napoleon II.
In 1808, Napoleon and Czar Alexander met at the Congress of Erfurt to preserve the Russo-French alliance. The leaders had a friendly personal relationship after their first meeting at Tilsit in 1807. By 1811, however, tensions had increased and Alexander was under pressure from the Russian nobility to break off the alliance. A major strain on the relationship between the two nations became the regular violations of the Continental System by the Russians, which led Napoleon to threaten Alexander with serious consequences if he formed an alliance with Britain.
In an attempt to gain increased support from Polish nationalists and patriots, Napoleon termed the war the Second Polish War—the First Polish War had been the Bar Confederation uprising by Polish nobles against Russia in 1768. Polish patriots wanted the Russian part of Poland to be joined with the Duchy of Warsaw and an independent Poland created. This was rejected by Napoleon, who stated he had promised his ally Austria this would not happen. Napoleon refused to manumit the Russian serfs because of concerns this might provoke a reaction in his army's rear. The serfs later committed atrocities against French soldiers during France's retreat.
The Russians avoided Napoleon's objective of a decisive engagement and instead retreated deeper into Russia. A brief attempt at resistance was made at Smolensk in August; the Russians were defeated in a series of battles, and Napoleon resumed his advance. The Russians again avoided battle, although in a few cases this was only achieved because Napoleon uncharacteristically hesitated to attack when the opportunity arose. Owing to the Russian army's scorched earth tactics, the French found it increasingly difficult to forage food for themselves and their horses.
The Russians eventually offered battle outside Moscow on 7 September: the Battle of Borodino resulted in approximately 44,000 Russian and 35,000 French dead, wounded or captured, and may have been the bloodiest day of battle in history up to that point in time. Although the French had won, the Russian army had accepted, and withstood, the major battle Napoleon had hoped would be decisive. Napoleon's own account was: "The most terrible of all my battles was the one before Moscow. The French showed themselves to be worthy of victory, but the Russians showed themselves worthy of being invincible."
The Russian army withdrew and retreated past Moscow. Napoleon entered the city, assuming its fall would end the war and Alexander would negotiate peace. However, on orders of the city's governor Feodor Rostopchin, rather than capitulation, Moscow was burned. After five weeks, Napoleon and his army left. In early November Napoleon got concerned about loss of control back in France after the Malet coup of 1812. His army walked through snow up to their knees and nearly 10,000 men and horses froze to death on the night of 8/9 November alone. After Battle of Berezina Napoleon succeeded to escape but had to abandon much of the remaining artillery and baggage train. On 5 December, shortly before arriving in Vilnius, Napoleon left the army in a sledge.
The Allies offered peace terms in the Frankfurt proposals in November 1813. Napoleon would remain as Emperor of France, but it would be reduced to its "natural frontiers." That meant that France could retain control of Belgium, Savoy and the Rhineland (the west bank of the Rhine River), while giving up control of all the rest, including all of Spain and the Netherlands, and most of Italy and Germany. Metternich told Napoleon these were the best terms the Allies were likely to offer; after further victories, the terms would be harsher and harsher. Metternich's motivation was to maintain France as a balance against Russian threats, while ending the highly destabilizing series of wars.
Napoleon, expecting to win the war, delayed too long and lost this opportunity; by December the Allies had withdrawn the offer. When his back was to the wall in 1814 he tried to reopen peace negotiations on the basis of accepting the Frankfurt proposals. The Allies now had new, harsher terms that included the retreat of France to its 1791 boundaries, which meant the loss of Belgium. Napoleon would remain Emperor, however he rejected the term. The British wanted Napoleon permanently removed; they prevailed. Napoleon adamantly refused.
On 1 April, Alexander addressed the Sénat conservateur. Long docile to Napoleon, under Talleyrand's prodding it had turned against him. Alexander told the Sénat that the Allies were fighting against Napoleon, not France, and they were prepared to offer honorable peace terms if Napoleon were removed from power. The next day, the Sénat passed the Acte de déchéance de l'Empereur ("Emperor's Demise Act"), which declared Napoleon deposed. Napoleon had advanced as far as Fontainebleau when he learned that Paris was lost. When Napoleon proposed the army march on the capital, his senior officers and marshals mutinied. On 4 April, led by Ney, they confronted Napoleon. Napoleon asserted the army would follow him, and Ney replied the army would follow its generals. While the ordinary soldiers and regimental officers wanted to fight on, without any senior officers or marshals any prospective invasion of Paris would have been impossible. Bowing to the inevitable, on 4 April Napoleon abdicated in favour of his son, with Marie-Louise as regent. However, the Allies refused to accept this under prodding from Alexander, who feared that Napoleon might find an excuse to retake the throne. Napoleon was then forced to announce his unconditional abdication only two days later.
In the Treaty of Fontainebleau, the Allies exiled him to Elba, an island of 12,000 inhabitants in the Mediterranean, 20 km (12 mi) off the Tuscan coast. They gave him sovereignty over the island and allowed him to retain the title of Emperor. Napoleon attempted suicide with a pill he had carried after nearly being captured by the Russians during the retreat from Moscow. Its potency had weakened with age, however, and he survived to be exiled while his wife and son took refuge in Austria. In the first few months on Elba he created a small navy and army, developed the iron mines, and issued decrees on modern agricultural methods.
The 5th Regiment was sent to intercept him and made contact just south of Grenoble on March 7, 1815. Napoleon approached the regiment alone, dismounted his horse and, when he was within gunshot range, shouted to the soldiers, "Here I am. Kill your Emperor, if you wish." The soldiers quickly responded with, "Vive L'Empereur!" Ney, who had boasted to the restored Bourbon king, Louis XVIII, that he would bring Napoleon to Paris in an iron cage, affectionately kissed his former emperor and forgot his oath of allegiance to the Bourbon monarch. The two then marched together towards Paris with a growing army. The unpopular Louis XVIII fled to Belgium after realizing he had little political support. On March 13, the powers at the Congress of Vienna declared Napoleon an outlaw. Four days later, Great Britain, Russia, Austria, and Prussia each pledged to put 150,000 men into the field to end his rule.
Napoleon returned to Paris and found that both the legislature and the people had turned against him. Realizing his position was untenable, he abdicated on 22 June in favour of his son. He left Paris three days later and settled at Josephine's former palace in Malmaison (on the western bank of the Seine about 17 kilometres (11 mi) west of Paris). Even as Napoleon travelled to Paris, the Coalition forces crossed the frontier swept through France (arriving in the vicinity of Paris on 29 June), with the stated intent on restoring Louis XVIII to the French throne.
In 1840, Louis Philippe I obtained permission from the British to return Napoleon's remains to France. On 15 December 1840, a state funeral was held. The hearse proceeded from the Arc de Triomphe down the Champs-Élysées, across the Place de la Concorde to the Esplanade des Invalides and then to the cupola in St Jérôme's Chapel, where it remained until the tomb designed by Louis Visconti was completed. In 1861, Napoleon's remains were entombed in a porphyry sarcophagus in the crypt under the dome at Les Invalides.
In 1955, the diaries of Napoleon's valet, Louis Marchand, were published. His description of Napoleon in the months before his death led Sten Forshufvud in a 1961 paper in Nature to put forward other causes for his death, including deliberate arsenic poisoning. Arsenic was used as a poison during the era because it was undetectable when administered over a long period. Forshufvud, in a 1978 book with Ben Weider, noted that Napoleon's body was found to be remarkably well preserved when moved in 1840. Arsenic is a strong preservative, and therefore this supported the poisoning hypothesis. Forshufvud and Weider observed that Napoleon had attempted to quench abnormal thirst by drinking large amounts of orgeat syrup that contained cyanide compounds in the almonds used for flavouring.
They maintained that the potassium tartrate used in his treatment prevented his stomach from expelling these compounds and that his thirst was a symptom of the poison. Their hypothesis was that the calomel given to Napoleon became an overdose, which killed him and left extensive tissue damage behind. According to a 2007 article, the type of arsenic found in Napoleon's hair shafts was mineral, the most toxic, and according to toxicologist Patrick Kintz, this supported the conclusion that he was murdered.
There have been modern studies that have supported the original autopsy finding. In a 2008 study, researchers analysed samples of Napoleon's hair from throughout his life, as well as samples from his family and other contemporaries. All samples had high levels of arsenic, approximately 100 times higher than the current average. According to these researchers, Napoleon's body was already heavily contaminated with arsenic as a boy, and the high arsenic concentration in his hair was not caused by intentional poisoning; people were constantly exposed to arsenic from glues and dyes throughout their lives.[note 7] Studies published in 2007 and 2008 dismissed evidence of arsenic poisoning, and confirmed evidence of peptic ulcer and gastric cancer as the cause of death.
Napoleon had a civil marriage with Joséphine de Beauharnais, without religious ceremony. During the campaign in Egypt, Napoleon showed much tolerance towards religion for a revolutionary general, holding discussions with Muslim scholars and ordering religious celebrations, but General Dupuy, who accompanied Napoleon, revealed, shortly after Pope Pius VI's death, the political reasons for such behaviour: "We are fooling Egyptians with our pretended interest for their religion; neither Bonaparte nor we believe in this religion more than we did in Pius the Defunct's one".[note 8] In his memoirs, Bonaparte's secretary Bourienne wrote about Napoleon's religious interests in the same vein. His religious opportunism is epitomized in his famous quote: "It is by making myself Catholic that I brought peace to Brittany and Vendée. It is by making myself Italian that I won minds in Italy. It is by making myself a Moslem that I established myself in Egypt. If I governed a nation of Jews, I should reestablish the Temple of Solomon." However, according to Juan Cole, "Bonaparte's admiration for the Prophet Muhammad, in contrast, was genuine" and during his captivity on St Helena he defended him against Voltaire's critical play Mahomet.
Napoleon was crowned Emperor Napoleon I on 2 December 1804 at Notre Dame de Paris by Pope Pius VII. On 1 April 1810, Napoleon religiously married the Austrian princess Marie Louise. During his brother's rule in Spain, he abolished the Spanish Inquisition in 1813. In a private discussion with general Gourgaud during his exile on Saint Helena, Napoleon expressed materialistic views on the origin of man,[note 9]and doubted the divinity of Jesus, stating that it is absurd to believe that Socrates, Plato, Muslims, and the Anglicans should be damned for not being Roman Catholics.[note 10] He also said to Gourgaud in 1817 "I like the Mohammedan religion best. It has fewer incredible things in it than ours." and that "the Mohammedan religion is the finest of all." However, Napoleon was anointed by a priest before his death.
Seeking national reconciliation between revolutionaries and Catholics, the Concordat of 1801 was signed on 15 July 1801 between Napoleon and Pope Pius VII. It solidified the Roman Catholic Church as the majority church of France and brought back most of its civil status. The hostility of devout Catholics against the state had now largely been resolved. It did not restore the vast church lands and endowments that had been seized during the revolution and sold off. As a part of the Concordat, he presented another set of laws called the Organic Articles.
While the Concordat restored much power to the papacy, the balance of church-state relations had tilted firmly in Napoleon's favour. He selected the bishops and supervised church finances. Napoleon and the pope both found the Concordat useful. Similar arrangements were made with the Church in territories controlled by Napoleon, especially Italy and Germany. Now, Napoleon could win favor with the Catholics while also controlling Rome in a political sense. Napoleon said in April 1801, "Skillful conquerors have not got entangled with priests. They can both contain them and use them." French children were issued a catechism that taught them to love and respect Napoleon.
Historians agree that Napoleon's remarkable personality was one key to his influence. They emphasize the strength of his ambition that took him from an obscure village to command of most of Europe. George F. E. Rudé stresses his "rare combination of will, intellect and physical vigour." At 5 ft 6 in (168 cm), he was not physically imposing but in one-on-one situations he typically had a hypnotic impact on people and seemingly bent the strongest leaders to his will. He understood military technology, but was not an innovator in that regard. He was an innovator in using the financial, bureaucratic, and diplomatic resources of France. He could rapidly dictate a series of complex commands to his subordinates, keeping in mind where major units were expected to be at each future point, and like a chess master, "seeing" the best plays moves ahead.
Napoleon maintained strict, efficient work habits, prioritizing what needed to be done. He cheated at cards, but repaid the losses; he had to win at everything he attempted. He kept relays of staff and secretaries at work. Unlike many generals, Napoleon did not examine history to ask what Hannibal or Alexander or anyone else did in a similar situation. Critics said he won many battles simply because of luck; Napoleon responded, "Give me lucky generals," aware that "luck" comes to leaders who recognize opportunity, and seize it. Dwyer argues that Napoleon's victories at Austerlitz and Jena in 1805-06 heightened his sense of self-grandiosity, leaving him even more certain of his destiny and invincibility. By the Russian campaign in 1812, however, Napoleon seems to have lost his verve. With crisis after crisis at hand, he rarely rose to the occasion. Some historians have suggested a physical deterioration, but others note that an impaired Napoleon was still a brilliant general.
In terms of impact on events, it was more than Napoleon's personality that took effect. He reorganized France itself to supply the men and money needed for great wars. Above all he inspired his men—Wellington said his presence on the battlefield was worth 40,000 soldiers, for he inspired confidence from privates to field marshals. He also unnerved the enemy. At the Battle of Auerstadt in 1806, King Frederick William III of Prussia outnumbered the French by 63,000 to 27,000; however, when he mistakenly was told that Napoleon was in command, he ordered a hasty retreat that turned into a rout. The force of his personality neutralized material difficulties as his soldiers fought with the confidence that with Napoleon in charge they would surely win.
During the Napoleonic Wars he was taken seriously by the British press as a dangerous tyrant, poised to invade. He was often referred to by the British as Boney. A nursery rhyme warned children that Bonaparte ravenously ate naughty people; the "bogeyman". The British Tory press sometimes depicted Napoleon as much smaller than average height, and this image persists. Confusion about his height also results from the difference between the French pouce and British inch—2.71 cm and 2.54 cm, respectively. The myth of the "Napoleon Complex” — named after him to describe men who have an inferiority complex — stems primarily from the fact that he was listed, incorrectly, as 5 feet 2 inches (in French units) at the time of his death. In fact, he was 1.68 metres (5 ft 6 in) tall, an average height for a man in that period.[note 11]
When he became First Consul and later Emperor, Napoleon eschewed his general's uniform and habitually wore the simple green colonel uniform (non-Hussar) of a colonel of the Chasseur à Cheval of the Imperial Guard, the regiment that often served as his personal escort, with a large bicorne. He also habitually wore (usually on Sundays) the blue uniform of a colonel of the Imperial Guard Foot Grenadiers (blue with white facings and red cuffs). He also wore his Légion d'honneur star, medal and ribbon, and the Order of the Iron Crown decorations, white French-style culottes and white stockings. This was in contrast to the gorgeous and complex uniforms with many decorations of his marshals and those around him.
Napoleon instituted lasting reforms, including higher education, a tax code, road and sewer systems, and established the Banque de France, the first central bank in French history. He negotiated the Concordat of 1801 with the Catholic Church, which sought to reconcile the mostly Catholic population to his regime. It was presented alongside the Organic Articles, which regulated public worship in France. His dissolution of the Holy Roman Empire paved the way to German Unification later in the 19th century. The sale of the Louisiana Territory to the United States doubled the size of the country and was a major event in American history.
Napoleon's set of civil laws, the Code Civil—now often known as the Napoleonic Code—was prepared by committees of legal experts under the supervision of Jean Jacques Régis de Cambacérès, the Second Consul. Napoleon participated actively in the sessions of the Council of State that revised the drafts. The development of the code was a fundamental change in the nature of the civil law legal system with its stress on clearly written and accessible law. Other codes ("Les cinq codes") were commissioned by Napoleon to codify criminal and commerce law; a Code of Criminal Instruction was published, which enacted rules of due process.
His opponents learned from Napoleon's innovations. The increased importance of artillery after 1807 stemmed from his creation of a highly mobile artillery force, the growth in artillery numbers, and changes in artillery practices. As a result of these factors, Napoleon, rather than relying on infantry to wear away the enemy's defenses, now could use massed artillery as a spearhead to pound a break in the enemy's line that was then exploited by supporting infantry and cavalry. McConachy rejects the alternative theory that growing reliance on artillery by the French army beginning in 1807 was an outgrowth of the declining quality of the French infantry and, later, France's inferiority in cavalry numbers. Weapons and other kinds of military technology remained largely static through the Revolutionary and Napoleonic eras, but 18th-century operational mobility underwent significant change.
The official introduction of the metric system in September 1799 was unpopular in large sections of French society. Napoleon's rule greatly aided adoption of the new standard not only across France but also across the French sphere of influence. Napoleon ultimately took a retrograde step in 1812 when he passed legislation to introduce the mesures usuelles (traditional units of measurement) for retail trade—a system of measure that resembled the pre-revolutionary units but were based on the kilogram and the metre; for example the livre metrique (metric pound) was 500 g instead of 489.5 g—the value of the livre du roi (the king's pound). Other units of measure were rounded in a similar manner. This however laid the foundations for the definitive introduction of the metric system across Europe in the middle of the 19th century.
Napoleon's educational reforms laid the foundation of a modern system of education in France and throughout much of Europe. Napoleon synthesized the best academic elements from the Ancien Régime, The Enlightenment, and the Revolution, with the aim of establishing a stable, well-educated and prosperous society. He made French the only official language. He left some primary education in the hands of religious orders, but he offered public support to secondary education. Napoleon founded a number of state secondary schools (lycées) designed to produce a standardized education that was uniform across France. All students were taught the sciences along with modern and classical languages. Unlike the system during the Ancien Régime, religious topics did not dominate the curriculum, although they were present in addition to teachers from the clergy. Napoleon simply hoped to use religion to produce social stability. He gave special attention to the advanced centers, notably the École Polytechnique, that provided both military expertise and state-of-the-art research in science. Napoleon made some of the first major efforts at establishing a system of secular and public education. The system featured scholarships and strict discipline, with the result being a French educational system that outperformed its European counterparts, many of which borrowed from the French system.
In the political realm, historians debate whether Napoleon was "an enlightened despot who laid the foundations of modern Europe or, instead, a megalomaniac who wrought greater misery than any man before the coming of Hitler." Many historians have concluded that he had grandiose foreign policy ambitions. The Continental powers as late as 1808 were willing to give him nearly all of his remarkable gains and titles, but some scholars maintain he was overly aggressive and pushed for too much, until his empire collapsed.
Napoleon ended lawlessness and disorder in post-Revolutionary France. He was, however, considered a tyrant and usurper by his opponents. His critics charge that he was not significantly troubled when faced with the prospect of war and death for thousands, turned his search for undisputed rule into a series of conflicts throughout Europe and ignored treaties and conventions alike. His role in the Haitian Revolution and decision to reinstate slavery in France's oversea colonies are controversial and have an impact on his reputation.
Napoleon institutionalised plunder of conquered territories: French museums contain art stolen by Napoleon's forces from across Europe. Artefacts were brought to the Musée du Louvre for a grand central museum; his example would later serve as inspiration for more notorious imitators. He was compared to Adolf Hitler most famously by the historian Pieter Geyl in 1947 and Claude Ribbe in 2005. David G. Chandler, a foremost historian of Napoleonic warfare, wrote in 1973 that, "Nothing could be more degrading to the former [Napoleon] and more flattering to the latter [Hitler]. The comparison is odious. On the whole Napoleon was inspired by a noble dream, wholly dissimilar from Hitler's... Napoleon left great and lasting testimonies to his genius—in codes of law and national identities which survive to the present day. Adolf Hitler left nothing but destruction."
Critics argue Napoleon's true legacy must reflect the loss of status for France and needless deaths brought by his rule: historian Victor Davis Hanson writes, "After all, the military record is unquestioned—17 years of wars, perhaps six million Europeans dead, France bankrupt, her overseas colonies lost." McLynn notes that, "He can be viewed as the man who set back European economic life for a generation by the dislocating impact of his wars." However, Vincent Cronin replies that such criticism relies on the flawed premise that Napoleon was responsible for the wars which bear his name, when in fact France was the victim of a series of coalitions which aimed to destroy the ideals of the Revolution.
Napoleon's use of propaganda contributed to his rise to power, legitimated his régime, and established his image for posterity. Strict censorship, controlling aspects of the press, books, theater, and art, was part of his propaganda scheme, aimed at portraying him as bringing desperately wanted peace and stability to France. The propagandistic rhetoric changed in relation to events and to the atmosphere of Napoleon's reign, focusing first on his role as a general in the army and identification as a soldier, and moving to his role as emperor and a civil leader. Specifically targeting his civilian audience, Napoleon fostered an important, though uneasy, relationship with the contemporary art community, taking an active role in commissioning and controlling different forms of art production to suit his propaganda goals.
Widespread rumors of Napoleon's return from St. Helena and Napoleon as an inspiration for patriotism, individual and collective liberties, and political mobilization manifested themselves in seditious materials, displaying the tricolor and rosettes. There were also subversive activities celebrating anniversaries of Napoleon's life and reign and disrupting royal celebrations—they demonstrated the prevailing and successful goal of the varied supporters of Napoleon to constantly destabilize the Bourbon regime.
Datta (2005) shows that, following the collapse of militaristic Boulangism in the late 1880s, the Napoleonic legend was divorced from party politics and revived in popular culture. Concentrating on two plays and two novels from the period—Victorien Sardou's Madame Sans-Gêne (1893), Maurice Barrès's Les Déracinés (1897), Edmond Rostand's L'Aiglon (1900), and André de Lorde and Gyp's Napoléonette (1913) Datta examines how writers and critics of the Belle Époque exploited the Napoleonic legend for diverse political and cultural ends.
After the fall of Napoleon, not only was Napoleonic Code retained by conquered countries including the Netherlands, Belgium, parts of Italy and Germany, but has been used as the basis of certain parts of law outside Europe including the Dominican Republic, the US state of Louisiana and the Canadian province of Quebec. The memory of Napoleon in Poland is favorable, for his support for independence and opposition to Russia, his legal code, the abolition of serfdom, and the introduction of modern middle class bureaucracies.
Napoleon could be considered one of the founders of modern Germany. After dissolving the Holy Roman Empire, he reduced the number of German states from 300 to less than 50, paving the way to German Unification. A byproduct of the French occupation was a strong development in German nationalism. Napoleon also significantly aided the United States when he agreed to sell the territory of Louisiana for 15 million dollars during the presidency of Thomas Jefferson. That territory almost doubled the size of the United States, adding the equivalent of 13 states to the Union.
Napoleon married Joséphine de Beauharnais in 1796, when he was 26; she was a 32-year-old widow whose first husband had been executed during the Revolution. Until she met Bonaparte, she had been known as "Rose", a name which he disliked. He called her "Joséphine" instead, and she went by this name henceforth. Bonaparte often sent her love letters while on his campaigns. He formally adopted her son Eugène and cousin Stéphanie and arranged dynastic marriages for them. Joséphine had her daughter Hortense marry Napoleon's brother Louis.
Napoleon acknowledged one illegitimate son: Charles Léon (1806–1881) by Eléonore Denuelle de La Plaigne. Alexandre Colonna-Walewski (1810–1868), the son of his mistress Maria Walewska, although acknowledged by Walewska's husband, was also widely known to be his child, and the DNA of his direct male descendant has been used to help confirm Napoleon's Y-chromosome haplotype. He may have had further unacknowledged illegitimate offspring as well, such as Eugen Megerle von Mühlfeld by Emilie Victoria Kraus and Hélène Napoleone Bonaparte (1816–1907) by Albine de Montholon.
The Roman Republic (Latin: Res publica Romana; Classical Latin: [ˈreːs ˈpuːb.lɪ.ka roːˈmaː.na]) was the period of ancient Roman civilization beginning with the overthrow of the Roman Kingdom, traditionally dated to 509 BC, and ending in 27 BC with the establishment of the Roman Empire. It was during this period that Rome's control expanded from the city's immediate surroundings to hegemony over the entire Mediterranean world. During the first two centuries of its existence, the Roman Republic expanded through a combination of conquest and alliance, from central Italy to the entire Italian peninsula. By the following century, it included North Africa, Spain, and what is now southern France. Two centuries after that, towards the end of the 1st century BC, it included the rest of modern France, Greece, and much of the eastern Mediterranean. By this time, internal tensions led to a series of civil wars, culminating with the assassination of Julius Caesar, which led to the transition from republic to empire. The exact date of transition can be a matter of interpretation. Historians have variously proposed Julius Caesar's crossing of the Rubicon River in 49 BC, Caesar's appointment as dictator for life in 44 BC, and the defeat of Mark Antony and Cleopatra at the Battle of Actium in 31 BC. However, most use the same date as did the ancient Romans themselves, the Roman Senate's grant of extraordinary powers to Octavian and his adopting the title Augustus in 27 BC, as the defining event ending the Republic.
Roman government was headed by two consuls, elected annually by the citizens and advised by a senate composed of appointed magistrates. As Roman society was very hierarchical by modern standards, the evolution of the Roman government was heavily influenced by the struggle between the patricians, Rome's land-holding aristocracy, who traced their ancestry to the founding of Rome, and the plebeians, the far more numerous citizen-commoners. Over time, the laws that gave patricians exclusive rights to Rome's highest offices were repealed or weakened, and leading plebeian families became full members of the aristocracy. The leaders of the Republic developed a strong tradition and morality requiring public service and patronage in peace and war, making military and political success inextricably linked. Many of Rome's legal and legislative structures (later codified into the Justinian Code, and again into the Napoleonic Code) can still be observed throughout Europe and much of the world in modern nation states and international organizations.
The exact causes and motivations for Rome's military conflicts and expansions during the republic are subject to wide debate. While they can be seen as motivated by outright aggression and imperialism, historians typically take a much more nuanced view. They argue that Rome's expansion was driven by short-term defensive and inter-state factors (that is, relations with city-states and kingdoms outside Rome's hegemony), and the new contingencies that these decisions created. In its early history, as Rome successfully defended itself against foreign threats in central and then northern Italy, neighboring city-states sought the protection a Roman alliance would bring. As such, early republican Rome was not an "empire" or "state" in the modern sense, but an alliance of independent city-states (similar to the Greek hegemonies of the same period) with varying degrees of genuine independence (which itself changed over time) engaged in an alliance of mutual self-protection, but led by Rome. With some important exceptions, successful wars in early republican Rome generally led not to annexation or military occupation, but to the restoration of the way things were. But the defeated city would be weakened (sometimes with outright land concessions) and thus less able to resist Romanizing influences, such as Roman settlers seeking land or trade with the growing Roman confederacy. It was also less able to defend itself against its non-Roman enemies, which made attack by these enemies more likely. It was, therefore, more likely to seek an alliance of protection with Rome.
This growing coalition expanded the potential enemies that Rome might face, and moved Rome closer to confrontation with major powers. The result was more alliance-seeking, on the part of both the Roman confederacy and city-states seeking membership (and protection) within that confederacy. While there were exceptions to this (such as military rule of Sicily after the First Punic War), it was not until after the Second Punic War that these alliances started to harden into something more like an empire, at least in certain locations. This shift mainly took place in parts of the west, such as the southern Italian towns that sided with Hannibal.
In contrast, Roman expansion into Spain and Gaul occurred as a mix of alliance-seeking and military occupation. In the 2nd century BC, Roman involvement in the Greek east remained a matter of alliance-seeking, but this time in the face of major powers that could rival Rome. According to Polybius, who sought to trace how Rome came to dominate the Greek east in less than a century, this was mainly a matter of several Greek city-states seeking Roman protection against the Macedonian kingdom and Seleucid Empire in the face of destabilisation created by the weakening of Ptolemaic Egypt. In contrast to the west, the Greek east had been dominated by major empires for centuries, and Roman influence and alliance-seeking led to wars with these empires that further weakened them and therefore created an unstable power vacuum that only Rome could fill. This had some important similarities to (and important differences from) the events in Italy centuries earlier, but this time on a global scale.
Historians see the growing Roman influence over the east, as with the west, as not a matter of intentional empire-building, but constant crisis management narrowly focused on short-term goals within a highly unstable, unpredictable, and inter-dependent network of alliances and dependencies. With some major exceptions of outright military rule, the Roman Republic remained an alliance of independent city-states and kingdoms (with varying degrees of independence, both de jure and de facto) until it transitioned into the Roman Empire. It was not until the time of the Roman Empire that the entire Roman world was organized into provinces under explicit Roman control.
The first Roman republican wars were wars of both expansion and defence, aimed at protecting Rome itself from neighbouring cities and nations and establishing its territory in the region. Initially, Rome's immediate neighbours were either Latin towns and villages, or else tribal Sabines from the Apennine hills beyond. One by one Rome defeated both the persistent Sabines and the local cities, both those under Etruscan control and those that had cast off their Etruscan rulers. Rome defeated Latin cities in the Battle of Lake Regillus in 496 BC, the Battle of Mons Algidus in 458 BC, the Battle of Corbione in 446 BC, the Battle of Aricia, and especially the Battle of the Cremera in 477 BC wherein it fought against the most important Etruscan city of Veii.
By 390 BC, several Gallic tribes were invading Italy from the north as their culture expanded throughout Europe. The Romans were alerted to this when a particularly warlike tribe invaded two Etruscan towns close to Rome's sphere of influence. These towns, overwhelmed by the enemy's numbers and ferocity, called on Rome for help. The Romans met the Gauls in pitched battle at the Battle of Allia River around 390–387 BC. The Gauls, led by chieftain Brennus, defeated the Roman army of approximately 15,000 troops, pursued the fleeing Romans back to Rome, and sacked the city before being either driven off or bought off. Romans and Gauls continued to war intermittently in Italy for more than two centuries.[relevant? – discuss]
After recovering surprisingly fast from the sack of Rome, the Romans immediately resumed their expansion within Italy. The First Samnite War from 343 BC to 341 BC was relatively short: the Romans beat the Samnites in two battles, but were forced to withdraw before they could pursue the conflict further due to the revolt of several of their Latin allies in the Latin War. Rome beat the Latins in the Battle of Vesuvius and again in the Battle of Trifanum, after which the Latin cities were obliged to submit to Roman rule.
Despite early victories, Pyrrhus found his position in Italy untenable. Rome steadfastly refused to negotiate with Pyrrhus as long as his army remained in Italy. Facing unacceptably heavy losses from each encounter with the Roman army, Pyrrhus withdrew from the peninsula (hence the term "Pyrrhic victory"). In 275 BC, Pyrrhus again met the Roman army at the Battle of Beneventum. While Beneventum was indecisive, Pyrrhus realised his army had been exhausted and reduced by years of foreign campaigns. Seeing little hope for further gains, he withdrew completely from Italy.
The first few naval battles were disasters for Rome. However, after training more sailors and inventing a grappling engine, a Roman naval force was able to defeat a Carthaginian fleet, and further naval victories followed. The Carthaginians then hired Xanthippus of Carthage, a Spartan mercenary general, to reorganise and lead their army. He cut off the Roman army from its base by re-establishing Carthaginian naval supremacy. The Romans then again defeated the Carthaginians in naval battle at the Battle of the Aegates Islands and left Carthage with neither a fleet nor sufficient coin to raise one. For a maritime power the loss of their access to the Mediterranean stung financially and psychologically, and the Carthaginians sued for peace.
The Romans held off Hannibal in three battles, but then Hannibal smashed a succession of Roman consular armies. By this time Hannibal's brother Hasdrubal Barca sought to cross the Alps into Italy and join his brother with a second army. Hasdrubal managed to break through into Italy only to be defeated decisively on the Metaurus River. Unable to defeat Hannibal on Italian soil, the Romans boldly sent an army to Africa under Scipio Africanus to threaten the Carthaginian capital. Hannibal was recalled to Africa, and defeated at the Battle of Zama.
Carthage never recovered militarily after the Second Punic War, but quickly economically and the Third Punic War that followed was in reality a simple punitive mission after the neighbouring Numidians allied to Rome robbed/attacked Carthaginian merchants. Treaties had forbidden any war with Roman allies, and defense against robbing/pirates was considered as "war action": Rome decided to annihilate the city of Carthage. Carthage was almost defenceless, and submitted when besieged. However, the Romans demanded complete surrender and moval of the city into the (desert) inland far off any coastal or harbour region, and the Carthaginians refused. The city was besieged, stormed, and completely destroyed. Ultimately, all of Carthage's North African and Iberian territories were acquired by Rome. Note that "Carthage" was not an 'empire', but a league of punic colonies (port cities in the western mediterranean) like the 1st and 2nd Athenian ("attic") leagues, under leadership of Carthage. Punic Carthago was gone, but the other punic cities in the western mediterranean flourished under Roman rule.
Rome's preoccupation with its war with Carthage provided an opportunity for Philip V of the kingdom of Macedonia, located in the north of the Greek peninsula, to attempt to extend his power westward. Philip sent ambassadors to Hannibal's camp in Italy, to negotiate an alliance as common enemies of Rome. However, Rome discovered the agreement when Philip's emissaries were captured by a Roman fleet. The First Macedonian War saw the Romans involved directly in only limited land operations, but they ultimately achieved their objective of pre-occupying Philip and preventing him from aiding Hannibal.
The past century had seen the Greek world dominated by the three primary successor kingdoms of Alexander the Great's empire: Ptolemaic Egypt, Macedonia and the Seleucid Empire. In 202 BC, internal problems led to a weakening of Egypt's position, thereby disrupting the power balance among the successor states. Macedonia and the Seleucid Empire agreed to an alliance to conquer and divide Egypt. Fearing this increasingly unstable situation, several small Greek kingdoms sent delegations to Rome to seek an alliance. The delegation succeeded, even though prior Greek attempts to involve Rome in Greek affairs had been met with Roman apathy. Our primary source about these events, the surviving works of Polybius, do not state Rome's reason for getting involved. Rome gave Philip an ultimatum to cease his campaigns against Rome's new Greek allies. Doubting Rome's strength (a reasonable doubt, given Rome's performance in the First Macedonian War) Philip ignored the request, and Rome sent an army of Romans and Greek allies, beginning the Second Macedonian War. Despite his recent successes against the Greeks and earlier successes against Rome, Philip's army buckled under the pressure from the Roman-Greek army. In 197 BC, the Romans decisively defeated Philip at the Battle of Cynoscephalae, and Philip was forced to give up his recent Greek conquests. The Romans declared the "Peace of the Greeks", believing that Philip's defeat now meant that Greece would be stable. They pulled out of Greece entirely, maintaining minimal contacts with their Greek allies.
With Egypt and Macedonia weakened, the Seleucid Empire made increasingly aggressive and successful attempts to conquer the entire Greek world. Now not only Rome's allies against Philip, but even Philip himself, sought a Roman alliance against the Seleucids. The situation was made worse by the fact that Hannibal was now a chief military advisor to the Seleucid emperor, and the two were believed to be planning an outright conquest not just of Greece, but of Rome itself. The Seleucids were much stronger than the Macedonians had ever been, because they controlled much of the former Persian Empire, and by now had almost entirely reassembled Alexander the Great's former empire.
Fearing the worst, the Romans began a major mobilization, all but pulling out of recently pacified Spain and Gaul. They even established a major garrison in Sicily in case the Seleucids ever got to Italy. This fear was shared by Rome's Greek allies, who had largely ignored Rome in the years after the Second Macedonian War, but now followed Rome again for the first time since that war. A major Roman-Greek force was mobilized under the command of the great hero of the Second Punic War, Scipio Africanus, and set out for Greece, beginning the Roman-Syrian War. After initial fighting that revealed serious Seleucid weaknesses, the Seleucids tried to turn the Roman strength against them at the Battle of Thermopylae (as they believed the 300 Spartans had done centuries earlier). Like the Spartans, the Seleucids lost the battle, and were forced to evacuate Greece. The Romans pursued the Seleucids by crossing the Hellespont, which marked the first time a Roman army had ever entered Asia. The decisive engagement was fought at the Battle of Magnesia, resulting in a complete Roman victory. The Seleucids sued for peace, and Rome forced them to give up their recent Greek conquests. Although they still controlled a great deal of territory, this defeat marked the decline of their empire, as they were to begin facing increasingly aggressive subjects in the east (the Parthians) and the west (the Greeks). Their empire disintegrated into a rump over the course of the next century, when it was eclipsed by Pontus. Following Magnesia, Rome again withdrew from Greece, assuming (or hoping) that the lack of a major Greek power would ensure a stable peace. In fact, it did the opposite.
In 179 BC Philip died. His talented and ambitious son, Perseus, took the throne and showed a renewed interest in conquering Greece. With her Greek allies facing a major new threat, Rome declared war on Macedonia again, starting the Third Macedonian War. Perseus initially had some success against the Romans. However, Rome responded by sending a stronger army. This second consular army decisively defeated the Macedonians at the Battle of Pydna in 168 BC and the Macedonians duly capitulated, ending the war.
Convinced now that the Greeks (and therefore the rest of the region) would not have peace if left alone, Rome decided to establish its first permanent foothold in the Greek world, and divided the Kingdom of Macedonia into four client republics. Yet, Macedonian agitation continued. The Fourth Macedonian War, 150 to 148 BC, was fought against a Macedonian pretender to the throne who was again destabilizing Greece by trying to re-establish the old kingdom. The Romans swiftly defeated the Macedonians at the Second battle of Pydna.
The Jugurthine War of 111–104 BC was fought between Rome and Jugurtha of the North African kingdom of Numidia. It constituted the final Roman pacification of Northern Africa, after which Rome largely ceased expansion on the continent after reaching natural barriers of desert and mountain. Following Jugurtha's usurpation of the throne of Numidia, a loyal ally of Rome since the Punic Wars, Rome felt compelled to intervene. Jugurtha impudently bribed the Romans into accepting his usurpation. Jugurtha was finally captured not in battle but by treachery.
In 121 BC, Rome came into contact with two Celtic tribes (from a region in modern France), both of which they defeated with apparent ease. The Cimbrian War (113–101 BC) was a far more serious affair than the earlier clashes of 121 BC. The Germanic tribes of the Cimbri and the Teutons migrated from northern Europe into Rome's northern territories, and clashed with Rome and her allies. At the Battle of Aquae Sextiae and the Battle of Vercellae both tribes were virtually annihilated, which ended the threat.
The extensive campaigning abroad by Roman generals, and the rewarding of soldiers with plunder on these campaigns, led to a general trend of soldiers becoming increasingly loyal to their generals rather than to the state. Rome was also plagued by several slave uprisings during this period, in part because vast tracts of land had been given over to slave farming in which the slaves greatly outnumbered their Roman masters. In the 1st century BC at least twelve civil wars and rebellions occurred. This pattern continued until 27 BC, when Octavian (later Augustus) successfully challenged the Senate's authority, and was made princeps (first citizen).
Between 135 BC and 71 BC there were three "Servile Wars" involving slave uprisings against the Roman state. The third and final uprising was the most serious, involving ultimately between 120,000 and 150,000 slaves under the command of the gladiator Spartacus. In 91 BC the Social War broke out between Rome and its former allies in Italy when the allies complained that they shared the risk of Rome's military campaigns, but not its rewards. Although they lost militarily, the allies achieved their objectives with legal proclamations which granted citizenship to more than 500,000 Italians.
The internal unrest reached its most serious state, however, in the two civil wars that were caused by the clash between generals Gaius Marius and Lucius Cornelius Sulla starting from 88 BC. In the Battle of the Colline Gate at the very door of the city of Rome, a Roman army under Sulla bested an army of the Marius supporters and entered the city. Sulla's actions marked a watershed in the willingness of Roman troops to wage war against one another that was to pave the way for the wars which ultimately overthrew the Republic, and caused the founding of the Roman Empire.
Mithridates the Great was the ruler of Pontus, a large kingdom in Asia Minor (modern Turkey), from 120 to 63 BC. Mithridates antagonised Rome by seeking to expand his kingdom, and Rome for her part seemed equally eager for war and the spoils and prestige that it might bring. In 88 BC, Mithridates ordered the killing of a majority of the 80,000 Romans living in his kingdom. The massacre was the official reason given for the commencement of hostilities in the First Mithridatic War. The Roman general Lucius Cornelius Sulla forced Mithridates out of Greece proper, but then had to return to Italy to answer the internal threat posed by his rival, Gaius Marius. A peace was made between Rome and Pontus, but this proved only a temporary lull.
During his term as praetor in the Iberian Peninsula (modern Portugal and Spain), Pompey's contemporary Julius Caesar defeated two local tribes in battle. After his term as consul in 59 BC, he was appointed to a five-year term as the proconsular Governor of Cisalpine Gaul (part of current northern Italy), Transalpine Gaul (current southern France) and Illyria (part of the modern Balkans). Not content with an idle governorship, Caesar strove to find reason to invade Gaul (modern France and Belgium), which would give him the dramatic military success he sought. When two local tribes began to migrate on a route that would take them near (not into) the Roman province of Transalpine Gaul, Caesar had the barely sufficient excuse he needed for his Gallic Wars, fought between 58 BC and 49 BC.
By 59 BC an unofficial political alliance known as the First Triumvirate was formed between Gaius Julius Caesar, Marcus Licinius Crassus, and Gnaeus Pompeius Magnus ("Pompey the Great") to share power and influence. In 53 BC, Crassus launched a Roman invasion of the Parthian Empire (modern Iraq and Iran). After initial successes, he marched his army deep into the desert; but here his army was cut off deep in enemy territory, surrounded and slaughtered at the Battle of Carrhae in which Crassus himself perished. The death of Crassus removed some of the balance in the Triumvirate and, consequently, Caesar and Pompey began to move apart. While Caesar was fighting in Gaul, Pompey proceeded with a legislative agenda for Rome that revealed that he was at best ambivalent towards Caesar and perhaps now covertly allied with Caesar's political enemies. In 51 BC, some Roman senators demanded that Caesar not be permitted to stand for consul unless he turned over control of his armies to the state, which would have left Caesar defenceless before his enemies. Caesar chose civil war over laying down his command and facing trial.
By the spring of 49 BC, the hardened legions of Caesar crossed the river Rubicon, the legal boundary of Roman Italy beyond which no commander might bring his army, and swept down the Italian peninsula towards Rome, while Pompey ordered the abandonment of Rome. Afterwards Caesar turned his attention to the Pompeian stronghold of Hispania (modern Spain) but decided to tackle Pompey himself in Greece. Pompey initially defeated Caesar, but failed to follow up on the victory, and was decisively defeated at the Battle of Pharsalus in 48 BC, despite outnumbering Caesar's forces two to one, albeit with inferior quality troops. Pompey fled again, this time to Egypt, where he was murdered.
Caesar was now the primary figure of the Roman state, enforcing and entrenching his powers. His enemies feared that he had ambitions to become an autocratic ruler. Arguing that the Roman Republic was in danger, a group of senators hatched a conspiracy and assassinated Caesar at a meeting of the Senate in March 44 BC.  Mark Antony, Caesar's lieutenant, condemned Caesar's assassination, and war broke out between the two factions. Antony was denounced as a public enemy, and Caesar's adopted son and chosen heir, Gaius Octavianus, was entrusted with the command of the war against him. At the Battle of Mutina Mark Antony was defeated by the consuls Hirtius and Pansa, who were both killed.
However, civil war flared again when the Second Triumvirate of Octavian, Lepidus and Mark Antony failed. The ambitious Octavian built a power base of patronage and then launched a campaign against Mark Antony. At the naval Battle of Actium off the coast of Greece, Octavian decisively defeated Antony and Cleopatra. Octavian was granted a series of special powers including sole "imperium" within the city of Rome, permanent consular powers and credit for every Roman military victory, since all future generals were assumed to be acting under his command. In 27 BC Octavian was granted the use of the names "Augustus" and "Princeps", indicating his primary status above all other Romans, and he adopted the title "Imperator Caesar" making him the first Roman Emperor.
The last king of the Roman Kingdom, Lucius Tarquinius Superbus, was overthrown in 509 BC by a group of noblemen led by Lucius Junius Brutus. Tarquin made a number of attempts to retake the throne, including the Tarquinian conspiracy, the war with Veii and Tarquinii and finally the war between Rome and Clusium, all of which failed to achieve Tarquin's objectives. The most important constitutional change during the transition from kingdom to republic concerned the chief magistrate. Before the revolution, a king would be elected by the senators for a life term. Now, two consuls were elected by the citizens for an annual term. Each consul would check his colleague, and their limited term in office would open them up to prosecution if they abused the powers of their office. Consular political powers, when exercised conjointly with a consular colleague, were no different from those of the old king.
In 494 BC, the city was at war with two neighboring tribes. The plebeian soldiers refused to march against the enemy, and instead seceded to the Aventine Hill. The plebeians demanded the right to elect their own officials. The patricians agreed, and the plebeians returned to the battlefield. The plebeians called these new officials "plebeian tribunes". The tribunes would have two assistants, called "plebeian aediles". During the 5th century BC, a series of reforms were passed. The result of these reforms was that any law passed by the plebeian would have the full force of law. In 443 BC, the censorship was created. From 375 BC to 371 BC, the republic experienced a constitutional crisis during which the tribunes used their vetoes to prevent the election of senior magistrates.
After the consulship had been opened to the plebeians, the plebeians were able to hold both the dictatorship and the censorship. Plebiscites of 342 BC placed limits on political offices; an individual could hold only one office at a time, and ten years must elapse between the end of his official term and his re-election. Further laws attempted to relieve the burden of debt from plebeians by banning interest on loans. In 337 BC, the first plebeian praetor was elected. During these years, the tribunes and the senators grew increasingly close. The senate realised the need to use plebeian officials to accomplish desired goals. To win over the tribunes, the senators gave the tribunes a great deal of power and the tribunes began to feel obligated to the senate. As the tribunes and the senators grew closer, plebeian senators were often able to secure the tribunate for members of their own families. In time, the tribunate became a stepping stone to higher office.
Shortly before 312 BCE, the Plebeian Council enacted the Plebiscitum Ovinium. During the early republic, only consuls could appoint new senators. This initiative, however, transferred this power to the censors. It also required the censor to appoint any newly elected magistrate to the senate. By this point, plebeians were already holding a significant number of magisterial offices. Thus, the number of plebeian senators probably increased quickly. However, it remained difficult for a plebeian to enter the senate if he was not from a well-known political family, as a new patrician-like plebeian aristocracy emerged. The old nobility existed through the force of law, because only patricians were allowed to stand for high office. The new nobility existed due to the organization of society. As such, only a revolution could overthrow this new structure.
By 287 BC, the economic condition of the average plebeian had become poor. The problem appears to have centered around widespread indebtedness. The plebeians demanded relief, but the senators refused to address their situation. The result was the final plebeian secession. The plebeians seceded to the Janiculum hill. To end the secession, a dictator was appointed. The dictator passed a law (the Lex Hortensia), which ended the requirement that the patrician senators must agree before any bill could be considered by the Plebeian Council. This was not the first law to require that an act of the Plebeian Council have the full force of law. The Plebeian Council acquired this power during a modification to the original Valerian law in 449 BC. The significance of this law was in the fact that it robbed the patricians of their final weapon over the plebeians. The result was that control over the state fell, not onto the shoulders of voters, but to the new plebeian nobility.
The plebeians had finally achieved political equality with the patricians. However, the plight of the average plebeian had not changed. A small number of plebeian families achieved the same standing that the old aristocratic patrician families had always had, but the new plebeian aristocrats became as uninterested in the plight of the average plebeian as the old patrician aristocrats had always been. The plebeians rebelled by leaving Rome and refusing to return until they had more rights. The patricians then noticed how much they needed the plebeians and accepted their terms. The plebeians then returned to Rome and continued their work.
The Hortensian Law deprived the patricians of their last weapon against the plebeians, and thus resolved the last great political question of the era. No such important political changes occurred between 287 BC and 133 BC. The important laws of this era were still enacted by the senate. In effect, the plebeians were satisfied with the possession of power, but did not care to use it. The senate was supreme during this era because the era was dominated by questions of foreign and military policy. This was the most militarily active era of the Roman Republic.
In the final decades of this era many plebeians grew poorer. The long military campaigns had forced citizens to leave their farms to fight, while their farms fell into disrepair. The landed aristocracy began buying bankrupted farms at discounted prices. As commodity prices fell, many farmers could no longer operate their farms at a profit. The result was the ultimate bankruptcy of countless farmers. Masses of unemployed plebeians soon began to flood into Rome, and thus into the ranks of the legislative assemblies. Their poverty usually led them to vote for the candidate who offered them the most. A new culture of dependency was emerging, in which citizens would look to any populist leader for relief.
Tiberius Gracchus was elected tribune in 133 BC. He attempted to enact a law which would have limited the amount of land that any individual could own. The aristocrats, who stood to lose an enormous amount of money, were bitterly opposed to this proposal. Tiberius submitted this law to the Plebeian Council, but the law was vetoed by a tribune named Marcus Octavius. Tiberius then used the Plebeian Council to impeach Octavius. The theory, that a representative of the people ceases to be one when he acts against the wishes of the people, was counter to Roman constitutional theory. If carried to its logical end, this theory would remove all constitutional restraints on the popular will, and put the state under the absolute control of a temporary popular majority. His law was enacted, but Tiberius was murdered with 300 of his associates when he stood for reelection to the tribunate.
Tiberius' brother Gaius was elected tribune in 123 BC. Gaius Gracchus' ultimate goal was to weaken the senate and to strengthen the democratic forces. In the past, for example, the senate would eliminate political rivals either by establishing special judicial commissions or by passing a senatus consultum ultimum ("ultimate decree of the senate"). Both devices would allow the Senate to bypass the ordinary due process rights that all citizens had. Gaius outlawed the judicial commissions, and declared the senatus consultum ultimum to be unconstitutional. Gaius then proposed a law which would grant citizenship rights to Rome's Italian allies. This last proposal was not popular with the plebeians and he lost much of his support. He stood for election to a third term in 121 BC, but was defeated and then murdered by representatives of the senate with 3,000 of his supporters on Capitoline Hill in Rome. Though the senate retained control, the Gracchi had strengthened the political influence of the plebeians.
In 118 BC, King Micipsa of Numidia (current-day Algeria and Tunisia) died. He was succeeded by two legitimate sons, Adherbal and Hiempsal, and an illegitimate son, Jugurtha. Micipsa divided his kingdom between these three sons. Jugurtha, however, turned on his brothers, killing Hiempsal and driving Adherbal out of Numidia. Adherbal fled to Rome for assistance, and initially Rome mediated a division of the country between the two brothers. Eventually, Jugurtha renewed his offensive, leading to a long and inconclusive war with Rome. He also bribed several Roman commanders, and at least two tribunes, before and during the war. His nemesis, Gaius Marius, a legate from a virtually unknown provincial family, returned from the war in Numidia and was elected consul in 107 BC over the objections of the aristocratic senators. Marius invaded Numidia and brought the war to a quick end, capturing Jugurtha in the process. The apparent incompetence of the Senate, and the brilliance of Marius, had been put on full display. The populares party took full advantage of this opportunity by allying itself with Marius.
Several years later, in 88 BC, a Roman army was sent to put down an emerging Asian power, king Mithridates of Pontus. The army, however, was defeated. One of Marius' old quaestors, Lucius Cornelius Sulla, had been elected consul for the year, and was ordered by the senate to assume command of the war against Mithridates. Marius, a member of the "populares" party, had a tribune revoke Sulla's command of the war against Mithridates. Sulla, a member of the aristocratic ("optimates") party, brought his army back to Italy and marched on Rome. Sulla was so angry at Marius' tribune that he passed a law intended to permanently weaken the tribunate. He then returned to his war against Mithridates. With Sulla gone, the populares under Marius and Lucius Cornelius Cinna soon took control of the city.
During the period in which the populares party controlled the city, they flouted convention by re-electing Marius consul several times without observing the customary ten-year interval between offices. They also transgressed the established oligarchy by advancing unelected individuals to magisterial office, and by substituting magisterial edicts for popular legislation. Sulla soon made peace with Mithridates. In 83 BC, he returned to Rome, overcame all resistance, and recaptured the city. Sulla and his supporters then slaughtered most of Marius' supporters. Sulla, having observed the violent results of radical popular reforms, was naturally conservative. As such, he sought to strengthen the aristocracy, and by extension the senate. Sulla made himself dictator, passed a series of constitutional reforms, resigned the dictatorship, and served one last term as consul. He died in 78 BC.
In 77 BC, the senate sent one of Sulla's former lieutenants, Gnaeus Pompeius Magnus ("Pompey the Great"), to put down an uprising in Spain. By 71 BC, Pompey returned to Rome after having completed his mission. Around the same time, another of Sulla's former lieutenants, Marcus Licinius Crassus, had just put down the Spartacus-led gladiator/slave revolt in Italy. Upon their return, Pompey and Crassus found the populares party fiercely attacking Sulla's constitution. They attempted to forge an agreement with the populares party. If both Pompey and Crassus were elected consul in 70 BC, they would dismantle the more obnoxious components of Sulla's constitution. The two were soon elected, and quickly dismantled most of Sulla's constitution.
Around 66 BC, a movement to use constitutional, or at least peaceful, means to address the plight of various classes began. After several failures, the movement's leaders decided to use any means that were necessary to accomplish their goals. The movement coalesced under an aristocrat named Lucius Sergius Catilina. The movement was based in the town of Faesulae, which was a natural hotbed of agrarian agitation. The rural malcontents were to advance on Rome, and be aided by an uprising within the city. After assassinating the consuls and most of the senators, Catiline would be free to enact his reforms. The conspiracy was set in motion in 63 BC. The consul for the year, Marcus Tullius Cicero, intercepted messages that Catiline had sent in an attempt to recruit more members. As a result, the top conspirators in Rome (including at least one former consul) were executed by authorisation (of dubious constitutionality) of the senate, and the planned uprising was disrupted. Cicero then sent an army, which cut Catiline's forces to pieces.
In 62 BC, Pompey returned victorious from Asia. The Senate, elated by its successes against Catiline, refused to ratify the arrangements that Pompey had made. Pompey, in effect, became powerless. Thus, when Julius Caesar returned from a governorship in Spain in 61 BC, he found it easy to make an arrangement with Pompey. Caesar and Pompey, along with Crassus, established a private agreement, now known as the First Triumvirate. Under the agreement, Pompey's arrangements would be ratified. Caesar would be elected consul in 59 BC, and would then serve as governor of Gaul for five years. Crassus was promised a future consulship.
Caesar became consul in 59 BC. His colleague, Marcus Calpurnius Bibulus, was an extreme aristocrat. Caesar submitted the laws that he had promised Pompey to the assemblies. Bibulus attempted to obstruct the enactment of these laws, and so Caesar used violent means to ensure their passage. Caesar was then made governor of three provinces. He facilitated the election of the former patrician Publius Clodius Pulcher to the tribunate for 58 BC. Clodius set about depriving Caesar's senatorial enemies of two of their more obstinate leaders in Cato and Cicero. Clodius was a bitter opponent of Cicero because Cicero had testified against him in a sacrilege case. Clodius attempted to try Cicero for executing citizens without a trial during the Catiline conspiracy, resulting in Cicero going into self-imposed exile and his house in Rome being burnt down. Clodius also passed a bill that forced Cato to lead the invasion of Cyprus which would keep him away from Rome for some years. Clodius also passed a law to expand the previous partial grain subsidy to a fully free grain dole for citizens.
Clodius formed armed gangs that terrorised the city and eventually began to attack Pompey's followers, who in response funded counter-gangs formed by Titus Annius Milo. The political alliance of the triumvirate was crumbling. Domitius Ahenobarbus ran for the consulship in 55 BC promising to take Caesar's command from him. Eventually, the triumvirate was renewed at Lucca. Pompey and Crassus were promised the consulship in 55 BC, and Caesar's term as governor was extended for five years. Crassus led an ill-fated expedition with legions led by his son, Caesar's lieutenant, against the Kingdom of Parthia. This resulted in his defeat and death at the Battle of Carrhae. Finally, Pompey's wife, Julia, who was Caesar's daughter, died in childbirth. This event severed the last remaining bond between Pompey and Caesar.
Beginning in the summer of 54 BC, a wave of political corruption and violence swept Rome. This chaos reached a climax in January of 52 BC, when Clodius was murdered in a gang war by Milo. On 1 January 49 BC, an agent of Caesar presented an ultimatum to the senate. The ultimatum was rejected, and the senate then passed a resolution which declared that if Caesar did not lay down his arms by July of that year, he would be considered an enemy of the Republic. Meanwhile, the senators adopted Pompey as their new champion against Caesar. On 7 January of 49 BC, the senate passed a senatus consultum ultimum, which vested Pompey with dictatorial powers. Pompey's army, however, was composed largely of untested conscripts. On 10 January, Caesar crossed the Rubicon with his veteran army (in violation of Roman laws) and marched towards Rome. Caesar's rapid advance forced Pompey, the consuls and the senate to abandon Rome for Greece. Caesar entered the city unopposed.
Caesar held both the dictatorship and the tribunate, and alternated between the consulship and the proconsulship. In 48 BC, Caesar was given permanent tribunician powers. This made his person sacrosanct, gave him the power to veto the senate, and allowed him to dominate the Plebeian Council. In 46 BC, Caesar was given censorial powers, which he used to fill the senate with his own partisans. Caesar then raised the membership of the Senate to 900. This robbed the senatorial aristocracy of its prestige, and made it increasingly subservient to him. While the assemblies continued to meet, he submitted all candidates to the assemblies for election, and all bills to the assemblies for enactment. Thus, the assemblies became powerless and were unable to oppose him.
Caesar was assassinated on March 15, 44 BC. The assassination was led by Gaius Cassius and Marcus Brutus. Most of the conspirators were senators, who had a variety of economic, political, or personal motivations for carrying out the assassination. Many were afraid that Caesar would soon resurrect the monarchy and declare himself king. Others feared loss of property or prestige as Caesar carried out his land reforms in favor of the landless classes. Virtually all the conspirators fled the city after Caesar's death in fear of retaliation. The civil war that followed destroyed what was left of the Republic.
After the assassination, Mark Antony formed an alliance with Caesar's adopted son and great-nephew, Gaius Octavian. Along with Marcus Lepidus, they formed an alliance known as the Second Triumvirate. They held powers that were nearly identical to the powers that Caesar had held under his constitution. As such, the Senate and assemblies remained powerless, even after Caesar had been assassinated. The conspirators were then defeated at the Battle of Philippi in 42 BC. Eventually, however, Antony and Octavian fought against each other in one last battle. Antony was defeated in the naval Battle of Actium in 31 BC, and he committed suicide with his lover, Cleopatra. In 29 BC, Octavian returned to Rome as the unchallenged master of the Empire and later accepted the title of Augustus ("Exalted One"). He was convinced that only a single strong ruler could restore order in Rome.
As with most ancient civilizations, Rome's military served the triple purposes of securing its borders, exploiting peripheral areas through measures such as imposing tribute on conquered peoples, and maintaining internal order. From the outset, Rome's military typified this pattern and the majority of Rome's wars were characterized by one of two types. The first is the foreign war, normally begun as a counter-offensive or defense of an ally. The second is the civil war, which plagued the Roman Republic in its final century. Roman armies were not invincible, despite their formidable reputation and host of victories. Over the centuries the Romans "produced their share of incompetents" who led Roman armies into catastrophic defeats. Nevertheless, it was generally the fate of the greatest of Rome's enemies, such as Pyrrhus and Hannibal, to win early battles but lose the war. The history of Rome's campaigning is, if nothing else, a history of obstinate persistence overcoming appalling losses.
During this period, Roman soldiers seem to have been modelled after those of the Etruscans to the north, who themselves seem to have copied their style of warfare from the Greeks. Traditionally, the introduction of the phalanx formation into the Roman army is ascribed to the city's penultimate king, Servius Tullius (ruled 578 to 534 BC). According to Livy and Dionysius of Halicarnassus, the front rank was composed of the wealthiest citizens, who were able to purchase the best equipment. Each subsequent rank consisted of those with less wealth and poorer equipment than the one before it.
One disadvantage of the phalanx was that it was only effective when fighting in large, open spaces, which left the Romans at a disadvantage when fighting in the hilly terrain of central Italian peninsula. In the 4th century BC, the Romans abandoned the phalanx in favour of the more flexible manipular formation. This change is sometimes attributed to Marcus Furius Camillus and placed shortly after the Gallic invasion of 390 BC; it is more likely, however, that they were copied from Rome's Samnite enemies to the south, possibly as a result of Samnite victories during the Second Samnite War (326 to 304 BC).
The heavy infantry of the maniples were supported by a number of light infantry and cavalry troops, typically 300 horsemen per manipular legion. The cavalry was drawn primarily from the richest class of equestrians. There was an additional class of troops who followed the army without specific martial roles and were deployed to the rear of the third line. Their role in accompanying the army was primarily to supply any vacancies that might occur in the maniples. The light infantry consisted of 1,200 unarmoured skirmishing troops drawn from the youngest and lower social classes. They were armed with a sword and a small shield, as well as several light javelins.
Rome's military confederation with the other peoples of the Italian peninsula meant that half of Rome's army was provided by the Socii, such as the Etruscans, Umbrians, Apulians, Campanians, Samnites, Lucani, Bruttii, and the various southern Greek cities. Polybius states that Rome could draw on 770,000 men at the beginning of the Second Punic War, of which 700,000 were infantry and 70,000 met the requirements for cavalry. Rome's Italian allies would be organized in alae, or wings, roughly equal in manpower to the Roman legions, though with 900 cavalry instead of 300.
The extraordinary demands of the Punic Wars, in addition to a shortage of manpower, exposed the tactical weaknesses of the manipular legion, at least in the short term. In 217 BC, near the beginning of the Second Punic War, Rome was forced to effectively ignore its long-standing principle that its soldiers must be both citizens and property owners. During the 2nd century BC, Roman territory saw an overall decline in population, partially due to the huge losses incurred during various wars. This was accompanied by severe social stresses and the greater collapse of the middle classes. As a result, the Roman state was forced to arm its soldiers at the expense of the state, which it had not had to do in the past.
In process known as the Marian reforms, Roman consul Gaius Marius carried out a programme of reform of the Roman military. In 107 BC, all citizens, regardless of their wealth or social class, were made eligible for entry into the Roman army. This move formalised and concluded a gradual process that had been growing for centuries, of removing property requirements for military service. The distinction between the three heavy infantry classes, which had already become blurred, had collapsed into a single class of heavy legionary infantry. The heavy infantry legionaries were drawn from citizen stock, while non-citizens came to dominate the ranks of the light infantry. The army's higher-level officers and commanders were still drawn exclusively from the Roman aristocracy.
The legions of the late Republic were, structurally, almost entirely heavy infantry. The legion's main sub-unit was called a cohort and consisted of approximately 480 infantrymen. The cohort was therefore a much larger unit than the earlier maniple sub-unit, and was divided into six centuries of 80 men each. Each century was separated further into 10 "tent groups" of 8 men each. The cavalry troops were used as scouts and dispatch riders rather than battlefield cavalry. Legions also contained a dedicated group of artillery crew of perhaps 60 men. Each legion was normally partnered with an approximately equal number of allied (non-Roman) troops.
After having declined in size following the subjugation of the Mediterranean, the Roman navy underwent short-term upgrading and revitalisation in the late Republic to meet several new demands. Under Caesar, an invasion fleet was assembled in the English Channel to allow the invasion of Britannia; under Pompey, a large fleet was raised in the Mediterranean Sea to clear the sea of Cilician pirates. During the civil war that followed, as many as a thousand ships were either constructed or pressed into service from Greek cities.
The senate's ultimate authority derived from the esteem and prestige of the senators. This esteem and prestige was based on both precedent and custom, as well as the caliber and reputation of the senators. The senate passed decrees, which were called senatus consulta. These were officially "advice" from the senate to a magistrate. In practice, however, they were usually followed by the magistrates. The focus of the Roman senate was usually directed towards foreign policy. Though it technically had no official role in the management of military conflict, the senate ultimately was the force that oversaw such affairs. The power of the senate expanded over time as the power of the legislative assemblies declined, and the senate took a greater role in ordinary law-making. Its members were usually appointed by Roman Censors, who ordinarily selected newly elected magistrates for membership in the senate, making the senate a partially elected body. During times of military emergency, such as the civil wars of the 1st century BC, this practice became less prevalent, as the Roman Dictator, Triumvir or the senate itself would select its members.
The legal status of Roman citizenship was limited and was a vital prerequisite to possessing many important legal rights such as the right to trial and appeal, to marry, to vote, to hold office, to enter binding contracts, and to special tax exemptions. An adult male citizen with the full complement of legal and political rights was called "optimo jure." The optimo jure elected their assemblies, whereupon the assemblies elected magistrates, enacted legislation, presided over trials in capital cases, declared war and peace, and forged or dissolved treaties. There were two types of legislative assemblies. The first was the comitia ("committees"), which were assemblies of all optimo jure. The second was the concilia ("councils"), which were assemblies of specific groups of optimo jure.
Citizens were organized on the basis of centuries and tribes, which would each gather into their own assemblies. The Comitia Centuriata ("Centuriate Assembly") was the assembly of the centuries (i.e. soldiers). The president of the Comitia Centuriata was usually a consul. The centuries would vote, one at a time, until a measure received support from a majority of the centuries. The Comitia Centuriata would elect magistrates who had imperium powers (consuls and praetors). It also elected censors. Only the Comitia Centuriata could declare war, and ratify the results of a census. It also served as the highest court of appeal in certain judicial cases.
The assembly of the tribes (i.e. the citizens of Rome), the Comitia Tributa, was presided over by a consul, and was composed of 35 tribes. The tribes were not ethnic or kinship groups, but rather geographical subdivisions. The order that the thirty-five tribes would vote in was selected randomly by lot. Once a measure received support from a majority of the tribes, the voting would end. While it did not pass many laws, the Comitia Tributa did elect quaestors, curule aediles, and military tribunes. The Plebeian Council was identical to the assembly of the tribes, but excluded the patricians (the elite who could trace their ancestry to the founding of Rome). They elected their own officers, plebeian tribunes and plebeian aediles. Usually a plebeian tribune would preside over the assembly. This assembly passed most laws, and could also act as a court of appeal.
Each republican magistrate held certain constitutional powers. Only the People of Rome (both plebeians and patricians) had the right to confer these powers on any individual magistrate. The most powerful constitutional power was imperium. Imperium was held by both consuls and praetors. Imperium gave a magistrate the authority to command a military force. All magistrates also had the power of coercion. This was used by magistrates to maintain public order. While in Rome, all citizens had a judgement against coercion. This protection was called provocatio (see below). Magistrates also had both the power and the duty to look for omens. This power would often be used to obstruct political opponents.
One check on a magistrate's power was his collegiality. Each magisterial office would be held concurrently by at least two people. Another such check was provocatio. Provocatio was a primordial form of due process. It was a precursor to habeas corpus. If any magistrate tried to use the powers of the state against a citizen, that citizen could appeal the decision of the magistrate to a tribune. In addition, once a magistrate's one-year term of office expired, he would have to wait ten years before serving in that office again. This created problems for some consuls and praetors, and these magistrates would occasionally have their imperium extended. In effect, they would retain the powers of the office (as a promagistrate), without officially holding that office.
The consuls of the Roman Republic were the highest ranking ordinary magistrates; each consul served for one year. Consuls had supreme power in both civil and military matters. While in the city of Rome, the consuls were the head of the Roman government. They would preside over the senate and the assemblies. While abroad, each consul would command an army. His authority abroad would be nearly absolute. Praetors administered civil law and commanded provincial armies. Every five years, two censors were elected for an 18-month term, during which they would conduct a census. During the census, they could enroll citizens in the senate, or purge them from the senate. Aediles were officers elected to conduct domestic affairs in Rome, such as managing public games and shows. The quaestors would usually assist the consuls in Rome, and the governors in the provinces. Their duties were often financial.
Since the tribunes were considered to be the embodiment of the plebeians, they were sacrosanct. Their sacrosanctity was enforced by a pledge, taken by the plebeians, to kill any person who harmed or interfered with a tribune during his term of office. All of the powers of the tribune derived from their sacrosanctity. One consequence was that it was considered a capital offense to harm a tribune, to disregard his veto, or to interfere with a tribune. In times of military emergency, a dictator would be appointed for a term of six months. Constitutional government would be dissolved, and the dictator would be the absolute master of the state. When the dictator's term ended, constitutional government would be restored.
Life in the Roman Republic revolved around the city of Rome, and its famed seven hills. The city also had several theatres, gymnasiums, and many taverns, baths and brothels. Throughout the territory under Rome's control, residential architecture ranged from very modest houses to country villas, and in the capital city of Rome, to the residences on the elegant Palatine Hill, from which the word "palace" is derived. The vast majority of the population lived in the city center, packed into apartment blocks.[citation needed]
Many aspects of Roman culture were borrowed from the Greeks. In architecture and sculpture, the difference between Greek models and Roman paintings are apparent. The chief Roman contributions to architecture were the arch and the dome. Rome has also had a tremendous impact on European cultures following it. Its significance is perhaps best reflected in its endurance and influence, as is seen in the longevity and lasting importance of works of Virgil and Ovid. Latin, the Republic's primary language, remains used for liturgical purposes by the Roman Catholic Church, and up to the 19th century was used extensively in scholarly writings in, for example, science and mathematics. Roman law laid the foundations for the laws of many European countries and their colonies.[citation needed]
Slavery and slaves were part of the social order; there were slave markets where they could be bought and sold. Many slaves were freed by the masters for services rendered; some slaves could save money to buy their freedom. Generally, mutilation and murder of slaves was prohibited by legislation. However, Rome did not have a law enforcement arm. All actions were treated as "torts," which were brought by an accuser who was forced to prove the entire case himself. If the accused were a noble and the victim, not a noble, the likelihood of finding for the accused was small. At most, the accused might have to pay a fine for killing a slave. It is estimated that over 25% of the Roman population was enslaved.
Men typically wore a toga, and women a stola. The woman's stola differed in looks from a toga, and was usually brightly coloured. The cloth and the dress distinguished one class of people from the other class. The tunic worn by plebeians, or common people, like shepherds and slaves, was made from coarse and dark material, whereas the tunic worn by patricians was of linen or white wool. A knight or magistrate would wear an augusticlavus, a tunic bearing small purple studs. Senators wore tunics with broad red stripes, called tunica laticlavia. Military tunics were shorter than the ones worn by civilians. Boys, up until the festival of Liberalia, wore the toga praetexta, which was a toga with a crimson or purple border. The toga virilis, (or toga pura) was worn by men over the age of 16 to signify their citizenship in Rome. The toga picta was worn by triumphant generals and had embroidery of their skill on the battlefield. The toga pulla was worn when in mourning.[citation needed]
The staple foods were generally consumed around 11 o'clock, and consisted of bread, lettuce, cheese, fruits, nuts, and cold meat left over from the dinner the night before.[citation needed] The Roman poet Horace mentions another Roman favorite, the olive, in reference to his own diet, which he describes as very simple: "As for me, olives, endives, and smooth mallows provide sustenance." The family ate together, sitting on stools around a table. Fingers were used to eat solid foods and spoons were used for soups.[citation needed]
Wine was considered the basic drink, consumed at all meals and occasions by all classes and was quite inexpensive. Cato the Elder once advised cutting his rations in half to conserve wine for the workforce. Many types of drinks involving grapes and honey were consumed as well. Drinking on an empty stomach was regarded as boorish and a sure sign for alcoholism, the debilitating physical and psychological effects of which were known to the Romans. An accurate accusation of being an alcoholic was an effective way to discredit political rivals. Prominent Roman alcoholics included Mark Antony, and Cicero's own son Marcus (Cicero Minor). Even Cato the Younger was known to be a heavy drinker.[citation needed]
Following various military conquests in the Greek East, Romans adapted a number of Greek educational precepts to their own fledgling system. They began physical training to prepare the boys to grow as Roman citizens and for eventual recruitment into the army. Conforming to discipline was a point of great emphasis. Girls generally received instruction from their mothers in the art of spinning, weaving, and sewing. Schooling in a more formal sense was begun around 200 BC. Education began at the age of around six, and in the next six to seven years, boys and girls were expected to learn the basics of reading, writing and counting. By the age of twelve, they would be learning Latin, Greek, grammar and literature, followed by training for public speaking. Oratory was an art to be practiced and learnt, and good orators commanded respect.[citation needed]
The native language of the Romans was Latin. Although surviving Latin literature consists almost entirely of Classical Latin, an artificial and highly stylised and polished literary language from the 1st century BC, the actual spoken language was Vulgar Latin, which significantly differed from Classical Latin in grammar, vocabulary, and eventually pronunciation. Rome's expansion spread Latin throughout Europe, and over time Vulgar Latin evolved and dialectised in different locations, gradually shifting into a number of distinct Romance languages. Many of these languages, including French, Italian, Portuguese, Romanian and Spanish, flourished, the differences between them growing greater over time. Although English is Germanic rather than Roman in origin, English borrows heavily from Latin and Latin-derived words.[citation needed]
Roman literature was from its very inception influenced heavily by Greek authors. Some of the earliest works we possess are of historical epics telling the early military history of Rome. As the republic expanded, authors began to produce poetry, comedy, history, and tragedy. Virgil represents the pinnacle of Roman epic poetry. His Aeneid tells the story of flight of Aeneas from Troy and his settlement of the city that would become Rome.[citation needed] Lucretius, in his On the Nature of Things, attempted to explicate science in an epic poem. The genre of satire was common in Rome, and satires were written by, among others, Juvenal and Persius. The rhetorical works of Cicero are considered[by whom?] to be some of the best bodies of correspondence recorded in antiquity.[citation needed]
Music was a major part of everyday life.[citation needed] The word itself derives from Greek μουσική (mousike), "(art) of the Muses". Many private and public events were accompanied by music, ranging from nightly dining to military parades and manoeuvres. In a discussion of any ancient music, however, non-specialists and even many musicians have to be reminded that much of what makes our modern music familiar to us is the result of developments only within the last 1,000 years; thus, our ideas of melody, scales, harmony, and even the instruments we use may not have been familiar to Romans who made and listened to music many centuries earlier.[citation needed]
Over time, Roman architecture was modified as their urban requirements changed, and the civil engineering and building construction technology became developed and refined. The Roman concrete has remained a riddle, and even after more than 2,000 years some Roman structures still stand magnificently. The architectural style of the capital city was emulated by other urban centers under Roman control and influence. Roman cities were well planned, efficiently managed and neatly maintained.[citation needed]
The city of Rome had a place called the Campus Martius ("Field of Mars"), which was a sort of drill ground for Roman soldiers. Later, the Campus became Rome's track and field playground. In the campus, the youth assembled to play and exercise, which included jumping, wrestling, boxing and racing.[citation needed] Equestrian sports, throwing, and swimming were also preferred physical activities.[citation needed] In the countryside, pastimes included fishing and hunting.[citation needed] Board games played in Rome included dice (Tesserae or Tali), Roman Chess (Latrunculi), Roman Checkers (Calculi), Tic-tac-toe (Terni Lapilli), and Ludus duodecim scriptorum and Tabula, predecessors of backgammon. Other activities included chariot races, and musical and theatrical performances.[citation needed]
Roman religious beliefs date back to the founding of Rome, around 800 BC. However, the Roman religion commonly associated with the republic and early empire did not begin until around 500 BC, when Romans came in contact with Greek culture, and adopted many of the Greek religious beliefs. Private and personal worship was an important aspect of religious practices. In a sense, each household was a temple to the gods. Each household had an altar (lararium), at which the family members would offer prayers, perform rites, and interact with the household gods. Many of the gods that Romans worshiped came from the Proto-Indo-European pantheon, others were based on Greek gods. The two most famous deities were Jupiter (the king God) and Mars (the god of war). With its cultural influence spreading over most of the Mediterranean, Romans began accepting foreign gods into their own culture, as well as other philosophical traditions such as Cynicism and Stoicism.
Apollo (Attic, Ionic, and Homeric Greek: Ἀπόλλων, Apollōn (GEN Ἀπόλλωνος); Doric: Ἀπέλλων, Apellōn; Arcadocypriot: Ἀπείλων, Apeilōn; Aeolic: Ἄπλουν, Aploun; Latin: Apollō) is one of the most important and complex of the Olympian deities in classical Greek and Roman religion and Greek and Roman mythology. The ideal of the kouros (a beardless, athletic youth), Apollo has been variously recognized as a god of music, truth and prophecy, healing, the sun and light, plague, poetry, and more. Apollo is the son of Zeus and Leto, and has a twin sister, the chaste huntress Artemis. Apollo is known in Greek-influenced Etruscan mythology as Apulu.
As the patron of Delphi (Pythian Apollo), Apollo was an oracular god—the prophetic deity of the Delphic Oracle. Medicine and healing are associated with Apollo, whether through the god himself or mediated through his son Asclepius, yet Apollo was also seen as a god who could bring ill-health and deadly plague. Amongst the god's custodial charges, Apollo became associated with dominion over colonists, and as the patron defender of herds and flocks. As the leader of the Muses (Apollon Musegetes) and director of their choir, Apollo functioned as the patron god of music and poetry. Hermes created the lyre for him, and the instrument became a common attribute of Apollo. Hymns sung to Apollo were called paeans.
In Hellenistic times, especially during the 3rd century BCE, as Apollo Helios he became identified among Greeks with Helios, Titan god of the sun, and his sister Artemis similarly equated with Selene, Titan goddess of the moon. In Latin texts, on the other hand, Joseph Fontenrose declared himself unable to find any conflation of Apollo with Sol among the Augustan poets of the 1st century, not even in the conjurations of Aeneas and Latinus in Aeneid XII (161–215). Apollo and Helios/Sol remained separate beings in literary and mythological texts until the 3rd century CE.
The etymology of the name is uncertain. The spelling Ἀπόλλων (pronounced [a.pól.lɔːn] in Classical Attic) had almost superseded all other forms by the beginning of the common era, but the Doric form Apellon (Ἀπέλλων), is more archaic, derived from an earlier *Ἀπέλjων. It probably is a cognate to the Doric month Apellaios (Ἀπελλαῖος), and the offerings apellaia (ἀπελλαῖα) at the initiation of the young men during the family-festival apellai (ἀπέλλαι). According to some scholars the words are derived from the Doric word apella (ἀπέλλα), which originally meant "wall," "fence for animals" and later "assembly within the limits of the square." Apella (Ἀπέλλα) is the name of the popular assembly in Sparta, corresponding to the ecclesia (ἐκκλησία). R. S. P. Beekes rejected the connection of the theonym with the noun apellai and suggested a Pre-Greek proto-form *Apalyun.
Several instances of popular etymology are attested from ancient authors. Thus, the Greeks most often associated Apollo's name with the Greek verb ἀπόλλυμι (apollymi), "to destroy". Plato in Cratylus connects the name with ἀπόλυσις (apolysis), "redemption", with ἀπόλουσις (apolousis), "purification", and with ἁπλοῦν ([h]aploun), "simple", in particular in reference to the Thessalian form of the name, Ἄπλουν, and finally with Ἀειβάλλων (aeiballon), "ever-shooting". Hesychius connects the name Apollo with the Doric ἀπέλλα (apella), which means "assembly", so that Apollo would be the god of political life, and he also gives the explanation σηκός (sekos), "fold", in which case Apollo would be the god of flocks and herds. In the Ancient Macedonian language πέλλα (pella) means "stone," and some toponyms may be derived from this word: Πέλλα (Pella, the capital of Ancient Macedonia) and Πελλήνη (Pellēnē/Pallene).
A number of non-Greek etymologies have been suggested for the name, The Hittite form Apaliunas (dx-ap-pa-li-u-na-aš) is attested in the Manapa-Tarhunta letter, perhaps related to Hurrian (and certainly the Etruscan) Aplu, a god of plague, in turn likely from Akkadian Aplu Enlil meaning simply "the son of Enlil", a title that was given to the god Nergal, who was linked to Shamash, Babylonian god of the sun. The role of Apollo as god of plague is evident in the invocation of Apollo Smintheus ("mouse Apollo") by Chryses, the Trojan priest of Apollo, with the purpose of sending a plague against the Greeks (the reasoning behind a god of the plague becoming a god of healing is of course apotropaic, meaning that the god responsible for bringing the plague must be appeased in order to remove the plague).
As sun-god and god of light, Apollo was also known by the epithets Aegletes (/əˈɡliːtiːz/ ə-GLEE-teez; Αἰγλήτης, Aiglētēs, from αἴγλη, "light of the sun"), Helius (/ˈhiːliəs/ HEE-lee-əs; Ἥλιος, Helios, literally "sun"), Phanaeus (/fəˈniːəs/ fə-NEE-əs; Φαναῖος, Phanaios, literally "giving or bringing light"), and Lyceus (/laɪˈsiːəs/ ly-SEE-əs; Λύκειος, Lykeios, from Proto-Greek *λύκη, "light"). The meaning of the epithet "Lyceus" later became associated with Apollo's mother Leto, who was the patron goddess of Lycia (Λυκία) and who was identified with the wolf (λύκος), earning him the epithets Lycegenes (/laɪˈsɛdʒəniːz/ ly-SEJ-ə-neez; Λυκηγενής, Lukēgenēs, literally "born of a wolf" or "born of Lycia") and Lycoctonus (/laɪˈkɒktənəs/ ly-KOK-tə-nəs; Λυκοκτόνος, Lykoktonos, from λύκος, "wolf", and κτείνειν, "to kill"). As god of the sun, Apollo was called Sol (/ˈsɒl/ SOL, literally "sun" in Latin) by the Romans.
Apollo was worshipped as Actiacus (/ækˈtaɪ.əkəs/ ak-TY-ə-kəs; Ἄκτιακός, Aktiakos, literally "Actian"), Delphinius (/dɛlˈfɪniəs/ del-FIN-ee-əs; Δελφίνιος, Delphinios, literally "Delphic"), and Pythius (/ˈpɪθiəs/ PITH-ee-əs; Πύθιος, Puthios, from Πυθώ, Pythō, the area around Delphi), after Actium (Ἄκτιον) and Delphi (Δελφοί) respectively, two of his principal places of worship. An etiology in the Homeric hymns associated the epithet "Delphinius" with dolphins. He was worshipped as Acraephius (/əˈkriːfiəs/ ə-KREE-fee-əs; Ἀκραιφιος,[clarification needed] Akraiphios, literally "Acraephian") or Acraephiaeus (/əˌkriːfiˈiːəs/ ə-KREE-fee-EE-əs; Ἀκραιφιαίος, Akraiphiaios, literally "Acraephian") in the Boeotian town of Acraephia (Ἀκραιφία), reputedly founded by his son Acraepheus; and as Smintheus (/ˈsmɪnθjuːs/ SMIN-thews; Σμινθεύς, Smintheus, "Sminthian"—that is, "of the town of Sminthos or Sminthe") near the Troad town of Hamaxitus. The epithet "Smintheus" has historically been confused with σμίνθος, "mouse", in association with Apollo's role as a god of disease. For this he was also known as Parnopius (/pɑːrˈnoʊpiəs/ par-NOH-pee-əs; Παρνόπιος, Parnopios, from πάρνοψ, "locust") and to the Romans as Culicarius (/ˌkjuːlᵻˈkæriəs/ KEW-li-KARR-ee-əs; from Latin culicārius, "of midges").
In Apollo's role as a healer, his appellations included Acesius (/əˈsiːʒəs/ ə-SEE-zhəs; Ἀκέσιος, Akesios, from ἄκεσις, "healing"), Acestor (/əˈsɛstər/ ə-SES-tər; Ἀκέστωρ, Akestōr, literally "healer"), Paean (/ˈpiːən/ PEE-ən; Παιάν, Paiān, from παίειν, "to touch"),[citation needed] and Iatrus (/aɪˈætrəs/ eye-AT-rəs; Ἰατρός, Iātros, literally "physician"). Acesius was the epithet of Apollo worshipped in Elis, where he had a temple in the agora. The Romans referred to Apollo as Medicus (/ˈmɛdᵻkəs/ MED-i-kəs; literally "physician" in Latin) in this respect. A temple was dedicated to Apollo Medicus at Rome, probably next to the temple of Bellona.
As a protector and founder, Apollo had the epithets Alexicacus (/əˌlɛksᵻˈkeɪkəs/ ə-LEK-si-KAY-kəs; Ἀλεξίκακος, Alexikakos, literally "warding off evil"), Apotropaeus (/əˌpɒtrəˈpiːəs/ ə-POT-rə-PEE-əs; Ἀποτρόπαιος, Apotropaios, from ἀποτρέπειν, "to avert"), and Epicurius (/ˌɛpᵻˈkjʊriəs/ EP-i-KEWR-ee-əs; Ἐπικούριος, Epikourios, from ἐπικουρέειν, "to aid"), and Archegetes (/ɑːrˈkɛdʒətiːz/ ar-KEJ-ə-teez; Ἀρχηγέτης, Arkhēgetēs, literally "founder"), Clarius (/ˈklæriəs/ KLARR-ee-əs; Κλάριος, Klārios, from Doric κλάρος, "allotted lot"), and Genetor (/ˈdʒɛnᵻtər/ JEN-i-tər; Γενέτωρ, Genetōr, literally "ancestor"). To the Romans, he was known in this capacity as Averruncus (/ˌævəˈrʌŋkəs/ AV-ər-RUNG-kəs; from Latin āverruncare, "to avert"). He was also called Agyieus (/əˈdʒaɪ.ᵻjuːs/ ə-GWEE-ews; Ἀγυιεύς, Aguīeus, from ἄγυια, "street") for his role in protecting roads and homes; and Nomius (/ˈnoʊmiəs/ NOH-mee-əs; Νόμιος, Nomios, literally "pastoral") and Nymphegetes (/nɪmˈfɛdʒᵻtiːz/ nim-FEJ-i-teez; Νυμφηγέτης, Numphēgetēs, from Νύμφη, "Nymph", and ἡγέτης, "leader") for his role as a protector of shepherds and pastoral life.
In his role as god of prophecy and truth, Apollo had the epithets Manticus (/ˈmæntᵻkəs/ MAN-ti-kəs; Μαντικός, Mantikos, literally "prophetic"), Leschenorius (/ˌlɛskᵻˈnɔəriəs/ LES-ki-NOHR-ee-əs; Λεσχηνόριος, Leskhēnorios, from λεσχήνωρ, "converser"), and Loxias (/ˈlɒksiəs/ LOK-see-əs; Λοξίας, Loxias, from λέγειν, "to say"). The epithet "Loxias" has historically been associated with λοξός, "ambiguous". In this respect, the Romans called him Coelispex (/ˈsɛlᵻspɛks/ SEL-i-speks; from Latin coelum, "sky", and specere, "to look at"). The epithet Iatromantis (/aɪˌætrəˈmæntɪs/ eye-AT-rə-MAN-tis; Ἰατρομάντις, Iātromantis, from ἰατρός, "physician", and μάντις, "prophet") refers to both his role as a god of healing and of prophecy. As god of music and arts, Apollo had the epithet Musagetes (/mjuːˈsædʒᵻtiːz/ mew-SAJ-i-teez; Doric Μουσαγέτας, Mousāgetās) or Musegetes (/mjuːˈsɛdʒᵻtiːz/ mew-SEJ-i-teez; Μουσηγέτης, Mousēgetēs, from Μούσα, "Muse", and ἡγέτης, "leader").
As a god of archery, Apollo was known as Aphetor (/əˈfiːtər/ ə-FEE-tər; Ἀφήτωρ, Aphētōr, from ἀφίημι, "to let loose") or Aphetorus (/əˈfɛtərəs/ ə-FET-ər-əs; Ἀφητόρος, Aphētoros, of the same origin), Argyrotoxus (/ˌɑːrdʒᵻrəˈtɒksəs/ AR-ji-rə-TOK-səs; Ἀργυρότοξος, Argyrotoxos, literally "with silver bow"), Hecaërgus (/ˌhɛkiˈɜːrɡəs/ HEK-ee-UR-gəs; Ἑκάεργος, Hekaergos, literally "far-shooting"), and Hecebolus (/hᵻˈsɛbələs/ hi-SEB-ə-ləs; Ἑκηβόλος, Hekēbolos, literally "far-shooting"). The Romans referred to Apollo as Articenens (/ɑːrˈtɪsᵻnənz/ ar-TISS-i-nənz; "bow-carrying"). Apollo was called Ismenius (/ɪzˈmiːniəs/ iz-MEE-nee-əs; Ἰσμηνιός, Ismēnios, literally "of Ismenus") after Ismenus, the son of Amphion and Niobe, whom he struck with an arrow.
The cult centers of Apollo in Greece, Delphi and Delos, date from the 8th century BCE. The Delos sanctuary was primarily dedicated to Artemis, Apollo's twin sister. At Delphi, Apollo was venerated as the slayer of Pytho. For the Greeks, Apollo was all the Gods in one and through the centuries he acquired different functions which could originate from different gods. In archaic Greece he was the prophet, the oracular god who in older times was connected with "healing". In classical Greece he was the god of light and of music, but in popular religion he had a strong function to keep away evil. Walter Burkert discerned three components in the prehistory of Apollo worship, which he termed "a Dorian-northwest Greek component, a Cretan-Minoan component, and a Syro-Hittite component."
From his eastern origin Apollo brought the art of inspection of "symbols and omina" (σημεία και τέρατα : semeia kai terata), and of the observation of the omens of the days. The inspiration oracular-cult was probably introduced from Anatolia. The ritualism belonged to Apollo from the beginning. The Greeks created the legalism, the supervision of the orders of the gods, and the demand for moderation and harmony. Apollo became the god of shining youth, the protector of music, spiritual-life, moderation and perceptible order. The improvement of the old Anatolian god, and his elevation to an intellectual sphere, may be considered an achievement of the Greek people.
The function of Apollo as a "healer" is connected with Paean (Παιών-Παιήων), the physician of the Gods in the Iliad, who seems to come from a more primitive religion. Paeοn is probably connected with the Mycenean pa-ja-wo-ne (Linear B: 𐀞𐀊𐀍𐀚), but this is not certain. He did not have a separate cult, but he was the personification of the holy magic-song sung by the magicians that was supposed to cure disease. Later the Greeks knew the original meaning of the relevant song "paean" (παιάν). The magicians were also called "seer-doctors" (ἰατρομάντεις), and they used an ecstatic prophetic art which was used exactly by the god Apollo at the oracles.
Some common epithets of Apollo as a healer are "paion" (παιών, literally "healer" or "helper") "epikourios" (ἐπικουρώ, "help"), "oulios" (οὐλή, "healed wound", also a "scar" ) and "loimios" (λοιμός, "plague"). In classical times, his strong function in popular religion was to keep away evil, and was therefore called "apotropaios" (ἀποτρέπω, "divert", "deter", "avert") and "alexikakos" (from v. ἀλέξω + n. κακόν, "defend from evil"). In later writers, the word, usually spelled "Paean", becomes a mere epithet of Apollo in his capacity as a god of healing.
Homer illustrated Paeon the god, and the song both of apotropaic thanksgiving or triumph.[citation needed] Such songs were originally addressed to Apollo, and afterwards to other gods: to Dionysus, to Apollo Helios, to Apollo's son Asclepius the healer. About the 4th century BCE, the paean became merely a formula of adulation; its object was either to implore protection against disease and misfortune, or to offer thanks after such protection had been rendered. It was in this way that Apollo had become recognised as the god of music. Apollo's role as the slayer of the Python led to his association with battle and victory; hence it became the Roman custom for a paean to be sung by an army on the march and before entering into battle, when a fleet left the harbour, and also after a victory had been won.
The connection with Dorians and their initiation festival apellai[clarification needed] is reinforced by the month Apellaios in northwest Greek calendars, but it can explain only the Doric type of the name, which is connected with the Ancient Macedonian word "pella" (Pella), stone. Stones played an important part in the cult of the god, especially in the oracular shrine of Delphi (Omphalos). The "Homeric hymn" represents Apollo as a Northern intruder. His arrival must have occurred during the "Dark Ages" that followed the destruction of the Mycenaean civilization, and his conflict with Gaia (Mother Earth) was represented by the legend of his slaying her daughter the serpent Python.
The earth deity had power over the ghostly world, and it is believed that she was the deity behind the oracle. The older tales mentioned two dragons who were perhaps intentionally conflated. A female dragon named Delphyne (δελφύς, "womb"), who is obviously connected with Delphi and Apollo Delphinios, and a male serpent Typhon (τύφειν, "to smoke"), the adversary of Zeus in the Titanomachy, who the narrators confused with Python. Python was the good daemon (ἀγαθὸς δαίμων) of the temple as it appears in Minoan religion, but she was represented as a dragon, as often happens in Northern European folklore as well as in the East.
Apollo and his sister Artemis can bring death with their arrows. The conception that diseases and death come from invisible shots sent by supernatural beings, or magicians is common in Germanic and Norse mythology. In Greek mythology Artemis was the leader (ἡγεμών, "hegemon") of the nymphs, who had similar functions with the Nordic Elves. The "elf-shot" originally indicated disease or death attributed to the elves, but it was later attested denoting stone arrow-heads which were used by witches to harm people, and also for healing rituals.
It seems an oracular cult existed in Delphi from the Mycenaean ages. In historical times, the priests of Delphi were called Labryaden, "the double-axe men", which indicates Minoan origin. The double-axe, labrys, was the holy symbol of the Cretan labyrinth. The Homeric hymn adds that Apollo appeared as a dolphin and carried Cretan priests to Delphi, where they evidently transferred their religious practices. Apollo Delphinios was a sea-god especially worshiped in Crete and in the islands, and his name indicates his connection with Delphi and the holy serpent Delphyne ("womb").[citation needed] Apollo's sister Artemis, who was the Greek goddess of hunting, is identified with Britomartis (Diktynna), the Minoan "Mistress of the animals". In her earliest depictions she is accompanied by the "Mister of the animals", a male god of hunting who had the bow as his attribute. His original name is unknown, but it seems that he was absorbed by the more popular Apollo, who stood by the virgin "Mistress of the Animals", becoming her brother.
The old oracles in Delphi seem to be connected with a local tradition of the priesthood, and there is not clear evidence that a kind of inspiration-prophecy existed in the temple. This led some scholars to the conclusion that Pythia carried on the rituals in a consistent procedure through many centuries, according to the local tradition. In that regard, the mythical seeress Sibyl of Anatolian origin, with her ecstatic art, looks unrelated to the oracle itself. However, the Greek tradition is referring to the existence of vapours and chewing of laurel-leaves, which seem to be confirmed by recent studies.
Plato describes the priestesses of Delphi and Dodona as frenzied women, obsessed by "mania" (μανία, "frenzy"), a Greek word he connected with mantis (μάντις, "prophet"). Frenzied women like Sibyls from whose lips the god speaks are recorded in the Near East as Mari in the second millennium BC. Although Crete had contacts with Mari from 2000 BC, there is no evidence that the ecstatic prophetic art existed during the Minoan and Mycenean ages. It is more probable that this art was introduced later from Anatolia and regenerated an existing oracular cult that was local to Delphi and dormant in several areas of Greece.
A non-Greek origin of Apollo has long been assumed in scholarship. The name of Apollo's mother Leto has Lydian origin, and she was worshipped on the coasts of Asia Minor. The inspiration oracular cult was probably introduced into Greece from Anatolia, which is the origin of Sibyl, and where existed some of the oldest oracular shrines. Omens, symbols, purifications, and exorcisms appear in old Assyro-Babylonian texts, and these rituals were spread into the empire of the Hittites. In a Hittite text is mentioned that the king invited a Babylonian priestess for a certain "purification".
A similar story is mentioned by Plutarch. He writes that the Cretan seer Epimenides purified Athens after the pollution brought by the Alcmeonidae, and that the seer's expertise in sacrifices and reform of funeral practices were of great help to Solon in his reform of the Athenian state. The story indicates that Epimenides was probably heir to the shamanic religions of Asia, and proves, together with the Homeric hymn, that Crete had a resisting religion up to historical times. It seems that these rituals were dormant in Greece, and they were reinforced when the Greeks migrated to Anatolia.
Homer pictures Apollo on the side of the Trojans, fighting against the Achaeans, during the Trojan War. He is pictured as a terrible god, less trusted by the Greeks than other gods. The god seems to be related to Appaliunas, a tutelary god of Wilusa (Troy) in Asia Minor, but the word is not complete. The stones found in front of the gates of Homeric Troy were the symbols of Apollo. The Greeks gave to him the name ἀγυιεύς agyieus as the protector god of public places and houses who wards off evil, and his symbol was a tapered stone or column. However, while usually Greek festivals were celebrated at the full moon, all the feasts of Apollo were celebrated at the seventh day of the month, and the emphasis given to that day (sibutu) indicates a Babylonian origin.
The Late Bronze Age (from 1700 to 1200 BCE) Hittite and Hurrian Aplu was a god of plague, invoked during plague years. Here we have an apotropaic situation, where a god originally bringing the plague was invoked to end it. Aplu, meaning the son of, was a title given to the god Nergal, who was linked to the Babylonian god of the sun Shamash. Homer interprets Apollo as a terrible god (δεινὸς θεός) who brings death and disease with his arrows, but who can also heal, possessing a magic art that separates him from the other Greek gods. In Iliad, his priest prays to Apollo Smintheus, the mouse god who retains an older agricultural function as the protector from field rats. All these functions, including the function of the healer-god Paean, who seems to have Mycenean origin, are fused in the cult of Apollo.
Unusually among the Olympic deities, Apollo had two cult sites that had widespread influence: Delos and Delphi. In cult practice, Delian Apollo and Pythian Apollo (the Apollo of Delphi) were so distinct that they might both have shrines in the same locality. Apollo's cult was already fully established when written sources commenced, about 650 BCE. Apollo became extremely important to the Greek world as an oracular deity in the archaic period, and the frequency of theophoric names such as Apollodorus or Apollonios and cities named Apollonia testify to his popularity. Oracular sanctuaries to Apollo were established in other sites. In the 2nd and 3rd century CE, those at Didyma and Clarus pronounced the so-called "theological oracles", in which Apollo confirms that all deities are aspects or servants of an all-encompassing, highest deity. "In the 3rd century, Apollo fell silent. Julian the Apostate (359 - 61) tried to revive the Delphic oracle, but failed."
A lot of temples dedicated to Apollo were built in Greece and in the Greek colonies, and they show the spread of the cult of Apollo, and the evolution of the Greek architecture, which was mostly based on the rightness of form, and on mathematical relations. Some of the earliest temples, especially in Crete, don't belong to any Greek order. It seems that the first peripteral temples were rectangle wooden structures. The different wooden elements were considered divine, and their forms were preserved in the marble or stone elements of the temples of Doric order. The Greeks used standard types, because they believed that the world of objects was a series of typical forms which could be represented in several instances. The temples should be canonic, and the architects were trying to achieve the esthetic perfection. From the earliest times there were certain rules strictly observed in rectangular peripteral and prostyle buildings. The first buildings were narrow to hold the roof, and when the dimensions changed, some mathematical relations became necessary, in order to keep the original forms. This probably influenced the theory of numbers of Pythagoras, who believed that behind the appearance of things, there was the permanent principle of mathematics.
It is also stated that Hera kidnapped Eileithyia, the goddess of childbirth, to prevent Leto from going into labor. The other gods tricked Hera into letting her go by offering her a necklace, nine yards (8 m) long, of amber. Mythographers agree that Artemis was born first and then assisted with the birth of Apollo, or that Artemis was born one day before Apollo, on the island of Ortygia and that she helped Leto cross the sea to Delos the next day to give birth to Apollo. Apollo was born on the seventh day (ἑβδομαγενής, hebdomagenes) of the month Thargelion —according to Delian tradition—or of the month Bysios—according to Delphian tradition. The seventh and twentieth, the days of the new and full moon, were ever afterwards held sacred to him.
Four days after his birth, Apollo killed the chthonic dragon Python, which lived in Delphi beside the Castalian Spring. This was the spring which emitted vapors that caused the oracle at Delphi to give her prophecies. Hera sent the serpent to hunt Leto to her death across the world. To protect his mother, Apollo begged Hephaestus for a bow and arrows. After receiving them, Apollo cornered Python in the sacred cave at Delphi. Apollo killed Python but had to be punished for it, since Python was a child of Gaia.
When Zeus struck down Apollo's son Asclepius with a lightning bolt for resurrecting Hippolytus from the dead (transgressing Themis by stealing Hades's subjects), Apollo in revenge killed the Cyclopes, who had fashioned the bolt for Zeus. Apollo would have been banished to Tartarus forever for this, but was instead sentenced to one year of hard labor, due to the intercession of his mother, Leto. During this time he served as shepherd for King Admetus of Pherae in Thessaly. Admetus treated Apollo well, and, in return, the god conferred great benefits on Admetus.
Daphne was a nymph, daughter of the river god Peneus, who had scorned Apollo. The myth explains the connection of Apollo with δάφνη (daphnē), the laurel whose leaves his priestess employed at Delphi. In Ovid's Metamorphoses, Phoebus Apollo chaffs Cupid for toying with a weapon more suited to a man, whereupon Cupid wounds him with a golden dart; simultaneously, however, Cupid shoots a leaden arrow into Daphne, causing her to be repulsed by Apollo. Following a spirited chase by Apollo, Daphne prays to her father, Peneus, for help, and he changes her into the laurel tree, sacred to Apollo.
Leucothea was daughter of Orchamus and sister of Clytia. She fell in love with Apollo who disguised himself as Leucothea's mother to gain entrance to her chambers. Clytia, jealous of her sister because she wanted Apollo for herself, told Orchamus the truth, betraying her sister's trust and confidence in her. Enraged, Orchamus ordered Leucothea to be buried alive. Apollo refused to forgive Clytia for betraying his beloved, and a grieving Clytia wilted and slowly died. Apollo changed her into an incense plant, either heliotrope or sunflower, which follows the sun every day.
Coronis, was daughter of Phlegyas, King of the Lapiths. Pregnant with Asclepius, Coronis fell in love with Ischys, son of Elatus. A crow informed Apollo of the affair. When first informed he disbelieved the crow and turned all crows black (where they were previously white) as a punishment for spreading untruths. When he found out the truth he sent his sister, Artemis, to kill Coronis (in other stories, Apollo himself had killed Coronis). As a result, he also made the crow sacred and gave them the task of announcing important deaths. Apollo rescued the baby and gave it to the centaur Chiron to raise. Phlegyas was irate after the death of his daughter and burned the Temple of Apollo at Delphi. Apollo then killed him for what he did.
Hyacinth or Hyacinthus was one of Apollo's male lovers. He was a Spartan prince, beautiful and athletic. The pair was practicing throwing the discus when a discus thrown by Apollo was blown off course by the jealous Zephyrus and struck Hyacinthus in the head, killing him instantly. Apollo is said to be filled with grief: out of Hyacinthus' blood, Apollo created a flower named after him as a memorial to his death, and his tears stained the flower petals with the interjection αἰαῖ, meaning alas. The Festival of Hyacinthus was a celebration of Sparta.
Apollo and the Furies argue about whether the matricide was justified; Apollo holds that the bond of marriage is sacred and Orestes was avenging his father, whereas the Erinyes say that the bond of blood between mother and son is more meaningful than the bond of marriage. They invade his temple, and he says that the matter should be brought before Athena. Apollo promises to protect Orestes, as Orestes has become Apollo's supplicant. Apollo advocates Orestes at the trial, and ultimately Athena rules in favor of Apollo.
Once Pan had the audacity to compare his music with that of Apollo, and to challenge Apollo, the god of the kithara, to a trial of skill. Tmolus, the mountain-god, was chosen to umpire. Pan blew on his pipes, and with his rustic melody gave great satisfaction to himself and his faithful follower, Midas, who happened to be present. Then Apollo struck the strings of his lyre. Tmolus at once awarded the victory to Apollo, and all but Midas agreed with the judgment. He dissented and questioned the justice of the award. Apollo would not suffer such a depraved pair of ears any longer, and caused them to become the ears of a donkey.
After they each performed, both were deemed equal until Apollo decreed they play and sing at the same time. As Apollo played the lyre, this was easy to do. Marsyas could not do this, as he only knew how to use the flute and could not sing at the same time. Apollo was declared the winner because of this. Apollo flayed Marsyas alive in a cave near Celaenae in Phrygia for his hubris to challenge a god. He then nailed Marsyas' shaggy skin to a nearby pine-tree. Marsyas' blood turned into the river Marsyas.
On the occasion of a pestilence in the 430s BCE, Apollo's first temple at Rome was established in the Flaminian fields, replacing an older cult site there known as the "Apollinare". During the Second Punic War in 212 BCE, the Ludi Apollinares ("Apollonian Games") were instituted in his honor, on the instructions of a prophecy attributed to one Marcius. In the time of Augustus, who considered himself under the special protection of Apollo and was even said to be his son, his worship developed and he became one of the chief gods of Rome.
As god of colonization, Apollo gave oracular guidance on colonies, especially during the height of colonization, 750–550 BCE. According to Greek tradition, he helped Cretan or Arcadian colonists found the city of Troy. However, this story may reflect a cultural influence which had the reverse direction: Hittite cuneiform texts mention a Minor Asian god called Appaliunas or Apalunas in connection with the city of Wilusa attested in Hittite inscriptions, which is now generally regarded as being identical with the Greek Ilion by most scholars. In this interpretation, Apollo's title of Lykegenes can simply be read as "born in Lycia", which effectively severs the god's supposed link with wolves (possibly a folk etymology).
In literary contexts, Apollo represents harmony, order, and reason—characteristics contrasted with those of Dionysus, god of wine, who represents ecstasy and disorder. The contrast between the roles of these gods is reflected in the adjectives Apollonian and Dionysian. However, the Greeks thought of the two qualities as complementary: the two gods are brothers, and when Apollo at winter left for Hyperborea, he would leave the Delphic oracle to Dionysus. This contrast appears to be shown on the two sides of the Borghese Vase.
The evolution of the Greek sculpture can be observed in his depictions from the almost static formal Kouros type in early archaic period, to the representation of motion in a relative harmonious whole in late archaic period. In classical Greece the emphasis is not given to the illusive imaginative reality represented by the ideal forms, but to the analogies and the interaction of the members in the whole, a method created by Polykleitos. Finally Praxiteles seems to be released from any art and religious conformities, and his masterpieces are a mixture of naturalism with stylization.
In classical Greece, Anaxagoras asserted that a divine reason (mind) gave order to the seeds of the universe, and Plato extended the Greek belief of ideal forms to his metaphysical theory of forms (ideai, "ideas"). The forms on earth are imperfect duplicates of the intellectual celestial ideas. The Greek words oida (οἶδα, "(I) know") and eidos (εἶδος, "species") have the same root as the word idea (ἰδέα), indicating how the Greek mind moved from the gift of the senses, to the principles beyond the senses. The artists in Plato's time moved away from his theories and art tends to be a mixture of naturalism with stylization. The Greek sculptors considered the senses more important, and the proportions were used to unite the sensible with the intellectual.
Kouros (male youth) is the modern term given to those representations of standing male youths which first appear in the archaic period in Greece. This type served certain religious needs and was first proposed for what was previously thought to be depictions of Apollo. The first statues are certainly still and formal. The formality of their stance seems to be related with the Egyptian precedent, but it was accepted for a good reason. The sculptors had a clear idea of what a young man is, and embodied the archaic smile of good manners, the firm and springy step, the balance of the body, dignity, and youthful happiness. When they tried to depict the most abiding qualities of men, it was because men had common roots with the unchanging gods. The adoption of a standard recognizable type for a long time, is probably because nature gives preference in survival of a type which has long be adopted by the climatic conditions, and also due to the general Greek belief that nature expresses itself in ideal forms that can be imagined and represented. These forms expressed immortality. Apollo was the immortal god of ideal balance and order. His shrine in Delphi, that he shared in winter with Dionysius had the inscriptions: γνῶθι σεαυτόν (gnōthi seautón="know thyself") and μηδὲν ἄγαν (mēdén ágan, "nothing in excess"), and ἐγγύα πάρα δ'ἄτη (eggýa pára d'atē, "make a pledge and mischief is nigh").
In the first large-scale depictions during the early archaic period (640–580 BC), the artists tried to draw one's attention to look into the interior of the face and the body which were not represented as lifeless masses, but as being full of life. The Greeks maintained, until late in their civilization, an almost animistic idea that the statues are in some sense alive. This embodies the belief that the image was somehow the god or man himself. A fine example is the statue of the Sacred gate Kouros which was found at the cemetery of Dipylon in Athens (Dipylon Kouros). The statue is the "thing in itself", and his slender face with the deep eyes express an intellectual eternity. According to the Greek tradition the Dipylon master was named Daedalus, and in his statues the limbs were freed from the body, giving the impression that the statues could move. It is considered that he created also the New York kouros, which is the oldest fully preserved statue of Kouros type, and seems to be the incarnation of the god himself.
The animistic idea as the representation of the imaginative reality, is sanctified in the Homeric poems and in Greek myths, in stories of the god Hephaestus (Phaistos) and the mythic Daedalus (the builder of the labyrinth) that made images which moved of their own accord. This kind of art goes back to the Minoan period, when its main theme was the representation of motion in a specific moment. These free-standing statues were usually marble, but also the form rendered in limestone, bronze, ivory and terracotta.
The earliest examples of life-sized statues of Apollo, may be two figures from the Ionic sanctuary on the island of Delos. Such statues were found across the Greek speaking world, the preponderance of these were found at the sanctuaries of Apollo with more than one hundred from the sanctuary of Apollo Ptoios, Boeotia alone. The last stage in the development of the Kouros type is the late archaic period (520–485 BC), in which the Greek sculpture attained a full knowledge of human anatomy and used to create a relative harmonious whole. Ranking from the very few bronzes survived to us is the masterpiece bronze Piraeus Apollo. It was found in Piraeus, the harbour of Athens. The statue originally held the bow in its left hand, and a cup of pouring libation in its right hand. It probably comes from north-eastern Peloponnesus. The emphasis is given in anatomy, and it is one of the first attempts to represent a kind of motion, and beauty relative to proportions, which appear mostly in post-Archaic art. The statue throws some light on an artistic centre which, with an independently developed harder, simpler, and heavier style, restricts Ionian influence in Athens. Finally, this is the germ from which the art of Polykleitos was to grow two or three generations later.
In the next century which is the beginning of the Classical period, it was considered that beauty in visible things as in everything else, consisted of symmetry and proportions. The artists tried also to represent motion in a specific moment (Myron), which may be considered as the reappearance of the dormant Minoan element. Anatomy and geometry are fused in one, and each does something to the other. The Greek sculptors tried to clarify it by looking for mathematical proportions, just as they sought some reality behind appearances. Polykleitos in his Canon wrote that beauty consists in the proportion not of the elements (materials), but of the parts, that is the interrelation of parts with one another and with the whole. It seems that he was influenced by the theories of Pythagoras. The famous Apollo of Mantua and its variants are early forms of the Apollo Citharoedus statue type, in which the god holds the cithara in his left arm. The type is represented by neo-Attic Imperial Roman copies of the late 1st or early 2nd century, modelled upon a supposed Greek bronze original made in the second quarter of the 5th century BCE, in a style similar to works of Polykleitos but more archaic. The Apollo held the cythara against his extended left arm, of which in the Louvre example, a fragment of one twisting scrolling horn upright remains against his biceps.
Though the proportions were always important in Greek art, the appeal of the Greek sculptures eludes any explanation by proportion alone. The statues of Apollo were thought to incarnate his living presence, and these representations of illusive imaginative reality had deep roots in the Minoan period, and in the beliefs of the first Greek speaking people who entered the region during the bronze-age. Just as the Greeks saw the mountains, forests, sea and rivers as inhabited by concrete beings, so nature in all of its manifestations possesses clear form, and the form of a work of art. Spiritual life is incorporated in matter, when it is given artistic form. Just as in the arts the Greeks sought some reality behind appearances, so in mathematics they sought permanent principles which could be applied wherever the conditions were the same. Artists and sculptors tried to find this ideal order in relation with mathematics, but they believed that this ideal order revealed itself not so much to the dispassionate intellect, as to the whole sentient self. Things as we see them, and as they really are, are one, that each stresses the nature of the other in a single unity.
These representations rely on presenting scenes directly to the eye for their own visible sake. They care for the schematic arrangements of bodies in space, but only as parts in a larger whole. While each scene has its own character and completeness it must fit into the general sequence to which it belongs. In these archaic pediments the sculptors use empty intervals, to suggest a passage to and fro a busy battlefield. The artists seem to have been dominated by geometrical pattern and order, and this was improved when classical art brought a greater freedom and economy.
Apollo as a handsome beardless young man, is often depicted with a kithara (as Apollo Citharoedus) or bow in his hand, or reclining on a tree (the Apollo Lykeios and Apollo Sauroctonos types). The Apollo Belvedere is a marble sculpture that was rediscovered in the late 15th century; for centuries it epitomized the ideals of Classical Antiquity for Europeans, from the Renaissance through the 19th century. The marble is a Hellenistic or Roman copy of a bronze original by the Greek sculptor Leochares, made between 350 and 325 BCE.
Relations between Grand Lodges are determined by the concept of Recognition. Each Grand Lodge maintains a list of other Grand Lodges that it recognises. When two Grand Lodges recognise and are in Masonic communication with each other, they are said to be in amity, and the brethren of each may visit each other's Lodges and interact Masonically. When two Grand Lodges are not in amity, inter-visitation is not allowed. There are many reasons why one Grand Lodge will withhold or withdraw recognition from another, but the two most common are Exclusive Jurisdiction and Regularity.
Since the middle of the 19th century, Masonic historians have sought the origins of the movement in a series of similar documents known as the Old Charges, dating from the Regius Poem in about 1425 to the beginning of the 18th century. Alluding to the membership of a lodge of operative masons, they relate a mythologised history of the craft, the duties of its grades, and the manner in which oaths of fidelity are to be taken on joining. The fifteenth century also sees the first evidence of ceremonial regalia.
A dispute during the Lausanne Congress of Supreme Councils of 1875 prompted the Grand Orient de France to commission a report by a Protestant pastor which concluded that, as Freemasonry was not a religion, it should not require a religious belief. The new constitutions read, "Its principles are absolute liberty of conscience and human solidarity", the existence of God and the immortality of the soul being struck out. It is possible that the immediate objections of the United Grand Lodge of England were at least partly motivated by the political tension between France and Britain at the time. The result was the withdrawal of recognition of the Grand Orient of France by the United Grand Lodge of England, a situation that continues today.
At the dawn of the Grand Lodge era, during the 1720s, James Anderson composed the first printed constitutions for Freemasons, the basis for most subsequent constitutions, which specifically excluded women from Freemasonry. As Freemasonry spread, continental masons began to include their ladies in Lodges of Adoption, which worked three degrees with the same names as the men's but different content. The French officially abandoned the experiment in the early 19th century. Later organisations with a similar aim emerged in the United States, but distinguished the names of the degrees from those of male masonry.
In contrast to Catholic allegations of rationalism and naturalism, Protestant objections are more likely to be based on allegations of mysticism, occultism, and even Satanism. Masonic scholar Albert Pike is often quoted (in some cases misquoted) by Protestant anti-Masons as an authority for the position of Masonry on these issues. However, Pike, although undoubtedly learned, was not a spokesman for Freemasonry and was also controversial among Freemasons in general. His writings represented his personal opinion only, and furthermore an opinion grounded in the attitudes and understandings of late 19th century Southern Freemasonry of the USA. Notably, his book carries in the preface a form of disclaimer from his own Grand Lodge. No one voice has ever spoken for the whole of Freemasonry.
In 1799, English Freemasonry almost came to a halt due to Parliamentary proclamation. In the wake of the French Revolution, the Unlawful Societies Act 1799 banned any meetings of groups that required their members to take an oath or obligation. The Grand Masters of both the Moderns and the Antients Grand Lodges called on Prime Minister William Pitt (who was not a Freemason) and explained to him that Freemasonry was a supporter of the law and lawfully constituted authority and was much involved in charitable work. As a result, Freemasonry was specifically exempted from the terms of the Act, provided that each private lodge's Secretary placed with the local "Clerk of the Peace" a list of the members of his lodge once a year. This continued until 1967 when the obligation of the provision was rescinded by Parliament.
In some countries anti-Masonry is often related to antisemitism and anti-Zionism. For example, In 1980, the Iraqi legal and penal code was changed by Saddam Hussein's ruling Ba'ath Party, making it a felony to "promote or acclaim Zionist principles, including Freemasonry, or who associate [themselves] with Zionist organisations". Professor Andrew Prescott of the University of Sheffield writes: "Since at least the time of the Protocols of the Elders of Zion, antisemitism has gone hand in hand with anti-masonry, so it is not surprising that allegations that 11 September was a Zionist plot have been accompanied by suggestions that the attacks were inspired by a masonic world order".
The bulk of Masonic ritual consists of degree ceremonies. Candidates for Freemasonry are progressively initiated into Freemasonry, first in the degree of Entered Apprentice. Some time later, in a separate ceremony, they will be passed to the degree of Fellowcraft, and finally they will be raised to the degree of Master Mason. In all of these ceremonies, the candidate is entrusted with passwords, signs and grips peculiar to his new rank. Another ceremony is the annual installation of the Master and officers of the Lodge. In some jurisdictions Installed Master is valued as a separate rank, with its own secrets to distinguish its members. In other jurisdictions, the grade is not recognised, and no inner ceremony conveys new secrets during the installation of a new Master of the Lodge.
English Freemasonry spread to France in the 1720s, first as lodges of expatriates and exiled Jacobites, and then as distinctively French lodges which still follow the ritual of the Moderns. From France and England, Freemasonry spread to most of Continental Europe during the course of the 18th century. The Grande Loge de France formed under the Grand Mastership of the Duke of Clermont, who exercised only nominal authority. His successor, the Duke of Orléans, reconstituted the central body as the Grand Orient de France in 1773. Briefly eclipsed during the French Revolution, French Freemasonry continued to grow in the next century.
The majority of Freemasonry considers the Liberal (Continental) strand to be Irregular, and thus withhold recognition. For the Continental lodges, however, having a different approach to Freemasonry was not a reason for severing masonic ties. In 1961, an umbrella organisation, Centre de Liaison et d'Information des Puissances maçonniques Signataires de l'Appel de Strasbourg (CLIPSAS) was set up, which today provides a forum for most of these Grand Lodges and Grand Orients worldwide. Included in the list of over 70 Grand Lodges and Grand Orients are representatives of all three of the above categories, including mixed and women's organisations. The United Grand Lodge of England does not communicate with any of these jurisdictions, and expects its allies to follow suit. This creates the distinction between Anglo-American and Continental Freemasonry.
The denomination with the longest history of objection to Freemasonry is the Roman Catholic Church. The objections raised by the Roman Catholic Church are based on the allegation that Masonry teaches a naturalistic deistic religion which is in conflict with Church doctrine. A number of Papal pronouncements have been issued against Freemasonry. The first was Pope Clement XII's In eminenti apostolatus, 28 April 1738; the most recent was Pope Leo XIII's Ab apostolici, 15 October 1890. The 1917 Code of Canon Law explicitly declared that joining Freemasonry entailed automatic excommunication, and banned books favouring Freemasonry.
In 1933, the Orthodox Church of Greece officially declared that being a Freemason constitutes an act of apostasy and thus, until he repents, the person involved with Freemasonry cannot partake of the Eucharist. This has been generally affirmed throughout the whole Eastern Orthodox Church. The Orthodox critique of Freemasonry agrees with both the Roman Catholic and Protestant versions: "Freemasonry cannot be at all compatible with Christianity as far as it is a secret organisation, acting and teaching in mystery and secret and deifying rationalism."
In addition, most Grand Lodges require the candidate to declare a belief in a Supreme Being. In a few cases, the candidate may be required to be of a specific religion. The form of Freemasonry most common in Scandinavia (known as the Swedish Rite), for example, accepts only Christians. At the other end of the spectrum, "Liberal" or Continental Freemasonry, exemplified by the Grand Orient de France, does not require a declaration of belief in any deity, and accepts atheists (a cause of discord with the rest of Freemasonry).
Exclusive Jurisdiction is a concept whereby only one Grand Lodge will be recognised in any geographical area. If two Grand Lodges claim jurisdiction over the same area, the other Grand Lodges will have to choose between them, and they may not all decide to recognise the same one. (In 1849, for example, the Grand Lodge of New York split into two rival factions, each claiming to be the legitimate Grand Lodge. Other Grand Lodges had to choose between them until the schism was healed.) Exclusive Jurisdiction can be waived when the two over-lapping Grand Lodges are themselves in Amity and agree to share jurisdiction (for example, since the Grand Lodge of Connecticut is in Amity with the Prince Hall Grand Lodge of Connecticut, the principle of Exclusive Jurisdiction does not apply, and other Grand Lodges may recognise both).
There is no clear mechanism by which these local trade organisations became today's Masonic Lodges, but the earliest rituals and passwords known, from operative lodges around the turn of the 17th–18th centuries, show continuity with the rituals developed in the later 18th century by accepted or speculative Masons, as those members who did not practice the physical craft came to be known. The minutes of the Lodge of Edinburgh (Mary's Chapel) No. 1 in Scotland show a continuity from an operative lodge in 1598 to a modern speculative Lodge. It is reputed to be the oldest Masonic Lodge in the world.
Prince Hall Freemasonry exists because of the refusal of early American lodges to admit African-Americans. In 1775, an African-American named Prince Hall, along with fourteen other African-Americans, was initiated into a British military lodge with a warrant from the Grand Lodge of Ireland, having failed to obtain admission from the other lodges in Boston. When the military Lodge left North America, those fifteen men were given the authority to meet as a Lodge, but not to initiate Masons. In 1784, these individuals obtained a Warrant from the Premier Grand Lodge of England (GLE) and formed African Lodge, Number 459. When the UGLE was formed in 1813, all U.S.-based Lodges were stricken from their rolls – due largely to the War of 1812. Thus, separated from both UGLE and any concordantly recognised U.S. Grand Lodge, African Lodge re-titled itself as the African Lodge, Number 1 – and became a de facto "Grand Lodge" (this Lodge is not to be confused with the various Grand Lodges on the Continent of Africa). As with the rest of U.S. Freemasonry, Prince Hall Freemasonry soon grew and organised on a Grand Lodge system for each state.
Maria Deraismes was initiated into Freemasonry in 1882, then resigned to allow her lodge to rejoin their Grand Lodge. Having failed to achieve acceptance from any masonic governing body, she and Georges Martin started a mixed masonic lodge that actually worked masonic ritual. Annie Besant spread the phenomenon to the English speaking world. Disagreements over ritual led to the formation of exclusively female bodies of Freemasons in England, which spread to other countries. Meanwhile, the French had re-invented Adoption as an all-female lodge in 1901, only to cast it aside again in 1935. The lodges, however, continued to meet, which gave rise, in 1959, to a body of women practising continental Freemasonry.
Many Islamic anti-Masonic arguments are closely tied to both antisemitism and Anti-Zionism, though other criticisms are made such as linking Freemasonry to al-Masih ad-Dajjal (the false Messiah). Some Muslim anti-Masons argue that Freemasonry promotes the interests of the Jews around the world and that one of its aims is to destroy the Al-Aqsa Mosque in order to rebuild the Temple of Solomon in Jerusalem. In article 28 of its Covenant, Hamas states that Freemasonry, Rotary, and other similar groups "work in the interest of Zionism and according to its instructions ..."
The preserved records of the Reichssicherheitshauptamt (the Reich Security Main Office) show the persecution of Freemasons during the Holocaust. RSHA Amt VII (Written Records) was overseen by Professor Franz Six and was responsible for "ideological" tasks, by which was meant the creation of antisemitic and anti-Masonic propaganda. While the number is not accurately known, it is estimated that between 80,000 and 200,000 Freemasons were killed under the Nazi regime. Masonic concentration camp inmates were graded as political prisoners and wore an inverted red triangle.
Freemasonry consists of fraternal organisations that trace their origins to the local fraternities of stonemasons, which from the end of the fourteenth century regulated the qualifications of stonemasons and their interaction with authorities and clients. The degrees of freemasonry retain the three grades of medieval craft guilds, those of Apprentice, Journeyman or fellow (now called Fellowcraft), and Master Mason. These are the degrees offered by Craft (or Blue Lodge) Freemasonry. Members of these organisations are known as Freemasons or Masons. There are additional degrees, which vary with locality and jurisdiction, and are usually administered by different bodies than the craft degrees.
Candidates for Freemasonry will have met most active members of the Lodge they are joining before they are initiated. The process varies between jurisdictions, but the candidate will typically have been introduced by a friend at a Lodge social function, or at some form of open evening in the Lodge. In modern times, interested people often track down a local Lodge through the Internet. The onus is on candidates to ask to join; while candidates may be encouraged to ask, they are never invited. Once the initial inquiry is made, an interview usually follows to determine the candidate's suitability. If the candidate decides to proceed from here, the Lodge ballots on the application before he (or she, depending on the Masonic Jurisdiction) can be accepted.
Freemasonry, as it exists in various forms all over the world, has a membership estimated by the United Grand Lodge of England at around six million worldwide. The fraternity is administratively organised into independent Grand Lodges (or sometimes Grand Orients), each of which governs its own Masonic jurisdiction, which consists of subordinate (or constituent) Lodges. The largest single jurisdiction, in terms of membership, is the United Grand Lodge of England (with a membership estimated at around a quarter million). The Grand Lodge of Scotland and Grand Lodge of Ireland (taken together) have approximately 150,000 members. In the United States total membership is just under two million.
The idea of Masonic brotherhood probably descends from a 16th-century legal definition of a brother as one who has taken an oath of mutual support to another. Accordingly, Masons swear at each degree to keep the contents of that degree secret, and to support and protect their brethren unless they have broken the law. In most Lodges the oath or obligation is taken on a Volume of Sacred Law, whichever book of divine revelation is appropriate to the religious beliefs of the individual brother (usually the Bible in the Anglo-American tradition). In Progressive continental Freemasonry, books other than scripture are permissible, a cause of rupture between Grand Lodges.
The earliest known American lodges were in Pennsylvania. The Collector for the port of Pennsylvania, John Moore, wrote of attending lodges there in 1715, two years before the formation of the first Grand Lodge in London. The Premier Grand Lodge of England appointed a Provincial Grand Master for North America in 1731, based in Pennsylvania. Other lodges in the colony obtained authorisations from the later Antient Grand Lodge of England, the Grand Lodge of Scotland, and the Grand Lodge of Ireland, which was particularly well represented in the travelling lodges of the British Army. Many lodges came into existence with no warrant from any Grand Lodge, applying and paying for their authorisation only after they were confident of their own survival.
Masonic lodges existed in Iraq as early as 1917, when the first lodge under the United Grand Lodge of England (UGLE) was opened. Nine lodges under UGLE existed by the 1950s, and a Scottish lodge was formed in 1923. However, the position changed following the revolution, and all lodges were forced to close in 1965. This position was later reinforced under Saddam Hussein; the death penalty was "prescribed" for those who "promote or acclaim Zionist principles, including freemasonry, or who associate [themselves] with Zionist organisations."
The ritual form on which the Grand Orient of France was based was abolished in England in the events leading to the formation of the United Grand Lodge of England in 1813. However the two jurisdictions continued in amity (mutual recognition) until events of the 1860s and 1870s drove a seemingly permanent wedge between them. In 1868 the Supreme Council of the Ancient and Accepted Scottish Rite of the State of Louisiana appeared in the jurisdiction of the Grand Lodge of Louisiana, recognised by the Grand Orient de France, but regarded by the older body as an invasion of their jurisdiction. The new Scottish rite body admitted blacks, and the resolution of the Grand Orient the following year that neither colour, race, nor religion could disqualify a man from Masonry prompted the Grand Lodge to withdraw recognition, and it persuaded other American Grand Lodges to do the same.
In 1983, the Church issued a new code of canon law. Unlike its predecessor, the 1983 Code of Canon Law did not explicitly name Masonic orders among the secret societies it condemns. It states: "A person who joins an association which plots against the Church is to be punished with a just penalty; one who promotes or takes office in such an association is to be punished with an interdict." This named omission of Masonic orders caused both Catholics and Freemasons to believe that the ban on Catholics becoming Freemasons may have been lifted, especially after the perceived liberalisation of Vatican II. However, the matter was clarified when Cardinal Joseph Ratzinger (later Pope Benedict XVI), as the Prefect of the Congregation for the Doctrine of the Faith, issued a Declaration on Masonic Associations, which states: "... the Church's negative judgment in regard to Masonic association remains unchanged since their principles have always been considered irreconcilable with the doctrine of the Church and therefore membership in them remains forbidden. The faithful who enroll in Masonic associations are in a state of grave sin and may not receive Holy Communion." For its part, Freemasonry has never objected to Catholics joining their fraternity. Those Grand Lodges in amity with UGLE deny the Church's claims. The UGLE now states that "Freemasonry does not seek to replace a Mason's religion or provide a substitute for it."
Even in modern democracies, Freemasonry is sometimes viewed with distrust. In the UK, Masons working in the justice system, such as judges and police officers, were from 1999 to 2009 required to disclose their membership. While a parliamentary inquiry found that there has been no evidence of wrongdoing, it was felt that any potential loyalties Masons might have, based on their vows to support fellow Masons, should be transparent to the public. The policy of requiring a declaration of masonic membership of applicants for judicial office (judges and magistrates) was ended in 2009 by Justice Secretary Jack Straw (who had initiated the requirement in the 1990s). Straw stated that the rule was considered disproportionate, since no impropriety or malpractice had been shown as a result of judges being Freemasons.
The Masonic Lodge is the basic organisational unit of Freemasonry. The Lodge meets regularly to conduct the usual formal business of any small organisation (pay bills, organise social and charitable events, elect new members, etc.). In addition to business, the meeting may perform a ceremony to confer a Masonic degree or receive a lecture, which is usually on some aspect of Masonic history or ritual. At the conclusion of the meeting, the Lodge might adjourn for a formal dinner, or festive board, sometimes involving toasting and song.
During the ceremony of initiation, the candidate is expected to swear (usually on a volume of sacred text appropriate to his personal religious faith) to fulfil certain obligations as a Mason. In the course of three degrees, new masons will promise to keep the secrets of their degree from lower degrees and outsiders, and to support a fellow Mason in distress (as far as practicality and the law permit). There is instruction as to the duties of a Freemason, but on the whole, Freemasons are left to explore the craft in the manner they find most satisfying. Some will further explore the ritual and symbolism of the craft, others will focus their involvement on the social side of the Lodge, while still others will concentrate on the charitable functions of the lodge.
Regularity is a concept based on adherence to Masonic Landmarks, the basic membership requirements, tenets and rituals of the craft. Each Grand Lodge sets its own definition of what these landmarks are, and thus what is Regular and what is Irregular (and the definitions do not necessarily agree between Grand Lodges). Essentially, every Grand Lodge will hold that its landmarks (its requirements, tenets and rituals) are Regular, and judge other Grand Lodges based on those. If the differences are significant, one Grand Lodge may declare the other "Irregular" and withdraw or withhold recognition.
All Freemasons begin their journey in the "craft" by being progressively initiated, passed and raised into the three degrees of Craft, or Blue Lodge Masonry. During these three rituals, the candidate is progressively taught the meanings of the Lodge symbols, and entrusted with grips, signs and words to signify to other Masons that he has been so initiated. The initiations are part allegory and part lecture, and revolve around the construction of the Temple of Solomon, and the artistry and death of his chief architect, Hiram Abiff. The degrees are those of Entered apprentice, Fellowcraft and Master Mason. While many different versions of these rituals exist, with at least two different lodge layouts and versions of the Hiram myth, each version is recognisable to any Freemason from any jurisdiction.
The first Grand Lodge, the Grand Lodge of London and Westminster (later called the Grand Lodge of England (GLE)), was founded on 24 June 1717, when four existing London Lodges met for a joint dinner. Many English Lodges joined the new regulatory body, which itself entered a period of self-publicity and expansion. However, many Lodges could not endorse changes which some Lodges of the GLE made to the ritual (they came to be known as the Moderns), and a few of these formed a rival Grand Lodge on 17 July 1751, which they called the "Antient Grand Lodge of England." These two Grand Lodges vied for supremacy until the Moderns promised to return to the ancient ritual. They united on 27 December 1813 to form the United Grand Lodge of England (UGLE).
Widespread segregation in 19th- and early 20th-century North America made it difficult for African-Americans to join Lodges outside of Prince Hall jurisdictions – and impossible for inter-jurisdiction recognition between the parallel U.S. Masonic authorities. By the 1980s, such discrimination was a thing of the past, and today most U.S. Grand Lodges recognise their Prince Hall counterparts, and the authorities of both traditions are working towards full recognition. The United Grand Lodge of England has no problem with recognising Prince Hall Grand Lodges. While celebrating their heritage as lodges of black Americans, Prince Hall is open to all men regardless of race or religion.
In general, Continental Freemasonry is sympathetic to Freemasonry amongst women, dating from the 1890s when French lodges assisted the emergent co-masonic movement by promoting enough of their members to the 33rd degree of the Ancient and Accepted Scottish Rite to allow them, in 1899, to form their own grand council, recognised by the other Continental Grand Councils of that Rite. The United Grand Lodge of England issued a statement in 1999 recognising the two women's grand lodges there to be regular in all but the participants. While they were not, therefore, recognised as regular, they were part of Freemasonry "in general". The attitude of most regular Anglo-American grand lodges remains that women Freemasons are not legitimate Masons.
Since the founding of Freemasonry, many Bishops of the Church of England have been Freemasons, such as Archbishop Geoffrey Fisher. In the past, few members of the Church of England would have seen any incongruity in concurrently adhering to Anglican Christianity and practicing Freemasonry. In recent decades, however, reservations about Freemasonry have increased within Anglicanism, perhaps due to the increasing prominence of the evangelical wing of the church. The former Archbishop of Canterbury, Dr Rowan Williams, appeared to harbour some reservations about Masonic ritual, whilst being anxious to avoid causing offence to Freemasons inside and outside the Church of England. In 2003 he felt it necessary to apologise to British Freemasons after he said that their beliefs were incompatible with Christianity and that he had barred the appointment of Freemasons to senior posts in his diocese when he was Bishop of Monmouth.
In Italy, Freemasonry has become linked to a scandal concerning the Propaganda Due lodge (a.k.a. P2). This lodge was chartered by the Grande Oriente d'Italia in 1877, as a lodge for visiting Masons unable to attend their own lodges. Under Licio Gelli's leadership, in the late 1970s, P2 became involved in the financial scandals that nearly bankrupted the Vatican Bank. However, by this time the lodge was operating independently and irregularly, as the Grand Orient had revoked its charter and expelled Gelli in 1976.
Miami (/maɪˈæmi/; Spanish pronunciation: [maiˈami]) is a city located on the Atlantic coast in southeastern Florida and the seat of Miami-Dade County. The 44th-most populated city proper in the United States, with a population of 430,332, it is the principal, central, and most populous city of the Miami metropolitan area, and the second most populous metropolis in the Southeastern United States after Washington, D.C. According to the U.S. Census Bureau, Miami's metro area is the eighth-most populous and fourth-largest urban area in the United States, with a population of around 5.5 million.
Miami is a major center, and a leader in finance, commerce, culture, media, entertainment, the arts, and international trade. In 2012, Miami was classified as an Alpha−World City in the World Cities Study Group's inventory. In 2010, Miami ranked seventh in the United States in terms of finance, commerce, culture, entertainment, fashion, education, and other sectors. It ranked 33rd among global cities. In 2008, Forbes magazine ranked Miami "America's Cleanest City", for its year-round good air quality, vast green spaces, clean drinking water, clean streets, and city-wide recycling programs. According to a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States, and the world's fifth-richest city in terms of purchasing power. Miami is nicknamed the "Capital of Latin America", is the second largest U.S. city with a Spanish-speaking majority, and the largest city with a Cuban-American plurality.
Downtown Miami is home to the largest concentration of international banks in the United States, and many large national and international companies. The Civic Center is a major center for hospitals, research institutes, medical centers, and biotechnology industries. For more than two decades, the Port of Miami, known as the "Cruise Capital of the World", has been the number one cruise passenger port in the world. It accommodates some of the world's largest cruise ships and operations, and is the busiest port in both passenger traffic and cruise lines.
Miami is noted as "the only major city in the United States conceived by a woman, Julia Tuttle", a local citrus grower and a wealthy Cleveland native. The Miami area was better known as "Biscayne Bay Country" in the early years of its growth. In the late 19th century, reports described the area as a promising wilderness. The area was also characterized as "one of the finest building sites in Florida." The Great Freeze of 1894–95 hastened Miami's growth, as the crops of the Miami area were the only ones in Florida that survived. Julia Tuttle subsequently convinced Henry Flagler, a railroad tycoon, to expand his Florida East Coast Railway to the region, for which she became known as "the mother of Miami." Miami was officially incorporated as a city on July 28, 1896 with a population of just over 300. It was named for the nearby Miami River, derived from Mayaimi, the historic name of Lake Okeechobee.
Black labor played a crucial role in Miami's early development. During the beginning of the 20th century, migrants from the Bahamas and African-Americans constituted 40 percent of the city's population. Whatever their role in the city's growth, their community's growth was limited to a small space. When landlords began to rent homes to African-Americans in neighborhoods close to Avenue J (what would later become NW Fifth Avenue), a gang of white man with torches visited the renting families and warned them to move or be bombed.
During the early 20th century, northerners were attracted to the city, and Miami prospered during the 1920s with an increase in population and infrastructure. The legacy of Jim Crow was embedded in these developments. Miami's chief of police, H. Leslie Quigg, did not hide the fact that he, like many other white Miami police officers, was a member of the Ku Klux Klan. Unsurprisingly, these officers enforced social codes far beyond the written law. Quigg, for example, "personally and publicly beat a colored bellboy to death for speaking directly to a white woman."
After Fidel Castro rose to power in Cuba in 1959, many wealthy Cubans sought refuge in Miami, further increasing the population. The city developed businesses and cultural amenities as part of the New South. In the 1980s and 1990s, South Florida weathered social problems related to drug wars, immigration from Haiti and Latin America, and the widespread destruction of Hurricane Andrew. Racial and cultural tensions were sometimes sparked, but the city developed in the latter half of the 20th century as a major international, financial, and cultural center. It is the second-largest U.S. city (after El Paso, Texas) with a Spanish-speaking majority, and the largest city with a Cuban-American plurality.
Miami and its suburbs are located on a broad plain between the Florida Everglades to the west and Biscayne Bay to the east, which also extends from Florida Bay north to Lake Okeechobee. The elevation of the area never rises above 40 ft (12 m) and averages at around 6 ft (1.8 m) above mean sea level in most neighborhoods, especially near the coast. The highest undulations are found along the coastal Miami Rock Ridge, whose substrate underlies most of the eastern Miami metropolitan region. The main portion of the city lies on the shores of Biscayne Bay which contains several hundred natural and artificially created barrier islands, the largest of which contains Miami Beach and South Beach. The Gulf Stream, a warm ocean current, runs northward just 15 miles (24 km) off the coast, allowing the city's climate to stay warm and mild all year.
The surface bedrock under the Miami area is called Miami oolite or Miami limestone. This bedrock is covered by a thin layer of soil, and is no more than 50 feet (15 m) thick. Miami limestone formed as the result of the drastic changes in sea level associated with recent glaciations or ice ages. Beginning some 130,000 years ago the Sangamonian Stage raised sea levels to approximately 25 feet (8 m) above the current level. All of southern Florida was covered by a shallow sea. Several parallel lines of reef formed along the edge of the submerged Florida plateau, stretching from the present Miami area to what is now the Dry Tortugas. The area behind this reef line was in effect a large lagoon, and the Miami limestone formed throughout the area from the deposition of oolites and the shells of bryozoans. Starting about 100,000 years ago the Wisconsin glaciation began lowering sea levels, exposing the floor of the lagoon. By 15,000 years ago, the sea level had dropped to 300 to 350 feet (90 to 110 m) below the contemporary level. The sea level rose quickly after that, stabilizing at the current level about 4000 years ago, leaving the mainland of South Florida just above sea level.
Beneath the plain lies the Biscayne Aquifer, a natural underground source of fresh water that extends from southern Palm Beach County to Florida Bay, with its highest point peaking around the cities of Miami Springs and Hialeah. Most of the Miami metropolitan area obtains its drinking water from this aquifer. As a result of the aquifer, it is not possible to dig more than 15 to 20 ft (5 to 6 m) beneath the city without hitting water, which impedes underground construction, though some underground parking garages exist. For this reason, the mass transit systems in and around Miami are elevated or at-grade.[citation needed]
Miami is partitioned into many different sections, roughly into North, South, West and Downtown. The heart of the city is Downtown Miami and is technically on the eastern side of the city. This area includes Brickell, Virginia Key, Watson Island, and PortMiami. Downtown is South Florida's central business district, and Florida's largest and most influential central business district. Downtown has the largest concentration of international banks in the U.S. along Brickell Avenue. Downtown is home to many major banks, courthouses, financial headquarters, cultural and tourist attractions, schools, parks and a large residential population. East of Downtown, across Biscayne Bay is South Beach. Just northwest of Downtown, is the Civic Center, which is Miami's center for hospitals, research institutes and biotechnology with hospitals such as Jackson Memorial Hospital, Miami VA Hospital, and the University of Miami's Leonard M. Miller School of Medicine.
The southern side of Miami includes Coral Way, The Roads and Coconut Grove. Coral Way is a historic residential neighborhood built in 1922 connecting Downtown with Coral Gables, and is home to many old homes and tree-lined streets. Coconut Grove was established in 1825 and is the location of Miami's City Hall in Dinner Key, the Coconut Grove Playhouse, CocoWalk, many nightclubs, bars, restaurants and bohemian shops, and as such, is very popular with local college students. It is a historic neighborhood with narrow, winding roads, and a heavy tree canopy. Coconut Grove has many parks and gardens such as Villa Vizcaya, The Kampong, The Barnacle Historic State Park, and is the home of the Coconut Grove Convention Center and numerous historic homes and estates.
The northern side of Miami includes Midtown, a district with a great mix of diversity with many West Indians, Hispanics, European Americans, bohemians, and artists. Edgewater, and Wynwood, are neighborhoods of Midtown and are made up mostly of high-rise residential towers and are home to the Adrienne Arsht Center for the Performing Arts. The wealthier residents usually live in the northeastern part, in Midtown, the Design District, and the Upper East Side, with many sought after 1920s homes and home of the MiMo Historic District, a style of architecture originated in Miami in the 1950s. The northern side of Miami also has notable African American and Caribbean immigrant communities such as Little Haiti, Overtown (home of the Lyric Theater), and Liberty City.
Miami has a tropical monsoon climate (Köppen climate classification Am) with hot and humid summers and short, warm winters, with a marked drier season in the winter. Its sea-level elevation, coastal location, position just above the Tropic of Cancer, and proximity to the Gulf Stream shapes its climate. With January averaging 67.2 °F (19.6 °C), winter features mild to warm temperatures; cool air usually settles after the passage of a cold front, which produces much of the little amount of rainfall during the season. Lows occasionally fall below 50 °F (10 °C), but very rarely below 35 °F (2 °C). Highs generally range between 70–77 °F (21–25 °C).
The wet season begins some time in May, ending in mid-October. During this period, temperatures are in the mid 80s to low 90s (29–35 °C), accompanied by high humidity, though the heat is often relieved by afternoon thunderstorms or a sea breeze that develops off the Atlantic Ocean, which then allow lower temperatures, but conditions still remain very muggy. Much of the year's 55.9 inches (1,420 mm) of rainfall occurs during this period. Dewpoints in the warm months range from 71.9 °F (22.2 °C) in June to 73.7 °F (23.2 °C) in August.
The city proper is home to less than one-thirteenth of the population of South Florida. Miami is the 42nd-most populous city in the United States. The Miami metropolitan area, which includes Miami-Dade, Broward and Palm Beach counties, had a combined population of more than 5.5 million people, ranked seventh largest in the United States, and is the largest metropolitan area in the Southeastern United States. As of 2008[update], the United Nations estimates that the Miami Urban Agglomeration is the 44th-largest in the world.
In 1960, non-Hispanic whites represented 80% of Miami-Dade county's population. In 1970, the Census Bureau reported Miami's population as 45.3% Hispanic, 32.9% non-Hispanic White, and 22.7% Black. Miami's explosive population growth has been driven by internal migration from other parts of the country, primarily up until the 1980s, as well as by immigration, primarily from the 1960s to the 1990s. Today, immigration to Miami has slowed significantly and Miami's growth today is attributed greatly to its fast urbanization and high-rise construction, which has increased its inner city neighborhood population densities, such as in Downtown, Brickell, and Edgewater, where one area in Downtown alone saw a 2,069% increase in population in the 2010 Census. Miami is regarded as more of a multicultural mosaic, than it is a melting pot, with residents still maintaining much of, or some of their cultural traits. The overall culture of Miami is heavily influenced by its large population of Hispanics and blacks mainly from the Caribbean islands.
Several large companies are headquartered in or around Miami, including but not limited to: Akerman Senterfitt, Alienware, Arquitectonica, Arrow Air, Bacardi, Benihana, Brightstar Corporation, Burger King, Celebrity Cruises, Carnival Corporation, Carnival Cruise Lines, Crispin Porter + Bogusky, Duany Plater-Zyberk & Company, Espírito Santo Financial Group, Fizber.com, Greenberg Traurig, Holland & Knight, Inktel Direct, Interval International, Lennar, Navarro Discount Pharmacies, Norwegian Cruise Lines, Oceania Cruises, Perry Ellis International, RCTV International, Royal Caribbean Cruise Lines, Ryder Systems, Seabourn Cruise Line, Sedano's, Telefónica USA, UniMÁS, Telemundo, Univision, U.S. Century Bank, Vector Group and World Fuel Services. Because of its proximity to Latin America, Miami serves as the headquarters of Latin American operations for more than 1400 multinational corporations, including AIG, American Airlines, Cisco, Disney, Exxon, FedEx, Kraft Foods, LEO Pharma Americas, Microsoft, Yahoo, Oracle, SBC Communications, Sony, Symantec, Visa International, and Wal-Mart.
Miami is a major television production center, and the most important city in the U.S. for Spanish language media. Univisión, Telemundo and UniMÁS have their headquarters in Miami, along with their production studios. The Telemundo Television Studios produces much of the original programming for Telemundo, such as their telenovelas and talk shows. In 2011, 85% of Telemundo's original programming was filmed in Miami. Miami is also a major music recording center, with the Sony Music Latin and Universal Music Latin Entertainment headquarters in the city, along with many other smaller record labels. The city also attracts many artists for music video and film shootings.
Since 2001, Miami has been undergoing a large building boom with more than 50 skyscrapers rising over 400 feet (122 m) built or currently under construction in the city. Miami's skyline is ranked third-most impressive in the U.S., behind New York City and Chicago, and 19th in the world according to the Almanac of Architecture and Design. The city currently has the eight tallest (as well as thirteen of the fourteen tallest) skyscrapers in the state of Florida, with the tallest being the 789-foot (240 m) Four Seasons Hotel & Tower.
During the mid-2000s, the city witnessed its largest real estate boom since the Florida land boom of the 1920s. During this period, the city had well over a hundred approved high-rise construction projects in which 50 were actually built. In 2007, however, the housing market crashed causing lots of foreclosures on houses. This rapid high-rise construction, has led to fast population growth in the city's inner neighborhoods, primarily in Downtown, Brickell and Edgewater, with these neighborhoods becoming the fastest-growing areas in the city. The Miami area ranks 8th in the nation in foreclosures. In 2011, Forbes magazine named Miami the second-most miserable city in the United States due to its high foreclosure rate and past decade of corruption among public officials. In 2012, Forbes magazine named Miami the most miserable city in the United States because of a crippling housing crisis that has cost multitudes of residents their homes and jobs. The metro area has one of the highest violent crime rates in the country and workers face lengthy daily commutes.
Miami International Airport and PortMiami are among the nation's busiest ports of entry, especially for cargo from South America and the Caribbean. The Port of Miami is the world's busiest cruise port, and MIA is the busiest airport in Florida, and the largest gateway between the United States and Latin America. Additionally, the city has the largest concentration of international banks in the country, primarily along Brickell Avenue in Brickell, Miami's financial district. Due to its strength in international business, finance and trade, many international banks have offices in Downtown such as Espírito Santo Financial Group, which has its U.S. headquarters in Miami. Miami was also the host city of the 2003 Free Trade Area of the Americas negotiations, and is one of the leading candidates to become the trading bloc's headquarters.
Tourism is also an important industry in Miami. Along with finance and business, the beaches, conventions, festivals and events draw over 38 million visitors annually into the city, from across the country and around the world, spending $17.1 billion. The Art Deco District in South Beach, is reputed as one of the most glamorous in the world for its nightclubs, beaches, historical buildings, and shopping. Annual events such as the Sony Ericsson Open, Art Basel, Winter Music Conference, South Beach Wine & Food Festival, and Mercedes-Benz Fashion Week Miami attract millions to the metropolis every year.
According to the U.S. Census Bureau, in 2004, Miami had the third highest incidence of family incomes below the federal poverty line in the United States, making it the third poorest city in the USA, behind only Detroit, Michigan (ranked #1) and El Paso, Texas (ranked #2). Miami is also one of the very few cities where its local government went bankrupt, in 2001. However, since that time, Miami has experienced a revival: in 2008, Miami was ranked as "America's Cleanest City" according to Forbes for its year-round good air quality, vast green spaces, clean drinking water, clean streets and city-wide recycling programs. In a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States (of four U.S. cities included in the survey) and the world's fifth-richest city, in terms of purchasing power.
In addition to such annual festivals like Calle Ocho Festival and Carnaval Miami, Miami is home to many entertainment venues, theaters, museums, parks and performing arts centers. The newest addition to the Miami arts scene is the Adrienne Arsht Center for the Performing Arts, the second-largest performing arts center in the United States after the Lincoln Center in New York City, and is the home of the Florida Grand Opera. Within it are the Ziff Ballet Opera House, the center's largest venue, the Knight Concert Hall, the Carnival Studio Theater and the Peacock Rehearsal Studio. The center attracts many large-scale operas, ballets, concerts, and musicals from around the world and is Florida's grandest performing arts center. Other performing arts venues in Miami include the Gusman Center for the Performing Arts, Coconut Grove Playhouse, Colony Theatre, Lincoln Theatre, New World Center, Actor's Playhouse at the Miracle Theatre, Jackie Gleason Theatre, Manuel Artime Theater, Ring Theatre, Playground Theatre, Wertheim Performing Arts Center, the Fair Expo Center and the Bayfront Park Amphitheater for outdoor music events.
In the early 1970s, the Miami disco sound came to life with TK Records, featuring the music of KC and the Sunshine Band, with such hits as "Get Down Tonight", "(Shake, Shake, Shake) Shake Your Booty" and "That's the Way (I Like It)"; and the Latin-American disco group, Foxy (band), with their hit singles "Get Off" and "Hot Number". Miami-area natives George McCrae and Teri DeSario were also popular music artists during the 1970s disco era. The Bee Gees moved to Miami in 1975 and have lived here ever since then. Miami-influenced, Gloria Estefan and the Miami Sound Machine, hit the popular music scene with their Cuban-oriented sound and had hits in the 1980s with "Conga" and "Bad Boys".
Miami is also considered a "hot spot" for dance music, Freestyle, a style of dance music popular in the 80's and 90's heavily influenced by Electro, hip-hop, and disco. Many popular Freestyle acts such as Pretty Tony, Debbie Deb, Stevie B, and Exposé, originated in Miami. Indie/folk acts Cat Power and Iron & Wine are based in the city, while alternative hip hop artist Sage Francis, electro artist Uffie, and the electroclash duo Avenue D were born in Miami, but musically based elsewhere. Also, ska punk band Against All Authority is from Miami, and rock/metal bands Nonpoint and Marilyn Manson each formed in neighboring Fort Lauderdale. Cuban American female recording artist, Ana Cristina, was born in Miami in 1985.
This was also a period of alternatives to nightclubs, the warehouse party, acid house, rave and outdoor festival scenes of the late 1980s and early 1990s were havens for the latest trends in electronic dance music, especially house and its ever-more hypnotic, synthetic offspring techno and trance, in clubs like the infamous Warsaw Ballroom better known as Warsaw and The Mix where DJs like david padilla (who was the resident DJ for both) and radio. The new sound fed back into mainstream clubs across the country. The scene in SoBe, along with a bustling secondhand market for electronic instruments and turntables, had a strong democratizing effect, offering amateur, "bedroom" DJs the opportunity to become proficient and popular as both music players and producers, regardless of the whims of the professional music and club industries. Some of these notable DJs are John Benetiz (better known as JellyBean Benetiz), Danny Tenaglia, and David Padilla.
Cuban immigrants in the 1960s brought the Cuban sandwich, medianoche, Cuban espresso, and croquetas, all of which have grown in popularity to all Miamians, and have become symbols of the city's varied cuisine. Today, these are part of the local culture, and can be found throughout the city in window cafés, particularly outside of supermarkets and restaurants. Restaurants such as Versailles restaurant in Little Havana is a landmark eatery of Miami. Located on the Atlantic Ocean, and with a long history as a seaport, Miami is also known for its seafood, with many seafood restaurants located along the Miami River, and in and around Biscayne Bay. Miami is also the home of restaurant chains such as Burger King, Tony Roma's and Benihana.
The Miami area has a unique dialect, (commonly called the "Miami accent") which is widely spoken. The dialect developed among second- or third-generation Hispanics, including Cuban-Americans, whose first language was English (though some non-Hispanic white, black, and other races who were born and raised the Miami area tend to adopt it as well.) It is based on a fairly standard American accent but with some changes very similar to dialects in the Mid-Atlantic (especially the New York area dialect, Northern New Jersey English, and New York Latino English.) Unlike Virginia Piedmont, Coastal Southern American, and Northeast American dialects and Florida Cracker dialect (see section below), "Miami accent" is rhotic; it also incorporates a rhythm and pronunciation heavily influenced by Spanish (wherein rhythm is syllable-timed). However, this is a native dialect of English, not learner English or interlanguage; it is possible to differentiate this variety from an interlanguage spoken by second-language speakers in that "Miami accent" does not generally display the following features: there is no addition of /ɛ/ before initial consonant clusters with /s/, speakers do not confuse of /dʒ/ with /j/, (e.g., Yale with jail), and /r/ and /rr/ are pronounced as alveolar approximant [ɹ] instead of alveolar tap [ɾ] or alveolar trill [r] in Spanish.
Miami's main four sports teams are the Miami Dolphins of the National Football League, the Miami Heat of the National Basketball Association, the Miami Marlins of Major League Baseball, and the Florida Panthers of the National Hockey League. As well as having all four major professional teams, Miami is also home to the Major League Soccer expansion team led by David Beckham, Sony Ericsson Open for professional tennis, numerous greyhound racing tracks, marinas, jai alai venues, and golf courses. The city streets has hosted professional auto races, the Miami Indy Challenge and later the Grand Prix Americas. The Homestead-Miami Speedway oval hosts NASCAR national races.
Miami's tropical weather allows for year-round outdoors activities. The city has numerous marinas, rivers, bays, canals, and the Atlantic Ocean, which make boating, sailing, and fishing popular outdoors activities. Biscayne Bay has numerous coral reefs which make snorkeling and scuba diving popular. There are over 80 parks and gardens in the city. The largest and most popular parks are Bayfront Park and Bicentennial Park (located in the heart of Downtown and the location of the American Airlines Arena and Bayside Marketplace), Tropical Park, Peacock Park, Morningside Park, Virginia Key, and Watson Island.
The government of the City of Miami (proper) uses the mayor-commissioner type of system. The city commission consists of five commissioners which are elected from single member districts. The city commission constitutes the governing body with powers to pass ordinances, adopt regulations, and exercise all powers conferred upon the city in the city charter. The mayor is elected at large and appoints a city manager. The City of Miami is governed by Mayor Tomás Regalado and 5 City commissioners which oversee the five districts in the City. The commission's regular meetings are held at Miami City Hall, which is located at 3500 Pan American Drive on Dinner Key in the neighborhood of Coconut Grove .
Miami has one of the largest television markets in the nation and the second largest in the state of Florida. Miami has several major newspapers, the main and largest newspaper being The Miami Herald. El Nuevo Herald is the major and largest Spanish-language newspaper. The Miami Herald and El Nuevo Herald are Miami's and South Florida's main, major and largest newspapers. The papers left their longtime home in downtown Miami in 2013. The newspapers are now headquartered at the former home of U.S. Southern Command in Doral.
Other major newspapers include Miami Today, headquartered in Brickell, Miami New Times, headquartered in Midtown, Miami Sun Post, South Florida Business Journal, Miami Times, and Biscayne Boulevard Times. An additional Spanish-language newspapers, Diario Las Americas also serve Miami. The Miami Herald is Miami's primary newspaper with over a million readers and is headquartered in Downtown in Herald Plaza. Several other student newspapers from the local universities, such as the oldest, the University of Miami's The Miami Hurricane, Florida International University's The Beacon, Miami-Dade College's The Metropolis, Barry University's The Buccaneer, amongst others. Many neighborhoods and neighboring areas also have their own local newspapers such as the Aventura News, Coral Gables Tribune, Biscayne Bay Tribune, and the Palmetto Bay News.
Miami is also the headquarters and main production city of many of the world's largest television networks, record label companies, broadcasting companies and production facilities, such as Telemundo, TeleFutura, Galavisión, Mega TV, Univisión, Univision Communications, Inc., Universal Music Latin Entertainment, RCTV International and Sunbeam Television. In 2009, Univisión announced plans to build a new production studio in Miami, dubbed 'Univisión Studios'. Univisión Studios is currently headquartered in Miami, and will produce programming for all of Univisión Communications' television networks.
Miami International Airport serves as the primary international airport of the Greater Miami Area. One of the busiest international airports in the world, Miami International Airport caters to over 35 million passengers a year. The airport is a major hub and the single largest international gateway for American Airlines. Miami International is the busiest airport in Florida, and is the United States' second-largest international port of entry for foreign air passengers behind New York's John F. Kennedy International Airport, and is the seventh-largest such gateway in the world. The airport's extensive international route network includes non-stop flights to over seventy international cities in North and South America, Europe, Asia, and the Middle East.
Miami is home to one of the largest ports in the United States, the PortMiami. It is the largest cruise ship port in the world. The port is often called the "Cruise Capital of the World" and the "Cargo Gateway of the Americas". It has retained its status as the number one cruise/passenger port in the world for well over a decade accommodating the largest cruise ships and the major cruise lines. In 2007, the port served 3,787,410 passengers. Additionally, the port is one of the nation's busiest cargo ports, importing 7.8 million tons of cargo in 2007. Among North American ports, it ranks second only to the Port of South Louisiana in New Orleans in terms of cargo tonnage imported/exported from Latin America. The port is on 518 acres (2 km2) and has 7 passenger terminals. China is the port's number one import country, and Honduras is the number one export country. Miami has the world's largest amount of cruise line headquarters, home to: Carnival Cruise Lines, Celebrity Cruises, Norwegian Cruise Line, Oceania Cruises, and Royal Caribbean International. In 2014, the Port of Miami Tunnel was completed and will serve the PortMiami.
Miami's heavy-rail rapid transit system, Metrorail, is an elevated system comprising two lines and 23 stations on a 24.4-mile (39.3 km)-long line. Metrorail connects the urban western suburbs of Hialeah, Medley, and inner-city Miami with suburban The Roads, Coconut Grove, Coral Gables, South Miami and urban Kendall via the central business districts of Miami International Airport, the Civic Center, and Downtown. A free, elevated people mover, Metromover, operates 21 stations on three different lines in greater Downtown Miami, with a station at roughly every two blocks of Downtown and Brickell. Several expansion projects are being funded by a transit development sales tax surcharge throughout Miami-Dade County.
Construction is currently underway on the Miami Intermodal Center and Miami Central Station, a massive transportation hub servicing Metrorail, Amtrak, Tri-Rail, Metrobus, Greyhound Lines, taxis, rental cars, MIA Mover, private automobiles, bicycles and pedestrians adjacent to Miami International Airport. Completion of the Miami Intermodal Center is expected to be completed by winter 2011, and will serve over 150,000 commuters and travelers in the Miami area. Phase I of Miami Central Station is scheduled to begin service in the spring of 2012, and Phase II in 2013.
Miami is the southern terminus of Amtrak's Atlantic Coast services, running two lines, the Silver Meteor and the Silver Star, both terminating in New York City. The Miami Amtrak Station is located in the suburb of Hialeah near the Tri-Rail/Metrorail Station on NW 79 St and NW 38 Ave. Current construction of the Miami Central Station will move all Amtrak operations from its current out-of-the-way location to a centralized location with Metrorail, MIA Mover, Tri-Rail, Miami International Airport, and the Miami Intermodal Center all within the same station closer to Downtown. The station was expected to be completed by 2012, but experienced several delays and was later expected to be completed in late 2014, again pushed back to early 2015.
Florida High Speed Rail was a proposed government backed high-speed rail system that would have connected Miami, Orlando, and Tampa. The first phase was planned to connect Orlando and Tampa and was offered federal funding, but it was turned down by Governor Rick Scott in 2011. The second phase of the line was envisioned to connect Miami. By 2014, a private project known as All Aboard Florida by a company of the historic Florida East Coast Railway began construction of a higher-speed rail line in South Florida that is planned to eventually terminate at Orlando International Airport.
Miami's road system is based along the numerical "Miami Grid" where Flagler Street forms the east-west baseline and Miami Avenue forms the north-south meridian. The corner of Flagler Street and Miami Avenue is in the middle of Downtown in front of the Downtown Macy's (formerly the Burdine's headquarters). The Miami grid is primarily numerical so that, for example, all street addresses north of Flagler Street and west of Miami Avenue have "NW" in their address. Because its point of origin is in Downtown, which is close to the coast, therefore, the "NW" and "SW" quadrants are much larger than the "SE" and "NE" quadrants. Many roads, especially major ones, are also named (e.g., Tamiami Trail/SW 8th St), although, with exceptions, the number is in more common usage among locals.
Miami has six major causeways that span over Biscayne Bay connecting the western mainland, with the eastern barrier islands along the Atlantic Ocean. The Rickenbacker Causeway is the southernmost causeway and connects Brickell to Virginia Key and Key Biscayne. The Venetian Causeway and MacArthur Causeway connect Downtown with South Beach. The Julia Tuttle Causeway connects Midtown and Miami Beach. The 79th Street Causeway connects the Upper East Side with North Beach. The northernmost causeway, the Broad Causeway, is the smallest of Miami's six causeways, and connects North Miami with Bal Harbour.
In recent years the city government, under Mayor Manny Diaz, has taken an ambitious stance in support of bicycling in Miami for both recreation and commuting. Every month, the city hosts "Bike Miami", where major streets in Downtown and Brickell are closed to automobiles, but left open for pedestrians and bicyclists. The event began in November 2008, and has doubled in popularity from 1,500 participants to about 3,000 in the October 2009 Bike Miami. This is the longest-running such event in the US. In October 2009, the city also approved an extensive 20-year plan for bike routes and paths around the city. The city has begun construction of bike routes as of late 2009, and ordinances requiring bike parking in all future construction in the city became mandatory as of October 2009.
Unlike in Westminster style legislatures or as with the Senate Majority Leader, the House Majority Leader's duties and prominence vary depending upon the style and power of the Speaker of the House. Typically, the Speaker does not participate in debate and rarely votes on the floor. In some cases, Majority Leaders have been more influential than the Speaker; notably Tom DeLay who was more prominent than Speaker Dennis Hastert. In addition, Speaker Newt Gingrich delegated to Dick Armey an unprecedented level of authority over scheduling legislation on the House floor.
The current Minority Leader Nancy Pelosi, of the United States House of Representatives serves as floor leader of the opposition party, and is the counterpart to the Majority Leader. Unlike the Majority Leader, the Minority Leader is on the ballot for Speaker of the House during the convening of the Congress. If the Minority Leader's party takes control of the House, and the party officers are all re-elected to their seats, the Minority Leader is usually the party's top choice for Speaker for the next Congress, while the Minority Whip is typically in line to become Majority Leader. The Minority Leader usually meets with the Majority Leader and the Speaker to discuss agreements on controversial issues.
Like the Speaker of the House, the Minority Leaders are typically experienced lawmakers when they win election to this position. When Nancy Pelosi, D-CA, became Minority Leader in the 108th Congress, she had served in the House nearly 20 years and had served as minority whip in the 107th Congress. When her predecessor, Richard Gephardt, D-MO, became minority leader in the 104th House, he had been in the House for almost 20 years, had served as chairman of the Democratic Caucus for four years, had been a 1988 presidential candidate, and had been majority leader from June 1989 until Republicans captured control of the House in the November 1994 elections. Gephardt's predecessor in the minority leadership position was Robert Michel, R-IL, who became GOP Leader in 1981 after spending 24 years in the House. Michel's predecessor, Republican John Rhodes of Arizona, was elected Minority Leader in 1973 after 20 years of House service.
Starting with Republican Nicholas Longworth in 1925, and continued through the Democrats' control of the House from 1931 to 1995, save for Republican majorities in 1947–49 and 1953–55, all majority leaders have directly ascended to the Speakership brought upon by the retirement of the incumbent. The only exceptions during this period were Charles A. Halleck who became Republican House leader and Minority Leader from 1959 to 1965, Hale Boggs who died in a plane crash, and Dick Gephardt who became the Democrats' House leader but as Minority Leader since his party lost control in the 1994 midterm elections. Since 1995, the only Majority Leader to become Speaker is John Boehner, though indirectly as his party lost control in the 2006 midterms elections. He subsequently served as Republican House leader and Minority Leader from 2007 to 2011 and then was elected Speaker when the House reconvened in 2011. In 1998, with Speaker Newt Gingrich announcing his resignation, both Majority Leader Dick Armey and Majority Whip Tom DeLay did not contest the Speakership which eventually went to Chief Deputy Whip Dennis Hastert.
Traditionally, the Speaker is reckoned as the leader of the majority party in the House, with the Majority Leader as second-in-command. For instance, when the Republicans gained the majority in the House after the 2010 elections, Eric Cantor succeeded Boehner as Majority Leader. Despite this, Cantor and his successor, Kevin McCarthy, have been reckoned as the second-ranking Republicans in the House, since Boehner is still reckoned as the leader of the House Republicans. However, there have been some exceptions. The most recent exception to this rule came when Majority Leader Tom DeLay generally overshadowed Speaker Dennis Hastert from 2003 to 2006. In contrast, the Minority Leader is the undisputed leader of the minority party.
When the Majority Leader's party loses control of the House, and if the Speaker and Majority Leader both remain in the leadership hierarchy, convention suggests that they would become the Minority Leader and Minority Whip, respectively. As the minority party has one less leadership position after losing the speaker's chair, there may be a contest for the remaining leadership positions. Nancy Pelosi is the most recent example of an outgoing Speaker seeking the Minority Leader post to retain the House party leadership, as the Democrats lost control of the House in the 2010 elections. Outgoing Speaker Nancy Pelosi ran successfully for Minority Leader in the 112th Congress.
From an institutional perspective, the rules of the House assign a number of specific responsibilities to the minority leader. For example, Rule XII, clause 6, grant the minority leader (or his designee) the right to offer a motion to recommit with instructions; Rule II, clause 6, states the Inspector General shall be appointed by joint recommendation of the Speaker, majority leader, and minority leader; and Rule XV, clause 6, provides that the Speaker, after consultation with the minority leader, may place legislation on the Corrections Calendar. The minority leader also has other institutional duties, such as appointing individuals to certain federal entities.
The roles and responsibilities of the minority leader are not well-defined. To a large extent, the functions of the minority leader are defined by tradition and custom. A minority leader from 1931 to 1939, Representative Bertrand Snell, R-N.Y., provided this "job description": "He is spokesman for his party and enunciates its policies. He is required to be alert and vigilant in defense of the minority's rights. It is his function and duty to criticize constructively the policies and programs of the majority, and to this end employ parliamentary tactics and give close attention to all proposed legislation."
To a large extent, the minority leader's position is a 20th-century innovation. Prior to this time congressional parties were often relatively disorganized, so it was not always evident who functioned as the opposition floor leader. Decades went by before anything like the modern two-party congressional system emerged on Capitol Hill with official titles for those who were its official leaders. However, from the beginning days of Congress, various House members intermittently assumed the role of "opposition leader." Some scholars suggest that Representative James Madison of Virginia informally functioned as the first "minority leader" because in the First Congress he led the opposition to Treasury Secretary Alexander Hamilton's fiscal policies.
During this early period, it was more usual that neither major party grouping (Federalists and Democratic-Republicans) had an official leader. In 1813, for instance, a scholar recounts that the Federalist minority of 36 Members needed a committee of 13 "to represent a party comprising a distinct minority" and "to coordinate the actions of men who were already partisans in the same cause." In 1828, a foreign observer of the House offered this perspective on the absence of formal party leadership on Capitol Hill:
Internal party disunity compounded the difficulty of identifying lawmakers who might have informally functioned as a minority leader. For instance, "seven of the fourteen speakership elections from 1834 through 1859 had at least twenty different candidates in the field. Thirty-six competed in 1839, ninety-seven in 1849, ninety-one in 1859, and 138 in 1855." With so many candidates competing for the speakership, it is not at all clear that one of the defeated lawmakers then assumed the mantle of "minority leader." The Democratic minority from 1861 to 1875 was so completely disorganized that they did not "nominate a candidate for Speaker in two of these seven Congresses and nominated no man more than once in the other five. The defeated candidates were not automatically looked to for leadership."
In the judgment of political scientist Randall Ripley, since 1883 "the candidate for Speaker nominated by the minority party has clearly been the Minority Leader." However, this assertion is subject to dispute. On December 3, 1883, the House elected Democrat John G. Carlisle of Kentucky as Speaker. Republicans placed in nomination for the speakership J. Warren Keifer of Ohio, who was Speaker the previous Congress. Clearly, Keifer was not the Republicans' minority leader. He was a discredited leader in part because as Speaker he arbitrarily handed out "choice jobs to close relatives ... all at handsome salaries." Keifer received "the empty honor of the minority nomination. But with it came a sting -- for while this naturally involves the floor leadership, he was deserted by his [partisan] associates and his career as a national figure terminated ingloriously." Representative Thomas Reed, R-ME, who later became Speaker, assumed the de facto role of minority floor leader in Keifer's stead. "[A]lthough Keifer was the minority's candidate for Speaker, Reed became its acknowledged leader, and ever after, so long as he served in the House, remained the most conspicuous member of his party.
Another scholar contends that the minority leader position emerged even before 1883. On the Democratic side, "there were serious caucus fights for the minority speakership nomination in 1871 and 1873," indicating that the "nomination carried with it some vestige of leadership." Further, when Republicans were in the minority, the party nominated for Speaker a series of prominent lawmakers, including ex-Speaker James Blaine of Maine in 1875, former Appropriations Chairman James A. Garfield of Ohio, in 1876, 1877, and 1879, and ex-Speaker Keifer in 1883. "It is hard to believe that House partisans would place a man in the speakership when in the majority, and nominate him for this office when in the minority, and not look to him for legislative guidance." This was not the case, according to some observers, with respect to ex-Speaker Keifer.
In brief, there is disagreement among historical analysts as to the exact time period when the minority leadership emerged officially as a party position. Nonetheless, it seems safe to conclude that the position emerged during the latter part of the 19th century, a period of strong party organization and professional politicians. This era was "marked by strong partisan attachments, resilient patronage-based party organizations, and...high levels of party voting in Congress." Plainly, these were conditions conducive to the establishment of a more highly differentiated House leadership structure.
Second, Democrats have always elevated their minority floor leader to the speakership upon reclaiming majority status. Republicans have not always followed this leadership succession pattern. In 1919, for instance, Republicans bypassed James R. Mann, R-IL, who had been minority leader for eight years, and elected Frederick Gillett, R-MA, to be Speaker. Mann "had angered many Republicans by objecting to their private bills on the floor;" also he was a protégé of autocratic Speaker Joseph Cannon, R-IL (1903–1911), and many Members "suspected that he would try to re-centralize power in his hands if elected Speaker." More recently, although Robert H. Michel was the Minority Leader in 1994 when the Republicans regained control of the House in the 1994 midterm elections, he had already announced his retirement and had little or no involvement in the campaign, including the Contract with America which was unveiled six weeks before voting day.
In the instance when the Presidency and both Houses of Congress are controlled by one party, the Speaker normally assumes a lower profile and defers to the President. For that situation the House Minority Leader can play the role of a de facto "leader of the opposition", often more so than the Senate Minority Leader, due to the more partisan nature of the House and the greater role of leadership. Minority Leaders who have played prominent roles in opposing the incumbent President have included Gerald Ford, Richard Gephardt, Nancy Pelosi, and John Boehner.
The style and role of any minority leader is influenced by a variety of elements, including personality and contextual factors, such as the size and cohesion of the minority party, whether his or her party controls the White House, the general political climate in the House, and the controversy that is sometimes associated with the legislative agenda. Despite the variability of these factors, there are a number of institutional obligations associated with this position. Many of these assignments or roles are spelled out in the House rule book. Others have devolved upon the position in other ways. To be sure, the minority leader is provided with extra staff resources—beyond those accorded him or her as a Representative—to assist in carrying out diverse leadership functions. Worth emphasis is that there are limits on the institutional role of the minority leader, because the majority party exercises disproportionate influence over the agenda, partisan ratios on committees, staff resources, administrative operations, and the day-to-day schedule and management of floor activities.
In addition, the minority leader has a number of other institutional functions. For instance, the minority leader is sometimes statutorily authorized to appoint individuals to certain federal entities; he or she and the majority leader each name three Members to serve as Private Calendar objectors; he or she is consulted with respect to reconvening the House per the usual formulation of conditional concurrent adjournment resolutions; he or she is a traditional member of the House Office Building Commission; he or she is a member of the United States Capitol Preservation Commission; and he or she may, after consultation with the Speaker, convene an early organizational party caucus or conference. Informally, the minority leader maintains ties with majority party leaders to learn about the schedule and other House matters and forges agreements or understandings with them insofar as feasible.
The minority leader has a number of formal and informal party responsibilities. Formally, the rules of each party specify certain roles and responsibilities for their leader. For example, under Democratic rules for the 106th Congress, the minority leader may call meetings of the Democratic Caucus. He or she is a member of the Democratic Congressional Campaign Committee; names the members of the Democratic Leadership Council; chairs the Policy Committee; and heads the Steering Committee. Examples of other assignments are making "recommendations to the Speaker on all Democratic Members who shall serve as conferees" and nominating party members to the Committees on Rules and House Administration. Republican rules identify generally comparable functions for their top party leader.
A party's floor leader, in conjunction with other party leaders, plays an influential role in the formulation of party policy and programs. He is instrumental in guiding legislation favored by his party through the House, or in resisting those programs of the other party that are considered undesirable by his own party. He is instrumental in devising and implementing his party's strategy on the floor with respect to promoting or opposing legislation. He is kept constantly informed as to the status of legislative business and as to the sentiment of his party respecting particular legislation under consideration. Such information is derived in part from the floor leader's contacts with his party's members serving on House committees, and with the members of the party's whip organization.
Provide Campaign Assistance. Minority leaders are typically energetic and aggressive campaigners for partisan incumbents and challengers. There is hardly any major aspect of campaigning that does not engage their attention. For example, they assist in recruiting qualified candidates; they establish "leadership PACs" to raise and distribute funds to House candidates of their party; they try to persuade partisan colleagues not to retire or run for other offices so as to hold down the number of open seats the party would need to defend; they coordinate their campaign activities with congressional and national party campaign committees; they encourage outside groups to back their candidates; they travel around the country to speak on behalf of party candidates; and they encourage incumbent colleagues to make significant financial contributions to the party's campaign committee. "The amount of time that [Minority Leader] Gephardt is putting in to help the DCCC [Democratic Congressional Campaign Committee] is unheard of," noted a Democratic lobbyist."No DCCC chairman has ever had that kind of support."
Devise Minority Party Strategies. The minority leader, in consultation with other party colleagues, has a range of strategic options that he or she can employ to advance minority party objectives. The options selected depend on a wide range of circumstances, such as the visibility or significance of the issue and the degree of cohesion within the majority party. For instance, a majority party riven by internal dissension, as occurred during the early 1900s when Progressive and "regular" Republicans were at loggerheads, may provide the minority leader with greater opportunities to achieve his or her priorities than if the majority party exhibited high degrees of party cohesion. Among the variable strategies available to the minority party, which can vary from bill to bill and be used in combination or at different stages of the lawmaking process, are the following:
A look at one minority leadership strategy—partisan opposition—may suggest why it might be employed in specific circumstances. The purposes of obstruction are several, such as frustrating the majority party's ability to govern or attracting press and media attention to the alleged ineffectiveness of the majority party. "We know how to delay," remarked Minority Leader Gephardt Dilatory motions to adjourn, appeals of the presiding officer's ruling, or numerous requests for roll call votes are standard time-consuming parliamentary tactics. By stalling action on the majority party's agenda, the minority leader may be able to launch a campaign against a "do-nothing Congress" and convince enough voters to put his party back in charge of the House. To be sure, the minority leader recognizes that "going negative" carries risks and may not be a winning strategy if his party fails to offer policy alternatives that appeal to broad segments of the general public.
Promote and Publicize the Party's Agenda. An important aim of the minority leader is to develop an electorally attractive agenda of ideas and proposals that unites his or her own House members and that energizes and appeals to core electoral supporters as well as independents and swing voters. Despite the minority leader's restricted ability to set the House's agenda, there are still opportunities for him to raise minority priorities. For example, the minority leader may employ, or threaten to use, discharge petitions to try and bring minority priorities to the floor. If he or she is able to attract the required 218 signatures on a discharge petition by attracting majority party supporters, he or she can force minority initiatives to the floor over the opposition of the majority leadership. As a GOP minority leader once said, the challenges he confronted are to "keep our people together, and to look for votes on the other side."
Minority leaders may engage in numerous activities to publicize their party's priorities and to criticize the opposition's. For instance, to keep their party colleagues "on message," they insure that partisan colleagues are sent packets of suggested press releases or "talking points" for constituent meetings in their districts; they help to organize "town meetings" in Members' districts around the country to publicize the party's agenda or a specific priority, such as health care or education; they sponsor party "retreats" to discuss issues and assess the party's public image; they create "theme teams" to craft party messages that might be raised during the one-minute, morning hour, or special order period in the House; they conduct surveys of party colleagues to discern their policy preferences; they establish websites that highlight and distribute party images and issues to users; and they organize task forces or issue teams to formulate party programs and to develop strategies for communicating these programs to the public.
House minority leaders also hold joint news conferences and consult with their counterparts in the Senate—and with the president if their party controls the White House. The overall objectives are to develop a coordinated communications strategy, to share ideas and information, and to present a united front on issues. Minority leaders also make floor speeches and close debate on major issues before the House; they deliver addresses in diverse forums across the country; and they write books or articles that highlight minority party goals and achievements. They must also be prepared "to debate on the floor, ad lib, no notes, on a moment's notice," remarked Minority Leader Michel. In brief, minority leaders are key strategists in developing and promoting the party's agenda and in outlining ways to neutralize the opposition's arguments and proposals.
Confer With the White House. If his or her party controls the White House, the minority leader confers regularly with the President and his aides about issues before Congress, the Administration's agenda, and political events generally. Strategically, the role of the minority leader will vary depending on whether the President is of the same party or the other party. In general, minority leaders will often work to advance the goals and aspirations of their party's President in Congress. When Robert Michel, R-IL, was minority leader (1981–1995), he typically functioned as the "point man" for Republican presidents. President Ronald Reagan's 1981 policy successes in the Democratic controlled House was due in no small measure to Minority Leader Michel's effectiveness in wooing so-called "Reagan Democrats" to support, for instance, the Administration's landmark budget reconciliation bill. There are occasions, of course, when minority leaders will fault the legislative initiatives of their President. On an administration proposal that could adversely affect his district, Michel stated that he might "abdicate my leadership role [on this issue] since I can't harmonize my own views with the administration's." Minority Leader Gephardt, as another example, has publicly opposed a number of President Clinton's legislative initiatives from "fast track" trade authority to various budget issues.
When the White House is controlled by the House majority party, then the House minority leader assumes a larger role in formulating alternatives to executive branch initiatives and in acting as a national spokesperson for his or her party. "As Minority Leader during [President Lyndon Johnson's] Democratic administration, my responsibility has been to propose Republican alternatives," said Minority Leader Gerald Ford, R-MI. Greatly outnumbered in the House, Minority Leader Ford devised a political strategy that allowed Republicans to offer their alternatives in a manner that provided them political protection. As Ford explained:
"We used a technique of laying our program out in general debate," he said. When we got to the amendment phase, we would offer our program as a substitute for the Johnson proposal. If we lost in the Committee of the Whole, then we would usually offer it as a motion to recommit and get a vote on that. And if we lost on the motion to recommit, our Republican members had a choice: They could vote against the Johnson program and say we did our best to come up with a better alternative. Or they could vote for it and make the same argument. Usually we lost; but when you're only 140 out of 435, you don't expect to win many.
Gephardt added that "inclusion and empowerment of the people on the line have to be done to get the best performance" from the minority party. Other techniques for fostering party harmony include the appointment of task forces composed of partisan colleagues with conflicting views to reach consensus on issues; the creation of new leadership positions as a way to reach out and involve a greater diversity of partisans in the leadership structure; and daily meetings in the Leader's office (or at breakfast, lunch, or dinner) to lay out floor strategy or political objectives for the minority party.
The Chief Deputy Whip is the primary assistant to the whip, who is the chief vote counter for his or her party. The current chief deputy majority whip is Republican Patrick McHenry. Within the House Republican Conference, the chief deputy whip is the highest appointed position and often a launching pad for future positions in the House Leadership. The House Democratic Conference has multiple chief deputy whips, led by a Senior Chief Deputy Whip, which is the highest appointed position within the House Democratic Caucus. The current senior chief deputy minority whip, John Lewis, has held his post since 1991.
Traditionally considered the last part of the Stone Age, the Neolithic followed the terminal Holocene Epipaleolithic period and commenced with the beginning of farming, which produced the "Neolithic Revolution". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age). The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.
The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called "proto-Neolithic" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming.
Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures that arose completely independent of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery before developing agriculture.
The Neolithic 1 (PPNA) period began roughly 10,000 years ago in the Levant. A temple area in southeastern Turkey at Göbekli Tepe dated around 9,500 BC may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, evidenced by the lack of permanent housing in the vicinity and may be the oldest known human-made place of worship. At least seven stone circles, covering 25 acres (10 ha), contain limestone pillars carved with animals, insects, and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which might have supported roofs. Other early PPNA sites dating to around 9,500 to 9,000 BCE have been found in Jericho, Israel (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh), Gilgal in the Jordan Valley, and Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.[citation needed]
The Neolithic 2 (PPNB) began around 8,800 BCE according to the ASPRO chronology in the Levant (Jericho, Israel). As with the PPNA dates, there are two versions from the same laboratories noted above. This system of terminology, however, is not convenient for southeast Anatolia and settlements of the middle Anatolia basin. This era was before the Mesolithic era.[citation needed] A settlement of 3,000 inhabitants was found in the outskirts of Amman, Jordan. Considered to be one of the largest prehistoric settlements in the Near East, called 'Ain Ghazal, it was continuously inhabited from approximately 7,250 – 5,000 B.
Around 10,200 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the fertile crescent. Around 10,700 to 9,400 BC a settlement was established in Tell Qaramel, 10 miles north of Aleppo. The settlement included 2 temples dating back to 9,650. Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone and marble wall and contained a population of 2000–3000 people and a massive stone tower. Around 6,400 BC the Halaf culture appeared in Lebanon, Israel and Palestine, Syria, Anatolia, and Northern Mesopotamia and subsisted on dryland agriculture.
In 1981 a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche divided Near East neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics. In 2002 Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods. Natufian (1) between 12,000 and 10,200 BC, Khiamian (2) between 10,200-8,800 BC, PPNA: Sultanian (Jericho), Mureybetian, early PPNB (PPNB ancien) (3) between 8,800-7,600 BC, middle PPNB (PPNB moyen) 7,600-6,900 BC, late PPNB (PPNB récent) (4) between 7,500 and 7,000 BC and a PPNB (sometimes called PPNC) transitional stage (PPNB final) (5) where Halaf and dark faced burnished ware begin to emerge between 6,900-6,400 BC. They also advanced the idea of a transitional stage between the PPNA and PPNB between 8,800 and 8,600 BC at sites like Jerf el Ahmar and Tell Aswad.
Domestication of sheep and goats reached Egypt from the Near East possibly as early as 6,000 BC. Graeme Barker states "The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium bc in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange. Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East.
In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6,500 BC. Anthropomorphic figurines have been found in the Balkans from 6000 BC, and in Central Europe by c. 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing. The Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to c. 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated c. 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and showing a degree of artistry in stone sculpture unique in prehistory to the Maltese islands. After 2500 BC, the Maltese Islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta. In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population certainly different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found in the largest island of the Mediterranean sea.
In 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia. "No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula". The farm was dated between 3600 and 3000 B.C. Pottery, stone projectile points, and possible houses were also found. "In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site.
In Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in America different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic and Paleo-Indian for the preceding period. The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the Southwestern United States it occurred from 500 to 1200 C.E. when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of maize, and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced.
During most of the Neolithic age of Eurasia, people lived in small tribes composed of multiple bands or lineages. There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age. Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian. Beyond Eurasia, however, states were formed during the local Neolithic in three areas, namely in the Preceramic Andes with the Norte Chico Civilization, Formative Mesoamerica and Ancient Hawaiʻi. However, most Neolithic societies were noticeably more hierarchical than the Paleolithic cultures that preceded them and hunter-gatherer cultures in general.
The domestication of large animals (c. 8000 BC) resulted in a dramatic increase in social inequality in most of the areas where it occurred; New Guinea being a notable exception. Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced. However, evidence of social inequality is still disputed, as settlements such as Catal Huyuk reveal a striking lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others.
Families and households were still largely independent economically, and the household was probably the center of life. However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures ("Linearbandkeramik") were building large arrangements of circular ditches between 4800 BC and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour — though non-hierarchical and voluntary work remain possibilities.
There is a large body of evidence for fortified settlements at Linearbandkeramik sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch. Settlements with palisades and weapon-traumatized bones have been discovered, such as at the Talheim Death Pit demonstrates "...systematic violence between groups" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period. This supplanted an earlier view of the Linear Pottery Culture as living a "peaceful, unfortified lifestyle".
Control of labour and inter-group conflict is characteristic of corporate-level or 'tribal' groups, headed by a charismatic individual; whether a 'big man' or a proto-chief, functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age. Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism.
The shelter of the early people changed dramatically from the paleolithic to the neolithic era. In the paleolithic, people did not normally live in permanent constructions. In the neolithic, mud brick houses started appearing that were coated with plaster. The growth of agriculture made permanent houses possible. Doorways were made on the roof, with ladders positioned both on the inside and outside of the houses. The roof was supported by beams from the inside. The rough ground was covered by platforms, mats, and skins on which residents slept. Stilt-houses settlements were common in the Alpine and Pianura Padana (Terramare) region. Remains have been found at the Ljubljana Marshes in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example.
A significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands.
However, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities. Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued.
Another significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development is still debated.
Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit their newly won farmland.
Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatal höyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives.
Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins which are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones which (depending on size) may have served as spindle whorls or loom weights. The clothing worn in the Neolithic Age might be similar to that worn by Ötzi the Iceman, although he was not Neolithic (since he belonged to the later Copper age).
Between 7 September 1940 and 21 May 1941, 16 British cities suffered aerial raids with at least 100 long tons of high explosives. Over a period of 267 days, London was attacked 71 times, Birmingham, Liverpool and Plymouth eight times, Bristol six, Glasgow five, Southampton four, Portsmouth and Hull three and a minimum of one large raid on eight other cities. This was a result of a rapid escalation starting on 24 August 1940, when night bombers aiming for RAF airfields drifted off course and accidentally destroyed several London homes, killing civilians, combined with the UK Prime Minister Winston Churchill's retaliatory bombing of Berlin on the following night.[clarification needed]
From 7 September 1940, one year into the war, London was bombed by the Luftwaffe for 57 consecutive nights. More than one million London houses were destroyed or damaged and more than 40,000 civilians were killed, almost half of them in London. Ports and industrial centres outside London were also attacked. The main Atlantic sea port of Liverpool was bombed, causing nearly 4,000 deaths within the Merseyside area during the war. The North Sea port of Hull, a convenient and easily found target or secondary target for bombers unable to locate their primary targets, was subjected to 86 raids in the Hull Blitz during the war, with a conservative estimate of 1,200 civilians killed and 95 percent of its housing stock destroyed or damaged. Other ports including Bristol, Cardiff, Portsmouth, Plymouth, Southampton and Swansea were also bombed, as were the industrial cities of Birmingham, Belfast, Coventry, Glasgow, Manchester and Sheffield. Birmingham and Coventry were chosen because of the Spitfire and tank factories in Birmingham and the many munitions factories in Coventry. The city centre of Coventry was almost destroyed, as was Coventry Cathedral.
The bombing failed to demoralise the British into surrender or significantly damage the war economy. The eight months of bombing never seriously hampered British production and the war industries continued to operate and expand. The Blitz was only authorised when the Luftwaffe had failed to meet preconditions for a 1940 launch of Operation Sea Lion, the provisionally planned German invasion of Britain. By May 1941 the threat of an invasion of Britain had passed, and Hitler's attention had turned to Operation Barbarossa in the East. In comparison to the later Allied bombing campaign against Germany, the Blitz resulted in relatively few casualties; the British bombing of Hamburg in July 1943 inflicted some 42,000 civilian deaths, about the same as the entire Blitz.
In the 1920s and 1930s, air power theorists Giulio Douhet and Billy Mitchell espoused the idea that air forces could win wars by themselves, without a need for land and sea fighting. It was thought there was no defence against air attack, particularly at night. Enemy industry, their seats of government, factories and communications could be destroyed, effectively taking away their means to resist. It was also thought the bombing of residential centres would cause a collapse of civilian will, which might have led to the collapse of production and civil life. Democracies, where the populace was allowed to show overt disapproval of the ruling government, were thought particularly vulnerable. This thinking was prevalent in both the RAF and what was then known as the United States Army Air Corps (USAAC) between the two world wars. RAF Bomber Command's policy in particular would attempt to achieve victory through the destruction of civilian will, communications and industry.
Within the Luftwaffe, there was a more muted view of strategic bombing. The OKL did not oppose the strategic bombardment of enemy industries and or cities, and believed it could greatly affect the balance of power on the battlefield in Germany's favour by disrupting production and damaging civilian morale, but they did not believe that air power alone could be decisive. Contrary to popular belief, the Luftwaffe did not have a systematic policy of what became known as "terror bombing". Evidence suggests that the Luftwaffe did not adopt an official bombing policy in which civilians became the primary target until 1942.
Wever argued that the Luftwaffe General Staff should not be solely educated in tactical and operational matters. He argued they should be educated in grand strategy, war economics, armament production, and the mentality of potential opponents (also known as mirror imaging). Wever's vision was not realised; the General Staff studies in those subjects fell by the wayside, and the Air Academies focused on tactics, technology, and operational planning, rather than on independent strategic air offensives.
In 1936, Wever was killed in an air crash. The failure to implement his vision for the new Luftwaffe was largely attributable to his immediate successors. Ex-Army personnel Albert Kesselring and Hans-Jürgen Stumpff are usually blamed for the turning away from strategic planning and focusing on close air support. However, it would seem the two most prominent enthusiasts for the focus on ground-support operations (direct or indirect) were actually Hugo Sperrle and Hans Jeschonnek. These men were long-time professional airmen involved in German air services since early in their careers. The Luftwaffe was not pressured into ground support operations because of pressure from the army, or because it was led by ex-army personnel. It was instead a mission that suited the Luftwaffe's existing approach to warfare; a culture of joint inter-service operations, rather than independent strategic air campaigns.
Adolf Hitler failed to pay as much attention to bombing the enemy as he did to protection from enemy bombing, although he had promoted the development of a bomber force in the 1930s and understood that it was possible to use bombers for major strategic purposes. He told the OKL in 1939 that ruthless employment of the Luftwaffe against the heart of the British will to resist could and would follow when the moment was right; however, he quickly developed a lively scepticism toward strategic bombing, confirmed by the results of the Blitz. He frequently complained of the Luftwaffe's inability to damage industries sufficiently, saying, "The munitions industry cannot be interfered with effectively by air raids ... usually the prescribed targets are not hit".
Ultimately, Hitler was trapped within his own vision of bombing as a terror weapon, formed in the 1930s when he threatened smaller nations into accepting German rule rather than submit to air bombardment. This fact had important implications. It showed the extent to which Hitler personally mistook Allied strategy for one of morale breaking instead of one of economic warfare, with the collapse of morale as an additional bonus. Hitler was much more attracted to the political aspects of bombing. As the mere threat of it had produced diplomatic results in the 1930s, he expected that the threat of German retaliation would persuade the Allies to adopt a policy of moderation and not to begin a policy of unrestricted bombing. His hope was — for reasons of political prestige within Germany itself — that the German population would be protected from the Allied bombings. When this proved impossible, he began to fear that popular feeling would turn against his regime, and he redoubled efforts to mount a similar "terror offensive" against Britain in order to produce a stalemate in which both sides would hesitate to use bombing at all.
A major problem in the managing of the Luftwaffe was Hermann Göring. Hitler believed the Luftwaffe was "the most effective strategic weapon", and in reply to repeated requests from the Kriegsmarine for control over aircraft insisted, "We should never have been able to hold our own in this war if we had not had an undivided Luftwaffe". Such principles made it much harder to integrate the air force into the overall strategy and produced in Göring a jealous and damaging defence of his "empire" while removing Hitler voluntarily from the systematic direction of the Luftwaffe at either the strategic or operational level. When Hitler tried to intervene more in the running of the air force later in the war, he was faced with a political conflict of his own making between himself and Göring, which was not fully resolved until the war was almost over. In 1940 and 1941, Göring's refusal to cooperate with the Kriegsmarine denied the entire Wehrmacht military forces of the Reich the chance to strangle British sea communications, which might have had strategic or decisive effect in the war against the British Empire.
The deliberate separation of the Luftwaffe from the rest of the military structure encouraged the emergence of a major "communications gap" between Hitler and the Luftwaffe, which other factors helped to exacerbate. For one thing, Göring's fear of Hitler led him to falsify or misrepresent what information was available in the direction of an uncritical and over-optimistic interpretation of air strength. When Göring decided against continuing Wever's original heavy bomber programme in 1937, the Reichsmarschall's own explanation was that Hitler wanted to know only how many bombers there were, not how many engines each had. In July 1939, Göring arranged a display of the Luftwaffe's most advanced equipment at Rechlin, to give the impression the air force was more prepared for a strategic air war than was actually the case.
Within hours of the UK and France declaring war on Germany on 3 September 1939, the RAF bombed German warships along the German coast at Wilhelmshaven. Thereafter bombing operations were against ports and shipping and propaganda leaflet drops. Operations were planned to minimize civilian casualties. From 15 May 1940 – the day after the Luftwaffe destroyed the centre of Rotterdam – the RAF also carried out operations east of the Rhine, attacking industrial and transportation targets. Operations were carried out every night thereafter.
Although not specifically prepared to conduct independent strategic air operations against an opponent, the Luftwaffe was expected to do so over Britain. From July until September 1940 the Luftwaffe attacked RAF Fighter Command to gain air superiority as a prelude to invasion. This involved the bombing of English Channel convoys, ports, and RAF airfields and supporting industries. Destroying RAF Fighter Command would allow the Germans to gain control of the skies over the invasion area. It was supposed that Bomber Command, RAF Coastal Command and the Royal Navy could not operate under conditions of German air superiority.
The Luftwaffe's poor intelligence meant that their aircraft were not always able to locate their targets, and thus attacks on factories and airfields failed to achieve the desired results. British fighter aircraft production continued at a rate surpassing Germany's by 2 to 1. The British produced 10,000 aircraft in 1940, in comparison to Germany's 8,000. The replacement of pilots and aircrew was more difficult. Both the RAF and Luftwaffe struggled to replace manpower losses, though the Germans had larger reserves of trained aircrew. The circumstances affected the Germans more than the British. Operating over home territory, British flyers could fly again if they survived being shot down. German crews, even if they survived, faced capture. Moreover, bombers had four to five crewmen on board, representing a greater loss of manpower. On 7 September, the Germans shifted away from the destruction of the RAF's supporting structures. German intelligence suggested Fighter Command was weakening, and an attack on London would force it into a final battle of annihilation while compelling the British Government to surrender.
The decision to change strategy is sometimes claimed as a major mistake by the Oberkommando der Luftwaffe (OKL). It is argued that persisting with attacks on RAF airfields might have won air superiority for the Luftwaffe. Others argue that the Luftwaffe made little impression on Fighter Command in the last week of August and first week of September and that the shift in strategy was not decisive. It has also been argued that it was doubtful the Luftwaffe could have won air superiority before the "weather window" began to deteriorate in October. It was also possible, if RAF losses became severe, that they could pull out to the north, wait for the German invasion, then redeploy southward again. Other historians argue that the outcome of the air battle was irrelevant; the massive numerical superiority of British naval forces and the inherent weakness of the Kriegsmarine would have made the projected German invasion, Unternehmen Seelöwe (Operation Sea Lion), a disaster with or without German air superiority.
Regardless of the ability of the Luftwaffe to win air superiority, Adolf Hitler was frustrated that it was not happening quickly enough. With no sign of the RAF weakening, and Luftwaffe air fleets (Luftflotten) taking punishing losses, the OKL was keen for a change in strategy. To reduce losses further, a change in strategy was also favoured to take place at night, to give the bombers greater protection under cover of darkness.[b] On 4 September 1940, in a long address at the Sportspalast, Hitler declared: "And should the Royal Air Force drop two thousand, or three thousand [kilograms ...] then we will now drop [...] 300,000, 400,000, yes one million kilograms in a single night. And should they declare they will greatly increase their attacks on our cities, then we will erase their cities."
It was decided to focus on bombing Britain's industrial cities in daylight to begin with. The main focus of the bombing operations was against the city of London. The first major raid in this regard took place on 7 September. On 15 September, on a date known as the Battle of Britain Day, a large-scale raid was launched in daylight, but suffered significant loss for no lasting gain. Although there were a few large air battles fought in daylight later in the month and into October, the Luftwaffe switched its main effort to night attacks in order to reduce losses. This became official policy on 7 October. The air campaign soon got underway against London and other British cities. However, the Luftwaffe faced limitations. Its aircraft—Dornier Do 17, Junkers Ju 88, and Heinkel He 111s—were capable of carrying out strategic missions, but were incapable of doing greater damage because of bomb-load limitations. The Luftwaffe's decision in the interwar period to concentrate on medium bombers can be attributed to several reasons: Hitler did not intend or foresee a war with Britain in 1939; the OKL believed a medium bomber could carry out strategic missions just as well as a heavy bomber force; and Germany did not possess the resources or technical ability to produce four-engined bombers before the war.
Although it had equipment capable of doing serious damage, the problem for the Luftwaffe was its unclear strategy and poor intelligence. OKL had not been informed that Britain was to be considered a potential opponent until early 1938. It had no time to gather reliable intelligence on Britain's industries. Moreover, OKL could not settle on an appropriate strategy. German planners had to decide whether the Luftwaffe should deliver the weight of its attacks against a specific segment of British industry such as aircraft factories, or against a system of interrelated industries such as Britain's import and distribution network, or even in a blow aimed at breaking the morale of the British population. The Luftwaffe's strategy became increasingly aimless over the winter of 1940–1941. Disputes among the OKL staff revolved more around tactics than strategy. This method condemned the offensive over Britain to failure before it began.
In an operational capacity, limitations in weapons technology and quick British reactions were making it more difficult to achieve strategic effect. Attacking ports, shipping and imports as well as disrupting rail traffic in the surrounding areas, especially the distribution of coal, an important fuel in all industrial economies of the Second World War, would net a positive result. However, the use of delayed-action bombs, while initially very effective, gradually had less impact, partly because they failed to detonate.[c] Moreover, the British had anticipated the change in strategy and dispersed its production facilities making them less vulnerable to a concentrated attack. Regional commissioners were given plenipotentiary powers to restore communications and organise the distribution of supplies to keep the war economy moving.
Based on experience with German strategic bombing during World War I against the United Kingdom, the British government estimated after the war that 50 casualties— with about one third killed— would result for every tonne of bombs dropped on London. The estimate of tonnes of bombs an enemy could drop per day grew as aircraft technology advanced, from 75 in 1922, to 150 in 1934, to 644 in 1937. That year the Committee on Imperial Defence estimated that an attack of 60 days would result in 600,000 dead and 1,200,000 wounded. News reports of the Spanish Civil War, such as the bombing of Barcelona, supported the 50-casualties-per-tonne estimate. By 1938 experts generally expected that Germany would attempt to drop as much as 3,500 tonnes in the first 24 hours of war and average 700 tonnes a day for several weeks. In addition to high explosive and incendiary bombs the enemy would possibly use poison gas and even bacteriological warfare, all with a high degree of accuracy. In 1939 military theorist Basil Liddell-Hart predicted that 250,000 deaths and injuries in Britain could occur in the first week of war.
In addition to the dead and wounded, government leaders feared mass psychological trauma from aerial attack and a resulting collapse of civil society. A committee of psychiatrists reported to the government in 1938 that there would be three times as many mental as physical casualties from aerial bombing, implying three to four million psychiatric patients. Winston Churchill told Parliament in 1934, "We must expect that, under the pressure of continuous attack upon London, at least three or four million people would be driven out into the open country around the metropolis." Panicked reactions during the Munich crisis, such as the migration by 150,000 to Wales, contributed to fear of societal chaos.
The government planned to voluntarily evacuate four million people—mostly women and children—from urban areas, including 1.4 million from London. It expected about 90% of evacuees to stay in private homes, and conducted an extensive survey to determine available space. Detailed preparations for transporting them were developed. A trial blackout was held on 10 August 1939, and when Germany invaded Poland on 1 September a blackout began at sunset. Lights would not be allowed after dark for almost six years, and the blackout became by far the most unpopular aspect of the war for civilians, more than rationing.:51,106 The relocation of the government and the civil service was also planned, but would only have occurred if necessary so as not to damage civilian morale.:33
Much civil-defence preparation in the form of shelters was left in the hands of local authorities, and many areas such as Birmingham, Coventry, Belfast and the East End of London did not have enough shelters. The Phoney War, however, and the unexpected delay of civilian bombing permitted the shelter programme to finish in June 1940.:35 The programme favoured backyard Anderson shelters and small brick surface shelters; many of the latter were soon abandoned in 1940 as unsafe. In addition, authorities expected that the raids would be brief and during the day. Few predicted that attacks by night would force Londoners to sleep in shelters.
Very deeply buried shelters provided the most protection against a direct hit. The government did not build them for large populations before the war because of cost, time to build, and fears that their very safety would cause occupants to refuse to leave to return to work, or that anti-war sentiment would develop in large groups. The government saw the Communist Party's leading role in advocating for building deep shelters as an attempt to damage civilian morale, especially after the Molotov-Ribbentrop Pact of August 1939.:34
The most important existing communal shelters were the London Underground stations. Although many civilians had used them as such during the First World War, the government in 1939 refused to allow the stations to be used as shelters so as not to interfere with commuter and troop travel, and the fears that occupants might refuse to leave. Underground officials were ordered to lock station entrances during raids; but by the second week of heavy bombing the government relented and ordered the stations to be opened. Each day orderly lines of people queued until 4 pm, when they were allowed to enter the stations. In mid-September 1940 about 150,000 a night slept in the Underground, although by the winter and spring months the numbers had declined to 100,000 or less. Noises of battle were muffled and sleep was easier in the deepest stations, but many were killed from direct hits on several stations.
Communal shelters never housed more than one seventh of Greater London residents, however. Peak use of the Underground as shelter was 177,000 on 27 September 1940, and a November 1940 census of London found that about 4% of residents used the Tube and other large shelters; 9% in public surface shelters; and 27% in private home shelters, implying that the remaining 60% of the city likely stayed at home. The government distributed Anderson shelters until 1941 and that year began distributing the Morrison shelter, which could be used inside homes.:190
Public demand caused the government in October 1940 to build new deep shelters:189–190 within the Underground to hold 80,000 people but were not completed until the period of heaviest bombing had passed. By the end of 1940 significant improvements had been made in the Underground and in many other large shelters. Authorities provided stoves and bathrooms and canteen trains provided food. Tickets were issued for bunks in large shelters to reduce the amount of time spent queuing. Committees quickly formed within shelters as informal governments, and organisations such as the British Red Cross and the Salvation Army worked to improve conditions. Entertainment included concerts, films, plays and books from local libraries. 
Although the intensity of the bombing was not as great as prewar expectations so an equal comparison is impossible, no psychiatric crisis occurred because of the Blitz even during the period of greatest bombing of September 1940. An American witness wrote "By every test and measure I am able to apply, these people are staunch to the bone and won't quit ... the British are stronger and in a better position than they were at its beginning". People referred to raids as if they were weather, stating that a day was "very blitzy".:75,261 However, another American who visited Britain, the publisher Ralph Ingersoll, wrote soon after the Blitz eased on 15 September that:
Ingersoll added that, according to Anna Freud and Edward Glover, London civilians surprisingly did not suffer from widespread shell shock, unlike the soldiers in the Dunkirk evacuation.:114,117–118 The psychoanalysts were correct, and the special network of psychiatric clinics opened to receive mental casualties of the attacks closed due to lack of need. Although the stress of the war resulted in many anxiety attacks, eating disorders, fatigue, weeping, miscarriages, and other physical and mental ailments, society did not collapse. The number of suicides and drunkenness declined, and London recorded only about two cases of "bomb neuroses" per week in the first three months of bombing. Many civilians found that the best way to retain mental stability was to be with family, and after the first few weeks of bombing avoidance of the evacuation programs grew.:80–81 Glover speculated that the knowledge that the entire country was being attacked, that there was no way to escape the bombs, forced people to accept and deal with the situation.:118
The cheerful crowds visiting bomb sites were so large they interfered with rescue work, pub visits increased in number (beer was never rationed), and 13,000 attended cricket at Lord's. People left shelters when told instead of refusing to leave, although many housewives reportedly enjoyed the break from housework. Some people even told government surveyors that they enjoyed air raids if they occurred occasionally, perhaps once a week. Despite the attacks, defeat in Norway and France, and the threat of invasion, overall morale remained high; a Gallup poll found only 3% of Britons expected to lose the war in May 1940, another found an 88% approval rating for Churchill in July, and a third found 89% support for his leadership in October. Support for peace negotiations declined from 29% in February. Each setback caused more civilians to volunteer to become unpaid Local Defence Volunteers, workers worked longer shifts and over weekends, contributions rose to the £5,000 "Spitfire Funds" to build fighters, and the number of work days lost to strikes in 1940 was the lowest in history.:60–63,67–68,75,78–79,215–216
The civilians of London had an enormous role to play in the protection of their city. Many civilians who were unwilling or unable to join the military became members of the Home Guard, the Air Raid Precautions service (ARP), the Auxiliary Fire Service, and many other organisations. The AFS had 138,000 personnel by July 1939. Only one year earlier, there had only been 6,600 full-time and 13,800 part-time firemen in the entire country. During the Blitz, The Scout Association guided fire engines to where they were most needed, and became known as the "Blitz Scouts". Many unemployed were drafted into the Royal Army Pay Corps. These personnel, along with others from the Pioneer Corps, were charged with the task of salvage and clean-up.
The WVS (Women's Voluntary Services for Civil Defence) was set up under the direction of Samuel Hoare, Home Secretary in 1938 specifically in the event of air raids. Hoare considered it the female branch of the ARP. They organised the evacuation of children, established centres for those displaced by bombing, and operated canteens, salvage and recycling schemes. By the end of 1941, the WVS had one million members. Prior to the outbreak of war, civilians were issued with 50 million respirators (gas masks). These were issued in the event of bombing taking place with gas before evacuation.
In the inter-war years and after 1940, Hugh Dowding, Air Officer Commanding Fighter Command has received credit for the defence of British air space and the failure of the Luftwaffe to achieve air superiority. However, Dowding had spent so much effort preparing day fighter defences, there was little to prevent the Germans carrying out an alternative strategy by bombing at night. When the Luftwaffe struck at British cities for the first time on 7 September 1940, a number of civic and political leaders were worried by Dowding's apparent lack of reaction to the new crisis.
Dowding accepted that as AOC, he was responsible for the day and night defence of Britain, and the blame, should he fail, would be laid at his door. When urgent changes and improvements needed to be made, Dowding seemed reluctant to act quickly. The Air Staff felt that this was due to his stubborn nature and reluctance to cooperate. Dowding's opponents in the Air Ministry, already critical of his handling of the day battle (see Battle of Britain Day and the Big Wing controversy), were ready to use these failings as a cudgel with which to attack him and his abilities.
Dowding was summoned to an Air Ministry conference on 17 October 1940 to explain the poor state of night defences and the supposed (but ultimately successful) "failure" of his daytime strategy. The criticism of his leadership extended far beyond the Air Council, and the Minister of Aircraft Production, Lord Beaverbrook, and Churchill themselves intimated their support was waning. While the failure of night defence preparation was undeniable, it was not the AOC's responsibility to accrue resources. The general neglect of the RAF until the late spurt in 1938 had left sparse resources to build defences. While it was permissible to disagree with Dowding's operational and tactical deployment of forces, the failure of the Government and Air Ministry to allot resources was ultimately the responsibility of the civil and military institutions at large. In the pre-war period, the Chamberlain Government stated that night defence from air attack should not take up much of the national effort and, along with the Air Ministry, did not make it a priority.
The attitude of the Air Ministry was in contrast to the experiences of the First World War when a few German bombers caused physical and psychological damage out of all proportion to their numbers. Around 280 short tons (250 t) (9,000 bombs) had been dropped, killing 1,413 people and injuring 3,500 more. Most people aged 35 or over remembered the threat and greeted the bombings with great trepidation. From 1916–1918, German raids had diminished against countermeasures which demonstrated defence against night air raids was possible.
Although night air defence was causing greater concern before the war, it was not at the forefront of RAF planning. Most of the resources went into planning for daylight fighter defences. The difficulty RAF bombers had navigating in darkness, led the British to believe German bombers would suffer the same problems and would be unable to reach and identify their targets. There was also a mentality in all air forces that, if they could carry out effective operations by day, night missions and their disadvantages could be avoided.
British air doctrine, since the time of Chief of the Air Staff Hugh Trenchard in the early 1920s, had stressed offence was the best means of defence. British defensive strategy revolved around offensive action, what became known as the cult of the offensive. To prevent German formations from hitting targets in Britain, RAF's Bomber Command would destroy Luftwaffe aircraft on their own bases, aircraft in their factories and fuel reserves by attacking oil plants. This philosophy was impractical as Bomber Command lacked the technology and equipment and needed several years to develop it. This strategy retarded the development of fighter defences in the 1930s. Dowding agreed air defence would require some offensive action, and fighters could not defend Britain alone. Until September 1940, the RAF lacked specialist night-fighting aircraft and relied on anti-aircraft units which were poorly equipped and lacking in numbers.
Bomber crews already had some experience with these types of systems due to the deployment of the Lorenz beam, a commercial blind-landing aid which allowed aircraft to land at night or in bad weather. The Germans developed the short-range Lorenz system into the Knickebein aid, a system which used two Lorenz beams with much stronger signal transmissions. The concept was the same as the Lorenz system. Two aerials were rotated for the two converging beams which were pointed to cross directly over the target. The German bombers would attach themselves to either beam and fly along it until they started to pick up the signal from the other beam. When a continuous sound was heard from the second beam the crew knew they were above the target and began dropping their bombs.
While Knickebein was used by German crews en masse, X-Gerät use was limited to specially trained pathfinder crews. Special receivers were mounted in He 111s, with a radio mast on the bomber's fuselage. The system worked on a higher frequency (66–77 MHz, compared to Knickebein's 30–33 MHz). Transmitters on the ground sent pulses at a rate of 180 per minute. X-Gerät received and analysed the pulses, giving the pilot both visual and aural "on course" signals. Three beams intersected the beam along the He 111's flight path. The first cross-beam acted as a warning for the bomb-aimer to start the bombing-clock which he would activate only when the second cross-beam was reached. When the third cross-beam was reached the bomb aimer activated a third trigger, which stopped the first hand of the equipment's clock, with the second hand continuing. When the second hand re-aligned with the first, the bombs were released. The clock's timing mechanism was co-ordinated with the distances of the intersecting beams from the target so the target was directly below when the bomb release occurred.
Y-Gerät was the most complex system of the three. It was, in effect, an automatic beam-tracking system, operated through the bomber's autopilot. The single approach beam along which the bomber tracked was monitored by a ground controller. The signals from the station were retransmitted by the bomber's equipment. This way the distance the bomber travelled along the beam could be precisely verified. Direction-finding checks also enabled the controller to keep the crew on an exact course. The crew would be ordered to drop their bombs either by issue of a code word by the ground controller, or at the conclusion of the signal transmissions which would stop. Although its maximum usable range was similar to the previous systems, it was not unknown for specific buildings to be hit.
In June 1940, a German prisoner of war was overheard boasting that the British would never find the Knickebein, even though it was under their noses. The details of the conversation were passed to an RAF Air Staff technical advisor, Dr. R. V. Jones, who started an in-depth investigation which discovered that the Luftwaffe's Lorenz receivers were more than blind-landing devices. Jones therefore began a search for the German beams. Avro Ansons of the Beam Approach Training Development Unit (BATDU) were flown up and down Britain fitted with a 30 MHz receiver to detect them. Soon a beam was traced to Derby (which had been mentioned in Luftwaffe transmissions). The first jamming operations were carried out using requisitioned hospital electrocautery machines. A subtle form of distortion was introduced. Up to nine special transmitters directed their signals at the beams in a manner that widened its path, negating its ability to accurately locate targets. Confidence in the device was diminished by the time the Luftwaffe decided to launch large-scale raids. The counter operations were carried out by British Electronic Counter Measures (ECM) units under Wing Commander Edward Addison, No. 80 Wing RAF. The production of false radio navigation signals by re-transmitting the originals was a technique known as masking beacons (meacons).
German beacons operated on the medium-frequency band and the signals involved a two-letter Morse identifier followed by a lengthy time-lapse which enabled the Luftwaffe crews to determine the signal's bearing. The Meacon system involved separate locations for a receiver with a directional aerial and a transmitter. The receipt of the German signal by the receiver was duly passed to the transmitter, the signal to be repeated. The action did not guarantee automatic success. If the German bomber flew closer to its own beam than the Meacon then the former signal would come through the stronger on the direction finder. The reverse would apply only if the meacon were closer.
In general, German bombers were likely to get through to their targets without too much difficulty. It was to be some months before an effective night fighter force would be ready, and anti-aircraft defences only became adequate after the Blitz was over, so ruses were created to lure German bombers away from their targets. Throughout 1940, dummy airfields were prepared, good enough to stand up to skilled observation. A number[clarification needed] of bombs fell on these diversionary ("Starfish") targets.
The use of diversionary techniques such as fires had to be made carefully. The fake fires could only begin when the bombing started over an adjacent target and its effects were brought under control. Too early and the chances of success receded; too late and the real conflagration at the target would exceed the diversionary fires. Another innovation was the boiler fire. These units were fed from two adjacent tanks containing oil and water. The oil-fed fires were then injected with water from time to time; the flashes produced were similar to those of the German C-250 and C-500 Flammbomben. The hope was that, if it could deceive German bombardiers, it would draw more bombers away from the real target.
Initially the change in strategy caught the RAF off-guard, and caused extensive damage and civilian casualties. Some 107,400 long tons (109,100 t) of shipping was damaged in the Thames Estuary and 1,600 civilians were casualties. Of this total around 400 were killed. The fighting in the air was more intense in daylight. Overall Loge had cost the Luftwaffe 41 aircraft; 14 bombers, 16 Messerschmitt Bf 109s, seven Messerschmitt Bf 110s and four reconnaissance aircraft. Fighter Command lost 23 fighters, with six pilots killed and another seven wounded. Another 247 bombers from Sperrle's Luftflotte 3 (Air Fleet 3) attacked that night. On 8 September, the Luftwaffe returned. This time 412 people were killed and 747 severely wounded.
On 9 September the OKL appeared to be backing two strategies. Its round-the-clock bombing of London was an immediate attempt to force the British government to capitulate, but it was also striking at Britain's vital sea communications to achieve a victory through siege. Although the weather was poor, heavy raids took place that afternoon on the London suburbs and the airfield at Farnborough. The day's fighting cost Kesselring and Luftflotte 2 (Air Fleet 2) 24 aircraft, including 13 Bf 109s. Fighter Command lost 17 fighters and six pilots. Over the next few days weather was poor and the next main effort would not be made until 15 September 1940.
On 15 September the Luftwaffe made two large daylight attacks on London along the Thames Estuary, targeting the docks and rail communications in the city. Its hope was to destroy its targets and draw the RAF into defending them, allowing the Luftwaffe to destroy their fighters in large numbers, thereby achieving an air superiority. Large air battles broke out, lasting for most of the day. The first attack merely damaged the rail network for three days, and the second attack failed altogether. The air battle was later commemorated by Battle of Britain Day. The Luftwaffe lost 18 percent of the bombers sent on the operations that day, and failed to gain air superiority.
While Göring was optimistic the Luftwaffe could prevail, Hitler was not. On 17 September he postponed Operation Sea Lion (as it turned out, indefinitely) rather than gamble Germany's newly gained military prestige on a risky cross-Channel operation, particularly in the face of a sceptical Joseph Stalin in the Soviet Union. In the last days of the battle, the bombers became lures in an attempt to draw the RAF into combat with German fighters. But their operations were to no avail; the worsening weather and unsustainable attrition in daylight gave the OKL an excuse to switch to night attacks on 7 October.
On 14 October, the heaviest night attack to date saw 380 German bombers from Luftflotte 3 hit London. Around 200 people were killed and another 2,000 injured. British anti-aircraft defences (General Frederick Alfred Pile) fired 8,326 rounds and shot down only two bombers. On 15 October, the bombers returned and about 900 fires were started by the mix of 415 short tons (376 t) of high explosive and 11 short tons (10.0 t) of incendiaries dropped. Five main rail lines were cut in London and rolling stock damaged.
Loge continued during October. According to German sources, 9,000 short tons (8,200 t) of bombs were dropped in that month, of which about 10 percent of which was dropped in daylight. Over 6,000 short tons (5,400 t) was aimed at London during the night. Attacks on Birmingham and Coventry were subject to 500 short tons (450 t) of bombs between them in the last 10 days of October. Liverpool suffered 200 short tons (180 t) of bombs dropped. Hull and Glasgow were attacked, but 800 short tons (730 t) of bombs were spread out all over Britain. The Metropolitan-Vickers works in Manchester was targeted and 12 short tons (11 t) of bombs dropped against it. Little tonnage was dropped on Fighter Command airfields; Bomber Command airfields were hit instead.
Luftwaffe policy at this point was primarily to continue progressive attacks on London, chiefly by night attack; second, to interfere with production in the vast industrial arms factories of the West Midlands, again chiefly by night attack; and third to disrupt plants and factories during the day by means of fighter-bombers. Kesselring, commanding Luftflotte 2, was ordered to send 50 sorties per night against London and attack eastern harbours in daylight. Sperrle, commanding Luftflotte 3, was ordered to dispatch 250 sorties per night including 100 against the West Midlands. Seeschlange would be carried out by Fliegerkorps X (10th Air Corps) which concentrated on mining operations against shipping. It also took part in the bombing over Britain. By 19/20 April 1941, it had dropped 3,984 mines, ⅓ of the total dropped. The mines' ability to destroy entire streets earned them respect in Britain, but several fell unexploded into British hands allowing counter-measures to be developed which damaged the German anti-shipping campaign.
By mid-November 1940, when the Germans adopted a changed plan, more than 13,000 short tons (12,000 t) of high explosive and nearly 1,000,000 incendiaries had fallen on London. Outside the capital, there had been widespread harassing activity by single aircraft, as well as fairly strong diversionary attacks on Birmingham, Coventry and Liverpool, but no major raids. The London docks and railways communications had taken a heavy pounding, and much damage had been done to the railway system outside. In September, there had been no less than 667 hits on railways in Great Britain, and at one period, between 5,000 and 6,000 wagons were standing idle from the effect of delayed action bombs. But the great bulk of the traffic went on; and Londoners—though they glanced apprehensively each morning at the list of closed stretches of line displayed at their local station, or made strange detours round back streets in the buses—still got to work. For all the destruction of life and property, the observers sent out by the Ministry of Home Security failed to discover the slightest sign of a break in morale. More than 13,000 civilians had been killed, and almost 20,000 injured, in September and October alone, but the death toll was much less than expected. In late 1940, Churchill credited the shelters:
The American observer Ingersoll reported at this time that "as to the accuracy of the bombing of military objectives, here I make no qualifications. The aim is surprisingly, astonishingly, amazingly inaccurate ... The physical damage to civilian London, to sum up, was more general and more extensive than I had imagined. The damage to military targets much less", and stated that he had seen numerous examples of untouched targets surrounded by buildings destroyed by errant bombs. For example, in two months of bombing, Battersea Power Station, perhaps the largest single target in London, had only received one minor hit ("a nick"). No bridge over the Thames had been hit, and the docks were still functioning despite great damage. An airfield was hit 56 times but the runways were never damaged and the field was never out of operation, despite German pilots' familiarity with it from prewar commercial flights. Ingersoll wrote that the difference between the failure of the German campaign against military targets versus its success in continental Europe was the RAF retaining control of the air.:79–80,174
British night air defences were in a poor state. Few anti-aircraft guns had fire-control systems, and the underpowered searchlights were usually ineffective against aircraft at altitudes above 12,000 ft (3,700 m). In July 1940, only 1,200 heavy and 549 light guns were deployed in the whole of Britain. Of the "heavies", some 200 were of the obsolescent 3 in (76 mm) type; the remainder were the effective 4.5 in (110 mm) and 3.7 in (94 mm) guns, with a theoretical "ceiling"' of over 30,000 ft (9,100 m) but a practical limit of 25,000 ft (7,600 m) because the predictor in use could not accept greater heights. The light guns, about half of which were of the excellent Bofors 40 mm, dealt with aircraft only up to 6,000 ft (1,800 m). Although the use of the guns improved civilian morale, with the knowledge the German bomber crews were facing the barrage, it is now believed that the anti-aircraft guns achieved little and in fact the falling shell fragments caused more British casualties on the ground.
London's defences were rapidly reorganised by General Pile, the Commander-in-Chief of Anti-Aircraft Command. The difference this made to the effectiveness of air defences is questionable. The British were still one-third below the establishment of heavy anti-aircraft artillery AAA (or ack-ack) in May 1941, with only 2,631 weapons available. Dowding had to rely on night fighters. From 1940 to 1941, the most successful night-fighter was the Boulton Paul Defiant; its four squadrons shot down more enemy aircraft than any other type. AA defences improved by better use of radar and searchlights. Over several months, the 20,000 shells spent per raider shot down in September 1940, was reduced to 4,087 in January 1941 and to 2,963 shells in February 1941.
Airborne Interception radar (AI) was unreliable. The heavy fighting in the Battle of Britain had eaten up most of Fighter Command's resources, so there was little investment in night fighting. Bombers were flown with airborne search lights out of desperation[citation needed], but to little avail. Of greater potential was the GL (Gunlaying) radar and searchlights with fighter direction from RAF fighter control rooms to begin a GCI system (Ground Control-led Interception) under Group-level control (No. 10 Group RAF, No. 11 Group RAF and No. 12 Group RAF).
Whitehall's disquiet at the failures of the RAF led to the replacement of Dowding (who was already due for retirement) with Sholto Douglas on 25 November. Douglas set about introducing more squadrons and dispersing the few GL sets to create a carpet effect in the southern counties. Still, in February 1941, there remained only seven squadrons with 87 pilots, under half the required strength. The GL carpet was supported by six GCI sets controlling radar-equipped night-fighters. By the height of the Blitz, they were becoming more successful. The number of contacts and combats rose in 1941, from 44 and two in 48 sorties in January 1941, to 204 and 74 in May (643 sorties). But even in May, 67% of the sorties were visual cat's-eye missions. Curiously, while 43% of the contacts in May 1941 were by visual sightings, they accounted for 61% of the combats. Yet when compared with Luftwaffe daylight operations, there was a sharp decline in German losses to 1%. If a vigilant bomber crew could spot the fighter first, they had a decent chance at evading it.
Nevertheless, it was radar that proved to be critical weapon in the night battles over Britain from this point onward. Dowding had introduced the concept of airborne radar and encouraged its usage. Eventually it would become a success. On the night of 22/23 July 1940, Flying Officer Cyril Ashfield (pilot), Pilot Officer Geoffrey Morris (Observer) and Flight Sergeant Reginald Leyland (Air Intercept radar operator) of the Fighter Interception Unit became the first pilot and crew to intercept and destroy an enemy aircraft using onboard radar to guide them to a visual interception, when their AI night fighter brought down a Do 17 off Sussex. On 19 November 1940 the famous RAF night fighter ace John Cunningham shot down a Ju 88 bomber using airborne radar, just as Dowding had predicted.
From November 1940 – February 1941, the Luftwaffe shifted its strategy and attacked other industrial cities. In particular, the West Midlands were targeted. On the night of 13/14 November, 77 He 111s of Kampfgeschwader 26 (26th Bomber Wing, or KG 26) bombed London while 63 from KG 55 hit Birmingham. The next night, a large force hit Coventry. "Pathfinders" from 12 Kampfgruppe 100 (Bomb Group 100 or KGr 100) led 437 bombers from KG 1, KG 3, KG 26, KG 27, KG 55 and Lehrgeschwader 1 (1st Training Wing, or LG 1) which dropped 394 short tons (357 t) of high explosive, 56 short tons (51 t) of incendiaries, and 127 parachute mines. Other sources say 449 bombers and a total of 530 short tons (480 t) of bombs were dropped. The raid against Coventry was particularly devastating, and led to widespread use of the phrase "to conventrate". Over 10,000 incendiaries were dropped. Around 21 factories were seriously damaged in Coventry, and loss of public utilities stopped work at nine others, disrupting industrial output for several months. Only one bomber was lost, to anti-aircraft fire, despite the RAF flying 125 night sorties. No follow up raids were made, as OKL underestimated the British power of recovery (as Bomber Command would do over Germany from 1943–1945). The Germans were surprised by the success of the attack. The concentration had been achieved by accident. The strategic effect of the raid was a brief 20 percent dip in aircraft production.
Five nights later, Birmingham was hit by 369 bombers from KG 54, KG 26, and KG 55. By the end of November, 1,100 bombers were available for night raids. An average of 200 were able to strike per night. This weight of attack went on for two months, with the Luftwaffe dropping 13,900 short tons (12,600 t) of bombs. In November 1940, 6,000 sorties and 23 major attacks (more than 100 tons of bombs dropped) were flown. Two heavy (50 short tons (45 t) of bombs) attacks were also flown. In December, only 11 major and five heavy attacks were made.
Probably the most devastating strike occurred on the evening of 29 December, when German aircraft attacked the City of London itself with incendiary and high explosive bombs, causing a firestorm that has been called the Second Great Fire of London. The first group to use these incendiaries was Kampfgruppe 100 which despatched 10 "pathfinder" He 111s. At 18:17, it released the first of 10,000 fire bombs, eventually amounting to 300 dropped per minute. Altogether, 130 German bombers destroyed the historical centre of London. Civilian casualties on London throughout the Blitz amounted to 28,556 killed, and 25,578 wounded. The Luftwaffe had dropped 18,291 short tons (16,593 t) of bombs.
Not all of the Luftwaffe's effort was made against inland cities. Port cities were also attacked to try to disrupt trade and sea communications. In January Swansea was bombed four times, very heavily. On 17 January around 100 bombers dropped a high concentration of incendiaries, some 32,000 in all. The main damage was inflicted on the commercial and domestic areas. Four days later 230 tons was dropped including 60,000 incendiaries. In Portsmouth Southsea and Gosport waves of 150 bombers destroyed vast swaths of the city with 40,000 incendiaries. Warehouses, rail lines and houses were destroyed and damaged, but the docks were largely untouched.
Although official German air doctrine did target civilian morale, it did not espouse the attacking of civilians directly. It hoped to destroy morale by destroying the enemy's factories and public utilities as well as its food stocks (by attacking shipping). Nevertheless, its official opposition to attacks on civilians became an increasingly moot point when large-scale raids were conducted in November and December 1940. Although not encouraged by official policy, the use of mines and incendiaries, for tactical expediency, came close to indiscriminate bombing. Locating targets in skies obscured by industrial haze meant they needed to be illuminated "without regard for the civilian population".
Special units, such as KGr 100, became the Beleuchtergruppe (Firelighter Group), which used incendiaries and high explosive to mark the target area. The tactic was expanded into Feuerleitung (Blaze Control) with the creation of Brandbombenfelder (Incendiary Fields) to mark targets. These were marked out by parachute flares. Then bombers carrying SC 1000 (1,000 kg (2,205 lb)), SC 1400 (1,400 kg (3,086 lb)), and SC 1800 (1,800 kg (3,968 lb)) "Satan" bombs were used to level streets and residential areas. By December, the SC 2500 (2,500 kg (5,512 lb)) "Max" bomb was used.
These decisions, apparently taken at the Luftflotte or Fliegerkorps level (see Organisation of the Luftwaffe (1933–1945)), meant attacks on individual targets were gradually replaced by what was, for all intents and purposes, an unrestricted area attack or Terrorangriff (Terror Attack). Part of the reason for this was inaccuracy of navigation. The effectiveness of British countermeasures against Knickebein, which was designed to avoid area attacks, forced the Luftwaffe to resort to these methods. The shift from precision bombing to area attack is indicated in the tactical methods and weapons dropped. KGr 100 increased its use of incendiaries from 13–28 percent. By December, this had increased to 92 percent. Use of incendiaries, which were inherently inaccurate, indicated much less care was taken to avoid civilian property close to industrial sites. Other units ceased using parachute flares and opted for explosive target markers. Captured German air crews also indicated the homes of industrial workers were deliberately targeted.
In 1941, the Luftwaffe shifted strategy again. Erich Raeder—commander-in-chief of the Kriegsmarine—had long argued the Luftwaffe should support the German submarine force (U-Bootwaffe) in the Battle of the Atlantic by attacking shipping in the Atlantic Ocean and attacking British ports. Eventually, he convinced Hitler of the need to attack British port facilities. Hitler had been convinced by Raeder that this was the right course of action due to the high success rates of the U-Boat force during this period of the war. Hitler correctly noted that the greatest damage to the British war economy had been done through submarines and air attacks by small numbers of Focke-Wulf Fw 200 naval aircraft. He ordered attacks to be carried out on those targets which were also the target of the Kriegsmarine. This meant that British coastal centres and shipping at sea west of Ireland were the prime targets.
Hitler's interest in this strategy forced Göring and Jeschonnek to review the air war against Britain in January 1941. This led to Göring and Jeschonnek agreeing to Hitler's Directive 23, Directions for operations against the British War Economy, which was published on 6 February 1941 and gave aerial interdiction of British imports by sea top priority. This strategy had been recognised before the war, but Operation Eagle Attack and the following Battle of Britain had got in the way of striking at Britain's sea communications and diverted German air strength to the campaign against the RAF and its supporting structures. The OKL had always regarded the interdiction of sea communications of less importance than bombing land-based aircraft industries.
Directive 23 was the only concession made by Göring to the Kriegsmarine over the strategic bombing strategy of the Luftwaffe against Britain. Thereafter, he would refuse to make available any air units to destroy British dockyards, ports, port facilities, or shipping in dock or at sea, lest Kriegsmarine gain control of more Luftwaffe units. Raeder's successor—Karl Dönitz—would—on the intervention of Hitler—gain control of one unit (KG 40), but Göring would soon regain it. Göring's lack of cooperation was detrimental to the one air strategy with potentially decisive strategic effect on Britain. Instead, he wasted aircraft of Fliegerführer Atlantik (Flying Command Atlantic) on bombing mainland Britain instead of attacks against convoys. For Göring, his prestige had been damaged by the defeat in the Battle of Britain, and he wanted to regain it by subduing Britain by air power alone. He was always reluctant to cooperate with Raeder.
Even so, the decision by OKL to support the strategy in Directive 23 was instigated by two considerations, both of which had little to do with wanting to destroy Britain's sea communications in conjunction with the Kriegsmarine. First, the difficulty in estimating the impact of bombing upon war production was becoming apparent, and second, the conclusion British morale was unlikely to break led OKL to adopt the naval option. The indifference displayed by OKL to Directive 23 was perhaps best demonstrated in operational directives which diluted its effect. They emphasised the core strategic interest was attacking ports but they insisted in maintaining pressure, or diverting strength, onto industries building aircraft, anti-aircraft guns, and explosives. Other targets would be considered if the primary ones could not be attacked because of weather conditions.
A further line in the directive stressed the need to inflict the heaviest losses possible, but also to intensify the air war in order to create the impression an amphibious assault on Britain was planned for 1941. However, meteorological conditions over Britain were not favourable for flying and prevented an escalation in air operations. Airfields became water-logged and the 18 Kampfgruppen (bomber groups) of the Luftwaffe's Kampfgeschwadern (bomber wings) were relocated to Germany for rest and re-equipment.
From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Gerät beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Gerät, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Gerät.
The attacks were focused against western ports in March. These attacks produced some breaks in morale, with civil leaders fleeing the cities before the offensive reached its height. But the Luftwaffe's effort eased in the last 10 attacks as seven Kampfgruppen moved to Austria in preparation for the Balkans Campaign in Yugoslavia and Greece. The shortage of bombers caused the OKL to improvise. Some 50 Junkers Ju 87 Stuka dive-bombers and Jabos (fighter-bombers) were used, officially classed as Leichte Kampfflugzeuge ("light bombers") and sometimes called Leichte Kesselringe ("Light Kesselrings"). The defences failed to prevent widespread damage but on some occasions did prevent German bombers concentrating on their targets. On occasion, only one-third of German bombs hit their targets.
The diversion of heavier bombers to the Balkans meant that the crews and units left behind were asked to fly two or three sorties per night. Bombers were noisy, cold, and vibrated badly. Added to the tension of the mission which exhausted and drained crews, tiredness caught up with and killed many. In one incident on 28/29 April, Peter Stahl of KG 30 was flying on his 50th mission. He fell asleep at the controls of his Ju 88 and woke up to discover the entire crew asleep. He roused them, ensured they took oxygen and Dextro-Energen tablets, then completed the mission.
Regardless, the Luftwaffe could still inflict huge damage. With the German occupation of Western Europe, the intensification of submarine and air attack on Britain's sea communications was feared by the British. Such an event would have serious consequences on the future course of the war, should the Germans succeed. Liverpool and its port became an important destination for convoys heading through the Western Approaches from North America, bringing supplies and materials. The considerable rail network distributed to the rest of the country. Operations against Liverpool in the Liverpool Blitz were successful. Air attacks sank 39,126 long tons (39,754 t) of shipping, with another 111,601 long tons (113,392 t) damaged. Minister of Home Security Herbert Morrison was also worried morale was breaking, noting the defeatism expressed by civilians. Other sources point to half of the port's 144 berths rendered unusable, while cargo unloading capability was reduced by 75%. Roads and railways were blocked and ships could not leave harbour. On 8 May 1941, 57 ships were destroyed, sunk or damaged amounting to 80,000 long tons (81,000 t). Around 66,000 houses were destroyed, 77,000 people made homeless, and 1,900 people killed and 1,450 seriously hurt on one night. Operations against London up until May 1941 could also have a severe impact on morale. The populace of the port of Hull became 'trekkers', people who underwent a mass exodus from cities before, during, and after attacks. However, the attacks failed to knock out or damage railways, or port facilities for long, even in the Port of London, a target of many attacks. The Port of London in particular was an important target, bringing in one-third of overseas trade.
On 13 March, the upper Clyde port of Clydebank near Glasgow was bombed. All but seven of its 12,000 houses were damaged. Many more ports were attacked. Plymouth was attacked five times before the end of the month while Belfast, Hull, and Cardiff were hit. Cardiff was bombed on three nights, Portsmouth centre was devastated by five raids. The rate of civilian housing lost was averaging 40,000 people per week dehoused in September 1940. In March 1941, two raids on Plymouth and London dehoused 148,000 people. Still, while heavily damaged, British ports continued to support war industry and supplies from North America continued to pass through them while the Royal Navy continued to operate in Plymouth, Southampton, and Portsmouth. Plymouth in particular, because of its vulnerable position on the south coast and close proximity to German air bases, was subjected to the heaviest attacks. On 10/11 March, 240 bombers dropped 193 tons of high explosives and 46,000 incendiaries. Many houses and commercial centres were heavily damaged, the electrical supply was knocked out, and five oil tanks and two magazines exploded. Nine days later, two waves of 125 and 170 bombers dropped heavy bombs, including 160 tons of high explosive and 32,000 incendiaries. Much of the city centre was destroyed. Damage was inflicted on the port installations, but many bombs fell on the city itself. On 17 April 346 tons of explosives and 46,000 incendiaries were dropped from 250 bombers led by KG 26. The damage was considerable, and the Germans also used aerial mines. Over 2,000 AAA shells were fired, destroying two Ju 88s. By the end of the air campaign over Britain, only eight percent of the German effort against British ports was made using mines.
In the north, substantial efforts were made against Newcastle-upon-Tyne and Sunderland, which were large ports on the English east coast. On 9 April 1941 Luftflotte 2 dropped 150 tons of high explosives and 50,000 incendiaries from 120 bombers in a five-hour attack. Sewer, rail, docklands, and electric installations were damaged. In Sunderland on 25 April, Luftflotte 2 sent 60 bombers which dropped 80 tons of high explosive and 9,000 incendiaries. Much damage was done. A further attack on the Clyde, this time at Greenock, took place on 6 and 7 May. However, as with the attacks in the south, the Germans failed to prevent maritime movements or cripple industry in the regions.
The last major attack on London was on 10/11 May 1941, on which the Luftwaffe flew 571 sorties and dropped 800 tonnes of bombs. This caused more than 2,000 fires. 1,436 people were killed and 1,792 seriously injured, which affected morale badly. Another raid was carried out on 11/12 May 1941. Westminster Abbey and the Law Courts were damaged, while the Chamber of the House of Commons was destroyed. One-third of London's streets were impassable. All but one railway station line was blocked for several weeks. This raid was significant, as 63 German fighters were sent with the bombers, indicating the growing effectiveness of RAF night fighter defences.
German air supremacy at night was also now under threat. British night-fighter operations out over the Channel were proving highly successful. This was not immediately apparent. The Bristol Blenheim F.1 was undergunned, with just four .303 in (7.7 mm) machine guns which struggled to down the Do 17, Ju 88, or Heinkel He 111. Moreover, the Blenheim struggled to reach the speed of the German bombers. Added to the fact an interception relied on visual sighting, a kill was most elusive even in the conditions of a moonlit sky.
The Boulton Paul Defiant, despite its poor performance during daylight engagements, was a much better night fighter. It was faster, able to catch the bombers and its configuration of four machine guns in a turret could (much like German night fighters in 1943–1945 with Schräge Musik) engage the unsuspecting German bomber from beneath. Attacks from below offered a larger target, compared to attacking tail-on, as well as a better chance of not being seen by the bomber (so less chance of evasion), as well as greater likelihood of detonating its bombload. In subsequent months a steady number of German bombers would fall to night fighters.
Improved aircraft designs were in the offing with the Bristol Beaufighter, then under development. It would prove formidable, but its development was slow. The Beaufighter had a maximum speed of 320 mph (510 km/h), an operational ceiling of 26,000 ft (7,900 m) and a climb rate of 2,500 ft (760 m) per minute. Its armament of four 20 mm (0.79 in) Hispano cannon and six .303 in Browning machine guns offered a serious threat to German bombers. On 19 November, John Cunningham of No. 604 Squadron RAF shot down a bomber flying an AI-equipped Beaufighter. It was the first air victory for the airborne radar.
By April and May 1941, the Luftwaffe was still getting through to their targets, taking no more than one- to two-percent losses on any given mission. On 19/20 April 1941, in honour of Hitler's 52nd birthday, 712 bombers hit Plymouth with a record 1,000 tons of bombs. Losses were minimal. In the following month, 22 German bombers were lost with 13 confirmed to have been shot down by night fighters. On 3/4 May, nine were shot down in one night. On 10/11 May, London suffered severe damage, but 10 German bombers were downed. In May 1941, RAF night fighters shot down 38 German bombers.
The military effectiveness of bombing varied. The Luftwaffe dropped around 45,000 short tons (41,000 t) of bombs during the Blitz disrupting production and transport, reducing food supplies and shaking the British morale. It also helped to support the U-Boat blockade by sinking some 58,000 long tons (59,000 t) of shipping destroyed and 450,000 long tons (460,000 t) damaged. Yet, overall the British production rose steadily throughout this period although there were significant falls during April 1941, probably influenced by the departure of workers of Easter Holidays according to the British official history. The British official history on war production noted the great impact was upon the supply of components rather than complete equipment. In aircraft production, the British were denied the opportunity to reach the planned target of 2,500 aircraft in a month, arguably the greatest achievement of the bombing, as it forced the dispersal of industry. In April 1941, when the targets were British ports, rifle production fell by 25%, filled-shell production by 4.6%, and in smallarms production 4.5% overall. The strategic impact on industrial cities was varied; most took from 10–15 days to recover from heavy raids, although Belfast and Liverpool took longer. The attacks against Birmingham took war industries some three months to recover fully from. The exhausted population took three weeks to overcome the effects of an attack.
The air offensive against the RAF and British industry failed to have the desired effect. More might have been achieved had the OKL exploited their enemy's weak spot, the vulnerability of British sea communications. The Allies did so later when Bomber Command attacked rail communications and the United States Army Air Forces targeted oil, but that would have required an economic-industrial analysis of which the Luftwaffe was incapable. The OKL instead sought clusters of targets that suited the latest policy (which changed frequently), and disputes within the leadership were about tactics rather than strategy. Though militarily ineffective, the Blitz caused enormous damage to Britain's infrastructure and housing stock. It cost around 41,000 lives, and may have injured another 139,000.
The relieved British began to assess the impact of the Blitz in August 1941, and the RAF Air Staff used the German experience to improve Bomber Command's offensives. They concluded bombers should strike a single target each night and use more incendiaries because they had a greater impact on production than high explosives. They also noted regional production was severely disrupted when city centres were devastated through the loss of administrative offices, utilities and transport. They believed the Luftwaffe had failed in precision attack, and concluded the German example of area attack using incendiaries was the way forward for operations over Germany.
Some writers claim the Air Staff ignored a critical lesson, however: British morale did not break. Targeting German morale, as Bomber Command would do, was no more successful. Aviation strategists dispute that morale was ever a major consideration for Bomber Command. Throughout 1933–39 none of the 16 Western Air Plans drafted mentioned morale as a target. The first three directives in 1940 did not mention civilian populations or morale in any way. Morale was not mentioned until the ninth wartime directive on 21 September 1940. The 10th directive in October 1940 mentioned morale by name. However, industrial cities were only to be targeted if weather denied strikes on Bomber Command's main concern, oil.
AOC Bomber Command Arthur Harris did see German morale as a major objective. However, he did not believe that the morale-collapse could occur without the destruction of the German economy. The primary goal of Bomber Command's offensives was to destroy the German industrial base (economic warfare), and in doing so reduce morale. In late 1943, just before the Battle of Berlin, he declared the power of Bomber Command would enable it to achieve "a state of devastation in which surrender is inevitable." A summary of Harris' strategic intentions was clear:
A converse popular image arose of British people in the Second World War: a collection of people locked in national solidarity. This image entered the historiography of the Second World War in the 1980s and 1990s, especially after the publication of Angus Calder's book The Myth of the Blitz (1991). It was evoked by both the right and left political factions in Britain during the Falklands War when it was embedded in a nostalgic narrative in which the Second World War represented aggressive British patriotism successfully defending democracy. This imagery of people in the Blitz was and is powerfully portrayed in film, radio, newspapers and magazines. At the time it was a useful propaganda tool for home and foreign consumption. Historians' critical response to this construction focused on what were seen as over-emphasised claims of righteous nationalism and national unity. In the Myth of the Blitz, Calder exposed some of the counter-evidence of anti-social and divisive behaviours. What he saw as the myth—serene national unity—became "historical truth". In particular, class division was most evident.
In the wake of the Coventry Blitz, there was widespread agitation from the Communist Party over the need for bomb-proof shelters. Many Londoners, in particular, took to using the Underground railway system, without authority, for shelter and sleeping through the night there until the following morning. So worried were the Government over the sudden campaign of leaflets and posters distributed by the Communist Party in Coventry and London, that the Police were sent in to seize their production facilities. The Government, up until November 1940, was opposed to the centralised organisation of shelter. Home Secretary Sir John Anderson was replaced by Morrison soon afterwards, in the wake of a Cabinet reshuffle as the dying Neville Chamberlain resigned. Morrison warned that he could not counter the Communist unrest unless provision of shelters were made. He recognised the right of the public to seize tube stations and authorised plans to improve their condition and expand them by tunnelling. Still, many British citizens, who had been members of the Labour Party, itself inert over the issue, turned to the Communist Party. The Communists attempted to blame the damage and casualties of the Coventry raid on the rich factory owners, big business and landowning interests and called for a negotiated peace. Though they failed to make a large gain in influence, the membership of the Party had doubled by June 1941. The "Communist threat" was deemed important enough for Herbert Morrison to order, with the support of the Cabinet, the stoppage of the Daily Worker and The Week; the Communist newspaper and journal.
The brief success of the Communists also fed into the hands of the British Union of Fascists (BUF). Anti-Semitic attitudes became widespread, particularly in London. Rumours that Jewish support was underpinning the Communist surge were frequent. Rumours that Jews were inflating prices, were responsible for the Black Market, were the first to panic under attack (even the cause of the panic), and secured the best shelters via underhanded methods, were also widespread. Moreover, there was also racial antagonism between the small Black, Indian and Jewish communities. However, the feared race riots did not transpire despite the mixing of different peoples into confined areas.
In other cities, class conflict was more evident. Over a quarter of London's population had left the city by November 1940. Civilians left for more remote areas of the country. Upsurges in population south Wales and Gloucester intimated where these displaced people went. Other reasons, including industry dispersal may have been a factor. However, resentment of rich self-evacuees or hostile treatment of poor ones were signs of persistence of class resentments although these factors did not appear to threaten social order. The total number of evacuees numbered 1.4 million, including a high proportion from the poorest inner-city families. Reception committees were completely unprepared for the condition of some of the children. Far from displaying the nation's unity in time of war, the scheme backfired, often aggravating class antagonism and bolstering prejudice about the urban poor. Within four months, 88% of evacuated mothers, 86% of small children, and 43% of school children had been returned home. The lack of bombing in the Phoney War contributed significantly to the return of people to the cities, but class conflict was not eased a year later when evacuation operations had to be put into effect again.
In recent years a large number of wartime recordings relating to the Blitz have been made available on audiobooks such as The Blitz, The Home Front and British War Broadcasting. These collections include period interviews with civilians, servicemen, aircrew, politicians and Civil Defence personnel, as well as Blitz actuality recordings, news bulletins and public information broadcasts. Notable interviews include Thomas Alderson, the first recipient of the George Cross, John Cormack, who survived eight days trapped beneath rubble on Clydeside, and Herbert Morrison's famous "Britain shall not burn" appeal for more fireguards in December 1940.
Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expression, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through paralanguage. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion.
Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out "noise", i.e. similar molecules without biotic content.
Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
Effective verbal or spoken communication is dependent on a number of factors and cannot be fully isolated from other important interpersonal skills such as non-verbal communication, listening skills and clarification. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word "language" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages. Communication is two-way process not merely one-way.
Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.
The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication.
The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:
In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of "communication noise" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized "nervous system" of plants. The original meaning of the word "neuron" in Greek is "vegetable fiber" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed]
Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997).
Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.
The Royal College of Chemistry was established by private subscription in 1845 as there was a growing awareness that practical aspects of the experimental sciences were not well taught and that in the United Kingdom the teaching of chemistry in particular had fallen behind that in Germany. As a result of a movement earlier in the decade, many politicians donated funds to establish the college, including Benjamin Disraeli, William Gladstone and Robert Peel. It was also supported by Prince Albert, who persuaded August Wilhelm von Hofmann to be the first professor.
City and Guilds College was founded in 1876 from a meeting of 16 of the City of London's livery companies for the Advancement of Technical Education (CGLI), which aimed to improve the training of craftsmen, technicians, technologists, and engineers. The two main objectives were to create a Central Institution in London and to conduct a system of qualifying examinations in technical subjects. Faced with their continuing inability to find a substantial site, the Companies were eventually persuaded by the Secretary of the Science and Art Department, General Sir John Donnelly (who was also a Royal Engineer) to found their institution on the eighty-seven acre (350,000 m²) site at South Kensington bought by the 1851 Exhibition Commissioners (for GBP 342,500) for 'purposes of art and science' in perpetuity. The latter two colleges were incorporated by Royal Charter into the Imperial College of Science and Technology and the CGLI Central Technical College was renamed the City and Guilds College in 1907, but not incorporated into Imperial College until 1910.
In December 2005, Imperial announced a science park programme at the Wye campus, with extensive housing; however, this was abandoned in September 2006 following complaints that the proposal infringed on Areas of Outstanding Natural Beauty, and that the true scale of the scheme, which could have raised £110m for the College, was known to Kent and Ashford Councils and their consultants but concealed from the public. One commentator observed that Imperial's scheme reflected "the state of democracy in Kent, the transformation of a renowned scientific college into a grasping, highly aggressive, neo-corporate institution, and the defence of the status of an Area of Outstanding Natural Beauty – throughout England, not just Wye – against rampant greed backed by the connivance of two important local authorities. Wye College campus was finally closed in September 2009.
The College's endowment is sub-divided into three distinct portfolios: (i) Unitised Scheme – a unit trust vehicle for College, Faculties and Departments to invest endowments and unfettered income to produce returns for the long term; (ii) Non-Core Property – a portfolio containing around 120 operational and developmental properties which College has determined are not core to the academic mission; and (iii) Strategic Asset Investments – containing College’s shareholding in Imperial Innovations and other restricted equity holdings. During the year 2014/15, the market value of the endowment increased by £78 million (18%) to £512.4 million on 31 July 2015.
Imperial submitted a total of 1,257 staff across 14 units of assessment to the 2014 Research Excellence Framework (REF) assessment. In the REF results 46% of Imperial's submitted research was classified as 4*, 44% as 3*, 9% as 2* and 1% as 1*, giving an overall GPA of 3.36. In rankings produced by Times Higher Education based upon the REF results Imperial was ranked 2nd overall for GPA and 8th for "research power" (compared to 6th and 7th respectively in the equivalent rankings for the RAE 2008).
In September 2014, Professor Stefan Grimm, of the Department of Medicine, was found dead after being threatened with dismissal for failure to raise enough grant money. The College made its first public announcement of his death on 4 December 2014. Grimm's last email accused his employers of bullying by demanding that he should get grants worth at least £200,000 per year. His last email was viewed more than 100,000 times in the first four days after it was posted. The College has announced an internal inquiry into Stefan Grimm's death. The inquest on his death has not yet reported.
Imperial College Boat Club
The Imperial College Boat Club was founded on 12 December 1919. The Gold medal winning GB 8+ at the 2000 Sydney Olympics had been based at Imperial College's recently refurbished boathouse and included 3 alumni of the college along with their coach Martin McElroy. The club has been highly successful, with many wins at Henley Royal Regatta including most recently in 2013 with victory in The Prince Albert Challenge Cup event. The club has been home to numerous National Squad oarsmen and women and is open to all rowers not just students of Imperial College London.
The Royal School of Mines was established by Sir Henry de la Beche in 1851, developing from the Museum of Economic Geology, a collection of minerals, maps and mining equipment. He created a school which laid the foundations for the teaching of science in the country, and which has its legacy today at Imperial. Prince Albert was a patron and supporter of the later developments in science teaching, which led to the Royal College of Chemistry becoming part of the Royal School of Mines, to the creation of the Royal College of Science and eventually to these institutions becoming part of his plan for South Kensington being an educational region.
In 2003 Imperial was granted degree-awarding powers in its own right by the Privy Council. The London Centre for Nanotechnology was established in the same year as a joint venture between UCL and Imperial College London. In 2004 the Tanaka Business School (now named the Imperial College Business School) and a new Main Entrance on Exhibition Road were opened by The Queen. The UK Energy Research Centre was also established in 2004 and opened its headquarters at Imperial College. In November 2005 the Faculties of Life Sciences and Physical Sciences merged to become the Faculty of Natural Sciences.
Imperial's main campus is located in the South Kensington area of central London. It is situated in an area of South Kensington, known as Albertopolis, which has a high concentration of cultural and academic institutions, adjacent to the Natural History Museum, the Science Museum, the Victoria and Albert Museum, the Royal College of Music, the Royal College of Art, the Royal Geographical Society and the Royal Albert Hall. Nearby public attractions include the Kensington Palace, Hyde Park and the Kensington Gardens, the National Art Library, and the Brompton Oratory. The expansion of the South Kensington campus in the 1950s & 1960s absorbed the site of the former Imperial Institute, designed by Thomas Collcutt, of which only the 287 foot (87 m) high Queen's Tower remains among the more modern buildings.
The Centre For Co-Curricular Studies provides elective subjects and language courses outside the field of science for students in the other faculties and departments. Students are encouraged to take these classes either for credit or in their own time, and in some departments this is mandatory. Courses exist in a wide range of topics including philosophy, ethics in science and technology, history, modern literature and drama, art in the 20th century, film studies. Language courses are available in French, German, Japanese, Italian, Russian, Spanish, Arabic and Mandarin Chinese. The Centre For Co-Curricular Studies is home to the Science Communication Unit which offers master's degrees in Science Communication and Science Media Production for science graduates.
Furthermore, in terms of job prospects, as of 2014 the average starting salary of an Imperial graduate was the highest of any UK university. In terms of specific course salaries, the Sunday Times ranked Computing graduates from Imperial as earning the second highest average starting salary in the UK after graduation, over all universities and courses. In 2012, the New York Times ranked Imperial College as one of the top 10 most-welcomed universities by the global job market. In May 2014, the university was voted highest in the UK for Job Prospects by students voting in the Whatuni Student Choice Awards Imperial is jointly ranked as the 3rd best university in the UK for the quality of graduates according to recruiters from the UK's major companies.
Imperial College TV
ICTV (formerly STOIC (Student Television of Imperial College)) is Imperial College Union's TV station, founded in 1969 and operated from a small TV studio in the Electrical Engineering block. The department had bought an early AMPEX Type A 1-inch videotape recorder and this was used to produce an occasional short news programme which was then played to students by simply moving the VTR and a monitor into a common room. A cable link to the Southside halls of residence was laid in a tunnel under Exhibition Road in 1972. Besides the news, early productions included a film of the Queen opening what was then called College Block and interview programmes with DJ Mike Raven, Richard O'Brian and Monty Python producer Ian MacNaughton. The society was renamed to ICTV for the start of the 2014/15 academic year.
Non-academic alumni: Author, H. G. Wells, McLaren and Ferrari Chief Designer, Nicholas Tombazis, CEO of Rolls Royce, Ralph Robins, rock band Queen, Brian May, CEO of Singapore Airlines, Chew Choon Seng, Prime Minister of New Zealand, Julius Vogel, Prime Minister of India, Rajiv Gandhi, Deputy Prime Minister of Singapore, Teo Chee Hean, Chief Medical Officer for England, Sir Liam Donaldson, Head Physician to the Queen, Huw Thomas, CEO of Moonfruit, Wendy Tan White, Businessman and philanthropist, Winston Wong, billionaire hedge fund manager Alan Howard.
The Great Exhibition was organised by Prince Albert, Henry Cole, Francis Fuller and other members of the Royal Society for the Encouragement of Arts, Manufactures and Commerce. The Great Exhibition made a surplus of £186,000 used in creating an area in the South of Kensington celebrating the encouragement of the arts, industry, and science. Albert insisted the Great Exhibition surplus should be used as a home for culture and education for everyone. His commitment was to find practical solutions to today's social challenges. Prince Albert's vision built the Victoria and Albert Museum, Science Museum, Natural History Museum, Geological Museum, Royal College of Science, Royal College of Art, Royal School of Mines, Royal School of Music, Royal College of Organists, Royal School of Needlework, Royal Geographical Society, Institute of Recorded Sound, Royal Horticultural Gardens, Royal Albert Hall and the Imperial Institute. Royal colleges and the Imperial Institute merged to form what is now Imperial College London.
In 1907, the newly established Board of Education found that greater capacity for higher technical education was needed and a proposal to merge the City and Guilds College, the Royal School of Mines and the Royal College of Science was approved and passed, creating The Imperial College of Science and Technology as a constituent college of the University of London. Imperial's Royal Charter, granted by Edward VII, was officially signed on 8 July 1907. The main campus of Imperial College was constructed beside the buildings of the Imperial Institute in South Kensington.
In the financial year ended 31 July 2013, Imperial had a total net income of £822.0 million (2011/12 – £765.2 million) and total expenditure of £754.9 million (2011/12 – £702.0 million). Key sources of income included £329.5 million from research grants and contracts (2011/12 – £313.9 million), £186.3 million from academic fees and support grants (2011/12 – £163.1 million), £168.9 million from Funding Council grants (2011/12 – £172.4 million) and £12.5 million from endowment and investment income (2011/12 – £8.1 million). During the 2012/13 financial year Imperial had a capital expenditure of £124 million (2011/12 – £152 million).
In 1988 Imperial merged with St Mary's Hospital Medical School, becoming The Imperial College of Science, Technology and Medicine. In 1995 Imperial launched its own academic publishing house, Imperial College Press, in partnership with World Scientific. Imperial merged with the National Heart and Lung Institute in 1995 and the Charing Cross and Westminster Medical School, Royal Postgraduate Medical School (RPMS) and the Institute of Obstetrics and Gynaecology in 1997. In the same year the Imperial College School of Medicine was formally established and all of the property of Charing Cross and Westminster Medical School, the National Heart and Lung Institute and the Royal Postgraduate Medical School were transferred to Imperial as the result of the Imperial College Act 1997. In 1998 the Sir Alexander Fleming Building was opened by Queen Elizabeth II to provide a headquarters for the College's medical and biomedical research.
The 2008 Research Assessment Exercise returned 26% of the 1225 staff submitted as being world-leading (4*) and a further 47% as being internationally excellent (3*). The 2008 Research Assessment Exercise also showed five subjects – Pure Mathematics, Epidemiology and Public Health, Chemical Engineering, Civil Engineering, and Mechanical, Aeronautical and Manufacturing Engineering – were assessed to be the best[clarification needed] in terms of the proportion of internationally recognised research quality.
Imperial College Healthcare NHS Trust was formed on 1 October 2007 by the merger of Hammersmith Hospitals NHS Trust (Charing Cross Hospital, Hammersmith Hospital and Queen Charlotte's and Chelsea Hospital) and St Mary's NHS Trust (St. Mary's Hospital and Western Eye Hospital) with Imperial College London Faculty of Medicine. It is an academic health science centre and manages five hospitals: Charing Cross Hospital, Queen Charlotte's and Chelsea Hospital, Hammersmith Hospital, St Mary's Hospital, and Western Eye Hospital. The Trust is currently the largest in the UK and has an annual turnover of £800 million, treating more than a million patients a year.[citation needed]
In 2003, it was reported that one third of female academics "believe that discrimination or bullying by managers has held back their careers". It was said then that "A spokesman for Imperial said the college was acting on the recommendations and had already made changes". Nevertheless, allegations of bullying have continued: in 2007, concerns were raised about the methods that were being used to fire people in the Faculty of Medicine. New President of Imperial College, Alice Gast says she sees bright lights in the horizon for female careers at Imperial College London.
Imperial College Union, the students' union at Imperial College, is run by five full-time sabbatical officers elected from the student body for a tenure of one year, and a number of permanent members of staff. The Union is given a large subvention by the university, much of which is spent on maintaining around 300 clubs, projects and societies. Examples of notable student groups and projects are Project Nepal which sends Imperial College students to work on educational development programmes in rural Nepal and the El Salvador Project, a construction based project in Central America. The Union also hosts sports-related clubs such as Imperial College Boat Club and Imperial College Gliding Club.
Imperial College London is a public research university located in London, United Kingdom. It was founded by Prince Albert who envisioned an area composed of the Natural History Museum, Science Museum, Victoria and Albert Museum, Royal Albert Hall and the Imperial Institute. The Imperial Institute was opened by his wife, Queen Victoria, who laid the first stone. In 1907, Imperial College London was formed by Royal Charter, and soon joined the University of London, with a focus on science and technology. The college has expanded its coursework to medicine through mergers with St Mary's Hospital. In 2004, Queen Elizabeth II opened the Imperial College Business School. Imperial became an independent university from the University of London during its one hundred year anniversary.
William Henry Perkin studied and worked at the college under von Hofmann, but resigned his position after discovering the first synthetic dye, mauveine, in 1856. Perkin's discovery was prompted by his work with von Hofmann on the substance aniline, derived from coal tar, and it was this breakthrough which sparked the synthetic dye industry, a boom which some historians have labelled the second chemical revolution. His contribution led to the creation of the Perkin Medal, an award given annually by the Society of Chemical Industry to a scientist residing in the United States for an "innovation in applied chemistry resulting in outstanding commercial development". It is considered the highest honour given in the industrial chemical industry.
Imperial acquired Silwood Park in 1947, to provide a site for research and teaching in those aspects of biology not well suited for the main London campus. Felix, Imperial's student newspaper, was launched on 9 December 1949. On 29 January 1950, the government announced that it was intended that Imperial should expand to meet the scientific and technological challenges of the 20th century and a major expansion of the College followed over the next decade. In 1959 the Wolfson Foundation donated £350,000 for the establishment of a new Biochemistry Department.[citation needed] A special relationship between Imperial and the Indian Institute of Technology Delhi was established in 1963.[citation needed]
Time has long been a major subject of study in religion, philosophy, and science, but defining it in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Some simple definitions of time include "time is what clocks measure", which is a problematically vague and self-referential definition that utilizes the device used to measure the subject as the definition of the subject, and "time is what keeps everything from happening at once", which is without substantive meaning in the absence of the definition of simultaneity in the context of the limitations of human sensation, observation of events, and the perception of such events.
Two contrasting viewpoints on time divide many prominent philosophers. One view is that time is part of the fundamental structure of the universe—a dimension independent of events, in which events occur in sequence. Sir Isaac Newton subscribed to this realist view, and hence it is sometimes referred to as Newtonian time. The opposing view is that time does not refer to any kind of "container" that events and objects "move through", nor to any entity that "flows", but that it is instead part of a fundamental intellectual structure (together with space and number) within which humans sequence and compare events. This second view, in the tradition of Gottfried Leibniz and Immanuel Kant, holds that time is neither an event nor a thing, and thus is not itself measurable nor can it be travelled.
Time is one of the seven fundamental physical quantities in both the International System of Units and International System of Quantities. Time is used to define other quantities—such as velocity—so defining time in terms of such quantities would result in circularity of definition. An operational definition of time, wherein one says that observing a certain number of repetitions of one or another standard cyclical event (such as the passage of a free-swinging pendulum) constitutes one standard unit such as the second, is highly useful in the conduct of both advanced experiments and everyday affairs of life. The operational definition leaves aside the question whether there is something called time, apart from the counting activity just mentioned, that flows and that can be measured. Investigations of a single continuum called spacetime bring questions about space into questions about time, questions that have their roots in the works of early students of natural philosophy.
Temporal measurement has occupied scientists and technologists, and was a prime motivation in navigation and astronomy. Periodic events and periodic motion have long served as standards for units of time. Examples include the apparent motion of the sun across the sky, the phases of the moon, the swing of a pendulum, and the beat of a heart. Currently, the international unit of time, the second, is defined by measuring the electronic transition frequency of caesium atoms (see below). Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day and in human life spans.
Temporal measurement, chronometry, takes two distinct period forms: the calendar, a mathematical tool for organizing intervals of time, and the clock, a physical mechanism that counts the passage of time. In day-to-day life, the clock is consulted for periods less than a day whereas the calendar is consulted for periods longer than a day. Increasingly, personal electronic devices display both calendars and clocks simultaneously. The number (as on a clock dial or calendar) that marks the occurrence of a specified event as to hour or date is obtained by counting from a fiducial epoch—a central reference point.
Artifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years. Other early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization. These calendars were religiously and astronomically based, with 18 months in a year and 20 days in a month.
The most precise timekeeping device of the ancient world was the water clock, or clepsydra, one of which was found in the tomb of Egyptian pharaoh Amenhotep I (1525–1504 BC). They could be used to measure the hours even at night, but required manual upkeep to replenish the flow of water. The Ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers in particular made improvements on the use of water clocks up to the Middle Ages. In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.
The hourglass uses the flow of sand to measure the flow of time. They were used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522). Incense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Waterclocks, and later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. Richard of Wallingford (1292–1336), abbot of St. Alban's abbey, famously built a mechanical clock as an astronomical orrery about 1330. Great advances in accurate time-keeping were made by Galileo Galilei and especially Christiaan Huygens with the invention of pendulum driven clocks along with the invention of the minute hand by Jost Burgi.
The most accurate timekeeping devices are atomic clocks, which are accurate to seconds in many millions of years, and are used to calibrate other clocks and timekeeping instruments. Atomic clocks use the frequency of electronic transitions in certain atoms to measure the second. One of the most common atoms used is caesium, most modern atomic clocks probe caesium with microwaves to determine the frequency of these electron vibrations. Since 1967, the International System of Measurements bases its unit of time, the second, on the properties of caesium atoms. SI defines the second as 9,192,631,770 cycles of the radiation that corresponds to the transition between two electron spin energy levels of the ground state of the 133Cs atom.
Greenwich Mean Time (GMT) is an older standard, adopted starting with British railways in 1847. Using telescopes instead of atomic clocks, GMT was calibrated to the mean solar time at the Royal Observatory, Greenwich in the UK. Universal Time (UT) is the modern term for the international telescope-based system, adopted to replace "Greenwich Mean Time" in 1928 by the International Astronomical Union. Observations at the Greenwich Observatory itself ceased in 1954, though the location is still used as the basis for the coordinate system. Because the rotational period of Earth is not perfectly constant, the duration of a second would vary if calibrated to a telescope-based standard like GMT or UT—in which a second was defined as a fraction of a day or year. The terms "GMT" and "Greenwich Mean Time" are sometimes used informally to refer to UT or UTC.
Two distinct viewpoints on time divide many prominent philosophers. One view is that time is part of the fundamental structure of the universe, a dimension in which events occur in sequence. Sir Isaac Newton subscribed to this realist view, and hence it is sometimes referred to as Newtonian time. An opposing view is that time does not refer to any kind of actually existing dimension that events and objects "move through", nor to any entity that "flows", but that it is instead an intellectual concept (together with space and number) that enables humans to sequence and compare events. This second view, in the tradition of Gottfried Leibniz and Immanuel Kant, holds that space and time "do not exist in and of themselves, but ... are the product of the way we represent things", because we can know objects only as they appear to us.
The Vedas, the earliest texts on Indian philosophy and Hindu philosophy dating back to the late 2nd millennium BC, describe ancient Hindu cosmology, in which the universe goes through repeated cycles of creation, destruction and rebirth, with each cycle lasting 4,320 million years. Ancient Greek philosophers, including Parmenides and Heraclitus, wrote essays on the nature of time. Plato, in the Timaeus, identified time with the period of motion of the heavenly bodies. Aristotle, in Book IV of his Physica defined time as 'number of movement in respect of the before and after'.
In Book 11 of his Confessions, St. Augustine of Hippo ruminates on the nature of time, asking, "What then is time? If no one asks me, I know: if I wish to explain it to one that asketh, I know not." He begins to define time by what it is not rather than what it is, an approach similar to that taken in other negative definitions. However, Augustine ends up calling time a “distention” of the mind (Confessions 11.26) by which we simultaneously grasp the past in memory, the present by attention, and the future by expectation.
Immanuel Kant, in the Critique of Pure Reason, described time as an a priori intuition that allows us (together with the other a priori intuition, space) to comprehend sense experience. With Kant, neither space nor time are conceived as substances, but rather both are elements of a systematic mental framework that necessarily structures the experiences of any rational agent, or observing subject. Kant thought of time as a fundamental part of an abstract conceptual framework, together with space and number, within which we sequence events, quantify their duration, and compare the motions of objects. In this view, time does not refer to any kind of entity that "flows," that objects "move through," or that is a "container" for events. Spatial measurements are used to quantify the extent of and distances between objects, and temporal measurements are used to quantify the durations of and between events. Time was designated by Kant as the purest possible schema of a pure concept or category.
According to Martin Heidegger we do not exist inside time, we are time. Hence, the relationship to the past is a present awareness of having been, which allows the past to exist in the present. The relationship to the future is the state of anticipating a potential possibility, task, or engagement. It is related to the human propensity for caring and being concerned, which causes "being ahead of oneself" when thinking of a pending occurrence. Therefore, this concern for a potential occurrence also allows the future to exist in the present. The present becomes an experience, which is qualitative instead of quantitative. Heidegger seems to think this is the way that a linear relationship with time, or temporal existence, is broken or transcended. We are not stuck in sequential time. We are able to remember the past and project into the future—we have a kind of random access to our representation of temporal existence; we can, in our thoughts, step out of (ecstasis) sequential time.
The theory of special relativity finds a convenient formulation in Minkowski spacetime, a mathematical structure that combines three dimensions of space with a single dimension of time. In this formalism, distances in space can be measured by how long light takes to travel that distance, e.g., a light-year is a measure of distance, and a meter is now defined in terms of how far light travels in a certain amount of time. Two events in Minkowski spacetime are separated by an invariant interval, which can be either space-like, light-like, or time-like. Events that have a time-like separation cannot be simultaneous in any frame of reference, there must be a temporal component (and possibly a spatial one) to their separation. Events that have a space-like separation will be simultaneous in some frame of reference, and there is no frame of reference in which they do not have a spatial separation. Different observers may calculate different distances and different time intervals between two events, but the invariant interval between the events is independent of the observer (and his velocity).
In non-relativistic classical mechanics, Newton's concept of "relative, apparent, and common time" can be used in the formulation of a prescription for the synchronization of clocks. Events seen by two different observers in motion relative to each other produce a mathematical concept of time that works sufficiently well for describing the everyday phenomena of most people's experience. In the late nineteenth century, physicists encountered problems with the classical understanding of time, in connection with the behavior of electricity and magnetism. Einstein resolved these problems by invoking a method of synchronizing clocks using the constant, finite speed of light as the maximum signal velocity. This led directly to the result that observers in motion relative to one another measure different elapsed times for the same event.
Time has historically been closely related with space, the two together merging into spacetime in Einstein's special relativity and general relativity. According to these theories, the concept of time depends on the spatial reference frame of the observer, and the human perception as well as the measurement by instruments such as clocks are different for observers in relative motion. For example, if a spaceship carrying a clock flies through space at (very nearly) the speed of light, its crew does not notice a change in the speed of time on board their vessel because everything traveling at the same speed slows down at the same rate (including the clock, the crew's thought processes, and the functions of their bodies). However, to a stationary observer watching the spaceship fly by, the spaceship appears flattened in the direction it is traveling and the clock on board the spaceship appears to move very slowly.
On the other hand, the crew on board the spaceship also perceives the observer as slowed down and flattened along the spaceship's direction of travel, because both are moving at very nearly the speed of light relative to each other. Because the outside universe appears flattened to the spaceship, the crew perceives themselves as quickly traveling between regions of space that (to the stationary observer) are many light years apart. This is reconciled by the fact that the crew's perception of time is different from the stationary observer's; what seems like seconds to the crew might be hundreds of years to the stationary observer. In either case, however, causality remains unchanged: the past is the set of events that can send light signals to an entity and the future is the set of events to which an entity can send light signals.
Einstein showed in his thought experiments that people travelling at different speeds, while agreeing on cause and effect, measure different time separations between events, and can even observe different chronological orderings between non-causally related events. Though these effects are typically minute in the human experience, the effect becomes much more pronounced for objects moving at speeds approaching the speed of light. Many subatomic particles exist for only a fixed fraction of a second in a lab relatively at rest, but some that travel close to the speed of light can be measured to travel farther and survive much longer than expected (a muon is one example). According to the special theory of relativity, in the high-speed particle's frame of reference, it exists, on the average, for a standard amount of time known as its mean lifetime, and the distance it travels in that time is zero, because its velocity is zero. Relative to a frame of reference at rest, time seems to "slow down" for the particle. Relative to the high-speed particle, distances seem to shorten. Einstein showed how both temporal and spatial dimensions can be altered (or "warped") by high-speed motion.
Time appears to have a direction—the past lies behind, fixed and immutable, while the future lies ahead and is not necessarily fixed. Yet for the most part the laws of physics do not specify an arrow of time, and allow any process to proceed both forward and in reverse. This is generally a consequence of time being modeled by a parameter in the system being analyzed, where there is no "proper time": the direction of the arrow of time is sometimes arbitrary. Examples of this include the Second law of thermodynamics, which states that entropy must increase over time (see Entropy); the cosmological arrow of time, which points away from the Big Bang, CPT symmetry, and the radiative arrow of time, caused by light only traveling forwards in time (see light cone). In particle physics, the violation of CP symmetry implies that there should be a small counterbalancing time asymmetry to preserve CPT symmetry as stated above. The standard description of measurement in quantum mechanics is also time asymmetric (see Measurement in quantum mechanics).
Stephen Hawking in particular has addressed a connection between time and the Big Bang. In A Brief History of Time and elsewhere, Hawking says that even if time did not begin with the Big Bang and there were another time frame before the Big Bang, no information from events then would be accessible to us, and nothing that happened then would have any effect upon the present time-frame. Upon occasion, Hawking has stated that time actually began with the Big Bang, and that questions about what happened before the Big Bang are meaningless. This less-nuanced, but commonly repeated formulation has received criticisms from philosophers such as Aristotelian philosopher Mortimer J. Adler.
While the Big Bang model is well established in cosmology, it is likely to be refined in the future. Little is known about the earliest moments of the universe's history. The Penrose–Hawking singularity theorems require the existence of a singularity at the beginning of cosmic time. However, these theorems assume that general relativity is correct, but general relativity must break down before the universe reaches the Planck temperature, and a correct treatment of quantum gravity may avoid the singularity.
Time travel is the concept of moving backwards or forwards to different points in time, in a manner analogous to moving through space, and different from the normal "flow" of time to an earthbound observer. In this view, all points in time (including future times) "persist" in some way. Time travel has been a plot device in fiction since the 19th century. Traveling backwards in time has never been verified, presents many theoretic problems, and may be an impossibility. Any technological device, whether fictional or hypothetical, that is used to achieve time travel is known as a time machine.
Another solution to the problem of causality-based temporal paradoxes is that such paradoxes cannot arise simply because they have not arisen. As illustrated in numerous works of fiction, free will either ceases to exist in the past or the outcomes of such decisions are predetermined. As such, it would not be possible to enact the grandfather paradox because it is a historical fact that your grandfather was not killed before his child (your parent) was conceived. This view doesn't simply hold that history is an unchangeable constant, but that any change made by a hypothetical future time traveler would already have happened in his or her past, resulting in the reality that the traveler moves from. More elaboration on this view can be found in the Novikov self-consistency principle.
Psychoactive drugs can impair the judgment of time. Stimulants can lead both humans and rats to overestimate time intervals, while depressants can have the opposite effect. The level of activity in the brain of neurotransmitters such as dopamine and norepinephrine may be the reason for this. Such chemicals will either excite or inhibit the firing of neurons in the brain, with a greater firing rate allowing the brain to register the occurrence of more events within a given interval (speed up time) and a decreased firing rate reducing the brain's capacity to distinguish events occurring within a given interval (slow down time).
The use of time is an important issue in understanding human behavior, education, and travel behavior. Time-use research is a developing field of study. The question concerns how time is allocated across a number of activities (such as time spent at home, at work, shopping, etc.). Time use changes with technology, as the television or the Internet created new opportunities to use time in different ways. However, some aspects of time use are relatively stable over long periods of time, such as the amount of time spent traveling to work, which despite major changes in transport, has been observed to be about 20–30 minutes one-way for a large number of cities over a long period.
A sequence of events, or series of events, is a sequence of items, facts, events, actions, changes, or procedural steps, arranged in time order (chronological order), often with causality relationships among the items. Because of causality, cause precedes effect, or cause and effect may appear together in a single item, but effect never precedes cause. A sequence of events can be presented in text, tables, charts, or timelines. The description of the items or events may include a timestamp. A sequence of events that includes the time along with place or location information to describe a sequential path may be referred to as a world line.
Uses of a sequence of events include stories, historical events (chronology), directions and steps in procedures, and timetables for scheduling activities. A sequence of events may also be used to help describe processes in science, technology, and medicine. A sequence of events may be focused on past events (e.g., stories, history, chronology), on future events that must be in a predetermined order (e.g., plans, schedules, procedures, timetables), or focused on the observation of past events with the expectation that the events will occur in the future (e.g., processes). The use of a sequence of events occurs in fields as diverse as machines (cam timer), documentaries (Seconds From Disaster), law (choice of law), computer simulation (discrete event simulation), and electric power transmission (sequence of events recorder). A specific example of a sequence of events is the timeline of the Fukushima Daiichi nuclear disaster.
The ultimate substantive legacy of Principia Mathematica is mixed. It is generally accepted that Kurt Gödel's incompleteness theorem of 1931 definitively demonstrated that for any set of axioms and inference rules proposed to encapsulate mathematics, there would in fact be some truths of mathematics which could not be deduced from them, and hence that Principia Mathematica could never achieve its aims. However, Gödel could not have come to this conclusion without Whitehead and Russell's book. In this way, Principia Mathematica's legacy might be described as its key role in disproving the possibility of achieving its own stated goals. But beyond this somewhat ironic legacy, the book popularized modern mathematical logic and drew important connections between logic, epistemology, and metaphysics.
Whitehead's most complete work on education is the 1929 book The Aims of Education and Other Essays, which collected numerous essays and addresses by Whitehead on the subject published between 1912 and 1927. The essay from which Aims of Education derived its name was delivered as an address in 1916 when Whitehead was president of the London Branch of the Mathematical Association. In it, he cautioned against the teaching of what he called "inert ideas" – ideas that are disconnected scraps of information, with no application to real life or culture. He opined that "education with inert ideas is not only useless: it is, above all things, harmful."
Rather than teach small parts of a large number of subjects, Whitehead advocated teaching a relatively few important concepts that the student could organically link to many different areas of knowledge, discovering their application in actual life. For Whitehead, education should be the exact opposite of the multidisciplinary, value-free school model – it should be transdisciplinary, and laden with values and general principles that provide students with a bedrock of wisdom and help them to make connections between areas of knowledge that are usually regarded as separate.
Whitehead did not begin his career as a philosopher. In fact, he never had any formal training in philosophy beyond his undergraduate education. Early in his life he showed great interest in and respect for philosophy and metaphysics, but it is evident that he considered himself a rank amateur. In one letter to his friend and former student Bertrand Russell, after discussing whether science aimed to be explanatory or merely descriptive, he wrote: "This further question lands us in the ocean of metaphysic, onto which my profound ignorance of that science forbids me to enter." Ironically, in later life Whitehead would become one of the 20th century's foremost metaphysicians.
Whitehead was unimpressed by this objection. In the notes of one his students for a 1927 class, Whitehead was quoted as saying: "Every scientific man in order to preserve his reputation has to say he dislikes metaphysics. What he means is he dislikes having his metaphysics criticized." In Whitehead's view, scientists and philosophers make metaphysical assumptions about how the universe works all the time, but such assumptions are not easily seen precisely because they remain unexamined and unquestioned. While Whitehead acknowledged that "philosophers can never hope finally to formulate these metaphysical first principles," he argued that people need to continually re-imagine their basic assumptions about how the universe works if philosophy and science are to make any real progress, even if that progress remains permanently asymptotic. For this reason Whitehead regarded metaphysical investigations as essential to both good science and good philosophy.
Perhaps foremost among what Whitehead considered faulty metaphysical assumptions was the Cartesian idea that reality is fundamentally constructed of bits of matter that exist totally independently of one another, which he rejected in favor of an event-based or "process" ontology in which events are primary and are fundamentally interrelated and dependent on one another. He also argued that the most basic elements of reality can all be regarded as experiential, indeed that everything is constituted by its experience. He used the term "experience" very broadly, so that even inanimate processes such as electron collisions are said to manifest some degree of experience. In this, he went against Descartes' separation of two different kinds of real existence, either exclusively material or else exclusively mental. Whitehead referred to his metaphysical system as "philosophy of organism", but it would become known more widely as "process philosophy."
In Whitehead's view, then, concepts such as "quality", "matter", and "form" are problematic. These "classical" concepts fail to adequately account for change, and overlook the active and experiential nature of the most basic elements of the world. They are useful abstractions, but are not the world's basic building blocks. What is ordinarily conceived of as a single person, for instance, is philosophically described as a continuum of overlapping events. After all, people change all the time, if only because they have aged by another second and had some further experience. These occasions of experience are logically distinct, but are progressively connected in what Whitehead calls a "society" of events. By assuming that enduring objects are the most real and fundamental things in the universe, materialists have mistaken the abstract for the concrete (what Whitehead calls the "fallacy of misplaced concreteness").
To put it another way, a thing or person is often seen as having a "defining essence" or a "core identity" that is unchanging, and describes what the thing or person really is. In this way of thinking, things and people are seen as fundamentally the same through time, with any changes being qualitative and secondary to their core identity (e.g. "Mark's hair has turned gray as he has gotten older, but he is still the same person"). But in Whitehead's cosmology, the only fundamentally existent things are discrete "occasions of experience" that overlap one another in time and space, and jointly make up the enduring person or thing. On the other hand, what ordinary thinking often regards as "the essence of a thing" or "the identity/core of a person" is an abstract generalization of what is regarded as that person or thing's most important or salient features across time. Identities do not define people, people define identities. Everything changes from moment to moment, and to think of anything as having an "enduring essence" misses the fact that "all things flow", though it is often a useful way of speaking.
Whitehead pointed to the limitations of language as one of the main culprits in maintaining a materialistic way of thinking, and acknowledged that it may be difficult to ever wholly move past such ideas in everyday speech. After all, each moment of each person's life can hardly be given a different proper name, and it is easy and convenient to think of people and objects as remaining fundamentally the same things, rather than constantly keeping in mind that each thing is a different thing from what it was a moment ago. Yet the limitations of everyday living and everyday speech should not prevent people from realizing that "material substances" or "essences" are a convenient generalized description of a continuum of particular, concrete processes. No one questions that a ten-year-old person is quite different by the time he or she turns thirty years old, and in many ways is not the same person at all; Whitehead points out that it is not philosophically or ontologically sound to think that a person is the same from one second to the next.
A second problem with materialism is that it obscures the importance of relations. It sees every object as distinct and discrete from all other objects. Each object is simply an inert clump of matter that is only externally related to other things. The idea of matter as primary makes people think of objects as being fundamentally separate in time and space, and not necessarily related to anything. But in Whitehead's view, relations take a primary role, perhaps even more important than the relata themselves. A student taking notes in one of Whitehead's fall 1924 classes wrote that:
In fact, Whitehead describes any entity as in some sense nothing more and nothing less than the sum of its relations to other entities – its synthesis of and reaction to the world around it. A real thing is just that which forces the rest of the universe to in some way conform to it; that is to say, if theoretically a thing made strictly no difference to any other entity (i.e. it was not related to any other entity), it could not be said to really exist. Relations are not secondary to what a thing is, they are what the thing is.
Isabelle Stengers wrote that "Whiteheadians are recruited among both philosophers and theologians, and the palette has been enriched by practitioners from the most diverse horizons, from ecology to feminism, practices that unite political struggle and spirituality with the sciences of education." Indeed, in recent decades attention to Whitehead's work has become more widespread, with interest extending to intellectuals in Europe and China, and coming from such diverse fields as ecology, physics, biology, education, economics, and psychology. One of the first theologians to attempt to interact with Whitehead's thought was the future Archbishop of Canterbury, William Temple. In Temple's Gifford Lectures of 1932-1934 (subsequently published as "Nature, Man and God"), Whitehead is one of a number of philosophers of the emergent evolution approach Temple interacts with. However, it was not until the 1970s and 1980s that Whitehead's thought drew much attention outside of a small group of philosophers and theologians, primarily Americans, and even today he is not considered especially influential outside of relatively specialized circles.
Early followers of Whitehead were found primarily at the University of Chicago's Divinity School, where Henry Nelson Wieman initiated an interest in Whitehead's work that would last for about thirty years. Professors such as Wieman, Charles Hartshorne, Bernard Loomer, Bernard Meland, and Daniel Day Williams made Whitehead's philosophy arguably the most important intellectual thread running through the Divinity School. They taught generations of Whitehead scholars, the most notable of which is John B. Cobb, Jr.
But while Claremont remains the most concentrated hub of Whiteheadian activity, the place where Whitehead's thought currently seems to be growing the most quickly is in China. In order to address the challenges of modernization and industrialization, China has begun to blend traditions of Taoism, Buddhism, and Confucianism with Whitehead's "constructive post-modern" philosophy in order to create an "ecological civilization." To date, the Chinese government has encouraged the building of twenty-three university-based centers for the study of Whitehead's philosophy, and books by process philosophers John Cobb and David Ray Griffin are becoming required reading for Chinese graduate students. Cobb has attributed China's interest in process philosophy partly to Whitehead's stress on the mutual interdependence of humanity and nature, as well as his emphasis on an educational system that includes the teaching of values rather than simply bare facts.
Deleuze's and Latour's opinions, however, are minority ones, as Whitehead has not been recognized as particularly influential within the most dominant philosophical schools. It is impossible to say exactly why Whitehead's influence has not been more widespread, but it may be partly due to his metaphysical ideas seeming somewhat counter-intuitive (such as his assertion that matter is an abstraction), or his inclusion of theistic elements in his philosophy, or the perception of metaphysics itself as passé, or simply the sheer difficulty and density of his prose.
One philosophical school which has historically had a close relationship with process philosophy is American pragmatism. Whitehead himself thought highly of William James and John Dewey, and acknowledged his indebtedness to them in the preface to Process and Reality. Charles Hartshorne (along with Paul Weiss) edited the collected papers of Charles Sanders Peirce, one of the founders of pragmatism. Noted neopragmatist Richard Rorty was in turn a student of Hartshorne. Today, Nicholas Rescher is one example of a philosopher who advocates both process philosophy and pragmatism.
In physics, Whitehead's thought has had some influence. He articulated a view that might perhaps be regarded as dual to Einstein's general relativity, see Whitehead's theory of gravitation. It has been severely criticized. Yutaka Tanaka, who suggests that the gravitational constant disagrees with experimental findings, proposes that Einstein's work does not actually refute Whitehead's formulation. Whitehead's view has now been rendered obsolete, with the discovery of gravitational waves. They are phenonena observed locally that largely violate the kind of local flatness of space that Whitehead assumes. Consequently, Whitehead's cosmology must be regarded as a local approximation, and his assumption of a uniform spatio-temporal geometry, Minkowskian in particular, as an often-locally-adequate approximation. An exact replacement of Whitehead's cosmology would need to admit a Riemannian geometry. Also, although Whitehead himself gave only secondary consideration to quantum theory, his metaphysics of processes has proved attractive to some physicists in that field. Henry Stapp and David Bohm are among those whose work has been influenced by Whitehead.
This work has been pioneered by John B. Cobb, Jr., whose book Is It Too Late? A Theology of Ecology (1971) was the first single-authored book in environmental ethics. Cobb also co-authored a book with economist Herman Daly entitled For the Common Good: Redirecting the Economy toward Community, the Environment, and a Sustainable Future (1989), which applied Whitehead's thought to economics, and received the Grawemeyer Award for Ideas Improving World Order. Cobb followed this with a second book, Sustaining the Common Good: A Christian Perspective on the Global Economy (1994), which aimed to challenge "economists' zealous faith in the great god of growth."
Another model is the FEELS model developed by Xie Bangxiu and deployed successfully in China. "FEELS" stands for five things in curriculum and education: Flexible-goals, Engaged-learner, Embodied-knowledge, Learning-through-interactions, and Supportive-teacher. It is used for understanding and evaluating educational curriculum under the assumption that the purpose of education is to "help a person become whole." This work is in part the product of cooperation between Chinese government organizations and the Institute for the Postmodern Development of China.
Whitehead has had some influence on philosophy of business administration and organizational theory. This has led in part to a focus on identifying and investigating the effect of temporal events (as opposed to static things) within organizations through an “organization studies” discourse that accommodates a variety of 'weak' and 'strong' process perspectives from a number of philosophers. One of the leading figures having an explicitly Whiteheadian and panexperientialist stance towards management is Mark Dibben, who works in what he calls "applied process thought" to articulate a philosophy of management and business administration as part of a wider examination of the social sciences through the lens of process metaphysics. For Dibben, this allows "a comprehensive exploration of life as perpetually active experiencing, as opposed to occasional – and thoroughly passive – happening." Dibben has published two books on applied process thought, Applied Process Thought I: Initial Explorations in Theory and Research (2008), and Applied Process Thought II: Following a Trail Ablaze (2009), as well as other papers in this vein in the fields of philosophy of management and business ethics.
Beginning in the late 1910s and early 1920s, Whitehead gradually turned his attention from mathematics to philosophy of science, and finally to metaphysics. He developed a comprehensive metaphysical system which radically departed from most of western philosophy. Whitehead argued that reality consists of processes rather than material objects, and that processes are best defined by their relations with other processes, thus rejecting the theory that reality is fundamentally constructed by bits of matter that exist independently of one another. Today Whitehead's philosophical works – particularly Process and Reality – are regarded as the foundational texts of process philosophy.
Alfred North Whitehead was born in Ramsgate, Kent, England, in 1861. His father, Alfred Whitehead, was a minister and schoolmaster of Chatham House Academy, a successful school for boys established by Thomas Whitehead, Alfred North's grandfather. Whitehead himself recalled both of them as being very successful schoolmasters, but that his grandfather was the more extraordinary man. Whitehead's mother was Maria Sarah Whitehead, formerly Maria Sarah Buckmaster. Whitehead was apparently not particularly close with his mother, as he never mentioned her in any of his writings, and there is evidence that Whitehead's wife, Evelyn, had a low opinion of her.
In 1918 Whitehead's academic responsibilities began to seriously expand as he accepted a number of high administrative positions within the University of London system, of which Imperial College London was a member at the time. He was elected Dean of the Faculty of Science at the University of London in late 1918 (a post he held for four years), a member of the University of London's Senate in 1919, and chairman of the Senate's Academic (leadership) Council in 1920, a post which he held until he departed for America in 1924. Whitehead was able to exert his newfound influence to successfully lobby for a new history of science department, help establish a Bachelor of Science degree (previously only Bachelor of Arts degrees had been offered), and make the school more accessible to less wealthy students.
The two volume biography of Whitehead by Victor Lowe is the most definitive presentation of the life of Whitehead. However, many details of Whitehead's life remain obscure because he left no Nachlass; his family carried out his instructions that all of his papers be destroyed after his death. Additionally, Whitehead was known for his "almost fanatical belief in the right to privacy", and for writing very few personal letters of the kind that would help to gain insight on his life. This led to Lowe himself remarking on the first page of Whitehead's biography, "No professional biographer in his right mind would touch him."
In addition to numerous articles on mathematics, Whitehead wrote three major books on the subject: A Treatise on Universal Algebra (1898), Principia Mathematica (co-written with Bertrand Russell and published in three volumes between 1910 and 1913), and An Introduction to Mathematics (1911). The former two books were aimed exclusively at professional mathematicians, while the latter book was intended for a larger audience, covering the history of mathematics and its philosophical foundations. Principia Mathematica in particular is regarded as one of the most important works in mathematical logic of the 20th century.
At the time structures such as Lie algebras and hyperbolic quaternions drew attention to the need to expand algebraic structures beyond the associatively multiplicative class. In a review Alexander Macfarlane wrote: "The main idea of the work is not unification of the several methods, nor generalization of ordinary algebra so as to include them, but rather the comparative study of their several structures." In a separate review, G. B. Mathews wrote, "It possesses a unity of design which is really remarkable, considering the variety of its themes."
Whitehead and Russell had thought originally that Principia Mathematica would take a year to complete; it ended up taking them ten years. To add insult to injury, when it came time for publication, the three-volume work was so massive (more than 2,000 pages) and its audience so narrow (professional mathematicians) that it was initially published at a loss of 600 pounds, 300 of which was paid by Cambridge University Press, 200 by the Royal Society of London, and 50 apiece by Whitehead and Russell themselves. Despite the initial loss, today there is likely no major academic library in the world which does not hold a copy of Principia Mathematica.
This is not to say that Whitehead's thought was widely accepted or even well-understood. His philosophical work is generally considered to be among the most difficult to understand in all of the western canon. Even professional philosophers struggled to follow Whitehead's writings. One famous story illustrating the level of difficulty of Whitehead's philosophy centers around the delivery of Whitehead's Gifford lectures in 1927–28 – following Arthur Eddington's lectures of the year previous – which Whitehead would later publish as Process and Reality:
However, Mathews' frustration with Whitehead's books did not negatively affect his interest. In fact, there were numerous philosophers and theologians at Chicago's Divinity School that perceived the importance of what Whitehead was doing without fully grasping all of the details and implications. In 1927 they invited one of America's only Whitehead experts – Henry Nelson Wieman – to Chicago to give a lecture explaining Whitehead's thought. Wieman's lecture was so brilliant that he was promptly hired to the faculty and taught there for twenty years, and for at least thirty years afterward Chicago's Divinity School was closely associated with Whitehead's thought.
Wieman's words proved prophetic. Though Process and Reality has been called "arguably the most impressive single metaphysical text of the twentieth century," it has been little-read and little-understood, partly because it demands – as Isabelle Stengers puts it – "that its readers accept the adventure of the questions that will separate them from every consensus." Whitehead questioned western philosophy's most dearly held assumptions about how the universe works, but in doing so he managed to anticipate a number of 21st century scientific and philosophical problems and provide novel solutions.
It must be emphasized, however, that an entity is not merely a sum of its relations, but also a valuation of them and reaction to them. For Whitehead, creativity is the absolute principle of existence, and every entity (whether it is a human being, a tree, or an electron) has some degree of novelty in how it responds to other entities, and is not fully determined by causal or mechanistic laws. Of course, most entities do not have consciousness. As a human being's actions cannot always be predicted, the same can be said of where a tree's roots will grow, or how an electron will move, or whether it will rain tomorrow. Moreover, inability to predict an electron's movement (for instance) is not due to faulty understanding or inadequate technology; rather, the fundamental creativity/freedom of all entities means that there will always remain phenomena that are unpredictable.
Since Whitehead's metaphysics described a universe in which all entities experience, he needed a new way of describing perception that was not limited to living, self-conscious beings. The term he coined was "prehension", which comes from the Latin prehensio, meaning "to seize." The term is meant to indicate a kind of perception that can be conscious or unconscious, applying to people as well as electrons. It is also intended to make clear Whitehead's rejection of the theory of representative perception, in which the mind only has private ideas about other entities. For Whitehead, the term "prehension" indicates that the perceiver actually incorporates aspects of the perceived thing into itself. In this way, entities are constituted by their perceptions and relations, rather than being independent of them. Further, Whitehead regards perception as occurring in two modes, causal efficacy (or "physical prehension") and presentational immediacy (or "conceptual prehension").
Whitehead describes causal efficacy as "the experience dominating the primitive living organisms, which have a sense for the fate from which they have emerged, and the fate towards which they go." It is, in other words, the sense of causal relations between entities, a feeling of being influenced and affected by the surrounding environment, unmediated by the senses. Presentational immediacy, on the other hand, is what is usually referred to as "pure sense perception", unmediated by any causal or symbolic interpretation, even unconscious interpretation. In other words, it is pure appearance, which may or may not be delusive (e.g. mistaking an image in a mirror for "the real thing").
In higher organisms (like people), these two modes of perception combine into what Whitehead terms "symbolic reference", which links appearance with causation in a process that is so automatic that both people and animals have difficulty refraining from it. By way of illustration, Whitehead uses the example of a person's encounter with a chair. An ordinary person looks up, sees a colored shape, and immediately infers that it is a chair. However, an artist, Whitehead supposes, "might not have jumped to the notion of a chair", but instead "might have stopped at the mere contemplation of a beautiful color and a beautiful shape." This is not the normal human reaction; most people place objects in categories by habit and instinct, without even thinking about it. Moreover, animals do the same thing. Using the same example, Whitehead points out that a dog "would have acted immediately on the hypothesis of a chair and would have jumped onto it by way of using it as such." In this way symbolic reference is a fusion of pure sense perceptions on the one hand and causal relations on the other, and that it is in fact the causal relationships that dominate the more basic mentality (as the dog illustrates), while it is the sense perceptions which indicate a higher grade mentality (as the artist illustrates).
Whitehead makes the startling observation that "life is comparatively deficient in survival value." If humans can only exist for about a hundred years, and rocks for eight hundred million, then one is forced to ask why complex organisms ever evolved in the first place; as Whitehead humorously notes, "they certainly did not appear because they were better at that game than the rocks around them." He then observes that the mark of higher forms of life is that they are actively engaged in modifying their environment, an activity which he theorizes is directed toward the three-fold goal of living, living well, and living better. In other words, Whitehead sees life as directed toward the purpose of increasing its own satisfaction. Without such a goal, he sees the rise of life as totally unintelligible.
Whitehead's idea of God differs from traditional monotheistic notions. Perhaps his most famous and pointed criticism of the Christian conception of God is that "the Church gave unto God the attributes which belonged exclusively to Caesar." Here Whitehead is criticizing Christianity for defining God as primarily a divine king who imposes his will on the world, and whose most important attribute is power. As opposed to the most widely accepted forms of Christianity, Whitehead emphasized an idea of God that he called "the brief Galilean vision of humility":
It should be emphasized, however, that for Whitehead God is not necessarily tied to religion. Rather than springing primarily from religious faith, Whitehead saw God as necessary for his metaphysical system. His system required that an order exist among possibilities, an order that allowed for novelty in the world and provided an aim to all entities. Whitehead posited that these ordered potentials exist in what he called the primordial nature of God. However, Whitehead was also interested in religious experience. This led him to reflect more intensively on what he saw as the second nature of God, the consequent nature. Whitehead's conception of God as a "dipolar" entity has called for fresh theological thinking.
God's consequent nature, on the other hand, is anything but unchanging – it is God's reception of the world's activity. As Whitehead puts it, "[God] saves the world as it passes into the immediacy of his own life. It is the judgment of a tenderness which loses nothing that can be saved." In other words, God saves and cherishes all experiences forever, and those experiences go on to change the way God interacts with the world. In this way, God is really changed by what happens in the world and the wider universe, lending the actions of finite creatures an eternal significance.
Whitehead thus sees God and the world as fulfilling one another. He sees entities in the world as fluent and changing things that yearn for a permanence which only God can provide by taking them into God's self, thereafter changing God and affecting the rest of the universe throughout time. On the other hand, he sees God as permanent but as deficient in actuality and change: alone, God is merely eternally unrealized possibilities, and requires the world to actualize them. God gives creatures permanence, while the creatures give God actuality and change. Here it is worthwhile to quote Whitehead at length:
For Whitehead the core of religion was individual. While he acknowledged that individuals cannot ever be fully separated from their society, he argued that life is an internal fact for its own sake before it is an external fact relating to others. His most famous remark on religion is that "religion is what the individual does with his own solitariness ... and if you are never solitary, you are never religious." Whitehead saw religion as a system of general truths that transformed a person's character. He took special care to note that while religion is often a good influence, it is not necessarily good – an idea which he called a "dangerous delusion" (e.g., a religion might encourage the violent extermination of a rival religion's adherents).
However, while Whitehead saw religion as beginning in solitariness, he also saw religion as necessarily expanding beyond the individual. In keeping with his process metaphysics in which relations are primary, he wrote that religion necessitates the realization of "the value of the objective world which is a community derivative from the interrelations of its component individuals." In other words, the universe is a community which makes itself whole through the relatedness of each individual entity to all the others – meaning and value do not exist for the individual alone, but only in the context of the universal community. Whitehead writes further that each entity "can find no such value till it has merged its individual claim with that of the objective universe. Religion is world-loyalty. The spirit at once surrenders itself to this universal claim and appropriates it for itself." In this way the individual and universal/social aspects of religion are mutually dependent.
Overall, however, Whitehead's influence is very difficult to characterize. In English-speaking countries, his primary works are little-studied outside of Claremont and a select number of liberal graduate-level theology and philosophy programs. Outside of these circles his influence is relatively small and diffuse, and has tended to come chiefly through the work of his students and admirers rather than Whitehead himself. For instance, Whitehead was a teacher and long-time friend and collaborator of Bertrand Russell, and he also taught and supervised the dissertation of Willard Van Orman Quine, both of whom are important figures in analytic philosophy – the dominant strain of philosophy in English-speaking countries in the 20th century. Whitehead has also had high-profile admirers in the continental tradition, such as French post-structuralist philosopher Gilles Deleuze, who once dryly remarked of Whitehead that "he stands provisionally as the last great Anglo-American philosopher before Wittgenstein's disciples spread their misty confusion, sufficiency, and terror." French sociologist and anthropologist Bruno Latour even went so far as to call Whitehead "the greatest philosopher of the 20th century."
Historically Whitehead's work has been most influential in the field of American progressive theology. The most important early proponent of Whitehead's thought in a theological context was Charles Hartshorne, who spent a semester at Harvard as Whitehead's teaching assistant in 1925, and is widely credited with developing Whitehead's process philosophy into a full-blown process theology. Other notable process theologians include John B. Cobb, Jr., David Ray Griffin, Marjorie Hewitt Suchocki, C. Robert Mesle, Roland Faber, and Catherine Keller.
Process theology typically stresses God's relational nature. Rather than seeing God as impassive or emotionless, process theologians view God as "the fellow sufferer who understands", and as the being who is supremely affected by temporal events. Hartshorne points out that people would not praise a human ruler who was unaffected by either the joys or sorrows of his followers – so why would this be a praise-worthy quality in God? Instead, as the being who is most affected by the world, God is the being who can most appropriately respond to the world. However, process theology has been formulated in a wide variety of ways. C. Robert Mesle, for instance, advocates a "process naturalism", i.e. a process theology without God.
In fact, process theology is difficult to define because process theologians are so diverse and transdisciplinary in their views and interests. John B. Cobb, Jr. is a process theologian who has also written books on biology and economics. Roland Faber and Catherine Keller integrate Whitehead with poststructuralist, postcolonialist, and feminist theory. Charles Birch was both a theologian and a geneticist. Franklin I. Gamwell writes on theology and political theory. In Syntheism - Creating God in The Internet Age, futurologists Alexander Bard and Jan Söderqvist repeatedly credit Whitehead for the process theology they see rising out of the participatory culture expected to dominate the digital era.
Whitehead also described religion more technically as "an ultimate craving to infuse into the insistent particularity of emotion that non-temporal generality which primarily belongs to conceptual thought alone." In other words, religion takes deeply felt emotions and contextualizes them within a system of general truths about the world, helping people to identify their wider meaning and significance. For Whitehead, religion served as a kind of bridge between philosophy and the emotions and purposes of a particular society. It is the task of religion to make philosophy applicable to the everyday lives of ordinary people.
Margaret Stout and Carrie M. Staton have also written recently on the mutual influence of Whitehead and Mary Parker Follett, a pioneer in the fields of organizational theory and organizational behavior. Stout and Staton see both Whitehead and Follett as sharing an ontology that "understands becoming as a relational process; difference as being related, yet unique; and the purpose of becoming as harmonizing difference." This connection is further analyzed by Stout and Jeannine M. Love in Integrative Process: Follettian Thinking from Ontology to Administration 
Somalis (Somali: Soomaali, Arabic: صومال‎) are an ethnic group inhabiting the Horn of Africa (Somali Peninsula). The overwhelming majority of Somalis speak the Somali language, which is part of the Cushitic branch of the Afro-Asiatic family. They are predominantly Sunni Muslim. Ethnic Somalis number around 16-20 million and are principally concentrated in Somalia (around 12.3 million), Ethiopia (4.6 million), Kenya (2.4 million), and Djibouti (464,600), with many also residing in parts of the Middle East, North America and Europe.
Irir Samaale, the oldest common ancestor of several Somali clans, is generally regarded as the source of the ethnonym Somali. The name "Somali" is, in turn, held to be derived from the words soo and maal, which together mean "go and milk" — a reference to the ubiquitous pastoralism of the Somali people. Another plausible etymology proposes that the term Somali is derived from the Arabic for "wealthy" (dhawamaal), again referring to Somali riches in livestock.
An Ancient Chinese document from the 9th century referred to the northern Somali coast — which was then called "Berbera" by Arab geographers in reference to the region's "Berber" (Cushitic) inhabitants — as Po-pa-li. The first clear written reference of the sobriquet Somali, however, dates back to the 15th century. During the wars between the Sultanate of Ifat based at Zeila and the Solomonic Dynasty, the Abyssinian Emperor had one of his court officials compose a hymn celebrating a military victory over the Sultan of Ifat's eponymous troops.
Ancient rock paintings in Somalia which date back to 5000 years have been found in the northern part of the country, depicting early life in the territory. The most famous of these is the Laas Geel complex, which contains some of the earliest known rock art on the African continent and features many elaborate pastoralist sketches of animal and human figures. In other places, such as the northern Dhambalin region, a depiction of a man on a horse is postulated as being one of the earliest known examples of a mounted huntsman.
Inscriptions have been found beneath many of the rock paintings, but archaeologists have so far been unable to decipher this form of ancient writing. During the Stone age, the Doian culture and the Hargeisan culture flourished here with their respective industries and factories.
The oldest evidence of burial customs in the Horn of Africa comes from cemeteries in Somalia dating back to 4th millennium BC. The stone implements from the Jalelo site in northern Somalia are said to be the most important link in evidence of the universality in palaeolithic times between the East and the West.
In antiquity, the ancestors of the Somali people were an important link in the Horn of Africa connecting the region's commerce with the rest of the ancient world. Somali sailors and merchants were the main suppliers of frankincense, myrrh and spices, items which were considered valuable luxuries by the Ancient Egyptians, Phoenicians, Mycenaeans and Babylonians.
According to most scholars, the ancient Land of Punt and its inhabitants formed part of the ethnogenesis of the Somali people. The ancient Puntites were a nation of people that had close relations with Pharaonic Egypt during the times of Pharaoh Sahure and Queen Hatshepsut. The pyramidal structures, temples and ancient houses of dressed stone littered around Somalia are said to date from this period.
In the classical era, several ancient city-states such as Opone, Essina, Sarapion, Nikon, Malao, Damo and Mosylon near Cape Guardafui, which competed with the Sabaeans, Parthians and Axumites for the wealthy Indo-Greco-Roman trade, also flourished in Somalia.
The birth of Islam on the opposite side of Somalia's Red Sea coast meant that Somali merchants, sailors and expatriates living in the Arabian Peninsula gradually came under the influence of the new religion through their converted Arab Muslim trading partners. With the migration of fleeing Muslim families from the Islamic world to Somalia in the early centuries of Islam and the peaceful conversion of the Somali population by Somali Muslim scholars in the following centuries, the ancient city-states eventually transformed into Islamic Mogadishu, Berbera, Zeila, Barawa and Merca, which were part of the Berberi civilization. The city of Mogadishu came to be known as the City of Islam, and controlled the East African gold trade for several centuries.
The Sultanate of Ifat, led by the Walashma dynasty with its capital at Zeila, ruled over parts of what is now eastern Ethiopia, Djibouti, and northern Somalia. The historian al-Umari records that Ifat was situated near the Red Sea coast, and states its size as 15 days travel by 20 days travel. Its army numbered 15,000 horsemen and 20,000 foot soldiers. Al-Umari also credits Ifat with seven "mother cities": Belqulzar, Kuljura, Shimi, Shewa, Adal, Jamme and Laboo.
In the Middle Ages, several powerful Somali empires dominated the regional trade including the Ajuran Sultanate, which excelled in hydraulic engineering and fortress building, the Sultanate of Adal, whose general Ahmad ibn Ibrahim al-Ghazi (Ahmed Gurey) was the first commander to use cannon warfare on the continent during Adal's conquest of the Ethiopian Empire, and the Sultanate of the Geledi, whose military dominance forced governors of the Omani empire north of the city of Lamu to pay tribute to the Somali Sultan Ahmed Yusuf.
In the late 19th century, after the Berlin conference had ended, European empires sailed with their armies to the Horn of Africa. The imperial clouds wavering over Somalia alarmed the Dervish leader Mohammed Abdullah Hassan, who gathered Somali soldiers from across the Horn of Africa and began one of the longest anti-colonial wars ever. The Dervish State successfully repulsed the British empire four times and forced it to retreat to the coastal region. As a result of its successes against the British, the Dervish State received support from the Ottoman and German empires. The Turks also named Hassan Emir of the Somali nation, and the Germans promised to officially recognize any territories the Dervishes were to acquire. After a quarter of a century of holding the British at bay, the Dervishes were finally defeated in 1920, when Britain for the first time in Africa used airplanes to bomb the Dervish capital of Taleex. As a result of this bombardment, former Dervish territories were turned into a protectorate of Britain. Italy similarly faced the same opposition from Somali Sultans and armies and did not acquire full control of parts of modern Somalia until the Fascist era in late 1927. This occupation lasted till 1941 and was replaced by a British military administration.
Following World War II, Britain retained control of both British Somaliland and Italian Somaliland as protectorates. In 1945, during the Potsdam Conference, the United Nations granted Italy trusteeship of Italian Somaliland, but only under close supervision and on the condition — first proposed by the Somali Youth League (SYL) and other nascent Somali political organizations, such as Hizbia Digil Mirifle Somali (HDMS) and the Somali National League (SNL) — that Somalia achieve independence within ten years. British Somaliland remained a protectorate of Britain until 1960.
To the extent that Italy held the territory by UN mandate, the trusteeship provisions gave the Somalis the opportunity to gain experience in political education and self-government. These were advantages that British Somaliland, which was to be incorporated into the new Somali state, did not have. Although in the 1950s British colonial officials attempted, through various administrative development efforts, to make up for past neglect, the protectorate stagnated. The disparity between the two territories in economic development and political experience would cause serious difficulties when it came time to integrate the two parts. Meanwhile, in 1948, under pressure from their World War II allies and to the dismay of the Somalis, the British "returned" the Haud (an important Somali grazing area that was presumably 'protected' by British treaties with the Somalis in 1884 and 1886) and the Ogaden to Ethiopia, based on a treaty they signed in 1897 in which the British ceded Somali territory to the Ethiopian Emperor Menelik in exchange for his help against plundering by Somali clans. Britain included the proviso that the Somali nomads would retain their autonomy, but Ethiopia immediately claimed sovereignty over them. This prompted an unsuccessful bid by Britain in 1956 to buy back the Somali lands it had turned over. Britain also granted administration of the almost exclusively Somali-inhabited Northern Frontier District (NFD) to Kenyan nationalists despite an informal plebiscite demonstrating the overwhelming desire of the region's population to join the newly formed Somali Republic.
A referendum was held in neighboring Djibouti (then known as French Somaliland) in 1958, on the eve of Somalia's independence in 1960, to decide whether or not to join the Somali Republic or to remain with France. The referendum turned out in favour of a continued association with France, largely due to a combined yes vote by the sizable Afar ethnic group and resident Europeans. There was also widespread vote rigging, with the French expelling thousands of Somalis before the referendum reached the polls. The majority of those who voted no were Somalis who were strongly in favour of joining a united Somalia, as had been proposed by Mahmoud Harbi, Vice President of the Government Council. Harbi was killed in a plane crash two years later. Djibouti finally gained its independence from France in 1977, and Hassan Gouled Aptidon, a Somali who had campaigned for a yes vote in the referendum of 1958, eventually wound up as Djibouti's first president (1977–1991).
British Somaliland became independent on 26 June 1960 as the State of Somaliland, and the Trust Territory of Somalia (the former Italian Somaliland) followed suit five days later. On 1 July 1960, the two territories united to form the Somali Republic, albeit within boundaries drawn up by Italy and Britain. A government was formed by Abdullahi Issa Mohamud and Muhammad Haji Ibrahim Egal other members of the trusteeship and protectorate governments, with Haji Bashir Ismail Yusuf as President of the Somali National Assembly, Aden Abdullah Osman Daar as the President of the Somali Republic and Abdirashid Ali Shermarke as Prime Minister (later to become President from 1967 to 1969). On 20 July 1961 and through a popular referendum, the people of Somalia ratified a new constitution, which was first drafted in 1960. In 1967, Muhammad Haji Ibrahim Egal became Prime Minister, a position to which he was appointed by Shermarke. Egal would later become the President of the autonomous Somaliland region in northwestern Somalia.
On 15 October 1969, while paying a visit to the northern town of Las Anod, Somalia's then President Abdirashid Ali Shermarke was shot dead by one of his own bodyguards. His assassination was quickly followed by a military coup d'état on 21 October 1969 (the day after his funeral), in which the Somali Army seized power without encountering armed opposition — essentially a bloodless takeover. The putsch was spearheaded by Major General Mohamed Siad Barre, who at the time commanded the army.
Alongside Barre, the Supreme Revolutionary Council (SRC) that assumed power after President Sharmarke's assassination was led by Lieutenant Colonel Salaad Gabeyre Kediye and Chief of Police Jama Korshel. The SRC subsequently renamed the country the Somali Democratic Republic, dissolved the parliament and the Supreme Court, and suspended the constitution.
The revolutionary army established large-scale public works programs and successfully implemented an urban and rural literacy campaign, which helped dramatically increase the literacy rate. In addition to a nationalization program of industry and land, the new regime's foreign policy placed an emphasis on Somalia's traditional and religious links with the Arab world, eventually joining the Arab League (AL) in 1974. That same year, Barre also served as chairman of the Organization of African Unity (OAU), the predecessor of the African Union (AU).
Somali people in the Horn of Africa are divided among different countries (Somalia, Djibouti, Ethiopia, and northeastern Kenya) that were artificially and some might say arbitrarily partitioned by the former imperial powers. Pan-Somalism is an ideology that advocates the unification of all ethnic Somalis once part of Somali empires such as the Ajuran Empire, the Adal Sultanate, the Gobroon Dynasty and the Dervish State under one flag and one nation. The Siad Barre regime actively promoted Pan-Somalism, which eventually led to the Ogaden War between Somalia on one side, and Ethiopia, Cuba and the Soviet Union on the other.
According to Y chromosome studies by Sanchez et al. (2005), Cruciani et al. (2004, 2007), the Somalis are paternally closely related to other Afro-Asiatic-speaking groups in Northeast Africa. Besides comprising the majority of the Y-DNA in Somalis, the E1b1b1a (formerly E3b1a) haplogroup also makes up a significant proportion of the paternal DNA of Ethiopians, Sudanese, Egyptians, Berbers, North African Arabs, as well as many Mediterranean populations. Sanchez et al. (2005) observed the M78 subclade of E1b1b in about 77% of their Somali male samples. According to Cruciani et al. (2007), the presence of this subhaplogroup in the Horn region may represent the traces of an ancient migration from Egypt/Libya. After haplogroup E1b1b, the second most frequently occurring Y-DNA haplogroup among Somalis is the West Asian haplogroup T (M70). It is observed in slightly more than 10% of Somali males. Haplogroup T, like haplogroup E1b1b, is also typically found among populations of Northeast Africa, North Africa, the Near East and the Mediterranean.
According to mtDNA studies by Holden (2005) and Richards et al. (2006), a significant proportion of the maternal lineages of Somalis consists of the M1 haplogroup. This mitochondrial clade is common among Ethiopians and North Africans, particularly Egyptians and Algerians. M1 is believed to have originated in Asia, where its parent M clade represents the majority of mtDNA lineages. This haplogroup is also thought to possibly correlate with the Afro-Asiatic language family:
According to an autosomal DNA study by Hodgson et al. (2014), the Afro-Asiatic languages were likely spread across Africa and the Near East by an ancestral population(s) carrying a newly identified non-African genetic component, which the researchers dub the "Ethio-Somali". This Ethio-Somali component is today most common among Afro-Asiatic-speaking populations in the Horn of Africa. It reaches a frequency peak among ethnic Somalis, representing the majority of their ancestry. The Ethio-Somali component is most closely related to the Maghrebi non-African genetic component, and is believed to have diverged from all other non-African ancestries at least 23,000 years ago. On this basis, the researchers suggest that the original Ethio-Somali carrying population(s) probably arrived in the pre-agricultural period from the Near East, having crossed over into northeastern Africa via the Sinai Peninsula. The population then likely split into two branches, with one group heading westward toward the Maghreb and the other moving south into the Horn.
The analysis of HLA antigens has also helped clarify the possible background of the Somali people, as the distribution of haplotype frequencies vary among population groups. According to Mohamoud et al. (2006):
The history of Islam in Somalia is as old as the religion itself. The early persecuted Muslims fled to various places in the region, including the city of Zeila in modern-day northern Somalia, so as to seek protection from the Quraysh. Somalis were among the first populations on the continent to embrace Islam. With very few exceptions, Somalis are entirely Muslims, the majority belonging to the Sunni branch of Islam and the Shafi`i school of Islamic jurisprudence, although a few are also adherents of the Shia Muslim denomination.
Qur'anic schools (also known as dugsi) remain the basic system of traditional religious instruction in Somalia. They provide Islamic education for children, thereby filling a clear religious and social role in the country. Known as the most stable local, non-formal system of education providing basic religious and moral instruction, their strength rests on community support and their use of locally made and widely available teaching materials. The Qur'anic system, which teaches the greatest number of students relative to other educational sub-sectors, is oftentimes the only system accessible to Somalis in nomadic as compared to urban areas. A study from 1993 found, among other things, that "unlike in primary schools where gender disparity is enormous, around 40 per cent of Qur'anic school pupils are girls; but the teaching staff have minimum or no qualification necessary to ensure intellectual development of children." To address these concerns, the Somali government on its own part subsequently established the Ministry of Endowment and Islamic Affairs, under which Qur'anic education is now regulated.
In the Somali diaspora, multiple Islamic fundraising events are held every year in cities like Birmingham, London, Toronto and Minneapolis, where Somali scholars and professionals give lectures and answer questions from the audience. The purpose of these events is usually to raise money for new schools or universities in Somalia, to help Somalis that have suffered as a consequence of floods and/or droughts, or to gather funds for the creation of new mosques like the Abuubakar-As-Saddique Mosque, which is currently undergoing construction in the Twin cities.
In addition, the Somali community has produced numerous important Muslim figures over the centuries, many of whom have significantly shaped the course of Islamic learning and practice in the Horn of Africa, the Arabian Peninsula and well beyond.
The clan groupings of the Somali people are important social units, and clan membership plays a central part in Somali culture and politics. Clans are patrilineal and are often divided into sub-clans, sometimes with many sub-divisions. The tombs of the founders of the Darod, Dir and Isaaq major clans as well as the Abgaal sub-clan of the Hawiye are all located in northern Somalia. Tradition holds this general area as an ancestral homeland of the Somali people.
Somali society is traditionally ethnically endogamous. So to extend ties of alliance, marriage is often to another ethnic Somali from a different clan. Thus, for example, a recent study observed that in 89 marriages contracted by men of the Dhulbahante clan, 55 (62%) were with women of Dhulbahante sub-clans other than those of their husbands; 30 (33.7%) were with women of surrounding clans of other clan families (Isaaq, 28; Hawiye, 3); and 3 (4.3%) were with women of other clans of the Darod clan family (Majerteen 2, Ogaden 1).
In 1975, the most prominent government reforms regarding family law in a Muslim country were set in motion in the Somali Democratic Republic, which put women and men, including husbands and wives, on complete equal footing. The 1975 Somali Family Law gave men and women equal division of property between the husband and wife upon divorce and the exclusive right to control by each spouse over his or her personal property.
Somalis constitute the largest ethnic group in Somalia, at approximately 85% of the nation's inhabitants. They are traditionally nomads, but since the late 20th century, many have moved to urban areas. While most Somalis can be found in Somalia proper, large numbers also live in Ethiopia, Djibouti, Kenya, Yemen, the Middle East, South Asia and Europe due to their seafaring tradition.
Civil strife in the early 1990s greatly increased the size of the Somali diaspora, as many of the best educated Somalis left for the Middle East, Europe and North America. In Canada, the cities of Toronto, Ottawa, Calgary, Edmonton, Montreal, Vancouver, Winnipeg and Hamilton all harbor Somali populations. Statistics Canada's 2006 census ranks people of Somali descent as the 69th largest ethnic group in Canada.
While the distribution of Somalis per country in Europe is hard to measure because the Somali community on the continent has grown so quickly in recent years, an official 2010 estimate reported 108,000 Somalis living in the United Kingdom. Somalis in Britain are largely concentrated in the cities of London, Sheffield, Bristol, Birmingham, Cardiff, Liverpool, Manchester, Leeds, and Leicester, with London alone accounting for roughly 78% of Britain's Somali population. There are also significant Somali communities in Sweden: 57,906 (2014); the Netherlands: 37,432 (2014); Norway: 38,413 (2015); Denmark: 18,645 (2014); and Finland: 16,721 (2014).
In the United States, Minneapolis, Saint Paul, Columbus, San Diego, Seattle, Washington, D.C., Houston, Atlanta, Los Angeles, Portland, Denver, Nashville, Green Bay, Lewiston, Portland, Maine and Cedar Rapids have the largest Somali populations.
An estimated 20,000 Somalis emigrated to the U.S. state of Minnesota some ten years ago and the Twin Cities (Minneapolis and Saint Paul) now have the highest population of Somalis in North America. The city of Minneapolis hosts hundreds of Somali-owned and operated businesses offering a variety of products, including leather shoes, jewelry and other fashion items, halal meat, and hawala or money transfer services. Community-based video rental stores likewise carry the latest Somali films and music. The number of Somalis has especially surged in the Cedar-Riverside area of Minneapolis.
There is a sizable Somali community in the United Arab Emirates. Somali-owned businesses line the streets of Deira, the Dubai city centre, with only Iranians exporting more products from the city at large. Internet cafés, hotels, coffee shops, restaurants and import-export businesses are all testimony to the Somalis' entrepreneurial spirit. Star African Air is also one of three Somali-owned airlines which are based in Dubai.
Besides their traditional areas of inhabitation in Greater Somalia, a Somali community mainly consisting of entrepreneurs, academics, and students also exists in Egypt. In addition, there is an historical Somali community in the general Sudan area. Primarily concentrated in the north and Khartoum, the expatriate community mainly consists of students as well as some businesspeople. More recently, Somali entrepreneurs have established themselves in Kenya, investing over $1.5 billion in the Somali enclave of Eastleigh alone. In South Africa, Somali businesspeople also provide most of the retail trade in informal settlements around the Western Cape province.
The Somali language is a member of the Cushitic branch of the Afro-Asiatic family. Its nearest relatives are the Afar and Saho languages. Somali is the best documented of the Cushitic languages, with academic studies of it dating from before 1900.
The exact number of speakers of Somali is unknown. One source estimates that there are 7.78 million speakers of Somali in Somalia itself and 12.65 million speakers globally. The Somali language is spoken by ethnic Somalis in Greater Somalia and the Somali diaspora.
Somali dialects are divided into three main groups: Northern, Benaadir and Maay. Northern Somali (or Northern-Central Somali) forms the basis for Standard Somali. Benaadir (also known as Coastal Somali) is spoken on the Benadir coast from Adale to south of Merca, including Mogadishu, as well as in the immediate hinterland. The coastal dialects have additional phonemes which do not exist in Standard Somali. Maay is principally spoken by the Digil and Mirifle (Rahanweyn) clans in the southern areas of Somalia.
A number of writing systems have been used over the years for transcribing the language. Of these, the Somali alphabet is the most widely used, and has been the official writing script in Somalia since the government of former President of Somalia Mohamed Siad Barre formally introduced it in October 1972. The script was developed by the Somali linguist Shire Jama Ahmed specifically for the Somali language, and uses all letters of the English Latin alphabet except p, v and z. Besides Ahmed's Latin script, other orthographies that have been used for centuries for writing Somali include the long-established Arabic script and Wadaad's writing. Indigenous writing systems developed in the twentieth century include the Osmanya, Borama and Kaddare scripts, which were invented by Osman Yusuf Kenadid, Abdurahman Sheikh Nuur and Hussein Sheikh Ahmed Kaddare, respectively.
In addition to Somali, Arabic, which is also an Afro-Asiatic tongue, is an official national language in both Somalia and Djibouti. Many Somalis speak it due to centuries-old ties with the Arab world, the far-reaching influence of the Arabic media, and religious education. Somalia and Djibouti are also both members of the Arab League.
The culture of Somalia is an amalgamation of traditions developed independently and through interaction with neighbouring and far away civilizations, such as other parts of Northeast Africa, the Arabian Peninsula, India and Southeast Asia.
The textile-making communities in Somalia are a continuation of an ancient textile industry, as is the culture of wood carving, pottery and monumental architecture that dominates Somali interiors and landscapes. The cultural diffusion of Somali commercial enterprise can be detected in its cuisine, which contains Southeast Asian influences. Due to the Somali people's passionate love for and facility with poetry, Somalia has often been referred to by scholars as a "Nation of Poets" and a "Nation of Bards" including, among others, the Canadian novelist Margaret Laurence.
All of these traditions, including festivals, martial arts, dress, literature, sport and games such as Shax, have immensely contributed to the enrichment of Somali heritage.
Somalis have a rich musical heritage centered on traditional Somali folklore. Most Somali songs are pentatonic. That is, they only use five pitches per octave in contrast to a heptatonic (seven note) scale, such as the major scale. At first listen, Somali music might be mistaken for the sounds of nearby regions such as Ethiopia, Sudan or Arabia, but it is ultimately recognizable by its own unique tunes and styles. Somali songs are usually the product of collaboration between lyricists (midho), songwriters (lahan) and singers ('odka or "voice").
Growing out of the Somali people's rich storytelling tradition, the first few feature-length Somali films and cinematic festivals emerged in the early 1960s, immediately after independence. Following the creation of the Somali Film Agency (SFA) regulatory body in 1975, the local film scene began to expand rapidly. The Somali filmmaker Ali Said Hassan concurrently served as the SFA's representative in Rome. In the 1970s and early 1980s, popular musicals known as riwaayado were the main driving force behind the Somali movie industry. Epic and period films as well as international co-productions followed suit, facilitated by the proliferation of video technology and national television networks. Said Salah Ahmed during this period directed his first feature film, The Somali Darwish (The Somalia Dervishes), devoted to the Dervish State. In the 1990s and 2000s, a new wave of more entertainment-oriented movies emerged. Referred to as Somaliwood, this upstart, youth-based cinematic movement has energized the Somali film industry and in the process introduced innovative storylines, marketing strategies and production techniques. The young directors Abdisalam Aato of Olol Films and Abdi Malik Isak are at the forefront of this quiet revolution.
Somali art is the artistic culture of the Somali people, both historic and contemporary. These include artistic traditions in pottery, music, architecture, wood carving and other genres. Somali art is characterized by its aniconism, partly as a result of the vestigial influence of the pre-Islamic mythology of the Somalis coupled with their ubiquitous Muslim beliefs. However, there have been cases in the past of artistic depictions representing living creatures, such as certain ancient rock paintings in northern Somalia, the golden birds on the Mogadishan canopies, and the plant decorations on religious tombs in southern Somalia. More typically, intricate patterns and geometric designs, bold colors and monumental architecture were the norm.
Additionally, henna is an important part of Somali culture. It is worn by Somali women on their hands, arms, feet and neck during weddings, Eid, Ramadan, and other festive occasions. Somali henna designs are similar to those in the Arabian peninsula, often featuring flower motifs and triangular shapes. The palm is also frequently decorated with a dot of henna and the fingertips are dipped in the dye. Henna parties are usually held before the wedding takes place. Somali women have likewise traditionally applied kohl (kuul) to their eyes. Usage of the eye cosmetic in the Horn region is believed to date to the ancient Land of Punt.
Football is the most popular sport amongst Somalis. Important competitions are the Somalia League and Somalia Cup. The multi-ethnic Ocean Stars, Somalia's national team, first participated at the Olympic Games in 1972 and has sent athletes to compete in most Summer Olympic Games since then. The equally diverse Somali beach soccer team also represents the country in international beach soccer competitions. In addition, several international footballers such as Mohammed Ahamed Jama, Liban Abdi, Ayub Daud and Abdisalam Ibrahim have played in European top divisions.
The FIBA Africa Championship 1981 was hosted by Somalia from 15–23 December 1981. The games were played in Mogadishu, and the Somali national team received the bronze prize. Abdi Bile won the 1500 m event at the World Championships in 1987, running the fastest final 800 m of any 1,500 meter race in history. He was a two-time Olympian (1984 and 1996) and dominated the event in the late 1980s. Hussein Ahmed Salah, a Somalia-born former long-distance runner from Djibouti, won a bronze medal in the marathon at the 1988 Summer Olympics. He also won silver medals in this event at the 1987 and 1991 World Championships, as well as the 1985 IAAF World Marathon Cup. Mo Farah is a double Olympic gold medal winner and world champion, and holds the European track record for 10,000 metres, the British road record for 10,000 metres, the British indoor record in the 3000 metres, the British track record for 5000 metres and the European indoor record for 5000 metres. Mohammed Ahmed (athlete) is a Canadian long-distance runner who represented Canada in the 10,000 meter races at the 2012 Summer Olympics and the 2013 World Championships in Athletics.
In the martial arts, Faisal Jeylani Aweys and Mohamed Deq Abdulle also took home a silver medal and fourth place, respectively, at the 2013 Open World Taekwondo Challenge Cup in Tongeren. The Somali National Olympic committee has devised a special support program to ensure continued success in future tournaments. Additionally, Mohamed Jama has won both world and European titles in K1 and Thai Boxing.
When not dressed in Westernized clothing such as jeans and t-shirts, Somali men typically wear the macawis, which is a sarong-like garment worn around the waist. On their heads, they often wrap a colorful turban or wear the koofiyad, an embroidered fez.
Due to Somalia's proximity to and close ties with the Arabian Peninsula, many Somali men also wear the jellabiya (jellabiyad or qamiis in Somali), a long white garment common in the Arab world.
During regular, day-to-day activities, Somali women usually wear the guntiino, a long stretch of cloth tied over the shoulder and draped around the waist. It is usually made out of alandi, which is a textile common in the Horn region and some parts of North Africa. The garment can be worn in different styles. It can also be made with other fabrics, including white cloth with gold borders. For more formal settings, such as at weddings or religious celebrations like Eid, women wear the dirac. It is a long, light, diaphanous voile dress made of silk, chiffon, taffeta or saree fabric. The gown is worn over a full-length half-slip and a brassiere. Known as the gorgorad, the underskirt is made out of silk and serves as a key part of the overall outfit. The dirac is usually sparkly and very colorful, the most popular styles being those with gilded borders or threads. The fabric is typically acquired from Somali clothing stores in tandem with the gorgorad. In the past, dirac fabric was also frequently purchased from South Asian merchandisers.
Married women tend to sport headscarves referred to as shaash. They also often cover their upper body with a shawl, which is known as garbasaar. Unmarried or young women, however, do not always cover their heads. Traditional Arabian garb, such as the jilbab and abaya, is also commonly worn.
Additionally, Somali women have a long tradition of wearing gold jewelry, particularly bangles. During weddings, the bride is frequently adorned in gold. Many Somali women by tradition also wear gold necklaces and anklets.
The Somali flag is an ethnic flag conceived to represent ethnic Somalis. It was created in 1954 by the Somali scholar Mohammed Awale Liban, after he had been selected by the labour trade union of the Trust Territory of Somalia to come up with a design. Upon independence in 1960, the flag was adopted as the national flag of the nascent Somali Republic. The five-pointed Star of Unity in the flag's center represents the Somali ethnic group inhabiting the five territories in Greater Somalia.
Somali cuisine varies from region to region and consists of a fusion of diverse culinary influences. It is the product of Somalia's rich tradition of trade and commerce. Despite the variety, there remains one thing that unites the various regional cuisines: all food is served halal. There are therefore no pork dishes, alcohol is not served, nothing that died on its own is eaten, and no blood is incorporated.
Qado or lunch is often elaborate. Varieties of bariis (rice), the most popular probably being basmati, usually serve as the main dish. Spices like cumin, cardamom, cloves, cinnamon, and garden sage are used to aromatize these different rice delicacies. Somalis eat dinner as late as 9 pm. During Ramadan, supper is often served after Tarawih prayers; sometimes as late as 11 pm.
Xalwo (halva) is a popular confection eaten during festive occasions, such as Eid celebrations or wedding receptions. It is made from sugar, corn starch, cardamom powder, nutmeg powder and ghee. Peanuts are also sometimes added to enhance texture and flavor. After meals, homes are traditionally perfumed using frankincense (lubaan) or incense (cuunsi), which is prepared inside an incense burner referred to as a dabqaad.
Somali scholars have for centuries produced many notable examples of Islamic literature ranging from poetry to Hadith. With the adoption of the Latin alphabet in 1972 to transcribe the Somali language, numerous contemporary Somali authors have also released novels, some of which have gone on to receive worldwide acclaim. Of these modern writers, Nuruddin Farah is probably the most celebrated. Books such as From a Crooked Rib and Links are considered important literary achievements, works which have earned Farah, among other accolades, the 1998 Neustadt International Prize for Literature. Farah Mohamed Jama Awl is another prominent Somali writer who is perhaps best known for his Dervish era novel, Ignorance is the enemy of love. Young upstart Nadifa Mohamed was also awarded the 2010 Betty Trask Prize. Additionally, Mohamed Ibrahim Warsame 'Hadrawi' is considered by many to be the greatest living Somali poet, and several of his works have been translated internationally.
Somalis for centuries have practiced a form of customary law, which they call Xeer. Xeer is a polycentric legal system where there is no monopolistic agent that determines what the law should be or how it should be interpreted.
The Xeer legal system is assumed to have developed exclusively in the Horn of Africa since approximately the 7th century. There is no evidence that it developed elsewhere or was greatly influenced by any foreign legal system. The fact that Somali legal terminology is practically devoid of loan words from foreign languages suggests that Xeer is truly indigenous.
The Xeer legal system also requires a certain amount of specialization of different functions within the legal framework. Thus, one can find odayal (judges), xeer boggeyaal (jurists), guurtiyaal (detectives), garxajiyaal (attorneys), murkhaatiyal (witnesses) and waranle (police officers) to enforce the law.
Somali architecture is a rich and diverse tradition of engineering and designing. It involves multiple different construction types, such as stone cities, castles, citadels, fortresses, mosques, mausoleums, towers, tombs, tumuli, cairns, megaliths, menhirs, stelae, dolmens, stone circles, monuments, temples, enclosures, cisterns, aqueducts, and lighthouses. Spanning the ancient, medieval and early modern periods in Greater Somalia, it also includes the fusion of Somalo-Islamic architecture with Western designs in contemporary times.
In ancient Somalia, pyramidical structures known in Somali as taalo were a popular burial style, with hundreds of these dry stone monuments scattered around the country today. Houses were built of dressed stone similar to the ones in Ancient Egypt. There are also examples of courtyards and large stone walls enclosing settlements, such as the Wargaade Wall.
The peaceful introduction of Islam in the early medieval era of Somalia's history brought Islamic architectural influences from Arabia and Persia. This had the effect of stimulating a shift in construction from drystone and other related materials to coral stone, sundried bricks, and the widespread use of limestone in Somali architecture. Many of the new architectural designs, such as mosques, were built on the ruins of older structures. This practice would continue over and over again throughout the following centuries.
The scholarly term for research concerning Somalis and Greater Somalia is known as Somali Studies. It consists of several disciplines such as anthropology, sociology, linguistics, historiography and archaeology. The field draws from old Somali chronicles, records and oral literature, in addition to written accounts and traditions about Somalis from explorers and geographers in the Horn of Africa and the Middle East. Since 1980, prominent Somalist scholars from around the world have also gathered annually to hold the International Congress of Somali Studies.
In modern molecular biology and genetics, the genome is the genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes and the non-coding sequences of the DNA/RNA.
The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name to be a blend of the words gene and chromosome. However, see omics for a more thorough discussion. A few related -ome words already existed—such as biome, rhizome, forming a vocabulary into which genome fits systematically.
Some organisms have multiple copies of chromosomes: diploid, triploid, tetraploid and so on. In classical genetics, in a sexually reproducing organism (typically eukarya) the gamete has half the number of chromosomes of the somatic cell and the genome is a full set of chromosomes in a diploid cell. The halving of the genetic material in gametes is accomplished by the segregation of homologous chromosomes during meiosis. In haploid organisms, including cells of bacteria, archaea, and in organelles including mitochondria and chloroplasts, or viruses, that similarly contain genes, the single or set of circular or linear chains of DNA (or RNA for some viruses), likewise constitute the genome. The term genome can be applied specifically to mean what is stored on a complete set of nuclear DNA (i.e., the "nuclear genome") but can also be applied to what is stored within organelles that contain their own DNA, as with the "mitochondrial genome" or the "chloroplast genome". Additionally, the genome can comprise non-chromosomal genetic elements such as viruses, plasmids, and transposable elements.
When people say that the genome of a sexually reproducing species has been "sequenced", typically they are referring to a determination of the sequences of one set of autosomes and one of each type of sex chromosome, which together represent both of the possible sexes. Even in species that exist in only one sex, what is described as a "genome sequence" may be a composite read from the chromosomes of various individuals. Colloquially, the phrase "genetic makeup" is sometimes used to signify the genome of a particular individual or organism.[citation needed] The study of the global properties of genomes of related organisms is usually referred to as genomics, which distinguishes it from genetics which generally studies the properties of single genes or groups of genes.
Both the number of base pairs and the number of genes vary widely from one species to another, and there is only a rough correlation between the two (an observation known as the C-value paradox). At present, the highest known number of genes is around 60,000, for the protozoan causing trichomoniasis (see List of sequenced eukaryotic genomes), almost three times as many as in the human genome.
In 1976, Walter Fiers at the University of Ghent (Belgium) was the first to establish the complete nucleotide sequence of a viral RNA-genome (Bacteriophage MS2). The next year Fred Sanger completed the first DNA-genome sequence: Phage Φ-X174, of 5386 base pairs. The first complete genome sequences among all three domains of life were released within a short period during the mid-1990s: The first bacterial genome to be sequenced was that of Haemophilus influenzae, completed by a team at The Institute for Genomic Research in 1995. A few months later, the first eukaryotic genome was completed, with sequences of the 16 chromosomes of budding yeast Saccharomyces cerevisiae published as the result of a European-led effort begun in the mid-1980s. The first genome sequence for an archaeon, Methanococcus jannaschii, was completed in 1996, again by The Institute for Genomic Research.
The development of new technologies has made it dramatically easier and cheaper to do sequencing, and the number of complete genome sequences is growing rapidly. The US National Institutes of Health maintains one of several comprehensive databases of genomic information. Among the thousands of completed genome sequencing projects include those for rice, a mouse, the plant Arabidopsis thaliana, the puffer fish, and the bacteria E. coli. In December 2013, scientists first sequenced the entire genome of a Neanderthal, an extinct species of humans. The genome was extracted from the toe bone of a 130,000-year-old Neanderthal found in a Siberian cave.
New sequencing technologies, such as massive parallel sequencing have also opened up the prospect of personal genome sequencing as a diagnostic tool, as pioneered by Manteia Predictive Medicine. A major step toward that goal was the completion in 2007 of the full genome of James D. Watson, one of the co-discoverers of the structure of DNA.
Whereas a genome sequence lists the order of every DNA base in a genome, a genome map identifies the landmarks. A genome map is less detailed than a genome sequence and aids in navigating around the genome. The Human Genome Project was organized to map and to sequence the human genome. A fundamental step in the project was the release of a detailed genomic map by Jean Weissenbach and his team at the Genoscope in Paris.
Genome composition is used to describe the make up of contents of a haploid genome, which should include genome size, proportions of non-repetitive DNA and repetitive DNA in details. By comparing the genome compositions between genomes, scientists can better understand the evolutionary history of a given genome.
When talking about genome composition, one should distinguish between prokaryotes and eukaryotes as the big differences on contents structure they have. In prokaryotes, most of the genome (85–90%) is non-repetitive DNA, which means coding DNA mainly forms it, while non-coding regions only take a small part. On the contrary, eukaryotes have the feature of exon-intron organization of protein coding genes; the variation of repetitive DNA content in eukaryotes is also extremely high. In mammals and plants, the major part of the genome is composed of repetitive DNA.
Most biological entities that are more complex than a virus sometimes or always carry additional genetic material besides that which resides in their chromosomes. In some contexts, such as sequencing the genome of a pathogenic microbe, "genome" is meant to include information stored on this auxiliary material, which is carried in plasmids. In such circumstances then, "genome" describes all of the genes and information on non-coding DNA that have the potential to be present.
In eukaryotes such as plants, protozoa and animals, however, "genome" carries the typical connotation of only information on chromosomal DNA. So although these organisms contain chloroplasts or mitochondria that have their own DNA, the genetic information contained by DNA within these organelles is not considered part of the genome. In fact, mitochondria are sometimes said to have their own genome often referred to as the "mitochondrial genome". The DNA found within the chloroplast may be referred to as the "plastome".
Genome size is the total number of DNA base pairs in one copy of a haploid genome. The genome size is positively correlated with the morphological complexity among prokaryotes and lower eukaryotes; however, after mollusks and all the other higher eukaryotes above, this correlation is no longer effective. This phenomenon also indicates the mighty influence coming from repetitive DNA act on the genomes.
Since genomes are very complex, one research strategy is to reduce the number of genes in a genome to the bare minimum and still have the organism in question survive. There is experimental work being done on minimal genomes for single cell organisms as well as minimal genomes for multi-cellular organisms (see Developmental biology). The work is both in vivo and in silico.
The proportion of non-repetitive DNA is calculated by using the length of non-repetitive DNA divided by genome size. Protein-coding genes and RNA-coding genes are generally non-repetitive DNA. A bigger genome does not mean more genes, and the proportion of non-repetitive DNA decreases along with increasing genome size in higher eukaryotes.
It had been found that the proportion of non-repetitive DNA can vary a lot between species. Some E. coli as prokaryotes only have non-repetitive DNA, lower eukaryotes such as C. elegans and fruit fly, still possess more non-repetitive DNA than repetitive DNA. Higher eukaryotes tend to have more repetitive DNA than non-repetitive ones. In some plants and amphibians, the proportion of non-repetitive DNA is no more than 20%, becoming a minority component.
The proportion of repetitive DNA is calculated by using length of repetitive DNA divide by genome size. There are two categories of repetitive DNA in genome: tandem repeats and interspersed repeats.
Tandem repeats are usually caused by slippage during replication, unequal crossing-over and gene conversion, satellite DNA and microsatellites are forms of tandem repeats in the genome. Although tandem repeats count for a significant proportion in genome, the largest proportion in mammalian is the other type, interspersed repeats.
Interspersed repeats mainly come from transposable elements (TEs), but they also include some protein coding gene families and pseudogenes. Transposable elements are able to integrate into the genome at another site within the cell. It is believed that TEs are an important driving force on genome evolution of higher eukaryotes. TEs can be classified into two categories, Class 1 (retrotransposons) and Class 2 (DNA transposons).
Retrotransposons can be transcribed into RNA, which are then duplicated at another site into the genome. Retrotransposons can be divided into Long terminal repeats (LTRs) and Non-Long Terminal Repeats (Non-LTR).
DNA transposons generally move by "cut and paste" in the genome, but duplication has also been observed. Class 2 TEs do not use RNA as intermediate and are popular in bacteria, in metazoan it has also been found.
Genomes are more than the sum of an organism's genes and have traits that may be measured and studied without reference to the details of any particular genes and their products. Researchers compare traits such as chromosome number (karyotype), genome size, gene order, codon usage bias, and GC-content to determine what mechanisms could have produced the great variety of genomes that exist today (for recent overviews, see Brown 2002; Saccone and Pesole 2003; Benfey and Protopapas 2004; Gibson and Muse 2004; Reese 2004; Gregory 2005).
Duplications play a major role in shaping the genome. Duplication may range from extension of short tandem repeats, to duplication of a cluster of genes, and all the way to duplication of entire chromosomes or even entire genomes. Such duplications are probably fundamental to the creation of genetic novelty.
Horizontal gene transfer is invoked to explain how there is often extreme similarity between small portions of the genomes of two organisms that are otherwise very distantly related. Horizontal gene transfer seems to be common among many microbes. Also, eukaryotic cells seem to have experienced a transfer of some genetic material from their chloroplast and mitochondrial genomes to their nuclear chromosomes.
Red is the color at the end of the spectrum of visible light next to orange and opposite violet. Red color has a predominant light wavelength of roughly 620–740 nanometres. Red is one of the additive primary colors of visible light, along with green and blue, which in Red Green Blue (RGB) color systems are combined to create all the colors on a computer monitor or television screen. Red is also one of the subtractive primary colors, along with yellow and blue, of the RYB color space and traditional color wheel used by painters and artists.
In nature, the red color of blood comes from hemoglobin, the iron-containing protein found in the red blood cells of all vertebrates. The red color of the Grand Canyon and other geological features is caused by hematite or red ochre, both forms of iron oxide. It also causes the red color of the planet Mars. The red sky at sunset and sunrise is caused by an optical effect known as Rayleigh scattering, which, when the sun is low or below the horizon, increases the red-wavelength light that reaches the eye. The color of autumn leaves is caused by pigments called anthocyanins, which are produced towards the end of summer, when the green chlorophyll is no longer produced. One to two percent of the human population has red hair; the color is produced by high levels of the reddish pigment pheomelanin (which also accounts for the red color of the lips) and relatively low levels of the dark pigment eumelanin.
A red dye called Kermes was made beginning in the Neolithic Period by drying and then crushing the bodies of the females of a tiny scale insect in the genus Kermes, primarily Kermes vermilio. The insects live on the sap of certain trees, especially Kermes oak trees near the Mediterranean region. Jars of kermes have been found in a Neolithic cave-burial at Adaoutse, Bouches-du-Rhône. Kermes from oak trees was later used by Romans, who imported it from Spain. A different variety of dye was made from Porphyrophora hamelii (Armenian cochineal) scale insects that lived on the roots and stems of certain herbs. It was mentioned in texts as early as the 8th century BC, and it was used by the ancient Assyrians and Persians.
Kermes is also mentioned in the Bible. In the Book of Exodus, God instructs Moses to have the Israelites bring him an offering including cloth "of blue, and purple, and scarlet." The term used for scarlet in the 4th century Latin Vulgate version of the Bible passage is coccumque bis tinctum, meaning "colored twice with coccus." Coccus, from the ancient Greek Kokkos, means a tiny grain and is the term that was used in ancient times for the Kermes vermilio insect used to make the Kermes dye. This was also the origin of the expression "dyed in the grain."
But, like many colors, it also had a negative association, with heat, destruction and evil. A prayer to god Isis said: "Oh Isis, protect me from all things evil and red." The ancient Egyptians began manufacturing pigments in about 4000 BC. Red ochre was widely used as a pigment for wall paintings, particularly as the skin color of men. An ivory painter's palette found inside the tomb of King Tutankhamun had small compartments with pigments of red ochre and five other colors. The Egyptians used the root of the rubia, or madder plant, to make a dye, later known as alizarin, and also used it to color white power to use as a pigment, which became known as madder lake, alizarin or alizarin crimson.
In Ancient Rome, Tyrian purple was the color of the Emperor, but red had an important religious symbolism. Romans wore togas with red stripes on holidays, and the bride at a wedding wore a red shawl, called a flammeum. Red was used to color statues and the skin of gladiators. Red was also the color associated with army; Roman soldiers wore red tunics, and officers wore a cloak called a paludamentum which, depending upon the quality of the dye, could be crimson, scarlet or purple. In Roman mythology red is associated with the god of war, Mars. The vexilloid of the Roman Empire had a red background with the letters SPQR in gold. A Roman general receiving a triumph had his entire body painted red in honor of his achievement.
The Romans liked bright colors, and many Roman villas were decorated with vivid red murals. The pigment used for many of the murals was called vermilion, and it came from the mineral cinnabar, a common ore of mercury. It was one of the finest reds of ancient times – the paintings have retained their brightness for more than twenty centuries. The source of cinnabar for the Romans was a group of mines near Almadén, southwest of Madrid, in Spain. Working in the mines was extremely dangerous, since mercury is highly toxic; the miners were slaves or prisoners, and being sent to the cinnabar mines was a virtual death sentence.
Red was the color of the banner of the Byzantine emperors. In Western Europe, Emperor Charlemagne painted his palace red as a very visible symbol of his authority, and wore red shoes at his coronation. Kings, princes and, beginning in 1295, Roman Catholic cardinals began to wear red costumes. When Abbe Suger rebuilt Saint Denis Basilica outside Paris in the early 12th century, he added stained glass windows colored blue cobalt glass and red glass tinted with copper. Together they flooded the basilica with a mystical light. Soon stained glass windows were being added to cathedrals all across France, England and Germany. In Medieval painting red was used to attract attention to the most important figures; both Christ and the Virgin Mary were commonly painted wearing red mantles.
Red clothing was a sign of status and wealth. It was worn not only by cardinals and princes, but also by merchants, artisans and townpeople, particularly on holidays or special occasions. Red dye for the clothing of ordinary people was made from the roots of the rubia tinctorum, the madder plant. This color leaned toward brick-red, and faded easily in the sun or during washing. The wealthy and aristocrats wore scarlet clothing dyed with kermes, or carmine, made from the carminic acid in tiny female scale insects, which lived on the leaves of oak trees in Eastern Europe and around the Mediterranean. The insects were gathered, dried, crushed, and boiled with different ingredients in a long and complicated process, which produced a brilliant scarlet.
Red played an important role in Chinese philosophy. It was believed that the world was composed of five elements: metal, wood, water, fire and earth, and that each had a color. Red was associated with fire. Each Emperor chose the color that his fortune-tellers believed would bring the most prosperity and good fortune to his reign. During the Zhou, Han, Jin, Song and Ming Dynasties, red considered a noble color, and it was featured in all court ceremonies, from coronations to sacrificial offerings, and weddings.
Red was also a badge of rank. During the Song dynasty (906–1279), officials of the top three ranks wore purple clothes; those of the fourth and fifth wore bright red; those of the sixth and seventh wore green; and the eighth and ninth wore blue. Red was the color worn by the royal guards of honor, and the color of the carriages of the imperial family. When the imperial family traveled, their servants and accompanying officials carried red and purple umbrellas. Of an official who had talent and ambition, it was said "he is so red he becomes purple."
Red was also featured in Chinese Imperial architecture. In the Tang and Song Dynasties, gates of palaces were usually painted red, and nobles often painted their entire mansion red. One of the most famous works of Chinese literature, A Dream of Red Mansions by Cao Xueqin (1715–1763), was about the lives of noble women who passed their lives out of public sight within the walls of such mansions. In later dynasties red was reserved for the walls of temples and imperial residences. When the Manchu rulers of the Qing Dynasty conquered the Ming and took over the Forbidden City and Imperial Palace in Beijing, all the walls, gates, beams and pillars were painted in red and gold.
There were guilds of dyers who specialized in red in Venice and other large Europeans cities. The Rubia plant was used to make the most common dye; it produced an orange-red or brick red color used to dye the clothes of merchants and artisans. For the wealthy, the dye used was Kermes, made from a tiny scale insect which fed on the branches and leaves of the oak tree. For those with even more money there was Polish Cochineal; also known as Kermes vermilio or "Blood of Saint John", which was made from a related insect, the Margodes polonicus. It made a more vivid red than ordinary Kermes. The finest and most expensive variety of red made from insects was the "Kermes" of Armenia (Armenian cochineal, also known as Persian kirmiz), made by collecting and crushing Porphyophora hamelii, an insect which lived on the roots and stems of certain grasses. The pigment and dye merchants of Venice imported and sold all of these products and also manufactured their own color, called Venetian red, which was considered the most expensive and finest red in Europe. Its secret ingredient was arsenic, which brightened the color.
But early in the 16th century, a brilliant new red appeared in Europe. When the Spanish conquistador Hernán Cortés and his soldiers conquered the Aztec Empire in 1519-1521, they discovered slowly that the Aztecs had another treasure beside silver and gold; they had the tiny cochineal, a parasitic scale insect which lived on cactus plants, which, when dried and crushed, made a magnificent red. The cochineal in Mexico was closely related to the Kermes varieties of Europe, but unlike European Kermes, it could be harvested several times a year, and it was ten times stronger than the Kermes of Poland. It worked particularly well on silk, satin and other luxury textiles. In 1523 Cortes sent the first shipment to Spain. Soon cochineal began to arrive in European ports aboard convoys of Spanish galleons.
The painters of the early Renaissance used two traditional lake pigments, made from mixing dye with either chalk or alum, kermes lake, made from kermes insects, and madder lake, made from the rubia tinctorum plant. With the arrival of cochineal, they had a third, carmine, which made a very fine crimson, though it had a tendency to change color if not used carefully. It was used by almost all the great painters of the 15th and 16th centuries, including Rembrandt, Vermeer, Rubens, Anthony van Dyck, Diego Velázquez and Tintoretto. Later it was used by Thomas Gainsborough, Seurat and J.M.W. Turner.
During the French Revolution, Red became a symbol of liberty and personal freedom used by the Jacobins and other more radical parties. Many of them wore a red Phrygian cap, or liberty cap, modeled after the caps worn by freed slaves in Ancient Rome. During the height of the Reign of Terror, Women wearing red caps gathered around the guillotine to celebrate each execution. They were called the "Furies of the guillotine". The guillotines used during the Reign of Terror in 1792 and 1793 were painted red, or made of red wood. During the Reign of Terror a statue of a woman titled liberty, painted red, was placed in the square in front of the guillotine. After the end of the Reign of Terror, France went back to the blue, white and red tricolor, whose red was taken from the traditional color of Saint Denis, the Christian martyr and patron saint of Paris.
As the Industrial Revolution spread across Europe, chemists and manufacturers sought new red dyes that could be used for large-scale manufacture of textiles. One popular color imported into Europe from Turkey and India in the 18th and early 19th century was Turkey red, known in France as rouge d'Adrinople. Beginning in the 1740s, this bright red color was used to dye or print cotton textiles in England, the Netherlands and France. Turkey red used madder as the colorant, but the process was longer and more complicated, involving multiple soaking of the fabrics in lye, olive oil, sheep's dung, and other ingredients. The fabric was more expensive but resulted in a fine bright and lasting red, similar to carmine, perfectly suited to cotton. The fabric was widely exported from Europe to Africa, the Middle East and America. In 19th century America, it was widely used in making the traditional patchwork quilt.
The 19th century also saw the use of red in art to create specific emotions, not just to imitate nature. It saw the systematic study of color theory, and particularly the study of how complementary colors such as red and green reinforced each other when they were placed next to each other. These studies were avidly followed by artists such as Vincent van Gogh. Describing his painting, The Night Cafe, to his brother Theo in 1888, Van Gogh wrote: "I sought to express with red and green the terrible human passions. The hall is blood red and pale yellow, with a green billiard table in the center, and four lamps of lemon yellow, with rays of orange and green. Everywhere it is a battle and antithesis of the most different reds and greens."
Matisse was also one of the first 20th-century artists to make color the central element of the painting, chosen to evoke emotions. "A certain blue penetrates your soul", he wrote. "A certain red affects your blood pressure." He also was familiar with the way that complementary colors, such as red and green, strengthened each other when they were placed next to each other. He wrote, "My choice of colors is not based on scientific theory; it is based on observation, upon feelings, upon the real nature of each experience ... I just try to find a color which corresponds to my feelings."
Rothko also began using the new synthetic pigments, but not always with happy results. In 1962 he donated to Harvard University a series of large murals of the Passion of Christ whose predominant colors were dark pink and deep crimson. He mixed mostly traditional colors to make the pink and crimson; synthetic ultramarine, cerulean blue, and titanium white, but he also used two new organic reds, Naphtol and Lithol. The Naphtol did well, but the Lithol slowly changed color when exposed to light. Within five years the deep pinks and reds had begun to turn light blue, and by 1979 the paintings were ruined and had to be taken down.
Unlike vermilion or red ochre, made from minerals, red lake pigments are made by mixing organic dyes, made from insects or plants, with white chalk or alum. Red lac was made from the gum lac, the dark red resinous substance secreted by various scale insects, particularly the Laccifer lacca from India. Carmine lake was made from the cochineal insect from Central and South America, Kermes lake came from a different scale insect, kermes vermilio, which thrived on oak trees around the Mediterranean. Other red lakes were made from the rose madder plant and from the brazilwood tree.
In modern color theory, also known as the RGB color model, red, green and blue are additive primary colors. Red, green and blue light combined together makes white light, and these three colors, combined in different mixtures, can produce nearly any other color. This is the principle that is used to make all of the colors on your computer screen and your television. For example, purple on a computer screen is made by a similar formula to used by Cennino Cennini in the Renaissance to make violet, but using additive colors and light instead of pigment: it is created by combining red and blue light at equal intensity on a black screen. Violet is made on a computer screen in a similar way, but with a greater amount of blue light and less red light.
So that the maximum number of colors can be accurately reproduced on your computer screen, each color has been given a code number, or sRGB, which tells your computer the intensity of the red, green and blue components of that color. The intensity of each component is measured on a scale of zero to 255, which means the complete list includes 16,777,216 distinct colors and shades. The sRGB number of pure red, for example, is 255, 00, 00, which means the red component is at its maximum intensity, and there is no green or blue. The sRGB number for crimson is 220, 20, 60, which means that the red is slightly less intense and therefore darker, there is some green, which leans it toward orange; and there is a larger amount of blue,which makes it slightly blue-violet.
As a ray of white sunlight travels through the atmosphere to the eye, some of the colors are scattered out of the beam by air molecules and airborne particles due to Rayleigh scattering, changing the final color of the beam that is seen. Colors with a shorter wavelength, such as blue and green, scatter more strongly, and are removed from the light that finally reaches the eye. At sunrise and sunset, when the path of the sunlight through the atmosphere to the eye is longest, the blue and green components are removed almost completely, leaving the longer wavelength orange and red light. The remaining reddened sunlight can also be scattered by cloud droplets and other relatively large particles, which give the sky above the horizon its red glow.
Lasers emitting in the red region of the spectrum have been available since the invention of the ruby laser in 1960. In 1962 the red helium–neon laser was invented, and these two types of lasers were widely used in many scientific applications including holography, and in education. Red helium–neon lasers were used commercially in LaserDisc players. The use of red laser diodes became widespread with the commercial success of modern DVD players, which use a 660 nm laser diode technology. Today, red and red-orange laser diodes are widely available to the public in the form of extremely inexpensive laser pointers. Portable, high-powered versions are also available for various applications. More recently, 671 nm diode-pumped solid state (DPSS) lasers have been introduced to the market for all-DPSS laser display systems, particle image velocimetry, Raman spectroscopy, and holography.
During the summer growing season, phosphate is at a high level. It has a vital role in the breakdown of the sugars manufactured by chlorophyll. But in the fall, phosphate, along with the other chemicals and nutrients, moves out of the leaf into the stem of the plant. When this happens, the sugar-breakdown process changes, leading to the production of anthocyanin pigments. The brighter the light during this period, the greater the production of anthocyanins and the more brilliant the resulting color display. When the days of autumn are bright and cool, and the nights are chilly but not freezing, the brightest colorations usually develop.
Red hair varies from a deep burgundy through burnt orange to bright copper. It is characterized by high levels of the reddish pigment pheomelanin (which also accounts for the red color of the lips) and relatively low levels of the dark pigment eumelanin. The term redhead (originally redd hede) has been in use since at least 1510. Cultural reactions have varied from ridicule to admiration; many common stereotypes exist regarding redheads and they are often portrayed as fiery-tempered. (See red hair).
Red is associated with dominance in a number of animal species. For example, in mandrills, red coloration of the face is greatest in alpha males, increasingly less prominent in lower ranking subordinates, and directly correlated with levels of testosterone. Red can also affect the perception of dominance by others, leading to significant differences in mortality, reproductive success and parental investment between individuals displaying red and those not. In humans, wearing red has been linked with increased performance in competitions, including professional sport and multiplayer video games. Controlled tests have demonstrated that wearing red does not increase performance or levels of testosterone during exercise, so the effect is likely to be produced by perceived rather than actual performance. Judges of tae kwon do have been shown to favor competitors wearing red protective gear over blue, and, when asked, a significant majority of people say that red abstract shapes are more "dominant", "aggressive", and "likely to win a physical competition" than blue shapes. In contrast to its positive effect in physical competition and dominance behavior, exposure to red decreases performance in cognitive tasks and elicits aversion in psychological tests where subjects are placed in an "achievement" context (e.g. taking an IQ test).
Surveys show that red is the color most associated with courage. In western countries red is a symbol of martyrs and sacrifice, particularly because of its association with blood. Beginning in the Middle Ages, the Pope and Cardinals of the Roman Catholic Church wore red to symbolize the blood of Christ and the Christian martyrs. The banner of the Christian soldiers in the First Crusade was a red cross on a white field, the St. George's Cross. According to Christian tradition, Saint George was a Roman soldier who was a member of the guards of the Emperor Diocletian, who refused to renounce his Christian faith and was martyred. The Saint George's Cross became the Flag of England in the 16th century, and now is part of the Union Flag of the United Kingdom, as well as the Flag of the Republic of Georgia.
Saint Valentine, a Roman Catholic Bishop or priest who was martyred in about 296 AD, seems to have had no known connection with romantic love, but the day of his martyrdom on the Roman Catholic calendar, Saint Valentine's Day (February 14), became, in the 14th century, an occasion for lovers to send messages to each other. In recent years the celebration of Saint Valentine' s day has spread beyond Christian countries to Japan and China and other parts of the world. The celebration of Saint Valentine's Day is forbidden or strongly condemned in many Islamic countries, including Saudi Arabia, Pakistan and Iran. In Saudi Arabia, in 2002 and 2011, religious police banned the sale of all Valentine's Day items, telling shop workers to remove any red items, as the day is considered a Christian holiday.
Red is the color most commonly associated with joy and well being. It is the color of celebration and ceremony. A red carpet is often used to welcome distinguished guests. Red is also the traditional color of seats in opera houses and theaters. Scarlet academic gowns are worn by new Doctors of Philosophy at degree ceremonies at Oxford University and other schools. In China, it is considered the color of good fortune and prosperity, and it is the color traditionally worn by brides. In Christian countries, it is the color traditionally worn at Christmas by Santa Claus, because in the 4th century the historic Saint Nicholas was the Greek Christian Bishop of Myra, in modern-day Turkey, and bishops then dressed in red.
Red is the traditional color of warning and danger. In the Middle Ages, a red flag announced that the defenders of a town or castle would fight to defend it, and a red flag hoisted by a warship meant they would show no mercy to their enemy. In Britain, in the early days of motoring, motor cars had to follow a man with a red flag who would warn horse-drawn vehicles, before the Locomotives on Highways Act 1896 abolished this law. In automobile races, the red flag is raised if there is danger to the drivers. In international football, a player who has made a serious violation of the rules is shown a red penalty card and ejected from the game.
Red is the international color of stop signs and stop lights on highways and intersections. It was standarized as the international color at the Vienna Convention on Road Signs and Signals of 1968. It was chosen partly because red is the brightest color in daytime (next to orange), though it is less visible at twilight, when green is the most visible color. Red also stands out more clearly against a cool natural backdrop of blue sky, green trees or gray buildings. But it was mostly chosen as the color for stoplights and stop signs because of its universal association with danger and warning.
Red is used in modern fashion much as it was used in Medieval painting; to attract the eyes of the viewer to the person who is supposed to be the center of attention. People wearing red seem to be closer than those dressed in other colors, even if they are actually the same distance away. Monarchs, wives of Presidential candidates and other celebrities often wear red to be visible from a distance in a crowd. It is also commonly worn by lifeguards and others whose job requires them to be easily found.
"So he carried me away in the spirit into the wilderness: and I saw a woman sit upon a scarlet coloured beast, full of names of blasphemy, having seven heads and ten horns. "And the woman was arrayed in purple and scarlet colour, and decked with gold and precious stones and pearls, having a golden cup in her hand full of abominations and filthiness of her fornication: "And upon her forehead was a name written a mystery: Babylon the Great, the Mother of Harlots and of all the abominations of the earth: And I saw the woman drunken with the blood of the saints, and with the blood of the martyrs of Jesus.
In China, red (simplified Chinese: 红; traditional Chinese: 紅; pinyin: hóng) is the symbol of fire and the south (both south in general and Southern China specifically). It carries a largely positive connotation, being associated with courage, loyalty, honor, success, fortune, fertility, happiness, passion, and summer. In Chinese cultural traditions, red is associated with weddings (where brides traditionally wear red dresses) and red paper is frequently used to wrap gifts of money or other objects. Special red packets (simplified Chinese: 红包; traditional Chinese: 紅包; pinyin: hóng bāo in Mandarin or lai see in Cantonese) are specifically used during Chinese New Year celebrations for giving monetary gifts. On the more negative side, obituaries are traditionally written in red ink, and to write someone's name in red signals either cutting them out of one's life, or that they have died. Red is also associated with either the feminine or the masculine (yin and yang respectively), depending on the source. The Little Red Book, a collection of quotations from Chairman Mao Tse-Tung, founding father of the People's Republic of China (PRC), was published in 1966 and widely distributed thereafter.
In Central Africa, Ndembu warriors rub themselves with red paint during celebrations. Since their culture sees the color as a symbol of life and health, sick people are also painted with it. Like most Central African cultures, the Ndembu see red as ambivalent, better than black but not as good as white. In other parts of Africa, however, red is a color of mourning, representing death. Because red bears are associated with death in many parts of Africa, the Red Cross has changed its colors to green and white in parts of the continent.
Major League Baseball is especially well known for red teams. The Cincinnati Red Stockings are the oldest professional baseball team, dating back to 1869. The franchise soon relocated to Boston and is now the Atlanta Braves, but its name survives as the origin for both the Cincinnati Reds and Boston Red Sox. During the 1950s when red was strongly associated with communism, the modern Cincinnati team was known as the "Redlegs" and the term was used on baseball cards. After the red scare faded, the team was known as the "Reds" again. The Los Angeles Angels of Anaheim are also known for their color red, as are the St. Louis Cardinals, Arizona Diamondbacks, and the Philadelphia Phillies.
In association football, teams such as Manchester United, Bayern Munich, Liverpool, Arsenal, Toronto FC, and S.L. Benfica primarily wear red jerseys. Other teams that prominently feature red on their kits include A.C. Milan (nicknamed i rossoneri for their red and black shirts), AFC Ajax, Olympiacos, River Plate, Atlético Madrid, and Flamengo. A red penalty card is issued to a player who commits a serious infraction: the player is immediately disqualified from further play and his team must continue with one less player for the game's duration.
Red is one of the most common colors used on national flags. The use of red has similar connotations from country to country: the blood, sacrifice, and courage of those who defended their country; the sun and the hope and warmth it brings; and the sacrifice of Christ's blood (in some historically Christian nations) are a few examples. Red is the color of the flags of several countries that once belonged to the former British Empire. The British flag bears the colors red, white, and blue; it includes the cross of Saint George, patron saint of England, and the saltire of Saint Patrick, patron saint of Ireland, both of which are red on white. The flag of the United States bears the colors of Britain, the colors of the French tricolore include red as part of the old Paris coat of arms, and other countries' flags, such as those of Australia, New Zealand, and Fiji, carry a small inset of the British flag in memory of their ties to that country. Many former colonies of Spain, such as Mexico, Colombia, Ecuador, Cuba, Puerto Rico, Peru, and Venezuela, also feature red-one of the colors of the Spanish flag-on their own banners. Red flags are also used to symbolize storms, bad water conditions, and many other dangers. Navy flags are often red and yellow. Red is prominently featured in the flag of the United States Marine Corps.
Red, blue, and white are also the Pan-Slavic colors adopted by the Slavic solidarity movement of the late nineteenth century. Initially these were the colors of the Russian flag; as the Slavic movement grew, they were adopted by other Slavic peoples including Slovaks, Slovenes, and Serbs. The flags of the Czech Republic and Poland use red for historic heraldic reasons (see Coat of arms of Poland and Coat of arms of the Czech Republic) & not due to Pan-Slavic connotations. In 2004 Georgia adopted a new white flag, which consists of four small and one big red cross in the middle touching all four sides.
Red, white, and black were the colors of the German Empire from 1870 to 1918, and as such they came to be associated with German nationalism. In the 1920s they were adopted as the colors of the Nazi flag. In Mein Kampf, Hitler explained that they were "revered colors expressive of our homage to the glorious past." The red part of the flag was also chosen to attract attention - Hitler wrote: "the new flag ... should prove effective as a large poster" because "in hundreds of thousands of cases a really striking emblem may be the first cause of awakening interest in a movement." The red also symbolized the social program of the Nazis, aimed at German workers. Several designs by a number of different authors were considered, but the one adopted in the end was Hitler's personal design.
The red flag appeared as a political symbol during the French Revolution, after the fall of Bastille. A law adopted by the new government on October 20, 1789 authorized the Garde Nationale to raise the red flag in the event of a riot, to signal that the Garde would imminently intervene. During a demonstration on the Champs de Mars on July 17, 1791, the Garde Nationale fired on the crowd, killed up to fifty people. The government was denounced by the more radical revolutionaries. In the words of his famous hymn, the Marseillaise, Rouget de Lisle wrote: "Against us they have raised the bloody flag of tyranny!" (Contre nous de la tyrannie, l'entendard sanglant est leve). Beginning in 1790, the most radical revolutionaries adopted the red flag themselves, to symbolize the blood of those killed in the demonstrations, and to call for the repression of those they considered counter-revolutionary.
Karl Marx published the Communist Manifesto in February 1848, with little attention. However, a few days later the French Revolution of 1848 broke out, which replaced the monarchy of Louis Philippe with the Second French Republic. In June 1848, Paris workers, disenchanted with the new government, built barricades and raised red flags. The new government called in the French Army to put down the uprising, the first of many such confrontations between the army and the new worker's movements in Europe.
In 1870, following the stunning defeat of the French Army by the Germans in the Franco-Prussian War, French workers and socialist revolutionaries seized Paris and created the Paris Commune. The Commune lasted for two months before it was crushed by the French Army, with much bloodshed. The original red banners of the Commune became icons of the socialist revolution; in 1921 members of the French Communist Party came to Moscow and presented the new Soviet government with one of the original Commune banners; it was placed (and is still in place) in the tomb of Vladimir Lenin, next to his open coffin.
In the United States, political commentators often refer to the "red states", which traditionally vote for Republican candidates in presidential elections, and "blue states", which vote for the Democratic candidate. This convention is relatively recent: before the 2000 presidential election, media outlets assigned red and blue to both parties, sometimes alternating the allocation for each election. Fixed usage was established during the 39-day recount following the 2000 election, when the media began to discuss the contest in terms of "red states" versus "blue states".
The Communist Party of China, founded in 1920, adopted the red flag and hammer and sickle emblem of the Soviet Union, which became the national symbols when the Party took power in China in 1949. Under Party leader Mao Zedong, the Party anthem became "The East Is Red", and Mao Zedong himself was sometimes referred to as a "red sun". During the Cultural Revolution in China, Party ideology was enforced by the Red Guards, and the sayings of Mao Zedong were published as a small red book in hundreds of millions of copies. Today the Communist Party of China claims to be the largest political party in the world, with eighty million members.
After the Communist Party of China took power in 1949, the flag of China became a red flag with a large star symbolizing the Communist Party, and smaller stars symbolizing workers, peasants, the urban middle class and rural middle class. The flag of the Communist Party of China became a red banner with a hammer and sickle, similar to that on the Soviet flag. In the 1950s and 1960s, other Communist regimes such as Vietnam and Laos also adopted red flags. Some Communist countries, such as Cuba, chose to keep their old flags; and other countries used red flags which had nothing to do with Communism or socialism; the red flag of Nepal, for instance, represents the national flower.
The history of India includes the prehistoric settlements and societies in the Indian subcontinent; the blending of the Indus Valley Civilization and Indo-Aryan culture into the Vedic Civilization; the development of Hinduism as a synthesis of various Indian cultures and traditions; the rise of the Śramaṇa movement; the decline of Śrauta sacrifices and the birth of the initiatory traditions of Jainism, Buddhism, Shaivism, Vaishnavism and Shaktism; the onset of a succession of powerful dynasties and empires for more than two millennia throughout various geographic areas of the subcontinent, including the growth of Muslim dynasties during the Medieval period intertwined with Hindu powers; the advent of European traders resulting in the establishment of the British rule; and the subsequent independence movement that led to the Partition of India and the creation of the Republic of India.
Evidence of Anatomically modern humans in the Indian subcontinent is recorded as long as 75,000 years ago, or with earlier hominids including Homo erectus from about 500,000 years ago. The Indus Valley Civilization which spread and flourished in the northwestern part of the Indian subcontinent from c. 3200 to 1300 BCE, was the first major civilization in South Asia. A sophisticated and technologically advanced urban culture developed in the Mature Harappan period, from 2600 to 1900 BCE. This civilization collapsed at the start of the second millennium BCE and was later followed by the Iron Age Vedic Civilization, which extended over much of the Indo-Gangetic plain and which witnessed the rise of major polities known as the Mahajanapadas. In one of these kingdoms, Magadha, Mahavira and Gautama Buddha propagated their Shramanic philosophies during the fifth and sixth century BCE.
Most of the subcontinent was conquered by the Maurya Empire during the 4th and 3rd centuries BCE. From the 3rd century BC onwards Prakrit and Pali literature in the north and the Sangam literature in southern India started to flourish. Wootz steel originated in south India in the 3rd century BC and was exported to foreign countries. Various parts of India were ruled by numerous dynasties for the next 1,500 years, among which the Gupta Empire stands out. This period, witnessing a Hindu religious and intellectual resurgence, is known as the classical or "Golden Age of India". During this period, aspects of Indian civilization, administration, culture, and religion (Hinduism and Buddhism) spread to much of Asia, while kingdoms in southern India had maritime business links with the Roman Empire from around 77 CE. Indian cultural influence spread over many parts of Southeast Asia which led to the establishment of Indianized kingdoms in Southeast Asia (Greater India).
The most significant event between the 7th and 11th century was the Tripartite struggle centered on Kannauj that lasted for more than two centuries between the Pala Empire, Rashtrakuta Empire, and Gurjara Pratihara Empire. Southern India was ruled by the Chalukya, Chola, Pallava, Chera, Pandyan, and Western Chalukya Empires. The seventh century also saw the advent of Islam as a political power, though as a fringe, in the western part of the subcontinent, in modern-day Pakistan. The Chola dynasty conquered southern India and successfully invaded parts of Southeast Asia, Sri Lanka, Maldives and Bengal in the 11th century. The early medieval period Indian mathematics influenced the development of mathematics and astronomy in the Arab world and the Hindu numerals were introduced.
Muslim rule started in parts of north India in the 13th century when the Delhi Sultanate was founded in 1206 CE by the Central Asian Turks. The Delhi Sultanate ruled the major part of northern India in the early 14th century, but declined in the late 14th century when several powerful Hindu states such as the Vijayanagara Empire, Gajapati Kingdom, Ahom Kingdom, as well as Rajput dynasties and states, such as Mewar dynasty, emerged. The 15th century saw the emergence of Sikhism. In the 16th century, Mughals came from Central Asia and gradually covered most of India. The Mughal Empire suffered a gradual decline in the early 18th century, which provided opportunities for the Maratha Empire, Sikh Empire and Mysore Kingdom to exercise control over large areas of the subcontinent.
From the late 18th century to the mid-19th century, large areas of India were annexed by the British East India Company of British Empire. Dissatisfaction with Company rule led to the Indian Rebellion of 1857, after which the British provinces of India were directly administered by the British Crown and witnessed a period of both rapid development of infrastructure and economic stagnation. During the first half of the 20th century, a nationwide struggle for independence was launched with the leading party involved being the Indian National Congress which was later joined by other organizations. The subcontinent gained independence from the United Kingdom in 1947, after the British provinces were partitioned into the dominions of India and Pakistan and the princely states all acceded to one of the new states.
Romila Thapar notes that the division into Hindu-Muslim-British periods of Indian history gives too much weight to "ruling dynasties and foreign invasions", neglecting the social-economic history which often showed a strong continuity. The division into Ancient-Medieval-Modern periods overlooks the fact that the Muslim conquests occurred gradually during which time many things came and went off, while the south was never completely conquered. According to Thapar, a periodisation could also be based on "significant social and economic changes", which are not strictly related to a change of ruling powers.[note 1]
Isolated remains of Homo erectus in Hathnora in the Narmada Valley in central India indicate that India might have been inhabited since at least the Middle Pleistocene era, somewhere between 500,000 and 200,000 years ago. Tools crafted by proto-humans that have been dated back two million years have been discovered in the northwestern part of the subcontinent. The ancient history of the region includes some of South Asia's oldest settlements and some of its major civilisations. The earliest archaeological site in the subcontinent is the palaeolithic hominid site in the Soan River valley. Soanian sites are found in the Sivalik region across what are now India, Pakistan, and Nepal.
The Mesolithic period in the Indian subcontinent was followed by the Neolithic period, when more extensive settlement of the subcontinent occurred after the end of the last Ice Age approximately 12,000 years ago. The first confirmed semipermanent settlements appeared 9,000 years ago in the Bhimbetka rock shelters in modern Madhya Pradesh, India. Early Neolithic culture in South Asia is represented by the Bhirrana findings (7500 BCE) in Haryana, India & Mehrgarh findings (7000–9000 BCE) in Balochistan, Pakistan.
The Mature Indus civilisation flourished from about 2600 to 1900 BCE, marking the beginning of urban civilisation on the subcontinent. The civilisation included urban centres such as Dholavira, Kalibangan, Ropar, Rakhigarhi, and Lothal in modern-day India, as well as Harappa, Ganeriwala, and Mohenjo-daro in modern-day Pakistan. The civilisation is noted for its cities built of brick, roadside drainage system, and multistoreyed houses and is thought to have had some kind of municipal organization.
The Vedic period is named after the Indo-Aryan culture of north-west India, although other parts of India had a distinct cultural identity during this period. The Vedic culture is described in the texts of Vedas, still sacred to Hindus, which were orally composed in Vedic Sanskrit. The Vedas are some of the oldest extant texts in India. The Vedic period, lasting from about 1750 to 500 BCE, and contributed the foundations of several cultural aspects of Indian subcontinent. In terms of culture, many regions of the subcontinent transitioned from the Chalcolithic to the Iron Age in this period.
At the end of the Rigvedic period, the Aryan society began to expand from the northwestern region of the Indian subcontinent, into the western Ganges plain. It became increasingly agricultural and was socially organised around the hierarchy of the four varnas, or social classes. This social structure was characterized both by syncretising with the native cultures of northern India, but also eventually by the excluding of indigenous peoples by labelling their occupations impure. During this period, many of the previous small tribal units and chiefdoms began to coalesce into monarchical, state-level polities.
The Kuru kingdom was the first state-level society of the Vedic period, corresponding to the beginning of the Iron Age in northwestern India, around 1200 – 800 BCE, as well as with the composition of the Atharvaveda (the first Indian text to mention iron, as śyāma ayas, literally "black metal"). The Kuru state organized the Vedic hymns into collections, and developed the orthodox srauta ritual to uphold the social order. When the Kuru kingdom declined, the center of Vedic culture shifted to their eastern neighbours, the Panchala kingdom. The archaeological Painted Grey Ware culture, which flourished in the Haryana and western Uttar Pradesh regions of northern India from about 1100 to 600 BCE, is believed to correspond to the Kuru and Panchala kingdoms.
In addition to the Vedas, the principal texts of Hinduism, the core themes of the Sanskrit epics Ramayana and Mahabharata are said to have their ultimate origins during this period. The Mahabharata remains, today, the longest single poem in the world. Historians formerly postulated an "epic age" as the milieu of these two epic poems, but now recognize that the texts (which are both familiar with each other) went through multiple stages of development over centuries. For instance, the Mahabharata may have been based on a small-scale conflict (possibly about 1000 BCE) which was eventually "transformed into a gigantic epic war by bards and poets". There is no conclusive proof from archaeology as to whether the specific events of the Mahabharat have any historical basis. The existing texts of these epics are believed to belong to the post-Vedic age, between c. 400 BCE and 400 CE. Some even attempted to date the events using methods of archaeoastronomy which have produced, depending on which passages are chosen and how they are interpreted, estimated dates ranging up to mid 2nd millennium BCE.
During the time between 800 and 200 BCE the Shramana-movement formed, from which originated Jainism and Buddhism. In the same period the first Upanishads were written. After 500 BCE, the so-called "Second urbanization" started, with new urban settlements arising at the Ganges plain, especially the Central Ganges plain. The Central Ganges Plain, where Magadha gained prominence, forming the base of the Mauryan Empire, was a distinct cultural area, with new states arising after 500 BC[web 1] during the so-called "Second urbanization".[note 3] It was influenced by the Vedic culture, but differed markedly from the Kuru-Panchala region. It "was the area of the earliest known cultivation of rice in South Asia and by 1800 BC was the location of an advanced neolithic population associated with the sites of Chirand and Chechar". In this region the Shramanic movements flourished, and Jainism and Buddhism originated.
In the later Vedic Age, a number of small kingdoms or city states had covered the subcontinent, many mentioned in Vedic, early Buddhist and Jaina literature as far back as 500 BCE. sixteen monarchies and "republics" known as the Mahajanapadas—Kashi, Kosala, Anga, Magadha, Vajji (or Vriji), Malla, Chedi, Vatsa (or Vamsa), Kuru, Panchala, Matsya (or Machcha), Shurasena, Assaka, Avanti, Gandhara, and Kamboja—stretched across the Indo-Gangetic Plain from modern-day Afghanistan to Bengal and Maharashtra. This period saw the second major rise of urbanism in India after the Indus Valley Civilisation.
Many smaller clans mentioned within early literature seem to have been present across the rest of the subcontinent. Some of these kings were hereditary; other states elected their rulers. Early "republics" such as the Vajji (or Vriji) confederation centered in the city of Vaishali, existed as early as the 6th century BCE and persisted in some areas until the 4th century CE. The educated speech at that time was Sanskrit, while the languages of the general population of northern India are referred to as Prakrits. Many of the sixteen kingdoms had coalesced to four major ones by 500/400 BCE, by the time of Gautama Buddha. These four were Vatsa, Avanti, Kosala, and Magadha. The Life of Gautam Budhha was mainly associated with these four kingdoms.
The 7th and 6th centuries BC witnessed the composition of the earliest Upanishads. Upanishads form the theoretical basis of classical Hinduism and are known as Vedanta (conclusion of the Vedas). The older Upanishads launched attacks of increasing intensity on the ritual. Anyone who worships a divinity other than the Self is called a domestic animal of the gods in the Brihadaranyaka Upanishad. The Mundaka launches the most scathing attack on the ritual by comparing those who value sacrifice with an unsafe boat that is endlessly overtaken by old age and death.
Increasing urbanisation of India in 7th and 6th centuries BCE led to the rise of new ascetic or shramana movements which challenged the orthodoxy of rituals. Mahavira (c. 549–477 BC), proponent of Jainism, and Buddha (c. 563-483), founder of Buddhism were the most prominent icons of this movement. Shramana gave rise to the concept of the cycle of birth and death, the concept of samsara, and the concept of liberation. Buddha found a Middle Way that ameliorated the extreme asceticism found in the Sramana religions.
Magadha (Sanskrit: मगध) formed one of the sixteen Mahā-Janapadas (Sanskrit: "Great Countries") or kingdoms in ancient India. The core of the kingdom was the area of Bihar south of the Ganges; its first capital was Rajagriha (modern Rajgir) then Pataliputra (modern Patna). Magadha expanded to include most of Bihar and Bengal with the conquest of Licchavi and Anga respectively, followed by much of eastern Uttar Pradesh and Orissa. The ancient kingdom of Magadha is heavily mentioned in Jain and Buddhist texts. It is also mentioned in the Ramayana, Mahabharata, Puranas. A state of Magadha, possibly a tribal kingdom, is recorded in Vedic texts much earlier in time than 600 BC. Magadha Empire had great rulers like Bimbisara and Ajatshatru.
The earliest reference to the Magadha people occurs in the Atharva-Veda where they are found listed along with the Angas, Gandharis, and Mujavats. Magadha played an important role in the development of Jainism and Buddhism, and two of India's greatest empires, the Maurya Empire and Gupta Empire, originated from Magadha. These empires saw advancements in ancient India's science, mathematics, astronomy, religion, and philosophy and were considered the Indian "Golden Age". The Magadha kingdom included republican communities such as the community of Rajakumara. Villages had their own assemblies under their local chiefs called Gramakas. Their administrations were divided into executive, judicial, and military functions.
In 530 BC Cyrus the Great, King of the Persian Achaemenid Empire crossed the Hindu-Kush mountains to seek tribute from the tribes of Kamboja, Gandhara and the trans-India region (modern Afghanistan and Pakistan). By 520 BC, during the reign of Darius I of Persia, much of the northwestern subcontinent (present-day eastern Afghanistan and Pakistan) came under the rule of the Persian Achaemenid Empire, as part of the far easternmost territories. The area remained under Persian control for two centuries. During this time India supplied mercenaries to the Persian army then fighting in Greece.
By 326 BC, Alexander the Great had conquered Asia Minor and the Achaemenid Empire and had reached the northwest frontiers of the Indian subcontinent. There he defeated King Porus in the Battle of the Hydaspes (near modern-day Jhelum, Pakistan) and conquered much of the Punjab. Alexander's march east put him in confrontation with the Nanda Empire of Magadha and the Gangaridai of Bengal. His army, exhausted and frightened by the prospect of facing larger Indian armies at the Ganges River, mutinied at the Hyphasis (modern Beas River) and refused to march further East. Alexander, after the meeting with his officer, Coenus, and learning about the might of Nanda Empire, was convinced that it was better to return.
The Maurya Empire (322–185 BCE) was the first empire to unify India into one state, and was the largest on the Indian subcontinent. At its greatest extent, the Mauryan Empire stretched to the north up to the natural boundaries of the Himalayas and to the east into what is now Assam. To the west, it reached beyond modern Pakistan, to the Hindu Kush mountains in what is now Afghanistan. The empire was established by Chandragupta Maurya in Magadha (in modern Bihar) when he overthrew the Nanda Dynasty. Chandragupta's son Bindusara succeeded to the throne around 297 BC. By the time he died in c. 272 BC, a large part of the subcontinent was under Mauryan suzerainty. However, the region of Kalinga (around modern day Odisha) remained outside Mauryan control, perhaps interfering with their trade with the south.
The Arthashastra and the Edicts of Ashoka are the primary written records of the Mauryan times. Archaeologically, this period falls into the era of Northern Black Polished Ware (NBPW). The Mauryan Empire was based on a modern and efficient economy and society. However, the sale of merchandise was closely regulated by the government. Although there was no banking in the Mauryan society, usury was customary. A significant amount of written records on slavery are found, suggesting a prevalence thereof. During this period, a high quality steel called Wootz steel was developed in south India and was later exported to China and Arabia.
During the Sangam period Tamil literateure flourished from the 3rd century BCE to the 4th century CE. During this period the 3 Tamil Dynasties Chera dynasty, Chola dynasty and the Pandyan Dynasty ruled parts of southern India. The Sangam literature deals with the history, politics, wars and culture of the Tamil people of this period. The scholars of the Sangam period rose from among the common people who sought the patronage of the Tamil Kings but who mainly wrote about the common people and their concerns. Unlike Sanskrit writers who were mostly Brahmins, Sangam writers came from diverse classes and social backgrounds and were mostly non-Brahmins. They belonged to different faiths and professions like farmers, artisans, merchants, monks, priests and even princes and quite few of them were even women.
The Śātavāhana Empire was a royal Indian dynasty based from Amaravati in Andhra Pradesh as well as Junnar (Pune) and Prathisthan (Paithan) in Maharashtra. The territory of the empire covered much of India from 230 BCE onward. Sātavāhanas started out as feudatories to the Mauryan dynasty, but declared independence with its decline. They are known for their patronage of Hinduism and Buddhism which resulted in Buddhist monuments from Ellora (a UNESCO World Heritage Site) to Amaravati. The Sātavāhanas were one of the first Indian states to issue coins struck with their rulers embossed. They formed a cultural bridge and played a vital role in trade as well as the transfer of ideas and culture to and from the Indo-Gangetic Plain to the southern tip of India. They had to compete with the Shunga Empire and then the Kanva dynasty of Magadha to establish their rule. Later, they played a crucial role to protect a huge part of India against foreign invaders like the Sakas, Yavanas and Pahlavas. In particular their struggles with the Western Kshatrapas went on for a long time. The notable rulers of the Satavahana Dynasty Gautamiputra Satakarni and Sri Yajna Sātakarni were able to defeat the foreign invaders like the Western Kshatrapas and to stop their expansion. In the 3rd century CE the empire was split into smaller states.
The Shunga Empire or Shunga Empire was an ancient Indian dynasty from Magadha that controlled vast areas of the Indian subcontinent from around 187 to 78 BCE. The dynasty was established by Pushyamitra Shunga, after the fall of the Maurya Empire. Its capital was Pataliputra, but later emperors such as Bhagabhadra also held court at Besnagar, modern Vidisha in Eastern Malwa. Pushyamitra Shunga ruled for 36 years and was succeeded by his son Agnimitra. There were ten Shunga rulers. The empire is noted for its numerous wars with both foreign and indigenous powers. They fought battles with the Kalingas, Satavahanas, the Indo-Greeks, and possibly the Panchalas and Mathuras. Art, education, philosophy, and other forms of learning flowered during this period including small terracotta images, larger stone sculptures, and architectural monuments such as the Stupa at Bharhut, and the renowned Great Stupa at Sanchi. The Shunga rulers helped to establish the tradition of royal sponsorship of learning and art. The script used by the empire was a variant of Brahmi and was used to write the Sanskrit language. The Shunga Empire played an imperative role in patronizing Indian culture at a time when some of the most important developments in Hindu thought were taking place.
During the reign of Khārabēḷa, the Chedi dynasty of Kaḷinga ascended to eminence and restored the lost power and glory of Kaḷinga, which had been subdued since the devastating war with Ashoka. Kaḷingan military might was reinstated by Khārabēḷa: under Khārabēḷa's generalship, the Kaḷinga state had a formidable maritime reach with trade routes linking it to the then-Simhala (Sri Lanka), Burma (Myanmar), Siam (Thailand), Vietnam, Kamboja (Cambodia), Malaysia, Borneo, Bali, Samudra (Sumatra) and Jabadwipa (Java). Khārabēḷa led many successful campaigns against the states of Magadha, Anga, Satavahanas till the southern most regions of Pandyan Empire (modern Tamil Nadu).
The Kushan Empire expanded out of what is now Afghanistan into the northwest of the subcontinent under the leadership of their first emperor, Kujula Kadphises, about the middle of the 1st century CE. They came of an Indo-European language speaking Central Asian tribe called the Yuezhi, a branch of which was known as the Kushans. By the time of his grandson, Kanishka, they had conquered most of northern India, at least as far as Saketa and Pataliputra, in the middle Ganges Valley, and probably as far as the Bay of Bengal.
Classical India refers to the period when much of the Indian subcontinent was reunited under the Gupta Empire (c. 320–550 CE). This period has been called the Golden Age of India and was marked by extensive achievements in science, technology, engineering, art, dialectic, literature, logic, mathematics, astronomy, religion, and philosophy that crystallized the elements of what is generally known as Hindu culture. The Hindu-Arabic numerals, a positional numeral system, originated in India and was later transmitted to the West through the Arabs. Early Hindu numerals had only nine symbols, until 600 to 800 CE, when a symbol for zero was developed for the numeral system. The peace and prosperity created under leadership of Guptas enabled the pursuit of scientific and artistic endeavors in India.
The high points of this cultural creativity are magnificent architecture, sculpture, and painting. The Gupta period produced scholars such as Kalidasa, Aryabhata, Varahamihira, Vishnu Sharma, and Vatsyayana who made great advancements in many academic fields. The Gupta period marked a watershed of Indian culture: the Guptas performed Vedic sacrifices to legitimize their rule, but they also patronized Buddhism, which continued to provide an alternative to Brahmanical orthodoxy. The military exploits of the first three rulers – Chandragupta I, Samudragupta, and Chandragupta II - brought much of India under their leadership. Science and political administration reached new heights during the Gupta era. Strong trade ties also made the region an important cultural centre and established it as a base that would influence nearby kingdoms and regions in Burma, Sri Lanka, Maritime Southeast Asia, and Indochina. For these reasons, historian Dr.Barnett remarked:
Kadamba (345 – 525 CE) was an ancient royal dynasty of Karnataka, India that ruled northern Karnataka and the Konkan from Banavasi in present-day Uttara Kannada district. At the peak of their power under King Kakushtavarma, the Kadambas of Banavasi ruled large parts of modern Karnataka state. The dynasty was founded by Mayurasharma in 345 CE which at later times showed the potential of developing into imperial proportions, an indication to which is provided by the titles and epithets assumed by its rulers. King Mayurasharma defeated the armies of Pallavas of Kanchi possibly with help of some native tribes. The Kadamba fame reached its peak during the rule of Kakusthavarma, a notable ruler with whom even the kings of Gupta Dynasty of northern India cultivated marital alliances. The Kadambas were contemporaries of the Western Ganga Dynasty and together they formed the earliest native kingdoms to rule the land with absolute autonomy. The dynasty later continued to rule as a feudatory of larger Kannada empires, the Chalukya and the Rashtrakuta empires, for over five hundred years during which time they branched into minor dynasties known as the Kadambas of Goa, Kadambas of Halasi and Kadambas of Hangal.
The Hephthalites (or Ephthalites), also known as the White Huns, were a nomadic confederation in Central Asia during the late antiquity period. The White Huns established themselves in modern-day Afghanistan by the first half of the 5th century. Led by the Hun military leader Toramana, they overran the northern region of Pakistan and North India. Toramana's son Mihirakula, a Saivite Hindu, moved up to near Pataliputra to the east and Gwalior to the central India. Hiuen Tsiang narrates Mihirakula's merciless persecution of Buddhists and destruction of monasteries, though the description is disputed as far as the authenticity is concerned. The Huns were defeated by the Indian kings Yasodharman of Malwa and Narasimhagupta in the 6th century. Some of them were driven out of India and others were assimilated in the Indian society.
After the downfall of the prior Gupta Empire in the middle of the 6th century, North India reverted to small republics and small monarchical states ruled by Gupta rulers. Harsha was a convert to Buddhism. He united the small republics from Punjab to central India, and their representatives crowned Harsha king at an assembly in April 606 giving him the title of Maharaja when he was merely 16 years old. Harsha belonged to Kanojia. He brought all of northern India under his control. The peace and prosperity that prevailed made his court a center of cosmopolitanism, attracting scholars, artists and religious visitors from far and wide. The Chinese traveler Xuan Zang visited the court of Harsha and wrote a very favorable account of him, praising his justice and generosity.
From the fifth century to the thirteenth, Śrauta sacrifices declined, and initiatory traditions of Buddhism, Jainism or more commonly Shaivism, Vaishnavism and Shaktism expanded in royal courts. This period produced some of India's finest art, considered the epitome of classical development, and the development of the main spiritual and philosophical systems which continued to be in Hinduism, Buddhism and Jainism. Emperor Harsha of Kannauj succeeded in reuniting northern India during his reign in the 7th century, after the collapse of the Gupta dynasty. His empire collapsed after his death.
Ronald Inden writes that by the 8th century CE symbols of Hindu gods "replaced the Buddha at the imperial centre and pinnacle of the cosmo-political system, the image or symbol of the Hindu god comes to be housed in a monumental temple and given increasingly elaborate imperial-style puja worship". Although Buddhism did not disappear from India for several centuries after the eighth, royal proclivities for the cults of Vishnu and Shiva weakened Buddhism's position within the sociopolitical context and helped make possible its decline.
From the 8th to the 10th century, three dynasties contested for control of northern India: the Gurjara Pratiharas of Malwa, the Palas of Bengal, and the Rashtrakutas of the Deccan. The Sena dynasty would later assume control of the Pala Empire, and the Gurjara Pratiharas fragmented into various states. These were the first of the Rajput states. The first recorded Rajput kingdoms emerged in Rajasthan in the 6th century, and small Rajput dynasties later ruled much of northern India. One Gurjar Rajput of the Chauhan clan, Prithvi Raj Chauhan, was known for bloody conflicts against the advancing Turkic sultanates. The Chola empire emerged as a major power during the reign of Raja Raja Chola I and Rajendra Chola I who successfully invaded parts of Southeast Asia and Sri Lanka in the 11th century. Lalitaditya Muktapida (r. 724 CE–760 CE) was an emperor of the Kashmiri Karkoṭa dynasty, which exercised influence in northwestern India from 625 CE until 1003, and was followed by Lohara dynasty. He is known primarily for his successful battles against the Muslim and Tibetan advances into Kashmiri-dominated regions. Kalhana in his Rajatarangini credits king Lalitaditya with leading an aggressive military campaign in Northern India and Central Asia. He broke into the Uttarapatha and defeated the rebellious tribes of the Kambojas, Tukharas (Turks in Turkmenistan and Tocharians in Badakhshan), Bhautas (Tibetans in Baltistan and Tibet) and Daradas (Dards). His campaign then led him to subjugate the kingdoms of Pragjyotisha, Strirajya and the Uttarakurus. The Shahi dynasty ruled portions of eastern Afghanistan, northern Pakistan, and Kashmir from the mid-7th century to the early 11th century.
The Chalukya Empire (Kannada: ಚಾಲುಕ್ಯರು [tʃaːɭukjə]) was an Indian royal dynasty that ruled large parts of southern and central India between the 6th and the 12th centuries. During this period, they ruled as three related yet individual dynasties. The earliest dynasty, known as the "Badami Chalukyas", ruled from Vatapi (modern Badami) from the middle of the 6th century. The Badami Chalukyas began to assert their independence at the decline of the Kadamba kingdom of Banavasi and rapidly rose to prominence during the reign of Pulakeshin II. The rule of the Chalukyas marks an important milestone in the history of South India and a golden age in the history of Karnataka. The political atmosphere in South India shifted from smaller kingdoms to large empires with the ascendancy of Badami Chalukyas. A Southern India-based kingdom took control and consolidated the entire region between the Kaveri and the Narmada rivers. The rise of this empire saw the birth of efficient administration, overseas trade and commerce and the development of new style of architecture called "Chalukyan architecture". The Chalukya dynasty ruled parts of southern and central India from Badami in Karnataka between 550 and 750, and then again from Kalyani between 970 and 1190.
Founded by Dantidurga around 753, the Rashtrakuta Empire ruled from its capital at Manyakheta for almost two centuries. At its peak, the Rashtrakutas ruled from the Ganges River and Yamuna River doab in the north to Cape Comorin in the south, a fruitful time of political expansion, architectural achievements and famous literary contributions.[citation needed] The early kings of this dynasty were Hindu but the later kings were strongly influenced by Jainism. Govinda III and Amoghavarsha were the most famous of the long line of able administrators produced by the dynasty. Amoghavarsha, who ruled for 64 years, was also an author and wrote Kavirajamarga, the earliest known Kannada work on poetics. Architecture reached a milestone in the Dravidian style, the finest example of which is seen in the Kailasanath Temple at Ellora. Other important contributions are the sculptures of Elephanta Caves in modern Maharashtra as well as the Kashivishvanatha temple and the Jain Narayana temple at Pattadakal in modern Karnataka, all of which are UNESCO World Heritage Sites. The Arab traveler Suleiman described the Rashtrakuta Empire as one of the four great Empires of the world. The Rashtrakuta period marked the beginning of the golden age of southern Indian mathematics. The great south Indian mathematician Mahāvīra (mathematician) lived in the Rashtrakuta Empire and his text had a huge impact on the medieval south Indian mathematicians who lived after him. The Rashtrakuta rulers also patronised men of letters, who wrote in a variety of languages from Sanskrit to the Apabhraṃśas.
The Pala Empire (Bengali: পাল সাম্রাজ্য Pal Samrajyô) flourished during the Classical period of India, and may be dated during 750–1174 CE. Founded by Gopala I, it was ruled by a Buddhist dynasty from Bengal in the eastern region of the Indian subcontinent. Though the Palas were followers of the Mahayana and Tantric schools of Buddhism, they also patronised Shaivism and Vaishnavism. The morpheme Pala, meaning "protector", was used as an ending for the names of all the Pala monarchs. The empire reached its peak under Dharmapala and Devapala. Dharmapala is believed to have conquered Kanauj and extended his sway up to the farthest limits of India in the northwest. The Pala Empire can be considered as the golden era of Bengal in many ways. Dharmapala founded the Vikramashila and revived Nalanda, considered one of the first great universities in recorded history. Nalanda reached its height under the patronage of the Pala Empire. The Palas also built many viharas. They maintained close cultural and commercial ties with countries of Southeast Asia and Tibet. Sea trade added greatly to the prosperity of the Pala kingdom. The Arab merchant Suleiman notes the enormity of the Pala army in his memoirs.
Medieval Cholas rose to prominence during the middle of the 9th century C.E. and established the greatest empire South India had seen. They successfully united the South India under their rule and through their naval strength extended their influence in the Southeast Asian countries such as Srivijaya. Under Rajaraja Chola I and his successors Rajendra Chola I, Rajadhiraja Chola, Virarajendra Chola and Kulothunga Chola I the dynasty became a military, economic and cultural power in South Asia and South-East Asia. Rajendra Chola I's navies went even further, occupying the sea coasts from Burma to Vietnam, the Andaman and Nicobar Islands, the Lakshadweep (Laccadive) islands, Sumatra, and the Malay Peninsula in Southeast Asia and the Pegu islands. The power of the new empire was proclaimed to the eastern world by the expedition to the Ganges which Rajendra Chola I undertook and by the occupation of cities of the maritime empire of Srivijaya in Southeast Asia, as well as by the repeated embassies to China. They dominated the political affairs of Sri Lanka for over two centuries through repeated invasions and occupation. They also had continuing trade contacts with the Arabs in the west and with the Chinese empire in the east. Rajaraja Chola I and his equally distinguished son Rajendra Chola I gave political unity to the whole of Southern India and established the Chola Empire as a respected sea power. Under the Cholas, the South India reached new heights of excellence in art, religion and literature. In all of these spheres, the Chola period marked the culmination of movements that had begun in an earlier age under the Pallavas. Monumental architecture in the form of majestic temples and sculpture in stone and bronze reached a finesse never before achieved in India.
The Western Chalukya Empire (Kannada:ಪಶ್ಚಿಮ ಚಾಲುಕ್ಯ ಸಾಮ್ರಾಜ್ಯ) ruled most of the western Deccan, South India, between the 10th and 12th centuries. Vast areas between the Narmada River in the north and Kaveri River in the south came under Chalukya control. During this period the other major ruling families of the Deccan, the Hoysalas, the Seuna Yadavas of Devagiri, the Kakatiya dynasty and the Southern Kalachuri, were subordinates of the Western Chalukyas and gained their independence only when the power of the Chalukya waned during the later half of the 12th century. The Western Chalukyas developed an architectural style known today as a transitional style, an architectural link between the style of the early Chalukya dynasty and that of the later Hoysala empire. Most of its monuments are in the districts bordering the Tungabhadra River in central Karnataka. Well known examples are the Kasivisvesvara Temple at Lakkundi, the Mallikarjuna Temple at Kuruvatti, the Kallesvara Temple at Bagali and the Mahadeva Temple at Itagi. This was an important period in the development of fine arts in Southern India, especially in literature as the Western Chalukya kings encouraged writers in the native language of Kannada, and Sanskrit like the philosopher and statesman Basava and the great mathematician Bhāskara II.
The early Islamic literature indicates that the conquest of India was one of the very early ambitions of the Muslims, though it was recognized as a particularly difficult one. After conquering Persia, the Arab Umayyad Caliphate incorporated parts of what are now Afghanistan and Pakistan around 720. The book Chach Nama chronicles the Chacha Dynasty's period, following the demise of the Rai Dynasty and the ascent of Chach of Alor to the throne, down to the Arab conquest by Muhammad bin Qasim in the early 8th century AD, by defeating the last Hindu monarch of Sindh, Raja Dahir.
In 712, Arab Muslim general Muhammad bin Qasim conquered most of the Indus region in modern-day Pakistan for the Umayyad Empire, incorporating it as the "As-Sindh" province with its capital at Al-Mansurah, 72 km (45 mi) north of modern Hyderabad in Sindh, Pakistan. After several incursions, the Hindu kings east of Indus defeated the Arabs at the Battle of Rajasthan, halting their expansion and containing them at Sindh in Pakistan. The south Indian Chalukya empire under Vikramaditya II, Nagabhata I of the Pratihara dynasty and Bappa Rawal of the Guhilot dynasty repulsed the Arab invaders in the early 8th century.
Several Islamic kingdoms (sultanates) under both foreign and, newly converted, Rajput rulers were established across the north western subcontinent (Afghanistan and Pakistan) over a period of a few centuries. From the 10th century, Sindh was ruled by the Rajput Soomra dynasty, and later, in the mid-13th century by the Rajput Samma dynasty. Additionally, Muslim trading communities flourished throughout coastal south India, particularly on the western coast where Muslim traders arrived in small numbers, mainly from the Arabian peninsula. This marked the introduction of a third Abrahamic Middle Eastern religion, following Judaism and Christianity, often in puritanical form. Mahmud of Ghazni in the early 11th century raided mainly the north-western parts of the Indian sub-continent 17 times, but he did not seek to establish "permanent dominion" in those areas.
The Kabul Shahi dynasties ruled the Kabul Valley and Gandhara (modern-day Pakistan and Afghanistan) from the decline of the Kushan Empire in the 3rd century to the early 9th century. The Shahis are generally split up into two eras: the Buddhist Shahis and the Hindu Shahis, with the change-over thought to have occurred sometime around 870. The kingdom was known as the Kabul Shahan or Ratbelshahan from 565-670, when the capitals were located in Kapisa and Kabul, and later Udabhandapura, also known as Hund for its new capital.
The Hindu Shahis under Jayapala, is known for his struggles in defending his kingdom against the Ghaznavids in the modern-day eastern Afghanistan and Pakistan region. Jayapala saw a danger in the consolidation of the Ghaznavids and invaded their capital city of Ghazni both in the reign of Sebuktigin and in that of his son Mahmud, which initiated the Muslim Ghaznavid and Hindu Shahi struggles. Sebuk Tigin, however, defeated him, and he was forced to pay an indemnity. Jayapala defaulted on the payment and took to the battlefield once more. Jayapala however, lost control of the entire region between the Kabul Valley and Indus River.
However, the army was hopeless in battle against the western forces, particularly against the young Mahmud of Ghazni. In the year 1001, soon after Sultan Mahmud came to power and was occupied with the Qarakhanids north of the Hindu Kush, Jaipal attacked Ghazni once more and upon suffering yet another defeat by the powerful Ghaznavid forces, near present-day Peshawar. After the Battle of Peshawar, he committed suicide because his subjects thought he had brought disaster and disgrace to the Shahi dynasty.
Like other settled, agrarian societies in history, those in the Indian subcontinent have been attacked by nomadic tribes throughout its long history. In evaluating the impact of Islam on the sub-continent, one must note that the northwestern sub-continent was a frequent target of tribes raiding from Central Asia. In that sense, the Muslim intrusions and later Muslim invasions were not dissimilar to those of the earlier invasions during the 1st millennium. What does however, make the Muslim intrusions and later Muslim invasions different is that unlike the preceding invaders who assimilated into the prevalent social system, the successful Muslim conquerors retained their Islamic identity and created new legal and administrative systems that challenged and usually in many cases superseded the existing systems of social conduct and ethics, even influencing the non-Muslim rivals and common masses to a large extent, though non-Muslim population was left to their own laws and customs. They also introduced new cultural codes that in some ways were very different from the existing cultural codes. This led to the rise of a new Indian culture which was mixed in nature, though different from both the ancient Indian culture and later westernized modern Indian culture. At the same time it must be noted that overwhelming majority of Muslims in India are Indian natives converted to Islam. This factor also played an important role in the synthesis of cultures.
The subsequent Slave dynasty of Delhi managed to conquer large areas of northern India, while the Khilji dynasty conquered most of central India but were ultimately unsuccessful in conquering and uniting the subcontinent. The Sultanate ushered in a period of Indian cultural renaissance. The resulting "Indo-Muslim" fusion of cultures left lasting syncretic monuments in architecture, music, literature, religion, and clothing. It is surmised that the language of Urdu (literally meaning "horde" or "camp" in various Turkic dialects) was born during the Delhi Sultanate period as a result of the intermingling of the local speakers of Sanskritic Prakrits with immigrants speaking Persian, Turkic, and Arabic under the Muslim rulers. The Delhi Sultanate is the only Indo-Islamic empire to enthrone one of the few female rulers in India, Razia Sultana (1236–1240).
A Turco-Mongol conqueror in Central Asia, Timur (Tamerlane), attacked the reigning Sultan Nasir-u Din Mehmud of the Tughlaq Dynasty in the north Indian city of Delhi. The Sultan's army was defeated on 17 December 1398. Timur entered Delhi and the city was sacked, destroyed, and left in ruins, after Timur's army had killed and plundered for three days and nights. He ordered the whole city to be sacked except for the sayyids, scholars, and the "other Muslims" (artists); 100,000 war prisoners were put to death in one day. The Sultanate suffered significantly from the sacking of Delhi revived briefly under the Lodi Dynasty, but it was a shadow of the former.
The Empire was established in 1336 by Harihara I and his brother Bukka Raya I of Sangama Dynasty. The empire rose to prominence as a culmination of attempts by the southern powers to ward off Islamic invasions by the end of the 13th century. The empire is named after its capital city of Vijayanagara, whose ruins surround present day Hampi, now a World Heritage Site in Karnataka, India. The empire's legacy includes many monuments spread over South India, the best known of which is the group at Hampi. The previous temple building traditions in South India came together in the Vijayanagara Architecture style. The mingling of all faiths and vernaculars inspired architectural innovation of Hindu temple construction, first in the Deccan and later in the Dravidian idioms using the local granite. South Indian mathematics flourished under the protection of the Vijayanagara Empire in Kerala. The south Indian mathematician Madhava of Sangamagrama founded the famous Kerala school of astronomy and mathematics in the 14th century which produced a lot of great south Indian mathematicians like Parameshvara, Nilakantha Somayaji and Jyeṣṭhadeva in medieval south India. Efficient administration and vigorous overseas trade brought new technologies such as water management systems for irrigation. The empire's patronage enabled fine arts and literature to reach new heights in Kannada, Telugu, Tamil and Sanskrit, while Carnatic music evolved into its current form. The Vijayanagara Empire created an epoch in South Indian history that transcended regionalism by promoting Hinduism as a unifying factor. The empire reached its peak during the rule of Sri Krishnadevaraya when Vijayanagara armies were consistently victorious. The empire annexed areas formerly under the Sultanates in the northern Deccan and the territories in the eastern Deccan, including Kalinga, while simultaneously maintaining control over all its subordinates in the south. Many important monuments were either completed or commissioned during the time of Krishna Deva Raya. Vijayanagara went into decline after the defeat in the Battle of Talikota (1565).
For two and a half centuries from the mid 13th, the politics in the Northern India was dominated by the Delhi Sultanate and in the Southern India by the Vijayanagar Empire which originated as a political heir of the erstwhile Hoysala Empire and Pandyan Empire. However, there were other regional powers present as well. In the North, the Rajputs were a dominant force in the Western and Central India. Their power reached to the zenith under Rana Sanga during whose time Rajput armies were constantly victorious against the Sultanate army. In the South, the Bahmani Sultanate was the chief rival of the Vijaynagara and gave Vijayanagara tough days many a times. In the early 16th century Krishnadevaraya of the Vijayanagara Empire defeated the last remnant of Bahmani Sultanate power after which the Bahmani Sultanate collapsed. It was established either by a Brahman convert or patronized by a Brahman and form that source it got the name Bahmani. In the early 16th century, it collapsed and got split into five small Deccan sultanates. In the East, the Gajapati Kingdom remained a strong regional power to reckon with, so was the Ahom Kingdom in the North-east for six centuries.
Ahom Kingdom (1228–1826) was a kingdom and tribe which rose to prominence in present-day Assam early in the thirteenth century. They ruled much of Assam from the 13th century until the establishment of British rule in 1838. The Ahoms brought with them a tribal religion and a language of their own, however they later merged with the Hindu religion. From thirteenth till seventeenth century, repeated attempts were made by the Muslim rulers of Delhi to invade and subdue Ahoms, however the Ahoms managed to maintain their independence and ruled themselves for nearly 600 years.
In 1526, Babur, a Timurid descendant of Timur and Genghis Khan from Fergana Valley (modern day Uzbekistan), swept across the Khyber Pass and established the Mughal Empire, which at its zenith covered modern day Afghanistan, Pakistan, India and Bangladesh. However, his son Humayun was defeated by the Afghan warrior Sher Shah Suri in the year 1540, and Humayun was forced to retreat to Kabul. After Sher Shah's death, his son Islam Shah Suri and the Hindu emperor Hemu Vikramaditya, who had won 22 battles against Afghan rebels and forces of Akbar, from Punjab to Bengal and had established a secular rule in North India from Delhi till 1556 after winning Battle of Delhi. Akbar's forces defeated and killed Hemu in the Second Battle of Panipat on 6 November 1556.
Akbar's son, Jahangir more or less followed father's policy. The Mughal dynasty ruled most of the Indian subcontinent by 1600. The reign of Shah Jahan was the golden age of Mughal architecture. He erected several large monuments, the most famous of which is the Taj Mahal at Agra, as well as the Moti Masjid, Agra, the Red Fort, the Jama Masjid, Delhi, and the Lahore Fort. The Mughal Empire reached the zenith of its territorial expanse during the reign of Aurangzeb and also started its terminal decline in his reign due to Maratha military resurgence under Shivaji. Historian Sir. J.N. Sarkar wrote, "All seemed to have been gained by Aurangzeb now, but in reality all was lost." The same was echoed by Vincent Smith: "The Deccan proved to be the graveyard not only of Aurangzeb's body but also of his empire".
The empire went into decline thereafter. The Mughals suffered several blows due to invasions from Marathas and Afghans. During the decline of the Mughal Empire, several smaller states rose to fill the power vacuum and themselves were contributing factors to the decline. In 1737, the Maratha general Bajirao of the Maratha Empire invaded and plundered Delhi. Under the general Amir Khan Umrao Al Udat, the Mughal Emperor sent 8,000 troops to drive away the 5,000 Maratha cavalry soldiers. Baji Rao, however, easily routed the novice Mughal general and the rest of the imperial Mughal army fled. In 1737, in the final defeat of Mughal Empire, the commander-in-chief of the Mughal Army, Nizam-ul-mulk, was routed at Bhopal by the Maratha army. This essentially brought an end to the Mughal Empire. In 1739, Nader Shah, emperor of Iran, defeated the Mughal army at the Battle of Karnal. After this victory, Nader captured and sacked Delhi, carrying away many treasures, including the Peacock Throne. The Mughal dynasty was reduced to puppet rulers by 1757. The remnants of the Mughal dynasty were finally defeated during the Indian Rebellion of 1857, also called the 1857 War of Independence, and the remains of the empire were formally taken over by the British while the Government of India Act 1858 let the British Crown assume direct control of India in the form of the new British Raj.
The Mughals were perhaps the richest single dynasty to have ever existed. During the Mughal era, the dominant political forces consisted of the Mughal Empire and its tributaries and, later on, the rising successor states – including the Maratha Empire – which fought an increasingly weak Mughal dynasty. The Mughals, while often employing brutal tactics to subjugate their empire, had a policy of integration with Indian culture, which is what made them successful where the short-lived Sultanates of Delhi had failed. This period marked vast social change in the subcontinent as the Hindu majority were ruled over by the Mughal emperors, most of whom showed religious tolerance, liberally patronising Hindu culture. The famous emperor Akbar, who was the grandson of Babar, tried to establish a good relationship with the Hindus. However, later emperors such as Aurangazeb tried to establish complete Muslim dominance, and as a result several historical temples were destroyed during this period and taxes imposed on non-Muslims. Akbar declared "Amari" or non-killing of animals in the holy days of Jainism. He rolled back the jizya tax for non-Muslims. The Mughal emperors married local royalty, allied themselves with local maharajas, and attempted to fuse their Turko-Persian culture with ancient Indian styles, creating a unique Indo-Saracenic architecture. It was the erosion of this tradition coupled with increased brutality and centralization that played a large part in the dynasty's downfall after Aurangzeb, who unlike previous emperors, imposed relatively non-pluralistic policies on the general population, which often inflamed the majority Hindu population.
The post-Mughal era was dominated by the rise of the Maratha suzerainty as other small regional states (mostly late Mughal tributary states) emerged, and also by the increasing activities of European powers. There is no doubt that the single most important power to emerge in the long twilight of the Mughal dynasty was the Maratha confederacy. The Maratha kingdom was founded and consolidated by Chatrapati Shivaji, a Maratha aristocrat of the Bhonsle clan who was determined to establish Hindavi Swarajya. Sir J.N. Sarkar described Shivaji as "the last great constructive genius and nation builder that the Hindu race has produced". However, the credit for making the Marathas formidable power nationally goes to Peshwa Bajirao I. Historian K.K. Datta wrote about Bajirao I:
By the early 18th century, the Maratha Kingdom had transformed itself into the Maratha Empire under the rule of the Peshwas (prime ministers). In 1737, the Marathas defeated a Mughal army in their capital, Delhi itself in Battle of Delhi (1737). The Marathas continued their military campaigns against Mughals, Nizam, Nawab of Bengal and Durrani Empire to further extend their boundaries. Gordon explained how the Maratha systematically took control over new regions. They would start with annual raids, followed by collecting ransom from villages and towns while the declining Mughal Empire retained nominal control and finally taking over the region. He explained it with the example of Malwa region. Marathas built an efficient system of public administration known for its attention to detail. It succeeded in raising revenue in districts that recovered from years of raids, up to levels previously enjoyed by the Mughals. For example, the cornerstone of the Maratha rule in Malwa rested on the 60 or so local tax collectors who advanced the Maratha ruler Peshwa a portion of their district revenues at interest. By 1760, the domain of the Marathas stretched across practically the entire subcontinent. The north-western expansion of the Marathas was stopped after the Third Battle of Panipat (1761). However, the Maratha authority in the north was re-established within a decade under Peshwa Madhavrao I. The defeat of Marathas by British in third Anglo-Maratha Wars brought end to the empire by 1820. The last peshwa, Baji Rao II, was defeated by the British in the Third Anglo-Maratha War. With the defeat of the Marathas, no native power represented any significant threat for the British afterwards.
The Punjabi kingdom, ruled by members of the Sikh religion, was a political entity that governed the region of modern-day Punjab. The empire, based around the Punjab region, existed from 1799 to 1849. It was forged, on the foundations of the Khalsa, under the leadership of Maharaja Ranjit Singh (1780–1839) from an array of autonomous Punjabi Misls. He consolidated many parts of northern India into a kingdom. He primarily used his highly disciplined Sikh army that he trained and equipped to be the equal of a European force. Ranjit Singh proved himself to be a master strategist and selected well qualified generals for his army. In stages, he added the central Punjab, the provinces of Multan and Kashmir, the Peshawar Valley, and the Derajat to his kingdom. This came in the face of the powerful British East India Company. At its peak, in the 19th century, the empire extended from the Khyber Pass in the west, to Kashmir in the north, to Sindh in the south, running along Sutlej river to Himachal in the east. This was among the last areas of the subcontinent to be conquered by the British. The first Anglo-Sikh war and second Anglo-Sikh war marked the downfall of the Sikh Empire.
There were several other kingdoms which ruled over parts of India in the later medieval period prior to the British occupation. However, most of them were bound to pay regular tribute to the Marathas. The rule of Wodeyar dynasty which established the Kingdom of Mysore in southern India in around 1400 CE by was interrupted by Hyder Ali and his son Tipu Sultan in the later half of the 18th century. Under their rule, Mysore fought a series of wars sometimes against the combined forces of the British and Marathas, but mostly against the British, with Mysore receiving some aid or promise of aid from the French.
The next to arrive were the Dutch, with their main base in Ceylon. The British—who set up a trading post in the west coast port of Surat in 1619—and the French. The internal conflicts among Indian kingdoms gave opportunities to the European traders to gradually establish political influence and appropriate lands. Although these continental European powers controlled various coastal regions of southern and eastern India during the ensuing century, they eventually lost all their territories in India to the British islanders, with the exception of the French outposts of Pondichéry and Chandernagore, the Dutch port of Travancore, and the Portuguese colonies of Goa, Daman and Diu.[citation needed]
The Nawab of Bengal Siraj Ud Daulah, the de facto ruler of the Bengal province, opposed British attempts to use these permits. This led to the Battle of Plassey on 23 June 1757, in which the Bengal Army of the East India Company, led by Robert Clive, defeated the French-supported Nawab's forces. This was the first real political foothold with territorial implications that the British acquired in India. Clive was appointed by the company as its first 'Governor of Bengal' in 1757. This was combined with British victories over the French at Madras, Wandiwash and Pondichéry that, along with wider British successes during the Seven Years' War, reduced French influence in India. The British East India Company extended its control over the whole of Bengal. After the Battle of Buxar in 1764, the company acquired the rights of administration in Bengal from de jure Mughal Emperor Shah Alam II; this marked the beginning of its formal rule, which within the next century engulfed most of India. The East India Company monopolized the trade of Bengal. They introduced a land taxation system called the Permanent Settlement which introduced a feudal-like structure in Bengal, often with zamindars set in place.
As a result of the three Carnatic Wars, the British East India Company gained exclusive control over the entire Carnatic region of India. The Company soon expanded its territories around its bases in Bombay and Madras; the Anglo-Mysore Wars (1766–1799) and later the Anglo-Maratha Wars (1772–1818) led to control of the vast regions of India. Ahom Kingdom of North-east India first fell to Burmese invasion and then to British after Treaty of Yandabo in 1826. Punjab, North-West Frontier Province, and Kashmir were annexed after the Second Anglo-Sikh War in 1849; however, Kashmir was immediately sold under the Treaty of Amritsar to the Dogra Dynasty of Jammu and thereby became a princely state. The border dispute between Nepal and British India, which sharpened after 1801, had caused the Anglo-Nepalese War of 1814–16 and brought the defeated Gurkhas under British influence. In 1854, Berar was annexed, and the state of Oudh was added two years later.
The Indian rebellion of 1857 was a large-scale rebellion by soldiers employed by the British East India in northern and central India against the Company's rule. The rebels were disorganized, had differing goals, and were poorly equipped, led, and trained, and had no outside support or funding. They were brutally suppressed and the British government took control of the Company and eliminated many of the grievances that caused it. The government also was determined to keep full control so that no rebellion of such size would ever happen again.
In the aftermath, all power was transferred from the East India Company to the British Crown, which began to administer most of India as a number of provinces. The Crown controlled the Company's lands directly and had considerable indirect influence over the rest of India, which consisted of the Princely states ruled by local royal families. There were officially 565 princely states in 1947, but only 21 had actual state governments, and only three were large (Mysore, Hyderabad and Kashmir). They were absorbed into the independent nation in 1947–48.
After 1857, the colonial government strengthened and expanded its infrastructure via the court system, legal procedures, and statutes. The Indian Penal Code came into being. In education, Thomas Babington Macaulay had made schooling a priority for the Raj in his famous minute of February 1835 and succeeded in implementing the use of English as the medium of instruction. By 1890 some 60,000 Indians had matriculated. The Indian economy grew at about 1% per year from 1880 to 1920, and the population also grew at 1%. However, from 1910s Indian private industry began to grow significantly. India built a modern railway system in the late 19th century which was the fourth largest in the world. The British Raj invested heavily in infrastructure, including canals and irrigation systems in addition to railways, telegraphy, roads and ports. However, historians have been bitterly divided on issues of economic history, with the Nationalist school arguing that India was poorer at the end of British rule than at the beginning and that impoverishment occurred because of the British.
In 1905, Lord Curzon split the large province of Bengal into a largely Hindu western half and "Eastern Bengal and Assam", a largely Muslim eastern half. The British goal was said to be for efficient administration but the people of Bengal were outraged at the apparent "divide and rule" strategy. It also marked the beginning of the organized anti-colonial movement. When the Liberal party in Britain came to power in 1906, he was removed. Bengal was reunified in 1911. The new Viceroy Gilbert Minto and the new Secretary of State for India John Morley consulted with Congress leaders on political reforms. The Morley-Minto reforms of 1909 provided for Indian membership of the provincial executive councils as well as the Viceroy's executive council. The Imperial Legislative Council was enlarged from 25 to 60 members and separate communal representation for Muslims was established in a dramatic step towards representative and responsible government. Several socio-religious organizations came into being at that time. Muslims set up the All India Muslim League in 1906. It was not a mass party but was designed to protect the interests of the aristocratic Muslims. It was internally divided by conflicting loyalties to Islam, the British, and India, and by distrust of Hindus. The Akhil Bharatiya Hindu Mahasabha and Rashtriya Swayamsevak Sangh (RSS) sought to represent Hindu interests though the later always claimed it to be a "cultural" organization. Sikhs founded the Shiromani Akali Dal in 1920. However, the largest and oldest political party Indian National Congress, founded in 1885, is perceived to have attempted to keep a distance from the socio-religious movements and identity politics.
The Bengali Renaissance refers to a social reform movement during the nineteenth and early twentieth centuries in the Bengal region of India during the period of British rule dominated by English educated Bengali Hindus. The Bengal Renaissance can be said to have started with Raja Ram Mohan Roy (1772–1833) and ended with Rabindranath Tagore (1861–1941), although many stalwarts thereafter continued to embody particular aspects of the unique intellectual and creative output of the region. Nineteenth century Bengal was a unique blend of religious and social reformers, scholars, literary giants, journalists, patriotic orators, and scientists, all merging to form the image of a renaissance, and marked the transition from the 'medieval' to the 'modern'.
During this period, Bengal witnessed an intellectual awakening that is in some way similar to the Renaissance in Europe during the 16th century, although Europeans of that age were not confronted with the challenge and influence of alien colonialism. This movement questioned existing orthodoxies, particularly with respect to women, marriage, the dowry system, the caste system, and religion. One of the earliest social movements that emerged during this time was the Young Bengal movement, which espoused rationalism and atheism as the common denominators of civil conduct among upper caste educated Hindus. It played an important role in reawakening Indian minds and intellect across the sub-continent.
During the British Raj, famines in India, often attributed to failed government policies, were some of the worst ever recorded, including the Great Famine of 1876–78 in which 6.1 million to 10.3 million people died and the Indian famine of 1899–1900 in which 1.25 to 10 million people died. The Third Plague Pandemic in the mid-19th century killed 10 million people in India. Despite persistent diseases and famines, the population of the Indian subcontinent, which stood at about 125 million in 1750, had reached 389 million by 1941.
One of the most important events of the 19th century was the rise of Indian nationalism, leading Indians to seek first "self-rule" and later "complete independence". However, historians are divided over the causes of its rise. Probable reasons include a "clash of interests of the Indian people with British interests", "racial discriminations", "the revelation of India's past", "inter-linking of the new social groups in different regions", and Indians coming in close contact with "European education".
The first step toward Indian self-rule was the appointment of councillors to advise the British viceroy in 1861 and the first Indian was appointed in 1909. Provincial Councils with Indian members were also set up. The councillors' participation was subsequently widened into legislative councils. The British built a large British Indian Army, with the senior officers all British and many of the troops from small minority groups such as Gurkhas from Nepal and Sikhs. The civil service was increasingly filled with natives at the lower levels, with the British holding the more senior positions.
Bal Gangadhar Tilak, an Indian nationalist leader, declared Swaraj as the destiny of the nation. His popular sentence "Swaraj is my birthright, and I shall have it" became the source of inspiration for Indians. Tilak was backed by rising public leaders like Bipin Chandra Pal and Lala Lajpat Rai, who held the same point of view. Under them, India's three big provinces – Maharashtra, Bengal and Punjab, India shaped the demand of the people and India's nationalism. In 1907, the Congress was split into two factions: The radicals, led by Tilak, advocated civil agitation and direct revolution to overthrow the British Empire and the abandonment of all things British. The moderates, led by leaders like Dadabhai Naoroji and Gopal Krishna Gokhale, on the other hand wanted reform within the framework of British rule.
From 1920 leaders such as Mahatma Gandhi began highly popular mass movements to campaign against the British Raj using largely peaceful methods. The Gandhi-led independence movement opposed the British rule using non-violent methods like non-cooperation, civil disobedience and economic resistance. However, revolutionary activities against the British rule took place throughout the Indian subcontinent and some others adopted a militant approach like the Indian National Army that sought to overthrow British rule by armed struggle. The Government of India Act 1935 was a major success in this regard. All these movements succeeded in bringing independence to the new dominions of India and Pakistan on 15 August 1947.
Along with the desire for independence, tensions between Hindus and Muslims had also been developing over the years. The Muslims had always been a minority within the subcontinent, and the prospect of an exclusively Hindu government made them wary of independence; they were as inclined to mistrust Hindu rule as they were to resist the foreign Raj, although Gandhi called for unity between the two groups in an astonishing display of leadership. The British, extremely weakened by the Second World War, promised that they would leave and participated in the formation of an interim government. The British Indian territories gained independence in 1947, after being partitioned into the Union of India and Dominion of Pakistan. Following the controversial division of pre-partition Punjab and Bengal, rioting broke out between Sikhs, Hindus and Muslims in these provinces and spread to several other parts of India, leaving some 500,000 dead. Also, this period saw one of the largest mass migrations ever recorded in modern history, with a total of 12 million Hindus, Sikhs and Muslims moving between the newly created nations of India and Pakistan (which gained independence on 15 and 14 August 1947 respectively). In 1971, Bangladesh, formerly East Pakistan and East Bengal, seceded from Pakistan.
Chinese political philosophy dates back to the Spring and Autumn Period, specifically with Confucius in the 6th century BC. Chinese political philosophy was developed as a response to the social and political breakdown of the country characteristic of the Spring and Autumn Period and the Warring States period. The major philosophies during the period, Confucianism, Legalism, Mohism, Agrarianism and Taoism, each had a political aspect to their philosophical schools. Philosophers such as Confucius, Mencius, and Mozi, focused on political unity and political stability as the basis of their political philosophies. Confucianism advocated a hierarchical, meritocratic government based on empathy, loyalty, and interpersonal relationships. Legalism advocated a highly authoritarian government based on draconian punishments and laws. Mohism advocated a communal, decentralized government centered on frugality and ascetism. The Agrarians advocated a peasant utopian communalism and egalitarianism. Taoism advocated a proto-anarchism. Legalism was the dominant political philosophy of the Qin Dynasty, but was replaced by State Confucianism in the Han Dynasty. Prior to China's adoption of communism, State Confucianism remained the dominant political philosophy of China up to the 20th century.
Western political philosophy originates in the philosophy of ancient Greece, where political philosophy dates back to at least Plato. Ancient Greece was dominated by city-states, which experimented with various forms of political organization, grouped by Plato into four categories: timocracy, tyranny, democracy and oligarchy. One of the first, extremely important classical works of political philosophy is Plato's Republic, which was followed by Aristotle's Nichomachean Ethics and Politics. Roman political philosophy was influenced by the Stoics, including the Roman statesman Cicero.
Indian political philosophy evolved in ancient times and demarcated a clear distinction between (1) nation and state (2) religion and state. The constitutions of Hindu states evolved over time and were based on political and legal treatises and prevalent social institutions. The institutions of state were broadly divided into governance, administration, defense, law and order. Mantranga, the principal governing body of these states, consisted of the King, Prime Minister, Commander in chief of army, Chief Priest of the King. The Prime Minister headed the committee of ministers along with head of executive (Maha Amatya).
Chanakya, 4th Century BC Indian political philosopher. The Arthashastra provides an account of the science of politics for a wise ruler, policies for foreign affairs and wars, the system of a spy state and surveillance and economic stability of the state. Chanakya quotes several authorities including Bruhaspati, Ushanas, Prachetasa Manu, Parasara, and Ambi, and described himself as a descendant of a lineage of political philosophers, with his father Chanaka being his immediate predecessor. Another influential extant Indian treatise on political philosophy is the Sukra Neeti. An example of a code of law in ancient India is the Manusmṛti or Laws of Manu.
The early Christian philosophy of Augustine of Hippo was heavily influenced by Plato. A key change brought about by Christian thought was the moderatation of the Stoicism and theory of justice of the Roman world, as well emphasis on the role of the state in applying mercy as a moral example. Augustine also preached that one was not a member of his or her city, but was either a citizen of the City of God (Civitas Dei) or the City of Man (Civitas Terrena). Augustine's City of God is an influential work of this period that attacked the thesis, held by many Christian Romans, that the Christian view could be realized on Earth.
The rise of Islam, based on both the Qur'an and Muhammad strongly altered the power balances and perceptions of origin of power in the Mediterranean region. Early Islamic philosophy emphasized an inexorable link between science and religion, and the process of ijtihad to find truth—in effect all philosophy was "political" as it had real implications for governance. This view was challenged by the "rationalist" Mutazilite philosophers, who held a more Hellenic view, reason above revelation, and as such are known to modern scholars as the first speculative theologians of Islam; they were supported by a secular aristocracy who sought freedom of action independent of the Caliphate. By the late ancient period, however, the "traditionalist" Asharite view of Islam had in general triumphed. According to the Asharites, reason must be subordinate to the Quran and the Sunna.
Islamic political philosophy, was, indeed, rooted in the very sources of Islam—i.e., the Qur'an and the Sunnah, the words and practices of Muhammad—thus making it essentially theocratic. However, in the Western thought, it is generally supposed that it was a specific area peculiar merely to the great philosophers of Islam: al-Kindi (Alkindus), al-Farabi (Abunaser), İbn Sina (Avicenna), Ibn Bajjah (Avempace), Ibn Rushd (Averroes), and Ibn Khaldun. The political conceptions of Islam such as kudrah (power), sultan, ummah, cemaa (obligation)-and even the "core" terms of the Qur'an—i.e., ibadah (worship), din (religion), rab (master) and ilah (deity)—is taken as the basis of an analysis. Hence, not only the ideas of the Muslim political philosophers but also many other jurists and ulama posed political ideas and theories. For example, the ideas of the Khawarij in the very early years of Islamic history on Khilafa and Ummah, or that of Shia Islam on the concept of Imamah are considered proofs of political thought. The clashes between the Ehl-i Sunna and Shia in the 7th and 8th centuries had a genuine political character.
Medieval political philosophy in Europe was heavily influenced by Christian thinking. It had much in common with the Mutazalite Islamic thinking in that the Roman Catholics though subordinating philosophy to theology did not subject reason to revelation but in the case of contradictions, subordinated reason to faith as the Asharite of Islam. The Scholastics by combining the philosophy of Aristotle with the Christianity of St. Augustine emphasized the potential harmony inherent in reason and revelation. Perhaps the most influential political philosopher of medieval Europe was St. Thomas Aquinas who helped reintroduce Aristotle's works, which had only been transmitted to Catholic Europe through Muslim Spain, along with the commentaries of Averroes. Aquinas's use of them set the agenda, for scholastic political philosophy dominated European thought for centuries even unto the Renaissance.
One of the most influential works during this burgeoning period was Niccolò Machiavelli's The Prince, written between 1511–12 and published in 1532, after Machiavelli's death. That work, as well as The Discourses, a rigorous analysis of the classical period, did much to influence modern political thought in the West. A minority (including Jean-Jacques Rousseau) interpreted The Prince as a satire meant to be given to the Medici after their recapture of Florence and their subsequent expulsion of Machiavelli from Florence. Though the work was written for the di Medici family in order to perhaps influence them to free him from exile, Machiavelli supported the Republic of Florence rather than the oligarchy of the di Medici family. At any rate, Machiavelli presents a pragmatic and somewhat consequentialist view of politics, whereby good and evil are mere means used to bring about an end—i.e., the secure and powerful state. Thomas Hobbes, well known for his theory of the social contract, goes on to expand this view at the start of the 17th century during the English Renaissance. Although neither Machiavelli nor Hobbes believed in the divine right of kings, they both believed in the inherent selfishness of the individual. It was necessarily this belief that led them to adopt a strong central power as the only means of preventing the disintegration of the social order.
These theorists were driven by two basic questions: one, by what right or need do people form states; and two, what the best form for a state could be. These fundamental questions involved a conceptual distinction between the concepts of "state" and "government." It was decided that "state" would refer to a set of enduring institutions through which power would be distributed and its use justified. The term "government" would refer to a specific group of people who occupied the institutions of the state, and create the laws and ordinances by which the people, themselves included, would be bound. This conceptual distinction continues to operate in political science, although some political scientists, philosophers, historians and cultural anthropologists have argued that most political action in any given society occurs outside of its state, and that there are societies that are not organized into states that nevertheless must be considered in political terms. As long as the concept of natural order was not introduced, the social sciences could not evolve independently of theistic thinking. Since the cultural revolution of the 17th century in England, which spread to France and the rest of Europe, society has been considered subject to natural laws akin to the physical world.
Political and economic relations were drastically influenced by these theories as the concept of the guild was subordinated to the theory of free trade, and Roman Catholic dominance of theology was increasingly challenged by Protestant churches subordinate to each nation-state, which also (in a fashion the Roman Catholic Church often decried angrily) preached in the vulgar or native language of each region. However, the enlightenment was an outright attack on religion, particularly Christianity. The most outspoken critic of the church in France was François Marie Arouet de Voltaire, a representative figure of the enlightenment. After Voltaire, religion would never be the same again in France.
In the Ottoman Empire, these ideological reforms did not take place and these views did not integrate into common thought until much later. As well, there was no spread of this doctrine within the New World and the advanced civilizations of the Aztec, Maya, Inca, Mohican, Delaware, Huron and especially the Iroquois. The Iroquois philosophy in particular gave much to Christian thought of the time and in many cases actually inspired some of the institutions adopted in the United States: for example, Benjamin Franklin was a great admirer of some of the methods of the Iroquois Confederacy, and much of early American literature emphasized the political philosophy of the natives.
John Locke in particular exemplified this new age of political theory with his work Two Treatises of Government. In it Locke proposes a state of nature theory that directly complements his conception of how political development occurs and how it can be founded through contractual obligation. Locke stood to refute Sir Robert Filmer's paternally founded political theory in favor of a natural system based on nature in a particular given system. The theory of the divine right of kings became a passing fancy, exposed to the type of ridicule with which John Locke treated it. Unlike Machiavelli and Hobbes but like Aquinas, Locke would accept Aristotle's dictum that man seeks to be happy in a state of social harmony as a social animal. Unlike Aquinas's preponderant view on the salvation of the soul from original sin, Locke believes man's mind comes into this world as tabula rasa. For Locke, knowledge is neither innate, revealed nor based on authority but subject to uncertainty tempered by reason, tolerance and moderation. According to Locke, an absolute ruler as proposed by Hobbes is unnecessary, for natural law is based on reason and seeking peace and survival for man.
The Marxist critique of capitalism — developed with Friedrich Engels — was, alongside liberalism and fascism, one of the defining ideological movements of the Twentieth Century. The industrial revolution produced a parallel revolution in political thought. Urbanization and capitalism greatly reshaped society. During this same period, the socialist movement began to form. In the mid-19th century, Marxism was developed, and socialism in general gained increasing popular support, mostly from the urban working class. Without breaking entirely from the past, Marx established principles that would be used by future revolutionaries of the 20th century namely Vladimir Lenin, Mao Zedong, Ho Chi Minh, and Fidel Castro. Though Hegel's philosophy of history is similar to Immanuel Kant's, and Karl Marx's theory of revolution towards the common good is partly based on Kant's view of history—Marx declared that he was turning Hegel's dialectic, which was "standing on its head", "the right side up again". Unlike Marx who believed in historical materialism, Hegel believed in the Phenomenology of Spirit. By the late 19th century, socialism and trade unions were established members of the political landscape. In addition, the various branches of anarchism, with thinkers such as Mikhail Bakunin, Pierre-Joseph Proudhon or Peter Kropotkin, and syndicalism also gained some prominence. In the Anglo-American world, anti-imperialism and pluralism began gaining currency at the turn of the 20th century.
World War I was a watershed event in human history, changing views of governments and politics. The Russian Revolution of 1917 (and similar, albeit less successful, revolutions in many other European countries) brought communism - and in particular the political theory of Leninism, but also on a smaller level Luxemburgism (gradually) - on the world stage. At the same time, social democratic parties won elections and formed governments for the first time, often as a result of the introduction of universal suffrage. However, a group of central European economists led by Austrian School economists Ludwig von Mises and Friedrich Hayek identified the collectivist underpinnings to the various new socialist and fascist doctrines of government power as being different brands of political totalitarianism.
From the end of World War II until 1971, when John Rawls published A Theory of Justice, political philosophy declined in the Anglo-American academic world, as analytic philosophers expressed skepticism about the possibility that normative judgments had cognitive content, and political science turned toward statistical methods and behavioralism. In continental Europe, on the other hand, the postwar decades saw a huge blossoming of political philosophy, with Marxism dominating the field. This was the time of Jean-Paul Sartre and Louis Althusser, and the victories of Mao Zedong in China and Fidel Castro in Cuba, as well as the events of May 1968 led to increased interest in revolutionary ideology, especially by the New Left. A number of continental European émigrés to Britain and the United States—including Karl Popper, Friedrich Hayek, Leo Strauss, Isaiah Berlin, Eric Voegelin and Judith Shklar—encouraged continued study in political philosophy in the Anglo-American world, but in the 1950s and 1960s they and their students remained at odds with the analytic establishment.
Communism remained an important focus especially during the 1950s and 1960s. Colonialism and racism were important issues that arose. In general, there was a marked trend towards a pragmatic approach to political issues, rather than a philosophical one. Much academic debate regarded one or both of two pragmatic topics: how (or whether) to apply utilitarianism to problems of political policy, or how (or whether) to apply economic models (such as rational choice theory) to political issues. The rise of feminism, LGBT social movements and the end of colonial rule and of the political exclusion of such minorities as African Americans and sexual minorities in the developed world has led to feminist, postcolonial, and multicultural thought becoming significant. This led to a challenge to the social contract by philosophers Charles W. Mills in his book The Racial Contract and Carole Patemen in her book The Sexual Contract that the social contract excluded persons of colour and women respectively.
In Anglo-American academic political philosophy, the publication of John Rawls's A Theory of Justice in 1971 is considered a milestone. Rawls used a thought experiment, the original position, in which representative parties choose principles of justice for the basic structure of society from behind a veil of ignorance. Rawls also offered a criticism of utilitarian approaches to questions of political justice. Robert Nozick's 1974 book Anarchy, State, and Utopia, which won a National Book Award, responded to Rawls from a libertarian perspective and gained academic respectability for libertarian viewpoints.
Contemporaneously with the rise of analytic ethics in Anglo-American thought, in Europe several new lines of philosophy directed at critique of existing societies arose between the 1950s and 1980s. Most of these took elements of Marxist economic analysis, but combined them with a more cultural or ideological emphasis. Out of the Frankfurt School, thinkers like Herbert Marcuse, Theodor W. Adorno, Max Horkheimer, and Jürgen Habermas combined Marxian and Freudian perspectives. Along somewhat different lines, a number of other continental thinkers—still largely influenced by Marxism—put new emphases on structuralism and on a "return to Hegel". Within the (post-) structuralist line (though mostly not taking that label) are thinkers such as Gilles Deleuze, Michel Foucault, Claude Lefort, and Jean Baudrillard. The Situationists were more influenced by Hegel; Guy Debord, in particular, moved a Marxist analysis of commodity fetishism to the realm of consumption, and looked at the relation between consumerism and dominant ideology formation.
Another debate developed around the (distinct) criticisms of liberal political theory made by Michael Walzer, Michael Sandel and Charles Taylor. The liberal-communitarian debate is often considered valuable for generating a new set of philosophical problems, rather than a profound and illuminating clash of perspective.These and other communitarians (such as Alasdair MacIntyre and Daniel A. Bell) argue that, contra liberalism, communities are prior to individuals and therefore should be the center of political focus. Communitarians tend to support greater local control as well as economic and social policies which encourage the growth of social capital.
A pair of overlapping political perspectives arising toward the end of the 20th century are republicanism (or neo- or civic-republicanism) and the capability approach. The resurgent republican movement aims to provide an alternate definition of liberty from Isaiah Berlin's positive and negative forms of liberty, namely "liberty as non-domination." Unlike liberals who understand liberty as "non-interference," "non-domination" entails individuals not being subject to the arbitrary will of anyother person. To a liberal, a slave who is not interfered with may be free, yet to a republican the mere status as a slave, regardless of how that slave is treated, is objectionable. Prominent republicans include historian Quentin Skinner, jurist Cass Sunstein, and political philosopher Philip Pettit. The capability approach, pioneered by economists Mahbub ul Haq and Amartya Sen and further developed by legal scholar Martha Nussbaum, understands freedom under allied lines: the real-world ability to act. Both the capability approach and republicanism treat choice as something which must be resourced. In other words, it is not enough to be legally able to do something, but to have the real option of doing it.
The Republic of Liberia, beginning as a settlement of the American Colonization Society (ACS), declared its independence on July 26, 1847. The United States did not recognize Liberia's independence until during the American Civil War on February 5, 1862. Between January 7, 1822 and the American Civil War, more than 15,000 freed and free-born Black Americans from United States and 3,198 Afro-Caribbeans relocated to the settlement. The Black American settlers carried their culture with them to Liberia. The Liberian constitution and flag were modeled after the United States. In January 3, 1848 Joseph Jenkins Roberts, a wealthy free-born Black American from Virginia who settled in Liberia, was elected as Liberia's first president after the people proclaimed independence.
Longstanding political tensions from the 27 year rule of William Tubman resulted in a military coup in 1980 that overthrew the leadership soon after his death, marking the beginning of political instability. Five years of military rule by the People's Redemption Council and five years of civilian rule by the National Democratic Party of Liberia were followed by the First and Second Liberian Civil Wars. These resulted in the deaths and displacement of more than half a million people and devastated Liberia's economy. A peace agreement in 2003 led to democratic elections in 2005. Recovery proceeds but about 85% of the population live below the international poverty line.
This influx was compounded by the decline of the Western Sudanic Mali Empire in 1375 and the Songhai Empire in 1591. Additionally, as inland regions underwent desertification, inhabitants moved to the wetter coast. These new inhabitants brought skills such as cotton spinning, cloth weaving, iron smelting, rice and sorghum cultivation, and social and political institutions from the Mali and Songhai empires. Shortly after the Mane conquered the region, the Vai people of the former Mali Empire immigrated into the Grand Cape Mount region. The ethnic Kru opposed the influx of Vai, forming an alliance with the Mane to stop further influx of Vai.[citation needed]
In the United States, there was a movement to resettle American free blacks and freed slaves in Africa. The American Colonization Society was founded in 1816 in Washington, DC for this purpose, by a group of prominent politicians and slaveholders. But its membership grew to include mostly people who supported abolition of slavery. Slaveholders wanted to get free people of color out of the South, where they were thought to threaten the stability of the slave societies. Some abolitionists collaborated on relocation of free blacks, as they were discouraged by discrimination against them in the North and believed they would never be accepted in the larger society. Most African Americans, who were native-born by this time, wanted to improve conditions in the United States rather than emigrate. Leading activists in the North strongly opposed the ACS, but some free blacks were ready to try a different environment.
In 1822, the American Colonization Society began sending African-American volunteers to the Pepper Coast to establish a colony for freed African Americans. By 1867, the ACS (and state-related chapters) had assisted in the migration of more than 13,000 African Americans to Liberia. These free African Americans and their descendants married within their community and came to identify as Americo-Liberians. Many were of mixed race and educated in American culture; they did not identify with the indigenous natives of the tribes they encountered. They intermarried largely within the colonial community, developing an ethnic group that had a cultural tradition infused with American notions of political republicanism and Protestant Christianity.
The Americo-Liberian settlers did not identify with the indigenous peoples they encountered, especially those in communities of the more isolated "bush." They knew nothing of their cultures, languages or animist religion. Encounters with tribal Africans in the bush often developed as violent confrontations. The colonial settlements were raided by the Kru and Grebo people from their inland chiefdoms. Because of feeling set apart and superior by their culture and education to the indigenous peoples, the Americo-Liberians developed as a small elite that held on to political power. It excluded the indigenous tribesmen from birthright citizenship in their own lands until 1904, in a repetition of the United States' treatment of Native Americans. Because of the cultural gap between the groups and assumption of superiority of western culture, the Americo-Liberians envisioned creating a western-style state to which the tribesmen should assimilate. They encouraged religious organizations to set up missions and schools to educate the indigenous peoples.
On April 12, 1980, a military coup led by Master Sergeant Samuel Doe of the Krahn ethnic group overthrew and killed President William R. Tolbert, Jr.. Doe and the other plotters later executed a majority of Tolbert's cabinet and other Americo-Liberian government officials and True Whig Party members. The coup leaders formed the People's Redemption Council (PRC) to govern the country. A strategic Cold War ally of the West, Doe received significant financial backing from the United States while critics condemned the PRC for corruption and political repression.
The rebels soon split into various factions fighting one another. The Economic Community Monitoring Group under the Economic Community of West African States organized a military task force to intervene in the crisis. From 1989 to 1996 one of Africa's bloodiest civil wars ensued, claiming the lives of more than 200,000 Liberians and displacing a million others into refugee camps in neighboring countries. A peace deal between warring parties was reached in 1995, leading to Taylor's election as president in 1997.
In March 2003, a second rebel group, Movement for Democracy in Liberia, began launching attacks against Taylor from the southeast. Peace talks between the factions began in Accra in June of that year, and Taylor was indicted by the Special Court for Sierra Leone for crimes against humanity that same month. By July 2003, the rebels had launched an assault on Monrovia. Under heavy pressure from the international community and the domestic Women of Liberia Mass Action for Peace movement, Taylor resigned in August 2003 and went into exile in Nigeria.
The subsequent 2005 elections were internationally regarded as the most free and fair in Liberian history. Ellen Johnson Sirleaf, a Harvard-trained economist and former Minister of Finance, was elected as the first female president in Africa. Upon her inauguration, Sirleaf requested the extradition of Taylor from Nigeria and transferred him to the SCSL for trial in The Hague. In 2006, the government established a Truth and Reconciliation Commission to address the causes and crimes of the civil war.
Liberia is divided into fifteen counties, which, in turn, are subdivided into a total of 90 districts and further subdivided into clans. The oldest counties are Grand Bassa and Montserrado, both founded in 1839 prior to Liberian independence. Gbarpolu is the newest county, created in 2001. Nimba is the largest of the counties in size at 11,551 km2 (4,460 sq mi), while Montserrado is the smallest at 1,909 km2 (737 sq mi). Montserrado is also the most populous county with 1,144,806 residents as of the 2008 census.
The Legislature is composed of the Senate and the House of Representatives. The House, led by a speaker, has 73 members apportioned among the 15 counties on the basis of the national census, with each county receiving a minimum of two members. Each House member represents an electoral district within a county as drawn by the National Elections Commission and is elected by a plurality of the popular vote of their district into a six-year term. The Senate is made up of two senators from each county for a total of 30 senators. Senators serve nine-year terms and are elected at-large by a plurality of the popular vote. The vice president serves as the President of the Senate, with a President pro tempore serving in their absence.
Liberia's highest judicial authority is the Supreme Court, made up of five members and headed by the Chief Justice of Liberia. Members are nominated to the court by the president and are confirmed by the Senate, serving until the age of 70. The judiciary is further divided into circuit and speciality courts, magistrate courts and justices of the peace. The judicial system is a blend of common law, based on Anglo-American law, and customary law. An informal system of traditional courts still exists within the rural areas of the country, with trial by ordeal remaining common despite being officially outlawed.
Liberia scored a 3.3 on a scale from 10 (highly clean) to 0 (highly corrupt) on the 2010 Corruption Perceptions Index. This gave it a ranking 87th of 178 countries worldwide and 11th of 47 in Sub-Saharan Africa. This score represented a significant improvement since 2007, when the country scored 2.1 and ranked 150th of 180 countries. When seeking attention of a selection of service[clarification needed] providers, 89% of Liberians had to pay a bribe, the highest national percentage in the world according to the organization's 2010 Global Corruption Barometer.
The Central Bank of Liberia is responsible for printing and maintaining the Liberian dollar, which is the primary form of currency in Liberia. Liberia is one of the world's poorest countries, with a formal employment rate of 15%. GDP per capita peaked in 1980 at US$496, when it was comparable to Egypt's (at the time). In 2011, the country's nominal GDP was US$1.154 billion, while nominal GDP per capita stood at US$297, the third-lowest in the world. Historically, the Liberian economy has depended heavily on foreign aid, foreign direct investment and exports of natural resources such as iron ore, rubber and timber.
Following a peak in growth in 1979, the Liberian economy began a steady decline due to economic mismanagement following the 1980 coup. This decline was accelerated by the outbreak of civil war in 1989; GDP was reduced by an estimated 90% between 1989 and 1995, one of the fastest declines in history. Upon the end of the war in 2003, GDP growth began to accelerate, reaching 9.4% in 2007. The global financial crisis slowed GDP growth to 4.6% in 2009, though a strengthening agricultural sector led by rubber and timber exports increased growth to 5.1% in 2010 and an expected 7.3% in 2011, making the economy one of the 20 fastest growing in the world.
In 2003, additional UN sanctions were placed on Liberian timber exports, which had risen from US$5 million in 1997 to over US$100 million in 2002 and were believed to be funding rebels in Sierra Leone. These sanctions were lifted in 2006. Due in large part to foreign aid and investment inflow following the end of the war, Liberia maintains a large account deficit, which peaked at nearly 60% in 2008. Liberia gained observer status with the World Trade Organization in 2010 and is in the process of acquiring full member status.
Liberia has the highest ratio of foreign direct investment to GDP in the world, with US$16 billion in investment since 2006. Following the inauguration of the Sirleaf administration in 2006, Liberia signed several multibillion-dollar concession agreements in the iron ore and palm oil industries with numerous multinational corporations, including BHP Billiton, ArcelorMittal, and Sime Darby. Especially palm oil companies like Sime Darby (Malaysia) and Golden Veroleum (USA) are being accused by critics of the destruction of livelihoods and the displacement of local communities, enabled through government concessions. The Firestone Tire and Rubber Company has operated the world's largest rubber plantation in Liberia since 1926.
The Kpelle comprise more than 20% of the population and are the largest ethnic group in Liberia, residing mostly in Bong County and adjacent areas in central Liberia. Americo-Liberians, who are descendants of African American and West Indian, mostly Barbadian settlers, make up 2.5%. Congo people, descendants of repatriated Congo and Afro-Caribbean slaves who arrived in 1825, make up an estimated 2.5%. These latter two groups established political control in the 19th century which they kept well into the 20th century.
Numerous immigrants have come as merchants and become a major part of the business community, including Lebanese, Indians, and other West African nationals. There is a high percentage of interracial marriage between ethnic Liberians and the Lebanese, resulting in a significant mixed-race population especially in and around Monrovia. A small minority of Liberians of European descent reside in the country.[better source needed] The Liberian constitution restricts citizenship to people of Black African descent.
In 2010, the literacy rate of Liberia was estimated at 60.8% (64.8% for males and 56.8% for females). In some areas primary and secondary education is free and compulsory from the ages of 6 to 16, though enforcement of attendance is lax. In other areas children are required to pay a tuition fee to attend school. On average, children attain 10 years of education (11 for boys and 8 for girls). The country's education sector is hampered by inadequate schools and supplies, as well as a lack of qualified teachers.
Hospitals in Liberia include the John F. Kennedy Medical Center in Monrovia and several others. Life expectancy in Liberia is estimated to be 57.4 years in 2012. With a fertility rate of 5.9 births per woman, the maternal mortality rate stood at 990 per 100,000 births in 2010. A number of highly communicable diseases are widespread, including tuberculosis, diarrheal diseases and malaria. In 2007, the HIV infection rates stood at 2% of the population aged 15–49  whereas the incidence of tuberculosis was 420 per 100,000 people in 2008. Approximately 58.2% – 66% of women are estimated to have undergone female genital mutilation.
Liberia has a long, rich history in textile arts and quilting, as the settlers brought with them their sewing and quilting skills. Liberia hosted National Fairs in 1857 and 1858 in which prizes were awarded for various needle arts. One of the most well-known Liberian quilters was Martha Ann Ricks, who presented a quilt featuring the famed Liberian coffee tree to Queen Victoria in 1892. When President Ellen Johnson Sirleaf moved into the Executive Mansion, she reportedly had a Liberian-made quilt installed in her presidential office.
The Times is a British daily national newspaper based in London. It began in 1785 under the title The Daily Universal Register and became The Times on 1 January 1788. The Times and its sister paper The Sunday Times (founded in 1821) are published by Times Newspapers, since 1981 a subsidiary of News UK, itself wholly owned by the News Corp group headed by Rupert Murdoch. The Times and The Sunday Times do not share editorial staff, were founded independently and have only had common ownership since 1967.
The Times is the first newspaper to have borne that name, lending it to numerous other papers around the world, including The Times of India (founded in 1838), The Straits Times (Singapore) (1845), The New York Times (1851), The Irish Times (1859), Le Temps (France) (1861-1942), the Cape Times (South Africa) (1872), the Los Angeles Times (1881), The Seattle Times (1891), The Manila Times (1898), The Daily Times (Malawi) (1900), El Tiempo (Colombia) (1911), The Canberra Times (1926), and The Times (Malta) (1935). In these countries, the newspaper is often referred to as The London Times or The Times of London.
The Times is the originator of the widely used Times Roman typeface, originally developed by Stanley Morison of The Times in collaboration with the Monotype Corporation for its legibility in low-tech printing. In November 2006 The Times began printing headlines in a new font, Times Modern. The Times was printed in broadsheet format for 219 years, but switched to compact size in 2004 in an attempt to appeal more to younger readers and commuters using public transport. The Sunday Times remains a broadsheet.
Though traditionally a moderate newspaper and sometimes a supporter of the Conservative Party, it supported the Labour Party in the 2001 and 2005 general elections. In 2004, according to MORI, the voting intentions of its readership were 40% for the Conservative Party, 29% for the Liberal Democrats, and 26% for Labour. The Times had an average daily circulation of 394,448 in March 2014; in the same period, The Sunday Times had an average daily circulation of 839,077. An American edition of The Times has been published since 6 June 2006. It has been heavily used by scholars and researchers because of its widespread availability in libraries and its detailed index. A complete historical file of the digitized paper is online from Gage Cengage publisher.
The Times was founded by publisher John Walter on 1 January 1785 as The Daily Universal Register, with Walter in the role of editor. Walter had lost his job by the end of 1784 after the insurance company where he was working went bankrupt because of the complaints of a Jamaican hurricane. Being unemployed, Walter decided to set a new business up. It was in that time when Henry Johnson invented the logography, a new typography that was faster and more precise (three years later, it was proved that it was not as efficient as had been said). Walter bought the logography's patent and to use it, he decided to open a printing house, where he would daily produce an advertising sheet. The first publication of the newspaper The Daily Universal Register in Great Britain was 1 January 1785. Unhappy because people always omitted the word Universal, Ellias changed the title after 940 editions on 1 January 1788 to The Times. In 1803, Walter handed ownership and editorship to his son of the same name. Walter Sr had spent sixteen months in Newgate Prison for libel printed in The Times, but his pioneering efforts to obtain Continental news, especially from France, helped build the paper's reputation among policy makers and financiers.
The Times used contributions from significant figures in the fields of politics, science, literature, and the arts to build its reputation. For much of its early life, the profits of The Times were very large and the competition minimal, so it could pay far better than its rivals for information or writers. Beginning in 1814, the paper was printed on the new steam-driven cylinder press developed by Friedrich Koenig. In 1815, The Times had a circulation of 5,000.
Thomas Barnes was appointed general editor in 1817. In the same year, the paper's printer James Lawson, died and passed the business onto his son John Joseph Lawson(1802–1852). Under the editorship of Barnes and his successor in 1841, John Thadeus Delane, the influence of The Times rose to great heights, especially in politics and amongst the City of London. Peter Fraser and Edward Sterling were two noted journalists, and gained for The Times the pompous/satirical nickname 'The Thunderer' (from "We thundered out the other day an article on social and political reform."). The increased circulation and influence of the paper was based in part to its early adoption of the steam-driven rotary printing press. Distribution via steam trains to rapidly growing concentrations of urban populations helped ensure the profitability of the paper and its growing influence.
The Times was the first newspaper to send war correspondents to cover particular conflicts. W. H. Russell, the paper's correspondent with the army in the Crimean War, was immensely influential with his dispatches back to England.
In other events of the nineteenth century, The Times opposed the repeal of the Corn Laws until the number of demonstrations convinced the editorial board otherwise, and only reluctantly supported aid to victims of the Irish Potato Famine. It enthusiastically supported the Great Reform Bill of 1832, which reduced corruption and increased the electorate from 400,000 people to 800,000 people (still a small minority of the population). During the American Civil War, The Times represented the view of the wealthy classes, favouring the secessionists, but it was not a supporter of slavery.
The third John Walter, the founder's grandson, succeeded his father in 1847. The paper continued as more or less independent, but from the 1850s The Times was beginning to suffer from the rise in competition from the penny press, notably The Daily Telegraph and The Morning Post.
During the 19th century, it was not infrequent for the Foreign Office to approach The Times and ask for continental intelligence, which was often superior to that conveyed by official sources.[citation needed]
The Times faced financial extinction in 1890 under Arthur Fraser Walter, but it was rescued by an energetic editor, Charles Frederic Moberly Bell. During his tenure (1890–1911), The Times became associated with selling the Encyclopædia Britannica using aggressive American marketing methods introduced by Horace Everett Hooper and his advertising executive, Henry Haxton. Due to legal fights between the Britannica's two owners, Hooper and Walter Montgomery Jackson, The Times severed its connection in 1908 and was bought by pioneering newspaper magnate, Alfred Harmsworth, later Lord Northcliffe.
In editorials published on 29 and 31 July 1914, Wickham Steed, the Times's Chief Editor, argued that the British Empire should enter World War I. On 8 May 1920, also under the editorship of Steed, The Times in an editorial endorsed the anti-Semitic fabrication The Protocols of the Learned Elders of Zion as a genuine document, and called Jews the world's greatest danger. In the leader entitled "The Jewish Peril, a Disturbing Pamphlet: Call for Inquiry", Steed wrote about The Protocols of the Elders of Zion:
The following year, when Philip Graves, the Constantinople (modern Istanbul) correspondent of The Times, exposed The Protocols as a forgery, The Times retracted the editorial of the previous year.
In 1922, John Jacob Astor, son of the 1st Viscount Astor, bought The Times from the Northcliffe estate. The paper gained a measure of notoriety in the 1930s with its advocacy of German appeasement; then-editor Geoffrey Dawson was closely allied with those in the government who practised appeasement, most notably Neville Chamberlain.
Kim Philby, a Soviet double agent, was a correspondent for the newspaper in Spain during the Spanish Civil War of the late 1930s. Philby was admired for his courage in obtaining high-quality reporting from the front lines of the bloody conflict. He later joined MI6 during World War II, was promoted into senior positions after the war ended, then eventually defected to the Soviet Union in 1963.
Between 1941 and 1946, the left-wing British historian E.H. Carr was Assistant Editor. Carr was well known for the strongly pro-Soviet tone of his editorials. In December 1944, when fighting broke out in Athens between the Greek Communist ELAS and the British Army, Carr in a Times editorial sided with the Communists, leading Winston Churchill to condemn him and that leader in a speech to the House of Commons. As a result of Carr's editorial, The Times became popularly known during that stage of World War II as the threepenny Daily Worker (the price of the Daily Worker being one penny).
On 3 May 1966 it resumed printing news on the front page - previously the front page featured small advertisements, usually of interest to the moneyed classes in British society. In 1967, members of the Astor family sold the paper to Canadian publishing magnate Roy Thomson. His Thomson Corporation brought it under the same ownership as The Sunday Times to form Times Newspapers Limited.
The Thomson Corporation management were struggling to run the business due to the 1979 Energy Crisis and union demands. Management were left with no choice but to find a buyer who was in a position to guarantee the survival of both titles, and also one who had the resources and was committed to funding the introduction of modern printing methods.
Several suitors appeared, including Robert Maxwell, Tiny Rowland and Lord Rothermere; however, only one buyer was in a position to meet the full Thomson remit, Australian media magnate Rupert Murdoch. Robert Holmes à Court, another Australian magnate had previously tried to buy The Times in 1980.
In 1981, The Times and The Sunday Times were bought from Thomson by Rupert Murdoch's News International. The acquisition followed three weeks of intensive bargaining with the unions by company negotiators, John Collier and Bill O'Neill.
After 14 years as editor, William Rees-Mogg resigned the post upon completion of the change of ownership. Murdoch began to make his mark on the paper by appointing Harold Evans as his replacement. One of his most important changes was the introduction of new technology and efficiency measures. In March–May 1982, following agreement with print unions, the hot-metal Linotype printing process used to print The Times since the 19th century was phased out and replaced by computer input and photo-composition. This allowed print room staff at The Times and The Sunday Times to be reduced by half. However, direct input of text by journalists ("single stroke" input) was still not achieved, and this was to remain an interim measure until the Wapping dispute of 1986, when The Times moved from New Printing House Square in Gray's Inn Road (near Fleet Street) to new offices in Wapping.
Robert Fisk, seven times British International Journalist of the Year, resigned as foreign correspondent in 1988 over what he saw as "political censorship" of his article on the shooting-down of Iran Air Flight 655 in July 1988. He wrote in detail about his reasons for resigning from the paper due to meddling with his stories, and the paper's pro-Israel stance.
In June 1990, The Times ceased its policy of using courtesy titles ("Mr", "Mrs", or "Miss" prefixes) for living persons before full names on first reference, but it continues to use them before surnames on subsequent references. The more formal style is now confined to the "Court and Social" page, though "Ms" is now acceptable in that section, as well as before surnames in news sections.
In November 2003, News International began producing the newspaper in both broadsheet and tabloid sizes. On 13 September 2004, the weekday broadsheet was withdrawn from sale in Northern Ireland. Since 1 November 2004, the paper has been printed solely in tabloid format.
On 6 June 2005, The Times redesigned its Letters page, dropping the practice of printing correspondents' full postal addresses. Published letters were long regarded as one of the paper's key constituents. Author/solicitor David Green of Castle Morris Pembrokeshire has had more letters published on the main letters page than any other known contributor – 158 by 31 January 2008. According to its leading article, "From Our Own Correspondents", removal of full postal addresses was in order to fit more letters onto the page.
In a 2007 meeting with the House of Lords Select Committee on Communications, which was investigating media ownership and the news, Murdoch stated that the law and the independent board prevented him from exercising editorial control.
In May 2008 printing of The Times switched from Wapping to new plants at Broxbourne on the outskirts of London, and Merseyside and Glasgow, enabling the paper to be produced with full colour on every page for the first time.
On 26 July 2012, to coincide with the official start of the London 2012 Olympics and the issuing of a series of souvenir front covers, The Times added the suffix "of London" to its masthead.
The Times features news for the first half of the paper, the Opinion/Comment section begins after the first news section with world news normally following this. The business pages begin on the centre spread, and are followed by The Register, containing obituaries, Court & Social section, and related material. The sport section is at the end of the main paper. The Times current prices are £1.20 for the daily edition and £1.50 for the Saturday edition.
The Times's main supplement, every day, is the times2, featuring various lifestyle columns. It was discontinued on 1 March 2010 but reintroduced on 11 October 2010 after negative feedback. Its regular features include a puzzles section called Mind Games. Its previous incarnation began on 5 September 2005, before which it was called T2 and previously Times 2. Regular features include columns by a different columnist each weekday. There was a column by Marcus du Sautoy each Wednesday, for example. The back pages are devoted to puzzles and contain sudoku, "Killer Sudoku", "KenKen", word polygon puzzles, and a crossword simpler and more concise than the main "Times Crossword".
The Game is included in the newspaper on Mondays, and details all the weekend's football activity (Premier League and Football League Championship, League One and League Two.) The Scottish edition of The Game also includes results and analysis from Scottish Premier League games.
The Saturday edition of The Times contains a variety of supplements. These supplements were relaunched in January 2009 as: Sport, Weekend (including travel and lifestyle features), Saturday Review (arts, books, and ideas), The Times Magazine (columns on various topics), and Playlist (an entertainment listings guide).
The Times Magazine features columns touching on various subjects such as celebrities, fashion and beauty, food and drink, homes and gardens or simply writers' anecdotes. Notable contributors include Giles Coren, Food and Drink Writer of the Year in 2005 and Nadiya Hussain, winner of BBC's The Great British Bake Off.
The Times and The Sunday Times have had an online presence since March 1999, originally at the-times.co.uk and sunday-times.co.uk, and later at timesonline.co.uk. There are now two websites: thetimes.co.uk is aimed at daily readers, and the thesundaytimes.co.uk site at providing weekly magazine-like content. There are also iPad and Android editions of both newspapers. Since July 2010, News UK has required readers who do not subscribe to the print edition to pay £2 per week to read The Times and The Sunday Times online.
The Times Digital Archive (1785–2008) is freely accessible via Gale databases to readers affiliated with subscribing academic, public, and school libraries.
Visits to the websites have decreased by 87% since the paywall was introduced, from 21 million unique users per month to 2.7 million. In April 2009, the timesonline site had a readership of 750,000 readers per day. As of October 2011, there were around 111,000 subscribers to The Times' digital products.
At the time of Harold Evans' appointment as editor in 1981, The Times had an average daily sale of 282,000 copies in comparison to the 1.4 million daily sales of its traditional rival The Daily Telegraph. By November 2005 The Times sold an average of 691,283 copies per day, the second-highest of any British "quality" newspaper (after The Daily Telegraph, which had a circulation of 903,405 copies in the period), and the highest in terms of full-rate sales. By March 2014, average daily circulation of The Times had fallen to 394,448 copies, compared to The Daily Telegraph's 523,048, with the two retaining respectively the second-highest and highest circulations among British "quality" newspapers. In contrast The Sun, the highest-selling "tabloid" daily newspaper in the United Kingdom, sold an average of 2,069,809 copies in March 2014, and the Daily Mail, the highest-selling "middle market" British daily newspaper, sold an average of 1,708,006 copies in the period.
The Sunday Times has a significantly higher circulation than The Times, and sometimes outsells The Sunday Telegraph. As of January 2013, The Times has a circulation of 399,339 and The Sunday Times of 885,612.
In a 2009 national readership survey The Times was found to have the highest number of ABC1 25–44 readers and the largest numbers of readers in London of any of the "quality" papers.
The Times commissioned the serif typeface Times New Roman, created by Victor Lardent at the English branch of Monotype, in 1931. It was commissioned after Stanley Morison had written an article criticizing The Times for being badly printed and typographically antiquated. The font was supervised by Morison and drawn by Victor Lardent, an artist from the advertising department of The Times. Morison used an older font named Plantin as the basis for his design, but made revisions for legibility and economy of space. Times New Roman made its debut in the issue of 3 October 1932. After one year, the design was released for commercial sale. The Times stayed with Times New Roman for 40 years, but new production techniques and the format change from broadsheet to tabloid in 2004 have caused the newspaper to switch font five times since 1972. However, all the new fonts have been variants of the original New Roman font:
Historically, the paper was not overtly pro-Tory or Whig, but has been a long time bastion of the English Establishment and empire. The Times adopted a stance described as "peculiarly detached" at the 1945 general election; although it was increasingly critical of the Conservative Party's campaign, it did not advocate a vote for any one party. However, the newspaper reverted to the Tories for the next election five years later. It supported the Conservatives for the subsequent three elections, followed by support for both the Conservatives and the Liberal Party for the next five elections, expressly supporting a Con-Lib coalition in 1974. The paper then backed the Conservatives solidly until 1997, when it declined to make any party endorsement but supported individual (primarily Eurosceptic) candidates.
For the 2001 general election The Times declared its support for Tony Blair's Labour government, which was re-elected by a landslide. It supported Labour again in 2005, when Labour achieved a third successive win, though with a reduced majority. For the 2010 general election, however, the newspaper declared its support for the Tories once again; the election ended in the Tories taking the most votes and seats but having to form a coalition with the Liberal Democrats in order to form a government as they had failed to gain an overall majority.
This makes it the most varied newspaper in terms of political support in British history. Some columnists in The Times are connected to the Conservative Party such as Daniel Finkelstein, Tim Montgomerie, Matthew Parris and Matt Ridley, but there are also columnists connected to the Labour Party such as David Aaronovitch, Phil Collins, Oliver Kamm and Jenni Russell.
The Times occasionally makes endorsements for foreign elections. In November 2012, it endorsed a second term for Barack Obama although it also expressed reservations about his foreign policy.
The Times, along with the British Film Institute, sponsors the "The Times" bfi London Film Festival. It also sponsors the Cheltenham Literature Festival and the Asia House Festival of Asian Literature at Asia House, London.
The Times Literary Supplement (TLS) first appeared in 1902 as a supplement to The Times, becoming a separately paid-for weekly literature and society magazine in 1914. The Times and the TLS have continued to be co-owned, and as of 2012 the TLS is also published by News International and cooperates closely with The Times, with its online version hosted on The Times website, and its editorial offices based in Times House, Pennington Street, London.
Times Atlases have been produced since 1895. They are currently produced by the Collins Bartholomew imprint of HarperCollins Publishers. The flagship product is The Times Comprehensive Atlas of the World.
This 164-page monthly magazine is sold separately from the newspaper of record and is Britain's best-selling travel magazine. The first issue of The Sunday Times Travel Magazine was in 2003, and it includes news, features and insider guides.
In the dystopian future world of George Orwell's Nineteen Eighty-Four, The Times has been transformed into the organ of the totalitarian ruling party, its editorials—of which several are quoted in the book—reflecting Big Brother's pronouncements.
Rex Stout's fictional detective Nero Wolfe is described as fond of solving the London Times' crossword puzzle at his New York home, in preference to those of American papers.
In the James Bond series by Ian Fleming, James Bond, reads The Times. As described by Fleming in From Russia, with Love: "The Times was the only paper that Bond ever read."
In The Wombles, Uncle Bulgaria read The Times and asked for the other Wombles to bring him any copies that they found amongst the litter. The newspaper played a central role in the episode Very Behind the Times (Series 2, Episode 12).
The domestic dog (Canis lupus familiaris or Canis familiaris) is a domesticated canid which has been selectively bred for millennia for various behaviors, sensory capabilities, and physical attributes.
Although initially thought to have originated as a manmade variant of an extant canid species (variously supposed as being the dhole, golden jackal, or gray wolf), extensive genetic studies undertaken during the 2010s indicate that dogs diverged from an extinct wolf-like canid in Eurasia 40,000 years ago. Being the oldest domesticated animal, their long association with people has allowed dogs to be uniquely attuned to human behavior, as well as thrive on a starch-rich diet which would be inadequate for other canid species.
Dogs perform many roles for people, such as hunting, herding, pulling loads, protection, assisting police and military, companionship, and, more recently, aiding handicapped individuals. This impact on human society has given them the nickname "man's best friend" in the Western world. In some cultures, however, dogs are a source of meat.
The term "domestic dog" is generally used for both of the domesticated and feral varieties. The English word dog comes from Middle English dogge, from Old English docga, a "powerful dog breed". The term may possibly derive from Proto-Germanic *dukkōn, represented in Old English finger-docce ("finger-muscle"). The word also shows the familiar petname diminutive -ga also seen in frogga "frog", picga "pig", stagga "stag", wicga "beetle, worm", among others. The term dog may ultimately derive from the earliest layer of Proto-Indo-European vocabulary.
In 14th-century England, hound (from Old English: hund) was the general word for all domestic canines, and dog referred to a subtype of hound, a group including the mastiff. It is believed this "dog" type was so common, it eventually became the prototype of the category "hound". By the 16th century, dog had become the general word, and hound had begun to refer only to types used for hunting. The word "hound" is ultimately derived from the Proto-Indo-European word *kwon- "dog".
In breeding circles, a male canine is referred to as a dog, while a female is called a bitch (Middle English bicche, from Old English bicce, ultimately from Old Norse bikkja). A group of offspring is a litter. The father of a litter is called the sire, and the mother is called the dam. Offspring are, in general, called pups or puppies, from French poupée, until they are about a year old. The process of birth is whelping, from the Old English word hwelp.
In 1758, the taxonomist Linnaeus published in Systema Naturae a categorization of species which included the Canis species. Canis is a Latin word meaning dog, and the list included the dog-like carnivores: the domestic dog, wolves, foxes and jackals. The dog was classified as Canis familiaris, which means "Dog-family" or the family dog. On the next page he recorded the wolf as Canis lupus, which means "Dog-wolf". In 1978, a review aimed at reducing the number of recognized Canis species proposed that "Canis dingo is now generally regarded as a distinctive feral domestic dog. Canis familiaris is used for domestic dogs, although taxonomically it should probably be synonymous with Canis lupus." In 1982, the first edition of Mammal Species of the World listed Canis familiaris under Canis lupus with the comment: "Probably ancestor of and conspecific with the domestic dog, familiaris. Canis familiaris has page priority over Canis lupus, but both were published simultaneously in Linnaeus (1758), and Canis lupus has been universally used for this species", which avoided classifying the wolf as the family dog. The dog is now listed among the many other Latin-named subspecies of Canis lupus as Canis lupus familiaris.
In 2003, the ICZN ruled in its Opinion 2027 that if wild animals and their domesticated derivatives are regarded as one species, then the scientific name of that species is the scientific name of the wild animal. In 2005, the third edition of Mammal Species of the World upheld Opinion 2027 with the name Lupus and the note: "Includes the domestic dog as a subspecies, with the dingo provisionally separate - artificial variants created by domestication and selective breeding". However, Canis familiaris is sometimes used due to an ongoing nomenclature debate because wild and domestic animals are separately recognizable entities and that the ICZN allowed users a choice as to which name they could use, and a number of internationally recognized researchers prefer to use Canis familiaris.
Later genetic studies strongly supported dogs and gray wolves forming two sister monophyletic clades within the one species, and that the common ancestor of dogs and extant wolves is extinct.
The origin of the domestic dog (Canis lupus familiaris or Canis familiaris) is not clear. Whole genome sequencing indicates that the dog, the gray wolf and the extinct Taymyr wolf diverged at around the same time 27,000–40,000 years ago. These dates imply that the earliest dogs arose in the time of human hunter-gatherers and not agriculturists. Modern dogs are more closely related to ancient wolf fossils that have been found in Europe than they are to modern gray wolves. Nearly all dog breeds' genetic closeness to the gray wolf are due to admixture, except several Arctic dog breeds are close to the Taimyr wolf of North Asia due to admixture.
Domestic dogs have been selectively bred for millennia for various behaviors, sensory capabilities, and physical attributes. Modern dog breeds show more variation in size, appearance, and behavior than any other domestic animal. Dogs are predators and scavengers, and like many other predatory mammals, the dog has powerful muscles, fused wrist bones, a cardiovascular system that supports both sprinting and endurance, and teeth for catching and tearing.
Dogs are highly variable in height and weight. The smallest known adult dog was a Yorkshire Terrier, that stood only 6.3 cm (2.5 in) at the shoulder, 9.5 cm (3.7 in) in length along the head-and-body, and weighed only 113 grams (4.0 oz). The largest known dog was an English Mastiff which weighed 155.6 kg (343 lb) and was 250 cm (98 in) from the snout to the tail. The tallest dog is a Great Dane that stands 106.7 cm (42.0 in) at the shoulder.
The coats of domestic dogs are of two varieties: "double" being common with dogs (as well as wolves) originating from colder climates, made up of a coarse guard hair and a soft down hair, or "single", with the topcoat only.
Domestic dogs often display the remnants of countershading, a common natural camouflage pattern. A countershaded animal will have dark coloring on its upper surfaces and light coloring below, which reduces its general visibility. Thus, many breeds will have an occasional "blaze", stripe, or "star" of white fur on their chest or underside.
There are many different shapes for dog tails: straight, straight up, sickle, curled, or cork-screw. As with many canids, one of the primary functions of a dog's tail is to communicate their emotional state, which can be important in getting along with others. In some hunting dogs, however, the tail is traditionally docked to avoid injuries. In some breeds, such as the Braque du Bourbonnais, puppies can be born with a short tail or no tail at all.
Some breeds of dogs are prone to certain genetic ailments such as elbow and hip dysplasia, blindness, deafness, pulmonic stenosis, cleft palate, and trick knees. Two serious medical conditions particularly affecting dogs are pyometra, affecting unspayed females of all types and ages, and bloat, which affects the larger breeds or deep-chested dogs. Both of these are acute conditions, and can kill rapidly. Dogs are also susceptible to parasites such as fleas, ticks, and mites, as well as hookworms, tapeworms, roundworms, and heartworms.
A number of common human foods and household ingestibles are toxic to dogs, including chocolate solids (theobromine poisoning), onion and garlic (thiosulphate, sulfoxide or disulfide poisoning), grapes and raisins, macadamia nuts, xylitol, as well as various plants and other potentially ingested materials. The nicotine in tobacco can also be dangerous. Dogs can get it by scavenging in garbage or ashtrays; eating cigars and cigarettes. Signs can be vomiting of large amounts (e.g., from eating cigar butts) or diarrhea. Some other signs are abdominal pain, loss of coordination, collapse, or death. Dogs are highly susceptible to theobromine poisoning, typically from ingestion of chocolate. Theobromine is toxic to dogs because, although the dog's metabolism is capable of breaking down the chemical, the process is so slow that even small amounts of chocolate can be fatal, especially dark chocolate.
In 2013, a study found that mixed breeds live on average 1.2 years longer than pure breeds, and that increasing body-weight was negatively correlated with longevity (i.e. the heavier the dog the shorter its lifespan).
The typical lifespan of dogs varies widely among breeds, but for most the median longevity, the age at which half the dogs in a population have died and half are still alive, ranges from 10 to 13 years. Individual dogs may live well beyond the median of their breed.
The breed with the shortest lifespan (among breeds for which there is a questionnaire survey with a reasonable sample size) is the Dogue de Bordeaux, with a median longevity of about 5.2 years, but several breeds, including Miniature Bull Terriers, Bloodhounds, and Irish Wolfhounds are nearly as short-lived, with median longevities of 6 to 7 years.
The longest-lived breeds, including Toy Poodles, Japanese Spitz, Border Terriers, and Tibetan Spaniels, have median longevities of 14 to 15 years. The median longevity of mixed-breed dogs, taken as an average of all sizes, is one or more years longer than that of purebred dogs when all breeds are averaged. The dog widely reported to be the longest-lived is "Bluey", who died in 1939 and was claimed to be 29.5 years old at the time of his death. On 5 December 2011, Pusuke, the world's oldest living dog recognized by Guinness Book of World Records, died aged 26 years and 9 months.
In domestic dogs, sexual maturity begins to happen around age six to twelve months for both males and females, although this can be delayed until up to two years old for some large breeds. This is the time at which female dogs will have their first estrous cycle. They will experience subsequent estrous cycles biannually, during which the body prepares for pregnancy. At the peak of the cycle, females will come into estrus, being mentally and physically receptive to copulation. Because the ova survive and are capable of being fertilized for a week after ovulation, it is possible for a female to mate with more than one male.
Dogs bear their litters roughly 58 to 68 days after fertilization, with an average of 63 days, although the length of gestation can vary. An average litter consists of about six puppies, though this number may vary widely based on the breed of dog. In general, toy dogs produce from one to four puppies in each litter, while much larger breeds may average as many as twelve.
Neutering refers to the sterilization of animals, usually by removal of the male's testicles or the female's ovaries and uterus, in order to eliminate the ability to procreate and reduce sex drive. Because of the overpopulation of dogs in some countries, many animal control agencies, such as the American Society for the Prevention of Cruelty to Animals (ASPCA), advise that dogs not intended for further breeding should be neutered, so that they do not have undesired puppies that may have to later be euthanized.
Neutering reduces problems caused by hypersexuality, especially in male dogs. Spayed female dogs are less likely to develop some forms of cancer, affecting mammary glands, ovaries, and other reproductive organs. However, neutering increases the risk of urinary incontinence in female dogs, and prostate cancer in males, as well as osteosarcoma, hemangiosarcoma, cruciate ligament rupture, obesity, and diabetes mellitus in either sex.
Dog intelligence is the ability of the dog to perceive information and retain it as knowledge for applying to solve problems. Dogs have been shown to learn by inference. A study with Rico showed that he knew the labels of over 200 different items. He inferred the names of novel items by exclusion learning and correctly retrieved those novel items immediately and also 4 weeks after the initial exposure. Dogs have advanced memory skills. A study documented the learning and memory capabilities of a border collie, "Chaser", who had learned the names and could associate by verbal command over 1,000 words. Dogs are able to read and react appropriately to human body language such as gesturing and pointing, and to understand human voice commands. Dogs demonstrate a theory of mind by engaging in deception. A study showed compelling evidence that Australian dingos can outperform domestic dogs in non-social problem-solving experiment, indicating that domestic dogs may have lost much of their original problem-solving abilities once they joined humans. Another study indicated that after undergoing training to solve a simple manipulation task, dogs that are faced with an insoluble version of the same problem look at the human, while socialized wolves do not. Modern domestic dogs use humans to solve their problems for them.
Dog behavior is the internally coordinated responses (actions or inactions) of the domestic dog (individuals or groups) to internal and/or external stimuli. As the oldest domesticated species, with estimates ranging from 9,000–30,000 years BCE, the minds of dogs inevitably have been shaped by millennia of contact with humans. As a result of this physical and social evolution, dogs, more than any other species, have acquired the ability to understand and communicate with humans and they are uniquely attuned to our behaviors. Behavioral scientists have uncovered a surprising set of social-cognitive abilities in the otherwise humble domestic dog. These abilities are not possessed by the dog's closest canine relatives nor by other highly intelligent mammals such as great apes. Rather, these skills parallel some of the social-cognitive skills of human children.
Dog communication is about how dogs "speak" to each other, how they understand messages that humans send to them, and how humans can translate the ideas that dogs are trying to transmit.:xii These communication behaviors include eye gaze, facial expression, vocalization, body posture (including movements of bodies and limbs) and gustatory communication (scents, pheromones and taste). Humans communicate with dogs by using vocalization, hand signals and body posture.
Despite their close genetic relationship and the ability to inter-breed, there are a number of diagnostic features to distinguish the gray wolves from domestic dogs. Domesticated dogs are clearly distinguishable from wolves by starch gel electrophoresis of red blood cell acid phosphatase. The tympanic bullae are large, convex and almost spherical in gray wolves, while the bullae of dogs are smaller, compressed and slightly crumpled. Compared to equally sized wolves, dogs tend to have 20% smaller skulls and 30% smaller brains.:35 The teeth of gray wolves are also proportionately larger than those of dogs; the premolars and molars of wolves are much less crowded and have more complex cusp patterns. Wolves do not have dewclaws on their back legs, unless there has been admixture with dogs. Dogs lack a functioning pre-caudal gland, and most enter estrus twice yearly, unlike gray wolves which only do so once a year. Dogs require fewer calories to function than wolves. The dog's limp ears may be the result of atrophy of the jaw muscles. The skin of domestic dogs tends to be thicker than that of wolves, with some Inuit tribes favoring the former for use as clothing due to its greater resistance to wear and tear in harsh weather.
Unlike other domestic species which were primarily selected for production-related traits, dogs were initially selected for their behaviors. In 2016, a study found that there were only 11 fixed genes that showed variation between wolves and dogs. These gene variations were unlikely to have been the result of natural evolution, and indicate selection on both morphology and behavior during dog domestication. These genes have been shown to have an impact on the catecholamine synthesis pathway, with the majority of the genes affecting the fight-or-flight response (i.e. selection for tameness), and emotional processing. Dogs generally show reduced fear and aggression compared to wolves. Some of these genes have been associated with aggression in some dog breeds, indicating their importance in both the initial domestication and then later in breed formation.
The global dog population is estimated at 525 million:225 based on a transparent methodology, as opposed to other estimates where the methodology has not been made available – all dog population estimates are based on regional human population densities and land uses.
Although large wild dogs, like wolves, are apex predators, they can be killed in territory disputes with wild animals. Furthermore, in areas where both dogs and other large predators live, dogs can be a major food source for big cats or canines. Reports from Croatia indicate wolves kill dogs more frequently than they kill sheep. Wolves in Russia apparently limit feral dog populations. In Wisconsin, more compensation has been paid for dog losses than livestock. Some wolf pairs have been reported to prey on dogs by having one wolf lure the dog out into heavy brush where the second animal waits in ambush. In some instances, wolves have displayed an uncharacteristic fearlessness of humans and buildings when attacking dogs, to the extent that they have to be beaten off or killed.
Coyotes and big cats have also been known to attack dogs. Leopards in particular are known to have a predilection for dogs, and have been recorded to kill and consume them regardless of the dog's size or ferocity. Tigers in Manchuria, Indochina, Indonesia, and Malaysia are reputed to kill dogs with the same vigor as leopards. Striped hyenas are major predators of village dogs in Turkmenistan, India, and the Caucasus. Reptiles such as alligators and pythons have been known to kill and eat dogs.
Despite their descent from wolves and classification as Carnivora, dogs are variously described in scholarly and other writings as carnivores or omnivores. Unlike obligate carnivores, such as the cat family with its shorter small intestine, dogs can adapt to a wide-ranging diet, and are not dependent on meat-specific protein nor a very high level of protein in order to fulfill their basic dietary requirements. Dogs will healthily digest a variety of foods, including vegetables and grains, and can consume a large proportion of these in their diet. Comparing dogs and wolves, dogs have adaptations in genes involved in starch digestion that contribute to an increased ability to thrive on a starch-rich diet.
Most breeds of dog are at most a few hundred years old, having been artificially selected for particular morphologies and behaviors by people for specific functional roles. Through this selective breeding, the dog has developed into hundreds of varied breeds, and shows more behavioral and morphological variation than any other land mammal. For example, height measured to the withers ranges from 15.2 centimetres (6.0 in) in the Chihuahua to about 76 cm (30 in) in the Irish Wolfhound; color varies from white through grays (usually called "blue") to black, and browns from light (tan) to dark ("red" or "chocolate") in a wide variation of patterns; coats can be short or long, coarse-haired to wool-like, straight, curly, or smooth. It is common for most breeds to shed this coat.
While all dogs are genetically very similar, natural selection and selective breeding have reinforced certain characteristics in certain populations of dogs, giving rise to dog types and dog breeds. Dog types are broad categories based on function, genetics, or characteristics. Dog breeds are groups of animals that possess a set of inherited characteristics that distinguishes them from other animals within the same species. Modern dog breeds are non-scientific classifications of dogs kept by modern kennel clubs.
Purebred dogs of one breed are genetically distinguishable from purebred dogs of other breeds, but the means by which kennel clubs classify dogs is unsystematic. Systematic analyses of the dog genome has revealed only four major types of dogs that can be said to be statistically distinct. These include the "old world dogs" (e.g., Malamute and Shar Pei), "Mastiff"-type (e.g., English Mastiff), "herding"-type (e.g., Border Collie), and "all others" (also called "modern"- or "hunting"-type).
Domestic dogs inherited complex behaviors, such as bite inhibition, from their wolf ancestors, which would have been pack hunters with complex body language. These sophisticated forms of social cognition and communication may account for their trainability, playfulness, and ability to fit into human households and social situations, and these attributes have given dogs a relationship with humans that has enabled them to become one of the most successful species on the planet today.:pages95-136
The dogs' value to early human hunter-gatherers led to them quickly becoming ubiquitous across world cultures. Dogs perform many roles for people, such as hunting, herding, pulling loads, protection, assisting police and military, companionship, and, more recently, aiding handicapped individuals. This impact on human society has given them the nickname "man's best friend" in the Western world. In some cultures, however, dogs are also a source of meat.
Humans would also have derived enormous benefit from the dogs associated with their camps. For instance, dogs would have improved sanitation by cleaning up food scraps. Dogs may have provided warmth, as referred to in the Australian Aboriginal expression "three dog night" (an exceptionally cold night), and they would have alerted the camp to the presence of predators or strangers, using their acute hearing to provide an early warning.
Anthropologists believe the most significant benefit would have been the use of dogs' robust sense of smell to assist with the hunt. The relationship between the presence of a dog and success in the hunt is often mentioned as a primary reason for the domestication of the wolf, and a 2004 study of hunter groups with and without a dog gives quantitative support to the hypothesis that the benefits of cooperative hunting was an important factor in wolf domestication.
Emigrants from Siberia that walked across the Bering land bridge into North America may have had dogs in their company, and one writer suggests that the use of sled dogs may have been critical to the success of the waves that entered North America roughly 12,000 years ago, although the earliest archaeological evidence of dog-like canids in North America dates from about 9,400 years ago.:104 Dogs were an important part of life for the Athabascan population in North America, and were their only domesticated animal. Dogs also carried much of the load in the migration of the Apache and Navajo tribes 1,400 years ago. Use of dogs as pack animals in these cultures often persisted after the introduction of the horse to North America.
"The most widespread form of interspecies bonding occurs between humans and dogs" and the keeping of dogs as companions, particularly by elites, has a long history. (As a possible example, at the Natufian culture site of Ain Mallaha in Israel, dated to 12,000 BC, the remains of an elderly human and a four-to-five-month-old puppy were found buried together). However, pet dog populations grew significantly after World War II as suburbanization increased. In the 1950s and 1960s, dogs were kept outside more often than they tend to be today (using the expression "in the doghouse" to describe exclusion from the group signifies the distance between the doghouse and the home) and were still primarily functional, acting as a guard, children's playmate, or walking companion. From the 1980s, there have been changes in the role of the pet dog, such as the increased role of dogs in the emotional support of their human guardians. People and dogs have become increasingly integrated and implicated in each other's lives, to the point where pet dogs actively shape the way a family and home are experienced.
There have been two major trends in the changing status of pet dogs. The first has been the 'commodification' of the dog, shaping it to conform to human expectations of personality and behaviour. The second has been the broadening of the concept of the family and the home to include dogs-as-dogs within everyday routines and practices.
There are a vast range of commodity forms available to transform a pet dog into an ideal companion. The list of goods, services and places available is enormous: from dog perfumes, couture, furniture and housing, to dog groomers, therapists, trainers and caretakers, dog cafes, spas, parks and beaches, and dog hotels, airlines and cemeteries. While dog training as an organized activity can be traced back to the 18th century, in the last decades of the 20th century it became a high profile issue as many normal dog behaviors such as barking, jumping up, digging, rolling in dung, fighting, and urine marking (which dogs do to establish territory through scent), became increasingly incompatible with the new role of a pet dog. Dog training books, classes and television programs proliferated as the process of commodifying the pet dog continued.
The majority of contemporary people with dogs describe their pet as part of the family, although some ambivalence about the relationship is evident in the popular reconceptualization of the dog–human family as a pack. A dominance model of dog–human relationships has been promoted by some dog trainers, such as on the television program Dog Whisperer. However it has been disputed that "trying to achieve status" is characteristic of dog–human interactions. Pet dogs play an active role in family life; for example, a study of conversations in dog–human families showed how family members use the dog as a resource, talking to the dog, or talking through the dog, to mediate their interactions with each other.
Another study of dogs' roles in families showed many dogs have set tasks or routines undertaken as family members, the most common of which was helping with the washing-up by licking the plates in the dishwasher, and bringing in the newspaper from the lawn. Increasingly, human family members are engaging in activities centered on the perceived needs and interests of the dog, or in which the dog is an integral partner, such as dog dancing and dog yoga.
According to statistics published by the American Pet Products Manufacturers Association in the National Pet Owner Survey in 2009–2010, it is estimated there are 77.5 million people with pet dogs in the United States. The same survey shows nearly 40% of American households own at least one dog, of which 67% own just one dog, 25% two dogs and nearly 9% more than two dogs. There does not seem to be any gender preference among dogs as pets, as the statistical data reveal an equal number of female and male dog pets. Yet, although several programs are undergoing to promote pet adoption, less than a fifth of the owned dogs come from a shelter.
The latest study using magnetic resonance imaging (MRI) to humans and dogs together proved that dogs have same response to voices and use the same parts of the brain as humans to do so. This gives dogs the ability to recognize emotional human sounds, making them friendly social pets to humans.
Dogs have lived and worked with humans in so many roles that they have earned the unique nickname, "man's best friend", a phrase used in other languages as well. They have been bred for herding livestock, hunting (e.g. pointers and hounds), rodent control, guarding, helping fishermen with nets, detection dogs, and pulling loads, in addition to their roles as companions. In 1957, a husky-terrier mix named Laika became the first animal to orbit the Earth.
Service dogs such as guide dogs, utility dogs, assistance dogs, hearing dogs, and psychological therapy dogs provide assistance to individuals with physical or mental disabilities. Some dogs owned by epileptics have been shown to alert their handler when the handler shows signs of an impending seizure, sometimes well in advance of onset, allowing the guardian to seek safety, medication, or medical care.
In conformation shows, also referred to as breed shows, a judge familiar with the specific dog breed evaluates individual purebred dogs for conformity with their established breed type as described in the breed standard. As the breed standard only deals with the externally observable qualities of the dog (such as appearance, movement, and temperament), separately tested qualities (such as ability or health) are not part of the judging in conformation shows.
Dog meat is consumed in some East Asian countries, including Korea, China, and Vietnam, a practice that dates back to antiquity. It is estimated that 13–16 million dogs are killed and consumed in Asia every year. Other cultures, such as Polynesia and pre-Columbian Mexico, also consumed dog meat in their history. However, Western, South Asian, African, and Middle Eastern cultures, in general, regard consumption of dog meat as taboo. In some places, however, such as in rural areas of Poland, dog fat is believed to have medicinal properties—being good for the lungs for instance. Dog meat is also consumed in some parts of Switzerland. Proponents of eating dog meat have argued that placing a distinction between livestock and dogs is western hypocrisy, and that there is no difference with eating the meat of different animals.
The most popular Korean dog dish is gaejang-guk (also called bosintang), a spicy stew meant to balance the body's heat during the summer months; followers of the custom claim this is done to ensure good health by balancing one's gi, or vital energy of the body. A 19th century version of gaejang-guk explains that the dish is prepared by boiling dog meat with scallions and chili powder. Variations of the dish contain chicken and bamboo shoots. While the dishes are still popular in Korea with a segment of the population, dog is not as widely consumed as beef, chicken, and pork.
Citing a 2008 study, the U.S. Center for Disease Control estimated in 2015 that 4.5 million people in the USA are bitten by dogs each year. A 2015 study estimated that 1.8% of the U.S. population is bitten each year. In the 1980s and 1990s the US averaged 17 fatalities per year, while in the 2000s this has increased to 26. 77% of dog bites are from the pet of family or friends, and 50% of attacks occur on the property of the dog's legal owner.
A Colorado study found bites in children were less severe than bites in adults. The incidence of dog bites in the US is 12.9 per 10,000 inhabitants, but for boys aged 5 to 9, the incidence rate is 60.7 per 10,000. Moreover, children have a much higher chance to be bitten in the face or neck. Sharp claws with powerful muscles behind them can lacerate flesh in a scratch that can lead to serious infections.
In the United States, cats and dogs are a factor in more than 86,000 falls each year. It has been estimated around 2% of dog-related injuries treated in UK hospitals are domestic accidents. The same study found that while dog involvement in road traffic accidents was difficult to quantify, dog-associated road accidents involving injury more commonly involved two-wheeled vehicles.
Toxocara canis (dog roundworm) eggs in dog feces can cause toxocariasis. In the United States, about 10,000 cases of Toxocara infection are reported in humans each year, and almost 14% of the U.S. population is infected. In Great Britain, 24% of soil samples taken from public parks contained T. canis eggs. Untreated toxocariasis can cause retinal damage and decreased vision. Dog feces can also contain hookworms that cause cutaneous larva migrans in humans.
A 2005 paper states "recent research has failed to support earlier findings that pet ownership is associated with a reduced risk of cardiovascular disease, a reduced use of general practitioner services, or any psychological or physical benefits on health for community dwelling older people. Research has, however, pointed to significantly less absenteeism from school through sickness among children who live with pets." In one study, new guardians reported a highly significant reduction in minor health problems during the first month following pet acquisition, and this effect was sustained in those with dogs through to the end of the study.
In addition, people with pet dogs took considerably more physical exercise than those with cats and those without pets. The results provide evidence that keeping pets may have positive effects on human health and behaviour, and that for guardians of dogs these effects are relatively long-term. Pet guardianship has also been associated with increased coronary artery disease survival, with human guardians being significantly less likely to die within one year of an acute myocardial infarction than those who did not own dogs.
The health benefits of dogs can result from contact with dogs in general, and not solely from having dogs as pets. For example, when in the presence of a pet dog, people show reductions in cardiovascular, behavioral, and psychological indicators of anxiety. Other health benefits are gained from exposure to immune-stimulating microorganisms, which, according to the hygiene hypothesis, can protect against allergies and autoimmune diseases. The benefits of contact with a dog also include social support, as dogs are able to not only provide companionship and social support themselves, but also to act as facilitators of social interactions between humans. One study indicated that wheelchair users experience more positive social interactions with strangers when they are accompanied by a dog than when they are not. In 2015, a study found that pet owners were significantly more likely to get to know people in their neighborhood than non-pet owners.
The practice of using dogs and other animals as a part of therapy dates back to the late 18th century, when animals were introduced into mental institutions to help socialize patients with mental disorders. Animal-assisted intervention research has shown that animal-assisted therapy with a dog can increase social behaviors, such as smiling and laughing, among people with Alzheimer's disease. One study demonstrated that children with ADHD and conduct disorders who participated in an education program with dogs and other animals showed increased attendance, increased knowledge and skill objectives, and decreased antisocial and violent behavior compared to those who were not in an animal-assisted program.
Medical detection dogs are capable of detecting diseases by sniffing a person directly or samples of urine or other specimens. Dogs can detect odour in one part per trillion, as their brain's olfactory cortex is (relative to total brain size) 40 times larger than humans. Dogs may have as many as 300 million odour receptors in their nose, while humans may have only 5 million. Each dog is trained specifically for the detection of single disease from the blood glucose level indicative to diabetes to cancer. To train a cancer dog requires 6 months. A Labrador Retriever called Daisy has detected 551 cancer patients with an accuracy of 93 percent and received the Blue Cross (for pets) Medal for her life-saving skills.
In Greek mythology, Cerberus is a three-headed watchdog who guards the gates of Hades. In Norse mythology, a bloody, four-eyed dog called Garmr guards Helheim. In Persian mythology, two four-eyed dogs guard the Chinvat Bridge. In Philippine mythology, Kimat who is the pet of Tadaklan, god of thunder, is responsible for lightning. In Welsh mythology, Annwn is guarded by Cŵn Annwn.
In Hindu mythology, Yama, the god of death owns two watch dogs who have four eyes. They are said to watch over the gates of Naraka. Hunter god Muthappan from North Malabar region of Kerala has a hunting dog as his mount. Dogs are found in and out of the Muthappan Temple and offerings at the shrine take the form of bronze dog figurines.
In Islam, dogs are viewed as unclean because they are viewed as scavengers. In 2015 city councillor Hasan Küçük of The Hague called for dog ownership to be made illegal in that city. Islamic activists in Lérida, Spain, lobbied for dogs to be kept out of Muslim neighborhoods, saying their presence violated Muslims' religious freedom. In Britain, police sniffer dogs are carefully used, and are not permitted to contact passengers, only their luggage. They are required to wear leather dog booties when searching mosques or Muslim homes.
Jewish law does not prohibit keeping dogs and other pets. Jewish law requires Jews to feed dogs (and other animals that they own) before themselves, and make arrangements for feeding them before obtaining them. In Christianity, dogs represent faithfulness.
In Asian countries such as China, Korea, and Japan, dogs are viewed as kind protectors. The role of the dog in Chinese mythology includes a position as one of the twelve animals which cyclically represent years (the zodiacal dog).
Cultural depictions of dogs in art extend back thousands of years to when dogs were portrayed on the walls of caves. Representations of dogs became more elaborate as individual breeds evolved and the relationships between human and canine developed. Hunting scenes were popular in the Middle Ages and the Renaissance. Dogs were depicted to symbolize guidance, protection, loyalty, fidelity, faithfulness, watchfulness, and love.
Dogs are also vulnerable to some of the same health conditions as humans, including diabetes, dental and heart disease, epilepsy, cancer, hypothyroidism, and arthritis.
Some dog breeds have acquired traits through selective breeding that interfere with reproduction. Male French Bulldogs, for instance, are incapable of mounting the female. For many dogs of this breed, the female must be artificially inseminated in order to reproduce.
Although it is said that the "dog is man's best friend" regarding 17–24% of dogs in the developed countries, in the developing world they are feral, village or community dogs, with pet dogs uncommon. These live their lives as scavengers and have never been owned by humans, with one study showing their most common response when approached by strangers was to run away (52%) or respond with aggression (11%). We know little about these dogs, nor about the dogs that live in developed countries that are feral, stray or are in shelters, yet the great majority of modern research on dog cognition has focused on pet dogs living in human homes.
Wolves, and their dog descendants, would have derived significant benefits from living in human camps—more safety, more reliable food, lesser caloric needs, and more chance to breed. They would have benefited from humans' upright gait that gives them larger range over which to see potential predators and prey, as well as color vision that, at least by day, gives humans better visual discrimination. Camp dogs would also have benefited from human tool use, as in bringing down larger prey and controlling fire for a range of purposes.
The cohabitation of dogs and humans would have greatly improved the chances of survival for early human groups, and the domestication of dogs may have been one of the key forces that led to human success.
The scientific evidence is mixed as to whether companionship of a dog can enhance human physical health and psychological wellbeing. Studies suggesting that there are benefits to physical health and psychological wellbeing have been criticised for being poorly controlled, and finding that "[t]he health of elderly people is related to their health habits and social supports but not to their ownership of, or attachment to, a companion animal." Earlier studies have shown that people who keep pet dogs or cats exhibit better mental and physical health than those who do not, making fewer visits to the doctor and being less likely to be on medication than non-guardians.
Located approximately 250 kilometres (160 mi) east of Puerto Rico and the nearer Virgin Islands, St. Barthélemy lies immediately southeast of the islands of Saint Martin and Anguilla. It is one of the Renaissance Islands. St. Barthélemy is separated from Saint Martin by the Saint-Barthélemy Channel. It lies northeast of Saba and St Eustatius, and north of St Kitts. Some small satellite islets belong to St. Barthélemy including Île Chevreau (Île Bonhomme), Île Frégate, Île Toc Vers, Île Tortue and Gros Îlets (Îlots Syndare). A much bigger islet, Île Fourchue, lies on the north of the island, in the Saint-Barthélemy Channel. Other rocky islets which include Coco, the Roques (or little Turtle rocks), the Goat, and the Sugarloaf.
Residents of Saint-Barthélemy (Saint-Barthélemoise people) are French citizens and work at establishments on the island. Most of them are descendants of the first settlers, of Breton, Norman, Poitevin, Saintongeais and Angevin lineage. French is the native tongue of the population. English is understood in hotels and restaurants, and a small population of Anglophones have been resident in Gustavia for many years. The St. Barthélemy French patois is spoken by some 500–700 people in the leeward portion of the island and is superficially related to Quebec French, whereas Créole French is limited to the windward side. Unlike other populations in the Caribbean, language preference between the Créole and Patois is geographically, and not racially, determined.[page needed]
On 7 February 2007, the French Parliament passed a bill granting COM status to both St. Barthélemy and (separately) to the neighbouring Saint Martin. The new status took effect on 15 July 2007, when the first territorial council was elected, according to the law. The island has a president (elected every five years), a unicameral Territorial Council of nineteen members who are elected by popular vote and serve for five-year terms, and an executive council of seven members. Elections to these councils were first held on 1 July 2007 with the last election in March 2012.
One senator represents the island in the French Senate. The first election was held on 21 September 2008 with the last election in September 2014. St. Barthélemy became an overseas territory of the European Union on 1 January 2012, but the island's inhabitants remain French citizens with EU status holding EU passports. France is responsible for the defence of the island and as such has stationed a security force on the island comprising six policemen and thirteen gendarmes (posted on two-year term).
Agricultural production on the island is difficult given the dry and rocky terrain, but the early settlers managed to produce vegetables, cotton, pineapples, salt, bananas and also fishing. Sweet potato is also grown in patches. The islanders developed commerce through the port of Gustavia. Duty-free port attractions, retail trade, high-end tourism (mostly from North America) and its luxury hotels and villas have increased the island's prosperity, reflected in the high standard of living of its citizens.
International investment and the wealth generated by wealthy tourists explain the high standard of living on the island. St. Barthélemy is considered a playground of the rich and famous,[citation needed] especially as a winter haven, and is known for its beaches, gourmet dining and high-end designers. Most of the food is imported by airplane or boat from the US or France. Tourism attracts about 200,000 visitors every year. As a result, there is a boom in house building activity catering to the tourists and also to the permanent residents of the island, with prices as high as €61,200,000 for a beachfront villa.
St. Barthélemy has about 25 hotels, most of them with 15 rooms or fewer. The largest has 58 rooms. Hotels are classified in the traditional French manner; 3 Star, 4 Star and 4 Star Luxe. Of particular note are Eden Rock and Cheval Blanc. Hotel Le Toiny, the most expensive hotel on the island, has 12 rooms. Most places of accommodation are in the form of private villas, of which there are some 400 available to rent on the island. The island's tourism industry, though expensive, attracts 70,000 visitors every year to its luxury hotels and villas and another 130,000 people arrive by luxury boats. It also attracts a labour force from Brazil and Portugal to meet the industry needs.
As the terrain is generally arid, the hills have mostly poor soil and support only cacti and succulent plants. During the rainy season the area turns green with vegetation and grass. The eastern part of the island is greener as it receives more rainfall. A 1994 survey has revealed several hundred indigenous species of plants including the naturalized varieties of flora; some growing in irrigated areas while the dry areas are dominated by the cacti variety. Sea grapes and palm trees are a common sight with mangroves and shrubs surviving in the saline coastal swamps. Coconut palm was brought to the island from the Pacific islands. Important plants noted on the island are:
Other trees of note include the royal palm, sea grape trees in the form of shrubs on the beaches and as 5 to 7 m trees in the interior areas of the island, aloe or aloe vera (brought from the Mediterranean), the night blooming cereus, mamillaria nivosa, yellow prickly pear or barbary fig which was planted as barbed wire defences against invading British army in 1773, Mexican cactus, stapelia gigantea, golden trumpet or yellow bell which was originally from South America, bougainvillea and others.
Marine mammals are many, such as the dolphins, porpoises and whales, which are seen here during the migration period from December till May. Turtles are a common sight along the coastline of the island. They are a protected species and in the endangered list. It is stated that it will take 15–50 years for this species to attain reproductive age. Though they live in the sea, the females come to the shore to lay eggs and are protected by private societies. Three species of turtles are particularly notable. These are: The leatherback sea turtles which have leather skin instead of a shell and are the largest of the type found here, some times measuring a much as 3 m (average is about 1.5 m) and weighing about 450 kg (jellyfish is their favourite diet); the hawksbill turtles, which have hawk-like beaks and found near reefs, generally about 90 cm in diameter and weigh about 60 kg and their diet consists of crabs and snails; and the green turtles, herbivores which have rounded heads, generally about 90 cm in diameter and live amidst tall sea grasses.
The marine life found here consists of anemones, urchins, sea cucumbers, and eels, which all live on the reefs along with turtles, conch and many varieties of marine fishes. The marine aquafauna is rich in conch, which has pearly-pink shells. Its meat is a favourite food supplement item and their shells are a collectors item. Other species of fish which are recorded close to the shore line in shallow waters are: sergeant majors, the blue chromis, brown chromis, surgeon fish; blue tangs and trumpet fish. On the shore are ghost crabs, which always live on the beach in small burrowed tunnels made in sand, and the hermit crabs, which live in land but lay eggs in water and which also eat garbage and sewerage. They spend some months in the sea during and after the hatching season.
Saint-Barthélemy has a marine nature reserve, known as the Reserve Naturelle that covers 1.200 ha, and is divided into 5 zones all around the island to form a network of protected areas. The Reserve includes the bays of Grand Cul de Sac, Colombier, Marigot, Petit Cul de Sac, Petite Anse as well as waters around offshore rocks such as Les Gross Islets, Pain de Sucre, Tortue and Forchue. The Reserve is designed to protect the islands coral reefs, seagrass and endangered marine species including sea turtles. The Reserve has two levels of protection, the yellow zones of protection where certain non-extractive activities, like snorkeling and boating, are allowed and the red zones of high protection where most activities including SCUBA are restricted in order to protect or recover marine life. Anchoring is prohibited in the Reserve and mooring buoys are in place in some of the protected bays like Colombier
Among the notable structures in the town are the three forts built by the Swedes for defense purposes. One of these forts, known as Fort Oscar (formerly Gustav Adolph), which overlooks the sea is located on the far side of La Pointe. However, the ruins have been replaced by a modern military building which now houses the local gendarmerie. The other fort known as Fort Karl now presents a very few ruins. The third fort built by the Swedes is the Fort Gustav, which is also seen in ruins strewn around the weather station and the Light House. The fort built in 1787 over a hill slope has ruins of ramparts, guardhouse, munitions depot, wood-burning oven and so forth.
French cuisine, West Indian cuisine, Creole cuisine, Italian cuisine and Asian cuisine are common in St. Barthélemy. The island has over 70 restaurants serving many dishes and others are a significant number of gourmet restaurants; many of the finest restaurants are located in the hotels. There are also a number of snack restaurants which the French call "les snacks" or "les petits creux" which include sandwiches, pizzas and salads. West Indian cuisine, steamed vegetables with fresh fish is common; Creole dishes tend to be spicier. The island hosts gastronomic events throughout the year, with dishes such as spring roll of shrimp and bacon, fresh grilled lobster, Chinese noodle salad with coconut milk, and grilled beef fillet etc.
The Transat AG2R Race, held every alternate year, is an event which originates in Concarneau in Brittany, France, reaching St. Barthélemy. It is a boat race with boats of 10 m length with a single hull and with essential safety equipment. Each boat is navigated by two sailors. Kitesurfing and other water sports have also become popular on the island in recent years, especially at Grand Cul-de-Sac beach (Baie de Grand Cul de Sac) for windy sports as kitesurfing and Saint Jean Beach ( Baie de Saint Jean), Lorient, Toiny and Anse des Cayes for surfing. Tennis is also popular on the island and it has several tennis clubs, Tennis Clube de Flamboyant in Grand Cul-de-Sac, AJOE Tennis Club in Orient and ASCO in Colombier.
St. Barthélemy has a small airport known as Gustaf III Airport on the north coast of the island that is served by small regional commercial aircraft and charters. The nearest airport with a runway length sufficient to land a typical commercial jet airliner is on the neighboring island of Sint Maarten: Princess Juliana International Airport, which acts as a hub, providing connecting flights with regional carriers to St. Barthélemy. Several international airlines and domestic Caribbean airlines operate in this sector.
Saint-Barthélemy (French: Saint-Barthélemy, French pronunciation: ​[sɛ̃baʁtelemi]), officially the Territorial collectivity of Saint-Barthélemy (French: Collectivité territoriale de Saint-Barthélemy), is an overseas collectivity of France. Often abbreviated to Saint-Barth in French, or St. Barts or St. Barths in English, the indigenous people called the island Ouanalao. St. Barthélemy lies about 35 kilometres (22 mi) southeast of St. Martin and north of St. Kitts. Puerto Rico is 240 kilometres (150 mi) to the west in the Greater Antilles.
Saint Barthélemy, a volcanic island fully encircled by shallow reefs, has an area of 25 square kilometres (9.7 sq mi) and a population of 9,035 (Jan. 2011 estimate). Its capital is Gustavia[citation needed], which also contains the main harbour to the island. It is the only Caribbean island which was a Swedish colony for any significant length of time; Guadeloupe was under Swedish rule only briefly at the end of the Napoleonic Wars. Symbolism from the Swedish national arms, the Three Crowns, still appears in the island's coat of arms. The language, cuisine, and culture, however, are distinctly French. The island is a popular tourist destination during the winter holiday season, especially for the rich and famous during the Christmas and new year period.
Saint Barthélemy was for many years a French commune forming part of Guadeloupe, which is an overseas region and department of France. Through a referendum in 2003, island residents sought separation from the administrative jurisdiction of Guadeloupe, and it was finally accomplished in 2007. The island of Saint Barthélemy became an Overseas Collectivity (COM). A governing territorial council was elected for its administration, which has provided the island with a certain degree of autonomy. The Hotel de Ville, which was the town hall, is now the Hotel de la Collectivité. A senator represents the island in Paris. St. Barthélemy has retained its free port status.
Grande Saline Bay provides temporary anchorage for small vessels while Colombier Bay, to the northwest, has a 4 fathoms patch near mid entrance. In the bight of St. Jean Bay there is a narrow cut through the reef. The north and east sides of the island are fringed, to a short distance from the shore, by a visible coral reef. Reefs are mostly in shallow waters and are clearly visible. The coastal areas abound with beaches and many of these have offshore reefs, some of which are part of a marine reserve.
There are as many as 22 public beaches (most beaches on St Barthélémy are known as "Anse de..." etc. ) of which 15 are considered suitable for swimming. They are categorized and divided into two groups, the leeward side (calm waters protected by the island itself) and windward side (some of which are protected by hills and reefs). The windward beaches are popular for windsurfing. The beach of St Jean is suitable for water sports and facilities have been created for that purpose. The long beach at Lorient has shade and is a quiet beach as compared to St. Jean.
The island covers an area of 25 square kilometres (2,500 ha). The eastern side is wetter than the western. Although the climate is essentially arid, the rainfall does average 1000 mm annually, but with considerable variation over the terrain. Summer is from May to November, which is also the rainy season. Winter from December to April is the dry season. Sunshine is very prominent for nearly the entire year and even during the rainy season. Humidity, however, is not very high due to the winds. The average temperature is around 25 °C with day temperatures rising to 32 °C. The average high and low temperatures in January are 28 °C and 22 °C, respectively, while in July they are 30 °C and 24 °C. The lowest night temperature recorded is 13 °C. The Caribbean sea waters in the vicinity generally maintain a temperature of about 27 °C.
When the British invaded the harbour town in 1744[verification needed], the town’s architectural buildings were destroyed[verification needed]. Subsequently, new structures were built in the town around the harbour area[verification needed] and the Swedes had also further added to the architectural beauty of the town in 1785 with more buildings, when they had occupied the town. Earlier to their occupation, the port was known as "Carénage". The Swedes renamed it as Gustavia in honour of their king Gustav III. It was then their prime trading center. The port maintained a neutral stance since the Caribbean war was on in the 18th century. They used it as trading post of contraband and the city of Gustavia prospered but this prosperity was short lived.
Musée Territorial de St.-Barthélemy is a historical museum known as the "St. Barts Municipal Museum" also called the "Wall House" (musée – bibliothèque) in Gustavia, which is located on the far end of La Pointe. The museum is housed in an old stone house, a two-storey building which has been refurbished. The island’s history relating to French, Swedish and British period of occupation is well presented in the museum with photographs, maps and paintings. Also on display are the ancestral costumes, antique tools, models of Creole houses and ancient fishing boats. It also houses a library.
Insects (from Latin insectum, a calque of Greek ἔντομον [éntomon], "cut into sections") are a class of invertebrates within the arthropod phylum that have a chitinous exoskeleton, a three-part body (head, thorax and abdomen), three pairs of jointed legs, compound eyes and one pair of antennae. They are the most diverse group of animals on the planet, including more than a million described species and representing more than half of all known living organisms. The number of extant species is estimated at between six and ten million, and potentially represent over 90% of the differing animal life forms on Earth. Insects may be found in nearly all environments, although only a small number of species reside in the oceans, a habitat dominated by another arthropod group, crustaceans.
The life cycles of insects vary but most hatch from eggs. Insect growth is constrained by the inelastic exoskeleton and development involves a series of molts. The immature stages can differ from the adults in structure, habit and habitat, and can include a passive pupal stage in those groups that undergo 4-stage metamorphosis (see holometabolism). Insects that undergo 3-stage metamorphosis lack a pupal stage and adults develop through a series of nymphal stages. The higher level relationship of the Hexapoda is unclear. Fossilized insects of enormous size have been found from the Paleozoic Era, including giant dragonflies with wingspans of 55 to 70 cm (22–28 in). The most diverse insect groups appear to have coevolved with flowering plants.
Adult insects typically move about by walking, flying, or sometimes swimming (see below, Locomotion). As it allows for rapid yet stable movement, many insects adopt a tripedal gait in which they walk with their legs touching the ground in alternating triangles. Insects are the only invertebrates to have evolved flight. Many insects spend at least part of their lives under water, with larval adaptations that include gills, and some adult insects are aquatic and have adaptations for swimming. Some species, such as water striders, are capable of walking on the surface of water. Insects are mostly solitary, but some, such as certain bees, ants and termites, are social and live in large, well-organized colonies. Some insects, such as earwigs, show maternal care, guarding their eggs and young. Insects can communicate with each other in a variety of ways. Male moths can sense the pheromones of female moths over great distances. Other species communicate with sounds: crickets stridulate, or rub their wings together, to attract a mate and repel other males. Lampyridae in the beetle order Coleoptera communicate with light.
Humans regard certain insects as pests, and attempt to control them using insecticides and a host of other techniques. Some insects damage crops by feeding on sap, leaves or fruits. A few parasitic species are pathogenic. Some insects perform complex ecological roles; blow-flies, for example, help consume carrion but also spread diseases. Insect pollinators are essential to the life-cycle of many flowering plant species on which most organisms, including humans, are at least partly dependent; without them, the terrestrial portion of the biosphere (including humans) would be devastated. Many other insects are considered ecologically beneficial as predators and a few provide direct economic benefit. Silkworms and bees have been used extensively by humans for the production of silk and honey, respectively. In some cultures, people eat the larvae or adults of certain insects.
The word "insect" comes from the Latin word insectum, meaning "with a notched or divided body", or literally "cut into", from the neuter singular perfect passive participle of insectare, "to cut into, to cut up", from in- "into" and secare "to cut"; because insects appear "cut into" three sections. Pliny the Elder introduced the Latin designation as a loan-translation of the Greek word ἔντομος (éntomos) or "insect" (as in entomology), which was Aristotle's term for this class of life, also in reference to their "notched" bodies. "Insect" first appears documented in English in 1601 in Holland's translation of Pliny. Translations of Aristotle's term also form the usual word for "insect" in Welsh (trychfil, from trychu "to cut" and mil, "animal"), Serbo-Croatian (zareznik, from rezati, "to cut"), Russian (насекомое nasekomoje, from seč'/-sekat', "to cut"), etc.
The higher-level phylogeny of the arthropods continues to be a matter of debate and research. In 2008, researchers at Tufts University uncovered what they believe is the world's oldest known full-body impression of a primitive flying insect, a 300 million-year-old specimen from the Carboniferous period. The oldest definitive insect fossil is the Devonian Rhyniognatha hirsti, from the 396-million-year-old Rhynie chert. It may have superficially resembled a modern-day silverfish insect. This species already possessed dicondylic mandibles (two articulations in the mandible), a feature associated with winged insects, suggesting that wings may already have evolved at this time. Thus, the first insects probably appeared earlier, in the Silurian period.
Late Carboniferous and Early Permian insect orders include both extant groups, their stem groups, and a number of Paleozoic groups, now extinct. During this era, some giant dragonfly-like forms reached wingspans of 55 to 70 cm (22 to 28 in), making them far larger than any living insect. This gigantism may have been due to higher atmospheric oxygen levels that allowed increased respiratory efficiency relative to today. The lack of flying vertebrates could have been another factor. Most extinct orders of insects developed during the Permian period that began around 270 million years ago. Many of the early groups became extinct during the Permian-Triassic extinction event, the largest mass extinction in the history of the Earth, around 252 million years ago.
Insects were among the earliest terrestrial herbivores and acted as major selection agents on plants. Plants evolved chemical defenses against this herbivory and the insects, in turn, evolved mechanisms to deal with plant toxins. Many insects make use of these toxins to protect themselves from their predators. Such insects often advertise their toxicity using warning colors. This successful evolutionary pattern has also been used by mimics. Over time, this has led to complex groups of coevolved species. Conversely, some interactions between plants and insects, like pollination, are beneficial to both organisms. Coevolution has led to the development of very specific mutualisms in such systems.
Insects can be divided into two groups historically treated as subclasses: wingless insects, known as Apterygota, and winged insects, known as Pterygota. The Apterygota consist of the primitively wingless order of the silverfish (Thysanura). Archaeognatha make up the Monocondylia based on the shape of their mandibles, while Thysanura and Pterygota are grouped together as Dicondylia. The Thysanura themselves possibly are not monophyletic, with the family Lepidotrichidae being a sister group to the Dicondylia (Pterygota and the remaining Thysanura).
Traditional morphology-based or appearance-based systematics have usually given the Hexapoda the rank of superclass,:180 and identified four groups within it: insects (Ectognatha), springtails (Collembola), Protura, and Diplura, the latter three being grouped together as the Entognatha on the basis of internalized mouth parts. Supraordinal relationships have undergone numerous changes with the advent of methods based on evolutionary history and genetic data. A recent theory is that the Hexapoda are polyphyletic (where the last common ancestor was not a member of the group), with the entognath classes having separate evolutionary histories from the Insecta. Many of the traditional appearance-based taxa have been shown to be paraphyletic, so rather than using ranks like subclass, superorder, and infraorder, it has proved better to use monophyletic groupings (in which the last common ancestor is a member of the group). The following represents the best-supported monophyletic groupings for the Insecta.
Paleoptera and Neoptera are the winged orders of insects differentiated by the presence of hardened body parts called sclerites, and in the Neoptera, muscles that allow their wings to fold flatly over the abdomen. Neoptera can further be divided into incomplete metamorphosis-based (Polyneoptera and Paraneoptera) and complete metamorphosis-based groups. It has proved difficult to clarify the relationships between the orders in Polyneoptera because of constant new findings calling for revision of the taxa. For example, the Paraneoptera have turned out to be more closely related to the Endopterygota than to the rest of the Exopterygota. The recent molecular finding that the traditional louse orders Mallophaga and Anoplura are derived from within Psocoptera has led to the new taxon Psocodea. Phasmatodea and Embiidina have been suggested to form the Eukinolabia. Mantodea, Blattodea, and Isoptera are thought to form a monophyletic group termed Dictyoptera.
The Exopterygota likely are paraphyletic in regard to the Endopterygota. Matters that have incurred controversy include Strepsiptera and Diptera grouped together as Halteria based on a reduction of one of the wing pairs – a position not well-supported in the entomological community. The Neuropterida are often lumped or split on the whims of the taxonomist. Fleas are now thought to be closely related to boreid mecopterans. Many questions remain in the basal relationships amongst endopterygote orders, particularly the Hymenoptera.
Though the true dimensions of species diversity remain uncertain, estimates range from 2.6–7.8 million species with a mean of 5.5 million. This probably represents less than 20% of all species on Earth[citation needed], and with only about 20,000 new species of all organisms being described each year, most species likely will remain undescribed for many years unless species descriptions increase in rate. About 850,000–1,000,000 of all described species are insects. Of the 24 orders of insects, four dominate in terms of numbers of described species, with at least 3 million species included in Coleoptera, Diptera, Hymenoptera and Lepidoptera. A recent study estimated the number of beetles at 0.9–2.1 million with a mean of 1.5 million.
Insects have segmented bodies supported by exoskeletons, the hard outer covering made mostly of chitin. The segments of the body are organized into three distinctive but interconnected units, or tagmata: a head, a thorax and an abdomen. The head supports a pair of sensory antennae, a pair of compound eyes, and, if present, one to three simple eyes (or ocelli) and three sets of variously modified appendages that form the mouthparts. The thorax has six segmented legs—one pair each for the prothorax, mesothorax and the metathorax segments making up the thorax—and, none, two or four wings. The abdomen consists of eleven segments, though in a few species of insects, these segments may be fused together or reduced in size. The abdomen also contains most of the digestive, respiratory, excretory and reproductive internal structures.:22–48 Considerable variation and many adaptations in the body parts of insects occur, especially wings, legs, antenna and mouthparts.
The head is enclosed in a hard, heavily sclerotized, unsegmented, exoskeletal head capsule, or epicranium, which contains most of the sensing organs, including the antennae, ocellus or eyes, and the mouthparts. Of all the insect orders, Orthoptera displays the most features found in other insects, including the sutures and sclerites. Here, the vertex, or the apex (dorsal region), is situated between the compound eyes for insects with a hypognathous and opisthognathous head. In prognathous insects, the vertex is not found between the compound eyes, but rather, where the ocelli are normally. This is because the primary axis of the head is rotated 90° to become parallel to the primary axis of the body. In some species, this region is modified and assumes a different name.:13
The thorax is a tagma composed of three sections, the prothorax, mesothorax and the metathorax. The anterior segment, closest to the head, is the prothorax, with the major features being the first pair of legs and the pronotum. The middle segment is the mesothorax, with the major features being the second pair of legs and the anterior wings. The third and most posterior segment, abutting the abdomen, is the metathorax, which features the third pair of legs and the posterior wings. Each segment is dilineated by an intersegmental suture. Each segment has four basic regions. The dorsal surface is called the tergum (or notum) to distinguish it from the abdominal terga. The two lateral regions are called the pleura (singular: pleuron) and the ventral aspect is called the sternum. In turn, the notum of the prothorax is called the pronotum, the notum for the mesothorax is called the mesonotum and the notum for the metathorax is called the metanotum. Continuing with this logic, the mesopleura and metapleura, as well as the mesosternum and metasternum, are used.
The abdomen is the largest tagma of the insect, which typically consists of 11–12 segments and is less strongly sclerotized than the head or thorax. Each segment of the abdomen is represented by a sclerotized tergum and sternum. Terga are separated from each other and from the adjacent sterna or pleura by membranes. Spiracles are located in the pleural area. Variation of this ground plan includes the fusion of terga or terga and sterna to form continuous dorsal or ventral shields or a conical tube. Some insects bear a sclerite in the pleural area called a laterotergite. Ventral sclerites are sometimes called laterosternites. During the embryonic stage of many insects and the postembryonic stage of primitive insects, 11 abdominal segments are present. In modern insects there is a tendency toward reduction in the number of the abdominal segments, but the primitive number of 11 is maintained during embryogenesis. Variation in abdominal segment number is considerable. If the Apterygota are considered to be indicative of the ground plan for pterygotes, confusion reigns: adult Protura have 12 segments, Collembola have 6. The orthopteran family Acrididae has 11 segments, and a fossil specimen of Zoraptera has a 10-segmented abdomen.
The insect outer skeleton, the cuticle, is made up of two layers: the epicuticle, which is a thin and waxy water resistant outer layer and contains no chitin, and a lower layer called the procuticle. The procuticle is chitinous and much thicker than the epicuticle and has two layers: an outer layer known as the exocuticle and an inner layer known as the endocuticle. The tough and flexible endocuticle is built from numerous layers of fibrous chitin and proteins, criss-crossing each other in a sandwich pattern, while the exocuticle is rigid and hardened.:22–24 The exocuticle is greatly reduced in many soft-bodied insects (e.g., caterpillars), especially during their larval stages.
Insects are the only invertebrates to have developed active flight capability, and this has played an important role in their success.:186 Their muscles are able to contract multiple times for each single nerve impulse, allowing the wings to beat faster than would ordinarily be possible. Having their muscles attached to their exoskeletons is more efficient and allows more muscle connections; crustaceans also use the same method, though all spiders use hydraulic pressure to extend their legs, a system inherited from their pre-arthropod ancestors. Unlike insects, though, most aquatic crustaceans are biomineralized with calcium carbonate extracted from the water.
The thoracic segments have one ganglion on each side, which are connected into a pair, one pair per segment. This arrangement is also seen in the abdomen but only in the first eight segments. Many species of insects have reduced numbers of ganglia due to fusion or reduction. Some cockroaches have just six ganglia in the abdomen, whereas the wasp Vespa crabro has only two in the thorax and three in the abdomen. Some insects, like the house fly Musca domestica, have all the body ganglia fused into a single large thoracic ganglion.
At least a few insects have nociceptors, cells that detect and transmit sensations of pain. This was discovered in 2003 by studying the variation in reactions of larvae of the common fruitfly Drosophila to the touch of a heated probe and an unheated one. The larvae reacted to the touch of the heated probe with a stereotypical rolling behavior that was not exhibited when the larvae were touched by the unheated probe. Although nociception has been demonstrated in insects, there is no consensus that insects feel pain consciously
The salivary glands (element 30 in numbered diagram) in an insect's mouth produce saliva. The salivary ducts lead from the glands to the reservoirs and then forward through the head to an opening called the salivarium, located behind the hypopharynx. By moving its mouthparts (element 32 in numbered diagram) the insect can mix its food with saliva. The mixture of saliva and food then travels through the salivary tubes into the mouth, where it begins to break down. Some insects, like flies, have extra-oral digestion. Insects using extra-oral digestion expel digestive enzymes onto their food to break it down. This strategy allows insects to extract a significant proportion of the available nutrients from the food source.:31 The gut is where almost all of insects' digestion takes place. It can be divided into the foregut, midgut and hindgut.
Once food leaves the crop, it passes to the midgut (element 13 in numbered diagram), also known as the mesenteron, where the majority of digestion takes place. Microscopic projections from the midgut wall, called microvilli, increase the surface area of the wall and allow more nutrients to be absorbed; they tend to be close to the origin of the midgut. In some insects, the role of the microvilli and where they are located may vary. For example, specialized microvilli producing digestive enzymes may more likely be near the end of the midgut, and absorption near the origin or beginning of the midgut.:32
In the hindgut (element 16 in numbered diagram), or proctodaeum, undigested food particles are joined by uric acid to form fecal pellets. The rectum absorbs 90% of the water in these fecal pellets, and the dry pellet is then eliminated through the anus (element 17), completing the process of digestion. The uric acid is formed using hemolymph waste products diffused from the Malpighian tubules (element 20). It is then emptied directly into the alimentary canal, at the junction between the midgut and hindgut. The number of Malpighian tubules possessed by a given insect varies between species, ranging from only two tubules in some insects to over 100 tubules in others.:71–72, 78–80
The reproductive system of female insects consist of a pair of ovaries, accessory glands, one or more spermathecae, and ducts connecting these parts. The ovaries are made up of a number of egg tubes, called ovarioles, which vary in size and number by species. The number of eggs that the insect is able to make vary by the number of ovarioles with the rate that eggs can be develop being also influenced by ovariole design. Female insects are able make eggs, receive and store sperm, manipulate sperm from different males, and lay eggs. Accessory glands or glandular parts of the oviducts produce a variety of substances for sperm maintenance, transport and fertilization, as well as for protection of eggs. They can produce glue and protective substances for coating eggs or tough coverings for a batch of eggs called oothecae. Spermathecae are tubes or sacs in which sperm can be stored between the time of mating and the time an egg is fertilized.:880
For males, the reproductive system is the testis, suspended in the body cavity by tracheae and the fat body. Most male insects have a pair of testes, inside of which are sperm tubes or follicles that are enclosed within a membranous sac. The follicles connect to the vas deferens by the vas efferens, and the two tubular vasa deferentia connect to a median ejaculatory duct that leads to the outside. A portion of the vas deferens is often enlarged to form the seminal vesicle, which stores the sperm before they are discharged into the female. The seminal vesicles have glandular linings that secrete nutrients for nourishment and maintenance of the sperm. The ejaculatory duct is derived from an invagination of the epidermal cells during development and, as a result, has a cuticular lining. The terminal portion of the ejaculatory duct may be sclerotized to form the intromittent organ, the aedeagus. The remainder of the male reproductive system is derived from embryonic mesoderm, except for the germ cells, or spermatogonia, which descend from the primordial pole cells very early during embryogenesis.:885
Insect respiration is accomplished without lungs. Instead, the insect respiratory system uses a system of internal tubes and sacs through which gases either diffuse or are actively pumped, delivering oxygen directly to tissues that need it via their trachea (element 8 in numbered diagram). Since oxygen is delivered directly, the circulatory system is not used to carry oxygen, and is therefore greatly reduced. The insect circulatory system has no veins or arteries, and instead consists of little more than a single, perforated dorsal tube which pulses peristaltically. Toward the thorax, the dorsal tube (element 14) divides into chambers and acts like the insect's heart. The opposite end of the dorsal tube is like the aorta of the insect circulating the hemolymph, arthropods' fluid analog of blood, inside the body cavity.:61–65 Air is taken in through openings on the sides of the abdomen called spiracles.
There are many different patterns of gas exchange demonstrated by different groups of insects. Gas exchange patterns in insects can range from continuous and diffusive ventilation, to discontinuous gas exchange.:65–68 During continuous gas exchange, oxygen is taken in and carbon dioxide is released in a continuous cycle. In discontinuous gas exchange, however, the insect takes in oxygen while it is active and small amounts of carbon dioxide are released when the insect is at rest. Diffusive ventilation is simply a form of continuous gas exchange that occurs by diffusion rather than physically taking in the oxygen. Some species of insect that are submerged also have adaptations to aid in respiration. As larvae, many insects have gills that can extract oxygen dissolved in water, while others need to rise to the water surface to replenish air supplies which may be held or trapped in special structures.
The majority of insects hatch from eggs. The fertilization and development takes place inside the egg, enclosed by a shell (chorion) that consists of maternal tissue. In contrast to eggs of other arthropods, most insect eggs are drought resistant. This is because inside the chorion two additional membranes develop from embryonic tissue, the amnion and the serosa. This serosa secretes a cuticle rich in chitin that protects the embryo against desiccation. In Schizophora however the serosa does not develop, but these flies lay their eggs in damp places, such as rotting matter. Some species of insects, like the cockroach Blaptica dubia, as well as juvenile aphids and tsetse flies, are ovoviviparous. The eggs of ovoviviparous animals develop entirely inside the female, and then hatch immediately upon being laid. Some other species, such as those in the genus of cockroaches known as Diploptera, are viviparous, and thus gestate inside the mother and are born alive.:129, 131, 134–135 Some insects, like parasitic wasps, show polyembryony, where a single fertilized egg divides into many and in some cases thousands of separate embryos.:136–137 Insects may be univoltine, bivoltine or multivoltine, i.e. they may have one, two or many broods (generations) in a year.
Other developmental and reproductive variations include haplodiploidy, polymorphism, paedomorphosis or peramorphosis, sexual dimorphism, parthenogenesis and more rarely hermaphroditism.:143 In haplodiploidy, which is a type of sex-determination system, the offspring's sex is determined by the number of sets of chromosomes an individual receives. This system is typical in bees and wasps. Polymorphism is where a species may have different morphs or forms, as in the oblong winged katydid, which has four different varieties: green, pink and yellow or tan. Some insects may retain phenotypes that are normally only seen in juveniles; this is called paedomorphosis. In peramorphosis, an opposite sort of phenomenon, insects take on previously unseen traits after they have matured into adults. Many insects display sexual dimorphism, in which males and females have notably different appearances, such as the moth Orgyia recens as an exemplar of sexual dimorphism in insects.
Some insects use parthenogenesis, a process in which the female can reproduce and give birth without having the eggs fertilized by a male. Many aphids undergo a form of parthenogenesis, called cyclical parthenogenesis, in which they alternate between one or many generations of asexual and sexual reproduction. In summer, aphids are generally female and parthenogenetic; in the autumn, males may be produced for sexual reproduction. Other insects produced by parthenogenesis are bees, wasps and ants, in which they spawn males. However, overall, most individuals are female, which are produced by fertilization. The males are haploid and the females are diploid. More rarely, some insects display hermaphroditism, in which a given individual has both male and female reproductive organs.
Hemimetabolous insects, those with incomplete metamorphosis, change gradually by undergoing a series of molts. An insect molts when it outgrows its exoskeleton, which does not stretch and would otherwise restrict the insect's growth. The molting process begins as the insect's epidermis secretes a new epicuticle inside the old one. After this new epicuticle is secreted, the epidermis releases a mixture of enzymes that digests the endocuticle and thus detaches the old cuticle. When this stage is complete, the insect makes its body swell by taking in a large quantity of water or air, which makes the old cuticle split along predefined weaknesses where the old exocuticle was thinnest.:142
Holometabolism, or complete metamorphosis, is where the insect changes in four stages, an egg or embryo, a larva, a pupa and the adult or imago. In these species, an egg hatches to produce a larva, which is generally worm-like in form. This worm-like form can be one of several varieties: eruciform (caterpillar-like), scarabaeiform (grub-like), campodeiform (elongated, flattened and active), elateriform (wireworm-like) or vermiform (maggot-like). The larva grows and eventually becomes a pupa, a stage marked by reduced movement and often sealed within a cocoon. There are three types of pupae: obtect, exarate or coarctate. Obtect pupae are compact, with the legs and other appendages enclosed. Exarate pupae have their legs and other appendages free and extended. Coarctate pupae develop inside the larval skin.:151 Insects undergo considerable change in form during the pupal stage, and emerge as adults. Butterflies are a well-known example of insects that undergo complete metamorphosis, although most insects use this life cycle. Some insects have evolved this system to hypermetamorphosis.
Many insects possess very sensitive and, or specialized organs of perception. Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers. The yellow paper wasp (Polistes versicolor) is known for its wagging movements as a form of communication within the colony; it can waggle with a frequency of 10.6±2.1 Hz (n=190). These wagging movements can signal the arrival of new material into the nest and aggression between workers can be used to stimulate others to increase foraging expeditions. There is a pronounced tendency for there to be a trade-off between visual acuity and chemical or tactile acuity, such that most insects with well-developed eyes have reduced or simple antennae, and vice versa. There are a variety of different mechanisms by which insects perceive sound, while the patterns are not universal, insects can generally hear sound if they can produce it. Different insect species can have varying hearing, though most insects can hear only a narrow range of frequencies related to the frequency of the sounds they can produce. Mosquitoes have been found to hear up to 2 kHz., and some grasshoppers can hear up to 50 kHz. Certain predatory and parasitic insects can detect the characteristic sounds made by their prey or hosts, respectively. For instance, some nocturnal moths can perceive the ultrasonic emissions of bats, which helps them avoid predation.:87–94 Insects that feed on blood have special sensory structures that can detect infrared emissions, and use them to home in on their hosts.
Some insects display a rudimentary sense of numbers, such as the solitary wasps that prey upon a single species. The mother wasp lays her eggs in individual cells and provides each egg with a number of live caterpillars on which the young feed when hatched. Some species of wasp always provide five, others twelve, and others as high as twenty-four caterpillars per cell. The number of caterpillars is different among species, but always the same for each sex of larva. The male solitary wasp in the genus Eumenes is smaller than the female, so the mother of one species supplies him with only five caterpillars; the larger female receives ten caterpillars in her cell.
A few insects, such as members of the families Poduridae and Onychiuridae (Collembola), Mycetophilidae (Diptera) and the beetle families Lampyridae, Phengodidae, Elateridae and Staphylinidae are bioluminescent. The most familiar group are the fireflies, beetles of the family Lampyridae. Some species are able to control this light generation to produce flashes. The function varies with some species using them to attract mates, while others use them to lure prey. Cave dwelling larvae of Arachnocampa (Mycetophilidae, Fungus gnats) glow to lure small flying insects into sticky strands of silk. Some fireflies of the genus Photuris mimic the flashing of female Photinus species to attract males of that species, which are then captured and devoured. The colors of emitted light vary from dull blue (Orfelia fultoni, Mycetophilidae) to the familiar greens and the rare reds (Phrixothrix tiemanni, Phengodidae).
Most insects, except some species of cave crickets, are able to perceive light and dark. Many species have acute vision capable of detecting minute movements. The eyes may include simple eyes or ocelli as well as compound eyes of varying sizes. Many species are able to detect light in the infrared, ultraviolet and the visible light wavelengths. Color vision has been demonstrated in many species and phylogenetic analysis suggests that UV-green-blue trichromacy existed from at least the Devonian period between 416 and 359 million years ago.
Insects were the earliest organisms to produce and sense sounds. Insects make sounds mostly by mechanical action of appendages. In grasshoppers and crickets, this is achieved by stridulation. Cicadas make the loudest sounds among the insects by producing and amplifying sounds with special modifications to their body and musculature. The African cicada Brevisana brevis has been measured at 106.7 decibels at a distance of 50 cm (20 in). Some insects, such as the Helicoverpa zeamoths, hawk moths and Hedylid butterflies, can hear ultrasound and take evasive action when they sense that they have been detected by bats. Some moths produce ultrasonic clicks that were once thought to have a role in jamming bat echolocation. The ultrasonic clicks were subsequently found to be produced mostly by unpalatable moths to warn bats, just as warning colorations are used against predators that hunt by sight. Some otherwise palatable moths have evolved to mimic these calls. More recently, the claim that some moths can jam bat sonar has been revisited. Ultrasonic recording and high-speed infrared videography of bat-moth interactions suggest the palatable tiger moth really does defend against attacking big brown bats using ultrasonic clicks that jam bat sonar.
Very low sounds are also produced in various species of Coleoptera, Hymenoptera, Lepidoptera, Mantodea and Neuroptera. These low sounds are simply the sounds made by the insect's movement. Through microscopic stridulatory structures located on the insect's muscles and joints, the normal sounds of the insect moving are amplified and can be used to warn or communicate with other insects. Most sound-making insects also have tympanal organs that can perceive airborne sounds. Some species in Hemiptera, such as the corixids (water boatmen), are known to communicate via underwater sounds. Most insects are also able to sense vibrations transmitted through surfaces.
Some species use vibrations for communicating within members of the same species, such as to attract mates as in the songs of the shield bug Nezara viridula. Vibrations can also be used to communicate between entirely different species; lycaenid (gossamer-winged butterfly) caterpillars which are myrmecophilous (living in a mutualistic association with ants) communicate with ants in this way. The Madagascar hissing cockroach has the ability to press air through its spiracles to make a hissing noise as a sign of aggression; the Death's-head Hawkmoth makes a squeaking noise by forcing air out of their pharynx when agitated, which may also reduce aggressive worker honey bee behavior when the two are in close proximity.
Chemical communications in animals rely on a variety of aspects including taste and smell. Chemoreception is the physiological response of a sense organ (i.e. taste or smell) to a chemical stimulus where the chemicals act as signals to regulate the state or activity of a cell. A semiochemical is a message-carrying chemical that is meant to attract, repel, and convey information. Types of semiochemicals include pheromones and kairomones. One example is the butterfly Phengaris arion which uses chemical signals as a form of mimicry to aid in predation.
In addition to the use of sound for communication, a wide range of insects have evolved chemical means for communication. These chemicals, termed semiochemicals, are often derived from plant metabolites include those meant to attract, repel and provide other kinds of information. Pheromones, a type of semiochemical, are used for attracting mates of the opposite sex, for aggregating conspecific individuals of both sexes, for deterring other individuals from approaching, to mark a trail, and to trigger aggression in nearby individuals. Allomonea benefit their producer by the effect they have upon the receiver. Kairomones benefit their receiver instead of their producer. Synomones benefit the producer and the receiver. While some chemicals are targeted at individuals of the same species, others are used for communication across species. The use of scents is especially well known to have developed in social insects.:96–105
Social insects, such as termites, ants and many bees and wasps, are the most familiar species of eusocial animal. They live together in large well-organized colonies that may be so tightly integrated and genetically similar that the colonies of some species are sometimes considered superorganisms. It is sometimes argued that the various species of honey bee are the only invertebrates (and indeed one of the few non-human groups) to have evolved a system of abstract symbolic communication where a behavior is used to represent and convey specific information about something in the environment. In this communication system, called dance language, the angle at which a bee dances represents a direction relative to the sun, and the length of the dance represents the distance to be flown.:309–311 Though perhaps not as advanced as honey bees, bumblebees also potentially have some social communication behaviors. Bombus terrestris, for example, exhibit a faster learning curve for visiting unfamiliar, yet rewarding flowers, when they can see a conspecific foraging on the same species.
Only insects which live in nests or colonies demonstrate any true capacity for fine-scale spatial orientation or homing. This can allow an insect to return unerringly to a single hole a few millimeters in diameter among thousands of apparently identical holes clustered together, after a trip of up to several kilometers' distance. In a phenomenon known as philopatry, insects that hibernate have shown the ability to recall a specific location up to a year after last viewing the area of interest. A few insects seasonally migrate large distances between different geographic regions (e.g., the overwintering areas of the Monarch butterfly).:14
The eusocial insects build nest, guard eggs, and provide food for offspring full-time (see Eusociality). Most insects, however, lead short lives as adults, and rarely interact with one another except to mate or compete for mates. A small number exhibit some form of parental care, where they will at least guard their eggs, and sometimes continue guarding their offspring until adulthood, and possibly even feeding them. Another simple form of parental care is to construct a nest (a burrow or an actual construction, either of which may be simple or complex), store provisions in it, and lay an egg upon those provisions. The adult does not contact the growing offspring, but it nonetheless does provide food. This sort of care is typical for most species of bees and various types of wasps.
Insects are the only group of invertebrates to have developed flight. The evolution of insect wings has been a subject of debate. Some entomologists suggest that the wings are from paranotal lobes, or extensions from the insect's exoskeleton called the nota, called the paranotal theory. Other theories are based on a pleural origin. These theories include suggestions that wings originated from modified gills, spiracular flaps or as from an appendage of the epicoxa. The epicoxal theory suggests the insect wings are modified epicoxal exites, a modified appendage at the base of the legs or coxa. In the Carboniferous age, some of the Meganeura dragonflies had as much as a 50 cm (20 in) wide wingspan. The appearance of gigantic insects has been found to be consistent with high atmospheric oxygen. The respiratory system of insects constrains their size, however the high oxygen in the atmosphere allowed larger sizes. The largest flying insects today are much smaller and include several moth species such as the Atlas moth and the White Witch (Thysania agrippina).
Many adult insects use six legs for walking and have adopted a tripedal gait. The tripedal gait allows for rapid walking while always having a stable stance and has been studied extensively in cockroaches. The legs are used in alternate triangles touching the ground. For the first step, the middle right leg and the front and rear left legs are in contact with the ground and move the insect forward, while the front and rear right leg and the middle left leg are lifted and moved forward to a new position. When they touch the ground to form a new stable triangle the other legs can be lifted and brought forward in turn and so on. The purest form of the tripedal gait is seen in insects moving at high speeds. However, this type of locomotion is not rigid and insects can adapt a variety of gaits. For example, when moving slowly, turning, or avoiding obstacles, four or more feet may be touching the ground. Insects can also adapt their gait to cope with the loss of one or more limbs.
Cockroaches are among the fastest insect runners and, at full speed, adopt a bipedal run to reach a high velocity in proportion to their body size. As cockroaches move very quickly, they need to be video recorded at several hundred frames per second to reveal their gait. More sedate locomotion is seen in the stick insects or walking sticks (Phasmatodea). A few insects have evolved to walk on the surface of the water, especially members of the Gerridae family, commonly known as water striders. A few species of ocean-skaters in the genus Halobates even live on the surface of open oceans, a habitat that has few insect species.
Many of these species have adaptations to help in under-water locomotion. Water beetles and water bugs have legs adapted into paddle-like structures. Dragonfly naiads use jet propulsion, forcibly expelling water out of their rectal chamber. Some species like the water striders are capable of walking on the surface of water. They can do this because their claws are not at the tips of the legs as in most insects, but recessed in a special groove further up the leg; this prevents the claws from piercing the water's surface film. Other insects such as the Rove beetle Stenus are known to emit pygidial gland secretions that reduce surface tension making it possible for them to move on the surface of water by Marangoni propulsion (also known by the German term Entspannungsschwimmen).
Insect ecology is the scientific study of how insects, individually or as a community, interact with the surrounding environment or ecosystem.:3 Insects play one of the most important roles in their ecosystems, which includes many roles, such as soil turning and aeration, dung burial, pest control, pollination and wildlife nutrition. An example is the beetles, which are scavengers that feed on dead animals and fallen trees and thereby recycle biological materials into forms found useful by other organisms. These insects, and others, are responsible for much of the process by which topsoil is created.:3, 218–228
Camouflage is an important defense strategy, which involves the use of coloration or shape to blend into the surrounding environment. This sort of protective coloration is common and widespread among beetle families, especially those that feed on wood or vegetation, such as many of the leaf beetles (family Chrysomelidae) or weevils. In some of these species, sculpturing or various colored scales or hairs cause the beetle to resemble bird dung or other inedible objects. Many of those that live in sandy environments blend in with the coloration of the substrate. Most phasmids are known for effectively replicating the forms of sticks and leaves, and the bodies of some species (such as O. macklotti and Palophus centaurus) are covered in mossy or lichenous outgrowths that supplement their disguise. Some species have the ability to change color as their surroundings shift (B. scabrinota, T. californica). In a further behavioral adaptation to supplement crypsis, a number of species have been noted to perform a rocking motion where the body is swayed from side to side that is thought to reflect the movement of leaves or twigs swaying in the breeze. Another method by which stick insects avoid predation and resemble twigs is by feigning death (catalepsy), where the insect enters a motionless state that can be maintained for a long period. The nocturnal feeding habits of adults also aids Phasmatodea in remaining concealed from predators.
Another defense that often uses color or shape to deceive potential enemies is mimicry. A number of longhorn beetles (family Cerambycidae) bear a striking resemblance to wasps, which helps them avoid predation even though the beetles are in fact harmless. Batesian and Müllerian mimicry complexes are commonly found in Lepidoptera. Genetic polymorphism and natural selection give rise to otherwise edible species (the mimic) gaining a survival advantage by resembling inedible species (the model). Such a mimicry complex is referred to as Batesian and is most commonly known by the mimicry by the limenitidine Viceroy butterfly of the inedible danaine Monarch. Later research has discovered that the Viceroy is, in fact more toxic than the Monarch and this resemblance should be considered as a case of Müllerian mimicry. In Müllerian mimicry, inedible species, usually within a taxonomic order, find it advantageous to resemble each other so as to reduce the sampling rate by predators who need to learn about the insects' inedibility. Taxa from the toxic genus Heliconius form one of the most well known Müllerian complexes.
Chemical defense is another important defense found amongst species of Coleoptera and Lepidoptera, usually being advertised by bright colors, such as the Monarch butterfly. They obtain their toxicity by sequestering the chemicals from the plants they eat into their own tissues. Some Lepidoptera manufacture their own toxins. Predators that eat poisonous butterflies and moths may become sick and vomit violently, learning not to eat those types of species; this is actually the basis of Müllerian mimicry. A predator who has previously eaten a poisonous lepidopteran may avoid other species with similar markings in the future, thus saving many other species as well. Some ground beetles of the Carabidae family can spray chemicals from their abdomen with great accuracy, to repel predators.
Pollination is the process by which pollen is transferred in the reproduction of plants, thereby enabling fertilisation and sexual reproduction. Most flowering plants require an animal to do the transportation. While other animals are included as pollinators, the majority of pollination is done by insects. Because insects usually receive benefit for the pollination in the form of energy rich nectar it is a grand example of mutualism. The various flower traits (and combinations thereof) that differentially attract one type of pollinator or another are known as pollination syndromes. These arose through complex plant-animal adaptations. Pollinators find flowers through bright colorations, including ultraviolet, and attractant pheromones. The study of pollination by insects is known as anthecology.
Many insects are considered pests by humans. Insects commonly regarded as pests include those that are parasitic (e.g. lice, bed bugs), transmit diseases (mosquitoes, flies), damage structures (termites), or destroy agricultural goods (locusts, weevils). Many entomologists are involved in various forms of pest control, as in research for companies to produce insecticides, but increasingly rely on methods of biological pest control, or biocontrol. Biocontrol uses one organism to reduce the population density of another organism — the pest — and is considered a key element of integrated pest management.
Although pest insects attract the most attention, many insects are beneficial to the environment and to humans. Some insects, like wasps, bees, butterflies and ants, pollinate flowering plants. Pollination is a mutualistic relationship between plants and insects. As insects gather nectar from different plants of the same species, they also spread pollen from plants on which they have previously fed. This greatly increases plants' ability to cross-pollinate, which maintains and possibly even improves their evolutionary fitness. This ultimately affects humans since ensuring healthy crops is critical to agriculture. As well as pollination ants help with seed distribution of plants. This helps to spread the plants which increases plant diversity. This leads to an overall better environment. A serious environmental problem is the decline of populations of pollinator insects, and a number of species of insects are now cultured primarily for pollination management in order to have sufficient pollinators in the field, orchard or greenhouse at bloom time.:240–243 Another solution, as shown in Delaware, has been to raise native plants to help support native pollinators like L. vierecki. Insects also produce useful substances such as honey, wax, lacquer and silk. Honey bees have been cultured by humans for thousands of years for honey, although contracting for crop pollination is becoming more significant for beekeepers. The silkworm has greatly affected human history, as silk-driven trade established relationships between China and the rest of the world.
Insectivorous insects, or insects which feed on other insects, are beneficial to humans because they eat insects that could cause damage to agriculture and human structures. For example, aphids feed on crops and cause problems for farmers, but ladybugs feed on aphids, and can be used as a means to get significantly reduce pest aphid populations. While birds are perhaps more visible predators of insects, insects themselves account for the vast majority of insect consumption. Ants also help control animal populations by consuming small vertebrates. Without predators to keep them in check, insects can undergo almost unstoppable population explosions.:328–348:400
Insects play important roles in biological research. For example, because of its small size, short generation time and high fecundity, the common fruit fly Drosophila melanogaster is a model organism for studies in the genetics of higher eukaryotes. D. melanogaster has been an essential part of studies into principles like genetic linkage, interactions between genes, chromosomal genetics, development, behavior and evolution. Because genetic systems are well conserved among eukaryotes, understanding basic cellular processes like DNA replication or transcription in fruit flies can help to understand those processes in other eukaryotes, including humans. The genome of D. melanogaster was sequenced in 2000, reflecting the organism's important role in biological research. It was found that 70% of the fly genome is similar to the human genome, supporting the evolution theory.
In some cultures, insects, especially deep-fried cicadas, are considered to be delicacies, while in other places they form part of the normal diet. Insects have a high protein content for their mass, and some authors suggest their potential as a major source of protein in human nutrition.:10–13 In most first-world countries, however, entomophagy (the eating of insects), is taboo. Since it is impossible to entirely eliminate pest insects from the human food chain, insects are inadvertently present in many foods, especially grains. Food safety laws in many countries do not prohibit insect parts in food, but rather limit their quantity. According to cultural materialist anthropologist Marvin Harris, the eating of insects is taboo in cultures that have other protein sources such as fish or livestock.
Scarab beetles held religious and cultural symbolism in Old Egypt, Greece and some shamanistic Old World cultures. The ancient Chinese regarded cicadas as symbols of rebirth or immortality. In Mesopotamian literature, the epic poem of Gilgamesh has allusions to Odonata which signify the impossibility of immortality. Amongst the Aborigines of Australia of the Arrernte language groups, honey ants and witchety grubs served as personal clan totems. In the case of the 'San' bush-men of the Kalahari, it is the praying mantis which holds much cultural significance including creation and zen-like patience in waiting.:9
Bacteria (i/bækˈtɪəriə/; singular: bacterium) constitute a large domain of prokaryotic microorganisms. Typically a few micrometres in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste, and the deep portions of Earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. They are also known to have flourished in manned spacecraft.
There are typically 40 million bacterial cells in a gram of soil and a million bacterial cells in a millilitre of fresh water. There are approximately 5×1030 bacteria on Earth, forming a biomass which exceeds that of all plants and animals. Bacteria are vital in recycling nutrients, with many of the stages in nutrient cycles dependent on these organisms, such as the fixation of nitrogen from the atmosphere and putrefaction. In the biological communities surrounding hydrothermal vents and cold seeps, bacteria provide the nutrients needed to sustain life by converting dissolved compounds, such as hydrogen sulphide and methane, to energy. On 17 March 2013, researchers reported data that suggested bacterial life forms thrive in the Mariana Trench, which with a depth of up to 11 kilometres is the deepest part of the Earth's oceans. Other researchers reported related studies that microbes thrive inside rocks up to 580 metres below the sea floor under 2.6 kilometres of ocean off the coast of the northwestern United States. According to one of the researchers, "You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are."
There are approximately ten times as many bacterial cells in the human flora as there are human cells in the body, with the largest number of the human flora being in the gut flora, and a large number on the skin. The vast majority of the bacteria in the body are rendered harmless by the protective effects of the immune system, and some are beneficial. However, several species of bacteria are pathogenic and cause infectious diseases, including cholera, syphilis, anthrax, leprosy, and bubonic plague. The most common fatal bacterial diseases are respiratory infections, with tuberculosis alone killing about 2 million people per year, mostly in sub-Saharan Africa. In developed countries, antibiotics are used to treat bacterial infections and are also used in farming, making antibiotic resistance a growing problem. In industry, bacteria are important in sewage treatment and the breakdown of oil spills, the production of cheese and yogurt through fermentation, and the recovery of gold, palladium, copper and other metals in the mining sector, as well as in biotechnology, and the manufacture of antibiotics and other chemicals.
Once regarded as plants constituting the class Schizomycetes, bacteria are now classified as prokaryotes. Unlike cells of animals and other eukaryotes, bacterial cells do not contain a nucleus and rarely harbour membrane-bound organelles. Although the term bacteria traditionally included all prokaryotes, the scientific classification changed after the discovery in the 1990s that prokaryotes consist of two very different groups of organisms that evolved from an ancient common ancestor. These evolutionary domains are called Bacteria and Archaea.
The ancestors of modern bacteria were unicellular microorganisms that were the first forms of life to appear on Earth, about 4 billion years ago. For about 3 billion years, most organisms were microscopic, and bacteria and archaea were the dominant forms of life. In 2008, fossils of macroorganisms were discovered and named as the Francevillian biota. Although bacterial fossils exist, such as stromatolites, their lack of distinctive morphology prevents them from being used to examine the history of bacterial evolution, or to date the time of origin of a particular bacterial species. However, gene sequences can be used to reconstruct the bacterial phylogeny, and these studies indicate that bacteria diverged first from the archaeal/eukaryotic lineage. Bacteria were also involved in the second great evolutionary divergence, that of the archaea and eukaryotes. Here, eukaryotes resulted from the entering of ancient bacteria into endosymbiotic associations with the ancestors of eukaryotic cells, which were themselves possibly related to the Archaea. This involved the engulfment by proto-eukaryotic cells of alphaproteobacterial symbionts to form either mitochondria or hydrogenosomes, which are still found in all known Eukarya (sometimes in highly reduced form, e.g. in ancient "amitochondrial" protozoa). Later on, some eukaryotes that already contained mitochondria also engulfed cyanobacterial-like organisms. This led to the formation of chloroplasts in algae and plants. There are also some algae that originated from even later endosymbiotic events. Here, eukaryotes engulfed a eukaryotic algae that developed into a "second-generation" plastid. This is known as secondary endosymbiosis.
Bacteria display a wide diversity of shapes and sizes, called morphologies. Bacterial cells are about one-tenth the size of eukaryotic cells and are typically 0.5–5.0 micrometres in length. However, a few species are visible to the unaided eye — for example, Thiomargarita namibiensis is up to half a millimetre long and Epulopiscium fishelsoni reaches 0.7 mm. Among the smallest bacteria are members of the genus Mycoplasma, which measure only 0.3 micrometres, as small as the largest viruses. Some bacteria may be even smaller, but these ultramicrobacteria are not well-studied.
Most bacterial species are either spherical, called cocci (sing. coccus, from Greek kókkos, grain, seed), or rod-shaped, called bacilli (sing. bacillus, from Latin baculus, stick). Elongation is associated with swimming. Some bacteria, called vibrio, are shaped like slightly curved rods or comma-shaped; others can be spiral-shaped, called spirilla, or tightly coiled, called spirochaetes. A small number of species even have tetrahedral or cuboidal shapes. More recently, some bacteria were discovered deep under Earth's crust that grow as branching filamentous types with a star-shaped cross-section. The large surface area to volume ratio of this morphology may give these bacteria an advantage in nutrient-poor environments. This wide variety of shapes is determined by the bacterial cell wall and cytoskeleton, and is important because it can influence the ability of bacteria to acquire nutrients, attach to surfaces, swim through liquids and escape predators.
Many bacterial species exist simply as single cells, others associate in characteristic patterns: Neisseria form diploids (pairs), Streptococcus form chains, and Staphylococcus group together in "bunch of grapes" clusters. Bacteria can also be elongated to form filaments, for example the Actinobacteria. Filamentous bacteria are often surrounded by a sheath that contains many individual cells. Certain types, such as species of the genus Nocardia, even form complex, branched filaments, similar in appearance to fungal mycelia.
Bacteria often attach to surfaces and form dense aggregations called biofilms or bacterial mats. These films can range from a few micrometers in thickness to up to half a meter in depth, and may contain multiple species of bacteria, protists and archaea. Bacteria living in biofilms display a complex arrangement of cells and extracellular components, forming secondary structures, such as microcolonies, through which there are networks of channels to enable better diffusion of nutrients. In natural environments, such as soil or the surfaces of plants, the majority of bacteria are bound to surfaces in biofilms. Biofilms are also important in medicine, as these structures are often present during chronic bacterial infections or in infections of implanted medical devices, and bacteria protected within biofilms are much harder to kill than individual isolated bacteria.
Even more complex morphological changes are sometimes possible. For example, when starved of amino acids, Myxobacteria detect surrounding cells in a process known as quorum sensing, migrate toward each other, and aggregate to form fruiting bodies up to 500 micrometres long and containing approximately 100,000 bacterial cells. In these fruiting bodies, the bacteria perform separate tasks; this type of cooperation is a simple type of multicellular organisation. For example, about one in 10 cells migrate to the top of these fruiting bodies and differentiate into a specialised dormant state called myxospores, which are more resistant to drying and other adverse environmental conditions than are ordinary cells.
The bacterial cell is surrounded by a cell membrane (also known as a lipid, cytoplasmic or plasma membrane). This membrane encloses the contents of the cell and acts as a barrier to hold nutrients, proteins and other essential components of the cytoplasm within the cell. As they are prokaryotes, bacteria do not usually have membrane-bound organelles in their cytoplasm, and thus contain few large intracellular structures. They lack a true nucleus, mitochondria, chloroplasts and the other organelles present in eukaryotic cells. Bacteria were once seen as simple bags of cytoplasm, but structures such as the prokaryotic cytoskeleton and the localization of proteins to specific locations within the cytoplasm that give bacteria some complexity have been discovered. These subcellular levels of organization have been called "bacterial hyperstructures".
Many important biochemical reactions, such as energy generation, use concentration gradients across membranes. The general lack of internal membranes in bacteria means reactions such as electron transport occur across the cell membrane between the cytoplasm and the periplasmic space. However, in many photosynthetic bacteria the plasma membrane is highly folded and fills most of the cell with layers of light-gathering membrane. These light-gathering complexes may even form lipid-enclosed structures called chlorosomes in green sulfur bacteria. Other proteins import nutrients across the cell membrane, or expel undesired molecules from the cytoplasm.
Bacteria do not have a membrane-bound nucleus, and their genetic material is typically a single circular DNA chromosome located in the cytoplasm in an irregularly shaped body called the nucleoid. The nucleoid contains the chromosome with its associated proteins and RNA. The phylum Planctomycetes and candidate phylum Poribacteria may be exceptions to the general absence of internal membranes in bacteria, because they appear to have a double membrane around their nucleoids and contain other membrane-bound cellular structures. Like all living organisms, bacteria contain ribosomes, often grouped in chains called polyribosomes, for the production of proteins, but the structure of the bacterial ribosome is different from that of eukaryotes and Archaea. Bacterial ribosomes have a sedimentation rate of 70S (measured in Svedberg units): their subunits have rates of 30S and 50S. Some antibiotics bind specifically to 70S ribosomes and inhibit bacterial protein synthesis. Those antibiotics kill bacteria without affecting the larger 80S ribosomes of eukaryotic cells and without harming the host.
Some bacteria produce intracellular nutrient storage granules for later use, such as glycogen, polyphosphate, sulfur or polyhydroxyalkanoates. Certain bacterial species, such as the photosynthetic Cyanobacteria, produce internal gas vesicles, which they use to regulate their buoyancy – allowing them to move up or down into water layers with different light intensities and nutrient levels. Intracellular membranes called chromatophores are also found in membranes of phototrophic bacteria. Used primarily for photosynthesis, they contain bacteriochlorophyll pigments and carotenoids. An early idea was that bacteria might contain membrane folds termed mesosomes, but these were later shown to be artifacts produced by the chemicals used to prepare the cells for electron microscopy. Inclusions are considered to be nonliving components of the cell that do not possess metabolic activity and are not bounded by membranes. The most common inclusions are glycogen, lipid droplets, crystals, and pigments. Volutin granules are cytoplasmic inclusions of complexed inorganic polyphosphate. These granules are called metachromatic granules due to their displaying the metachromatic effect; they appear red or blue when stained with the blue dyes methylene blue or toluidine blue. Gas vacuoles, which are freely permeable to gas, are membrane-bound vesicles present in some species of Cyanobacteria. They allow the bacteria to control their buoyancy. Microcompartments are widespread, membrane-bound organelles that are made of a protein shell that surrounds and encloses various enzymes. Carboxysomes are bacterial microcompartments that contain enzymes involved in carbon fixation. Magnetosomes are bacterial microcompartments, present in magnetotactic bacteria, that contain magnetic crystals.
In most bacteria, a cell wall is present on the outside of the cell membrane. The cell membrane and cell wall comprise the cell envelope. A common bacterial cell wall material is peptidoglycan (called "murein" in older sources), which is made from polysaccharide chains cross-linked by peptides containing D-amino acids. Bacterial cell walls are different from the cell walls of plants and fungi, which are made of cellulose and chitin, respectively. The cell wall of bacteria is also distinct from that of Archaea, which do not contain peptidoglycan. The cell wall is essential to the survival of many bacteria, and the antibiotic penicillin is able to kill bacteria by inhibiting a step in the synthesis of peptidoglycan.
Gram-positive bacteria possess a thick cell wall containing many layers of peptidoglycan and teichoic acids. In contrast, gram-negative bacteria have a relatively thin cell wall consisting of a few layers of peptidoglycan surrounded by a second lipid membrane containing lipopolysaccharides and lipoproteins. Lipopolysaccharides, also called endotoxins, are composed of polysaccharides and lipid A that is responsible for much of the toxicity of gram-negative bacteria. Most bacteria have the gram-negative cell wall, and only the Firmicutes and Actinobacteria have the alternative gram-positive arrangement. These two groups were previously known as the low G+C and high G+C Gram-positive bacteria, respectively. These differences in structure can produce differences in antibiotic susceptibility; for instance, vancomycin can kill only gram-positive bacteria and is ineffective against gram-negative pathogens, such as Haemophilus influenzae or Pseudomonas aeruginosa. If the bacterial cell wall is entirely removed, it is called a protoplast, whereas if it is partially removed, it is called a spheroplast. β-Lactam antibiotics, such as penicillin, inhibit the formation of peptidoglycan cross-links in the bacterial cell wall. The enzyme lysozyme, found in human tears, also digests the cell wall of bacteria and is the body's main defense against eye infections.
Acid-fast bacteria, such as Mycobacteria, are resistant to decolorization by acids during staining procedures. The high mycolic acid content of Mycobacteria, is responsible for the staining pattern of poor absorption followed by high retention. The most common staining technique used to identify acid-fast bacteria is the Ziehl-Neelsen stain or acid-fast stain, in which the acid-fast bacilli are stained bright-red and stand out clearly against a blue background. L-form bacteria are strains of bacteria that lack cell walls. The main pathogenic bacteria in this class is Mycoplasma (not to be confused with Mycobacteria).
Fimbriae (sometimes called "attachment pili") are fine filaments of protein, usually 2–10 nanometres in diameter and up to several micrometers in length. They are distributed over the surface of the cell, and resemble fine hairs when seen under the electron microscope. Fimbriae are believed to be involved in attachment to solid surfaces or to other cells, and are essential for the virulence of some bacterial pathogens. Pili (sing. pilus) are cellular appendages, slightly larger than fimbriae, that can transfer genetic material between bacterial cells in a process called conjugation where they are called conjugation pili or "sex pili" (see bacterial genetics, below). They can also generate movement where they are called type IV pili (see movement, below).
Certain genera of Gram-positive bacteria, such as Bacillus, Clostridium, Sporohalobacter, Anaerobacter, and Heliobacterium, can form highly resistant, dormant structures called endospores. In almost all cases, one endospore is formed and this is not a reproductive process, although Anaerobacter can make up to seven endospores in a single cell. Endospores have a central core of cytoplasm containing DNA and ribosomes surrounded by a cortex layer and protected by an impermeable and rigid coat. Dipicolinic acid is a chemical compound that composes 5% to 15% of the dry weight of bacterial spores. It is implicated as responsible for the heat resistance of the endospore.
Endospores show no detectable metabolism and can survive extreme physical and chemical stresses, such as high levels of UV light, gamma radiation, detergents, disinfectants, heat, freezing, pressure, and desiccation. In this dormant state, these organisms may remain viable for millions of years, and endospores even allow bacteria to survive exposure to the vacuum and radiation in space. According to scientist Dr. Steinn Sigurdsson, "There are viable bacterial spores that have been found that are 40 million years old on Earth — and we know they're very hardened to radiation." Endospore-forming bacteria can also cause disease: for example, anthrax can be contracted by the inhalation of Bacillus anthracis endospores, and contamination of deep puncture wounds with Clostridium tetani endospores causes tetanus.
Bacteria exhibit an extremely wide variety of metabolic types. The distribution of metabolic traits within a group of bacteria has traditionally been used to define their taxonomy, but these traits often do not correspond with modern genetic classifications. Bacterial metabolism is classified into nutritional groups on the basis of three major criteria: the kind of energy used for growth, the source of carbon, and the electron donors used for growth. An additional criterion of respiratory microorganisms are the electron acceptors used for aerobic or anaerobic respiration.
Carbon metabolism in bacteria is either heterotrophic, where organic carbon compounds are used as carbon sources, or autotrophic, meaning that cellular carbon is obtained by fixing carbon dioxide. Heterotrophic bacteria include parasitic types. Typical autotrophic bacteria are phototrophic cyanobacteria, green sulfur-bacteria and some purple bacteria, but also many chemolithotrophic species, such as nitrifying or sulfur-oxidising bacteria. Energy metabolism of bacteria is either based on phototrophy, the use of light through photosynthesis, or based on chemotrophy, the use of chemical substances for energy, which are mostly oxidised at the expense of oxygen or alternative electron acceptors (aerobic/anaerobic respiration).
Bacteria are further divided into lithotrophs that use inorganic electron donors and organotrophs that use organic compounds as electron donors. Chemotrophic organisms use the respective electron donors for energy conservation (by aerobic/anaerobic respiration or fermentation) and biosynthetic reactions (e.g., carbon dioxide fixation), whereas phototrophic organisms use them only for biosynthetic purposes. Respiratory organisms use chemical compounds as a source of energy by taking electrons from the reduced substrate and transferring them to a terminal electron acceptor in a redox reaction. This reaction releases energy that can be used to synthesise ATP and drive metabolism. In aerobic organisms, oxygen is used as the electron acceptor. In anaerobic organisms other inorganic compounds, such as nitrate, sulfate or carbon dioxide are used as electron acceptors. This leads to the ecologically important processes of denitrification, sulfate reduction, and acetogenesis, respectively.
These processes are also important in biological responses to pollution; for example, sulfate-reducing bacteria are largely responsible for the production of the highly toxic forms of mercury (methyl- and dimethylmercury) in the environment. Non-respiratory anaerobes use fermentation to generate energy and reducing power, secreting metabolic by-products (such as ethanol in brewing) as waste. Facultative anaerobes can switch between fermentation and different terminal electron acceptors depending on the environmental conditions in which they find themselves.
Lithotrophic bacteria can use inorganic compounds as a source of energy. Common inorganic electron donors are hydrogen, carbon monoxide, ammonia (leading to nitrification), ferrous iron and other reduced metal ions, and several reduced sulfur compounds. In unusual circumstances, the gas methane can be used by methanotrophic bacteria as both a source of electrons and a substrate for carbon anabolism. In both aerobic phototrophy and chemolithotrophy, oxygen is used as a terminal electron acceptor, whereas under anaerobic conditions inorganic compounds are used instead. Most lithotrophic organisms are autotrophic, whereas organotrophic organisms are heterotrophic.
Regardless of the type of metabolic process they employ, the majority of bacteria are able to take in raw materials only in the form of relatively small molecules, which enter the cell by diffusion or through molecular channels in cell membranes. The Planctomycetes are the exception (as they are in possessing membranes around their nuclear material). It has recently been shown that Gemmata obscuriglobus is able to take in large molecules via a process that in some ways resembles endocytosis, the process used by eukaryotic cells to engulf external items.
Unlike in multicellular organisms, increases in cell size (cell growth) and reproduction by cell division are tightly linked in unicellular organisms. Bacteria grow to a fixed size and then reproduce through binary fission, a form of asexual reproduction. Under optimal conditions, bacteria can grow and divide extremely rapidly, and bacterial populations can double as quickly as every 9.8 minutes. In cell division, two identical clone daughter cells are produced. Some bacteria, while still reproducing asexually, form more complex reproductive structures that help disperse the newly formed daughter cells. Examples include fruiting body formation by Myxobacteria and aerial hyphae formation by Streptomyces, or budding. Budding involves a cell forming a protrusion that breaks away and produces a daughter cell.
In the laboratory, bacteria are usually grown using solid or liquid media. Solid growth media, such as agar plates, are used to isolate pure cultures of a bacterial strain. However, liquid growth media are used when measurement of growth or large volumes of cells are required. Growth in stirred liquid media occurs as an even cell suspension, making the cultures easy to divide and transfer, although isolating single bacteria from liquid media is difficult. The use of selective media (media with specific nutrients added or deficient, or with antibiotics added) can help identify specific organisms.
Most laboratory techniques for growing bacteria use high levels of nutrients to produce large amounts of cells cheaply and quickly. However, in natural environments, nutrients are limited, meaning that bacteria cannot continue to reproduce indefinitely. This nutrient limitation has led the evolution of different growth strategies (see r/K selection theory). Some organisms can grow extremely rapidly when nutrients become available, such as the formation of algal (and cyanobacterial) blooms that often occur in lakes during the summer. Other organisms have adaptations to harsh environments, such as the production of multiple antibiotics by Streptomyces that inhibit the growth of competing microorganisms. In nature, many organisms live in communities (e.g., biofilms) that may allow for increased supply of nutrients and protection from environmental stresses. These relationships can be essential for growth of a particular organism or group of organisms (syntrophy).
Bacterial growth follows four phases. When a population of bacteria first enter a high-nutrient environment that allows growth, the cells need to adapt to their new environment. The first phase of growth is the lag phase, a period of slow growth when the cells are adapting to the high-nutrient environment and preparing for fast growth. The lag phase has high biosynthesis rates, as proteins necessary for rapid growth are produced. The second phase of growth is the log phase, also known as the logarithmic or exponential phase. The log phase is marked by rapid exponential growth. The rate at which cells grow during this phase is known as the growth rate (k), and the time it takes the cells to double is known as the generation time (g). During log phase, nutrients are metabolised at maximum speed until one of the nutrients is depleted and starts limiting growth. The third phase of growth is the stationary phase and is caused by depleted nutrients. The cells reduce their metabolic activity and consume non-essential cellular proteins. The stationary phase is a transition from rapid growth to a stress response state and there is increased expression of genes involved in DNA repair, antioxidant metabolism and nutrient transport. The final phase is the death phase where the bacteria run out of nutrients and die.
Most bacteria have a single circular chromosome that can range in size from only 160,000 base pairs in the endosymbiotic bacteria Candidatus Carsonella ruddii, to 12,200,000 base pairs in the soil-dwelling bacteria Sorangium cellulosum. Spirochaetes of the genus Borrelia are a notable exception to this arrangement, with bacteria such as Borrelia burgdorferi, the cause of Lyme disease, containing a single linear chromosome. The genes in bacterial genomes are usually a single continuous stretch of DNA and although several different types of introns do exist in bacteria, these are much rarer than in eukaryotes.
Bacteria, as asexual organisms, inherit identical copies of their parent's genes (i.e., they are clonal). However, all bacteria can evolve by selection on changes to their genetic material DNA caused by genetic recombination or mutations. Mutations come from errors made during the replication of DNA or from exposure to mutagens. Mutation rates vary widely among different species of bacteria and even among different clones of a single species of bacteria. Genetic changes in bacterial genomes come from either random mutation during replication or "stress-directed mutation", where genes involved in a particular growth-limiting process have an increased mutation rate.
Transduction of bacterial genes by bacteriophage appears to be a consequence of infrequent errors during intracellular assembly of virus particles, rather than a bacterial adaptation. Conjugation, in the much-studied E. coli system is determined by plasmid genes, and is an adaptation for transferring copies of the plasmid from one bacterial host to another. It is seldom that a conjugative plasmid integrates into the host bacterial chromosome, and subsequently transfers part of the host bacterial DNA to another bacterium. Plasmid-mediated transfer of host bacterial DNA also appears to be an accidental process rather than a bacterial adaptation.
Transformation, unlike transduction or conjugation, depends on numerous bacterial gene products that specifically interact to perform this complex process, and thus transformation is clearly a bacterial adaptation for DNA transfer. In order for a bacterium to bind, take up and recombine donor DNA into its own chromosome, it must first enter a special physiological state termed competence (see Natural competence). In Bacillus subtilis, about 40 genes are required for the development of competence. The length of DNA transferred during B. subtilis transformation can be between a third of a chromosome up to the whole chromosome. Transformation appears to be common among bacterial species, and thus far at least 60 species are known to have the natural ability to become competent for transformation. The development of competence in nature is usually associated with stressful environmental conditions, and seems to be an adaptation for facilitating repair of DNA damage in recipient cells.
In ordinary circumstances, transduction, conjugation, and transformation involve transfer of DNA between individual bacteria of the same species, but occasionally transfer may occur between individuals of different bacterial species and this may have significant consequences, such as the transfer of antibiotic resistance. In such cases, gene acquisition from other bacteria or the environment is called horizontal gene transfer and may be common under natural conditions. Gene transfer is particularly important in antibiotic resistance as it allows the rapid transfer of resistance genes between different pathogens.
Bacteriophages are viruses that infect bacteria. Many types of bacteriophage exist, some simply infect and lyse their host bacteria, while others insert into the bacterial chromosome. A bacteriophage can contain genes that contribute to its host's phenotype: for example, in the evolution of Escherichia coli O157:H7 and Clostridium botulinum, the toxin genes in an integrated phage converted a harmless ancestral bacterium into a lethal pathogen. Bacteria resist phage infection through restriction modification systems that degrade foreign DNA, and a system that uses CRISPR sequences to retain fragments of the genomes of phage that the bacteria have come into contact with in the past, which allows them to block virus replication through a form of RNA interference. This CRISPR system provides bacteria with acquired immunity to infection.
Bacterial species differ in the number and arrangement of flagella on their surface; some have a single flagellum (monotrichous), a flagellum at each end (amphitrichous), clusters of flagella at the poles of the cell (lophotrichous), while others have flagella distributed over the entire surface of the cell (peritrichous). The bacterial flagella is the best-understood motility structure in any organism and is made of about 20 proteins, with approximately another 30 proteins required for its regulation and assembly. The flagellum is a rotating structure driven by a reversible motor at the base that uses the electrochemical gradient across the membrane for power. This motor drives the motion of the filament, which acts as a propeller.
Classification seeks to describe the diversity of bacterial species by naming and grouping organisms based on similarities. Bacteria can be classified on the basis of cell structure, cellular metabolism or on differences in cell components, such as DNA, fatty acids, pigments, antigens and quinones. While these schemes allowed the identification and classification of bacterial strains, it was unclear whether these differences represented variation between distinct species or between strains of the same species. This uncertainty was due to the lack of distinctive structures in most bacteria, as well as lateral gene transfer between unrelated species. Due to lateral gene transfer, some closely related bacteria can have very different morphologies and metabolisms. To overcome this uncertainty, modern bacterial classification emphasizes molecular systematics, using genetic techniques such as guanine cytosine ratio determination, genome-genome hybridization, as well as sequencing genes that have not undergone extensive lateral gene transfer, such as the rRNA gene. Classification of bacteria is determined by publication in the International Journal of Systematic Bacteriology, and Bergey's Manual of Systematic Bacteriology. The International Committee on Systematic Bacteriology (ICSB) maintains international rules for the naming of bacteria and taxonomic categories and for the ranking of them in the International Code of Nomenclature of Bacteria.
The term "bacteria" was traditionally applied to all microscopic, single-cell prokaryotes. However, molecular systematics showed prokaryotic life to consist of two separate domains, originally called Eubacteria and Archaebacteria, but now called Bacteria and Archaea that evolved independently from an ancient common ancestor. The archaea and eukaryotes are more closely related to each other than either is to the bacteria. These two domains, along with Eukarya, are the basis of the three-domain system, which is currently the most widely used classification system in microbiolology. However, due to the relatively recent introduction of molecular systematics and a rapid increase in the number of genome sequences that are available, bacterial classification remains a changing and expanding field. For example, a few biologists argue that the Archaea and Eukaryotes evolved from Gram-positive bacteria.
The Gram stain, developed in 1884 by Hans Christian Gram, characterises bacteria based on the structural characteristics of their cell walls. The thick layers of peptidoglycan in the "Gram-positive" cell wall stain purple, while the thin "Gram-negative" cell wall appears pink. By combining morphology and Gram-staining, most bacteria can be classified as belonging to one of four groups (Gram-positive cocci, Gram-positive bacilli, Gram-negative cocci and Gram-negative bacilli). Some organisms are best identified by stains other than the Gram stain, particularly mycobacteria or Nocardia, which show acid-fastness on Ziehl–Neelsen or similar stains. Other organisms may need to be identified by their growth in special media, or by other techniques, such as serology.
As with bacterial classification, identification of bacteria is increasingly using molecular methods. Diagnostics using DNA-based tools, such as polymerase chain reaction, are increasingly popular due to their specificity and speed, compared to culture-based methods. These methods also allow the detection and identification of "viable but nonculturable" cells that are metabolically active but non-dividing. However, even using these improved methods, the total number of bacterial species is not known and cannot even be estimated with any certainty. Following present classification, there are a little less than 9,300 known species of prokaryotes, which includes bacteria and archaea; but attempts to estimate the true number of bacterial diversity have ranged from 107 to 109 total species – and even these diverse estimates may be off by many orders of magnitude.
Some species of bacteria kill and then consume other microorganisms, these species are called predatory bacteria. These include organisms such as Myxococcus xanthus, which forms swarms of cells that kill and digest any bacteria they encounter. Other bacterial predators either attach to their prey in order to digest them and absorb nutrients, such as Vampirovibrio chlorellavorus, or invade another cell and multiply inside the cytosol, such as Daptobacter. These predatory bacteria are thought to have evolved from saprophages that consumed dead microorganisms, through adaptations that allowed them to entrap and kill other organisms.
Certain bacteria form close spatial associations that are essential for their survival. One such mutualistic association, called interspecies hydrogen transfer, occurs between clusters of anaerobic bacteria that consume organic acids, such as butyric acid or propionic acid, and produce hydrogen, and methanogenic Archaea that consume hydrogen. The bacteria in this association are unable to consume the organic acids as this reaction produces hydrogen that accumulates in their surroundings. Only the intimate association with the hydrogen-consuming Archaea keeps the hydrogen concentration low enough to allow the bacteria to grow.
In soil, microorganisms that reside in the rhizosphere (a zone that includes the root surface and the soil that adheres to the root after gentle shaking) carry out nitrogen fixation, converting nitrogen gas to nitrogenous compounds. This serves to provide an easily absorbable form of nitrogen for many plants, which cannot fix nitrogen themselves. Many other bacteria are found as symbionts in humans and other organisms. For example, the presence of over 1,000 bacterial species in the normal human gut flora of the intestines can contribute to gut immunity, synthesise vitamins, such as folic acid, vitamin K and biotin, convert sugars to lactic acid (see Lactobacillus), as well as fermenting complex undigestible carbohydrates. The presence of this gut flora also inhibits the growth of potentially pathogenic bacteria (usually through competitive exclusion) and these beneficial bacteria are consequently sold as probiotic dietary supplements.
If bacteria form a parasitic association with other organisms, they are classed as pathogens. Pathogenic bacteria are a major cause of human death and disease and cause infections such as tetanus, typhoid fever, diphtheria, syphilis, cholera, foodborne illness, leprosy and tuberculosis. A pathogenic cause for a known medical disease may only be discovered many years after, as was the case with Helicobacter pylori and peptic ulcer disease. Bacterial diseases are also important in agriculture, with bacteria causing leaf spot, fire blight and wilts in plants, as well as Johne's disease, mastitis, salmonella and anthrax in farm animals.
Each species of pathogen has a characteristic spectrum of interactions with its human hosts. Some organisms, such as Staphylococcus or Streptococcus, can cause skin infections, pneumonia, meningitis and even overwhelming sepsis, a systemic inflammatory response producing shock, massive vasodilation and death. Yet these organisms are also part of the normal human flora and usually exist on the skin or in the nose without causing any disease at all. Other organisms invariably cause disease in humans, such as the Rickettsia, which are obligate intracellular parasites able to grow and reproduce only within the cells of other organisms. One species of Rickettsia causes typhus, while another causes Rocky Mountain spotted fever. Chlamydia, another phylum of obligate intracellular parasites, contains species that can cause pneumonia, or urinary tract infection and may be involved in coronary heart disease. Finally, some species, such as Pseudomonas aeruginosa, Burkholderia cenocepacia, and Mycobacterium avium, are opportunistic pathogens and cause disease mainly in people suffering from immunosuppression or cystic fibrosis.
Bacterial infections may be treated with antibiotics, which are classified as bacteriocidal if they kill bacteria, or bacteriostatic if they just prevent bacterial growth. There are many types of antibiotics and each class inhibits a process that is different in the pathogen from that found in the host. An example of how antibiotics produce selective toxicity are chloramphenicol and puromycin, which inhibit the bacterial ribosome, but not the structurally different eukaryotic ribosome. Antibiotics are used both in treating human disease and in intensive farming to promote animal growth, where they may be contributing to the rapid development of antibiotic resistance in bacterial populations. Infections can be prevented by antiseptic measures such as sterilizing the skin prior to piercing it with the needle of a syringe, and by proper care of indwelling catheters. Surgical and dental instruments are also sterilized to prevent contamination by bacteria. Disinfectants such as bleach are used to kill bacteria or other pathogens on surfaces to prevent contamination and further reduce the risk of infection.
The ability of bacteria to degrade a variety of organic compounds is remarkable and has been used in waste processing and bioremediation. Bacteria capable of digesting the hydrocarbons in petroleum are often used to clean up oil spills. Fertilizer was added to some of the beaches in Prince William Sound in an attempt to promote the growth of these naturally occurring bacteria after the 1989 Exxon Valdez oil spill. These efforts were effective on beaches that were not too thickly covered in oil. Bacteria are also used for the bioremediation of industrial toxic wastes. In the chemical industry, bacteria are most important in the production of enantiomerically pure chemicals for use as pharmaceuticals or agrichemicals.
Because of their ability to quickly grow and the relative ease with which they can be manipulated, bacteria are the workhorses for the fields of molecular biology, genetics and biochemistry. By making mutations in bacterial DNA and examining the resulting phenotypes, scientists can determine the function of genes, enzymes and metabolic pathways in bacteria, then apply this knowledge to more complex organisms. This aim of understanding the biochemistry of a cell reaches its most complex expression in the synthesis of huge amounts of enzyme kinetic and gene expression data into mathematical models of entire organisms. This is achievable in some well-studied bacteria, with models of Escherichia coli metabolism now being produced and tested. This understanding of bacterial metabolism and genetics allows the use of biotechnology to bioengineer bacteria for the production of therapeutic proteins, such as insulin, growth factors, or antibodies.
Bacteria were first observed by the Dutch microscopist Antonie van Leeuwenhoek in 1676, using a single-lens microscope of his own design. He then published his observations in a series of letters to the Royal Society of London. Bacteria were Leeuwenhoek's most remarkable microscopic discovery. They were just at the limit of what his simple lenses could make out and, in one of the most striking hiatuses in the history of science, no one else would see them again for over a century. Only then were his by-then-largely-forgotten observations of bacteria — as opposed to his famous "animalcules" (spermatozoa) — taken seriously.
Though it was known in the nineteenth century that bacteria are the cause of many diseases, no effective antibacterial treatments were available. In 1910, Paul Ehrlich developed the first antibiotic, by changing dyes that selectively stained Treponema pallidum — the spirochaete that causes syphilis — into compounds that selectively killed the pathogen. Ehrlich had been awarded a 1908 Nobel Prize for his work on immunology, and pioneered the use of stains to detect and identify bacteria, with his work being the basis of the Gram stain and the Ziehl–Neelsen stain.
Zinc is a chemical element with symbol Zn and atomic number 30. It is the first element of group 12 of the periodic table. In some respects zinc is chemically similar to magnesium: its ion is of similar size and its only common oxidation state is +2. Zinc is the 24th most abundant element in Earth's crust and has five stable isotopes. The most common zinc ore is sphalerite (zinc blende), a zinc sulfide mineral. The largest mineable amounts are found in Australia, Asia, and the United States. Zinc production includes froth flotation of the ore, roasting, and final extraction using electricity (electrowinning).
Brass, which is an alloy of copper and zinc, has been used since at least the 10th century BC in Judea and by the 7th century BC in Ancient Greece. Zinc metal was not produced on a large scale until the 12th century in India and was unknown to Europe until the end of the 16th century. The mines of Rajasthan have given definite evidence of zinc production going back to the 6th century BC. To date, the oldest evidence of pure zinc comes from Zawar, in Rajasthan, as early as the 9th century AD when a distillation process was employed to make pure zinc. Alchemists burned zinc in air to form what they called "philosopher's wool" or "white snow".
The element was probably named by the alchemist Paracelsus after the German word Zinke (prong, tooth). German chemist Andreas Sigismund Marggraf is credited with discovering pure metallic zinc in 1746. Work by Luigi Galvani and Alessandro Volta uncovered the electrochemical properties of zinc by 1800. Corrosion-resistant zinc plating of iron (hot-dip galvanizing) is the major application for zinc. Other applications are in batteries, small non-structural castings, and alloys, such as brass. A variety of zinc compounds are commonly used, such as zinc carbonate and zinc gluconate (as dietary supplements), zinc chloride (in deodorants), zinc pyrithione (anti-dandruff shampoos), zinc sulfide (in luminescent paints), and zinc methyl or zinc diethyl in the organic laboratory.
Zinc is an essential mineral perceived by the public today as being of "exceptional biologic and public health importance", especially regarding prenatal and postnatal development. Zinc deficiency affects about two billion people in the developing world and is associated with many diseases. In children it causes growth retardation, delayed sexual maturation, infection susceptibility, and diarrhea. Enzymes with a zinc atom in the reactive center are widespread in biochemistry, such as alcohol dehydrogenase in humans. Consumption of excess zinc can cause ataxia, lethargy and copper deficiency.
Zinc is a bluish-white, lustrous, diamagnetic metal, though most common commercial grades of the metal have a dull finish. It is somewhat less dense than iron and has a hexagonal crystal structure, with a distorted form of hexagonal close packing, in which each atom has six nearest neighbors (at 265.9 pm) in its own plane and six others at a greater distance of 290.6 pm. The metal is hard and brittle at most temperatures but becomes malleable between 100 and 150 °C. Above 210 °C, the metal becomes brittle again and can be pulverized by beating. Zinc is a fair conductor of electricity. For a metal, zinc has relatively low melting (419.5 °C) and boiling points (907 °C). Its melting point is the lowest of all the transition metals aside from mercury and cadmium.
Several dozen radioisotopes have been characterized. 65Zn, which has a half-life of 243.66 days, is the most long-lived radioisotope, followed by 72Zn with a half-life of 46.5 hours. Zinc has 10 nuclear isomers. 69mZn has the longest half-life, 13.76 h. The superscript m indicates a metastable isotope. The nucleus of a metastable isotope is in an excited state and will return to the ground state by emitting a photon in the form of a gamma ray. 61Zn has three excited states and 73Zn has two. The isotopes 65Zn, 71Zn, 77Zn and 78Zn each have only one excited state.
The chemistry of zinc is dominated by the +2 oxidation state. When compounds in this oxidation state are formed the outer shell s electrons are lost, which yields a bare zinc ion with the electronic configuration [Ar]3d10. In aqueous solution an octahedral complex, [Zn(H
2O)6]2+ is the predominant species. The volatilization of zinc in combination with zinc chloride at temperatures above 285 °C indicates the formation of Zn
2Cl
2, a zinc compound with a +1 oxidation state. No compounds of zinc in oxidation states other than +1 or +2 are known. Calculations indicate that a zinc compound with the oxidation state of +4 is unlikely to exist.
Zinc chemistry is similar to the chemistry of the late first-row transition metals nickel and copper, though it has a filled d-shell, so its compounds are diamagnetic and mostly colorless. The ionic radii of zinc and magnesium happen to be nearly identical. Because of this some of their salts have the same crystal structure and in circumstances where ionic radius is a determining factor zinc and magnesium chemistries have much in common. Otherwise there is little similarity. Zinc tends to form bonds with a greater degree of covalency and it forms much more stable complexes with N- and S- donors. Complexes of zinc are mostly 4- or 6- coordinate although 5-coordinate complexes are known.
Zinc(I) compounds are rare, and require bulky ligands to stabilize the low oxidation state. Most zinc(I) compounds contain formally the [Zn2]2+ core, which is analogous to the [Hg2]2+ dimeric cation present in mercury(I) compounds. The diamagnetic nature of the ion confirms its dimeric structure. The first zinc(I) compound containing the Zn—Zn bond, (η5-C5Me5)2Zn2, is also the first dimetallocene. The [Zn2]2+ ion rapidly disproportionates into zinc metal and zinc(II), and has only been obtained as a yellow glass formed by cooling a solution of metallic zinc in molten ZnCl2.
Binary compounds of zinc are known for most of the metalloids and all the nonmetals except the noble gases. The oxide ZnO is a white powder that is nearly insoluble in neutral aqueous solutions, but is amphoteric, dissolving in both strong basic and acidic solutions. The other chalcogenides (ZnS, ZnSe, and ZnTe) have varied applications in electronics and optics. Pnictogenides (Zn
3N
2, Zn
3P
2, Zn
3As
2 and Zn
3Sb
2), the peroxide (ZnO
2), the hydride (ZnH
2), and the carbide (ZnC
2) are also known. Of the four halides, ZnF
2 has the most ionic character, whereas the others (ZnCl
2, ZnBr
2, and ZnI
2) have relatively low melting points and are considered to have more covalent character.
In weak basic solutions containing Zn2+ ions, the hydroxide Zn(OH)
2 forms as a white precipitate. In stronger alkaline solutions, this hydroxide is dissolved to form zincates ([Zn(OH)4]2−). The nitrate Zn(NO3)
2, chlorate Zn(ClO3)
2, sulfate ZnSO
4, phosphate Zn
3(PO4)
2, molybdate ZnMoO
4, cyanide Zn(CN)
2, arsenite Zn(AsO2)
2, arsenate Zn(AsO4)
2·8H
2O and the chromate ZnCrO
4 (one of the few colored zinc compounds) are a few examples of other common inorganic compounds of zinc. One of the simplest examples of an organic compound of zinc is the acetate (Zn(O
2CCH3)
2).
The Charaka Samhita, thought to have been written between 300 and 500 AD, mentions a metal which, when oxidized, produces pushpanjan, thought to be zinc oxide. Zinc mines at Zawar, near Udaipur in India, have been active since the Mauryan period. The smelting of metallic zinc here, however, appears to have begun around the 12th century AD. One estimate is that this location produced an estimated million tonnes of metallic zinc and zinc oxide from the 12th to 16th centuries. Another estimate gives a total production of 60,000 tonnes of metallic zinc over this period. The Rasaratna Samuccaya, written in approximately the 13th century AD, mentions two types of zinc-containing ores: one used for metal extraction and another used for medicinal purposes.
The name of the metal was probably first documented by Paracelsus, a Swiss-born German alchemist, who referred to the metal as "zincum" or "zinken" in his book Liber Mineralium II, in the 16th century. The word is probably derived from the German zinke, and supposedly meant "tooth-like, pointed or jagged" (metallic zinc crystals have a needle-like appearance). Zink could also imply "tin-like" because of its relation to German zinn meaning tin. Yet another possibility is that the word is derived from the Persian word سنگ seng meaning stone. The metal was also called Indian tin, tutanego, calamine, and spinter.
William Champion's brother, John, patented a process in 1758 for calcining zinc sulfide into an oxide usable in the retort process. Prior to this, only calamine could be used to produce zinc. In 1798, Johann Christian Ruberg improved on the smelting process by building the first horizontal retort smelter. Jean-Jacques Daniel Dony built a different kind of horizontal zinc smelter in Belgium, which processed even more zinc. Italian doctor Luigi Galvani discovered in 1780 that connecting the spinal cord of a freshly dissected frog to an iron rail attached by a brass hook caused the frog's leg to twitch. He incorrectly thought he had discovered an ability of nerves and muscles to create electricity and called the effect "animal electricity". The galvanic cell and the process of galvanization were both named for Luigi Galvani and these discoveries paved the way for electrical batteries, galvanization and cathodic protection.
Zinc metal is produced using extractive metallurgy. After grinding the ore, froth flotation, which selectively separates minerals from gangue by taking advantage of differences in their hydrophobicity, is used to get an ore concentrate. This concentrate consists of about 50% zinc with the rest being sulfur (32%), iron (13%), and SiO
2 (5%). The composition of this is normally zinc sulfide (80% to 85%), iron sulfide (7.0% to 12%), lead sulfide (3.0% to 5.0%) silica (2.5% to 3.5%), and cadmium sulfide (0.35% to 0.41%).
The production for sulfidic zinc ores produces large amounts of sulfur dioxide and cadmium vapor. Smelter slag and other residues of process also contain significant amounts of heavy metals. About 1.1 million tonnes of metallic zinc and 130 thousand tonnes of lead were mined and smelted in the Belgian towns of La Calamine and Plombières between 1806 and 1882. The dumps of the past mining operations leach significant amounts of zinc and cadmium, and, as a result, the sediments of the Geul River contain significant amounts of heavy metals. About two thousand years ago emissions of zinc from mining and smelting totaled 10 thousand tonnes a year. After increasing 10-fold from 1850, zinc emissions peaked at 3.4 million tonnes per year in the 1980s and declined to 2.7 million tonnes in the 1990s, although a 2005 study of the Arctic troposphere found that the concentrations there did not reflect the decline. Anthropogenic and natural emissions occur at a ratio of 20 to 1.
Zinc is more reactive than iron or steel and thus will attract almost all local oxidation until it completely corrodes away. A protective surface layer of oxide and carbonate (Zn
5(OH)
6(CO
3)
2) forms as the zinc corrodes. This protection lasts even after the zinc layer is scratched but degrades through time as the zinc corrodes away. The zinc is applied electrochemically or as molten zinc by hot-dip galvanizing or spraying. Galvanization is used on chain-link fencing, guard rails, suspension bridges, lightposts, metal roofs, heat exchangers, and car bodies.
The relative reactivity of zinc and its ability to attract oxidation to itself makes it an efficient sacrificial anode in cathodic protection (CP). For example, cathodic protection of a buried pipeline can be achieved by connecting anodes made from zinc to the pipe. Zinc acts as the anode (negative terminus) by slowly corroding away as it passes electric current to the steel pipeline.[note 2] Zinc is also used to cathodically protect metals that are exposed to sea water from corrosion. A zinc disc attached to a ship's iron rudder will slowly corrode, whereas the rudder stays unattacked. Other similar uses include a plug of zinc attached to a propeller or the metal protective guard for the keel of the ship.
Other widely used alloys that contain zinc include nickel silver, typewriter metal, soft and aluminium solder, and commercial bronze. Zinc is also used in contemporary pipe organs as a substitute for the traditional lead/tin alloy in pipes. Alloys of 85–88% zinc, 4–10% copper, and 2–8% aluminium find limited use in certain types of machine bearings. Zinc is the primary metal used in making American one cent coins since 1982. The zinc core is coated with a thin layer of copper to give the impression of a copper coin. In 1994, 33,200 tonnes (36,600 short tons) of zinc were used to produce 13.6 billion pennies in the United States.
Alloys of primarily zinc with small amounts of copper, aluminium, and magnesium are useful in die casting as well as spin casting, especially in the automotive, electrical, and hardware industries. These alloys are marketed under the name Zamak. An example of this is zinc aluminium. The low melting point together with the low viscosity of the alloy makes the production of small and intricate shapes possible. The low working temperature leads to rapid cooling of the cast products and therefore fast assembly is possible. Another alloy, marketed under the brand name Prestal, contains 78% zinc and 22% aluminium and is reported to be nearly as strong as steel but as malleable as plastic. This superplasticity of the alloy allows it to be molded using die casts made of ceramics and cement.
Similar alloys with the addition of a small amount of lead can be cold-rolled into sheets. An alloy of 96% zinc and 4% aluminium is used to make stamping dies for low production run applications for which ferrous metal dies would be too expensive. In building facades, roofs or other applications in which zinc is used as sheet metal and for methods such as deep drawing, roll forming or bending, zinc alloys with titanium and copper are used. Unalloyed zinc is too brittle for these kinds of manufacturing processes.
Roughly one quarter of all zinc output in the United States (2009), is consumed in the form of zinc compounds; a variety of which are used industrially. Zinc oxide is widely used as a white pigment in paints, and as a catalyst in the manufacture of rubber. It is also used as a heat disperser for the rubber and acts to protect its polymers from ultraviolet radiation (the same UV protection is conferred to plastics containing zinc oxide). The semiconductor properties of zinc oxide make it useful in varistors and photocopying products. The zinc zinc-oxide cycle is a two step thermochemical process based on zinc and zinc oxide for hydrogen production.
Zinc chloride is often added to lumber as a fire retardant and can be used as a wood preservative. It is also used to make other chemicals. Zinc methyl (Zn(CH3)
2) is used in a number of organic syntheses. Zinc sulfide (ZnS) is used in luminescent pigments such as on the hands of clocks, X-ray and television screens, and luminous paints. Crystals of ZnS are used in lasers that operate in the mid-infrared part of the spectrum. Zinc sulfate is a chemical in dyes and pigments. Zinc pyrithione is used in antifouling paints.
64Zn, the most abundant isotope of zinc, is very susceptible to neutron activation, being transmuted into the highly radioactive 65Zn, which has a half-life of 244 days and produces intense gamma radiation. Because of this, Zinc Oxide used in nuclear reactors as an anti-corrosion agent is depleted of 64Zn before use, this is called depleted zinc oxide. For the same reason, zinc has been proposed as a salting material for nuclear weapons (cobalt is another, better-known salting material). A jacket of isotopically enriched 64Zn would be irradiated by the intense high-energy neutron flux from an exploding thermonuclear weapon, forming a large amount of 65Zn significantly increasing the radioactivity of the weapon's fallout. Such a weapon is not known to have ever been built, tested, or used. 65Zn is also used as a tracer to study how alloys that contain zinc wear out, or the path and the role of zinc in organisms.
Zinc is included in most single tablet over-the-counter daily vitamin and mineral supplements. Preparations include zinc oxide, zinc acetate, and zinc gluconate. It is believed to possess antioxidant properties, which may protect against accelerated aging of the skin and muscles of the body; studies differ as to its effectiveness. Zinc also helps speed up the healing process after an injury. It is also suspected of being beneficial to the body's immune system. Indeed, zinc deficiency may have effects on virtually all parts of the human immune system.
Although not yet tested as a therapy in humans, a growing body of evidence indicates that zinc may preferentially kill prostate cancer cells. Because zinc naturally homes to the prostate and because the prostate is accessible with relatively non-invasive procedures, its potential as a chemotherapeutic agent in this type of cancer has shown promise. However, other studies have demonstrated that chronic use of zinc supplements in excess of the recommended dosage may actually increase the chance of developing prostate cancer, also likely due to the natural buildup of this heavy metal in the prostate.
There are many important organozinc compounds. Organozinc chemistry is the science of organozinc compounds describing their physical properties, synthesis and reactions. Among important applications is the Frankland-Duppa Reaction in which an oxalate ester(ROCOCOOR) reacts with an alkyl halide R'X, zinc and hydrochloric acid to the α-hydroxycarboxylic esters RR'COHCOOR, the Reformatskii reaction which converts α-halo-esters and aldehydes to β-hydroxy-esters, the Simmons–Smith reaction in which the carbenoid (iodomethyl)zinc iodide reacts with alkene(or alkyne) and converts them to cyclopropane, the Addition reaction of organozinc compounds to carbonyl compounds. The Barbier reaction (1899) is the zinc equivalent of the magnesium Grignard reaction and is better of the two. In presence of just about any water the formation of the organomagnesium halide will fail, whereas the Barbier reaction can even take place in water. On the downside organozincs are much less nucleophilic than Grignards, are expensive and difficult to handle. Commercially available diorganozinc compounds are dimethylzinc, diethylzinc and diphenylzinc. In one study the active organozinc compound is obtained from much cheaper organobromine precursors:
Zinc serves a purely structural role in zinc fingers, twists and clusters. Zinc fingers form parts of some transcription factors, which are proteins that recognize DNA base sequences during the replication and transcription of DNA. Each of the nine or ten Zn2+ ions in a zinc finger helps maintain the finger's structure by coordinately binding to four amino acids in the transcription factor. The transcription factor wraps around the DNA helix and uses its fingers to accurately bind to the DNA sequence.
Other sources include fortified food and dietary supplements, which come in various forms. A 1998 review concluded that zinc oxide, one of the most common supplements in the United States, and zinc carbonate are nearly insoluble and poorly absorbed in the body. This review cited studies which found low plasma zinc concentrations after zinc oxide and zinc carbonate were consumed compared with those seen after consumption of zinc acetate and sulfate salts. However, harmful excessive supplementation is a problem among the relatively affluent, and should probably not exceed 20 mg/day in healthy people, although the U.S. National Research Council set a Tolerable Upper Intake of 40 mg/day.
For fortification, however, a 2003 review recommended zinc oxide in cereals as cheap, stable, and as easily absorbed as more expensive forms. A 2005 study found that various compounds of zinc, including oxide and sulfate, did not show statistically significant differences in absorption when added as fortificants to maize tortillas. A 1987 study found that zinc picolinate was better absorbed than zinc gluconate or zinc citrate. However, a study published in 2008 determined that zinc glycinate is the best absorbed of the four dietary supplement types available.
Symptoms of mild zinc deficiency are diverse. Clinical outcomes include depressed growth, diarrhea, impotence and delayed sexual maturation, alopecia, eye and skin lesions, impaired appetite, altered cognition, impaired host defense properties, defects in carbohydrate utilization, and reproductive teratogenesis. Mild zinc deficiency depresses immunity, although excessive zinc does also. Animals with a diet deficient in zinc require twice as much food in order to attain the same weight gain as animals given sufficient zinc.
Despite some concerns, western vegetarians and vegans have not been found to suffer from overt zinc deficiencies any more than meat-eaters. Major plant sources of zinc include cooked dried beans, sea vegetables, fortified cereals, soyfoods, nuts, peas, and seeds. However, phytates in many whole-grains and fiber in many foods may interfere with zinc absorption and marginal zinc intake has poorly understood effects. The zinc chelator phytate, found in seeds and cereal bran, can contribute to zinc malabsorption. There is some evidence to suggest that more than the US RDA (15 mg) of zinc daily may be needed in those whose diet is high in phytates, such as some vegetarians. These considerations must be balanced against the fact that there is a paucity of adequate zinc biomarkers, and the most widely used indicator, plasma zinc, has poor sensitivity and specificity. Diagnosing zinc deficiency is a persistent challenge.
Nearly two billion people in the developing world are deficient in zinc. In children it causes an increase in infection and diarrhea, contributing to the death of about 800,000 children worldwide per year. The World Health Organization advocates zinc supplementation for severe malnutrition and diarrhea. Zinc supplements help prevent disease and reduce mortality, especially among children with low birth weight or stunted growth. However, zinc supplements should not be administered alone, because many in the developing world have several deficiencies, and zinc interacts with other micronutrients.
Zinc deficiency is crop plants' most common micronutrient deficiency; it is particularly common in high-pH soils. Zinc-deficient soil is cultivated in the cropland of about half of Turkey and India, a third of China, and most of Western Australia, and substantial responses to zinc fertilization have been reported in these areas. Plants that grow in soils that are zinc-deficient are more susceptible to disease. Zinc is primarily added to the soil through the weathering of rocks, but humans have added zinc through fossil fuel combustion, mine waste, phosphate fertilizers, pesticide (zinc phosphide), limestone, manure, sewage sludge, and particles from galvanized surfaces. Excess zinc is toxic to plants, although zinc toxicity is far less widespread.
There is evidence of induced copper deficiency in those taking 100–300 mg of zinc daily. A 2007 trial observed that elderly men taking 80 mg daily were hospitalized for urinary complications more often than those taking a placebo. The USDA RDA is 11 and 8 mg Zn/day for men and women, respectively. Levels of 100–300 mg may interfere with the utilization of copper and iron or adversely affect cholesterol. Levels of zinc in excess of 500 ppm in soil interfere with the ability of plants to absorb other essential metals, such as iron and manganese. There is also a condition called the zinc shakes or "zinc chills" that can be induced by the inhalation of freshly formed zinc oxide formed during the welding of galvanized materials. Zinc is a common ingredient of denture cream which may contain between 17 and 38 mg of zinc per gram. There have been claims of disability, and even death, due to excessive use of these products.
The U.S. Food and Drug Administration (FDA) has stated that zinc damages nerve receptors in the nose, which can cause anosmia. Reports of anosmia were also observed in the 1930s when zinc preparations were used in a failed attempt to prevent polio infections. On June 16, 2009, the FDA said that consumers should stop using zinc-based intranasal cold products and ordered their removal from store shelves. The FDA said the loss of smell can be life-threatening because people with impaired smell cannot detect leaking gas or smoke and cannot tell if food has spoiled before they eat it. Recent research suggests that the topical antimicrobial zinc pyrithione is a potent heat shock response inducer that may impair genomic integrity with induction of PARP-dependent energy crisis in cultured human keratinocytes and melanocytes.
In 1982, the US Mint began minting pennies coated in copper but made primarily of zinc. With the new zinc pennies, there is the potential for zinc toxicosis, which can be fatal. One reported case of chronic ingestion of 425 pennies (over 1 kg of zinc) resulted in death due to gastrointestinal bacterial and fungal sepsis, whereas another patient, who ingested 12 grams of zinc, only showed lethargy and ataxia (gross lack of coordination of muscle movements). Several other cases have been reported of humans suffering zinc intoxication by the ingestion of zinc coins.
Pennies and other small coins are sometimes ingested by dogs, resulting in the need for medical treatment to remove the foreign body. The zinc content of some coins can cause zinc toxicity, which is commonly fatal in dogs, where it causes a severe hemolytic anemia, and also liver or kidney damage; vomiting and diarrhea are possible symptoms. Zinc is highly toxic in parrots and poisoning can often be fatal. The consumption of fruit juices stored in galvanized cans has resulted in mass parrot poisonings with zinc.
Dwight David "Ike" Eisenhower (/ˈaɪzənˌhaʊ.ər/ EYE-zən-HOW-ər; October 14, 1890 – March 28, 1969) was an American politician and general who served as the 34th President of the United States from 1953 until 1961. He was a five-star general in the United States Army during World War II and served as Supreme Commander of the Allied Forces in Europe. He was responsible for planning and supervising the invasion of North Africa in Operation Torch in 1942–43 and the successful invasion of France and Germany in 1944–45 from the Western Front. In 1951, he became the first Supreme Commander of NATO.
Eisenhower's main goals in office were to keep pressure on the Soviet Union and reduce federal deficits. In the first year of his presidency, he threatened the use of nuclear weapons in an effort to conclude the Korean War; his New Look policy of nuclear deterrence prioritized inexpensive nuclear weapons while reducing funding for conventional military forces. He ordered coups in Iran and Guatemala. Eisenhower refused to give major aid to help France in Vietnam. He gave strong financial support to the new nation of South Vietnam. Congress agreed to his request in 1955 for the Formosa Resolution, which obliged the U.S. to militarily support the pro-Western Republic of China in Taiwan and continue the isolation of the People's Republic of China.
After the Soviet Union launched the world's first artificial satellite in 1957, Eisenhower authorized the establishment of NASA, which led to the space race. During the Suez Crisis of 1956, Eisenhower condemned the Israeli, British and French invasion of Egypt, and forced them to withdraw. He also condemned the Soviet invasion during the Hungarian Revolution of 1956 but took no action. In 1958, Eisenhower sent 15,000 U.S. troops to Lebanon to prevent the pro-Western government from falling to a Nasser-inspired revolution. Near the end of his term, his efforts to set up a summit meeting with the Soviets collapsed because of the U-2 incident. In his January 17, 1961 farewell address to the nation, Eisenhower expressed his concerns about the dangers of massive military spending, particularly deficit spending and government contracts to private military manufacturers, and coined the term "military–industrial complex".
On the domestic front, he covertly opposed Joseph McCarthy and contributed to the end of McCarthyism by openly invoking the modern expanded version of executive privilege. He otherwise left most political activity to his Vice President, Richard Nixon. He was a moderate conservative who continued New Deal agencies and expanded Social Security. He also launched the Interstate Highway System, the Defense Advanced Research Projects Agency (DARPA), the establishment of strong science education via the National Defense Education Act, and encouraged peaceful use of nuclear power via amendments to the Atomic Energy Act.
His parents set aside specific times at breakfast and at dinner for daily family Bible reading. Chores were regularly assigned and rotated among all the children, and misbehavior was met with unequivocal discipline, usually from David. His mother, previously a member (with David) of the River Brethren sect of the Mennonites, joined the International Bible Students Association, later known as Jehovah's Witnesses. The Eisenhower home served as the local meeting hall from 1896 to 1915, though Eisenhower never joined the International Bible Students. His later decision to attend West Point saddened his mother, who felt that warfare was "rather wicked," but she did not overrule him. While speaking of himself in 1948, Eisenhower said he was "one of the most deeply religious men I know" though unattached to any "sect or organization". He was baptized in the Presbyterian Church in 1953.
Eisenhower attended Abilene High School and graduated with the class of 1909. As a freshman, he injured his knee and developed a leg infection that extended into his groin, and which his doctor diagnosed as life-threatening. The doctor insisted that the leg be amputated but Dwight refused to allow it, and miraculously recovered, though he had to repeat his freshman year. He and brother Edgar both wanted to attend college, though they lacked the funds. They made a pact to take alternate years at college while the other worked to earn the tuitions.
Edgar took the first turn at school, and Dwight was employed as a night supervisor at the Belle Springs Creamery. Edgar asked for a second year, Dwight consented and worked for a second year. At that time, a friend "Swede" Hazlet was applying to the Naval Academy and urged Dwight to apply to the school, since no tuition was required. Eisenhower requested consideration for either Annapolis or West Point with his U.S. Senator, Joseph L. Bristow. Though Eisenhower was among the winners of the entrance-exam competition, he was beyond the age limit for the Naval Academy. He then accepted an appointment to West Point in 1911.
The Eisenhowers had two sons. Doud Dwight "Icky" Eisenhower was born September 24, 1917, and died of scarlet fever on January 2, 1921, at the age of three; Eisenhower was mostly reticent to discuss his death. Their second son, John Eisenhower (1922–2013), was born in Denver Colorado. John served in the United States Army, retired as a brigadier general, became an author and served as U.S. Ambassador to Belgium from 1969 to 1971. Coincidentally, John graduated from West Point on D-Day, June 6, 1944. He married Barbara Jean Thompson on June 10, 1947. John and Barbara had four children: David, Barbara Ann, Susan Elaine and Mary Jean. David, after whom Camp David is named, married Richard Nixon's daughter Julie in 1968. John died on December 21, 2013.
Eisenhower was a golf enthusiast later in life, and joined the Augusta National Golf Club in 1948. He played golf frequently during and after his presidency and was unreserved in expressing his passion for the game, to the point of golfing during winter; he ordered his golf balls painted black so he could see them better against snow on the ground. He had a small, basic golf facility installed at Camp David, and became close friends with the Augusta National Chairman Clifford Roberts, inviting Roberts to stay at the White House on several occasions. Roberts, an investment broker, also handled the Eisenhower family's investments. Roberts also advised Eisenhower on tax aspects of publishing his memoirs, which proved financially lucrative.
After golf, oil painting was Eisenhower's second hobby. While at Columbia University, Eisenhower began the art after watching Thomas E. Stephens paint Mamie's portrait. Eisenhower painted about 260 oils during the last 20 years of his life to relax, mostly landscapes but also portraits of subjects such as Mamie, their grandchildren, General Montgomery, George Washington, and Abraham Lincoln. Wendy Beckett stated that Eisenhower's work, "simple and earnest, rather cause us to wonder at the hidden depths of this reticent president". A conservative in both art and politics, he in a 1962 speech denounced modern art as "a piece of canvas that looks like a broken-down Tin Lizzie, loaded with paint, has been driven over it."
Angels in the Outfield was Eisenhower's favorite movie. His favorite reading material for relaxation were the Western novels of Zane Grey. With his excellent memory and ability to focus, Eisenhower was skilled at card games. He learned poker, which he called his "favorite indoor sport," in Abilene. Eisenhower recorded West Point classmates' poker losses for payment after graduation, and later stopped playing because his opponents resented having to pay him. A classmate reported that after learning to play contract bridge at West Point, Eisenhower played the game six nights a week for five months.
When the U.S. entered World War I he immediately requested an overseas assignment but was again denied and then assigned to Ft. Leavenworth, Kansas. In February 1918 he was transferred to Camp Meade in Maryland with the 65th Engineers. His unit was later ordered to France but to his chagrin he received orders for the new tank corps, where he was promoted to brevet Lieutenant Colonel in the National Army. He commanded a unit that trained tank crews at Camp Colt – his first command – at the site of "Pickett's Charge" on the Gettysburg, Pennsylvania Civil War battleground. Though Eisenhower and his tank crews never saw combat, he displayed excellent organizational skills, as well as an ability to accurately assess junior officers' strengths and make optimal placements of personnel.
Once again his spirits were raised when the unit under his command received orders overseas to France. This time his wishes were thwarted when the armistice was signed, just a week before departure. Completely missing out on the warfront left him depressed and bitter for a time, despite being given the Distinguished Service Medal for his work at home.[citation needed] In World War II, rivals who had combat service in the first great war (led by Gen. Bernard Montgomery) sought to denigrate Eisenhower for his previous lack of combat duty, despite his stateside experience establishing a camp, completely equipped, for thousands of troops, and developing a full combat training schedule.
He assumed duties again at Camp Meade, Maryland, commanding a battalion of tanks, where he remained until 1922. His schooling continued, focused on the nature of the next war and the role of the tank in it. His new expertise in tank warfare was strengthened by a close collaboration with George S. Patton, Sereno E. Brett, and other senior tank leaders. Their leading-edge ideas of speed-oriented offensive tank warfare were strongly discouraged by superiors, who considered the new approach too radical and preferred to continue using tanks in a strictly supportive role for the infantry. Eisenhower was even threatened with court martial for continued publication of these proposed methods of tank deployment, and he relented.
From 1920, Eisenhower served under a succession of talented generals – Fox Conner, John J. Pershing, Douglas MacArthur and George Marshall. He first became executive officer to General Conner in the Panama Canal Zone, where, joined by Mamie, he served until 1924. Under Conner's tutelage, he studied military history and theory (including Carl von Clausewitz's On War), and later cited Conner's enormous influence on his military thinking, saying in 1962 that "Fox Conner was the ablest man I ever knew." Conner's comment on Eisenhower was, "[He] is one of the most capable, efficient and loyal officers I have ever met." On Conner's recommendation, in 1925–26 he attended the Command and General Staff College at Fort Leavenworth, Kansas, where he graduated first in a class of 245 officers. He then served as a battalion commander at Fort Benning, Georgia, until 1927.
During the late 1920s and early 1930s, Eisenhower's career in the post-war army stalled somewhat, as military priorities diminished; many of his friends resigned for high-paying business jobs. He was assigned to the American Battle Monuments Commission directed by General Pershing, and with the help of his brother Milton Eisenhower, then a journalist at the Agriculture Department, he produced a guide to American battlefields in Europe. He then was assigned to the Army War College and graduated in 1928. After a one-year assignment in France, Eisenhower served as executive officer to General George V. Mosely, Assistant Secretary of War, from 1929 to February 1933. Major Dwight D. Eisenhower graduated from the Army Industrial College (Washington, DC) in 1933 and later served on the faculty (it was later expanded to become the Industrial College of the Armed Services and is now known as the Dwight D. Eisenhower School for National Security and Resource Strategy).
His primary duty was planning for the next war, which proved most difficult in the midst of the Great Depression. He then was posted as chief military aide to General MacArthur, Army Chief of Staff. In 1932, he participated in the clearing of the Bonus March encampment in Washington, D.C. Although he was against the actions taken against the veterans and strongly advised MacArthur against taking a public role in it, he later wrote the Army's official incident report, endorsing MacArthur's conduct.
Historians have concluded that this assignment provided valuable preparation for handling the challenging personalities of Winston Churchill, George S. Patton, George Marshall, and General Montgomery during World War II. Eisenhower later emphasized that too much had been made of the disagreements with MacArthur, and that a positive relationship endured. While in Manila, Mamie suffered a life-threatening stomach ailment but recovered fully. Eisenhower was promoted to the rank of permanent lieutenant colonel in 1936. He also learned to fly, making a solo flight over the Philippines in 1937 and obtained his private pilot's license in 1939 at Fort Lewis. Also around this time, he was offered a post by the Philippine Commonwealth Government, namely by then Philippine President Manuel L. Quezon on recommendations by MacArthur, to become the chief of police of a new capital being planned, now named Quezon City, but he declined the offer.
Eisenhower returned to the U.S. in December 1939 and was assigned as a battalion commander and regimental executive officer of the 15th Infantry at Fort Lewis, Washington. In March 1941 he was promoted to colonel and assigned as chief of staff of the newly activated IX Corps under Major General Kenyon Joyce. In June 1941, he was appointed Chief of Staff to General Walter Krueger, Commander of the 3rd Army, at Fort Sam Houston in San Antonio, Texas. After successfully participating in the Louisiana Maneuvers, he was promoted to brigadier general on October 3, 1941. Although his administrative abilities had been noticed, on the eve of the U.S. entry into World War II he had never held an active command above a battalion and was far from being considered by many as a potential commander of major operations.
After the Japanese attack on Pearl Harbor, Eisenhower was assigned to the General Staff in Washington, where he served until June 1942 with responsibility for creating the major war plans to defeat Japan and Germany. He was appointed Deputy Chief in charge of Pacific Defenses under the Chief of War Plans Division (WPD), General Leonard T. Gerow, and then succeeded Gerow as Chief of the War Plans Division. Next, he was appointed Assistant Chief of Staff in charge of the new Operations Division (which replaced WPD) under Chief of Staff General George C. Marshall, who spotted talent and promoted accordingly.
At the end of May 1942, Eisenhower accompanied Lt. Gen. Henry H. Arnold, commanding general of the Army Air Forces, to London to assess the effectiveness of the theater commander in England, Maj. Gen. James E. Chaney. He returned to Washington on June 3 with a pessimistic assessment, stating he had an "uneasy feeling" about Chaney and his staff. On June 23, 1942, he returned to London as Commanding General, European Theater of Operations (ETOUSA), based in London and with a house on Coombe, Kingston upon Thames, and replaced Chaney. He was promoted to lieutenant general on July 7.
In November 1942, he was also appointed Supreme Commander Allied Expeditionary Force of the North African Theater of Operations (NATOUSA) through the new operational Headquarters Allied (Expeditionary) Force Headquarters (A(E)FHQ). The word "expeditionary" was dropped soon after his appointment for security reasons. The campaign in North Africa was designated Operation Torch and was planned underground within the Rock of Gibraltar. Eisenhower was the first non-British person to command Gibraltar in 200 years.
French cooperation was deemed necessary to the campaign, and Eisenhower encountered a "preposterous situation" with the multiple rival factions in France. His primary objective was to move forces successfully into Tunisia, and intending to facilitate that objective, he gave his support to François Darlan as High Commissioner in North Africa, despite Darlan's previous high offices of state in Vichy France and his continued role as commander-in-chief of the French armed forces. The Allied leaders were "thunderstruck" by this from a political standpoint, though none of them had offered Eisenhower guidance with the problem in the course of planning the operation. Eisenhower was severely criticized for the move. Darlan was assassinated on December 24 by Fernand Bonnier de La Chapelle. Eisenhower did not take action to prevent the arrest and extrajudicial execution of Bonnier de La Chapelle by associates of Darlan acting without authority from either Vichy or the Allies, considering it a criminal rather than a military matter. Eisenhower later appointed General Henri Giraud as High Commissioner, who had been installed by the Allies as Darlan's commander-in-chief, and who had refused to postpone the execution.
Operation Torch also served as a valuable training ground for Eisenhower's combat command skills; during the initial phase of Generalfeldmarschall Erwin Rommel's move into the Kasserine Pass, Eisenhower created some confusion in the ranks by some interference with the execution of battle plans by his subordinates. He also was initially indecisive in his removal of Lloyd Fredendall, commanding U.S. II Corps. He became more adroit in such matters in later campaigns. In February 1943, his authority was extended as commander of AFHQ across the Mediterranean basin to include the British Eighth Army, commanded by General Sir Bernard Montgomery. The Eighth Army had advanced across the Western Desert from the east and was ready for the start of the Tunisia Campaign. Eisenhower gained his fourth star and gave up command of ETOUSA to become commander of NATOUSA.
After the capitulation of Axis forces in North Africa, Eisenhower oversaw the highly successful invasion of Sicily. Once Mussolini, the Italian leader, had fallen in Italy, the Allies switched their attention to the mainland with Operation Avalanche. But while Eisenhower argued with President Roosevelt and British Prime Minister Churchill, who both insisted on unconditional terms of surrender in exchange for helping the Italians, the Germans pursued an aggressive buildup of forces in the country – making the job more difficult, by adding 19 divisions and initially outnumbering the Allied forces 2 to 1; nevertheless, the invasion of Italy was highly successful.
In December 1943, President Roosevelt decided that Eisenhower – not Marshall – would be Supreme Allied Commander in Europe. The following month, he resumed command of ETOUSA and the following month was officially designated as the Supreme Allied Commander of the Allied Expeditionary Force (SHAEF), serving in a dual role until the end of hostilities in Europe in May 1945. He was charged in these positions with planning and carrying out the Allied assault on the coast of Normandy in June 1944 under the code name Operation Overlord, the liberation of Western Europe and the invasion of Germany.
Eisenhower, as well as the officers and troops under him, had learned valuable lessons in their previous operations, and their skills had all strengthened in preparation for the next most difficult campaign against the Germans—a beach landing assault. His first struggles, however, were with Allied leaders and officers on matters vital to the success of the Normandy invasion; he argued with Roosevelt over an essential agreement with De Gaulle to use French resistance forces in covert and sabotage operations against the Germans in advance of Overlord. Admiral Ernest J. King fought with Eisenhower over King's refusal to provide additional landing craft from the Pacific. He also insisted that the British give him exclusive command over all strategic air forces to facilitate Overlord, to the point of threatening to resign unless Churchill relented, as he did. Eisenhower then designed a bombing plan in France in advance of Overlord and argued with Churchill over the latter's concern with civilian casualties; de Gaulle interjected that the casualties were justified in shedding the yoke of the Germans, and Eisenhower prevailed. He also had to skillfully manage to retain the services of the often unruly George S. Patton, by severely reprimanding him when Patton earlier had slapped a subordinate, and then when Patton gave a speech in which he made improper comments about postwar policy.
The D-Day Normandy landings on June 6, 1944, were costly but successful. A month later, the invasion of Southern France took place, and control of forces in the southern invasion passed from the AFHQ to the SHAEF. Many prematurely considered that victory in Europe would come by summer's end—however the Germans did not capitulate for almost a year. From then until the end of the war in Europe on May 8, 1945, Eisenhower, through SHAEF, commanded all Allied forces, and through his command of ETOUSA had administrative command of all U.S. forces on the Western Front north of the Alps. He was ever mindful of the inevitable loss of life and suffering that would be experienced on an individual level by the troops under his command and their families. This prompted him to make a point of visiting every division involved in the invasion. Eisenhower's sense of responsibility was underscored by his draft of a statement to be issued if the invasion failed. It has been called one of the great speeches of history:
Once the coastal assault had succeeded, Eisenhower insisted on retaining personal control over the land battle strategy, and was immersed in the command and supply of multiple assaults through France on Germany. Field Marshal Montgomery insisted priority be given to his 21st Army Group's attack being made in the north, while Generals Bradley (12th U.S. Army Group) and Devers (Sixth U.S. Army Group) insisted they be given priority in the center and south of the front (respectively). Eisenhower worked tirelessly to address the demands of the rival commanders to optimize Allied forces, often by giving them tactical, though sometimes ineffective, latitude; many historians conclude this delayed the Allied victory in Europe. However, due to Eisenhower's persistence, the pivotal supply port at Antwerp was successfully, albeit belatedly, opened in late 1944, and victory became a more distinct probability.
In recognition of his senior position in the Allied command, on December 20, 1944, he was promoted to General of the Army, equivalent to the rank of Field Marshal in most European armies. In this and the previous high commands he held, Eisenhower showed his great talents for leadership and diplomacy. Although he had never seen action himself, he won the respect of front-line commanders. He interacted adeptly with allies such as Winston Churchill, Field Marshal Bernard Montgomery and General Charles de Gaulle. He had serious disagreements with Churchill and Montgomery over questions of strategy, but these rarely upset his relationships with them. He dealt with Soviet Marshal Zhukov, his Russian counterpart, and they became good friends.
The Germans launched a surprise counter offensive, in the Battle of the Bulge in December 1944, which the Allies turned back in early 1945 after Eisenhower repositioned his armies and improved weather allowed the Air Force to engage. German defenses continued to deteriorate on both the eastern front with the Soviets and the western front with the Allies. The British wanted Berlin, but Eisenhower decided it would be a military mistake for him to attack Berlin, and said orders to that effect would have to be explicit. The British backed down, but then wanted Eisenhower to move into Czechoslovakia for political reasons. Washington refused to support Churchill's plan to use Eisenhower's army for political maneuvers against Moscow. The actual division of Germany followed the lines that Roosevelt, Churchill and Stalin had previously agreed upon. The Soviet Red Army captured Berlin in a very large-scale bloody battle, and the Germans finally surrendered on May 7, 1945.
Following the German unconditional surrender, Eisenhower was appointed Military Governor of the U.S. Occupation Zone, based at the IG Farben Building in Frankfurt am Main. He had no responsibility for the other three zones, controlled by Britain, France and the Soviet Union, except for the city of Berlin, which was managed by the Four-Power Authorities through the Allied Kommandatura as the governing body. Upon discovery of the Nazi concentration camps, he ordered camera crews to document evidence of the atrocities in them for use in the Nuremberg Trials. He reclassified German prisoners of war (POWs) in U.S. custody as Disarmed Enemy Forces (DEFs), who were no longer subject to the Geneva Convention. Eisenhower followed the orders laid down by the Joint Chiefs of Staff (JCS) in directive JCS 1067, but softened them by bringing in 400,000 tons of food for civilians and allowing more fraternization. In response to the devastation in Germany, including food shortages and an influx of refugees, he arranged distribution of American food and medical equipment. His actions reflected the new American attitudes of the German people as Nazi victims not villains, while aggressively purging the ex-Nazis.
In November 1945, Eisenhower returned to Washington to replace Marshall as Chief of Staff of the Army. His main role was rapid demobilization of millions of soldiers, a slow job that was delayed by lack of shipping. Eisenhower was convinced in 1946 that the Soviet Union did not want war and that friendly relations could be maintained; he strongly supported the new United Nations and favored its involvement in the control of atomic bombs. However, in formulating policies regarding the atomic bomb and relations with the Soviets, Truman was guided by the U.S. State Department and ignored Eisenhower and the Pentagon. Indeed, Eisenhower had opposed the use of the atomic bomb against the Japanese, writing, "First, the Japanese were ready to surrender and it wasn't necessary to hit them with that awful thing. Second, I hated to see our country be the first to use such a weapon." Initially, Eisenhower was characterized by hopes for cooperation with the Soviets. He even visited Warsaw in 1945. Invited by Bolesław Bierut and decorated with the highest military decoration, he was shocked by the scale of destruction in the city. However, by mid-1947, as East–West tensions over economic recovery in Germany and the Greek Civil War escalated, Eisenhower gave up and agreed with a containment policy to stop Soviet expansion.
In June 1943 a visiting politician had suggested to Eisenhower that he might become President of the United States after the war. Believing that a general should not participate in politics, one author later wrote that "figuratively speaking, [Eisenhower] kicked his political-minded visitor out of his office". As others asked him about his political future, Eisenhower told one that he could not imagine wanting to be considered for any political job "from dogcatcher to Grand High Supreme King of the Universe", and another that he could not serve as Army Chief of Staff if others believed he had political ambitions. In 1945 Truman told Eisenhower during the Potsdam Conference that if desired, the president would help the general win the 1948 election, and in 1947 he offered to run as Eisenhower's running mate on the Democratic ticket if MacArthur won the Republican nomination.
As the election approached, other prominent citizens and politicians from both parties urged Eisenhower to run for president. In January 1948, after learning of plans in New Hampshire to elect delegates supporting him for the forthcoming Republican National Convention, Eisenhower stated through the Army that he was "not available for and could not accept nomination to high political office"; "life-long professional soldiers", he wrote, "in the absence of some obvious and overriding reason, [should] abstain from seeking high political office". Eisenhower maintained no political party affiliation during this time. Many believed he was forgoing his only opportunity to be president; Republican Thomas E. Dewey was considered the other probable winner, would presumably serve two terms, and Eisenhower, at age 66 in 1956, would then be too old.
In 1948, Eisenhower became President of Columbia University, an Ivy League university in New York City. The assignment was described as not being a good fit in either direction. During that year Eisenhower's memoir, Crusade in Europe, was published. Critics regarded it as one of the finest U.S. military memoirs, and it was a major financial success as well. Eisenhower's profit on the book was substantially aided by an unprecedented ruling by the U.S. Department of the Treasury that Eisenhower was not a professional writer, but rather, marketing the lifetime asset of his experiences, and thus he only had to pay capital gains tax on his $635,000 advance instead of the much higher personal tax rate. This ruling saved Eisenhower about $400,000.
Eisenhower's stint as the president of Columbia University was punctuated by his activity within the Council on Foreign Relations, a study group he led as president concerning the political and military implications of the Marshall Plan, and The American Assembly, Eisenhower's "vision of a great cultural center where business, professional and governmental leaders could meet from time to time to discuss and reach conclusions concerning problems of a social and political nature". His biographer Blanche Wiesen Cook suggested that this period served as "the political education of General Eisenhower", since he had to prioritize wide-ranging educational, administrative, and financial demands for the university. Through his involvement in the Council on Foreign Relations, he also gained exposure to economic analysis, which would become the bedrock of his understanding in economic policy. "Whatever General Eisenhower knows about economics, he has learned at the study group meetings," one Aid to Europe member claimed.
Within months of beginning his tenure as the president of the university, Eisenhower was requested to advise U.S. Secretary of Defense James Forrestal on the unification of the armed services. About six months after his appointment, he became the informal Chairman of the Joint Chiefs of Staff in Washington. Two months later he fell ill, and he spent over a month in recovery at the Augusta National Golf Club. He returned to his post in New York in mid-May, and in July 1949 took a two-month vacation out-of-state. Because the American Assembly had begun to take shape, he traveled around the country during mid-to-late 1950, building financial support from Columbia Associates, an alumni association.
The contacts gained through university and American Assembly fund-raising activities would later become important supporters in Eisenhower's bid for the Republican party nomination and the presidency. Meanwhile, Columbia University's liberal faculty members became disenchanted with the university president's ties to oilmen and businessmen, including Leonard McCollum, the president of Continental Oil; Frank Abrams, the chairman of Standard Oil of New Jersey; Bob Kleberg, the president of the King Ranch; H. J. Porter, a Texas oil executive; Bob Woodruff, the president of the Coca-Cola Corporation; and Clarence Francis, the chairman of General Foods.
The trustees of Columbia University refused to accept Eisenhower's resignation in December 1950, when he took an extended leave from the university to become the Supreme Commander of the North Atlantic Treaty Organization (NATO), and he was given operational command of NATO forces in Europe. Eisenhower retired from active service as an Army general on May 31, 1952, and he resumed his presidency of Columbia. He held this position until January 20, 1953, when he became the President of the United States.
President Truman, symbolizing a broad-based desire for an Eisenhower candidacy for president, again in 1951 pressed him to run for the office as a Democrat. It was at this time that Eisenhower voiced his disagreements with the Democratic party and declared himself and his family to be Republicans. A "Draft Eisenhower" movement in the Republican Party persuaded him to declare his candidacy in the 1952 presidential election to counter the candidacy of non-interventionist Senator Robert A. Taft. The effort was a long struggle; Eisenhower had to be convinced that political circumstances had created a genuine duty for him to offer himself as a candidate, and that there was a mandate from the populace for him to be their President. Henry Cabot Lodge, who served as his campaign manager, and others succeeded in convincing him, and in June 1952 he resigned his command at NATO to campaign full-time. Eisenhower defeated Taft for the nomination, having won critical delegate votes from Texas. Eisenhower's campaign was noted for the simple but effective slogan, "I Like Ike". It was essential to his success that Eisenhower express opposition to Roosevelt's policy at Yalta and against Truman's policies in Korea and China—matters in which he had once participated. In defeating Taft for the nomination, it became necessary for Eisenhower to appease the right wing Old Guard of the Republican Party; his selection of Richard M. Nixon as the Vice-President on the ticket was designed in part for that purpose. Nixon also provided a strong anti-communist presence as well as some youth to counter Ike's more advanced age.
In the general election, against the advice of his advisors, Eisenhower insisted on campaigning in the South, refusing to surrender the region to the Democratic Party. The campaign strategy, dubbed "K1C2", was to focus on attacking the Truman and Roosevelt administrations on three issues: Korea, Communism and corruption. In an effort to accommodate the right, he stressed that the liberation of Eastern Europe should be by peaceful means only; he also distanced himself from his former boss President Truman.
Two controversies during the campaign tested him and his staff, but did not affect the campaign. One involved a report that Nixon had improperly received funds from a secret trust. Nixon spoke out adroitly to avoid potential damage, but the matter permanently alienated the two candidates. The second issue centered on Eisenhower's relented decision to confront the controversial methods of Joseph McCarthy on his home turf in a Wisconsin appearance. Just two weeks prior to the election, Eisenhower vowed to go to Korea and end the war there. He promised to maintain a strong commitment against Communism while avoiding the topic of NATO; finally, he stressed a corruption-free, frugal administration at home.
Eisenhower was the last president born in the 19th century, and at age 62, was the oldest man elected President since James Buchanan in 1856 (President Truman stood at 64 in 1948 as the incumbent president at the time of his election four years earlier). Eisenhower was the only general to serve as President in the 20th century and the most recent President to have never held elected office prior to the Presidency (The other Presidents who did not have prior elected office were Zachary Taylor, Ulysses S. Grant, William Howard Taft and Herbert Hoover).
Due to a complete estrangement between the two as a result of campaigning, Truman and Eisenhower had minimal discussions about the transition of administrations. After selecting his budget director, Joseph M. Dodge, Eisenhower asked Herbert Brownell and Lucius Clay to make recommendations for his cabinet appointments. He accepted their recommendations without exception; they included John Foster Dulles and George M. Humphrey with whom he developed his closest relationships, and one woman, Oveta Culp Hobby. Eisenhower's cabinet, consisting of several corporate executives and one labor leader, was dubbed by one journalist, "Eight millionaires and a plumber." The cabinet was notable for its lack of personal friends, office seekers, or experienced government administrators. He also upgraded the role of the National Security Council in planning all phases of the Cold War.
Prior to his inauguration, Eisenhower led a meeting of advisors at Pearl Harbor addressing foremost issues; agreed objectives were to balance the budget during his term, to bring the Korean War to an end, to defend vital interests at lower cost through nuclear deterrent, and to end price and wage controls. Eisenhower also conducted the first pre-inaugural cabinet meeting in history in late 1952; he used this meeting to articulate his anti-communist Russia policy. His inaugural address, as well, was exclusively devoted to foreign policy and included this same philosophy, as well as a commitment to foreign trade and the United Nations.
Throughout his presidency, Eisenhower adhered to a political philosophy of dynamic conservatism. A self-described "progressive conservative," he continued all the major New Deal programs still in operation, especially Social Security. He expanded its programs and rolled them into a new cabinet-level agency, the Department of Health, Education and Welfare, while extending benefits to an additional ten million workers. He implemented integration in the Armed Services in two years, which had not been completed under Truman.
As the 1954 congressional elections approached, and it became evident that the Republicans were in danger of losing their thin majority in both houses, Eisenhower was among those blaming the Old Guard for the losses, and took up the charge to stop suspected efforts by the right wing to take control of the GOP. Eisenhower then articulated his position as a moderate, progressive Republican: "I have just one purpose ... and that is to build up a strong progressive Republican Party in this country. If the right wing wants a fight, they are going to get it ... before I end up, either this Republican Party will reflect progressivism or I won't be with them anymore."
Initially Eisenhower planned on serving only one term, but as with other decisions, he maintained a position of maximum flexibility in case leading Republicans wanted him to run again. During his recovery from a heart attack late in 1955, he huddled with his closest advisors to evaluate the GOP's potential candidates; the group, in addition to his doctor, concluded a second term was well advised, and he announced in February 1956 he would run again. Eisenhower was publicly noncommittal about Nixon's repeating as the Vice President on his ticket; the question was an especially important one in light of his heart condition. He personally favored Robert B. Anderson, a Democrat, who rejected his offer; Eisenhower then resolved to leave the matter in the hands of the party. In 1956, Eisenhower faced Adlai Stevenson again and won by an even larger landslide, with 457 of 531 electoral votes and 57.6% of the popular vote. The level of campaigning was curtailed out of health considerations.
Eisenhower's goal to create improved highways was influenced by difficulties encountered during his involvement in the U.S. Army's 1919 Transcontinental Motor Convoy. He was assigned as an observer for the mission, which involved sending a convoy of U.S. Army vehicles coast to coast. His subsequent experience with encountering German autobahn limited-access road systems during the concluding stages of World War II convinced him of the benefits of an Interstate Highway System. Noticing the improved ability to move logistics throughout the country, he thought an Interstate Highway System in the U.S. would not only be beneficial for military operations, but provide a measure of continued economic growth. The legislation initially stalled in the Congress over the issuance of bonds to finance the project, but the legislative effort was renewed and the law was signed by Eisenhower in June 1956.
In 1953, the Republican Party's Old Guard presented Eisenhower with a dilemma by insisting he disavow the Yalta Agreements as beyond the constitutional authority of the Executive Branch; however, the death of Joseph Stalin in March 1953 made the matter a practical moot point. At this time Eisenhower gave his Chance for Peace speech in which he attempted, unsuccessfully, to forestall the nuclear arms race with the Soviet Union by suggesting multiple opportunities presented by peaceful uses of nuclear materials. Biographer Stephen Ambrose opined that this was the best speech of Eisenhower's presidency.
The U.N. speech was well received but the Soviets never acted upon it, due to an overarching concern for the greater stockpiles of nuclear weapons in the U.S. arsenal. Indeed, Eisenhower embarked upon a greater reliance on the use of nuclear weapons, while reducing conventional forces, and with them the overall defense budget, a policy formulated as a result of Project Solarium and expressed in NSC 162/2. This approach became known as the "New Look", and was initiated with defense cuts in late 1953.
In 1955 American nuclear arms policy became one aimed primarily at arms control as opposed to disarmament. The failure of negotiations over arms until 1955 was due mainly to the refusal of the Russians to permit any sort of inspections. In talks located in London that year, they expressed a willingness to discuss inspections; the tables were then turned on Eisenhower, when he responded with an unwillingness on the part of the U.S. to permit inspections. In May of that year the Russians agreed to sign a treaty giving independence to Austria, and paved the way for a Geneva summit with the U.S., U.K. and France. At the Geneva Conference Eisenhower presented a proposal called "Open Skies" to facilitate disarmament, which included plans for Russia and the U.S. to provide mutual access to each other's skies for open surveillance of military infrastructure. Russian leader Nikita Khrushchev dismissed the proposal out of hand.
In 1954, Eisenhower articulated the domino theory in his outlook towards communism in Southeast Asia and also in Central America. He believed that if the communists were allowed to prevail in Vietnam, this would cause a succession of countries to fall to communism, from Laos through Malaysia and Indonesia ultimately to India. Likewise, the fall of Guatemala would end with the fall of neighboring Mexico. That year the loss of North Vietnam to the communists and the rejection of his proposed European Defence Community (EDC) were serious defeats, but he remained optimistic in his opposition to the spread of communism, saying "Long faces don't win wars". As he had threatened the French in their rejection of EDC, he afterwards moved to restore West Germany, as a full NATO partner.
With Eisenhower's leadership and Dulles' direction, CIA activities increased under the pretense of resisting the spread of communism in poorer countries; the CIA in part deposed the leaders of Iran in Operation Ajax, of Guatemala through Operation Pbsuccess, and possibly the newly independent Republic of the Congo (Léopoldville). In 1954 Eisenhower wanted to increase surveillance inside the Soviet Union. With Dulles' recommendation, he authorized the deployment of thirty Lockheed U-2's at a cost of $35 million. The Eisenhower administration also planned the Bay of Pigs Invasion to overthrow Fidel Castro in Cuba, which John F. Kennedy was left to carry out."
Over New York City in 1953, Eastern Airlines Flight 8610, a commercial flight, had a near miss with Air Force Flight 8610, a Lockheed C-121 Constellation known as Columbine II, while the latter was carrying President Eisenhower. This prompted the adoption of the unique call sign Air Force One, to be used whenever the president is on board any US Air Force aircraft. Columbine II is the only presidential aircraft to have ever been sold to the public and is the only remaining presidential aircraft left unrestored and not on public display.
On the whole, Eisenhower's support of the nation's fledgling space program was officially modest until the Soviet launch of Sputnik in 1957, gaining the Cold War enemy enormous prestige around the world. He then launched a national campaign that funded not just space exploration but a major strengthening of science and higher education. His Open Skies Policy attempted to legitimize illegal Lockheed U-2 flyovers and Project Genetrix while paving the way for spy satellite technology to orbit over sovereign territory, created NASA as a civilian space agency, signed a landmark science education law, and fostered improved relations with American scientists.
In late 1952 Eisenhower went to Korea and discovered a military and political stalemate. Once in office, when the Chinese began a buildup in the Kaesong sanctuary, he threatened to use nuclear force if an armistice was not concluded. His earlier military reputation in Europe was effective with the Chinese. The National Security Council, the Joint Chiefs of Staff, and the Strategic Air Command (SAC) devised detailed plans for nuclear war against China. With the death of Stalin in early March 1953, Russian support for a Chinese hard-line weakened and China decided to compromise on the prisoner issue.
In July 1953, an armistice took effect with Korea divided along approximately the same boundary as in 1950. The armistice and boundary remain in effect today, with American soldiers stationed there to guarantee it. The armistice, concluded despite opposition from Secretary Dulles, South Korean President Syngman Rhee, and also within Eisenhower's party, has been described by biographer Ambrose as the greatest achievement of the administration. Eisenhower had the insight to realize that unlimited war in the nuclear age was unthinkable, and limited war unwinnable.
In November 1956, Eisenhower forced an end to the combined British, French and Israeli invasion of Egypt in response to the Suez Crisis, receiving praise from Egyptian president Gamal Abdel Nasser. Simultaneously he condemned the brutal Soviet invasion of Hungary in response to the Hungarian Revolution of 1956. He publicly disavowed his allies at the United Nations, and used financial and diplomatic pressure to make them withdraw from Egypt. Eisenhower explicitly defended his strong position against Britain and France in his memoirs, which were published in 1965.
Early in 1953, the French asked Eisenhower for help in French Indochina against the Communists, supplied from China, who were fighting the First Indochina War. Eisenhower sent Lt. General John W. "Iron Mike" O'Daniel to Vietnam to study and assess the French forces there. Chief of Staff Matthew Ridgway dissuaded the President from intervening by presenting a comprehensive estimate of the massive military deployment that would be necessary. Eisenhower stated prophetically that "this war would absorb our troops by divisions."
Eisenhower did provide France with bombers and non-combat personnel. After a few months with no success by the French, he added other aircraft to drop napalm for clearing purposes. Further requests for assistance from the French were agreed to but only on conditions Eisenhower knew were impossible to meet – allied participation and congressional approval. When the French fortress of Dien Bien Phu fell to the Vietnamese Communists in May 1954, Eisenhower refused to intervene despite urgings from the Chairman of the Joint Chiefs, the Vice President and the head of NCS.
Eisenhower responded to the French defeat with the formation of the SEATO (Southeast Asia Treaty Organization) Alliance with the U.K., France, New Zealand and Australia in defense of Vietnam against communism. At that time the French and Chinese reconvened Geneva peace talks; Eisenhower agreed the U.S. would participate only as an observer. After France and the Communists agreed to a partition of Vietnam, Eisenhower rejected the agreement, offering military and economic aid to southern Vietnam. Ambrose argues that Eisenhower, by not participating in the Geneva agreement, had kept the U.S out of Vietnam; nevertheless, with the formation of SEATO, he had in the end put the U.S. back into the conflict.
In late 1954, Gen. J. Lawton Collins was made ambassador to "Free Vietnam" (the term South Vietnam came into use in 1955), effectively elevating the country to sovereign status. Collins' instructions were to support the leader Ngo Dinh Diem in subverting communism, by helping him to build an army and wage a military campaign. In February 1955, Eisenhower dispatched the first American soldiers to Vietnam as military advisors to Diem's army. After Diem announced the formation of the Republic of Vietnam (RVN, commonly known as South Vietnam) in October, Eisenhower immediately recognized the new state and offered military, economic, and technical assistance.
In the years that followed, Eisenhower increased the number of U.S. military advisors in South Vietnam to 900 men. This was due to North Vietnam's support of "uprisings" in the south and concern the nation would fall. In May 1957 Diem, then President of South Vietnam, made a state visit to the United States for ten days. President Eisenhower pledged his continued support, and a parade was held in Diem's honor in New York City. Although Diem was publicly praised, in private Secretary of State John Foster Dulles conceded that Diem had been selected because there were no better alternatives.
On May 1, 1960, a U.S. one-man U-2 spy plane was reportedly shot down at high altitude over Soviet Union airspace. The flight was made to gain photo intelligence before the scheduled opening of an East–West summit conference, which had been scheduled in Paris, 15 days later. Captain Francis Gary Powers had bailed out of his aircraft and was captured after parachuting down onto Russian soil. Four days after Powers disappeared, the Eisenhower Administration had NASA issue a very detailed press release noting that an aircraft had "gone missing" north of Turkey. It speculated that the pilot might have fallen unconscious while the autopilot was still engaged, and falsely claimed that "the pilot reported over the emergency frequency that he was experiencing oxygen difficulties."
Soviet Premier Nikita Khrushchev announced that a "spy-plane" had been shot down but intentionally made no reference to the pilot. As a result, the Eisenhower Administration, thinking the pilot had died in the crash, authorized the release of a cover story claiming that the plane was a "weather research aircraft" which had unintentionally strayed into Soviet airspace after the pilot had radioed "difficulties with his oxygen equipment" while flying over Turkey. The Soviets put Captain Powers on trial and displayed parts of the U-2, which had been recovered almost fully intact.
The 1960 Four Power Paris Summit between President Dwight Eisenhower, Nikita Khrushchev, Harold Macmillan and Charles de Gaulle collapsed because of the incident. Eisenhower refused to accede to Khrushchev's demands that he apologize. Therefore, Khrushchev would not take part in the summit. Up until this event, Eisenhower felt he had been making progress towards better relations with the Soviet Union. Nuclear arms reduction and Berlin were to have been discussed at the summit. Eisenhower stated it had all been ruined because of that "stupid U-2 business".
While President Truman had begun the process of desegregating the Armed Forces in 1948, actual implementation had been slow. Eisenhower made clear his stance in his first State of the Union address in February 1953, saying "I propose to use whatever authority exists in the office of the President to end segregation in the District of Columbia, including the Federal Government, and any segregation in the Armed Forces". When he encountered opposition from the services, he used government control of military spending to force the change through, stating "Wherever Federal Funds are expended ..., I do not see how any American can justify ... a discrimination in the expenditure of those funds".
Eisenhower told District of Columbia officials to make Washington a model for the rest of the country in integrating black and white public school children. He proposed to Congress the Civil Rights Act of 1957 and of 1960 and signed those acts into law. The 1957 act for the first time established a permanent civil rights office inside the Justice Department and a Civil Rights Commission to hear testimony about abuses of voting rights. Although both acts were much weaker than subsequent civil rights legislation, they constituted the first significant civil rights acts since 1875.
In 1957, the state of Arkansas refused to honor a federal court order to integrate their public school system stemming from the Brown decision. Eisenhower demanded that Arkansas governor Orval Faubus obey the court order. When Faubus balked, the president placed the Arkansas National Guard under federal control and sent in the 101st Airborne Division. They escorted and protected nine black students' entry to Little Rock Central High School, an all-white public school, for the first time since the Reconstruction Era. Martin Luther King Jr. wrote to Eisenhower to thank him for his actions, writing "The overwhelming majority of southerners, Negro and white, stand firmly behind your resolute action to restore law and order in Little Rock".
This prevented Eisenhower from openly condemning Joseph McCarthy's highly criticized methods against communism. To facilitate relations with Congress, Eisenhower decided to ignore McCarthy's controversies and thereby deprive them of more energy from involvement of the White House. This position drew criticism from a number of corners. In late 1953 McCarthy declared on national television that the employment of communists within the government was a menace and would be a pivotal issue in the 1954 Senate elections. Eisenhower was urged to respond directly and specify the various measures he had taken to purge the government of communists. Nevertheless, he refused.
Among Ike's objectives in not directly confronting McCarthy was to prevent McCarthy from dragging the Atomic Energy Commission (AEC) into McCarthy's witch hunt for communists, which would interfere with, and perhaps delay, the AEC's important work on H-bombs. The administration had discovered through its own investigations that one of the leading scientists on the AEC, J. Robert Oppenheimer, had urged that the H-bomb work be delayed. Eisenhower removed him from the agency and revoked his security clearance, though he knew this would create fertile ground for McCarthy.
In May 1955, McCarthy threatened to issue subpoenas to White House personnel. Eisenhower was furious, and issued an order as follows: "It is essential to efficient and effective administration that employees of the Executive Branch be in a position to be completely candid in advising with each other on official matters ... it is not in the public interest that any of their conversations or communications, or any documents or reproductions, concerning such advice be disclosed." This was an unprecedented step by Eisenhower to protect communication beyond the confines of a cabinet meeting, and soon became a tradition known as executive privilege. Ike's denial of McCarthy's access to his staff reduced McCarthy's hearings to rants about trivial matters, and contributed to his ultimate downfall.
The Democrats gained a majority in both houses in the 1954 election. Eisenhower had to work with the Democratic Majority Leader Lyndon B. Johnson (later U.S. president) in the Senate and Speaker Sam Rayburn in the House, both from Texas. Joe Martin, the Republican Speaker from 1947 to 1949 and again from 1953 to 1955, wrote that Eisenhower "never surrounded himself with assistants who could solve political problems with professional skill. There were exceptions, Leonard W. Hall, for example, who as chairman of the Republican National Committee tried to open the administration's eyes to the political facts of life, with occasional success. However, these exceptions were not enough to right the balance."
Speaker Martin concluded that Eisenhower worked too much through subordinates in dealing with Congress, with results, "often the reverse of what he has desired" because Members of Congress, "resent having some young fellow who was picked up by the White House without ever having been elected to office himself coming around and telling them 'The Chief wants this'. The administration never made use of many Republicans of consequence whose services in one form or another would have been available for the asking."
Whittaker was unsuited for the role and soon retired. Stewart and Harlan were conservative Republicans, while Brennan was a Democrat who became a leading voice for liberalism. In selecting a Chief Justice, Eisenhower looked for an experienced jurist who could appeal to liberals in the party as well as law-and-order conservatives, noting privately that Warren "represents the kind of political, economic, and social thinking that I believe we need on the Supreme Court ... He has a national name for integrity, uprightness, and courage that, again, I believe we need on the Court". In the next few years Warren led the Court in a series of liberal decisions that revolutionized the role of the Court.
Eisenhower began smoking cigarettes at West Point, often two or three packs a day. Eisenhower stated that he "gave [himself] an order" to stop cold turkey in March 1949 while at Columbia. He was probably the first president to release information about his health and medical records while in office. On September 24, 1955, while vacationing in Colorado, he had a serious heart attack that required six weeks' hospitalization, during which time Nixon, Dulles, and Sherman Adams assumed administrative duties and provided communication with the President. He was treated by Dr. Paul Dudley White, a cardiologist with a national reputation, who regularly informed the press of the President's progress. Instead of eliminating him as a candidate for a second term as President, his physician recommended a second term as essential to his recovery.
As a consequence of his heart attack, Eisenhower developed a left ventricular aneurysm, which was in turn the cause of a mild stroke on November 25, 1957. This incident occurred during a cabinet meeting when Eisenhower suddenly found himself unable to speak or move his right hand. The stroke had caused an aphasia. The president also suffered from Crohn's disease, chronic inflammatory condition of the intestine, which necessitated surgery for a bowel obstruction on June 9, 1956. To treat the intestinal block, surgeons bypassed about ten inches of his small intestine. His scheduled meeting with Indian Prime Minister Jawaharlal Nehru was postponed so he could recover from surgery at his farm in Gettysburg, Pennsylvania. He was still recovering from this operation during the Suez Crisis. Eisenhower's health issues forced him to give up smoking and make some changes to his dietary habits, but he still indulged in alcohol. During a visit to England he complained of dizziness and had to have his blood pressure checked on August 29, 1959; however, before dinner at Chequers on the next day his doctor General Howard Snyder recalled Eisenhower "drank several gin-and-tonics, and one or two gins on the rocks ... three or four wines with the dinner".
The last three years of Eisenhower's second term in office were ones of relatively good health. Eventually after leaving the White House, he suffered several additional and ultimately crippling heart attacks. A severe heart attack in August 1965 largely ended his participation in public affairs. In August 1966 he began to show symptoms of cholecystitis, for which he underwent surgery on December 12, 1966, when his gallbladder was removed, containing 16 gallstones. After Eisenhower's death in 1969 (see below), an autopsy unexpectedly revealed an adrenal pheochromocytoma, a benign adrenaline-secreting tumor that may have made the President more vulnerable to heart disease. Eisenhower suffered seven heart attacks in total from 1955 until his death.
In the 1960 election to choose his successor, Eisenhower endorsed his own Vice President, Republican Richard Nixon against Democrat John F. Kennedy. He told friends, "I will do almost anything to avoid turning my chair and country over to Kennedy." He actively campaigned for Nixon in the final days, although he may have done Nixon some harm. When asked by reporters at the end of a televised press conference to list one of Nixon's policy ideas he had adopted, Eisenhower joked, "If you give me a week, I might think of one. I don't remember." Kennedy's campaign used the quote in one of its campaign commercials. Nixon narrowly lost to Kennedy. Eisenhower, who was the oldest president in history at that time (then 70), was succeeded by the youngest elected president, as Kennedy was 43.
On January 17, 1961, Eisenhower gave his final televised Address to the Nation from the Oval Office. In his farewell speech, Eisenhower raised the issue of the Cold War and role of the U.S. armed forces. He described the Cold War: "We face a hostile ideology global in scope, atheistic in character, ruthless in purpose and insidious in method ..." and warned about what he saw as unjustified government spending proposals and continued with a warning that "we must guard against the acquisition of unwarranted influence, whether sought or unsought, by the military–industrial complex."
Eisenhower retired to the place where he and Mamie had spent much of their post-war time, a working farm adjacent to the battlefield at Gettysburg, Pennsylvania, only 70 miles from his ancestral home in Elizabethville, Dauphin County, Pennsylvania. In 1967 the Eisenhowers donated the farm to the National Park Service. In retirement, the former president did not completely retreat from political life; he spoke at the 1964 Republican National Convention and appeared with Barry Goldwater in a Republican campaign commercial from Gettysburg. However, his endorsement came somewhat reluctantly because Goldwater had attacked the former president as "a dime-store New Dealer".
On the morning of March 28, 1969, at the age of 78, Eisenhower died in Washington, D.C. of congestive heart failure at Walter Reed Army Medical Center. The following day his body was moved to the Washington National Cathedral's Bethlehem Chapel, where he lay in repose for 28 hours. On March 30, his body was brought by caisson to the United States Capitol, where he lay in state in the Capitol Rotunda. On March 31, Eisenhower's body was returned to the National Cathedral, where he was given an Episcopal Church funeral service.
That evening, Eisenhower's body was placed onto a train en route to Abilene, Kansas, the last time a funeral train has been used as part of funeral proceedings of an American president. His body arrived on April 2, and was interred later that day in a small chapel on the grounds of the Eisenhower Presidential Library. The president's body was buried as a General of the Army. The family used an $80 standard soldier's casket, and dressed Eisenhower's body in his famous short green jacket. His only medals worn were: the Army Distinguished Service Medal with three oak leaf clusters, the Navy Distinguished Service Medal, and the Legion of Merit. Eisenhower is buried alongside his son Doud, who died at age 3 in 1921. His wife Mamie was buried next to him after her death in 1979.
In the immediate years after Eisenhower left office, his reputation declined. He was widely seen by critics as an inactive, uninspiring, golf-playing president compared to his vigorous young successor. Despite his unprecedented use of Army troops to enforce a federal desegregation order at Central High School in Little Rock, Eisenhower was criticized for his reluctance to support the civil rights movement to the degree that activists wanted. Eisenhower also attracted criticism for his handling of the 1960 U-2 incident and the associated international embarrassment, for the Soviet Union's perceived leadership in the nuclear arms race and the Space Race, and for his failure to publicly oppose McCarthyism.
Since the 19th century, many if not all presidents were assisted by a central figure or "gatekeeper", sometimes described as the President's Private Secretary, sometimes with no official title at all. Eisenhower formalized this role, introducing the office of White House Chief of Staff – an idea he borrowed from the United States Army. Every president after Lyndon Johnson has also appointed staff to this position. Initially, Gerald Ford and Jimmy Carter tried to operate without a chief of staff, but each eventually appointed one.
The development of the appreciation medals was initiated by the White House and executed by the Bureau of the Mint through the U.S. Mint in Philadelphia. The medals were struck from September 1958 through October 1960. A total of twenty designs are cataloged with a total mintage of 9,858. Each of the designs incorporates the text "with appreciation" or "with personal and official gratitude" accompanied with Eisenhower's initials "D.D.E." or facsimile signature. The design also incorporates location, date, and/or significant event. Prior to the end of his second term as President, 1,451 medals were turned-in to the Bureau of the Mint and destroyed. The Eisenhower appreciation medals are part of the Presidential Medal of Appreciation Award Medal Series.
The Interstate Highway System is officially known as the 'Dwight D. Eisenhower National System of Interstate and Defense Highways' in his honor. It was inspired in part by Eisenhower's own Army experiences in World War II, where he recognized the advantages of the autobahn systems in Germany, Austria, and Switzerland. Commemorative signs reading "Eisenhower Interstate System" and bearing Eisenhower's permanent 5-star rank insignia were introduced in 1993 and are currently displayed throughout the Interstate System. Several highways are also named for him, including the Eisenhower Expressway (Interstate 290) near Chicago and the Eisenhower Tunnel on Interstate 70 west of Denver.
A loblolly pine, known as the "Eisenhower Pine", was located on Augusta's 17th hole, approximately 210 yards (192 m) from the Masters tee. President Dwight D. Eisenhower, an Augusta National member, hit the tree so many times that, at a 1956 club meeting, he proposed that it be cut down. Not wanting to offend the president, the club's chairman, Clifford Roberts, immediately adjourned the meeting rather than reject the request. The tree was removed in February 2014 after an ice storm caused it significant damage.

A gene is a locus (or region) of DNA that encodes a functional RNA or protein product, and is the molecular unit of heredity.:Glossary The transmission of genes to an organism's offspring is the basis of the inheritance of phenotypic traits. Most biological traits are under the influence of polygenes (many different genes) as well as the gene–environment interactions. Some genetic traits are instantly visible, such as eye colour or number of limbs, and some are not, such as blood type, risk for specific diseases, or the thousands of basic biochemical processes that comprise life.
Genes can acquire mutations in their sequence, leading to different variants, known as alleles, in the population. These alleles encode slightly different versions of a protein, which cause different phenotype traits. Colloquial usage of the term "having a gene" (e.g., "good genes," "hair colour gene") typically refers to having a different allele of the gene. Genes evolve due to natural selection or survival of the fittest of the alleles.
The concept of a gene continues to be refined as new phenomena are discovered. For example, regulatory regions of a gene can be far removed from its coding regions, and coding regions can be split into several exons. Some viruses store their genome in RNA instead of DNA and some gene products are functional non-coding RNAs. Therefore, a broad, modern working definition of a gene is any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product or by regulation of gene expression.
The existence of discrete inheritable units was first suggested by Gregor Mendel (1822–1884). From 1857 to 1864, he studied inheritance patterns in 8000 common edible pea plants, tracking distinct traits from parent to offspring. He described these mathematically as 2n combinations where n is the number of differing characteristics in the original peas. Although he did not use the term gene, he explained his results in terms of discrete inherited units that give rise to observable physical characteristics. This description prefigured the distinction between genotype (the genetic material of an organism) and phenotype (the visible traits of that organism). Mendel was also the first to demonstrate independent assortment, the distinction between dominant and recessive traits, the distinction between a heterozygote and homozygote, and the phenomenon of discontinuous inheritance.
Prior to Mendel's work, the dominant theory of heredity was one of blending inheritance, which suggested that each parent contributed fluids to the fertilisation process and that the traits of the parents blended and mixed to produce the offspring. Charles Darwin developed a theory of inheritance he termed pangenesis, which used the term gemmule to describe hypothetical particles that would mix during reproduction. Although Mendel's work was largely unrecognized after its first publication in 1866, it was 'rediscovered' in 1900 by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak, who claimed to have reached similar conclusions in their own research.
The word gene is derived (via pangene) from the Ancient Greek word γένος (génos) meaning "race, offspring". Gene was coined in 1909 by Danish botanist Wilhelm Johannsen to describe the fundamental physical and functional unit of heredity, while the related word genetics was first used by William Bateson in 1905.
Advances in understanding genes and inheritance continued throughout the 20th century. Deoxyribonucleic acid (DNA) was shown to be the molecular repository of genetic information by experiments in the 1940s to 1950s. The structure of DNA was studied by Rosalind Franklin using X-ray crystallography, which led James D. Watson and Francis Crick to publish a model of the double-stranded DNA molecule whose paired nucleotide bases indicated a compelling hypothesis for the mechanism of genetic replication. Collectively, this body of research established the central dogma of molecular biology, which states that proteins are translated from RNA, which is transcribed from DNA. This dogma has since been shown to have exceptions, such as reverse transcription in retroviruses. The modern study of genetics at the level of DNA is known as molecular genetics.
In 1972, Walter Fiers and his team at the University of Ghent were the first to determine the sequence of a gene: the gene for Bacteriophage MS2 coat protein. The subsequent development of chain-termination DNA sequencing in 1977 by Frederick Sanger improved the efficiency of sequencing and turned it into a routine laboratory tool. An automated version of the Sanger method was used in early phases of the Human Genome Project.
The theories developed in the 1930s and 1940s to integrate molecular genetics with Darwinian evolution are called the modern evolutionary synthesis, a term introduced by Julian Huxley. Evolutionary biologists subsequently refined this concept, such as George C. Williams' gene-centric view of evolution. He proposed an evolutionary concept of the gene as a unit of natural selection with the definition: "that which segregates and recombines with appreciable frequency.":24 In this view, the molecular gene transcribes as a unit, and the evolutionary gene inherits as a unit. Related ideas emphasizing the centrality of genes in evolution were popularized by Richard Dawkins.
The vast majority of living organisms encode their genes in long strands of DNA (deoxyribonucleic acid). DNA consists of a chain made from four types of nucleotide subunits, each composed of: a five-carbon sugar (2'-deoxyribose), a phosphate group, and one of the four bases adenine, cytosine, guanine, and thymine.:2.1
Two chains of DNA twist around each other to form a DNA double helix with the phosphate-sugar backbone spiralling around the outside, and the bases pointing inwards with adenine base pairing to thymine and guanine to cytosine. The specificity of base pairing occurs because adenine and thymine align form two hydrogen bonds, whereas cytosine and guanine form three hydrogen bonds. The two strands in a double helix must therefore be complementary, with their sequence of bases matching such that the adenines of one strand are paired with the thymines of the other strand, and so on.:4.1
Due to the chemical composition of the pentose residues of the bases, DNA strands have directionality. One end of a DNA polymer contains an exposed hydroxyl group on the deoxyribose; this is known as the 3' end of the molecule. The other end contains an exposed phosphate group; this is the 5' end. The two strands of a double-helix run in opposite directions. Nucleic acid synthesis, including DNA replication and transcription occurs in the 5'→3' direction, because new nucleotides are added via a dehydration reaction that uses the exposed 3' hydroxyl as a nucleophile.:27.2
The expression of genes encoded in DNA begins by transcribing the gene into RNA, a second type of nucleic acid that is very similar to DNA, but whose monomers contain the sugar ribose rather than deoxyribose. RNA also contains the base uracil in place of thymine. RNA molecules are less stable than DNA and are typically single-stranded. Genes that encode proteins are composed of a series of three-nucleotide sequences called codons, which serve as the "words" in the genetic "language". The genetic code specifies the correspondence during protein translation between codons and amino acids. The genetic code is nearly the same for all known organisms.:4.1
The total complement of genes in an organism or cell is known as its genome, which may be stored on one or more chromosomes. A chromosome consists of a single, very long DNA helix on which thousands of genes are encoded.:4.2 The region of the chromosome at which a particular gene is located is called its locus. Each locus contains one allele of a gene; however, members of a population may have different alleles at the locus, each with a slightly different gene sequence.
The majority of eukaryotic genes are stored on a set of large, linear chromosomes. The chromosomes are packed within the nucleus in complex with storage proteins called histones to form a unit called a nucleosome. DNA packaged and condensed in this way is called chromatin.:4.2 The manner in which DNA is stored on the histones, as well as chemical modifications of the histone itself, regulate whether a particular region of DNA is accessible for gene expression. In addition to genes, eukaryotic chromosomes contain sequences involved in ensuring that the DNA is copied without degradation of end regions and sorted into daughter cells during cell division: replication origins, telomeres and the centromere.:4.2 Replication origins are the sequence regions where DNA replication is initiated to make two copies of the chromosome. Telomeres are long stretches of repetitive sequence that cap the ends of the linear chromosomes and prevent degradation of coding and regulatory regions during DNA replication. The length of the telomeres decreases each time the genome is replicated and has been implicated in the aging process. The centromere is required for binding spindle fibres to separate sister chromatids into daughter cells during cell division.:18.2
Prokaryotes (bacteria and archaea) typically store their genomes on a single large, circular chromosome. Similarly, some eukaryotic organelles contain a remnant circular chromosome with a small number of genes.:14.4 Prokaryotes sometimes supplement their chromosome with additional small circles of DNA called plasmids, which usually encode only a few genes and are transferable between individuals. For example, the genes for antibiotic resistance are usually encoded on bacterial plasmids and can be passed between individual cells, even those of different species, via horizontal gene transfer.
Whereas the chromosomes of prokaryotes are relatively gene-dense, those of eukaryotes often contain regions of DNA that serve no obvious function. Simple single-celled eukaryotes have relatively small amounts of such DNA, whereas the genomes of complex multicellular organisms, including humans, contain an absolute majority of DNA without an identified function. This DNA has often been referred to as "junk DNA". However, more recent analyses suggest that, although protein-coding DNA makes up barely 2% of the human genome, about 80% of the bases in the genome may be expressed, so the term "junk DNA" may be a misnomer.
The structure of a gene consists of many elements of which the actual protein coding sequence is often only a small part. These include DNA regions that are not transcribed as well as untranslated regions of the RNA.
Firstly, flanking the open reading frame, all genes contain a regulatory sequence that is required for their expression. In order to be expressed, genes require a promoter sequence. The promoter is recognized and bound by transcription factors and RNA polymerase to initiate transcription.:7.1 A gene can have more than one promoter, resulting in messenger RNAs (mRNA) that differ in how far they extend in the 5' end. Promoter regions have a consensus sequence, however highly transcribed genes have "strong" promoter sequences that bind the transcription machinery well, whereas others have "weak" promoters that bind poorly and initiate transcription less frequently.:7.2 Eukaryotic promoter regions are much more complex and difficult to identify than prokaryotic promoters.:7.3
Additionally, genes can have regulatory regions many kilobases upstream or downstream of the open reading frame. These act by binding to transcription factors which then cause the DNA to loop so that the regulatory sequence (and bound transcription factor) become close to the RNA polymerase binding site. For example, enhancers increase transcription by binding an activator protein which then helps to recruit the RNA polymerase to the promoter; conversely silencers bind repressor proteins and make the DNA less available for RNA polymerase.
The transcribed pre-mRNA contains untranslated regions at both ends which contain a ribosome binding site, terminator and start and stop codons. In addition, most eukaryotic open reading frames contain untranslated introns which are removed before the exons are translated. The sequences at the ends of the introns, dictate the splice sites to generate the final mature mRNA which encodes the protein or RNA product.
Many prokaryotic genes are organized into operons, with multiple protein-coding sequences that are transcribed as a unit. The products of operon genes typically have related functions and are involved in the same regulatory network.:7.3
Defining exactly what section of a DNA sequence comprises a gene is difficult. Regulatory regions of a gene such as enhancers do not necessarily have to be close to the coding sequence on the linear molecule because the intervening DNA can be looped out to bring the gene and its regulatory region into proximity. Similarly, a gene's introns can be much larger than its exons. Regulatory regions can even be on entirely different chromosomes and operate in trans to allow regulatory regions on one chromosome to come in contact with target genes on another chromosome.
Early work in molecular genetics suggested the model that one gene makes one protein. This model has been refined since the discovery of genes that can encode multiple proteins by alternative splicing and coding sequences split in short section across the genome whose mRNAs are concatenated by trans-splicing.
A broad operational definition is sometimes used to encompass the complexity of these diverse phenomena, where a gene is defined as a union of genomic sequences encoding a coherent set of potentially overlapping functional products. This definition categorizes genes by their functional products (proteins or RNA) rather than their specific DNA loci, with regulatory elements classified as gene-associated regions.
In all organisms, two steps are required to read the information encoded in a gene's DNA and produce the protein it specifies. First, the gene's DNA is transcribed to messenger RNA (mRNA).:6.1 Second, that mRNA is translated to protein.:6.2 RNA-coding genes must still go through the first step, but are not translated into protein. The process of producing a biologically functional molecule of either RNA or protein is called gene expression, and the resulting molecule is called a gene product.
The nucleotide sequence of a gene's DNA specifies the amino acid sequence of a protein through the genetic code. Sets of three nucleotides, known as codons, each correspond to a specific amino acid.:6 Additionally, a "start codon", and three "stop codons" indicate the beginning and end of the protein coding region. There are 64 possible codons (four possible nucleotides at each of three positions, hence 43 possible codons) and only 20 standard amino acids; hence the code is redundant and multiple codons can specify the same amino acid. The correspondence between codons and amino acids is nearly universal among all known living organisms.
Transcription produces a single-stranded RNA molecule known as messenger RNA, whose nucleotide sequence is complementary to the DNA from which it was transcribed.:6.1 The mRNA acts as an intermediate between the DNA gene and its final protein product. The gene's DNA is used as a template to generate a complementary mRNA. The mRNA matches the sequence of the gene's DNA coding strand because it is synthesised as the complement of the template strand. Transcription is performed by an enzyme called an RNA polymerase, which reads the template strand in the 3' to 5' direction and synthesizes the RNA from 5' to 3'. To initiate transcription, the polymerase first recognizes and binds a promoter region of the gene. Thus, a major mechanism of gene regulation is the blocking or sequestering the promoter region, either by tight binding by repressor molecules that physically block the polymerase, or by organizing the DNA so that the promoter region is not accessible.:7
In prokaryotes, transcription occurs in the cytoplasm; for very long transcripts, translation may begin at the 5' end of the RNA while the 3' end is still being transcribed. In eukaryotes, transcription occurs in the nucleus, where the cell's DNA is stored. The RNA molecule produced by the polymerase is known as the primary transcript and undergoes post-transcriptional modifications before being exported to the cytoplasm for translation. One of the modifications performed is the splicing of introns which are sequences in the transcribed region that do not encode protein. Alternative splicing mechanisms can result in mature transcripts from the same gene having different sequences and thus coding for different proteins. This is a major form of regulation in eukaryotic cells and also occurs in some prokaryotes.:7.5
Translation is the process by which a mature mRNA molecule is used as a template for synthesizing a new protein.:6.2 Translation is carried out by ribosomes, large complexes of RNA and protein responsible for carrying out the chemical reactions to add new amino acids to a growing polypeptide chain by the formation of peptide bonds. The genetic code is read three nucleotides at a time, in units called codons, via interactions with specialized RNA molecules called transfer RNA (tRNA). Each tRNA has three unpaired bases known as the anticodon that are complementary to the codon it reads on the mRNA. The tRNA is also covalently attached to the amino acid specified by the complementary codon. When the tRNA binds to its complementary codon in an mRNA strand, the ribosome attaches its amino acid cargo to the new polypeptide chain, which is synthesized from amino terminus to carboxyl terminus. During and after synthesis, most new proteins must folds to their active three-dimensional structure before they can carry out their cellular functions.:3
Genes are regulated so that they are expressed only when the product is needed, since expression draws on limited resources.:7 A cell regulates its gene expression depending on its external environment (e.g. available nutrients, temperature and other stresses), its internal environment (e.g. cell division cycle, metabolism, infection status), and its specific role if in a multicellular organism. Gene expression can be regulated at any step: from transcriptional initiation, to RNA processing, to post-translational modification of the protein. The regulation of lactose metabolism genes in E. coli (lac operon) was the first such mechanism to be described in 1961.
A typical protein-coding gene is first copied into RNA as an intermediate in the manufacture of the final protein product.:6.1 In other cases, the RNA molecules are the actual functional products, as in the synthesis of ribosomal RNA and transfer RNA. Some RNAs known as ribozymes are capable of enzymatic function, and microRNA has a regulatory role. The DNA sequences from which such RNAs are transcribed are known as non-coding RNA genes.
Some viruses store their entire genomes in the form of RNA, and contain no DNA at all. Because they use RNA to store genes, their cellular hosts may synthesize their proteins as soon as they are infected and without the delay in waiting for transcription. On the other hand, RNA retroviruses, such as HIV, require the reverse transcription of their genome from RNA into DNA before their proteins can be synthesized. RNA-mediated epigenetic inheritance has also been observed in plants and very rarely in animals.
Organisms inherit their genes from their parents. Asexual organisms simply inherit a complete copy of their parent's genome. Sexual organisms have two copies of each chromosome because they inherit one complete set from each parent.:1
According to Mendelian inheritance, variations in an organism's phenotype (observable physical and behavioral characteristics) are due in part to variations in its genotype (particular set of genes). Each gene specifies a particular trait with different sequence of a gene (alleles) giving rise to different phenotypes. Most eukaryotic organisms (such as the pea plants Mendel worked on) have two alleles for each trait, one inherited from each parent.:20
Alleles at a locus may be dominant or recessive; dominant alleles give rise to their corresponding phenotypes when paired with any other allele for the same trait, whereas recessive alleles give rise to their corresponding phenotype only when paired with another copy of the same allele. For example, if the allele specifying tall stems in pea plants is dominant over the allele specifying short stems, then pea plants that inherit one tall allele from one parent and one short allele from the other parent will also have tall stems. Mendel's work demonstrated that alleles assort independently in the production of gametes, or germ cells, ensuring variation in the next generation. Although Mendelian inheritance remains a good model for many traits determined by single genes (including a number of well-known genetic disorders) it does not include the physical processes of DNA replication and cell division.
The growth, development, and reproduction of organisms relies on cell division, or the process by which a single cell divides into two usually identical daughter cells. This requires first making a duplicate copy of every gene in the genome in a process called DNA replication.:5.2 The copies are made by specialized enzymes known as DNA polymerases, which "read" one strand of the double-helical DNA, known as the template strand, and synthesize a new complementary strand. Because the DNA double helix is held together by base pairing, the sequence of one strand completely specifies the sequence of its complement; hence only one strand needs to be read by the enzyme to produce a faithful copy. The process of DNA replication is semiconservative; that is, the copy of the genome inherited by each daughter cell contains one original and one newly synthesized strand of DNA.:5.2
After DNA replication is complete, the cell must physically separate the two copies of the genome and divide into two distinct membrane-bound cells.:18.2 In prokaryotes (bacteria and archaea) this usually occurs via a relatively simple process called binary fission, in which each circular genome attaches to the cell membrane and is separated into the daughter cells as the membrane invaginates to split the cytoplasm into two membrane-bound portions. Binary fission is extremely fast compared to the rates of cell division in eukaryotes. Eukaryotic cell division is a more complex process known as the cell cycle; DNA replication occurs during a phase of this cycle known as S phase, whereas the process of segregating chromosomes and splitting the cytoplasm occurs during M phase.:18.1
The duplication and transmission of genetic material from one generation of cells to the next is the basis for molecular inheritance, and the link between the classical and molecular pictures of genes. Organisms inherit the characteristics of their parents because the cells of the offspring contain copies of the genes in their parents' cells. In asexually reproducing organisms, the offspring will be a genetic copy or clone of the parent organism. In sexually reproducing organisms, a specialized form of cell division called meiosis produces cells called gametes or germ cells that are haploid, or contain only one copy of each gene.:20.2 The gametes produced by females are called eggs or ova, and those produced by males are called sperm. Two gametes fuse to form a diploid fertilized egg, a single cell that has two sets of genes, with one copy of each gene from the mother and one from the father.:20
During the process of meiotic cell division, an event called genetic recombination or crossing-over can sometimes occur, in which a length of DNA on one chromatid is swapped with a length of DNA on the corresponding sister chromatid. This has no effect if the alleles on the chromatids are the same, but results in reassortment of otherwise linked alleles if they are different.:5.5 The Mendelian principle of independent assortment asserts that each of a parent's two genes for each trait will sort independently into gametes; which allele an organism inherits for one trait is unrelated to which allele it inherits for another trait. This is in fact only true for genes that do not reside on the same chromosome, or are located very far from one another on the same chromosome. The closer two genes lie on the same chromosome, the more closely they will be associated in gametes and the more often they will appear together; genes that are very close are essentially never separated because it is extremely unlikely that a crossover point will occur between them. This is known as genetic linkage.
DNA replication is for the most part extremely accurate, however errors (mutations) do occur.:7.6 The error rate in eukaryotic cells can be as low as 10−8 per nucleotide per replication, whereas for some RNA viruses it can be as high as 10−3. This means that each generation, each human genome accumulates 1–2 new mutations. Small mutations can be caused by DNA replication and the aftermath of DNA damage and include point mutations in which a single base is altered and frameshift mutations in which a single base is inserted or deleted. Either of these mutations can change the gene by missense (change a codon to encode a different amino acid) or nonsense (a premature stop codon). Larger mutations can be caused by errors in recombination to cause chromosomal abnormalities including the duplication, deletion, rearrangement or inversion of large sections of a chromosome. Additionally, the DNA repair mechanisms that normally revert mutations can introduce errors when repairing the physical damage to the molecule is more important than restoring an exact copy, for example when repairing double-strand breaks.:5.4
When multiple different alleles for a gene are present in a species's population it is called polymorphic. Most different alleles are functionally equivalent, however some alleles can give rise to different phenotypic traits. A gene's most common allele is called the wild type, and rare alleles are called mutants. The genetic variation in relative frequencies of different alleles in a population is due to both natural selection and genetic drift. The wild-type allele is not necessarily the ancestor of less common alleles, nor is it necessarily fitter.
Most mutations within genes are neutral, having no effect on the organism's phenotype (silent mutations). Some mutations do not change the amino acid sequence because multiple codons encode the same amino acid (synonymous mutations). Other mutations can be neutral if they lead to amino acid sequence changes, but the protein still functions similarly with the new amino acid (e.g. conservative mutations). Many mutations, however, are deleterious or even lethal, and are removed from populations by natural selection. Genetic disorders are the result of deleterious mutations and can be due to spontaneous mutation in the affected individual, or can be inherited. Finally, a small fraction of mutations are beneficial, improving the organism's fitness and are extremely important for evolution, since their directional selection leads to adaptive evolution.:7.6
Genes with a most recent common ancestor, and thus a shared evolutionary ancestry, are known as homologs. These genes appear either from gene duplication within an organism's genome, where they are known as paralogous genes, or are the result of divergence of the genes after a speciation event, where they are known as orthologous genes,:7.6 and often perform the same or similar functions in related organisms. It is often assumed that the functions of orthologous genes are more similar than those of paralogous genes, although the difference is minimal.
The relationship between genes can be measured by comparing the sequence alignment of their DNA.:7.6 The degree of sequence similarity between homologous genes is called conserved sequence. Most changes to a gene's sequence do not affect its function and so genes accumulate mutations over time by neutral molecular evolution. Additionally, any selection on a gene will cause its sequence to diverge at a different rate. Genes under stabilizing selection are constrained and so change more slowly whereas genes under directional selection change sequence more rapidly. The sequence differences between genes can be used for phylogenetic analyses to study how those genes have evolved and how the organisms they come from are related.
The most common source of new genes in eukaryotic lineages is gene duplication, which creates copy number variation of an existing gene in the genome. The resulting genes (paralogs) may then diverge in sequence and in function. Sets of genes formed in this way comprise a gene family. Gene duplications and losses within a family are common and represent a major source of evolutionary biodiversity. Sometimes, gene duplication may result in a nonfunctional copy of a gene, or a functional copy may be subject to mutations that result in loss of function; such nonfunctional genes are called pseudogenes.:7.6
De novo or "orphan" genes, whose sequence shows no similarity to existing genes, are extremely rare. Estimates of the number of de novo genes in the human genome range from 18 to 60. Such genes are typically shorter and simpler in structure than most eukaryotic genes, with few if any introns. Two primary sources of orphan protein-coding genes are gene duplication followed by extremely rapid sequence change, such that the original relationship is undetectable by sequence comparisons, and formation through mutation of "cryptic" transcription start sites that introduce a new open reading frame in a region of the genome that did not previously code for a protein.
Horizontal gene transfer refers to the transfer of genetic material through a mechanism other than reproduction. This mechanism is a common source of new genes in prokaryotes, sometimes thought to contribute more to genetic variation than gene duplication. It is a common means of spreading antibiotic resistance, virulence, and adaptive metabolic functions. Although horizontal gene transfer is rare in eukaryotes, likely examples have been identified of protist and alga genomes containing genes of bacterial origin.
The genome size, and the number of genes it encodes varies widely between organisms. The smallest genomes occur in viruses (which can have as few as 2 protein-coding genes), and viroids (which act as a single non-coding RNA gene). Conversely, plants can have extremely large genomes, with rice containing >46,000 protein-coding genes. The total number of protein-coding genes (the Earth's proteome) is estimated to be 5 million sequences.
Although the number of base-pairs of DNA in the human genome has been known since the 1960s, the estimated number of genes has changed over time as definitions of genes, and methods of detecting them have been refined. Initial theoretical predictions of the number of human genes were as high as 2,000,000. Early experimental measures indicated there to be 50,000–100,000 transcribed genes (expressed sequence tags). Subsequently, the sequencing in the Human Genome Project indicated that many of these transcripts were alternative variants of the same genes, and the total number of protein-coding genes was revised down to ~20,000 with 13 genes encoded on the mitochondrial genome. Of the human genome, only 1–2% consists of protein-coding genes, with the remainder being 'noncoding' DNA such as introns, retrotransposons, and noncoding RNAs.
Essential genes are the set of genes thought to be critical for an organism's survival. This definition assumes the abundant availability of all relevant nutrients and the absence of environmental stress. Only a small portion of an organism's genes are essential. In bacteria, an estimated 250–400 genes are essential for Escherichia coli and Bacillus subtilis, which is less than 10% of their genes. Half of these genes are orthologs in both organisms and are largely involved in protein synthesis. In the budding yeast Saccharomyces cerevisiae the number of essential genes is slightly higher, at 1000 genes (~20% of their genes). Although the number is more difficult to measure in higher eukaryotes, mice and humans are estimated to have around 2000 essential genes (~10% of their genes).
Housekeeping genes are critical for carrying out basic cell functions and so are expressed at a relatively constant level (constitutively). Since their expression is constant, housekeeping genes are used as experimental controls when analysing gene expression. Not all essential genes are housekeeping genes since some essential genes are developmentally regulated or expressed at certain times during the organism's life cycle.
Gene nomenclature has been established by the HUGO Gene Nomenclature Committee (HGNC) for each known human gene in the form of an approved gene name and symbol (short-form abbreviation), which can be accessed through a database maintained by HGNC. Symbols are chosen to be unique, and each gene has only one symbol (although approved symbols sometimes change). Symbols are preferably kept consistent with other members of a gene family and with homologs in other species, particularly the mouse due to its role as a common model organism.
Genetic engineering is the modification of an organism's genome through biotechnology. Since the 1970s, a variety of techniques have been developed to specifically add, remove and edit genes in an organism. Recently developed genome engineering techniques use engineered nuclease enzymes to create targeted DNA repair in a chromosome to either disrupt or edit a gene when the break is repaired. The related term synthetic biology is sometimes used to refer to extensive genetic engineering of an organism.
Genetic engineering is now a routine research tool with model organisms. For example, genes are easily added to bacteria and lineages of knockout mice with a specific gene's function disrupted are used to investigate that gene's function. Many organisms have been genetically modified for applications in agriculture, industrial biotechnology, and medicine.
For multicellular organisms, typically the embryo is engineered which grows into the adult genetically modified organism. However, the genomes of cells in an adult organism can be edited using gene therapy techniques to treat genetic diseases.
Madrasa (Arabic: مدرسة‎, madrasah, pl. مدارس, madāris, Turkish: Medrese) is the Arabic word for any type of educational institution, whether secular or religious (of any religion). The word is variously transliterated madrasah, madarasaa, medresa, madrassa, madraza, medrese, etc. In the West, the word usually refers to a specific type of religious school or college for the study of the Islamic religion, though this may not be the only subject studied. Not all students in madaris are Muslims; there is also a modern curriculum.
The word madrasah derives from the triconsonantal Semitic root د-ر-س D-R-S 'to learn, study', through the wazn (form/stem) مفعل(ة)‎; mafʻal(ah), meaning "a place where something is done". Therefore, madrasah literally means "a place where learning and studying take place". The word is also present as a loanword with the same innocuous meaning in many Arabic-influenced languages, such as: Urdu, Bengali, Hindi, Persian, Turkish, Azeri, Kurdish, Indonesian, Malay and Bosnian / Croatian. In the Arabic language, the word مدرسة madrasah simply means the same as school does in the English language, whether that is private, public or parochial school, as well as for any primary or secondary school whether Muslim, non-Muslim, or secular. Unlike the use of the word school in British English, the word madrasah more closely resembles the term school in American English, in that it can refer to a university-level or post-graduate school as well as to a primary or secondary school. For example, in the Ottoman Empire during the Early Modern Period, madaris had lower schools and specialised schools where the students became known as danişmends. The usual Arabic word for a university, however, is جامعة (jāmiʻah). The Hebrew cognate midrasha also connotes the meaning of a place of learning; the related term midrash literally refers to study or learning, but has acquired mystical and religious connotations.
However, in English, the term madrasah usually refers to the specifically Islamic institutions. A typical Islamic school usually offers two courses of study: a ḥifẓ course teaching memorization of the Qur'an (the person who commits the entire Qurʼan to memory is called a ḥāfiẓ); and an ʻālim course leading the candidate to become an accepted scholar in the community. A regular curriculum includes courses in Arabic, tafsir (Qur'anic interpretation), sharīʻah (Islamic law), hadiths (recorded sayings and deeds of Muhammad), mantiq (logic), and Muslim history. In the Ottoman Empire, during the Early Modern Period, the study of hadiths was introduced by Süleyman I. Depending on the educational demands, some madaris also offer additional advanced courses in Arabic literature, English and other foreign languages, as well as science and world history. Ottoman madaris along with religious teachings also taught "styles of writing, grammary, syntax, poetry, composition, natural sciences, political sciences, and etiquette."
People of all ages attend, and many often move on to becoming imams.[citation needed] The certificate of an ʻālim, for example, requires approximately twelve years of study.[citation needed] A good number of the ḥuffāẓ (plural of ḥāfiẓ) are the product of the madaris. The madaris also resemble colleges, where people take evening classes and reside in dormitories. An important function of the madaris is to admit orphans and poor children in order to provide them with education and training. Madaris may enroll female students; however, they study separately from the men.[citation needed]
The term "Islamic education" means education in the light of Islam itself, which is rooted in the teachings of the Quran - holy book of Muslims. Islamic education and Muslim education are not the same. Because Islamic education has epistemological integration which is founded on Tawhid - Oneness or monotheism. For details Read "A Qur’anic Methodology for Integrating Knowledge and Education: Implications for Malaysia’s Islamic Education Strategy" written Tareq M Zayed  and "Knowledge of Shariah and Knowledge to Manage ‘Self’ and ‘System’: Integration of Islamic Epistemology with the Knowledge and Education" authored by Tareq M Zayed
The first institute of madrasa education was at the estate of Hazrat Zaid bin Arkam near a hill called Safa, where Hazrat Muhammad was the teacher and the students were some of his followers.[citation needed] After Hijrah (migration) the madrasa of "Suffa" was established in Madina on the east side of the Al-Masjid an-Nabawi mosque. Hazrat 'Ubada bin Samit was appointed there by Hazrat Muhammad as teacher and among the students.[citation needed] In the curriculum of the madrasa, there were teachings of The Qur'an,The Hadith, fara'iz, tajweed, genealogy, treatises of first aid, etc. There were also trainings of horse-riding, art of war, handwriting and calligraphy, athletics and martial arts. The first part of madrasa based education is estimated from the first day of "nabuwwat" to the first portion of the "Umaiya" caliphate.[citation needed]
During the rule of the Fatimid and Mamluk dynasties and their successor states in the medieval Middle East, many of the ruling elite founded madaris through a religious endowment known as the waqf. Not only was the madrasa a potent symbol of status but it was an effective means of transmitting wealth and status to their descendants. Especially during the Mamlūk period, when only former slaves could assume power, the sons of the ruling Mamlūk elite were unable to inherit. Guaranteed positions within the new madaris thus allowed them to maintain status. Madaris built in this period include the Mosque-Madrasah of Sultan Ḥasan in Cairo.
At the beginning of the Caliphate or Islamic Empire, the reliance on courts initially confined sponsorship and scholarly activities to major centres. Within several centuries, the development of Muslim educational institutions such as the madrasah and masjid eventually introduced such activities to provincial towns and dispersed them across the Islamic legal schools and Sufi orders. In addition to religious subjects, they also taught the "rational sciences," as varied as mathematics, astronomy, astrology, geography, alchemy, philosophy, magic, and occultism, depending on the curriculum of the specific institution in question. The madaris, however, were not centres of advanced scientific study; scientific advances in Islam were usually carried out by scholars working under the patronage of royal courts. During this time,[when?] the Caliphate experienced a growth in literacy, having the highest literacy rate of the Middle Ages, comparable to classical Athens' literacy in antiquity but on a much larger scale. The emergence of the maktab and madrasa institutions played a fundamental role in the relatively high literacy rates of the medieval Islamic world.
In the medieval Islamic world, an elementary school was known as a maktab, which dates back to at least the 10th century. Like madaris (which referred to higher education), a maktab was often attached to an endowed mosque. In the 11th century, the famous Persian Islamic philosopher and teacher Ibn Sīnā (known as Avicenna in the West), in one of his books, wrote a chapter about the maktab entitled "The Role of the Teacher in the Training and Upbringing of Children," as a guide to teachers working at maktab schools. He wrote that children can learn better if taught in classes instead of individual tuition from private tutors, and he gave a number of reasons for why this is the case, citing the value of competition and emulation among pupils, as well as the usefulness of group discussions and debates. Ibn Sīnā described the curriculum of a maktab school in some detail, describing the curricula for two stages of education in a maktab school.
Ibn Sīnā refers to the secondary education stage of maktab schooling as a period of specialisation when pupils should begin to acquire manual skills, regardless of their social status. He writes that children after the age of 14 should be allowed to choose and specialise in subjects they have an interest in, whether it was reading, manual skills, literature, preaching, medicine, geometry, trade and commerce, craftsmanship, or any other subject or profession they would be interested in pursuing for a future career. He wrote that this was a transitional stage and that there needs to be flexibility regarding the age in which pupils graduate, as the student's emotional development and chosen subjects need to be taken into account.
During its formative period, the term madrasah referred to a higher education institution, whose curriculum initially included only the "religious sciences", whilst philosophy and the secular sciences were often excluded. The curriculum slowly began to diversify, with many later madaris teaching both the religious and the "secular sciences", such as logic, mathematics and philosophy. Some madaris further extended their curriculum to history, politics, ethics, music, metaphysics, medicine, astronomy and chemistry. The curriculum of a madrasah was usually set by its founder, but most generally taught both the religious sciences and the physical sciences. Madaris were established throughout the Islamic world, examples being the 9th century University of al-Qarawiyyin, the 10th century al-Azhar University (the most famous), the 11th century Niẓāmīyah, as well as 75 madaris in Cairo, 51 in Damascus and up to 44 in Aleppo between 1155 and 1260. Many more were also established in the Andalusian cities of Córdoba, Seville, Toledo, Granada (Madrasah of Granada), Murcia, Almería, Valencia and Cádiz during the Caliphate of Córdoba.
Madaris were largely centred on the study of fiqh (Islamic jurisprudence). The ijāzat al-tadrīs wa-al-iftāʼ ("licence to teach and issue legal opinions") in the medieval Islamic legal education system had its origins in the 9th century after the formation of the madhāhib (schools of jurisprudence). George Makdisi considers the ijāzah to be the origin of the European doctorate. However, in an earlier article, he considered the ijāzah to be of "fundamental difference" to the medieval doctorate, since the former was awarded by an individual teacher-scholar not obliged to follow any formal criteria, whereas the latter was conferred on the student by the collective authority of the faculty. To obtain an ijāzah, a student "had to study in a guild school of law, usually four years for the basic undergraduate course" and ten or more years for a post-graduate course. The "doctorate was obtained after an oral examination to determine the originality of the candidate's theses", and to test the student's "ability to defend them against all objections, in disputations set up for the purpose." These were scholarly exercises practised throughout the student's "career as a graduate student of law." After students completed their post-graduate education, they were awarded ijazas giving them the status of faqīh 'scholar of jurisprudence', muftī 'scholar competent in issuing fatwās', and mudarris 'teacher'.
The Arabic term ijāzat al-tadrīs was awarded to Islamic scholars who were qualified to teach. According to Makdisi, the Latin title licentia docendi 'licence to teach' in the European university may have been a translation of the Arabic, but the underlying concept was very different. A significant difference between the ijāzat al-tadrīs and the licentia docendi was that the former was awarded by the individual scholar-teacher, while the latter was awarded by the chief official of the university, who represented the collective faculty, rather than the individual scholar-teacher.
Much of the study in the madrasah college centred on examining whether certain opinions of law were orthodox. This scholarly process of "determining orthodoxy began with a question which the Muslim layman, called in that capacity mustaftī, presented to a jurisconsult, called mufti, soliciting from him a response, called fatwa, a legal opinion (the religious law of Islam covers civil as well as religious matters). The mufti (professor of legal opinions) took this question, studied it, researched it intensively in the sacred scriptures, in order to find a solution to it. This process of scholarly research was called ijtihād, literally, the exertion of one's efforts to the utmost limit."
There is disagreement whether madaris ever became universities. Scholars like Arnold H. Green and Seyyed Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madaris indeed became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Darleen Pryds questions this view, pointing out that madaris and European universities in the Mediterranean region shared similar foundations by princely patrons and were intended to provide loyal administrators to further the rulers' agenda. Other scholars regard the university as uniquely European in origin and characteristics.
al-Qarawīyīn University in Fez, Morocco is recognised by many historians as the oldest degree-granting university in the world, having been founded in 859 by Fatima al-Fihri. While the madrasa college could also issue degrees at all levels, the jāmiʻahs (such as al-Qarawīyīn and al-Azhar University) differed in the sense that they were larger institutions, more universal in terms of their complete source of studies, had individual faculties for different subjects, and could house a number of mosques, madaris, and other institutions within them. Such an institution has thus been described as an "Islamic university".
Al-Azhar University, founded in Cairo, Egypt in 975 by the Ismaʻīlī Shīʻī Fatimid dynasty as a jāmiʻah, had individual faculties for a theological seminary, Islamic law and jurisprudence, Arabic grammar, Islamic astronomy, early Islamic philosophy and logic in Islamic philosophy. The postgraduate doctorate in law was only obtained after "an oral examination to determine the originality of the candidate's theses", and to test the student's "ability to defend them against all objections, in disputations set up for the purpose." ‘Abd al-Laṭīf al-Baghdādī also delivered lectures on Islamic medicine at al-Azhar, while Maimonides delivered lectures on medicine and astronomy there during the time of Saladin. Another early jāmiʻah was the Niẓāmīyah of Baghdād (founded 1091), which has been called the "largest university of the Medieval world." Mustansiriya University, established by the ʻAbbāsid caliph al-Mustanṣir in 1233, in addition to teaching the religious subjects, offered courses dealing with philosophy, mathematics and the natural sciences.
However, the classification of madaris as "universities" is disputed on the question of understanding of each institution on its own terms. In madaris, the ijāzahs were only issued in one field, the Islamic religious law of sharīʻah, and in no other field of learning. Other academic subjects, including the natural sciences, philosophy and literary studies, were only treated "ancillary" to the study of the Sharia. For example, a natural science like astronomy was only studied (if at all) to supply religious needs, like the time for prayer. This is why Ptolemaic astronomy was considered adequate, and is still taught in some modern day madaris. The Islamic law undergraduate degree from al-Azhar, the most prestigious madrasa, was traditionally granted without final examinations, but on the basis of the students' attentive attendance to courses. In contrast to the medieval doctorate which was granted by the collective authority of the faculty, the Islamic degree was not granted by the teacher to the pupil based on any formal criteria, but remained a "personal matter, the sole prerogative of the person bestowing it; no one could force him to give one".
Medievalist specialists who define the university as a legally autonomous corporation disagree with the term "university" for the Islamic madaris and jāmi‘ahs because the medieval university (from Latin universitas) was structurally different, being a legally autonomous corporation rather than a waqf institution like the madrasa and jāmiʻah. Despite the many similarities, medieval specialists have coined the term "Islamic college" for madrasa and jāmiʻah to differentiate them from the legally autonomous corporations that the medieval European universities were. In a sense, the madrasa resembles a university college in that it has most of the features of a university, but lacks the corporate element. Toby Huff summarises the difference as follows:
As Muslim institutions of higher learning, the madrasa had the legal designation of waqf. In central and eastern Islamic lands, the view that the madrasa, as a charitable endowment, will remain under the control of the donor (and their descendent), resulted in a "spurt" of establishment of madaris in the 11th and 12th centuries. However, in Western Islamic lands, where the Maliki views prohibited donors from controlling their endowment, madaris were not as popular. Unlike the corporate designation of Western institutions of higher learning, the waqf designation seemed to have led to the exclusion of non-orthodox religious subjects such a philosophy and natural science from the curricula. The madrasa of al-Qarawīyīn, one of the two surviving madaris that predate the founding of the earliest medieval universities and are thus claimed to be the "first universities" by some authors, has acquired official university status as late as 1947. The other, al-Azhar, did acquire this status in name and essence only in the course of numerous reforms during the 19th and 20th century, notably the one of 1961 which introduced non-religious subjects to its curriculum, such as economics, engineering, medicine, and agriculture. It should also be noted that many medieval universities were run for centuries as Christian cathedral schools or monastic schools prior to their formal establishment as universitas scholarium; evidence of these immediate forerunners of the university dates back to the 6th century AD, thus well preceding the earliest madaris. George Makdisi, who has published most extensively on the topic concludes in his comparison between the two institutions:
Nevertheless, Makdisi has asserted that the European university borrowed many of its features from the Islamic madrasa, including the concepts of a degree and doctorate. Makdisi and Hugh Goddard have also highlighted other terms and concepts now used in modern universities which most likely have Islamic origins, including "the fact that we still talk of professors holding the 'Chair' of their subject" being based on the "traditional Islamic pattern of teaching where the professor sits on a chair and the students sit around him", the term 'academic circles' being derived from the way in which Islamic students "sat in a circle around their professor", and terms such as "having 'fellows', 'reading' a subject, and obtaining 'degrees', can all be traced back" to the Islamic concepts of aṣḥāb ('companions, as of Muhammad'), qirāʼah ('reading aloud the Qur'an') and ijāzah ('licence [to teach]') respectively. Makdisi has listed eighteen such parallels in terminology which can be traced back to their roots in Islamic education. Some of the practices now common in modern universities which Makdisi and Goddard trace back to an Islamic root include "practices such as delivering inaugural lectures, wearing academic robes, obtaining doctorates by defending a thesis, and even the idea of academic freedom are also modelled on Islamic custom." The Islamic scholarly system of fatwá and ijmāʻ, meaning opinion and consensus respectively, formed the basis of the "scholarly system the West has practised in university scholarship from the Middle Ages down to the present day." According to Makdisi and Goddard, "the idea of academic freedom" in universities was also "modelled on Islamic custom" as practised in the medieval Madrasa system from the 9th century. Islamic influence was "certainly discernible in the foundation of the first deliberately planned university" in Europe, the University of Naples Federico II founded by Frederick II, Holy Roman Emperor in 1224.
However, all of these facets of medieval university life are considered by standard scholarship to be independent medieval European developments with no tracable Islamic influence. Generally, some reviewers have pointed out the strong inclination of Makdisi of overstating his case by simply resting on "the accumulation of close parallels", but all the while failing to point to convincing channels of transmission between the Muslim and Christian world. Norman Daniel points out that the Arab equivalent of the Latin disputation, the taliqa, was reserved for the ruler's court, not the madrasa, and that the actual differences between Islamic fiqh and medieval European civil law were profound. The taliqa only reached Islamic Spain, the only likely point of transmission, after the establishment of the first medieval universities. In fact, there is no Latin translation of the taliqa and, most importantly, no evidence of Latin scholars ever showing awareness of Arab influence on the Latin method of disputation, something they would have certainly found noteworthy. Rather, it was the medieval reception of the Greek Organon which set the scholastic sic et non in motion. Daniel concludes that resemblances in method had more to with the two religions having "common problems: to reconcile the conflicting statements of their own authorities, and to safeguard the data of revelation from the impact of Greek philosophy"; thus Christian scholasticism and similar Arab concepts should be viewed in terms of a parallel occurrence, not of the transmission of ideas from one to the other, a view shared by Hugh Kennedy.
Prior to the 12th century, women accounted for less than one percent of the world’s Islamic scholars. However, al-Sakhawi and Mohammad Akram Nadwi have since found evidence of over 8,000 female scholars since the 15th century. al-Sakhawi devotes an entire volume of his 12-volume biographical dictionary al-Ḍawʾ al-lāmiʻ to female scholars, giving information on 1,075 of them. More recently, the scholar Mohammad Akram Nadwi, currently a researcher from the Oxford Centre for Islamic Studies, has written 40 volumes on the muḥaddithāt (the women scholars of ḥadīth), and found at least 8,000 of them.
From around 750, during the Abbasid Caliphate, women “became renowned for their brains as well as their beauty”. In particular, many well known women of the time were trained from childhood in music, dancing and poetry. Mahbuba was one of these. Another feminine figure to be remembered for her achievements was Tawaddud, "a slave girl who was said to have been bought at great cost by Hārūn al-Rashīd because she had passed her examinations by the most eminent scholars in astronomy, medicine, law, philosophy, music, history, Arabic grammar, literature, theology and chess". Moreover, among the most prominent feminine figures was Shuhda who was known as "the Scholar" or "the Pride of Women" during the 12th century in Baghdad. Despite the recognition of women's aptitudes during the Abbasid dynasty, all these came to an end in Iraq with the sack of Baghdad in 1258.
According to the Sunni scholar Ibn ʻAsākir in the 12th century, there were opportunities for female education in the medieval Islamic world, writing that women could study, earn ijazahs (academic degrees), and qualify as scholars and teachers. This was especially the case for learned and scholarly families, who wanted to ensure the highest possible education for both their sons and daughters. Ibn ʻAsakir had himself studied under 80 different female teachers in his time. Female education in the Islamic world was inspired by Muhammad's wives, such as Khadijah, a successful businesswoman. According to a hadith attributed to Muhammad, he praised the women of Medina because of their desire for religious knowledge:
"The first Ottoman Medrese was created in İznik in 1331 and most Ottoman medreses followed the traditions of Sunni Islam." "When an Ottoman sultan established a new medrese, he would invite scholars from the Islamic world—for example, Murad II brought scholars from Persia, such as ʻAlāʼ al-Dīn and Fakhr al-Dīn who helped enhance the reputation of the Ottoman medrese". This reveals that the Islamic world was interconnected in the early modern period as they travelled around to other Islamic states exchanging knowledge. This sense that the Ottoman Empire was becoming modernised through globalization is also recognised by Hamadeh who says: "Change in the eighteenth century as the beginning of a long and unilinear march toward westernisation reflects the two centuries of reformation in sovereign identity." İnalcık also mentions that while scholars from for example Persia travelled to the Ottomans in order to share their knowledge, Ottomans travelled as well to receive education from scholars of these Islamic lands, such as Egypt, Persia and Turkestan. Hence, this reveals that similar to today's modern world, individuals from the early modern society travelled abroad to receive education and share knowledge and that the world was more interconnected than it seems. Also, it reveals how the system of "schooling" was also similar to today's modern world where students travel abroad to different countries for studies. Examples of Ottoman madaris are the ones built by Mehmed the Conqueror. He built eight madaris that were built "on either side of the mosque where there were eight higher madaris for specialised studies and eight lower medreses, which prepared students for these." The fact that they were built around, or near mosques reveals the religious impulses behind madrasa building and it reveals the interconnectedness between institutions of learning and religion. The students who completed their education in the lower medreses became known as danismends. This reveals that similar to the education system today, the Ottomans' educational system involved different kinds of schools attached to different kinds of levels. For example, there were lower madaris and specialised ones, and for one to get into the specialised area meant that he had to complete the classes in the lower one in order to adequately prepare himself for higher learning.
Although Ottoman madaris had a number of different branches of study, such as calligraphic sciences, oral sciences, and intellectual sciences, they primarily served the function of an Islamic centre for spiritual learning. "The goal of all knowledge and in particular, of the spiritual sciences is knowledge of God." Religion, for the most part, determines the significance and importance of each science. As İnalcık mentions: "Those which aid religion are good and sciences like astrology are bad." However, even though mathematics, or studies in logic were part of the madrasa's curriculum, they were all centred around religion. Even mathematics had a religious impulse behind its teachings. "The Ulema of the Ottoman medreses held the view that hostility to logic and mathematics was futile since these accustomed the mind to correct thinking and thus helped to reveal divine truths" – key word being "divine". İnalcık also mentions that even philosophy was only allowed to be studied so that it helped to confirm the doctrines of Islam." Hence, madaris – schools were basically religious centres for religious teachings and learning in the Ottoman world. Although scholars such as Goffman have argued that the Ottomans were highly tolerant and lived in a pluralistic society, it seems that schools that were the main centres for learning were in fact heftily religious and were not religiously pluralistic, but centred around Islam. Similarly, in Europe "Jewish children learned the Hebrew letters and texts of basic prayers at home, and then attended a school organised by the synagogue to study the Torah." Wiesner-Hanks also says that Protestants also wanted to teach "proper religious values." This shows that in the early modern period, Ottomans and Europeans were similar in their ideas about how schools should be managed and what they should be primarily focused on. Thus, Ottoman madaris were very similar to present day schools in the sense that they offered a wide range of studies; however, these studies, in their ultimate objective, aimed to further solidify and consolidate Islamic practices and theories.
As with any other country during the Early Modern Period, such as Italy and Spain in Europe, the Ottoman social life was interconnected with the medrese. Medreses were built in as part of a Mosque complex where many programmes, such as aid to the poor through soup kitchens, were held under the infrastructure of a mosque, which reveals the interconnectedness of religion and social life during this period. "The mosques to which medreses were attached, dominated the social life in Ottoman cities." Social life was not dominated by religion only in the Muslim world of the Ottoman Empire; it was also quite similar to the social life of Europe during this period. As Goffman says: "Just as mosques dominated social life for the Ottomans, churches and synagogues dominated life for the Christians and Jews as well." Hence, social life and the medrese were closely linked, since medreses taught many curricula, such as religion, which highly governed social life in terms of establishing orthodoxy. "They tried moving their developing state toward Islamic orthodoxy." Overall, the fact that mosques contained medreses comes to show the relevance of education to religion in the sense that education took place within the framework of religion and religion established social life by trying to create a common religious orthodoxy. Hence, medreses were simply part of the social life of society as students came to learn the fundamentals of their societal values and beliefs.
In India the majority of these schools follow the Hanafi school of thought. The religious establishment forms part of the mainly two large divisions within the country, namely the Deobandis, who dominate in numbers (of whom the Darul Uloom Deoband constitutes one of the biggest madaris) and the Barelvis, who also make up a sizeable portion (Sufi-oriented). Some notable establishments include: Al Jamiatul Ashrafia, Mubarakpur, Manzar Islam Bareilly, Jamia Nizamdina New Delhi, Jamia Nayeemia Muradabad which is one of the largest learning centres for the Barelvis. The HR[clarification needed] ministry of the government of India has recently[when?] declared that a Central Madrasa Board would be set up. This will enhance the education system of madaris in India. Though the madaris impart Quranic education mainly, efforts are on to include Mathematics, Computers and science in the curriculum. In July 2015, the state government of Maharashtra created a stir de-recognised madrasa education, receiving critisicm from several political parties with the NCP accusing the ruling BJP of creating Hindu-Muslim friction in the state, and Kamal Farooqui of the All India Muslim Personal Law Board saying it was "ill-designed" 
Today, the system of Arabic and Islamic education has grown and further integrated with Kerala government administration. In 2005, an estimated 6,000 Muslim Arabic teachers taught in Kerala government schools, with over 500,000 Muslim students. State-appointed committees, not private mosques or religious scholars outside the government, determine the curriculum and accreditation of new schools and colleges. Primary education in Arabic and Islamic studies is available to Kerala Muslims almost entirely in after-school madrasa programs - sharply unlike full-time madaris common in north India, which may replace formal schooling. Arabic colleges (over eleven of which exist within the state-run University of Calicut and the Kannur University) provide B.A. and Masters' level degrees. At all levels, instruction is co-educational, with many women instructors and professors. Islamic education boards are independently run by the following organizations, accredited by the Kerala state government: Samastha Kerala Islamic Education Board, Kerala Nadvathul Mujahideen, Jamaat-e-Islami Hind, and Jamiat Ulema-e-Hind.
In Southeast Asia, Muslim students have a choice of attending a secular government or an Islamic school. Madaris or Islamic schools are known as Sekolah Agama (Malay: religious school) in Malaysia and Indonesia, โรงเรียนศาสนาอิสลาม (Thai: school of Islam) in Thailand and madaris in the Philippines. In countries where Islam is not the majority or state religion, Islamic schools are found in regions such as southern Thailand (near the Thai-Malaysian border) and the southern Philippines in Mindanao, where a significant Muslim population can be found.
In Singapore, madrasahs are private schools which are overseen by Majlis Ugama Islam Singapura (MUIS, English: Islamic Religious Council of Singapore). There are six Madrasahs in Singapore, catering to students from Primary 1 to Secondary 4. Four Madrasahs are coeducational and two are for girls. Students take a range of Islamic Studies subjects in addition to mainstream MOE curriculum subjects and sit for the PSLE and GCE 'O' Levels like their peers. In 2009, MUIS introduced the "Joint Madrasah System" (JMS), a joint collaboration of Madrasah Al-Irsyad Al-Islamiah primary school and secondary schools Madrasah Aljunied Al-Islamiah (offering the ukhrawi, or religious stream) and Madrasah Al-Arabiah Al-Islamiah (offering the academic stream). The JMS aims to introduce the International Baccalaureate (IB) programme into the Madrasah Al-Arabiah Al-Islamiah by 2019. Students attending a madrasah are required to wear the traditional Malay attire, including the songkok for boys and tudong for girls, in contrast to mainstream government schools which ban religious headgear as Singapore is officially a secular state. For students who wish to attend a mainstream school, they may opt to take classes on weekends at the madrasah instead of enrolling full-time.
In 2004, madaris were mainstreamed in 16 Regions nationwide, primarily in Muslim-majority areas in Mindanao under the auspices of the Department of Education (DepEd). The DepEd adopted Department Order No. 51, which instituted Arabic-language and Islamic Values instruction for Muslim children in state schools, and authorised implementation of the Standard Madrasa Curriculum (SMC) in private-run madaris. While there are state-recognised Islamic schools, such as Ibn Siena Integrated School in the Islamic City of Marawi, Sarang Bangun LC in Zamboanga and SMIE in Jolo, their Islamic studies programmes initially varied in application and content.
The first Madressa established in North America, Al-Rashid Islamic Institute, was established in Cornwall, Ontario in 1983 and has graduates who are Hafiz (Quran) and Ulama. The seminary was established by Mazhar Alam under the direction of his teacher the leading Indian Tablighi scholar Muhammad Zakariya Kandhlawi and focuses on the traditional Hanafi school of thought and shuns Salafist / Wahabi teachings. Due to its proximity to the US border city of Messina the school has historically had a high ratio of US students. Their most prominent graduate Shaykh Muhammad Alshareef completed his Hifz in the early 1990s then went on to deviate from his traditional roots and form the Salafist organization the AlMaghrib Institute.
Western commentators post-9/11 often perceive madaris as places of radical revivalism with a connotation of anti-Americanism and radical extremism, frequently associated in the Western press with Wahhabi attitudes toward non-Muslims. In Arabic the word madrasa simply means "school" and does not imply a political or religious affiliation, radical or otherwise. Madaris have varied curricula, and are not all religious. Some madaris in India, for example, have a secularised identity. Although early madaris were founded primarily to gain "knowledge of God" they also taught subjects such as mathematics and poetry. For example, in the Ottoman Empire, "Madrasahs had seven categories of sciences that were taught, such as: styles of writing, oral sciences like the Arabic language, grammar, rhetoric, and history and intellectual sciences, such as logic." This is similar to the Western world, in which universities began as institutions of the Catholic church.
The flowering plants (angiosperms), also known as Angiospermae or Magnoliophyta, are the most diverse group of land plants, with about 350,000 species. Like gymnosperms, angiosperms are seed-producing plants; they are distinguished from gymnosperms by characteristics including flowers, endosperm within the seeds, and the production of fruits that contain the seeds. Etymologically, angiosperm means a plant that produces seeds within an enclosure, in other words, a fruiting plant. The term "angiosperm" comes from the Greek composite word (angeion-, "case" or "casing", and sperma, "seed") meaning "enclosed seeds", after the enclosed condition of the seeds.
Fossilized spores suggest that higher plants (embryophytes) have lived on land for at least 475 million years. Early land plants reproduced sexually with flagellated, swimming sperm, like the green algae from which they evolved. An adaptation to terrestrialization was the development of upright meiosporangia for dispersal by spores to new habitats. This feature is lacking in the descendants of their nearest algal relatives, the Charophycean green algae. A later terrestrial adaptation took place with retention of the delicate, avascular sexual stage, the gametophyte, within the tissues of the vascular sporophyte. This occurred by spore germination within sporangia rather than spore release, as in non-seed plants. A current example of how this might have happened can be seen in the precocious spore germination in Selaginella, the spike-moss. The result for the ancestors of angiosperms was enclosing them in a case, the seed. The first seed bearing plants, like the ginkgo, and conifers (such as pines and firs), did not produce flowers. The pollen grains (males) of Ginkgo and cycads produce a pair of flagellated, mobile sperm cells that "swim" down the developing pollen tube to the female and her eggs.
The apparently sudden appearance of nearly modern flowers in the fossil record initially posed such a problem for the theory of evolution that it was called an "abominable mystery" by Charles Darwin. However, the fossil record has considerably grown since the time of Darwin, and recently discovered angiosperm fossils such as Archaefructus, along with further discoveries of fossil gymnosperms, suggest how angiosperm characteristics may have been acquired in a series of steps. Several groups of extinct gymnosperms, in particular seed ferns, have been proposed as the ancestors of flowering plants, but there is no continuous fossil evidence showing exactly how flowers evolved. Some older fossils, such as the upper Triassic Sanmiguelia, have been suggested. Based on current evidence, some propose that the ancestors of the angiosperms diverged from an unknown group of gymnosperms in the Triassic period (245–202 million years ago). Fossil angiosperm-like pollen from the Middle Triassic (247.2–242.0 Ma) suggests an older date for their origin. A close relationship between angiosperms and gnetophytes, proposed on the basis of morphological evidence, has more recently been disputed on the basis of molecular evidence that suggest gnetophytes are instead more closely related to other gymnosperms.[citation needed]
The evolution of seed plants and later angiosperms appears to be the result of two distinct rounds of whole genome duplication events. These occurred at 319 million years ago and 192 million years ago. Another possible whole genome duplication event at 160 million years ago perhaps created the ancestral line that led to all modern flowering plants. That event was studied by sequencing the genome of an ancient flowering plant, Amborella trichopoda, and directly addresses Darwin's "abominable mystery."
The earliest known macrofossil confidently identified as an angiosperm, Archaefructus liaoningensis, is dated to about 125 million years BP (the Cretaceous period), whereas pollen considered to be of angiosperm origin takes the fossil record back to about 130 million years BP. However, one study has suggested that the early-middle Jurassic plant Schmeissneria, traditionally considered a type of ginkgo, may be the earliest known angiosperm, or at least a close relative. In addition, circumstantial chemical evidence has been found for the existence of angiosperms as early as 250 million years ago. Oleanane, a secondary metabolite produced by many flowering plants, has been found in Permian deposits of that age together with fossils of gigantopterids. Gigantopterids are a group of extinct seed plants that share many morphological traits with flowering plants, although they are not known to have been flowering plants themselves.
The great angiosperm radiation, when a great diversity of angiosperms appears in the fossil record, occurred in the mid-Cretaceous (approximately 100 million years ago). However, a study in 2007 estimated that the division of the five most recent (the genus Ceratophyllum, the family Chloranthaceae, the eudicots, the magnoliids, and the monocots) of the eight main groups occurred around 140 million years ago. By the late Cretaceous, angiosperms appear to have dominated environments formerly occupied by ferns and cycadophytes, but large canopy-forming trees replaced conifers as the dominant trees only close to the end of the Cretaceous 66 million years ago or even later, at the beginning of the Tertiary. The radiation of herbaceous angiosperms occurred much later. Yet, many fossil plants recognizable as belonging to modern families (including beech, oak, maple, and magnolia) had already appeared by the late Cretaceous.
Island genetics provides one proposed explanation for the sudden, fully developed appearance of flowering plants. Island genetics is believed to be a common source of speciation in general, especially when it comes to radical adaptations that seem to have required inferior transitional forms. Flowering plants may have evolved in an isolated setting like an island or island chain, where the plants bearing them were able to develop a highly specialized relationship with some specific animal (a wasp, for example). Such a relationship, with a hypothetical wasp carrying pollen from one plant to another much the way fig wasps do today, could result in the development of a high degree of specialization in both the plant(s) and their partners. Note that the wasp example is not incidental; bees, which, it is postulated, evolved specifically due to mutualistic plant relationships, are descended from wasps.
Animals are also involved in the distribution of seeds. Fruit, which is formed by the enlargement of flower parts, is frequently a seed-dispersal tool that attracts animals to eat or otherwise disturb it, incidentally scattering the seeds it contains (see frugivory). Although many such mutualistic relationships remain too fragile to survive competition and to spread widely, flowering proved to be an unusually effective means of reproduction, spreading (whatever its origin) to become the dominant form of land plant life.
Flower ontogeny uses a combination of genes normally responsible for forming new shoots. The most primitive flowers probably had a variable number of flower parts, often separate from (but in contact with) each other. The flowers tended to grow in a spiral pattern, to be bisexual (in plants, this means both male and female parts on the same flower), and to be dominated by the ovary (female part). As flowers evolved, some variations developed parts fused together, with a much more specific number and design, and with either specific sexes per flower or plant or at least "ovary-inferior".
Flower evolution continues to the present day; modern flowers have been so profoundly influenced by humans that some of them cannot be pollinated in nature. Many modern domesticated flower species were formerly simple weeds, which sprouted only when the ground was disturbed. Some of them tended to grow with human crops, perhaps already having symbiotic companion plant relationships with them, and the prettiest did not get plucked because of their beauty, developing a dependence upon and special adaptation to human affection.
The exact relationship between these eight groups is not yet clear, although there is agreement that the first three groups to diverge from the ancestral angiosperm were Amborellales, Nymphaeales, and Austrobaileyales. The term basal angiosperms refers to these three groups. Among the rest, the relationship between the three broadest of these groups (magnoliids, monocots, and eudicots) remains unclear. Some analyses make the magnoliids the first to diverge, others the monocots. Ceratophyllum seems to group with the eudicots rather than with the monocots.
The botanical term "Angiosperm", from the Ancient Greek αγγείον, angeíon (bottle, vessel) and σπέρμα, (seed), was coined in the form Angiospermae by Paul Hermann in 1690, as the name of one of his primary divisions of the plant kingdom. This included flowering plants possessing seeds enclosed in capsules, distinguished from his Gymnospermae, or flowering plants with achenial or schizo-carpic fruits, the whole fruit or each of its pieces being here regarded as a seed and naked. The term and its antonym were maintained by Carl Linnaeus with the same sense, but with restricted application, in the names of the orders of his class Didynamia. Its use with any approach to its modern scope became possible only after 1827, when Robert Brown established the existence of truly naked ovules in the Cycadeae and Coniferae, and applied to them the name Gymnosperms.[citation needed] From that time onward, as long as these Gymnosperms were, as was usual, reckoned as dicotyledonous flowering plants, the term Angiosperm was used antithetically by botanical writers, with varying scope, as a group-name for other dicotyledonous plants.
In most taxonomies, the flowering plants are treated as a coherent group. The most popular descriptive name has been Angiospermae (Angiosperms), with Anthophyta ("flowering plants") a second choice. These names are not linked to any rank. The Wettstein system and the Engler system use the name Angiospermae, at the assigned rank of subdivision. The Reveal system treated flowering plants as subdivision Magnoliophytina (Frohne & U. Jensen ex Reveal, Phytologia 79: 70 1996), but later split it to Magnoliopsida, Liliopsida, and Rosopsida. The Takhtajan system and Cronquist system treat this group at the rank of division, leading to the name Magnoliophyta (from the family name Magnoliaceae). The Dahlgren system and Thorne system (1992) treat this group at the rank of class, leading to the name Magnoliopsida. The APG system of 1998, and the later 2003 and 2009 revisions, treat the flowering plants as a clade called angiosperms without a formal botanical name. However, a formal classification was published alongside the 2009 revision in which the flowering plants form the Subclass Magnoliidae.
The internal classification of this group has undergone considerable revision. The Cronquist system, proposed by Arthur Cronquist in 1968 and published in its full form in 1981, is still widely used but is no longer believed to accurately reflect phylogeny. A consensus about how the flowering plants should be arranged has recently begun to emerge through the work of the Angiosperm Phylogeny Group (APG), which published an influential reclassification of the angiosperms in 1998. Updates incorporating more recent research were published as APG II in 2003 and as APG III in 2009.
Recent studies, as by the APG, show that the monocots form a monophyletic group (clade) but that the dicots do not (they are paraphyletic). Nevertheless, the majority of dicot species do form a monophyletic group, called the eudicots or tricolpates. Of the remaining dicot species, most belong to a third major clade known as the magnoliids, containing about 9,000 species. The rest include a paraphyletic grouping of primitive species known collectively as the basal angiosperms, plus the families Ceratophyllaceae and Chloranthaceae.
The number of species of flowering plants is estimated to be in the range of 250,000 to 400,000. This compares to around 12,000 species of moss or 11,000 species of pteridophytes, showing that the flowering plants are much more diverse. The number of families in APG (1998) was 462. In APG II (2003) it is not settled; at maximum it is 457, but within this number there are 55 optional segregates, so that the minimum number of families in this system is 402. In APG III (2009) there are 415 families.
In the dicotyledons, the bundles in the very young stem are arranged in an open ring, separating a central pith from an outer cortex. In each bundle, separating the xylem and phloem, is a layer of meristem or active formative tissue known as cambium. By the formation of a layer of cambium between the bundles (interfascicular cambium), a complete ring is formed, and a regular periodical increase in thickness results from the development of xylem on the inside and phloem on the outside. The soft phloem becomes crushed, but the hard wood persists and forms the bulk of the stem and branches of the woody perennial. Owing to differences in the character of the elements produced at the beginning and end of the season, the wood is marked out in transverse section into concentric rings, one for each season of growth, called annual rings.
The characteristic feature of angiosperms is the flower. Flowers show remarkable variation in form and elaboration, and provide the most trustworthy external characteristics for establishing relationships among angiosperm species. The function of the flower is to ensure fertilization of the ovule and development of fruit containing seeds. The floral apparatus may arise terminally on a shoot or from the axil of a leaf (where the petiole attaches to the stem). Occasionally, as in violets, a flower arises singly in the axil of an ordinary foliage-leaf. More typically, the flower-bearing portion of the plant is sharply distinguished from the foliage-bearing or vegetative portion, and forms a more or less elaborate branch-system called an inflorescence.
The flower may consist only of these parts, as in willow, where each flower comprises only a few stamens or two carpels. Usually, other structures are present and serve to protect the sporophylls and to form an envelope attractive to pollinators. The individual members of these surrounding structures are known as sepals and petals (or tepals in flowers such as Magnolia where sepals and petals are not distinguishable from each other). The outer series (calyx of sepals) is usually green and leaf-like, and functions to protect the rest of the flower, especially the bud. The inner series (corolla of petals) is, in general, white or brightly colored, and is more delicate in structure. It functions to attract insect or bird pollinators. Attraction is effected by color, scent, and nectar, which may be secreted in some part of the flower. The characteristics that attract pollinators account for the popularity of flowers and flowering plants among humans.
While the majority of flowers are perfect or hermaphrodite (having both pollen and ovule producing parts in the same flower structure), flowering plants have developed numerous morphological and physiological mechanisms to reduce or prevent self-fertilization. Heteromorphic flowers have short carpels and long stamens, or vice versa, so animal pollinators cannot easily transfer pollen to the pistil (receptive part of the carpel). Homomorphic flowers may employ a biochemical (physiological) mechanism called self-incompatibility to discriminate between self and non-self pollen grains. In other species, the male and female parts are morphologically separated, developing on different flowers.
Double fertilization refers to a process in which two sperm cells fertilize cells in the ovary. This process begins when a pollen grain adheres to the stigma of the pistil (female reproductive structure), germinates, and grows a long pollen tube. While this pollen tube is growing, a haploid generative cell travels down the tube behind the tube nucleus. The generative cell divides by mitosis to produce two haploid (n) sperm cells. As the pollen tube grows, it makes its way from the stigma, down the style and into the ovary. Here the pollen tube reaches the micropyle of the ovule and digests its way into one of the synergids, releasing its contents (which include the sperm cells). The synergid that the cells were released into degenerates and one sperm makes its way to fertilize the egg cell, producing a diploid (2n) zygote. The second sperm cell fuses with both central cell nuclei, producing a triploid (3n) cell. As the zygote develops into an embryo, the triploid cell develops into the endosperm, which serves as the embryo's food supply. The ovary now will develop into fruit and the ovule will develop into seed.
The character of the seed coat bears a definite relation to that of the fruit. They protect the embryo and aid in dissemination; they may also directly promote germination. Among plants with indehiscent fruits, in general, the fruit provides protection for the embryo and secures dissemination. In this case, the seed coat is only slightly developed. If the fruit is dehiscent and the seed is exposed, in general, the seed-coat is well developed, and must discharge the functions otherwise executed by the fruit.
Agriculture is almost entirely dependent on angiosperms, which provide virtually all plant-based food, and also provide a significant amount of livestock feed. Of all the families of plants, the Poaceae, or grass family (grains), is by far the most important, providing the bulk of all feedstocks (rice, corn — maize, wheat, barley, rye, oats, pearl millet, sugar cane, sorghum). The Fabaceae, or legume family, comes in second place. Also of high importance are the Solanaceae, or nightshade family (potatoes, tomatoes, and peppers, among others), the Cucurbitaceae, or gourd family (also including pumpkins and melons), the Brassicaceae, or mustard plant family (including rapeseed and the innumerable varieties of the cabbage species Brassica oleracea), and the Apiaceae, or parsley family. Many of our fruits come from the Rutaceae, or rue family (including oranges, lemons, grapefruits, etc.), and the Rosaceae, or rose family (including apples, pears, cherries, apricots, plums, etc.).
Popper is known for his rejection of the classical inductivist views on the scientific method, in favour of empirical falsification: A theory in the empirical sciences can never be proven, but it can be falsified, meaning that it can and should be scrutinized by decisive experiments. He used the black swan fallacy to discuss falsification. If the outcome of an experiment contradicts the theory, one should refrain from ad hoc manoeuvres that evade the contradiction merely by making it less falsifiable. Popper is also known for his opposition to the classical justificationist account of knowledge, which he replaced with critical rationalism, "the first non-justificational philosophy of criticism in the history of philosophy."
Karl Popper was born in Vienna (then in Austria-Hungary) in 1902, to upper middle-class parents. All of Karl Popper's grandparents were Jewish, but the Popper family converted to Lutheranism before Karl was born, and so he received Lutheran baptism. They understood this as part of their cultural assimilation, not as an expression of devout belief. Karl's father Simon Siegmund Carl Popper was a lawyer from Bohemia and a doctor of law at the Vienna University, and mother Jenny Schiff was of Silesian and Hungarian descent. After establishing themselves in Vienna, the Poppers made a rapid social climb in Viennese society: Simon Siegmund Carl became a partner in the law firm of Vienna's liberal Burgomaster Herr Grübl and, after Grübl's death in 1898, Simon took over the business. (Malachi Hacohen records that Herr Grübl's first name was Raimund, after which Karl received his middle name. Popper himself, in his autobiography, erroneously recalls that Herr Grübl's first name was Carl.) His father was a bibliophile who had 12,000–14,000 volumes in his personal library. Popper inherited both the library and the disposition from him.
Popper left school at the age of 16 and attended lectures in mathematics, physics, philosophy, psychology and the history of music as a guest student at the University of Vienna. In 1919, Popper became attracted by Marxism and subsequently joined the Association of Socialist School Students. He also became a member of the Social Democratic Workers' Party of Austria, which was at that time a party that fully adopted the Marxist ideology. After the street battle in the Hörlgasse on 15 June 1919, when police shot eight of his unarmed party comrades, he became disillusioned by what he saw to be the "pseudo-scientific" historical materialism of Marx, abandoned the ideology, and remained a supporter of social liberalism throughout his life.
He worked in street construction for a short amount of time, but was unable to cope with the heavy labour. Continuing to attend university as a guest student, he started an apprenticeship as cabinetmaker, which he completed as a journeyman. He was dreaming at that time of starting a daycare facility for children, for which he assumed the ability to make furniture might be useful. After that he did voluntary service in one of psychoanalyst Alfred Adler's clinics for children. In 1922, he did his matura by way of a second chance education and finally joined the University as an ordinary student. He completed his examination as an elementary teacher in 1924 and started working at an after-school care club for socially endangered children. In 1925, he went to the newly founded Pädagogisches Institut and continued studying philosophy and psychology. Around that time he started courting Josefine Anna Henninger, who later became his wife.
In 1928, he earned a doctorate in psychology, under the supervision of Karl Bühler. His dissertation was entitled "Die Methodenfrage der Denkpsychologie" (The question of method in cognitive psychology). In 1929, he obtained the authorisation to teach mathematics and physics in secondary school, which he started doing. He married his colleague Josefine Anna Henninger (1906–1985) in 1930. Fearing the rise of Nazism and the threat of the Anschluss, he started to use the evenings and the nights to write his first book Die beiden Grundprobleme der Erkenntnistheorie (The Two Fundamental Problems of the Theory of Knowledge). He needed to publish one to get some academic position in a country that was safe for people of Jewish descent. However, he ended up not publishing the two-volume work, but a condensed version of it with some new material, Logik der Forschung (The Logic of Scientific Discovery), in 1934. Here, he criticised psychologism, naturalism, inductionism, and logical positivism, and put forth his theory of potential falsifiability as the criterion demarcating science from non-science. In 1935 and 1936, he took unpaid leave to go to the United Kingdom for a study visit.
In 1937, Popper finally managed to get a position that allowed him to emigrate to New Zealand, where he became lecturer in philosophy at Canterbury University College of the University of New Zealand in Christchurch. It was here that he wrote his influential work The Open Society and its Enemies. In Dunedin he met the Professor of Physiology John Carew Eccles and formed a lifelong friendship with him. In 1946, after the Second World War, he moved to the United Kingdom to become reader in logic and scientific method at the London School of Economics. Three years later, in 1949, he was appointed professor of logic and scientific method at the University of London. Popper was president of the Aristotelian Society from 1958 to 1959. He retired from academic life in 1969, though he remained intellectually active for the rest of his life. In 1985, he returned to Austria so that his wife could have her relatives around her during the last months of her life; she died in November that year. After the Ludwig Boltzmann Gesellschaft failed to establish him as the director of a newly founded branch researching the philosophy of science, he went back again to the United Kingdom in 1986, settling in Kenley, Surrey.
Popper died of "complications of cancer, pneumonia and kidney failure" in Kenley at the age of 92 on 17 September 1994. He had been working continuously on his philosophy until two weeks before, when he suddenly fell terminally ill. After cremation, his ashes were taken to Vienna and buried at Lainzer cemetery adjacent to the ORF Centre, where his wife Josefine Anna Popper (called ‘Hennie’) had already been buried. Popper's estate is managed by his secretary and personal assistant Melitta Mew and her husband Raymond. Popper's manuscripts went to the Hoover Institution at Stanford University, partly during his lifetime and partly as supplementary material after his death. Klagenfurt University possesses Popper's library, including his precious bibliophilia, as well as hard copies of the original Hoover material and microfilms of the supplementary material. The remaining parts of the estate were mostly transferred to The Karl Popper Charitable Trust. In October 2008 Klagenfurt University acquired the copyrights from the estate.
Popper won many awards and honours in his field, including the Lippincott Award of the American Political Science Association, the Sonning Prize, the Otto Hahn Peace Medal of the United Nations Association of Germany in Berlin and fellowships in the Royal Society, British Academy, London School of Economics, King's College London, Darwin College, Cambridge, and Charles University, Prague. Austria awarded him the Grand Decoration of Honour in Gold for Services to the Republic of Austria in 1986, and the Federal Republic of Germany its Grand Cross with Star and Sash of the Order of Merit, and the peace class of the Order Pour le Mérite. He received the Humanist Laureate Award from the International Academy of Humanism. He was knighted by Queen Elizabeth II in 1965, and was elected a Fellow of the Royal Society in 1976. He was invested with the Insignia of a Companion of Honour in 1982.
Other awards and recognition for Popper included the City of Vienna Prize for the Humanities (1965), Karl Renner Prize (1978), Austrian Decoration for Science and Art (1980), Dr. Leopold Lucas Prize (1981), Ring of Honour of the City of Vienna (1983) and the Premio Internazionale of the Italian Federico Nietzsche Society (1988). In 1992, he was awarded the Kyoto Prize in Arts and Philosophy for "symbolising the open spirit of the 20th century" and for his "enormous influence on the formation of the modern intellectual climate".
Karl Popper's rejection of Marxism during his teenage years left a profound mark on his thought. He had at one point joined a socialist association, and for a few months in 1919 considered himself a communist. During this time he became familiar with the Marxist view of economics, class-war, and history. Although he quickly became disillusioned with the views expounded by Marxism, his flirtation with the ideology led him to distance himself from those who believed that spilling blood for the sake of a revolution was necessary. He came to realise that when it came to sacrificing human lives, one was to think and act with extreme prudence.
The failure of democratic parties to prevent fascism from taking over Austrian politics in the 1920s and 1930s traumatised Popper. He suffered from the direct consequences of this failure, since events after the Anschluss, the annexation of Austria by the German Reich in 1938, forced him into permanent exile. His most important works in the field of social science—The Poverty of Historicism (1944) and The Open Society and Its Enemies (1945)—were inspired by his reflection on the events of his time and represented, in a sense, a reaction to the prevalent totalitarian ideologies that then dominated Central European politics. His books defended democratic liberalism as a social and political philosophy. They also represented extensive critiques of the philosophical presuppositions underpinning all forms of totalitarianism.
Popper puzzled over the stark contrast between the non-scientific character of Freud and Adler's theories in the field of psychology and the revolution set off by Einstein's theory of relativity in physics in the early 20th century. Popper thought that Einstein's theory, as a theory properly grounded in scientific thought and method, was highly "risky", in the sense that it was possible to deduce consequences from it which were, in the light of the then-dominant Newtonian physics, highly improbable (e.g., that light is deflected towards solid bodies—confirmed by Eddington's experiments in 1919), and which would, if they turned out to be false, falsify the whole theory. In contrast, nothing could, even in principle, falsify psychoanalytic theories. He thus came to the conclusion that psychoanalytic theories had more in common with primitive myths than with genuine science.
This led Popper to conclude that what were regarded[by whom?] as the remarkable strengths of psychoanalytical theories were actually their weaknesses. Psychoanalytical theories were crafted in a way that made them able to refute any criticism and to give an explanation for every possible form of human behaviour. The nature of such theories made it impossible for any criticism or experiment - even in principle - to show them to be false. This realisation had an important consequence when Popper later tackled the problem of demarcation in the philosophy of science, as it led him to posit that the strength of a scientific theory lies in its both being susceptible to falsification, and not actually being falsified by criticism made of it. He considered that if a theory cannot, in principle, be falsified by criticism, it is not a scientific theory.
Popper coined the term "critical rationalism" to describe his philosophy. Concerning the method of science, the term indicates his rejection of classical empiricism, and the classical observationalist-inductivist account of science that had grown out of it. Popper argued strongly against the latter, holding that scientific theories are abstract in nature, and can be tested only indirectly, by reference to their implications. He also held that scientific theory, and human knowledge generally, is irreducibly conjectural or hypothetical, and is generated by the creative imagination to solve problems that have arisen in specific historico-cultural settings.
Logically, no number of positive outcomes at the level of experimental testing can confirm a scientific theory, but a single counterexample is logically decisive: it shows the theory, from which the implication is derived, to be false. To say that a given statement (e.g., the statement of a law of some scientific theory) -- [call it "T"] -- is "falsifiable" does not mean that "T" is false. Rather, it means that, if "T" is false, then (in principle), "T" could be shown to be false, by observation or by experiment. Popper's account of the logical asymmetry between verification and falsifiability lies at the heart of his philosophy of science. It also inspired him to take falsifiability as his criterion of demarcation between what is, and is not, genuinely scientific: a theory should be considered scientific if, and only if, it is falsifiable. This led him to attack the claims of both psychoanalysis and contemporary Marxism to scientific status, on the basis that their theories are not falsifiable.
In All Life is Problem Solving, Popper sought to explain the apparent progress of scientific knowledge – that is, how it is that our understanding of the universe seems to improve over time. This problem arises from his position that the truth content of our theories, even the best of them, cannot be verified by scientific testing, but can only be falsified. Again, in this context the word "falsified" does not refer to something being "fake"; rather, that something can be (i.e., is capable of being) shown to be false by observation or experiment. Some things simply do not lend themselves to being shown to be false, and therefore, are not falsifiable. If so, then how is it that the growth of science appears to result in a growth in knowledge? In Popper's view, the advance of scientific knowledge is an evolutionary process characterised by his formula:
In response to a given problem situation (), a number of competing conjectures, or tentative theories (), are systematically subjected to the most rigorous attempts at falsification possible. This process, error elimination (), performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more "fit"—in other words, more applicable to the problem situation at hand (). Consequently, just as a species' biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may, in Popper's view, reflect a certain type of progress: toward more and more interesting problems (). For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances toward greater and greater problems; in a process very much akin to the interplay between genetic variation and natural selection.
Among his contributions to philosophy is his claim to have solved the philosophical problem of induction. He states that while there is no way to prove that the sun will rise, it is possible to formulate the theory that every day the sun will rise; if it does not rise on some particular day, the theory will be falsified and will have to be replaced by a different one. Until that day, there is no need to reject the assumption that the theory is true. Nor is it rational according to Popper to make instead the more complex assumption that the sun will rise until a given day, but will stop doing so the day after, or similar statements with additional conditions.
Popper held that rationality is not restricted to the realm of empirical or scientific theories, but that it is merely a special case of the general method of criticism, the method of finding and eliminating contradictions in knowledge without ad-hoc-measures. According to this view, rational discussion about metaphysical ideas, about moral values and even about purposes is possible. Popper's student W.W. Bartley III tried to radicalise this idea and made the controversial claim that not only can criticism go beyond empirical knowledge, but that everything can be rationally criticised.
To Popper, who was an anti-justificationist, traditional philosophy is misled by the false principle of sufficient reason. He thinks that no assumption can ever be or needs ever to be justified, so a lack of justification is not a justification for doubt. Instead, theories should be tested and scrutinised. It is not the goal to bless theories with claims of certainty or justification, but to eliminate errors in them. He writes, "there are no such things as good positive reasons; nor do we need such things [...] But [philosophers] obviously cannot quite bring [themselves] to believe that this is my opinion, let alone that it is right" (The Philosophy of Karl Popper, p. 1043)
In The Open Society and Its Enemies and The Poverty of Historicism, Popper developed a critique of historicism and a defence of the "Open Society". Popper considered historicism to be the theory that history develops inexorably and necessarily according to knowable general laws towards a determinate end. He argued that this view is the principal theoretical presupposition underpinning most forms of authoritarianism and totalitarianism. He argued that historicism is founded upon mistaken assumptions regarding the nature of scientific law and prediction. Since the growth of human knowledge is a causal factor in the evolution of human history, and since "no society can predict, scientifically, its own future states of knowledge", it follows, he argued, that there can be no predictive science of human history. For Popper, metaphysical and historical indeterminism go hand in hand.
As early as 1934, Popper wrote of the search for truth as "one of the strongest motives for scientific discovery." Still, he describes in Objective Knowledge (1972) early concerns about the much-criticised notion of truth as correspondence. Then came the semantic theory of truth formulated by the logician Alfred Tarski and published in 1933. Popper writes of learning in 1935 of the consequences of Tarski's theory, to his intense joy. The theory met critical objections to truth as correspondence and thereby rehabilitated it. The theory also seemed, in Popper's eyes, to support metaphysical realism and the regulative idea of a search for truth.
According to this theory, the conditions for the truth of a sentence as well as the sentences themselves are part of a metalanguage. So, for example, the sentence "Snow is white" is true if and only if snow is white. Although many philosophers have interpreted, and continue to interpret, Tarski's theory as a deflationary theory, Popper refers to it as a theory in which "is true" is replaced with "corresponds to the facts". He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. He identifies Tarski's formulation of the truth conditions of sentences as the introduction of a "metalinguistic predicate" and distinguishes the following cases:
Upon this basis, along with that of the logical content of assertions (where logical content is inversely proportional to probability), Popper went on to develop his important notion of verisimilitude or "truthlikeness". The intuitive idea behind verisimilitude is that the assertions or hypotheses of scientific theories can be objectively measured with respect to the amount of truth and falsity that they imply. And, in this way, one theory can be evaluated as more or less true than another on a quantitative basis which, Popper emphasises forcefully, has nothing to do with "subjective probabilities" or other merely "epistemic" considerations.
Knowledge, for Popper, was objective, both in the sense that it is objectively true (or truthlike), and also in the sense that knowledge has an ontological status (i.e., knowledge as object) independent of the knowing subject (Objective Knowledge: An Evolutionary Approach, 1972). He proposed three worlds: World One, being the physical world, or physical states; World Two, being the world of mind, or mental states, ideas, and perceptions; and World Three, being the body of human knowledge expressed in its manifold forms, or the products of the second world made manifest in the materials of the first world (i.e., books, papers, paintings, symphonies, and all the products of the human mind). World Three, he argued, was the product of individual human beings in exactly the same sense that an animal path is the product of individual animals, and that, as such, has an existence and evolution independent of any individual knowing subjects. The influence of World Three, in his view, on the individual human mind (World Two) is at least as strong as the influence of World One. In other words, the knowledge held by a given individual mind owes at least as much to the total accumulated wealth of human knowledge, made manifest, as to the world of direct experience. As such, the growth of human knowledge could be said to be a function of the independent evolution of World Three. Many contemporary philosophers, such as Daniel Dennett, have not embraced Popper's Three World conjecture, due mostly, it seems, to its resemblance to mind-body dualism.
The creation–evolution controversy in the United States raises the issue of whether creationistic ideas may be legitimately called science and whether evolution itself may be legitimately called science. In the debate, both sides and even courts in their decisions have frequently invoked Popper's criterion of falsifiability (see Daubert standard). In this context, passages written by Popper are frequently quoted in which he speaks about such issues himself. For example, he famously stated "Darwinism is not a testable scientific theory, but a metaphysical research program—a possible framework for testable scientific theories." He continued:
Popper had his own sophisticated views on evolution that go much beyond what the frequently-quoted passages say. In effect, Popper agreed with some of the points of both creationists and naturalists, but also disagreed with both views on crucial aspects. Popper understood the universe as a creative entity that invents new things, including life, but without the necessity of something like a god, especially not one who is pulling strings from behind the curtain. He said that evolution must, as the creationists say, work in a goal-directed way but disagreed with their view that it must necessarily be the hand of god that imposes these goals onto the stage of life.
Instead, he formulated the spearhead model of evolution, a version of genetic pluralism. According to this model, living organisms themselves have goals, and act according to these goals, each guided by a central control. In its most sophisticated form, this is the brain of humans, but controls also exist in much less sophisticated ways for species of lower complexity, such as the amoeba. This control organ plays a special role in evolution—it is the "spearhead of evolution". The goals bring the purpose into the world. Mutations in the genes that determine the structure of the control may then cause drastic changes in behaviour, preferences and goals, without having an impact on the organism's phenotype. Popper postulates that such purely behavioural changes are less likely to be lethal for the organism compared to drastic changes of the phenotype.
Popper contrasts his views with the notion of the "hopeful monster" that has large phenotype mutations and calls it the "hopeful behavioural monster". After behaviour has changed radically, small but quick changes of the phenotype follow to make the organism fitter to its changed goals. This way it looks as if the phenotype were changing guided by some invisible hand, while it is merely natural selection working in combination with the new behaviour. For example, according to this hypothesis, the eating habits of the giraffe must have changed before its elongated neck evolved. Popper contrasted this view as "evolution from within" or "active Darwinism" (the organism actively trying to discover new ways of life and being on a quest for conquering new ecological niches), with the naturalistic "evolution from without" (which has the picture of a hostile environment only trying to kill the mostly passive organism, or perhaps segregate some of its groups).
About the creation-evolution controversy, Popper wrote that he considered it "a somewhat sensational clash between a brilliant scientific hypothesis concerning the history of the various species of animals and plants on earth, and an older metaphysical theory which, incidentally, happened to be part of an established religious belief" with a footnote to the effect that "[he] agree[s] with Professor C.E. Raven when, in his Science, Religion, and the Future, 1943, he calls this conflict "a storm in a Victorian tea-cup"; though the force of this remark is perhaps a little impaired by the attention he pays to the vapours still emerging from the cup—to the Great Systems of Evolutionist Philosophy, produced by Bergson, Whitehead, Smuts, and others."
In an interview that Popper gave in 1969 with the condition that it shall be kept secret until after his death, he summarised his position on God as follows: "I don't know whether God exists or not. ... Some forms of atheism are arrogant and ignorant and should be rejected, but agnosticism—to admit that we don't know and to search—is all right. ... When I look at what I call the gift of life, I feel a gratitude which is in tune with some religious ideas of God. However, the moment I even speak of it, I am embarrassed that I may do something wrong to God in talking about God." He objected to organised religion, saying "it tends to use the name of God in vain", noting the danger of fanaticism because of religious conflicts: "The whole thing goes back to myths which, though they may have a kernel of truth, are untrue. Why then should the Jewish myth be true and the Indian and Egyptian myths not be true?" In a letter unrelated to the interview, he stressed his tolerant attitude: "Although I am not for religion, I do think that we should show respect for anybody who believes honestly."
Popper played a vital role in establishing the philosophy of science as a vigorous, autonomous discipline within philosophy, through his own prolific and influential works, and also through his influence on his own contemporaries and students. Popper founded in 1946 the Department of Philosophy, Logic and Scientific Method at the London School of Economics and there lectured and influenced both Imre Lakatos and Paul Feyerabend, two of the foremost philosophers of science in the next generation of philosophy of science. (Lakatos significantly modified Popper's position,:1 and Feyerabend repudiated it entirely, but the work of both is deeply influenced by Popper and engaged with many of the problems that Popper set.)
While there is some dispute as to the matter of influence, Popper had a long-standing and close friendship with economist Friedrich Hayek, who was also brought to the London School of Economics from Vienna. Each found support and similarities in the other's work, citing each other often, though not without qualification. In a letter to Hayek in 1944, Popper stated, "I think I have learnt more from you than from any other living thinker, except perhaps Alfred Tarski." Popper dedicated his Conjectures and Refutations to Hayek. For his part, Hayek dedicated a collection of papers, Studies in Philosophy, Politics, and Economics, to Popper, and in 1982 said, "...ever since his Logik der Forschung first came out in 1934, I have been a complete adherent to his general theory of methodology."
He does not argue that any such conclusions are therefore true, or that this describes the actual methods of any particular scientist.[citation needed] Rather, it is recommended as an essential principle of methodology that, if enacted by a system or community, will lead to slow but steady progress of a sort (relative to how well the system or community enacts the method). It has been suggested that Popper's ideas are often mistaken for a hard logical account of truth because of the historical co-incidence of their appearing at the same time as logical positivism, the followers of which mistook his aims for their own.
The Quine-Duhem thesis argues that it's impossible to test a single hypothesis on its own, since each one comes as part of an environment of theories. Thus we can only say that the whole package of relevant theories has been collectively falsified, but cannot conclusively say which element of the package must be replaced. An example of this is given by the discovery of the planet Neptune: when the motion of Uranus was found not to match the predictions of Newton's laws, the theory "There are seven planets in the solar system" was rejected, and not Newton's laws themselves. Popper discussed this critique of naïve falsificationism in Chapters 3 and 4 of The Logic of Scientific Discovery. For Popper, theories are accepted or rejected via a sort of selection process. Theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value. Thus Newton's laws, with their wide general application, are to be preferred over the much more specific "the solar system has seven planets".[dubious – discuss]
Popper claimed to have recognised already in the 1934 version of his Logic of Discovery a fact later stressed by Kuhn, "that scientists necessarily develop their ideas within a definite theoretical framework", and to that extent to have anticipated Kuhn's central point about "normal science". (But Popper criticised what he saw as Kuhn's relativism.) Also, in his collection Conjectures and Refutations: The Growth of Scientific Knowledge (Harper & Row, 1963), Popper writes, "Science must begin with myths, and with the criticism of myths; neither with the collection of observations, nor with the invention of experiments, but with the critical discussion of myths, and of magical techniques and practices. The scientific tradition is distinguished from the pre-scientific tradition in having two layers. Like the latter, it passes on its theories; but it also passes on a critical attitude towards them. The theories are passed on, not as dogmas, but rather with the challenge to discuss them and improve upon them."
Another objection is that it is not always possible to demonstrate falsehood definitively, especially if one is using statistical criteria to evaluate a null hypothesis. More generally it is not always clear, if evidence contradicts a hypothesis, that this is a sign of flaws in the hypothesis rather than of flaws in the evidence. However, this is a misunderstanding of what Popper's philosophy of science sets out to do. Rather than offering a set of instructions that merely need to be followed diligently to achieve science, Popper makes it clear in The Logic of Scientific Discovery that his belief is that the resolution of conflicts between hypotheses and observations can only be a matter of the collective judgment of scientists, in each individual case.
In a book called Science Versus Crime, Houck writes that Popper's falsificationism can be questioned logically: it is not clear how Popper would deal with a statement like "for every metal, there is a temperature at which it will melt." The hypothesis cannot be falsified by any possible observation, for there will always be a higher temperature than tested at which the metal may in fact melt, yet it seems to be a valid scientific hypothesis. These examples were pointed out by Carl Gustav Hempel. Hempel came to acknowledge that Logical Positivism's verificationism was untenable, but argued that falsificationism was equally untenable on logical grounds alone. The simplest response to this is that, because Popper describes how theories attain, maintain and lose scientific status, individual consequences of currently accepted scientific theories are scientific in the sense of being part of tentative scientific knowledge, and both of Hempel's examples fall under this category. For instance, atomic theory implies that all metals melt at some temperature.
In 2004, philosopher and psychologist Michel ter Hark (Groningen, The Netherlands) published a book, called Popper, Otto Selz and the rise of evolutionary epistemology, in which he claimed that Popper took some of his ideas from his tutor, the German psychologist Otto Selz. Selz never published his ideas, partly because of the rise of Nazism, which forced him to quit his work in 1933, and the prohibition of referring to Selz' work. Popper, the historian of ideas and his scholarship, is criticised in some academic quarters for his rejection of Plato, Hegel and Marx.
According to John N. Gray, Popper held that "a theory is scientific only in so far as it is falsifiable, and should be given up as soon as it is falsified." By applying Popper's account of scientific method, Gray's Straw Dogs states that this would have "killed the theories of Darwin and Einstein at birth." When they were first advanced, Gray claims, each of them was "at odds with some available evidence; only later did evidence become available that gave them crucial support." Against this, Gray seeks to establish the irrationalist thesis that "the progress of science comes from acting against reason."
Gray does not, however, give any indication of what available evidence these theories were at odds with, and his appeal to "crucial support" illustrates the very inductivist approach to science that Popper sought to show was logically illegitimate. For, according to Popper, Einstein's theory was at least equally as well corroborated as Newton's upon its initial conception; they both equally well accounted for all the hitherto available evidence. Moreover, since Einstein also explained the empirical refutations of Newton's theory, general relativity was immediately deemed suitable for tentative acceptance on the Popperian account. Indeed, Popper wrote, several decades before Gray's criticism, in reply to a critical essay by Imre Lakatos:
Such a theory would be true with higher probability, because it cannot be attacked so easily: to falsify the first one, it is sufficient to find that the sun has stopped rising; to falsify the second one, one additionally needs the assumption that the given day has not yet been reached. Popper held that it is the least likely, or most easily falsifiable, or simplest theory (attributes which he identified as all the same thing) that explains known facts that one should rationally prefer. His opposition to positivism, which held that it is the theory most likely to be true that one should prefer, here becomes very apparent. It is impossible, Popper argues, to ensure a theory to be true; it is more important that its falsity can be detected as easily as possible.
In his early years Popper was impressed by Marxism, whether of Communists or socialists. An event that happened in 1919 had a profound effect on him: During a riot, caused by the Communists, the police shot several unarmed people, including some of Popper's friends, when they tried to free party comrades from prison. The riot had, in fact, been part of a plan by which leaders of the Communist party with connections to Béla Kun tried to take power by a coup; Popper did not know about this at that time. However, he knew that the riot instigators were swayed by the Marxist doctrine that class struggle would produce vastly more dead men than the inevitable revolution brought about as quickly as possible, and so had no scruples to put the life of the rioters at risk to achieve their selfish goal of becoming the future leaders of the working class. This was the start of his later criticism of historicism. Popper began to reject Marxist historicism, which he associated with questionable means, and later socialism, which he associated with placing equality before freedom (to the possible disadvantage of equality).
The word "animal" comes from the Latin animalis, meaning having breath, having soul or living being. In everyday non-scientific usage the word excludes humans – that is, "animal" is often used to refer only to non-human members of the kingdom Animalia; often, only closer relatives of humans such as mammals, or mammals and other vertebrates, are meant. The biological definition of the word refers to all members of the kingdom Animalia, encompassing creatures as diverse as sponges, jellyfish, insects, and humans.
All animals have eukaryotic cells, surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules. During development, it forms a relatively flexible framework upon which cells can move about and be reorganized, making complex structures possible. In contrast, other multicellular organisms, like plants and fungi, have cells held in place by cell walls, and so develop by progressive growth. Also, unique to animal cells are the following intercellular junctions: tight junctions, gap junctions, and desmosomes.
Predation is a biological interaction where a predator (a heterotroph that is hunting) feeds on its prey (the organism that is attacked). Predators may or may not kill their prey prior to feeding on them, but the act of predation almost always results in the death of the prey. The other main category of consumption is detritivory, the consumption of dead organic matter. It can at times be difficult to separate the two feeding behaviours, for example, where parasitic species prey on a host organism and then lay their eggs on it for their offspring to feed on its decaying corpse. Selective pressures imposed on one another has led to an evolutionary arms race between prey and predator, resulting in various antipredator adaptations.
Among the other phyla, the Ctenophora and the Cnidaria, which includes sea anemones, corals, and jellyfish, are radially symmetric and have digestive chambers with a single opening, which serves as both the mouth and the anus. Both have distinct tissues, but they are not organized into organs. There are only two main germ layers, the ectoderm and endoderm, with only scattered cells between them. As such, these animals are sometimes called diploblastic. The tiny placozoans are similar, but they do not have a permanent digestive chamber.
Animals have several characteristics that set them apart from other living things. Animals are eukaryotic and multicellular, which separates them from bacteria and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking rigid cell walls. All animals are motile, if only at certain life stages. In most animals, embryos pass through a blastula stage, which is a characteristic exclusive to animals.
A zygote initially develops into a hollow sphere, called a blastula, which undergoes rearrangement and differentiation. In sponges, blastula larvae swim to a new location and develop into a new sponge. In most other groups, the blastula undergoes more complicated rearrangement. It first invaginates to form a gastrula with a digestive chamber, and two separate germ layers — an external ectoderm and an internal endoderm. In most cases, a mesoderm also develops between them. These germ layers then differentiate to form tissues and organs.
Some paleontologists suggest that animals appeared much earlier than the Cambrian explosion, possibly as early as 1 billion years ago. Trace fossils such as tracks and burrows found in the Tonian period indicate the presence of triploblastic worms, like metazoans, roughly as large (about 5 mm wide) and complex as earthworms. During the beginning of the Tonian period around 1 billion years ago, there was a decrease in Stromatolite diversity, which may indicate the appearance of grazing animals, since stromatolite diversity increased when grazing animals went extinct at the End Permian and End Ordovician extinction events, and decreased shortly after the grazer populations recovered. However the discovery that tracks very similar to these early trace fossils are produced today by the giant single-celled protist Gromia sphaerica casts doubt on their interpretation as evidence of early animal evolution.
Animals are generally considered to have evolved from a flagellated eukaryote. Their closest known living relatives are the choanoflagellates, collared flagellates that have a morphology similar to the choanocytes of certain sponges. Molecular studies place animals in a supergroup called the opisthokonts, which also include the choanoflagellates, fungi and a few small parasitic protists. The name comes from the posterior location of the flagellum in motile cells, such as most animal spermatozoa, whereas other eukaryotes tend to have anterior flagella.
The remaining animals form a monophyletic group called the Bilateria. For the most part, they are bilaterally symmetric, and often have a specialized head with feeding and sensory organs. The body is triploblastic, i.e. all three germ layers are well-developed, and tissues form distinct organs. The digestive chamber has two openings, a mouth and an anus, and there is also an internal body cavity called a coelom or pseudocoelom. There are exceptions to each of these characteristics, however — for instance adult echinoderms are radially symmetric, and certain parasitic worms have extremely simplified body structures.
Traditional morphological and modern molecular phylogenetic analysis have both recognized a major evolutionary transition from "non-bilaterian" animals, which are those lacking a bilaterally symmetric body plan (Porifera, Ctenophora, Cnidaria and Placozoa), to "bilaterian" animals (Bilateria) whose body plans display bilateral symmetry. The latter are further classified based on a major division between Deuterostomes and Protostomes. The relationships among non-bilaterian animals are disputed, but all bilaterian animals are thought to form a monophyletic group. Current understanding of the relationships among the major groups of animals is summarized by the following cladogram:
The Ecdysozoa are protostomes, named after the common trait of growth by moulting or ecdysis. The largest animal phylum belongs here, the Arthropoda, including insects, spiders, crabs, and their kin. All these organisms have a body divided into repeating segments, typically with paired appendages. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The ecdysozoans also include the Nematoda or roundworms, perhaps the second largest animal phylum. Roundworms are typically microscopic, and occur in nearly every environment where there is water. A number are important parasites. Smaller phyla related to them are the Nematomorpha or horsehair worms, and the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom.
Because of the great diversity found in animals, it is more economical for scientists to study a small number of chosen species so that connections can be drawn from their work and conclusions extrapolated about how animals function in general. Because they are easy to keep and breed, the fruit fly Drosophila melanogaster and the nematode Caenorhabditis elegans have long been the most intensively studied metazoan model organisms, and were among the first life-forms to be genetically sequenced. This was facilitated by the severely reduced state of their genomes, but as many genes, introns, and linkages lost, these ecdysozoans can teach us little about the origins of animals in general. The extent of this type of evolution within the superphylum will be revealed by the crustacean, annelid, and molluscan genome projects currently in progress. Analysis of the starlet sea anemone genome has emphasised the importance of sponges, placozoans, and choanoflagellates, also being sequenced, in explaining the arrival of 1500 ancestral genes unique to the Eumetazoa.
The Lophotrochozoa, evolved within Protostomia, include two of the most successful animal phyla, the Mollusca and Annelida. The former, which is the second-largest animal phylum by number of described species, includes animals such as snails, clams, and squids, and the latter comprises the segmented worms, such as earthworms and leeches. These two groups have long been considered close relatives because of the common presence of trochophore larvae, but the annelids were considered closer to the arthropods because they are both segmented. Now, this is generally considered convergent evolution, owing to many morphological and genetic differences between the two phyla. The Lophotrochozoa also include the Nemertea or ribbon worms, the Sipuncula, and several phyla that have a ring of ciliated tentacles around the mouth, called a lophophore. These were traditionally grouped together as the lophophorates. but it now appears that the lophophorate group may be paraphyletic, with some closer to the nemerteans and some to the molluscs and annelids. They include the Brachiopoda or lamp shells, which are prominent in the fossil record, the Entoprocta, the Phoronida, and possibly the Bryozoa or moss animals.
Several animal phyla are recognized for their lack of bilateral symmetry, and are thought to have diverged from other animals early in evolution. Among these, the sponges (Porifera) were long thought to have diverged first, representing the oldest animal phylum. They lack the complex organization found in most other phyla. Their cells are differentiated, but in most cases not organized into distinct tissues. Sponges typically feed by drawing in water through pores. However, a series of phylogenomic studies from 2008-2015 have found support for Ctenophora, or comb jellies, as the basal lineage of animals. This result has been controversial, since it would imply that that sponges may not be so primitive, but may instead be secondarily simplified. Other researchers have argued that the placement of Ctenophora as the earliest-diverging animal phylum is a statistical anomaly caused by the high rate of evolution in ctenophore genomes.
Deuterostomes differ from protostomes in several ways. Animals from both groups possess a complete digestive tract. However, in protostomes, the first opening of the gut to appear in embryological development (the archenteron) develops into the mouth, with the anus forming secondarily. In deuterostomes the anus forms first, with the mouth developing secondarily. In most protostomes, cells simply fill in the interior of the gastrula to form the mesoderm, called schizocoelous development, but in deuterostomes, it forms through invagination of the endoderm, called enterocoelic pouching. Deuterostome embryos undergo radial cleavage during cell division, while protostomes undergo spiral cleavage.
The Platyzoa include the phylum Platyhelminthes, the flatworms. These were originally considered some of the most primitive Bilateria, but it now appears they developed from more complex ancestors. A number of parasites are included in this group, such as the flukes and tapeworms. Flatworms are acoelomates, lacking a body cavity, as are their closest relatives, the microscopic Gastrotricha. The other platyzoan phyla are mostly microscopic and pseudocoelomate. The most prominent are the Rotifera or rotifers, which are common in aqueous environments. They also include the Acanthocephala or spiny-headed worms, the Gnathostomulida, Micrognathozoa, and possibly the Cycliophora. These groups share the presence of complex jaws, from which they are called the Gnathifera.
Most animals indirectly use the energy of sunlight by eating plants or plant-eating animals. Most plants use light to convert inorganic molecules in their environment into carbohydrates, fats, proteins and other biomolecules, characteristically containing reduced carbon in the form of carbon-hydrogen bonds. Starting with carbon dioxide (CO2) and water (H2O), photosynthesis converts the energy of sunlight into chemical energy in the form of simple sugars (e.g., glucose), with the release of molecular oxygen. These sugars are then used as the building blocks for plant growth, including the production of other biomolecules. When an animal eats plants (or eats other animals which have eaten plants), the reduced carbon compounds in the food become a source of energy and building materials for the animal. They are either used directly to help the animal grow, or broken down, releasing stored solar energy, and giving the animal the energy required for motion.
In philosophy, idealism is the group of philosophies which assert that reality, or reality as we can know it, is fundamentally mental, mentally constructed, or otherwise immaterial. Epistemologically, idealism manifests as a skepticism about the possibility of knowing any mind-independent thing. In a sociological sense, idealism emphasizes how human ideas—especially beliefs and values—shape society. As an ontological doctrine, idealism goes further, asserting that all entities are composed of mind or spirit. Idealism thus rejects physicalist and dualist theories that fail to ascribe priority to the mind.
The earliest extant arguments that the world of experience is grounded in the mental derive from India and Greece. The Hindu idealists in India and the Greek Neoplatonists gave panentheistic arguments for an all-pervading consciousness as the ground or true nature of reality. In contrast, the Yogācāra school, which arose within Mahayana Buddhism in India in the 4th century CE, based its "mind-only" idealism to a greater extent on phenomenological analyses of personal experience. This turn toward the subjective anticipated empiricists such as George Berkeley, who revived idealism in 18th-century Europe by employing skeptical arguments against materialism.
Beginning with Immanuel Kant, German idealists such as G. W. F. Hegel, Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Arthur Schopenhauer dominated 19th-century philosophy. This tradition, which emphasized the mental or "ideal" character of all phenomena, gave birth to idealistic and subjectivist schools ranging from British idealism to phenomenalism to existentialism. The historical influence of this branch of idealism remains central even to the schools that rejected its metaphysical assumptions, such as Marxism, pragmatism and positivism.
Idealism is a term with several related meanings. It comes via idea from the Greek idein (ἰδεῖν), meaning "to see". The term entered the English language by 1743. In ordinary use, as when speaking of Woodrow Wilson's political idealism, it generally suggests the priority of ideals, principles, values, and goals over concrete realities. Idealists are understood to represent the world as it might or should be, unlike pragmatists, who focus on the world as it presently is. In the arts, similarly, idealism affirms imagination and attempts to realize a mental conception of beauty, a standard of perfection, juxtaposed to aesthetic naturalism and realism.
Any philosophy that assigns crucial importance to the ideal or spiritual realm in its account of human existence may be termed "idealist". Metaphysical idealism is an ontological doctrine that holds that reality itself is incorporeal or experiential at its core. Beyond this, idealists disagree on which aspects of the mental are more basic. Platonic idealism affirms that abstractions are more basic to reality than the things we perceive, while subjective idealists and phenomenalists tend to privilege sensory experience over abstract reasoning. Epistemological idealism is the view that reality can only be known through ideas, that only psychological experience can be apprehended by the mind.
Subjective idealists like George Berkeley are anti-realists in terms of a mind-independent world, whereas transcendental idealists like Immanuel Kant are strong skeptics of such a world, affirming epistemological and not metaphysical idealism. Thus Kant defines idealism as "the assertion that we can never be certain whether all of our putative outer experience is not mere imagining". He claimed that, according to idealism, "the reality of external objects does not admit of strict proof. On the contrary, however, the reality of the object of our internal sense (of myself and state) is clear immediately through consciousness."  However, not all idealists restrict the real or the knowable to our immediate subjective experience. Objective idealists make claims about a transempirical world, but simply deny that this world is essentially divorced from or ontologically prior to the mental. Thus Plato and Gottfried Leibniz affirm an objective and knowable reality transcending our subjective awareness—a rejection of epistemological idealism—but propose that this reality is grounded in ideal entities, a form of metaphysical idealism. Nor do all metaphysical idealists agree on the nature of the ideal; for Plato, the fundamental entities were non-mental abstract forms, while for Leibniz they were proto-mental and concrete monads.
Christian theologians have held idealist views, often based on Neoplatonism, despite the influence of Aristotelian scholasticism from the 12th century onward. Later western theistic idealism such as that of Hermann Lotze offers a theory of the "world ground" in which all things find their unity: it has been widely accepted by Protestant theologians. Several modern religious movements, for example the organizations within the New Thought Movement and the Unity Church, may be said to have a particularly idealist orientation. The theology of Christian Science includes a form of idealism: it teaches that all that truly exists is God and God's ideas; that the world as it appears to the senses is a distortion of the underlying spiritual reality, a distortion that may be corrected (both conceptually and in terms of human experience) through a reorientation (spiritualization) of thought.
Plato's theory of forms or "ideas" describes ideal forms (for example the platonic solids in geometry or abstracts like Goodness and Justice), as universals existing independently of any particular instance. Arne Grøn calls this doctrine "the classic example of a metaphysical idealism as a transcendent idealism", while Simone Klein calls Plato "the earliest representative of metaphysical objective idealism". Nevertheless, Plato holds that matter is real, though transitory and imperfect, and is perceived by our body and its senses and given existence by the eternal ideas that are perceived directly by our rational soul. Plato was therefore a metaphysical and epistemological dualist, an outlook that modern idealism has striven to avoid: Plato's thought cannot therefore be counted as idealist in the modern sense, although quantum physics' assertion that man's consciousness is an immutable and primary requisite for not merely perceiving but shaping matter, and thus his reality, would give more credence to Plato's dualist position.[citation needed]
With the neoplatonist Plotinus, wrote Nathaniel Alfred Boll; "there even appears, probably for the first time in Western philosophy, idealism that had long been current in the East even at that time, for it taught... that the soul has made the world by stepping from eternity into time...". Similarly, in regard to passages from the Enneads, "The only space or place of the world is the soul" and "Time must not be assumed to exist outside the soul", Ludwig Noiré wrote: "For the first time in Western philosophy we find idealism proper in Plotinus, However, Plotinus does not address whether we know external objects, unlike Schopenhauer and other modern philosophers.
Subjective Idealism (immaterialism or phenomenalism) describes a relationship between experience and the world in which objects are no more than collections or "bundles" of sense data in the perceiver. Proponents include Berkeley, Bishop of Cloyne, an Anglo-Irish philosopher who advanced a theory he called immaterialism, later referred to as "subjective idealism", contending that individuals can only know sensations and ideas of objects directly, not abstractions such as "matter", and that ideas also depend upon being perceived for their very existence - esse est percipi; "to be is to be perceived".
Arthur Collier published similar assertions though there seems to have been no influence between the two contemporary writers. The only knowable reality is the represented image of an external object. Matter as a cause of that image, is unthinkable and therefore nothing to us. An external world as absolute matter unrelated to an observer does not exist as far as we are concerned. The universe cannot exist as it appears if there is no perceiving mind. Collier was influenced by An Essay Towards the Theory of the Ideal or Intelligible World by "Cambridge Platonist" John Norris (1701).
and proliferation of hyphenated entities such as "thing-in-itself" (Immanuel Kant), "things-as-interacted-by-us" (Arthur Fine), "table-of-commonsense" and "table-of-physics" (Sir Arthur Eddington) which are "warning signs" for conceptual idealism according to Musgrave because they allegedly do not exist but only highlight the numerous ways in which people come to know the world. This argument does not take into account the issues pertaining to hermeneutics, especially at the backdrop of analytic philosophy. Musgrave criticized Richard Rorty and Postmodernist philosophy in general for confusion of use and mention.
A. A. Luce and John Foster are other subjectivists. Luce, in Sense without Matter (1954), attempts to bring Berkeley up to date by modernizing his vocabulary and putting the issues he faced in modern terms, and treats the Biblical account of matter and the psychology of perception and nature. Foster's The Case for Idealism argues that the physical world is the logical creation of natural, non-logical constraints on human sense-experience. Foster's latest defense of his views is in his book A World for Us: The Case for Phenomenalistic Idealism.
The 2nd edition (1787) contained a Refutation of Idealism to distinguish his transcendental idealism from Descartes's Sceptical Idealism and Berkeley's anti-realist strain of Subjective Idealism. The section Paralogisms of Pure Reason is an implicit critique of Descartes' idealism. Kant says that it is not possible to infer the 'I' as an object (Descartes' cogito ergo sum) purely from "the spontaneity of thought". Kant focused on ideas drawn from British philosophers such as Locke, Berkeley and Hume but distinguished his transcendental or critical idealism from previous varieties;
In the first volume of his Parerga and Paralipomena, Schopenhauer wrote his "Sketch of a History of the Doctrine of the Ideal and the Real". He defined the ideal as being mental pictures that constitute subjective knowledge. The ideal, for him, is what can be attributed to our own minds. The images in our head are what comprise the ideal. Schopenhauer emphasized that we are restricted to our own consciousness. The world that appears is only a representation or mental picture of objects. We directly and immediately know only representations. All objects that are external to the mind are known indirectly through the mediation of our mind. He offered a history of the concept of the "ideal" as "ideational" or "existing in the mind as an image".
Friedrich Nietzsche argued that Kant commits an agnostic tautology and does not offer a satisfactory answer as to the source of a philosophical right to such-or-other metaphysical claims; he ridicules his pride in tackling "the most difficult thing that could ever be undertaken on behalf of metaphysics." The famous "thing-in-itself" was called a product of philosophical habit, which seeks to introduce a grammatical subject: because wherever there is cognition, there must be a thing that is cognized and allegedly it must be added to ontology as a being (whereas, to Nietzsche, only the world as ever changing appearances can be assumed). Yet he attacks the idealism of Schopenhauer and Descartes with an argument similar to Kant's critique of the latter (see above).
Absolute idealism is G. W. F. Hegel's account of how existence is comprehensible as an all-inclusive whole. Hegel called his philosophy "absolute" idealism in contrast to the "subjective idealism" of Berkeley and the "transcendental idealism" of Kant and Fichte, which were not based on a critique of the finite and a dialectical philosophy of history as Hegel's idealism was. The exercise of reason and intellect enables the philosopher to know ultimate historical reality, the phenomenological constitution of self-determination, the dialectical development of self-awareness and personality in the realm of History.
In his Science of Logic (1812–1814) Hegel argues that finite qualities are not fully "real" because they depend on other finite qualities to determine them. Qualitative infinity, on the other hand, would be more self-determining and hence more fully real. Similarly finite natural things are less "real"—because they are less self-determining—than spiritual things like morally responsible people, ethical communities and God. So any doctrine, such as materialism, that asserts that finite qualities or natural objects are fully real is mistaken.
Hegel certainly intends to preserve what he takes to be true of German idealism, in particular Kant's insistence that ethical reason can and does go beyond finite inclinations. For Hegel there must be some identity of thought and being for the "subject" (any human observer)) to be able to know any observed "object" (any external entity, possibly even another human) at all. Under Hegel's concept of "subject-object identity," subject and object both have Spirit (Hegel's ersatz, redefined, nonsupernatural "God") as their conceptual (not metaphysical) inner reality—and in that sense are identical. But until Spirit's "self-realization" occurs and Spirit graduates from Spirit to Absolute Spirit status, subject (a human mind) mistakenly thinks every "object" it observes is something "alien," meaning something separate or apart from "subject." In Hegel's words, "The object is revealed to it [to "subject"] by [as] something alien, and it does not recognize itself." Self-realization occurs when Hegel (part of Spirit's nonsupernatural Mind, which is the collective mind of all humans) arrives on the scene and realizes that every "object" is himself, because both subject and object are essentially Spirit. When self-realization occurs and Spirit becomes Absolute Spirit, the "finite" (man, human) becomes the "infinite" ("God," divine), replacing the imaginary or "picture-thinking" supernatural God of theism: man becomes God. Tucker puts it this way: "Hegelianism . . . is a religion of self-worship whose fundamental theme is given in Hegel's image of the man who aspires to be God himself, who demands 'something more, namely infinity.'" The picture Hegel presents is "a picture of a self-glorifying humanity striving compulsively, and at the end successfully, to rise to divinity."
Kierkegaard criticised Hegel's idealist philosophy in several of his works, particularly his claim to a comprehensive system that could explain the whole of reality. Where Hegel argues that an ultimate understanding of the logical structure of the world is an understanding of the logical structure of God's mind, Kierkegaard asserting that for God reality can be a system but it cannot be so for any human individual because both reality and humans are incomplete and all philosophical systems imply completeness. A logical system is possible but an existential system is not. "What is rational is actual; and what is actual is rational". Hegel's absolute idealism blurs the distinction between existence and thought: our mortal nature places limits on our understanding of reality;
A major concern of Hegel's Phenomenology of Spirit (1807) and of the philosophy of Spirit that he lays out in his Encyclopedia of the Philosophical Sciences (1817–1830) is the interrelation between individual humans, which he conceives in terms of "mutual recognition." However, what Climacus means by the aforementioned statement, is that Hegel, in the Philosophy of Right, believed the best solution was to surrender one's individuality to the customs of the State, identifying right and wrong in view of the prevailing bourgeois morality. Individual human will ought, at the State's highest level of development, to properly coincide with the will of the State. Climacus rejects Hegel's suppression of individuality by pointing out it is impossible to create a valid set of rules or system in any society which can adequately describe existence for any one individual. Submitting one's will to the State denies personal freedom, choice, and responsibility.
In addition, Hegel does believe we can know the structure of God's mind, or ultimate reality. Hegel agrees with Kierkegaard that both reality and humans are incomplete, inasmuch as we are in time, and reality develops through time. But the relation between time and eternity is outside time and this is the "logical structure" that Hegel thinks we can know. Kierkegaard disputes this assertion, because it eliminates the clear distinction between ontology and epistemology. Existence and thought are not identical and one cannot possibly think existence. Thought is always a form of abstraction, and thus not only is pure existence impossible to think, but all forms in existence are unthinkable; thought depends on language, which merely abstracts from experience, thus separating us from lived experience and the living essence of all beings. In addition, because we are finite beings, we cannot possibly know or understand anything that is universal or infinite such as God, so we cannot know God exists, since that which transcends time simultaneously transcends human understanding.
Bradley was the apparent target of G. E. Moore's radical rejection of idealism. Moore claimed that Bradley did not understand the statement that something is real. We know for certain, through common sense and prephilosophical beliefs, that some things are real, whether they are objects of thought or not, according to Moore. The 1903 article The Refutation of Idealism is one of the first demonstrations of Moore's commitment to analysis. He examines each of the three terms in the Berkeleian aphorism esse est percipi, "to be is to be perceived", finding that it must mean that the object and the subject are necessarily connected so that "yellow" and "the sensation of yellow" are identical - "to be yellow" is "to be experienced as yellow". But it also seems there is a difference between "yellow" and "the sensation of yellow" and "that esse is held to be percipi, solely because what is experienced is held to be identical with the experience of it". Though far from a complete refutation, this was the first strong statement by analytic philosophy against its idealist predecessors, or at any rate against the type of idealism represented by Berkeley. This argument did not show that the GEM (in post–Stove vernacular, see below) is logically invalid.
Pluralistic idealism such as that of Gottfried Leibniz takes the view that there are many individual minds that together underlie the existence of the observed world and make possible the existence of the physical universe. Unlike absolute idealism, pluralistic idealism does not assume the existence of a single ultimate mental reality or "Absolute". Leibniz' form of idealism, known as Panpsychism, views "monads" as the true atoms of the universe and as entities having perception. The monads are "substantial forms of being",elemental, individual, subject to their own laws, non-interacting, each reflecting the entire universe. Monads are centers of force, which is substance while space, matter and motion are phenomenal and their form and existence is dependent on the simple and immaterial monads. There is a pre-established harmony established by God, the central monad, between the world in the minds of the monads and the external world of objects. Leibniz's cosmology embraced traditional Christian Theism. The English psychologist and philosopher James Ward inspired by Leibniz had also defended a form of pluralistic idealism. According to Ward the universe is composed of "psychic monads" of different levels, interacting for mutual self- betterment.
Howison's personal idealism  was also called "California Personalism" by others to distinguish it from the "Boston Personalism" which was of Bowne. Howison maintained that both impersonal, monistic idealism and materialism run contrary to the experience of moral freedom. To deny freedom to pursue truth, beauty, and "benignant love" is to undermine every profound human venture, including science, morality, and philosophy. Personalistic idealists Borden Parker Bowne and Edgar S. Brightman and realistic personal theist Saint Thomas Aquinas address a core issue, namely that of dependence upon an infinite personal God.
J. M. E. McTaggart of Cambridge University, argued that minds alone exist and only relate to each other through love. Space, time and material objects are unreal. In The Unreality of Time he argued that time is an illusion because it is impossible to produce a coherent account of a sequence of events. The Nature of Existence (1927) contained his arguments that space, time, and matter cannot possibly be real. In his Studies in Hegelian Cosmology (Cambridge, 1901, p196) he declared that metaphysics are not relevant to social and political action. McTaggart "thought that Hegel was wrong in supposing that metaphysics could show that the state is more than a means to the good of the individuals who compose it". For McTaggart "philosophy can give us very little, if any, guidance in action... Why should a Hegelian citizen be surprised that his belief as to the organic nature of the Absolute does not help him in deciding how to vote? Would a Hegelian engineer be reasonable in expecting that his belief that all matter is spirit should help him in planning a bridge?
Thomas Davidson taught a philosophy called "apeirotheism", a "form of pluralistic idealism...coupled with a stern ethical rigorism" which he defined as "a theory of Gods infinite in number." The theory was indebted to Aristotle's pluralism and his concepts of Soul, the rational, living aspect of a living substance which cannot exist apart from the body because it is not a substance but an essence, and nous, rational thought, reflection and understanding. Although a perennial source of controversy, Aristotle arguably views the latter as both eternal and immaterial in nature, as exemplified in his theology of unmoved movers. Identifying Aristotle's God with rational thought, Davidson argued, contrary to Aristotle, that just as the soul cannot exist apart from the body, God cannot exist apart from the world.
Idealist notions took a strong hold among physicists of the early 20th century confronted with the paradoxes of quantum physics and the theory of relativity. In The Grammar of Science, Preface to the 2nd Edition, 1900, Karl Pearson wrote, "There are many signs that a sound idealism is surely replacing, as a basis for natural philosophy, the crude materialism of the older physicists." This book influenced Einstein's regard for the importance of the observer in scientific measurements[citation needed]. In § 5 of that book, Pearson asserted that "...science is in reality a classification and analysis of the contents of the mind...." Also, "...the field of science is much more consciousness than an external world."
"The mind-stuff of the world is, of course, something more general than our individual conscious minds.... The mind-stuff is not spread in space and time; these are part of the cyclic scheme ultimately derived out of it.... It is necessary to keep reminding ourselves that all knowledge of our environment from which the world of physics is constructed, has entered in the form of messages transmitted along the nerves to the seat of consciousness.... Consciousness is not sharply defined, but fades into subconsciousness; and beyond that we must postulate something indefinite but yet continuous with our mental nature.... It is difficult for the matter-of-fact physicist to accept the view that the substratum of everything is of mental character. But no one can deny that mind is the first and most direct thing in our experience, and all else is remote inference."
In a career spanning more than four decades, Spielberg's films have covered many themes and genres. Spielberg's early science-fiction and adventure films were seen as archetypes of modern Hollywood blockbuster filmmaking. In later years, his films began addressing humanistic issues such as the Holocaust (in Schindler's List), the transatlantic slave trade (in Amistad), war (in Empire of the Sun, Saving Private Ryan, War Horse and Bridge of Spies) and terrorism (in Munich). His other films include Close Encounters of the Third Kind, the Indiana Jones film series, and A.I. Artificial Intelligence.
Spielberg was born in Cincinnati, Ohio, to an Orthodox Jewish family. His mother, Leah (Adler) Posner (born 1920), was a restaurateur and concert pianist, and his father, Arnold Spielberg (born 1917), was an electrical engineer involved in the development of computers. His paternal grandparents were immigrants from Ukraine who settled in Cincinnati in the first decade of the 1900s. In 1950, his family moved to Haddon Township, New Jersey when his father took a job with RCA. Three years later, the family moved to Phoenix, Arizona.:548 Spielberg attended Hebrew school from 1953 to 1957, in classes taught by Rabbi Albert L. Lewis.
Spielberg won the Academy Award for Best Director for Schindler's List (1993) and Saving Private Ryan (1998). Three of Spielberg's films—Jaws (1975), E.T. the Extra-Terrestrial (1982), and Jurassic Park (1993)—achieved box office records, originated and came to epitomize the blockbuster film. The unadjusted gross of all Spielberg-directed films exceeds $9 billion worldwide, making him the highest-grossing director in history. His personal net worth is estimated to be more than $3 billion. He has been associated with composer John Williams since 1974, who composed music for all save five of Spielberg's feature films.
As a child, Spielberg faced difficulty reconciling being an Orthodox Jew with the perception of him by other children he played with. "It isn't something I enjoy admitting," he once said, "but when I was seven, eight, nine years old, God forgive me, I was embarrassed because we were Orthodox Jews. I was embarrassed by the outward perception of my parents' Jewish practices. I was never really ashamed to be Jewish, but I was uneasy at times." Spielberg also said he suffered from acts of anti-Semitic prejudice and bullying: "In high school, I got smacked and kicked around. Two bloody noses. It was horrible."
In 1958, he became a Boy Scout and fulfilled a requirement for the photography merit badge by making a nine-minute 8 mm film entitled The Last Gunfight. Years later, Spielberg recalled to a magazine interviewer, "My dad's still-camera was broken, so I asked the scoutmaster if I could tell a story with my father's movie camera. He said yes, and I got an idea to do a Western. I made it and got my merit badge. That was how it all started." At age thirteen, while living in Phoenix, Spielberg won a prize for a 40-minute war film he titled Escape to Nowhere, using a cast composed of other high school friends. That motivated him to make 15 more amateur 8mm films.:548 In 1963, at age sixteen, Spielberg wrote and directed his first independent film, a 140-minute science fiction adventure called Firelight, which would later inspire Close Encounters. The film was made for $500, most of which came from his father, and was shown in a local cinema for one evening, which earned back its cost.
While still a student, he was offered a small unpaid intern job at Universal Studios with the editing department. He was later given the opportunity to make a short film for theatrical release, the 26-minute, 35mm, Amblin', which he wrote and directed. Studio vice president Sidney Sheinberg was impressed by the film, which had won a number of awards, and offered Spielberg a seven-year directing contract. It made him the youngest director ever to be signed for a long-term deal with a major Hollywood studio.:548 He subsequently dropped out of college to begin professionally directing TV productions with Universal.
His first professional TV job came when he was hired to direct one of the segments for the 1969 pilot episode of Night Gallery. The segment, "Eyes," starred Joan Crawford; she and Spielberg were reportedly close friends until her death. The episode is unusual in his body of work, in that the camerawork is more highly stylized than his later, more "mature" films. After this, and an episode of Marcus Welby, M.D., Spielberg got his first feature-length assignment: an episode of The Name of the Game called "L.A. 2017". This futuristic science fiction episode impressed Universal Studios and they signed him to a short contract. He did another segment on Night Gallery and did some work for shows such as Owen Marshall: Counselor at Law and The Psychiatrist, before landing the first series episode of Columbo (previous episodes were actually TV films).
Based on the strength of his work, Universal signed Spielberg to do four TV films. The first was a Richard Matheson adaptation called Duel. The film is about a psychotic Peterbilt 281 tanker truck driver who chases the terrified driver (Dennis Weaver) of a small Plymouth Valiant and tries to run him off the road. Special praise of this film by the influential British critic Dilys Powell was highly significant to Spielberg's career. Another TV film (Something Evil) was made and released to capitalize on the popularity of The Exorcist, then a major best-selling book which had not yet been released as a film. He fulfilled his contract by directing the TV film-length pilot of a show called Savage, starring Martin Landau. Spielberg's debut full-length feature film was The Sugarland Express, about a married couple who are chased by police as the couple tries to regain custody of their baby. Spielberg's cinematography for the police chase was praised by reviewers, and The Hollywood Reporter stated that "a major new director is on the horizon.":223 However, the film fared poorly at the box office and received a limited release.
Studio producers Richard D. Zanuck and David Brown offered Spielberg the director's chair for Jaws, a thriller-horror film based on the Peter Benchley novel about an enormous killer shark. Spielberg has often referred to the gruelling shoot as his professional crucible. Despite the film's ultimate, enormous success, it was nearly shut down due to delays and budget over-runs. But Spielberg persevered and finished the film. It was an enormous hit, winning three Academy Awards (for editing, original score and sound) and grossing more than $470 million worldwide at the box office. It also set the domestic record for box office gross, leading to what the press described as "Jawsmania.":248 Jaws made Spielberg a household name and one of America's youngest multi-millionaires, allowing him a great deal of autonomy for his future projects.:250 It was nominated for Best Picture and featured Spielberg's first of three collaborations with actor Richard Dreyfuss.
Rejecting offers to direct Jaws 2, King Kong and Superman, Spielberg and actor Richard Dreyfuss re-convened to work on a film about UFOs, which became Close Encounters of the Third Kind (1977). One of the rare films both written and directed by Spielberg, Close Encounters was a critical and box office hit, giving Spielberg his first Best Director nomination from the Academy as well as earning six other Academy Awards nominations. It won Oscars in two categories (Cinematography, Vilmos Zsigmond, and a Special Achievement Award for Sound Effects Editing, Frank E. Warner). This second blockbuster helped to secure Spielberg's rise. His next film, 1941, a big-budgeted World War II farce, was not nearly as successful and though it grossed over $92.4 million worldwide (and did make a small profit for co-producing studios Columbia and Universal) it was seen as a disappointment, mainly with the critics.
Spielberg then revisited his Close Encounters project and, with financial backing from Columbia Pictures, released Close Encounters: The Special Edition in 1980. For this, Spielberg fixed some of the flaws he thought impeded the original 1977 version of the film and also, at the behest of Columbia, and as a condition of Spielberg revising the film, shot additional footage showing the audience the interior of the mothership seen at the end of the film (a decision Spielberg would later regret as he felt the interior of the mothership should have remained a mystery). Nevertheless, the re-release was a moderate success, while the 2001 DVD release of the film restored the original ending.
Next, Spielberg teamed with Star Wars creator and friend George Lucas on an action adventure film, Raiders of the Lost Ark, the first of the Indiana Jones films. The archaeologist and adventurer hero Indiana Jones was played by Harrison Ford (whom Lucas had previously cast in his Star Wars films as Han Solo). The film was considered an homage to the cliffhanger serials of the Golden Age of Hollywood. It became the biggest film at the box office in 1981, and the recipient of numerous Oscar nominations including Best Director (Spielberg's second nomination) and Best Picture (the second Spielberg film to be nominated for Best Picture). Raiders is still considered a landmark example of the action-adventure genre. The film also led to Ford's casting in Ridley Scott's Blade Runner.
His next directorial feature was the Raiders prequel Indiana Jones and the Temple of Doom. Teaming up once again with Lucas and Ford, the film was plagued with uncertainty for the material and script. This film and the Spielberg-produced Gremlins led to the creation of the PG-13 rating due to the high level of violence in films targeted at younger audiences. In spite of this, Temple of Doom is rated PG by the MPAA, even though it is the darkest and, possibly, most violent Indy film. Nonetheless, the film was still a huge blockbuster hit in 1984. It was on this project that Spielberg also met his future wife, actress Kate Capshaw.
In 1985, Spielberg released The Color Purple, an adaptation of Alice Walker's Pulitzer Prize-winning novel of the same name, about a generation of empowered African-American women during depression-era America. Starring Whoopi Goldberg and future talk-show superstar Oprah Winfrey, the film was a box office smash and critics hailed Spielberg's successful foray into the dramatic genre. Roger Ebert proclaimed it the best film of the year and later entered it into his Great Films archive. The film received eleven Academy Award nominations, including two for Goldberg and Winfrey. However, much to the surprise of many, Spielberg did not get a Best Director nomination.
In 1987, as China began opening to Western capital investment, Spielberg shot the first American film in Shanghai since the 1930s, an adaptation of J. G. Ballard's autobiographical novel Empire of the Sun, starring John Malkovich and a young Christian Bale. The film garnered much praise from critics and was nominated for several Oscars, but did not yield substantial box office revenues. Reviewer Andrew Sarris called it the best film of the year and later included it among the best films of the decade. Spielberg was also a co-producer of the 1987 film *batteries not included.
After two forays into more serious dramatic films, Spielberg then directed the third Indiana Jones film, 1989's Indiana Jones and the Last Crusade. Once again teaming up with Lucas and Ford, Spielberg also cast actor Sean Connery in a supporting role as Indy's father. The film earned generally positive reviews and was another box office success, becoming the highest grossing film worldwide that year; its total box office receipts even topped those of Tim Burton's much-anticipated film Batman, which had been the bigger hit domestically. Also in 1989, he re-united with actor Richard Dreyfuss for the romantic comedy-drama Always, about a daredevil pilot who extinguishes forest fires. Spielberg's first romantic film, Always was only a moderate success and had mixed reviews.
Spielberg's next film, Schindler's List, was based on the true story of Oskar Schindler, a man who risked his life to save 1,100 Jews from the Holocaust. Schindler's List earned Spielberg his first Academy Award for Best Director (it also won Best Picture). With the film a huge success at the box office, Spielberg used the profits to set up the Shoah Foundation, a non-profit organization that archives filmed testimony of Holocaust survivors. In 1997, the American Film Institute listed it among the 10 Greatest American Films ever Made (#9) which moved up to (#8) when the list was remade in 2007.
His next theatrical release in that same year was the World War II film Saving Private Ryan, about a group of U.S. soldiers led by Capt. Miller (Tom Hanks) sent to bring home a paratrooper whose three older brothers were killed in the same twenty-four hours, June 5–6, of the Normandy landing. The film was a huge box office success, grossing over $481 million worldwide and was the biggest film of the year at the North American box office (worldwide it made second place after Michael Bay's Armageddon). Spielberg won his second Academy Award for his direction. The film's graphic, realistic depiction of combat violence influenced later war films such as Black Hawk Down and Enemy at the Gates. The film was also the first major hit for DreamWorks, which co-produced the film with Paramount Pictures (as such, it was Spielberg's first release from the latter that was not part of the Indiana Jones series). Later, Spielberg and Tom Hanks produced a TV mini-series based on Stephen Ambrose's book Band of Brothers. The ten-part HBO mini-series follows Easy Company of the 101st Airborne Division's 506th Parachute Infantry Regiment. The series won a number of awards at the Golden Globes and the Emmys.
Spielberg and actor Tom Cruise collaborated for the first time for the futuristic neo-noir Minority Report, based upon the science fiction short story written by Philip K. Dick about a Washington D.C. police captain in the year 2054 who has been foreseen to murder a man he has not yet met. The film received strong reviews with the review tallying website Rotten Tomatoes giving it a 92% approval rating, reporting that 206 out of the 225 reviews they tallied were positive. The film earned over $358 million worldwide. Roger Ebert, who named it the best film of 2002, praised its breathtaking vision of the future as well as for the way Spielberg blended CGI with live-action.
Also in 2005, Spielberg directed a modern adaptation of War of the Worlds (a co-production of Paramount and DreamWorks), based on the H. G. Wells book of the same name (Spielberg had been a huge fan of the book and the original 1953 film). It starred Tom Cruise and Dakota Fanning, and, as with past Spielberg films, Industrial Light & Magic (ILM) provided the visual effects. Unlike E.T. and Close Encounters of the Third Kind, which depicted friendly alien visitors, War of the Worlds featured violent invaders. The film was another huge box office smash, grossing over $591 million worldwide.
Spielberg's film Munich, about the events following the 1972 Munich Massacre of Israeli athletes at the Olympic Games, was his second film essaying Jewish relations in the world (the first being Schindler's List). The film is based on Vengeance, a book by Canadian journalist George Jonas. It was previously adapted into the 1986 made-for-TV film Sword of Gideon. The film received strong critical praise, but underperformed at the U.S. and world box-office; it remains one of Spielberg's most controversial films to date. Munich received five Academy Awards nominations, including Best Picture, Film Editing, Original Music Score (by John Williams), Best Adapted Screenplay, and Best Director for Spielberg. It was Spielberg's sixth Best Director nomination and fifth Best Picture nomination.
In June 2006, Steven Spielberg announced he would direct a scientifically accurate film about "a group of explorers who travel through a worm hole and into another dimension", from a treatment by Kip Thorne and producer Lynda Obst. In January 2007, screenwriter Jonathan Nolan met with them to discuss adapting Obst and Thorne's treatment into a narrative screenplay. The screenwriter suggested the addition of a "time element" to the treatment's basic idea, which was welcomed by Obst and Thorne. In March of that year, Paramount hired Nolan, as well as scientists from Caltech, forming a workshop to adapt the treatment under the title Interstellar. The following July, Kip Thorne said there was a push by people for him to portray himself in the film. Spielberg later abandoned Interstellar, which was eventually directed by Christopher Nolan.
In early 2009, Spielberg shot the first film in a planned trilogy of motion capture films based on The Adventures of Tintin, written by Belgian artist Hergé, with Peter Jackson. The Adventures of Tintin: The Secret of the Unicorn, was not released until October 2011, due to the complexity of the computer animation involved. The world premiere took place on October 22, 2011 in Brussels, Belgium. The film was released in North American theaters on December 21, 2011, in Digital 3D and IMAX. It received generally positive reviews from critics, and grossed over $373 million worldwide. The Adventures of Tintin won the award for Best Animated Feature Film at the Golden Globe Awards that year. It is the first non-Pixar film to win the award since the category was first introduced. Jackson has been announced to direct the second film.
Spielberg followed with War Horse, shot in England in the summer of 2010. It was released just four days after The Adventures of Tintin, on December 25, 2011. The film, based on the novel of the same name written by Michael Morpurgo and published in 1982, follows the long friendship between a British boy and his horse Joey before and during World War I – the novel was also adapted into a hit play in London which is still running there, as well as on Broadway. The film was released and distributed by Disney, with whom DreamWorks made a distribution deal in 2009. War Horse received generally positive reviews from critics, and was nominated for six Academy Awards, including Best Picture.
Spielberg next directed the historical drama film Lincoln, starring Daniel Day-Lewis as United States President Abraham Lincoln and Sally Field as Mary Todd Lincoln. Based on Doris Kearns Goodwin's bestseller Team of Rivals: The Political Genius of Abraham Lincoln, the film covered the final four months of Lincoln's life. Written by Tony Kushner, the film was shot in Richmond, Virginia, in late 2011, and was released in the United States by Disney in November 2012. The film's international distribution was handled by 20th Century Fox. Upon release, Lincoln received widespread critical acclaim, and was nominated for twelve Academy Awards (the most of any film that year) including Best Picture and Best Director for Spielberg. It won the award for Best Production Design and Day-Lewis won the Academy Award for Best Actor for his portrayal of Lincoln, becoming the first three time winner in that category as well as the first to win for a performance directed by Spielberg.
Spielberg directed 2015's Bridge of Spies, a Cold War thriller based on the 1960 U-2 incident, and focusing on James B. Donovan's negotiations with the Soviets for the release of pilot Gary Powers after his aircraft was shot down over Soviet territory. The film starred Tom Hanks as Donovan, as well as Mark Rylance, Amy Ryan, and Alan Alda, with a script by the Coen brothers. The film was shot from September to December 2014 on location in New York City, Berlin and Wroclaw, Poland (which doubled for East Berlin), and was released by Disney on October 16, 2015. Bridge of Spies received positive reviews from critics, and was nominated for six Academy Awards, including Best Picture.
Since the mid-1980s, Spielberg has increased his role as a film producer. He headed up the production team for several cartoons, including the Warner Bros. hits Tiny Toon Adventures, Animaniacs, Pinky and the Brain, Toonsylvania, and Freakazoid!, for which he collaborated with Jean MacCurdy and Tom Ruegger. Due to his work on these series, in the official titles, most of them say, "Steven Spielberg presents" as well as making numerous cameos on the shows. Spielberg also produced the Don Bluth animated features, An American Tail and The Land Before Time, which were released by Universal Studios. He also served as one of the executive producers of Who Framed Roger Rabbit and its three related shorts (Tummy Trouble, Roller Coaster Rabbit, Trail Mix-Up), which were all released by Disney, under both the Walt Disney Pictures and the Touchstone Pictures banners. He was furthermore, for a short time, the executive producer of the long-running medical drama ER. In 1989, he brought the concept of The Dig to LucasArts. He contributed to the project from that time until 1995 when the game was released. He also collaborated with software publishers Knowledge Adventure on the multimedia game Steven Spielberg's Director's Chair, which was released in 1996. Spielberg appears, as himself, in the game to direct the player. The Spielberg name provided branding for a Lego Moviemaker kit, the proceeds of which went to the Starbright Foundation.
Spielberg served as an uncredited executive producer on The Haunting, The Prince of Egypt, Just Like Heaven, Shrek, Road to Perdition, and Evolution. He served as an executive producer for the 1997 film Men in Black, and its sequels, Men in Black II and Men in Black III. In 2005, he served as a producer of Memoirs of a Geisha, an adaptation of the novel by Arthur Golden, a film to which he was previously attached as director. In 2006, Spielberg co-executive produced with famed filmmaker Robert Zemeckis a CGI children's film called Monster House, marking their eighth collaboration since 1990's Back to the Future Part III. He also teamed with Clint Eastwood for the first time in their careers, co-producing Eastwood's Flags of Our Fathers and Letters from Iwo Jima with Robert Lorenz and Eastwood himself. He earned his twelfth Academy Award nomination for the latter film as it was nominated for Best Picture. Spielberg served as executive producer for Disturbia and the Transformers live action film with Brian Goldner, an employee of Hasbro. The film was directed by Michael Bay and written by Roberto Orci and Alex Kurtzman, and Spielberg continued to collaborate on the sequels, Transformers: Revenge of the Fallen and Transformers: Dark of the Moon. In 2011, he produced the J. J. Abrams science fiction thriller film Super 8 for Paramount Pictures.
Other major television series Spielberg produced were Band of Brothers, Taken and The Pacific. He was an executive producer on the critically acclaimed 2005 TV miniseries Into the West which won two Emmy awards, including one for Geoff Zanelli's score. For his 2010 miniseries The Pacific he teamed up once again with co-producer Tom Hanks, with Gary Goetzman also co-producing'. The miniseries is believed to have cost $250 million and is a 10-part war miniseries centered on the battles in the Pacific Theater during World War II. Writer Bruce McKenna, who penned several installments of (Band of Brothers), was the head writer.
In 2011, Spielberg launched Falling Skies, a science fiction television series, on the TNT network. He developed the series with Robert Rodat and is credited as an executive producer. Spielberg is also producing the Fox TV series Terra Nova. Terra Nova begins in the year 2149 when all life on the planet Earth is threatened with extinction resulting in scientists opening a door that allows people to travel back 85 million years to prehistoric times. Spielberg also produced The River, Smash, Under the Dome, Extant and The Whispers, as well as a TV adaptation of Minority Report.
Apart from being an ardent gamer Spielberg has had a long history of involvement in video games. He has been giving thanks to his games of his division DreamWorks Interactive most notable as Someone's in the Kitchen with script written by Animaniacs' Paul Rugg, Goosebumps: Escape from HorrorLand, The Neverhood (all in 1996), Skullmonkeys, Dilbert's Desktop Games, Goosebumps: Attack of the Mutant (all 1997), Boombots (1999), T'ai Fu: Wrath of the Tiger (1999), and Clive Barker's Undying (2001). In 2005 the director signed with Electronic Arts to collaborate on three games including an action game and an award winning puzzle game for the Wii called Boom Blox (and its 2009 sequel: Boom Blox Bash Party). Previously, he was involved in creating the scenario for the adventure game The Dig. In 1996, Spielberg worked on and shot original footage for a movie-making simulation game called Steven Spielberg's Director's Chair. He is the creator of the Medal of Honor series by Electronic Arts. He is credited in the special thanks section of the 1998 video game Trespasser. In 2013, Spielberg has announced he is collaborating with 343 Industries for a live-action TV show of Halo.
Spielberg has filmed and is currently in post-production on an adaptation of Roald Dahl's celebrated children's story The BFG. Spielberg's DreamWorks bought the rights in 2010, originally intending John Madden to direct. The film was written by E.T. screenwriter Melissa Mathison and is co-produced by Walt Disney Pictures, marking the first Disney-branded film to be directed by Spielberg. The BFG is set to premiere out of competition at the Cannes Film Festival in May 2016, before its wide release in the US on July 1, 2016.
After completing filming on Ready Player One, while it is in its lengthy, effects-heavy post-production, he will film his long-planned adaptation of David Kertzer's acclaimed The Kidnapping of Edgardo Mortara. The book follows the true story of a young Jewish boy in 1858 Italy who was secretly baptized by a family servant and then kidnapped from his family by the Papal States, where he was raised and trained as a priest, causing international outrage and becoming a media sensation. First announced in 2014, the book has been adapted by Tony Kushner and the film will again star Mark Rylance, as Pope Pius IX. It will be filmed in early 2017 for release at the end of that year, before Ready Player One is completed and released in 2018.
Spielberg was scheduled to shoot a $200 million adaptation of Daniel H. Wilson's novel Robopocalypse, adapted for the screen by Drew Goddard. The film would follow a global human war against a robot uprising about 15–20 years in the future. Like Lincoln, it was to be released by Disney in the United States and Fox overseas. It was set for release on April 25, 2014, with Anne Hathaway and Chris Hemsworth set to star, but Spielberg postponed production indefinitely in January 2013, just before it had been set to begin.
Spielberg's films often deal with several recurring themes. Most of his films deal with ordinary characters searching for or coming in contact with extraordinary beings or finding themselves in extraordinary circumstances. In an AFI interview in August 2000 Spielberg commented on his interest in the possibility of extra terrestrial life and how it has influenced some of his films. Spielberg described himself as feeling like an alien during childhood, and his interest came from his father, a science fiction fan, and his opinion that aliens would not travel light years for conquest, but instead curiosity and sharing of knowledge.
A strong consistent theme in his family-friendly work is a childlike, even naïve, sense of wonder and faith, as attested by works such as Close Encounters of the Third Kind, E.T. the Extra-Terrestrial, Hook, A.I. Artificial Intelligence and The BFG. According to Warren Buckland, these themes are portrayed through the use of low height camera tracking shots, which have become one of Spielberg's directing trademarks. In the cases when his films include children (E.T. the Extra-Terrestrial, Empire of the Sun, Jurassic Park, etc.), this type of shot is more apparent, but it is also used in films like Munich, Saving Private Ryan, The Terminal, Minority Report, and Amistad. If one views each of his films, one will see this shot utilized by the director, notably the water scenes in Jaws are filmed from the low-angle perspective of someone swimming. Another child oriented theme in Spielberg's films is that of loss of innocence and coming-of-age. In Empire of the Sun, Jim, a well-groomed and spoiled English youth, loses his innocence as he suffers through World War II China. Similarly, in Catch Me If You Can, Frank naively and foolishly believes that he can reclaim his shattered family if he accumulates enough money to support them.
The most persistent theme throughout his films is tension in parent-child relationships. Parents (often fathers) are reluctant, absent or ignorant. Peter Banning in Hook starts off in the beginning of the film as a reluctant married-to-his-work parent who through the course of the film regains the respect of his children. The notable absence of Elliott's father in E.T., is the most famous example of this theme. In Indiana Jones and the Last Crusade, it is revealed that Indy has always had a very strained relationship with his father, who is a professor of medieval literature, as his father always seemed more interested in his work, specifically in his studies of the Holy Grail, than in his own son, although his father does not seem to realize or understand the negative effect that his aloof nature had on Indy (he even believes he was a good father in the sense that he taught his son "self reliance," which is not how Indy saw it). Even Oskar Schindler, from Schindler's List, is reluctant to have a child with his wife. Munich depicts Avner as a man away from his wife and newborn daughter. There are of course exceptions; Brody in Jaws is a committed family man, while John Anderton in Minority Report is a shattered man after the disappearance of his son. This theme is arguably the most autobiographical aspect of Spielberg's films, since Spielberg himself was affected by his parents' divorce as a child and by the absence of his father. Furthermore, to this theme, protagonists in his films often come from families with divorced parents, most notably E.T. the Extra-Terrestrial (protagonist Elliot's mother is divorced) and Catch Me If You Can (Frank Abagnale's mother and father split early on in the film). Little known also is Tim in Jurassic Park (early in the film, another secondary character mentions Tim and Lex's parents' divorce). The family often shown divided is often resolved in the ending as well. Following this theme of reluctant fathers and father figures, Tim looks to Dr. Alan Grant as a father figure. Initially, Dr. Grant is reluctant to return those paternal feelings to Tim. However, by the end of the film, he has changed, and the kids even fall asleep with their heads on his shoulders.
In terms of casting and production itself, Spielberg has a known penchant for working with actors and production members from his previous films. For instance, he has cast Richard Dreyfuss in several films: Jaws, Close Encounters of the Third Kind, and Always. Aside from his role as Indiana Jones, Spielberg also cast Harrison Ford as a headteacher in E.T. the Extra-Terrestrial (though the scene was ultimately cut). Although Spielberg directed veteran voice actor Frank Welker only once (in Raiders of the Lost Ark, for which he voiced many of the animals), Welker has lent his voice in a number of productions Spielberg has executive produced from Gremlins to its sequel Gremlins 2: The New Batch, as well as The Land Before Time, Who Framed Roger Rabbit, and television shows such as Tiny Toons, Animaniacs, and SeaQuest DSV. Spielberg has used Tom Hanks on several occasions and has cast him in Saving Private Ryan, Catch Me If You Can, The Terminal, and Bridge of Spies. Spielberg has collaborated with Tom Cruise twice on Minority Report and War of the Worlds, and cast Shia LaBeouf in five films: Transformers, Eagle Eye, Indiana Jones and the Kingdom of the Crystal Skull, Transformers: Revenge of the Fallen, and Transformers: Dark of the Moon.
Spielberg prefers working with production members with whom he has developed an existing working relationship. An example of this is his production relationship with Kathleen Kennedy who has served as producer on all his major films from E.T. the Extra-Terrestrial to the recent Lincoln. For cinematography, Allen Daviau, a childhood friend and cinematographer, shot the early Spielberg film Amblin and most of his films up to Empire of the Sun; Janusz Kamiński who has shot every Spielberg film since Schindler's List (see List of film director and cinematographer collaborations); and the film editor Michael Kahn who has edited every film directed by Spielberg from Close Encounters to Munich (except E.T. the Extra-Terrestrial). Most of the DVDs of Spielberg's films have documentaries by Laurent Bouzereau.
A famous example of Spielberg working with the same professionals is his long-time collaboration with John Williams and the use of his musical scores in all of his films since The Sugarland Express (except Bridge of Spies, The Color Purple and Twilight Zone: The Movie). One of Spielberg's trademarks is his use of music by Williams to add to the visual impact of his scenes and to try and create a lasting picture and sound of the film in the memories of the film audience. These visual scenes often uses images of the sun (e.g. Empire of the Sun, Saving Private Ryan, the final scene of Jurassic Park, and the end credits of Indiana Jones and the Last Crusade (where they ride into the sunset)), of which the last two feature a Williams score at that end scene. Spielberg is a contemporary of filmmakers George Lucas, Francis Ford Coppola, Martin Scorsese, John Milius, and Brian De Palma, collectively known as the "Movie Brats". Aside from his principal role as a director, Spielberg has acted as a producer for a considerable number of films, including early hits for Joe Dante and Robert Zemeckis. Spielberg has often never worked with the same screenwriter in his films, beside Tony Kushner and David Koepp, who have written a few of his films more than once.
Spielberg first met actress Amy Irving in 1976 at the suggestion of director Brian De Palma, who knew he was looking for an actress to play in Close Encounters. After meeting her, Spielberg told his co-producer Julia Phillips, "I met a real heartbreaker last night.":293 Although she was too young for the role, she and Spielberg began dating and she eventually moved in to what she described as his "bachelor funky" house.:294 They lived together for four years, but the stresses of their professional careers took a toll on their relationship. Irving wanted to be certain that whatever success she attained as an actress would be her own: "I don't want to be known as Steven's girlfriend," she said, and chose not to be in any of his films during those years.:295
As a result, they broke up in 1979, but remained close friends. Then in 1984 they renewed their romance, and in November 1985, they married, already having had a son, Max Samuel. After three and a half years of marriage, however, many of the same competing stresses of their careers caused them to divorce in 1989. They agreed to maintain homes near each other as to facilitate the shared custody and parenting of their son.:403 Their divorce was recorded as the third most costly celebrity divorce in history.
In 2002, Spielberg was one of eight flagbearers who carried the Olympic Flag into Rice-Eccles Stadium at the Opening Ceremonies of the 2002 Winter Olympic Games in Salt Lake City. In 2006, Premiere listed him as the most powerful and influential figure in the motion picture industry. Time listed him as one of the 100 Most Important People of the Century. At the end of the 20th century, Life named him the most influential person of his generation. In 2009, Boston University presented him an honorary Doctor of Humane Letters degree.
According to Forbes' Most Influential Celebrities 2014 list, Spielberg was listed as the most influential celebrity in America. The annual list is conducted by E-Poll Market Research and it gave more than 6,600 celebrities on 46 different personality attributes a score representing "how that person is perceived as influencing the public, their peers, or both." Spielberg received a score of 47, meaning 47% of the US believes he is influential. Gerry Philpott, president of E-Poll Market Research, supported Spielberg's score by stating, "If anyone doubts that Steven Spielberg has greatly influenced the public, think about how many will think for a second before going into the water this summer."
A collector of film memorabilia, Spielberg purchased a balsa Rosebud sled from Citizen Kane (1941) in 1982. He bought Orson Welles's own directorial copy of the script for the radio broadcast The War of the Worlds (1938) in 1994. Spielberg has purchased Academy Award statuettes being sold on the open market and donated them to the Academy of Motion Picture Arts and Sciences, to prevent their further commercial exploitation. His donations include the Oscars that Bette Davis received for Dangerous (1935) and Jezebel (1938), and Clark Gable's Oscar for It Happened One Night (1934).
Since playing Pong while filming Jaws in 1974, Spielberg has been an avid video gamer. Spielberg played many of LucasArts adventure games, including the first Monkey Island games. He owns a Wii, a PlayStation 3, a PSP, and Xbox 360, and enjoys playing first-person shooters such as the Medal of Honor series and Call of Duty 4: Modern Warfare. He has also criticized the use of cut scenes in games, calling them intrusive, and feels making story flow naturally into the gameplay is a challenge for future game developers.
Drawing from his own experiences in Scouting, Spielberg helped the Boy Scouts of America develop a merit badge in cinematography in order to help promote filmmaking as a marketable skill. The badge was launched at the 1989 National Scout Jamboree, which Spielberg attended, and where he personally counseled many boys in their work on requirements. That same year, 1989, saw the release of Indiana Jones and the Last Crusade. The opening scene shows a teenage Indiana Jones in scout uniform bearing the rank of a Life Scout. Spielberg stated he made Indiana Jones a Boy Scout in honor of his experience in Scouting. For his career accomplishments, service to others, and dedication to a new merit badge Spielberg was awarded the Distinguished Eagle Scout Award.
In 2004 he was admitted as knight of the Légion d'honneur by president Jacques Chirac. On July 15, 2006, Spielberg was also awarded the Gold Hugo Lifetime Achievement Award at the Summer Gala of the Chicago International Film Festival, and also was awarded a Kennedy Center honour on December 3. The tribute to Spielberg featured a short, filmed biography narrated by Tom Hanks and included thank-yous from World War II veterans for Saving Private Ryan, as well as a performance of the finale to Leonard Bernstein's Candide, conducted by John Williams (Spielberg's frequent composer).[citation needed]
The Science Fiction Hall of Fame inducted Spielberg in 2005, the first year it considered non-literary contributors. In November 2007, he was chosen for a Lifetime Achievement Award to be presented at the sixth annual Visual Effects Society Awards in February 2009. He was set to be honored with the Cecil B. DeMille Award at the January 2008 Golden Globes; however, the new, watered-down format of the ceremony resulting from conflicts in the 2007–08 writers strike, the HFPA postponed his honor to the 2009 ceremony. In 2008, Spielberg was awarded the Légion d'honneur.
Boston (pronounced i/ˈbɒstən/) is the capital and largest city of the Commonwealth of Massachusetts in the United States. Boston also served as the historic county seat of Suffolk County until Massachusetts disbanded county government in 1999. The city proper covers 48 square miles (124 km2) with an estimated population of 655,884 in 2014, making it the largest city in New England and the 24th largest city in the United States. The city is the economic and cultural anchor of a substantially larger metropolitan area called Greater Boston, home to 4.7 million people and the tenth-largest metropolitan statistical area in the country. Greater Boston as a commuting region is home to 8.1 million people, making it the sixth-largest combined statistical area in the United States.
One of the oldest cities in the United States, Boston was founded on the Shawmut Peninsula in 1630 by Puritan settlers from England. It was the scene of several key events of the American Revolution, such as the Boston Massacre, the Boston Tea Party, the Battle of Bunker Hill, and the Siege of Boston. Upon American independence from Great Britain, the city continued to be an important port and manufacturing hub, as well as a center for education and culture. Through land reclamation and municipal annexation, Boston has expanded beyond the original peninsula. Its rich history attracts many tourists, with Faneuil Hall alone drawing over 20 million visitors per year. Boston's many firsts include the United States' first public school, Boston Latin School (1635), and first subway system (1897).
The area's many colleges and universities make Boston an international center of higher education and medicine, and the city is considered to be a world leader in innovation. Boston's economic base also includes finance, professional and business services, biotechnology, information technology, and government activities. Households in the city claim the highest average rate of philanthropy in the United States; businesses and institutions rank amongst the top in the country for environmental sustainability and investment. The city has one of the highest costs of living in the United States, though it remains high on world livability rankings.
Boston's early European settlers had first called the area Trimountaine (after its "three mountains"—only traces of which remain today) but later renamed it Boston after Boston, Lincolnshire, England, the origin of several prominent colonists. The renaming, on September 7, 1630 (Old Style),[b] was by Puritan colonists from England, who had moved over from Charlestown earlier that year in quest of fresh water. Their settlement was initially limited to the Shawmut Peninsula, at that time surrounded by the Massachusetts Bay and Charles River and connected to the mainland by a narrow isthmus. The peninsula is known to have been inhabited as early as 5000 BC.
In 1629, the Massachusetts Bay Colony's first governor, John Winthrop, led the signing of the Cambridge Agreement, a key founding document of the city. Puritan ethics and their focus on education influenced its early history; America's first public school was founded in Boston in 1635. Over the next 130 years, the city participated in four French and Indian Wars, until the British defeated the French and their native allies in North America. Boston was the largest town in British North America until Philadelphia grew larger in the mid 18th century.
Many of the crucial events of the American Revolution—the Boston Massacre, the Boston Tea Party, Paul Revere's midnight ride, the battles of Lexington and Concord and Bunker Hill, the Siege of Boston, and many others—occurred in or near Boston. After the Revolution, Boston's long seafaring tradition helped make it one of the world's wealthiest international ports, with rum, fish, salt, and tobacco being particularly important.
The Embargo Act of 1807, adopted during the Napoleonic Wars, and the War of 1812 significantly curtailed Boston's harbor activity. Although foreign trade returned after these hostilities, Boston's merchants had found alternatives for their capital investments in the interim. Manufacturing became an important component of the city's economy, and by the mid-19th century, the city's industrial manufacturing overtook international trade in economic importance. Until the early 20th century, Boston remained one of the nation's largest manufacturing centers and was notable for its garment production and leather-goods industries. A network of small rivers bordering the city and connecting it to the surrounding region facilitated shipment of goods and led to a proliferation of mills and factories. Later, a dense network of railroads furthered the region's industry and commerce.
During this period Boston flourished culturally as well, admired for its rarefied literary life and generous artistic patronage, with members of old Boston families—eventually dubbed Boston Brahmins—coming to be regarded as the nation's social and cultural elites.
Boston was an early port of the Atlantic triangular slave trade in the New England colonies, but was soon overtaken by Salem, Massachusetts and Newport, Rhode Island. Eventually Boston became a center of the abolitionist movement. The city reacted strongly to the Fugitive Slave Law of 1850, contributing to President Franklin Pierce's attempt to make an example of Boston after the Anthony Burns Fugitive Slave Case.
In 1822, the citizens of Boston voted to change the official name from "the Town of Boston" to "the City of Boston", and on March 4, 1822, the people of Boston accepted the charter incorporating the City. At the time Boston was chartered as a city, the population was about 46,226, while the area of the city was only 4.7 square miles (12 km2).
In the 1820s, Boston's population grew rapidly, and the city's ethnic composition changed dramatically with the first wave of European immigrants. Irish immigrants dominated the first wave of newcomers during this period, especially following the Irish Potato Famine; by 1850, about 35,000 Irish lived in Boston. In the latter half of the 19th century, the city saw increasing numbers of Irish, Germans, Lebanese, Syrians, French Canadians, and Russian and Polish Jews settled in the city. By the end of the 19th century, Boston's core neighborhoods had become enclaves of ethnically distinct immigrants—Italians inhabited the North End, Irish dominated South Boston and Charlestown, and Russian Jews lived in the West End. Irish and Italian immigrants brought with them Roman Catholicism. Currently, Catholics make up Boston's largest religious community, and since the early 20th century, the Irish have played a major role in Boston politics—prominent figures include the Kennedys, Tip O'Neill, and John F. Fitzgerald.
Between 1631 and 1890, the city tripled its area through land reclamation by filling in marshes, mud flats, and gaps between wharves along the waterfront. The largest reclamation efforts took place during the 19th century; beginning in 1807, the crown of Beacon Hill was used to fill in a 50-acre (20 ha) mill pond that later became the Haymarket Square area. The present-day State House sits atop this lowered Beacon Hill. Reclamation projects in the middle of the century created significant parts of the South End, the West End, the Financial District, and Chinatown. After The Great Boston Fire of 1872, workers used building rubble as landfill along the downtown waterfront. During the mid-to-late 19th century, workers filled almost 600 acres (2.4 km2) of brackish Charles River marshlands west of Boston Common with gravel brought by rail from the hills of Needham Heights. The city annexed the adjacent towns of South Boston (1804), East Boston (1836), Roxbury (1868), Dorchester (including present day Mattapan and a portion of South Boston) (1870), Brighton (including present day Allston) (1874), West Roxbury (including present day Jamaica Plain and Roslindale) (1874), Charlestown (1874), and Hyde Park (1912). Other proposals, for the annexation of Brookline, Cambridge, and Chelsea, were unsuccessful.
By the early and mid-20th century, the city was in decline as factories became old and obsolete, and businesses moved out of the region for cheaper labor elsewhere. Boston responded by initiating various urban renewal projects under the direction of the Boston Redevelopment Authority (BRA), which was established in 1957. In 1958, BRA initiated a project to improve the historic West End neighborhood. Extensive demolition was met with vociferous public opposition.
The BRA subsequently reevaluated its approach to urban renewal in its future projects, including the construction of Government Center. In 1965, the first Community Health Center in the United States opened, the Columbia Point Health Center, in the Dorchester neighborhood. It mostly served the massive Columbia Point public housing complex adjoining it, which was built in 1953. The health center is still in operation and was rededicated in 1990 as the Geiger-Gibson Community Health Center. The Columbia Point complex itself was redeveloped and revitalized into a mixed-income community called Harbor Point Apartments from 1984 to 1990. By the 1970s, the city's economy boomed after 30 years of economic downturn. A large number of high rises were constructed in the Financial District and in Boston's Back Bay during this time period. This boom continued into the mid-1980s and later began again. Hospitals such as Massachusetts General Hospital, Beth Israel Deaconess Medical Center, and Brigham and Women's Hospital lead the nation in medical innovation and patient care. Schools such as Boston College, Boston University, the Harvard Medical School, Northeastern University, Wentworth Institute of Technology, Berklee College of Music and Boston Conservatory attract students to the area. Nevertheless, the city experienced conflict starting in 1974 over desegregation busing, which resulted in unrest and violence around public schools throughout the mid-1970s.
Boston is an intellectual, technological, and political center but has lost some important regional institutions, including the acquisition of The Boston Globe by The New York Times, and the loss to mergers and acquisitions of local financial institutions such as FleetBoston Financial, which was acquired by Charlotte-based Bank of America in 2004. Boston-based department stores Jordan Marsh and Filene's have both been merged into the Cincinnati–based Macy's. Boston has experienced gentrification in the latter half of the 20th century, with housing prices increasing sharply since the 1990s. Living expenses have risen, and Boston has one of the highest costs of living in the United States, and was ranked the 129th most expensive major city in the world in a 2011 survey of 214 cities. Despite cost of living issues, Boston ranks high on livability ratings, ranking 36th worldwide in quality of living in 2011 in a survey of 221 major cities.
On April 15, 2013, two Chechen Islamist brothers exploded two bombs near the finish line of the Boston Marathon, killing three people and injuring roughly 264.
Boston has an area of 89.6 square miles (232.1 km2)—48.4 square miles (125.4 km2) (54.0%) of land and 41.2 square miles (106.7 km2) (46.0%) of water. The city's official elevation, as measured at Logan International Airport, is 19 ft (5.8 m) above sea level. The highest point in Boston is Bellevue Hill at 330 feet (100 m) above sea level, and the lowest point is at sea level. Situated onshore of the Atlantic Ocean, Boston is the only state capital in the contiguous United States with an oceanic coastline.
Boston is surrounded by the "Greater Boston" region and is contiguously bordered by the cities and towns of Winthrop, Revere, Chelsea, Everett, Somerville, Cambridge, Newton, Brookline, Needham, Dedham, Canton, Milton, and Quincy. The Charles River separates Boston from Watertown and the majority of Cambridge, and the mass of Boston from its own Charlestown neighborhood. To the east lie Boston Harbor and the Boston Harbor Islands National Recreation Area (which includes part of the city's territory, specifically Calf Island, Gallops Island, Great Brewster Island, Green Island, Little Brewster Island, Little Calf Island, Long Island, Lovells Island, Middle Brewster Island, Nixes Mate, Outer Brewster Island, Rainsford Island, Shag Rocks, Spectacle Island, The Graves, and Thompson Island). The Neponset River forms the boundary between Boston's southern neighborhoods and the city of Quincy and the town of Milton. The Mystic River separates Charlestown from Chelsea and Everett, and Chelsea Creek and Boston Harbor separate East Boston from Boston proper.
Boston is sometimes called a "city of neighborhoods" because of the profusion of diverse subsections; the city government's Office of Neighborhood Services has officially designated 23 neighborhoods.
More than two-thirds of inner Boston's modern land area did not exist when the city was founded, but was created via the gradual filling in of the surrounding tidal areas over the centuries, notably with earth from the leveling or lowering of Boston's three original hills (the "Trimountain", after which Tremont Street is named), and with gravel brought by train from Needham to fill the Back Bay. Downtown and its immediate surroundings consists largely of low-rise (often Federal style and Greek Revival) masonry buildings, interspersed with modern highrises, notably in the Financial District, Government Center, and South Boston. Back Bay includes many prominent landmarks, such as the Boston Public Library, Christian Science Center, Copley Square, Newbury Street, and New England's two tallest buildings—the John Hancock Tower and the Prudential Center. Near the John Hancock Tower is the old John Hancock Building with its prominent illuminated beacon, the color of which forecasts the weather. Smaller commercial areas are interspersed among areas of single-family homes and wooden/brick multi-family row houses. The South End Historic District is the largest surviving contiguous Victorian-era neighborhood in the US. The geography of downtown and South Boston was particularly impacted by the Central Artery/Tunnel Project (known unofficially as the "Big Dig"), which allowed for the removal of the unsightly elevated Central Artery and the incorporation of new green spaces and open areas.
Boston has a continental climate with some maritime influence, and using the −3 °C (27 °F) coldest month (January) isotherm, the city lies within the transition zone from a humid subtropical climate (Köppen Cfa) to a humid continental climate (Köppen Dfa), although the suburbs north and west of the city are significantly colder in winter and solidly fall under the latter categorization; the city lies at the transition between USDA plant hardiness zones 6b (most of the city) and 7a (Downtown, South Boston, and East Boston neighborhoods). Summers are typically warm to hot, rainy, and humid, while winters oscillate between periods of cold rain and snow, with cold temperatures. Spring and fall are usually mild, with varying conditions dependent on wind direction and jet stream positioning. Prevailing wind patterns that blow offshore minimize the influence of the Atlantic Ocean. The hottest month is July, with a mean temperature of 73.4 °F (23.0 °C). The coldest month is January, with a mean of 29.0 °F (−1.7 °C). Periods exceeding 90 °F (32 °C) in summer and below freezing in winter are not uncommon but rarely extended, with about 13 and 25 days per year seeing each, respectively. The most recent sub-0 °F (−18 °C) reading occurring on February 14, 2016 when the temperature dipped down to −9 °F (−23 °C), the coldest reading since 1957. In addition, several decades may pass between 100 °F (38 °C) readings, with the most recent such occurrence on July 22, 2011 when the temperature reached 103 °F (39 °C). The city's average window for freezing temperatures is November 9 through April 5.[c] Official temperature records have ranged from −18 °F (−28 °C) on February 9, 1934, up to 104 °F (40 °C) on July 4, 1911; the record cold daily maximum is 2 °F (−17 °C) on December 30, 1917, while, conversely, the record warm daily minimum is 83 °F (28 °C) on August 2, 1975.
Boston's coastal location on the North Atlantic moderates its temperature, but makes the city very prone to Nor'easter weather systems that can produce much snow and rain. The city averages 43.8 inches (1,110 mm) of precipitation a year, with 43.8 inches (111 cm) of snowfall per season. Snowfall increases dramatically as one goes inland away from the city (especially north and west of the city)—away from the moderating influence of the ocean. Most snowfall occurs from December through March, as most years see no measurable snow in April and November, and snow is rare in May and October. There is also high year-to-year variability in snowfall; for instance, the winter of 2011–12 saw only 9.3 in (23.6 cm) of accumulating snow, but the previous winter, the corresponding figure was 81.0 in (2.06 m).[d]
Fog is fairly common, particularly in spring and early summer, and the occasional tropical storm or hurricane can threaten the region, especially in late summer and early autumn. Due to its situation along the North Atlantic, the city often receives sea breezes, especially in the late spring, when water temperatures are still quite cold and temperatures at the coast can be more than 20 °F (11 °C) colder than a few miles inland, sometimes dropping by that amount near midday. Thunderstorms occur from May to September, that are occasionally severe with large hail, damaging winds and heavy downpours. Although downtown Boston has never been struck by a violent tornado, the city itself has experienced many tornado warnings. Damaging storms are more common to areas north, west, and northwest of the city. Boston has a relatively sunny climate for a coastal city at its latitude, averaging over 2,600 hours of sunshine per annum.
In 2010, Boston was estimated to have 617,594 residents (a density of 12,200 persons/sq mile, or 4,700/km2) living in 272,481 housing units— a 5% population increase over 2000. The city is the third most densely populated large U.S. city of over half a million residents. Some 1.2 million persons may be within Boston's boundaries during work hours, and as many as 2 million during special events. This fluctuation of people is caused by hundreds of thousands of suburban residents who travel to the city for work, education, health care, and special events.
In the city, the population was spread out with 21.9% at age 19 and under, 14.3% from 20 to 24, 33.2% from 25 to 44, 20.4% from 45 to 64, and 10.1% who were 65 years of age or older. The median age was 30.8 years. For every 100 females, there were 92.0 males. For every 100 females age 18 and over, there were 89.9 males. There were 252,699 households, of which 20.4% had children under the age of 18 living in them, 25.5% were married couples living together, 16.3% had a female householder with no husband present, and 54.0% were non-families. 37.1% of all households were made up of individuals and 9.0% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 3.08.
The median household income in Boston was $51,739, while the median income for a family was $61,035. Full-time year-round male workers had a median income of $52,544 versus $46,540 for full-time year-round female workers. The per capita income for the city was $33,158. 21.4% of the population and 16.0% of families are below the poverty line. Of the total population, 28.8% of those under the age of 18 and 20.4% of those 65 and older were living below the poverty line.
In 1950, whites represented 94.7% of Boston's population. From the 1950s to the end of the 20th century, the proportion of non-Hispanic whites in the city declined; in 2000, non-Hispanic whites made up 49.5% of the city's population, making the city majority-minority for the first time. However, in recent years the city has experienced significant gentrification, in which affluent whites have moved into formerly non-white areas. In 2006, the US Census Bureau estimated that non-Hispanic whites again formed a slight majority. But as of 2010, in part due to the housing crash, as well as increased efforts to make more affordable housing more available, the minority population has rebounded. This may also have to do with an increased Latino population and more clarity surrounding US Census statistics, which indicate a Non-Hispanic White population of 47 percent (some reports give slightly lower figures).
People of Irish descent form the largest single ethnic group in the city, making up 15.8% of the population, followed by Italians, accounting for 8.3% of the population. People of West Indian and Caribbean ancestry are another sizable group, at 6.0%, about half of whom are of Haitian ancestry. Over 27,000 Chinese Americans made their home in Boston city proper in 2013, and the city hosts a growing Chinatown accommodating heavily traveled Chinese-owned bus lines to and from Chinatown, Manhattan. Some neighborhoods, such as Dorchester, have received an influx of people of Vietnamese ancestry in recent decades. Neighborhoods such as Jamaica Plain and Roslindale have experienced a growing number of Dominican Americans. The city and greater area also has a growing immigrant population of South Asians, including the tenth-largest Indian community in the country.
The city has a sizable Jewish population with an estimated 25,000 Jews within the city and 227,000 within the Boston metro area; the number of congregations in Boston is estimated at 22. The adjacent communities of Brookline and Newton are both approximately one-third Jewish.
The city, especially the East Boston neighborhood, has a significant Hispanic community. Hispanics in Boston are mostly of Puerto Rican (30,506 or 4.9% of total city population), Dominican (25,648 or 4.2% of total city population), Salvadoran (10,850 or 1.8% of city population), Colombian (6,649 or 1.1% of total city population), Mexican (5,961 or 1.0% of total city population), and Guatemalan (4,451 or 0.7% of total city population) ethnic origin. When including all Hispanic national origins, they number 107,917. In Greater Boston, these numbers grow significantly with Puerto Ricans numbering 175,000+, Dominicans 95,000+, Salvadorans 40,000+, Guatemalans 31,000+, Mexicans 25,000+, and Colombians numbering 22,000+.
According to a 2014 study by the Pew Research Center, 57% of the population of the city identified themselves as Christians, with 25% professing attendance at a variety of churches that could be considered Protestant, and 29% professing Roman Catholic beliefs. while 33% claim no religious affiliation. The same study says that other religions (including Judaism, Buddhism, Islam, and Hinduism) collectively make up about 10% of the population.
As of 2010 the Catholic Church had the highest number of adherents as a single denomination in the Boston-Cambridge-Newton Metro area, with more than two million members and 339 churches, followed by the Episcopal Church with 58,000 adherents in 160 churches. The United Church of Christ had 55,000 members and 213 churches. The UCC is the successor of the city's Puritan religious traditions. Old South Church in Boston is one of the oldest congregations in the United States. It was organized in 1669 by dissenters from the First Church in Boston (1630). Notable past members include Samuel Adams, William Dawes, Benjamin Franklin, Samuel Sewall, and Phillis Wheatley. In 1773, Adams gave the signals from the Old South Meeting House that started the Boston Tea Party.
A global city, Boston is placed among the top 30 most economically powerful cities in the world. Encompassing $363 billion, the Greater Boston metropolitan area has the sixth-largest economy in the country and 12th-largest in the world.
Boston's colleges and universities have a significant effect on the regional economy. Boston attracts more than 350,000 college students from around the world, who contribute more than $4.8 billion annually to the city's economy. The area's schools are major employers and attract industries to the city and surrounding region. The city is home to a number of technology companies and is a hub for biotechnology, with the Milken Institute rating Boston as the top life sciences cluster in the country. Boston receives the highest absolute amount of annual funding from the National Institutes of Health of all cities in the United States.
The city is considered highly innovative for a variety of reasons, including the presence of academia, access to venture capital, and the presence of many high-tech companies. The Route 128 corridor and Greater Boston continue to be a major center for venture capital investment, and high technology remains an important sector.
Tourism also composes a large part of Boston's economy, with 21.2 million domestic and international visitors spending $8.3 billion in 2011; excluding visitors from Canada and Mexico, over 1.4 million international tourists visited Boston in 2014, with those from China and the United Kingdom leading the list. Boston's status as a state capital as well as the regional home of federal agencies has rendered law and government to be another major component of the city's economy. The city is a major seaport along the United States' East Coast and the oldest continuously operated industrial and fishing port in the Western Hemisphere.
Other important industries are financial services, especially mutual funds and insurance. Boston-based Fidelity Investments helped popularize the mutual fund in the 1980s and has made Boston one of the top financial cities in the United States. The city is home to the headquarters of Santander Bank, and Boston is a center for venture capital firms. State Street Corporation, which specializes in asset management and custody services, is based in the city. Boston is a printing and publishing center — Houghton Mifflin Harcourt is headquartered within the city, along with Bedford-St. Martin's Press and Beacon Press. Pearson PLC publishing units also employ several hundred people in Boston. The city is home to three major convention centers—the Hynes Convention Center in the Back Bay, and the Seaport World Trade Center and Boston Convention and Exhibition Center on the South Boston waterfront. The General Electric Corporation announced in January 2016 its decision to move the company's global headquarters to the Seaport District in Boston, from Fairfield, Connecticut, citing factors including Boston's preeminence in the realm of higher education.
The Boston Public Schools enrolls 57,000 students attending 145 schools, including the renowned Boston Latin Academy, John D. O'Bryant School of Math & Science, and Boston Latin School. The Boston Latin School, established 1635, is the oldest public high school in the US; Boston also operates the United States' second oldest public high school, and its oldest public elementary school. The system's students are 40% Hispanic or Latino, 35% Black or African American, 13% White, and 9% Asian. There are private, parochial, and charter schools as well, and approximately 3,300 minority students attend participating suburban schools through the Metropolitan Educational Opportunity Council.
Some of the most renowned and highly ranked universities in the world are located in the Boston area. Four members of the Association of American Universities are in Greater Boston (more than any other metropolitan area): Harvard University, the Massachusetts Institute of Technology, Boston University, and Brandeis University. Hospitals, universities, and research institutions in Greater Boston received more than $1.77 billion in National Institutes of Health grants in 2013, more money than any other American metropolitan area. Greater Boston has more than 100 colleges and universities, with 250,000 students enrolled in Boston and Cambridge alone. Its largest private universities include Boston University (the city's fourth-largest employer) with its main campus along Commonwealth Avenue and a medical campus in the South End; Northeastern University in the Fenway area; Suffolk University near Beacon Hill, which includes law school and business school; and Boston College, which straddles the Boston (Brighton)–Newton border. Boston's only public university is the University of Massachusetts Boston, on Columbia Point in Dorchester. Roxbury Community College and Bunker Hill Community College are the city's two public community colleges. Altogether, Boston's colleges and universities employ over 42,600 people, accounting for nearly 7 percent of the city's workforce.
Smaller private schools include Babson College, Bentley University, Boston Architectural College, Emmanuel College, Fisher College, MGH Institute of Health Professions, Massachusetts College of Pharmacy and Health Sciences, Simmons College, Wellesley College, Wheelock College, Wentworth Institute of Technology, New England School of Law (originally established as America's first all female law school), and Emerson College.
Metropolitan Boston is home to several conservatories and art schools, including Lesley University College of Art and Design, Massachusetts College of Art, the School of the Museum of Fine Arts, New England Institute of Art, New England School of Art and Design (Suffolk University), Longy School of Music of Bard College, and the New England Conservatory (the oldest independent conservatory in the United States). Other conservatories include the Boston Conservatory and Berklee College of Music, which has made Boston an important city for jazz music.
Several universities located outside Boston have a major presence in the city. Harvard University, the nation's oldest institute of higher education, is centered across the Charles River in Cambridge but has the majority of its land holdings and a substantial amount of its educational activities in Boston. Its business, medical, dental, and public health schools are located in Boston's Allston and Longwood neighborhoods. Harvard has plans for additional expansion into Allston. The Massachusetts Institute of Technology (MIT), which originated in Boston and was long known as "Boston Tech", moved across the river to Cambridge in 1916. Tufts University, whose main campus is north of the city in Somerville and Medford, locates its medical and dental school in Boston's Chinatown at Tufts Medical Center, a 451-bed academic medical institution that is home to both a full-service hospital for adults and the Floating Hospital for Children.
Like many major American cities, Boston has seen a great reduction in violent crime since the early 1990s. Boston's low crime rate since the 1990s has been credited to the Boston Police Department's collaboration with neighborhood groups and church parishes to prevent youths from joining gangs, as well as involvement from the United States Attorney and District Attorney's offices. This helped lead in part to what has been touted as the "Boston Miracle". Murders in the city dropped from 152 in 1990 (for a murder rate of 26.5 per 100,000 people) to just 31—not one of them a juvenile—in 1999 (for a murder rate of 5.26 per 100,000).
In 2008, there were 62 reported homicides. Through December 20 each of 2014 and 2015, the Boston Police Department reported 52 and 39 homicides, respectively.
Boston shares many cultural roots with greater New England, including a dialect of the non-rhotic Eastern New England accent known as Boston English, and a regional cuisine with a large emphasis on seafood, salt, and dairy products. Irish Americans are a major influence on Boston's politics and religious institutions. Boston also has its own collection of neologisms known as Boston slang.
Boston has been called the "Athens of America" for its literary culture, earning a reputation as "the intellectual capital of the United States." In the nineteenth century, Ralph Waldo Emerson, Henry David Thoreau, Nathaniel Hawthorne, Margaret Fuller, James Russell Lowell, and Henry Wadsworth Longfellow wrote in Boston. Some consider the Old Corner Bookstore, where these writers met and where The Atlantic Monthly was first published, to be "cradle of American literature. In 1852, the Boston Public Library was founded as the first free library in the United States. Boston's literary culture continues today thanks to the city's many universities and the Boston Book Festival.
Music is cherished in Boston. The Boston Symphony Orchestra is one of the "Big Five," a group of the greatest American orchestras, and the classical music magazine Gramophone called it one of the "world's best" orchestras. Symphony Hall (located west of Back Bay) is home to the Boston Symphony Orchestra, (and the related Boston Youth Symphony Orchestra, which is the largest youth orchestra in the nation) and the Boston Pops Orchestra. The British newspaper The Guardian called Boston Symphony Hall "one of the top venues for classical music in the world," adding that "Symphony Hall in Boston was where science became an essential part of concert hall design." Other concerts are held at the New England Conservatory's Jordan Hall. The Boston Ballet performs at the Boston Opera House. Other performing-arts organizations located in the city include the Boston Lyric Opera Company, Opera Boston, Boston Baroque (the first permanent Baroque orchestra in the US), and the Handel and Haydn Society (one of the oldest choral companies in the United States). The city is a center for contemporary classical music with a number of performing groups, several of which are associated with the city's conservatories and universities. These include the Boston Modern Orchestra Project and Boston Musica Viva. Several theaters are located in or near the Theater District south of Boston Common, including the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre.
There are several major annual events such as First Night, which occurs on New Year's Eve, the Boston Early Music Festival, the annual Boston Arts Festival at Christopher Columbus Waterfront Park, and Italian summer feasts in the North End honoring Catholic saints. The city is the site of several events during the Fourth of July period. They include the week-long Harborfest festivities and a Boston Pops concert accompanied by fireworks on the banks of the Charles River.
Because of the city's prominent role in the American Revolution, several historic sites relating to that period are preserved as part of the Boston National Historical Park. Many are found along the Freedom Trail, which is marked by a red line of bricks embedded in the ground. The city is also home to several art museums, including the Museum of Fine Arts and the Isabella Stewart Gardner Museum. The Institute of Contemporary Art is housed in a contemporary building designed by Diller Scofidio + Renfro in the Seaport District. The University of Massachusetts Boston campus on Columbia Point houses the John F. Kennedy Library. The Boston Athenaeum (one of the oldest independent libraries in the United States), Boston Children's Museum, Bull & Finch Pub (whose building is known from the television show Cheers), Museum of Science, and the New England Aquarium are within the city.
Boston has been a noted religious center from its earliest days. The Roman Catholic Archdiocese of Boston serves nearly 300 parishes and is based in the Cathedral of the Holy Cross (1875) in the South End, while the Episcopal Diocese of Massachusetts, with the Cathedral Church of St. Paul (1819) as its episcopal seat, serves just under 200 congregations. Unitarian Universalism has its headquarters on Beacon Hill. The Christian Scientists are headquartered in Back Bay at the Mother Church (1894). The oldest church in Boston is First Church in Boston, founded in 1630. King's Chapel, the city's first Anglican church, was founded in 1686 and converted to Unitarianism in 1785. Other churches include Christ Church (better known as Old North Church, 1723), the oldest church building in the city, Trinity Church (1733), Park Street Church (1809), Old South Church (1874), Jubilee Christian Church and Basilica and Shrine of Our Lady of Perpetual Help on Mission Hill (1878).
Air quality in Boston is generally very good: during the ten-year period 2004–2013, there were only 4 days in which the air was unhealthy for the general public, according to the EPA.
Some of the cleaner energy facilities in Boston include the Allston green district, with three ecologically compatible housing facilities. Boston is also breaking ground on multiple green affordable housing facilities to help reduce the carbon footprint of the city while simultaneously making these initiatives financially available to a greater population. Boston's climate plan is updated every three years and was most recently modified in 2013. This legislature includes the Building Energy Reporting and Disclosure Ordinance, which requires the city's larger buildings to disclose their yearly energy and water use statistics and partake in an energy assessment every five years. These statistics are made public by the city, thereby increasing incentives for buildings to be more environmentally conscious.
Another initiative, presented by the late Mayor Thomas Menino, is the Renew Boston Whole Building Incentive, which reduces the cost of living in buildings that are deemed energy efficient. This, much like the green housing developments, gives people of low socioeconomic status an opportunity to find housing in communities that support the environment. The ultimate goal of this initiative is to enlist 500 Bostonians to participate in a free, in-home energy assessment.
Many older buildings in certain areas of Boston are supported by wooden piles driven into the area's fill; these piles remain sound if submerged in water, but are subject to dry rot if exposed to air for long periods. Groundwater levels have been dropping, to varying degrees, in many areas of the city, due in part to an increase in the amount of rainwater discharged directly into sewers rather than absorbed by the ground. A city agency, the Boston Groundwater Trust, coordinates monitoring of groundwater levels throughout the city via a network of public and private monitoring wells. However, Boston's drinking water supply, from the Quabbin and Wachusett Reservoirs to the west, is one of the very few in the country so pure as to satisfy federal water quality standards without filtration.
Boston has teams in the four major North American professional sports leagues plus Major League Soccer, and has won 36 championships in these leagues, As of 2014[update]. It is one of six cities (along with Chicago, Detroit, Los Angeles, New York and Philadelphia) to have won championships in all four major sports. It has been suggested that Boston is the new "TitleTown, USA", as the city's professional sports teams have won nine championships since 2001: Patriots (2001, 2003, 2004, and 2014), Red Sox (2004, 2007, and 2013), Celtics (2008), and Bruins (2011). This love of sports has made Boston the United States Olympic Committee's choice to bid to hold the 2024 Summer Olympic Games, but the city cited financial concerns when it withdrew its bid on July 27, 2015.
The Boston Red Sox, a founding member of the American League of Major League Baseball in 1901, play their home games at Fenway Park, near Kenmore Square in the city's Fenway section. Built in 1912, it is the oldest sports arena or stadium in active use in the United States among the four major professional American sports leagues, encompassing Major League Baseball, the National Football League, National Basketball Association, and the National Hockey League. Boston was the site of the first game of the first modern World Series, in 1903. The series was played between the AL Champion Boston Americans and the NL champion Pittsburgh Pirates. Persistent reports that the team was known in 1903 as the "Boston Pilgrims" appear to be unfounded. Boston's first professional baseball team was the Red Stockings, one of the charter members of the National Association in 1871, and of the National League in 1876. The team played under that name until 1883, under the name Beaneaters until 1911, and under the name Braves from 1912 until they moved to Milwaukee after the 1952 season. Since 1966 they have played in Atlanta as the Atlanta Braves.
The TD Garden, formerly called the FleetCenter and built to replace the old, since-demolished Boston Garden, is adjoined to North Station and is the home of two major league teams: the Boston Bruins of the National Hockey League and the Boston Celtics of the National Basketball Association. The arena seats 18,624 for basketball games and 17,565 for ice hockey games. The Bruins were the first American member of the National Hockey League and an Original Six franchise. The Boston Celtics were founding members of the Basketball Association of America, one of the two leagues that merged to form the NBA. The Celtics have the distinction of having won more championships than any other NBA team, with seventeen.
While they have played in suburban Foxborough since 1971, the New England Patriots of the National Football League were founded in 1960 as the Boston Patriots, changing their name after relocating. The team won the Super Bowl after the 2001, 2003, 2004, and 2014 seasons. They share Gillette Stadium with the New England Revolution of Major League Soccer. The Boston Breakers of Women's Professional Soccer, which formed in 2009, play their home games at Dilboy Stadium in Somerville.
The area's many colleges and universities are active in college athletics. Four NCAA Division I members play in the city—Boston College, Boston University, Harvard University, and Northeastern University. Of the four, only Boston College participates in college football at the highest level, the Football Bowl Subdivision. Harvard participates in the second-highest level, the Football Championship Subdivision.
One of the best known sporting events in the city is the Boston Marathon, the 26.2-mile (42.2 km) race which is the world's oldest annual marathon, run on Patriots' Day in April. On April 15, 2013, two explosions killed three people and injured hundreds at the marathon. Another major annual event is the Head of the Charles Regatta, held in October.
Boston Common, located near the Financial District and Beacon Hill, is the oldest public park in the United States. Along with the adjacent Boston Public Garden, it is part of the Emerald Necklace, a string of parks designed by Frederick Law Olmsted to encircle the city. The Emerald Necklace includes Jamaica Pond, Boston's largest body of freshwater, and Franklin Park, the city's largest park and home of the Franklin Park Zoo. Another major park is the Esplanade, located along the banks of the Charles River. The Hatch Shell, an outdoor concert venue, is located adjacent to the Charles River Esplanade. Other parks are scattered throughout the city, with the major parks and beaches located near Castle Island; in Charlestown; and along the Dorchester, South Boston, and East Boston shorelines.
Boston's park system is well-reputed nationally. In its 2013 ParkScore ranking, The Trust for Public Land reported that Boston was tied with Sacramento and San Francisco for having the third-best park system among the 50 most populous US cities. ParkScore ranks city park systems by a formula that analyzes the city's median park size, park acres as percent of city area, the percent of residents within a half-mile of a park, spending of park services per resident, and the number of playgrounds per 10,000 residents.
Boston has a strong mayor – council government system in which the mayor (elected every fourth year) has extensive executive power. Marty Walsh became Mayor in January 2014, his predecessor Thomas Menino's twenty-year tenure having been the longest in the city's history. The Boston City Council is elected every two years; there are nine district seats, and four citywide "at-large" seats. The School Committee, which oversees the Boston Public Schools, is appointed by the mayor.
In addition to city government, numerous commissions and state authorities—including the Massachusetts Department of Conservation and Recreation, the Boston Public Health Commission, the Massachusetts Water Resources Authority (MWRA), and the Massachusetts Port Authority (Massport)—play a role in the life of Bostonians. As the capital of Massachusetts, Boston plays a major role in state politics.
The city has several federal facilities, including the John F. Kennedy Federal Office Building, the Thomas P. O'Neill Federal Building, the United States Court of Appeals for the First Circuit, the United States District Court for the District of Massachusetts, and the Federal Reserve Bank of Boston.
Federally, Boston is split between two congressional districts. The northern three-fourths of the city is in the 7th district, represented by Mike Capuano since 1998. The southern fourth is in the 8th district, represented by Stephen Lynch. Both are Democrats; a Republican has not represented a significant portion of Boston in over a century. The state's senior member of the United States Senate is Democrat Elizabeth Warren, first elected in 2012. The state's junior member of the United States Senate is Democrat Ed Markey, who was elected in 2013 to succeed John Kerry after Kerry's appointment and confirmation as the United States Secretary of State.
The Boston Globe and the Boston Herald are two of the city's major daily newspapers. The city is also served by other publications such as Boston magazine, The Improper Bostonian, DigBoston, and the Boston edition of Metro. The Christian Science Monitor, headquartered in Boston, was formerly a worldwide daily newspaper but ended publication of daily print editions in 2009, switching to continuous online and weekly magazine format publications. The Boston Globe also releases a teen publication to the city's public high schools, called Teens in Print or T.i.P., which is written by the city's teens and delivered quarterly within the school year.
The city's growing Latino population has given rise to a number of local and regional Spanish-language newspapers. These include El Planeta (owned by the former publisher of The Boston Phoenix), El Mundo, and La Semana. Siglo21, with its main offices in nearby Lawrence, is also widely distributed.
Various LGBT publications serve the city's large LGBT (lesbian, gay, bisexual and transgender) community such as The Rainbow Times, the only minority and lesbian-owned LGBT newsmagazine. Founded in 2006, The Rainbow Times is now based out of Boston, but serves all of New England.
Boston is the largest broadcasting market in New England, with the radio market being the 11th largest in the United States. Several major AM stations include talk radio WRKO, sports/talk station WEEI, and CBS Radio WBZ. WBZ (AM) broadcasts a news radio format. A variety of commercial FM radio formats serve the area, as do NPR stations WBUR and WGBH. College and university radio stations include WERS (Emerson), WHRB (Harvard), WUMB (UMass Boston), WMBR (MIT), WZBC (Boston College), WMFO (Tufts University), WBRS (Brandeis University), WTBU (Boston University, campus and web only), WRBB (Northeastern University) and WMLN-FM (Curry College).
The Boston television DMA, which also includes Manchester, New Hampshire, is the 8th largest in the United States. The city is served by stations representing every major American network, including WBZ-TV and its sister station WSBK-TV (the former a CBS O&O, the latter an MyNetwork TV affiliate), WCVB-TV (ABC), WHDH (NBC), WFXT (Fox), and WLVI (The CW). The city is also home to PBS station WGBH-TV, a major producer of PBS programs, which also operates WGBX. Spanish-language television networks, including MundoFox (WFXZ-CD), Univision (WUNI), Telemundo (WNEU), and Telefutura (WUTF-DT), have a presence in the region, with WNEU and WUTF serving as network owned-and-operated stations. Most of the area's television stations have their transmitters in nearby Needham and Newton along the Route 128 corridor. Six Boston television stations are carried by Canadian satellite television provider Bell TV and by cable television providers in Canada.
The Longwood Medical and Academic Area, adjacent to the Fenway district, is home to a large number of medical and research facilities, including Beth Israel Deaconess Medical Center, Brigham and Women's Hospital, Children's Hospital Boston, Dana-Farber Cancer Institute, Harvard Medical School, Joslin Diabetes Center, and the Massachusetts College of Pharmacy and Health Sciences. Prominent medical facilities, including Massachusetts General Hospital, Massachusetts Eye and Ear Infirmary and Spaulding Rehabilitation Hospital are located in the Beacon Hill area. St. Elizabeth's Medical Center is in Brighton Center of the city's Brighton neighborhood. New England Baptist Hospital is in Mission Hill. The city has Veterans Affairs medical centers in the Jamaica Plain and West Roxbury neighborhoods. The Boston Public Health Commission, an agency of the Massachusetts government, oversees health concerns for city residents. Boston EMS provides pre-hospital emergency medical services to residents and visitors.
Many of Boston's medical facilities are associated with universities. The facilities in the Longwood Medical and Academic Area and in Massachusetts General Hospital are affiliated with Harvard Medical School. Tufts Medical Center (formerly Tufts-New England Medical Center), located in the southern portion of the Chinatown neighborhood, is affiliated with Tufts University School of Medicine. Boston Medical Center, located in the South End neighborhood, is the primary teaching facility for the Boston University School of Medicine as well as the largest trauma center in the Boston area; it was formed by the merger of Boston University Hospital and Boston City Hospital, which was the first municipal hospital in the United States.
Logan Airport, located in East Boston and operated by the Massachusetts Port Authority (Massport), is Boston's principal airport. Nearby general aviation airports are Beverly Municipal Airport to the north, Hanscom Field to the west, and Norwood Memorial Airport to the south. Massport also operates several major facilities within the Port of Boston, including a cruise ship terminal and facilities to handle bulk and container cargo in South Boston, and other facilities in Charlestown and East Boston.
Downtown Boston's streets grew organically, so they do not form a planned grid, unlike those in later-developed Back Bay, East Boston, the South End, and South Boston. Boston is the eastern terminus of I-90, which in Massachusetts runs along the Massachusetts Turnpike. The elevated portion of the Central Artery, which carried most of the through traffic in downtown Boston, was replaced with the O'Neill Tunnel during the Big Dig, substantially completed in early 2006.
With nearly a third of Bostonians using public transit for their commute to work, Boston has the fifth-highest rate of public transit usage in the country. Boston's subway system, the Massachusetts Bay Transportation Authority (MBTA—known as the "T") operates the oldest underground rapid transit system in the Americas, and is the fourth-busiest rapid transit system in the country, with 65.5 miles (105 km) of track on four lines. The MBTA also operates busy bus and commuter rail networks, and water shuttles.
Amtrak's Northeast Corridor and Chicago lines originate at South Station, which serves as a major intermodal transportation hub, and stop at Back Bay. Fast Northeast Corridor trains, which serve New York City, Washington, D.C., and points in between, also stop at Route 128 Station in the southwestern suburbs of Boston. Meanwhile, Amtrak's Downeaster service to Maine originates at North Station, despite the current lack of a dedicated passenger rail link between the two railhubs, other than the "T" subway lines.
Nicknamed "The Walking City", Boston hosts more pedestrian commuters than do other comparably populated cities. Owing to factors such as the compactness of the city and large student population, 13 percent of the population commutes by foot, making it the highest percentage of pedestrian commuters in the country out of the major American cities. In 2011, Walk Score ranked Boston the third most walkable city in the United States. As of 2015[update], Walk Score still ranks Boston as the third most walkable US city, with a Walk Score of 80, a Transit Score of 75, and a Bike Score of 70.
Between 1999 and 2006, Bicycling magazine named Boston three times as one of the worst cities in the US for cycling; regardless, it has one of the highest rates of bicycle commuting. In 2008, as a consequence of improvements made to bicycling conditions within the city, the same magazine put Boston on its "Five for the Future" list as a "Future Best City" for biking, and Boston's bicycle commuting percentage increased from 1% in 2000 to 2.1% in 2009. The bikeshare program called Hubway launched in late July 2011, logging more than 140,000 rides before the close of its first season. The neighboring municipalities of Cambridge, Somerville, and Brookline joined the Hubway program in summer 2012.
Architecture (Latin architectura, from the Greek ἀρχιτέκτων arkhitekton "architect", from ἀρχι- "chief" and τέκτων "builder") is both the process and the product of planning, designing, and constructing buildings and other physical structures. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements.
The earliest surviving written work on the subject of architecture is De architectura, by the Roman architect Vitruvius in the early 1st century AD. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be:
According to Vitruvius, the architect should strive to fulfill each of these three attributes as well as possible. Leon Battista Alberti, who elaborates on the ideas of Vitruvius in his treatise, De Re Aedificatoria, saw beauty primarily as a matter of proportion, although ornament also played a part. For Alberti, the rules of proportion were those that governed the idealised human figure, the Golden mean. The most important aspect of beauty was therefore an inherent part of an object, rather than something applied superficially; and was based on universal, recognisable truths. The notion of style in the arts was not developed until the 16th century, with the writing of Vasari: by the 18th century, his Lives of the Most Excellent Painters, Sculptors, and Architects had been translated into Italian, French, Spanish and English.
In the early 19th century, Augustus Welby Northmore Pugin wrote Contrasts (1836) that, as the titled suggested, contrasted the modern, industrial world, which he disparaged, with an idealized image of neo-medieval world. Gothic architecture, Pugin believed, was the only "true Christian form of architecture."
The 19th-century English art critic, John Ruskin, in his Seven Lamps of Architecture, published 1849, was much narrower in his view of what constituted architecture. Architecture was the "art which so disposes and adorns the edifices raised by men ... that the sight of them" contributes "to his mental health, power, and pleasure".
For Ruskin, the aesthetic was of overriding significance. His work goes on to state that a building is not truly a work of architecture unless it is in some way "adorned". For Ruskin, a well-constructed, well-proportioned, functional building needed string courses or rustication, at the very least.
On the difference between the ideals of architecture and mere construction, the renowned 20th-century architect Le Corbusier wrote: "You employ stone, wood, and concrete, and with these materials you build houses and palaces: that is construction. Ingenuity is at work. But suddenly you touch my heart, you do me good. I am happy and I say: This is beautiful. That is Architecture".
While the notion that structural and aesthetic considerations should be entirely subject to functionality was met with both popularity and skepticism, it had the effect of introducing the concept of "function" in place of Vitruvius' "utility". "Function" came to be seen as encompassing all criteria of the use, perception and enjoyment of a building, not only practical but also aesthetic, psychological and cultural.
Among the philosophies that have influenced modern architects and their approach to building design are rationalism, empiricism, structuralism, poststructuralism, and phenomenology.
In the late 20th century a new concept was added to those included in the compass of both structure and function, the consideration of sustainability, hence sustainable architecture. To satisfy the contemporary ethos a building should be constructed in a manner which is environmentally friendly in terms of the production of its materials, its impact upon the natural and built environment of its surrounding area and the demands that it makes upon non-sustainable power sources for heating, cooling, water and waste management and lighting.
Building first evolved out of the dynamics between needs (shelter, security, worship, etc.) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and "architecture" is the name given to the most highly formalized and respected versions of that craft.
It is widely assumed that architectural success was the product of a process of trial and error, with progressively less trial and more replication as the results of the process proved increasingly satisfactory. What is termed vernacular architecture continues to be produced in many parts of the world. Indeed, vernacular buildings make up most of the built world that people experience every day. Early human settlements were mostly rural. Due to a surplus in production the economy began to expand resulting in urbanization thus creating urban areas which grew and evolved very rapidly in some cases, such as that of Çatal Höyük in Anatolia and Mohenjo Daro of the Indus Valley Civilization in modern-day Pakistan.
In many ancient civilizations, such as those of Egypt and Mesopotamia, architecture and urbanism reflected the constant engagement with the divine and the supernatural, and many ancient cultures resorted to monumentality in architecture to represent symbolically the political power of the ruler, the ruling elite, or the state itself.
Early Asian writings on architecture include the Kao Gong Ji of China from the 7th–5th centuries BCE; the Shilpa Shastras of ancient India and Manjusri Vasthu Vidya Sastra of Sri Lanka.
The architecture of different parts of Asia developed along different lines from that of Europe; Buddhist, Hindu and Sikh architecture each having different characteristics. Buddhist architecture, in particular, showed great regional diversity. Hindu temple architecture, which developed around the 3rd century BCE, is governed by concepts laid down in the Shastras, and is concerned with expressing the macrocosm and the microcosm. In many Asian countries, pantheistic religion led to architectural forms that were designed specifically to enhance the natural landscape.
Islamic architecture began in the 7th century CE, incorporating architectural forms from the ancient Middle East and Byzantium, but also developing features to suit the religious and social needs of the society. Examples can be found throughout the Middle East, North Africa, Spain and the Indian Sub-continent. The widespread application of the pointed arch was to influence European architecture of the Medieval period.
The major architectural undertakings were the buildings of abbeys and cathedrals. From about 900 CE onwards, the movements of both clerics and tradesmen carried architectural knowledge across Europe, resulting in the pan-European styles Romanesque and Gothic.
In Renaissance Europe, from about 1400 onwards, there was a revival of Classical learning accompanied by the development of Renaissance Humanism which placed greater emphasis on the role of the individual in society than had been the case during the Medieval period. Buildings were ascribed to specific architects – Brunelleschi, Alberti, Michelangelo, Palladio – and the cult of the individual had begun. There was still no dividing line between artist, architect and engineer, or any of the related vocations, and the appellation was often one of regional preference.
Architecture has to do with planning and designing form, space and ambience to reflect functional, technical, social, environmental and aesthetic considerations. It requires the creative manipulation and coordination of materials and technology, and of light and shadow. Often, conflicting requirements must be resolved. The practice of Architecture also encompasses the pragmatic aspects of realizing buildings and structures, including scheduling, cost estimation and construction administration. Documentation produced by architects, typically drawings, plans and technical specifications, defines the structure and/or behavior of a building or other kind of system that is to be or has been constructed.
Nunzia Rondanini stated, "Through its aesthetic dimension architecture goes beyond the functional aspects that it has in common with other human sciences. Through its own particular way of expressing values, architecture can stimulate and influence social life without presuming that, in and of itself, it will promote social development.'
To restrict the meaning of (architectural) formalism to art for art's sake is not only reactionary; it can also be a purposeless quest for perfection or originality which degrades form into a mere instrumentality".
The architecture and urbanism of the Classical civilizations such as the Greek and the Roman evolved from civic ideals rather than religious or empirical ones and new building types emerged. Architectural "style" developed in the form of the Classical orders.
Texts on architecture have been written since ancient time. These texts provided both general advice and specific formal prescriptions or canons. Some examples of canons are found in the writings of the 1st-century BCE Roman Architect Vitruvius. Some of the most important early examples of canonic architecture are religious.
In Europe during the Medieval period, guilds were formed by craftsmen to organise their trades and written contracts have survived, particularly in relation to ecclesiastical buildings. The role of architect was usually one with that of master mason, or Magister lathomorum as they are sometimes described in contemporary documents.
A revival of the Classical style in architecture was accompanied by a burgeoning of science and engineering which affected the proportions and structure of buildings. At this stage, it was still possible for an artist to design a bridge as the level of structural calculations involved was within the scope of the generalist.
With the emerging knowledge in scientific fields and the rise of new materials and technology, architecture and engineering began to separate, and the architect began to concentrate on aesthetics and the humanist aspects, often at the expense of technical aspects of building design. There was also the rise of the "gentleman architect" who usually dealt with wealthy clients and concentrated predominantly on visual qualities derived usually from historical prototypes, typified by the many country houses of Great Britain that were created in the Neo Gothic or Scottish Baronial styles. Formal architectural training in the 19th century, for example at École des Beaux-Arts in France, gave much emphasis to the production of beautiful drawings and little to context and feasibility. Effective architects generally received their training in the offices of other architects, graduating to the role from draughtsmen or clerks.
Meanwhile, the Industrial Revolution laid open the door for mass production and consumption. Aesthetics became a criterion for the middle class as ornamented products, once within the province of expensive craftsmanship, became cheaper under machine production.
Vernacular architecture became increasingly ornamental. House builders could use current architectural design in their work by combining features found in pattern books and architectural journals.
Around the beginning of the 20th century, a general dissatisfaction with the emphasis on revivalist architecture and elaborate decoration gave rise to many new lines of thought that served as precursors to Modern Architecture. Notable among these is the Deutscher Werkbund, formed in 1907 to produce better quality machine made objects. The rise of the profession of industrial design is usually placed here. Following this lead, the Bauhaus school, founded in Weimar, Germany in 1919, redefined the architectural bounds prior set throughout history, viewing the creation of a building as the ultimate synthesis—the apex—of art, craft, and technology.
When modern architecture was first practiced, it was an avant-garde movement with moral, philosophical, and aesthetic underpinnings. Immediately after World War I, pioneering modernist architects sought to develop a completely new style appropriate for a new post-war social and economic order, focused on meeting the needs of the middle and working classes. They rejected the architectural practice of the academic refinement of historical styles which served the rapidly declining aristocratic order. The approach of the Modernist architects was to reduce buildings to pure forms, removing historical references and ornament in favor of functionalist details. Buildings displayed their functional and structural elements, exposing steel beams and concrete surfaces instead of hiding them behind decorative forms.
Architects such as Frank Lloyd Wright developed Organic architecture, in which the form was defined by its environment and purpose, with an aim to promote harmony between human habitation and the natural world with prime examples being Robie House and Fallingwater.
Architects such as Mies van der Rohe, Philip Johnson and Marcel Breuer worked to create beauty based on the inherent qualities of building materials and modern construction techniques, trading traditional historic forms for simplified geometric forms, celebrating the new means and methods made possible by the Industrial Revolution, including steel-frame construction, which gave birth to high-rise superstructures. By mid-century, Modernism had morphed into the International Style, an aesthetic epitomized in many ways by the Twin Towers of New York's World Trade Center designed by Minoru Yamasaki.
Many architects resisted modernism, finding it devoid of the decorative richness of historical styles. As the first generation of modernists began to die after WWII, a second generation of architects including Paul Rudolph, Marcel Breuer, and Eero Saarinen tried to expand the aesthetics of modernism with Brutalism, buildings with expressive sculptural facades made of unfinished concrete. But an even new younger postwar generation critiqued modernism and Brutalism for being too austere, standardized, monotone, and not taking into account the richness of human experience offered in historical buildings across time and in different places and cultures.
One such reaction to the cold aesthetic of modernism and Brutalism is the school of metaphoric architecture, which includes such things as biomorphism and zoomorphic architecture, both using nature as the primary source of inspiration and design. While it is considered by some to be merely an aspect of postmodernism, others consider it to be a school in its own right and a later development of expressionist architecture.
Beginning in the late 1950s and 1960s, architectural phenomenology emerged as an important movement in the early reaction against modernism, with architects like Charles Moore in the USA, Christian Norberg-Schulz in Norway, and Ernesto Nathan Rogers and Vittorio Gregotti in Italy, who collectively popularized an interest in a new contemporary architecture aimed at expanding human experience using historical buildings as models and precedents. Postmodernism produced a style that combined contemporary building technology and cheap materials, with the aesthetics of older pre-modern and non-modern styles, from high classical architecture to popular or vernacular regional building styles. Robert Venturi famously defined postmodern architecture as a "decorated shed" (an ordinary building which is functionally designed inside and embellished on the outside), and upheld it against modernist and brutalist "ducks" (buildings with unnecessarily expressive tectonic forms).
Since the 1980s, as the complexity of buildings began to increase (in terms of structural systems, services, energy and technologies), the field of architecture became multi-disciplinary with specializations for each project type, technological expertise or project delivery methods. In addition, there has been an increased separation of the 'design' architect [Notes 1] from the 'project' architect who ensures that the project meets the required standards and deals with matters of liability.[Notes 2] The preparatory processes for the design of any large building have become increasingly complicated, and require preliminary studies of such matters as durability, sustainability, quality, money, and compliance with local laws. A large structure can no longer be the design of one person but must be the work of many. Modernism and Postmodernism have been criticised by some members of the architectural profession who feel that successful architecture is not a personal, philosophical, or aesthetic pursuit by individualists; rather it has to consider everyday needs of people and use technology to create liveable environments, with the design process being informed by studies of behavioral, environmental, and social sciences.
Environmental sustainability has become a mainstream issue, with profound effect on the architectural profession. Many developers, those who support the financing of buildings, have become educated to encourage the facilitation of environmentally sustainable design, rather than solutions based primarily on immediate cost. Major examples of this can be found in Passive solar building design, greener roof designs, biodegradable materials, and more attention to a structure's energy usage. This major shift in architecture has also changed architecture schools to focus more on the environment. Sustainability in architecture was pioneered by Frank Lloyd Wright, in the 1960s by Buckminster Fuller and in the 1970s by architects such as Ian McHarg and Sim Van der Ryn in the US and Brenda and Robert Vale in the UK and New Zealand. There has been an acceleration in the number of buildings which seek to meet green building sustainable design principles. Sustainable practices that were at the core of vernacular architecture increasingly provide inspiration for environmentally and socially sustainable contemporary techniques. The U.S. Green Building Council's LEED (Leadership in Energy and Environmental Design) rating system has been instrumental in this.
Concurrently, the recent movements of New Urbanism, Metaphoric architecture and New Classical Architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as leaning against solitary housing estates and suburban sprawl.
Pope Paul VI (Latin: Paulus VI; Italian: Paolo VI), born Giovanni Battista Enrico Antonio Maria Montini (Italian pronunciation: [dʒioˈvani baˈtista enˈriko anˈtonjo marˈija monˈtini]; 26 September 1897 – 6 August 1978), reigned as Pope from 21 June 1963 to his death in 1978. Succeeding Pope John XXIII, he continued the Second Vatican Council which he closed in 1965, implementing its numerous reforms, and fostered improved ecumenical relations with Eastern Orthodox and Protestants, which resulted in many historic meetings and agreements. Montini served in the Vatican's Secretariat of State from 1922 to 1954. While in the Secretariat of State, Montini and Domenico Tardini were considered as the closest and most influential colleagues of Pope Pius XII, who in 1954 named him Archbishop of Milan, the largest Italian diocese. Montini automatically became the Secretary of the Italian Bishops Conference. John XXIII elevated him to the College of Cardinals in 1958, and after the death of John XXIII, Montini was considered one of his most likely successors.
Upon his election to the papacy, Montini took the pontifical name Paul VI (the first to take the name "Paul" since 1605) to indicate a renewed worldwide mission to spread the message of Christ, following the example of Apostle St. Paul.[citation needed] He re-convened the Second Vatican Council, which was automatically closed with the death of John XXIII, and gave it priority and direction. After the council had concluded its work, Paul VI took charge of the interpretation and implementation of its mandates, often walking a thin line between the conflicting expectations of various groups within Catholicism. The magnitude and depth of the reforms affecting all fields of Church life during his pontificate exceeded similar reform policies of his predecessors and successors. Paul VI was a Marian devotee, speaking repeatedly to Marian congresses and mariological meetings, visiting Marian shrines and issuing three Marian encyclicals. Following his famous predecessor Saint Ambrose of Milan, he named Mary as the Mother of the Church during the Second Vatican Council. Paul VI sought dialogue with the world, with other Christians, other religions, and atheists, excluding nobody. He saw himself as a humble servant for a suffering humanity and demanded significant changes of the rich in North America and Europe in favour of the poor in the Third World. His positions on birth control, promulgated most famously in the 1968 encyclical Humanae vitae, and other political issues, were often controversial, especially in Western Europe and North America.
Giovanni Battista Montini was born in the village of Concesio, in the province of Brescia, Lombardy in 1897. His father Giorgio Montini was a lawyer, journalist, director of the Catholic Action and member of the Italian Parliament. His mother was Giudetta Alghisi, from a family of rural nobility. He had two brothers, Francesco Montini, who became a physician, and Lodovico Montini, who became a lawyer and politician. On 30 September 1897, he was baptized in the name of Giovanni Battista Enrico Antonio Maria Montini. He attended Cesare Arici, a school run by the Jesuits, and in 1916, he received a diploma from Arnaldo da Brescia, a public school in Brescia. His education was often interrupted by bouts of illness.
In 1916, he entered the seminary to become a Roman Catholic priest. He was ordained priest on 29 May 1920 in Brescia and celebrated his first Holy Mass in Brescia in the Basilica of Santa Maria delle Grazie. Montini concluded his studies in Milan with a doctorate in Canon Law in the same year. Afterwards he studied at the Gregorian University, the University of Rome La Sapienza and, at the request of Giuseppe Pizzardo at the Accademia dei Nobili Ecclesiastici. At the age of twenty-five, again at the request of Giuseppe Pizzardo, Montini entered the Secretariat of State in 1922, where he worked under Pizzardo together with Francesco Borgongini-Duca, Alfredo Ottaviani, Carlo Grano, Domenico Tardini and Francis Spellman. Consequently, he spent not a day as a parish priest. In 1925 he helped found the publishing house Morcelliana in Brescia, focused on promoting a 'Christian inspired culture'.
The only foreign diplomatic experience Montini underwent was his time in the nunciature in Warsaw, Poland in 1923. Like Achille Ratti before him,[a] he felt confronted with the huge problem, not limited to Poland, of excessive nationalism: "This form of nationalism treats foreigners as enemies, especially foreigners with whom one has common frontiers. Then one seeks the expansion of one's own country at the expense of the immediate neighbours. People grow up with a feeling of being hemmed in. Peace becomes a transient compromise between wars." When he was recalled to Rome he was happy to go, because "this concludes this episode of my life, which has provided useful, though not always joyful, experiences."
His organisational skills led him to a career in the Roman Curia, the papal civil service. In 1931, Pacelli appointed him to teach history at the Papal Academy for Diplomats In 1937, after his mentor Giuseppe Pizzardo was named a cardinal and was succeeded by Domenico Tardini, Montini was named Substitute for Ordinary Affairs under Cardinal Pacelli, the Secretary of State under Pope Pius XI. From Pius XI, whom he viewed with awe, he adopted the view, that learning is a life long process, and that history was the magister vitae teacher of life His immediate supervisor in the Vatican was Domenico Tardini, with whom he got along well. The election of Pacelli to the papacy in 1939, anticipated by everybody and openly promoted by Pope Pius XI in his last years, was a good omen for Montini, whose position was confirmed in the position under the new Cardinal Secretary of State Luigi Maglione. He met the pope every morning until 1954 and thus developed a rather close relationship:
As war broke out, Maglione, Tardini and Montini were the main figures in the Vatican's State Department, as despatches originated from or addressed to them during the war years.[page needed] Montini was in charge of taking care of the "ordinary affairs" of the Secretariat of State, which took much of the mornings of every working day. In the afternoon he moved to the third floor into the Office of the Private Secretary of the Pontiff. Pius XII did not have a personal secretary. As did several popes before him, he delegated the secretarial functions to the State Secretariat. During the war years, thousands of letters from all parts of the world arrived at the desk of the pope, most of them asking for understanding, prayer and help. Montini was tasked to formulate the replies in the name of Pius XII, expressing his empathy, and understanding and providing help, where possible.
At the request of the pope, he created an information office for prisoners of war and refugees, which in the years of its existence from 1939 until 1947 received almost ten million (9 891 497) information requests and produced over eleven million (11.293.511) answers about missing persons. Montini was several times openly attacked by Benito Mussolini's government as a politician, and meddling in politics, but each time he found powerful defenses by the Vatican. In 1944, Luigi Maglione died, and Pius XII appointed Tardini and Montini together as heads of the State Department. Montini's admiration was almost filial, when he described Pope Pius XII:
As Secretary of State Montini coordinated the activities of assistance to the persecuted hidden in convents, parishes, seminaries, and in ecclesiastical schools. At the request of the pope, together with Pascalina Lehnert, Ferdinando Baldelli and Otto Faller, he created the Pontificia Commissione di Assistenza, which aided large number of Romans and refugees from everywhere with shelter, food and other material assistance. In Rome alone this organization distributed almost two million portions of free food in the year 1944. The Vatican and the Papal Residence Castel Gandolfo were opened to refugees. Some 15,000 persons lived in Castel Gandolfo alone, supported by the Pontificia Commissione di Assistenza. At the request of Pius XII, Montini was also involved in the re-establishment of Church Asylum, providing protection to hundreds of Allied soldiers, who had escaped from Axis prison camps, Jews, anti-Fascists, Socialists, Communists, and after the liberation of Rome, German soldiers, partisans and other displaced persons. After the war and later as pope, Montini turned the Pontificia Commissione di Assistenza, into the major charity, Caritas Italiana.[b]
Pius XII delivered an address about Montini's appointment from his sick-bed over radio to those assembled in St. Peter's Basilica on 12 December 1954. Both Montini and the pope had tears in their eyes when Montini parted for his dioceses with 1,000 churches, 2,500 priests and 3,500,000 souls. On 5 January 1955, Montini formally took possession of his Cathedral of Milan. Montini, after a period of preparation, liked his new tasks as archbishop, connecting to all groups of faithful in Milan. He enjoyed meetings with intellectuals, artists and writers.
Montini and Angelo Roncalli were considered to be friends, but when Roncalli, as Pope John XXIII announced a new Ecumenical Council, Cardinal Montini reacted with disbelief and said to Giulio Bevilacqua: "This old boy does not know what a hornets nest he is stirring up." He was appointed to the Central Preparatory Commission in 1961. During the Council, his friend Pope John XXIII asked him to live in the Vatican. He was a member of the Commission for Extraordinary Affairs but did not engage himself much into the floor debates on various issues. His main advisor was Monsignore Giovanni Colombo, whom he later appointed to be his successor in Milan The Commission was greatly overshadowed by the insistence of John XXIII to have the Council complete all its work in one single session before Christmas 1962, to the 400th anniversary of the Council of Trent, an insistence which may have also been influenced by the Pope's recent knowledge that he had cancer.
During his period in Milan, Montini was known as a progressive member of the Catholic hierarchy. Montini went new ways in pastoral care, which he reformed. He used his authority to ensure that the liturgical reforms of Pius XII were carried out at the local level and employed innovative methods to reach the people of Milan: Huge posters announced that 1,000 voices would speak to them from 10 to 24 November 1957. More than 500 priests and many bishops, cardinals and lay persons delivered 7,000 sermons in the period not only in churches but in factories, meeting halls, houses, courtyards, schools, offices, military barracks, hospitals, hotels and other places, where people meet. His goal was the re-introduction of faith to a city without much religion. "If only we can say Our Father and know what this means, then we would understand the Christian faith."
Pius XII asked Archbishop Montini to Rome October 1957, where he gave the main presentation to the Second World Congress of Lay Apostolate. Previously as Pro-Secretary, he had worked hard to unify a worldwide organization of lay people of 58 nations, representing 42 national organizations. He presented them to Pius XII in Rome in 1951. The second meeting in 1957 gave Montini an opportunity to express the lay apostolate in modern terms: "Apostolate means love. We will love all, but especially those, who need help... We will love our time, our technology, our art, our sports, our world."
Although some cardinals seem to have viewed him as papabile, a likely candidate to become pope, and may have received some votes in the 1958 conclave, Montini was not yet a cardinal, which made him an unlikely choice.[c] Angelo Roncalli was elected pope on 28 October 1958 and assumed the name John XXIII. On 17 November 1958, L'Osservatore Romano announced a consistory for the creation of new cardinals. Montini's name led the list. When the pope raised Montini to the cardinalate on 15 December 1958, he became Cardinal-Priest of Ss. Silvestro e Martino ai Monti. He appointed him simultaneously to several Vatican congregations which resulted in many visits by Montini to Rome in the coming years.
As a Cardinal, Montini journeyed to Africa (1962), where he visited Ghana, Sudan, Kenya, Congo, Rhodesia, South Africa, and Nigeria. After his journey, John XXIII gave him a private audience on his trip which lasted for several hours. In fifteen other trips he visited Brazil (1960) and the USA (1960), including New York City, Washington, DC, Chicago, the University of Notre Dame in Indiana, Boston, Philadelphia, and Baltimore. While a cardinal, he usually vacationed in Engelberg Abbey, a secluded Benedictine monastery in Switzerland.
Unlike the papabile cardinals Giacomo Lercaro of Bologna and Giuseppe Siri of Genoa, he was not identified with either the left or right, nor was he seen as a radical reformer. He was viewed as most likely to continue the Second Vatican Council, which already, without any tangible results, had lasted longer than anticipated by John XXIII, who had a vision but "did not have a clear agenda. His rhetoric seems to have had a note of over-optimism, a confidence in progress, which was characteristic of the 1960s." When John XXIII died of stomach cancer on 3 June 1963, it triggered a conclave to elect a new pope.
Paul VI did away with much of the regal splendor of the papacy. He was the last pope to date to be crowned; his successor Pope John Paul I replaced the Papal Coronation (which Paul had already substantially modified, but which he left mandatory in his 1975 apostolic constitution Romano Pontifici Eligendo) with a Papal Inauguration. Paul VI donated his own Papal Tiara, a gift from his former Archdiocese of Milan, to the Basilica of the National Shrine of the Immaculate Conception in Washington, DC (where it is on permanent display in the Crypt) as a gift to American Catholics.
During Vatican II, the Council Fathers avoided statements which might anger Christians of other faiths.[page needed] Cardinal Augustin Bea, the President of the Christian Unity Secretariat, always had the full support of Paul VI in his attempts to ensure that the Council language was friendly and open to the sensitivities of Protestant and Orthodox Churches, whom he had invited to all sessions at the request of Pope John XXIII. Bea also was strongly involved in the passage of Nostra aetate, which regulates the Church's relations with the Jewish faith and members of other religions.[d]
After his election as Bishop of Rome, Paul VI first met with the priests in his new dioceses. He told them that in Milan he started a dialogue with the modern world and asked them to seek contact with all people from all walks of life. Six days after his election he announced that he would continue Vatican II and convened the opening to take place on 29 September 1963. In a radio address to the world, Paul VI recalled the uniqueness of his predecessors, the strength of Pius XI, the wisdom and intelligence of Pius XII and the love of John XXIII. As "his pontifical goals" he mentioned the continuation and completion of Vatican II, the reform of the Canon Law and improved social peace and justice in the world. The Unity of Christianity would be central to his activities.
He reminded the council fathers that only a few years earlier Pope Pius XII had issued the encyclical Mystici corporis about the mystical body of Christ. He asked them not to repeat or create new dogmatic definitions but to explain in simple words how the Church sees itself. He thanked the representatives of other Christian communities for their attendance and asked for their forgiveness if the Catholic Church is guilty for the separation. He also reminded the Council Fathers that many bishops from the east could not attend because the governments in the East did not permit their journeys.
Paul VI opened the third period on 14 September 1964, telling the Council Fathers that he viewed the text about the Church as the most important document to come out from the Council. As the Council discussed the role of bishops in the papacy, Paul VI issued an explanatory note confirming the primacy of the papacy, a step which was viewed by some as meddling in the affairs of the Council American bishops pushed for a speedy resolution on religious freedom, but Paul VI insisted this to be approved together with related texts such as ecumenism. The Pope concluded the session on 21 November 1964, with the formal pronouncement of Mary as Mother of the Church.
Between the third and fourth sessions the pope announced reforms in the areas of Roman Curia, revision of Canon Law, regulations for mixed marriages involving several faiths, and birth control issues. He opened the final session of the council, concelebrating with bishops from countries where the Church was persecuted. Several texts proposed for his approval had to be changed. But all texts were finally agreed upon. The Council was concluded on 8 December 1965, the Feast of the Immaculate Conception.
Pope Paul VI knew the Roman Curia well, having worked there for a generation from 1922 to 1954. He implemented his reforms in stages, rather than in one fell swoop. On 1 March 1968, he issued a regulation, a process that had been initiated by Pius XII and continued by John XXIII. On 28 March, with Pontificalis Domus, and in several additional Apostolic Constitutions in the following years, he revamped the entire Curia, which included reduction of bureaucracy, streamlining of existing congregations and a broader representation of non-Italians in the curial positions.
Paul VI revolutionized papal elections by ordering that only cardinals below the age of eighty might participate in future conclaves. In Ecclesiae Sanctae, his motu proprio of 6 August 1966, he further invited all bishops to offer their retirement to the pontiff no later than the completion of their 75th year of age. This requirement was extended to all Cardinals of the Catholic Church on 21 November 1970. With these two stipulations, the Pope filled several positions with younger bishops and cardinals, and further internationalized the Roman Curia in light of several resignations due to age.
Reform of the liturgy had been a part of the liturgical movements in the 20th century mainly in France, and Germany which were officially recognized by Pius XII in his encyclical Mediator Dei. During the pontificate of Pius XII, the Vatican eased regulations on the use of Latin in Roman Catholic liturgies, permitting some use of vernacular languages during baptisms, funerals and other events. In 1951 and 1955, the Easter liturgies underwent revision, most notably including the reintroduction of the Easter Triduum. The Second Vatican Council made no changes to the Roman Missal, but in the document Sacrosanctum Concilium mandated that a general revision of it take place. After the Vatican Council, in April 1969, Paul VI approved the "new Order of Mass" promulgated in 1970, as stated in the Acta Apostolica Sedis to "end experimentation" with the Mass and which included the introduction of three new Eucharistic Prayers to what was up to then a single Roman Canon.
The Mass of Paul VI was also in Latin but approval was given for the use of vernacular languages. There had been other instructions issued by the Pope in 1964, 1967, 1968, 1969 and 1970 which centered on the reform of all liturgies of the Roman Church. These major reforms were not welcomed by all and in all countries. The sudden apparent "outlawing" of the 400-year-old Mass, the last typical edition of which being promulgated only a few years earlier in 1962 by Paul's predecessor, Pope John XXIII, was not always explained well. Further experimentation with the new Mass by liturgists, such as the usage of pop/folk music (as opposed to the Gregorian Chant advocated by Pope Pius X), along with concurrent changes in the order of sanctuaries, was viewed by some as vandalism. In 2007, Pope Benedict XVI clarified that the 1962 Mass of John XXIII and the 1970 Mass of Paul VI are two forms of the same Roman Rite, the first, which had never been "juridically abrogated", now being an "extraordinary form of the Roman Rite", while the other "obviously is and continues to be the normal Form – the Forma ordinaria – of the Eucharistic Liturgy".
In 1964, Paul VI created a Secretariat for non-Christians, later renamed the Pontifical Council for Interreligious Dialogue and a year later a new Secretariat (later Pontifical Council) for Dialogue with Non-Believers. This latter was in 1993 incorporated by Pope John Paul II in the Pontifical Council for Culture, which he had established in 1982. In 1971, Paul VI created a papal office for economic development and catastrophic assistance. To foster common bonds with all persons of good will, he decreed an annual peace day to be celebrated on January first of every year. Trying to improve the condition of Christians behind the Iron Curtain, Paul VI engaged in dialogue with Communist authorities at several levels, receiving Foreign Minister Andrei Gromyko and Chairman of the Presidium of the Supreme Soviet Nikolai Podgorny in 1966 and 1967 in the Vatican. The situation of the Church in Hungary, Poland and Romania, improved during his pontificate.
In 1976 Montini became the first pontiff in modern history to deny the accusation of homosexuality. Published by his order in January 1976 was a homily Persona Humana: Declaration on Certain Questions concerning Sexual Ethics, which outlawed pre or extra-marital sex, condemned homosexuality, and forbade masturbation. It provoked French author and former diplomat Roger Peyrefitte, in an interview published by the magazine Tempo, to accuse Montini of hypocrisy, and of having a longtime lover who was a movie actor. According to rumors prevalent both inside the Curia and in Italian society, this was Paolo Carlini, who had a bit part as a hairdresser in the Audrey Hepburn film Roman Holiday. Peyrefitte had previously published the accusation in two books, but the interview (previously published in a French gay magazine) brought the rumors to a wider public and caused an uproar. In a brief address to a crowd of approximately 20,000 in St. Peters Square on April 18, Montini called the charges "horrible and slanderous insinuations" and appealed for prayers on his behalf. Special prayers for Montini were said in all Italian Roman Catholic churches in "a day of consolation". In 1984 a New York Times correspondent repeated the allegations.
Pope Paul VI became the first pope to visit six continents, and was the most travelled pope in history to that time, earning the nickname "the Pilgrim Pope". With his travels he opened new avenues for the papacy, which were continued by his successors John Paul II and Benedict XVI. He travelled to the Holy Land in 1964, to the Eucharistic Congresses in Bombay, India and Bogotá, Colombia. In 1966, however, he was twice denied permission to visit Poland for the 1,000th anniversary of the baptism of Poland. In 1967, however, fifty years after the first apparition, he visited Fátima in Portugal. He undertook a pastoral visit to Africa in 1969. On 27 November 1970 he was the target of an assassination attempt at Manila International Airport in the Philippines. He was only lightly stabbed by the would-be assassin Benjamín Mendoza y Amor Flores, who was subdued by the pope's personal bodyguard and trip organizer, Msgr. Paul Marcinkus.
Pope Paul VI became the first reigning pontiff ever to visit the Americas when he flew to New York in October 1965 to address the United Nations. As a gesture of goodwill, the pope gave to the UN two pieces of papal jewelry, a diamond cross and ring, with the hopes that the proceeds from their sale at auction would contribute to the UN's efforts to end human suffering. During the pope's visit, as the U.S. involvement in the Vietnam War escalated under President Johnson, Paul VI pleaded for peace before the UN:
Like his predecessor Pius XII, Paul VI put much emphasis on the dialogue with all nations of the world through establishing diplomatic relations. The number of foreign embassies accredited to the Vatican doubled during his pontificate. This was a reflection of a new understanding between Church and State, which had been formulated first by Pius XI and Pius XII but decreed by Vatican II. The pastoral constitution Gaudium et spes stated that the Catholic Church is not bound to any form of government and willing to cooperate with all forms. The Church maintained its right to select bishops on its own without any interference by the State.
Ecclesiam suam was given at St. Peter's, Rome, on the Feast of the Transfiguration, 6 August 1964, the second year of his Pontificate. It is considered an important document, identifying the Catholic Church with the Body of Christ. A later Council document Lumen Gentium stated that the Church subsists in the Body of Christ, raising questions as to the difference between "is" and "subsists in". Paul VI appealed to "all people of good will" and discussed necessary dialogues within the Church and between the Churches and with atheism.
Sacerdotalis caelibatus (Latin for "Of the celibate priesthood"), promulgated on 24 June 1967, defends the Catholic Church's tradition of priestly celibacy in the West. This encyclical was written in the wake of Vatican II, when the Catholic Church was questioning and revising many long-held practices. Priestly celibacy is considered a discipline rather than dogma, and some had expected that it might be relaxed. In response to these questions, the Pope reaffirms the discipline as a long-held practice with special importance in the Catholic Church. The encyclical Sacerdotalis caelibatus from 24 June 1967, confirms the traditional Church teaching, that celibacy is an ideal state and continues to be mandatory for Roman Catholic priests. Celibacy symbolizes the reality of the kingdom of God amid modern society. The priestly celibacy is closely linked to the sacramental priesthood. However, during his pontificate Paul VI was considered generous in permitting bishops to grant laicization of priests who wanted to leave the sacerdotal state, a position which was drastically reversed by John Paul II in 1980 and cemented in the 1983 Canon Law that only the pope can in exceptional circumstances grant laicization.
Of his eight encyclicals, Pope Paul VI is best known for his encyclical Humanae vitae (Of Human Life, subtitled On the Regulation of Birth), published on 25 July 1968. In this encyclical he reaffirmed the Catholic Church's traditional view of marriage and marital relations and a continued condemnation of artificial birth control. There were two Papal committees and numerous independent experts looking into the latest advancement of science and medicine on the question of artificial birth control. which were noted by the Pope in his encyclical The expressed views of Paul VI reflected the teachings of his predecessors, especially Pius XI, Pius XII and John XXIII and never changed, as he repeatedly stated them in the first few years of his Pontificate
To the pope as to all his predecessors, marital relations are much more than a union of two people. They constitute a union of the loving couple with a loving God, in which the two persons create a new person materially, while God completes the creation by adding the soul. For this reason, Paul VI teaches in the first sentence of Humanae vitae that the transmission of human life is a most serious role in which married people collaborate freely and responsibly with God the Creator. This divine partnership, according to Paul VI, does not allow for arbitrary human decisions, which may limit divine providence. The Pope does not paint an overly romantic picture of marriage: marital relations are a source of great joy, but also of difficulties and hardships. The question of human procreation exceeds in the view of Paul VI specific disciplines such as biology, psychology, demography or sociology. The reason for this, according to Paul VI, is that married love takes its origin from God, who "is love". From this basic dignity, he defines his position:
The reaction to the encyclical's continued prohibitions of artificial birth control was very mixed. In Italy, Spain, Portugal and Poland, the encyclical was welcomed. In Latin America, much support developed for the Pope and his encyclical. As World Bank President Robert McNamara declared at the 1968 Annual Meeting of the International Monetary Fund and the World Bank Group that countries permitting birth control practices would get preferential access to resources, doctors in La Paz, Bolivia called it insulting that money should be exchanged for the conscience of a Catholic nation. In Colombia, Cardinal archbishop Aníbal Muñoz Duque declared, if American conditionality undermines Papal teachings, we prefer not to receive one cent. The Senate of Bolivia passed a resolution stating that Humanae vitae could be discussed in its implications for individual consciences, but was of greatest significance because the papal document defended the rights of developing nations to determine their own population policies. The Jesuit Journal Sic dedicated one edition to the encyclical with supportive contributions.
Paul VI was concerned but not surprised by the negative reaction in Western Europe and the United States. He fully anticipated this reaction to be a temporary one: "Don't be afraid", he reportedly told Edouard Gagnon on the eve of the encyclical, "in twenty years time they'll call me a prophet." His biography on the Vatican's website notes of his reaffirmations of priestly celibacy and the traditional teaching on contraception that "[t]he controversies over these two pronouncements tended to overshadow the last years of his pontificate". Pope John Paul II later reaffirmed and expanded upon Humanae vitae with the encyclical Evangelium vitae.
After the Council, Paul VI contributed in two ways to the continued growth of ecumenical dialogue. The separated brothers and sisters, as he called them, were not able to contribute to the Council as invited observers. After the Council, many of them took initiative to seek out their Catholic counterparts and the Pope in Rome, who welcomed such visits. But the Catholic Church itself recognized from the many previous ecumenical encounters, that much needed to be done within, to be an open partner for ecumenism. To those who are entrusted the highest and deepest truth and therefore, so Paul VI, believed that he had the most difficult part to communicate. Ecumenical dialogue, in the view of Paul VI, requires from a Catholic the whole person: one's entire reason, will, and heart. Paul VI, like Pius XII before him, was reluctant to give in on a lowest possible point. And yet, Paul felt compelled to admit his ardent Gospel-based desire to be everything to everybody and to help all people Being the successor of Peter, he felt the words of Christ, "Do you love me more" like a sharp knife penetrating to the marrow of his soul. These words meant to Paul VI love without limits, and they underscore the Church's fundamental approach to ecumenism.
This was a significant step towards restoring communion between Rome and Constantinople. It produced the Catholic-Orthodox Joint declaration of 1965, which was read out on 7 December 1965, simultaneously at a public meeting of the Second Vatican Council in Rome and at a special ceremony in Istanbul. The declaration did not end the schism, but showed a desire for greater reconciliation between the two churches. In May 1973, the Coptic Patriarch Shenouda III of Alexandria visited the Vatican, where he met three times with Pope Paul VI. A common declaration and a joint Creed issued after the visit demonstrated that there are virtually no more[additional citation needed] theological discrepancies between the Coptic and Roman Catholic Churches.
Paul VI was the first pope to receive an Anglican Archbishop of Canterbury, Michael Ramsey in official audience as Head of Church, after the private audience visit of Archbishop Geoffrey Fisher to Pope John XXIII on 2 December 1960. Ramsey met Paul three times during his visit and opened the Anglican Centre in Rome to increase their mutual knowledge. He praised Paul VI[e] and his contributions in the service of unity. Paul replied that "by entering into our house, you are entering your own house, we are happy to open our door and heart to you." The two Church leaders signed a common declaration, which put an end to the disputes of the past and outlined a common agenda for the future.
Cardinal Augustin Bea, the head of the Secretariat for Promoting Christian Unity, added at the end of the visit, "Let us move forward in Christ. God wants it. Humanity is waiting for it." Unmoved by a harsh condemnation by the Congregation of Faith on mixed marriages precisely at this time of the visit, Paul VI and Ramsey appointed a preparatory commission which was to put the common agenda into practice on such issues as mixed marriages. This resulted in a joint Malta declaration, the first joint agreement on the Creed since the Reformation. Paul VI was a good friend of the Anglican Church, which he described as "our beloved sister Church". This description was unique to Paul and not used by later popes.
In 1965, Paul VI decided on the creation of a joint working group with the World Council of Churches to map all possible avenues of dialogue and cooperation. In the following three years, eight sessions were held which resulted in many joint proposals. It was proposed to work closely together in areas of social justice and development and Third World Issues such as hunger and poverty. On the religious side, it was agreed to share together in the Week of Prayer for Christian Unity, to be held every year. The joint working group was to prepare texts which were to be used by all Christians. On 19 July 1968, the meeting of the World Council of Churches took place in Uppsala, Sweden, which Pope Paul called a sign of the times. He sent his blessing in an ecumenical manner: "May the Lord bless everything you do for the case of Christian Unity." The World Council of Churches decided on including Catholic theologians in its committees, provided they have the backing of the Vatican.
The Lutherans were the first Protestant Church offering a dialogue to the Catholic Church in September 1964 in Reykjavík, Iceland. It resulted in joint study groups of several issues. The dialogue with the Methodist Church began October 1965, after its representatives officially applauded remarkable changes, friendship and cooperation of the past five years. The Reformed Churches entered four years later into a dialogue with the Catholic Church. The President of the Lutheran World Federation and member of the central committee of the World Council of Churches Fredrik A. Schiotz stated during the 450th anniversary of the Reformation, that earlier commemorations were viewed almost as a triumph. Reformation should be celebrated as a thanksgiving to God, his truth and his renewed life. He welcomed the announcement of Pope Paul VI to celebrate the 1900th anniversary of the death of the Apostle Peter and Apostle Paul, and promised the participation and cooperation in the festivities.
Paul VI supported the new-found harmony and cooperation with Protestants on so many levels. When Cardinal Augustin Bea went to see him for permission for a joint Catholic-Protestant translation of the Bible with Protestant Bible societies, the pope walked towards him and exclaimed, "as far as the cooperation with Bible societies is concerned, I am totally in favour." He issued a formal approval on Pentecost 1967, the feast on which the Holy Spirit descended on the Christians, overcoming all linguistic difficulties, according to Christian tradition.
The next three popes, including Pope Emeritus Benedict XVI, were created cardinals by him. His immediate successor, Albino Luciani, who took the name John Paul I, was created a cardinal in the consistory of 5 March 1973. Karol Wojtyła was created a cardinal in the consistory of 26 June 1967. Joseph Ratzinger was created a cardinal in the small four-appointment consistory of 27 June 1977, which also included Bernardin Gantin from Benin, Africa. This became the last of Paul VI's consistories before his death in August 1978. Pope Paul was asked towards the end of his papacy whether he would retire at age 80, he replied "Kings can abdicate, Popes cannot."[citation needed]
Pope Paul VI left the Vatican to go to the papal summer residence, Castel Gandolfo, on 14 July 1978, visiting on the way the tomb of Cardinal Giuseppe Pizzardo, who had introduced him to the Vatican half a century earlier. Although he was sick, he agreed to see the new Italian President Sandro Pertini for over two hours. In the evening he watched a Western on TV, happy only when he saw "horses, the most beautiful animals that God had created." He had breathing problems and needed oxygen. On Sunday, at the Feast of the Transfiguration, he was tired, but wanted to say the Angelus. He was neither able nor permitted to do so and instead stayed in bed, his temperature rising.
On 20 December 2012, Pope Benedict XVI, in an audience with the Cardinal Prefect of the Congregation for the Causes of Saints, declared that the late pontiff had lived a life of heroic virtue, which means that he could be called "Venerable". A miracle attributed to the intercession of Paul VI was approved on 9 May 2014 by Pope Francis. The beatification ceremony for Paul VI was held on 19 October 2014, which means that he can now be called "Blessed". His liturgical feast day is celebrated on the date of his birth, 26 September, rather than the day of his death as is usual.
In December 2013, Vatican officials approved a supposed miracle that was attributed to the intercession of the late pontiff which was the curing of an unborn child in California, U.S.A in the 1990s. It was expected that Pope Francis would approve the miracle in the near future, thus, warranting the beatification of the late pontiff. In February 2014, it was reported that the consulting Vatican theologians to the Congregation for the Causes of Saints recognized the miracle attributed to the late pontiff.
On 24 April 2014, it was reported in the Italian magazine Credere that the late pope could possibly be beatified on 19 October 2014. This report from the magazine further stated that several cardinals and bishops would meet on 5 May to confirm the miracle that had previously been approved, and then present it to Pope Francis who may sign the decree for beatification shortly after that. The Congregation for the Causes of Saints held that meeting and positively concluded that the healing was indeed a miracle that could be attributed to the late pope. The matter shall now soon be presented to the pope for approval.
On basic Church teachings, the pope was unwavering. On the tenth anniversary of Humanae vitae, he reconfirmed this teaching. In his style and methodology, he was a disciple of Pius XII, whom he deeply revered. He suffered for the attacks on Pius XII for his alleged silences during the Holocaust. Pope Paul VI was less outstanding than his predecessors: he was not credited with an encyclopedic memory, nor a gift for languages, nor the brilliant writing style of Pius XII, nor did he have the charisma and outpouring love, sense of humor and human warmth of John XXIII. He took on himself the unfinished reform work of these two popes, bringing them diligently with great humility and common sense and without much fanfare to conclusion. In doing so, Paul VI saw himself following in the footsteps of the Apostle Paul, torn to several directions as Saint Paul, who said, "I am attracted to two sides at once, because the Cross always divides."
Unlike his predecessors and successors, Paul VI refused to excommunicate the opponents. He admonished but did not punish those with other views. The new theological freedoms which he fostered resulted in a pluralism of opinions and uncertainties among the faithful. New demands were voiced, which were taboo at the Council, the reintegration of divorced Catholics, the sacramental character of the confession, and the role of women in the Church and its ministries. Conservatives complained, that "women wanted to be priests, priests wanted to get married, bishops became regional popes and theologians claimed absolute teaching authority. Protestants claimed equality, homosexuals and divorced called for full acceptance." Changes such as the reorientation of the liturgy, alterations to the ordinary of the Mass, alterations to the liturgical calendar in the motu proprio Mysterii Paschalis, and the relocation of the tabernacle were controversial among some Catholics.
Some critiqued Paul VI's decision; the newly created Synod of Bishops had an advisory role only and could not make decisions on their own, although the Council decided exactly that. During the pontificate of Paul VI, five such synods took place, and he is on record of implementing all their decisions. Related questions were raised about the new National Bishop Conferences, which became mandatory after Vatican II. Others questioned his Ostpolitik and contacts with Communism and the deals he engaged in for the faithful.
From his bed he participated in Sunday Mass at 18:00. After communion, the pope suffered a massive heart attack, after which he continued to live for three hours. On 6 August 1978 at 21:41 Paul VI died in Castel Gandolfo. According to his will, he was buried in the grottos of the Vatican not in an ornate tomb, but in a grave in the ground. He is buried beneath the floor of Saint Peter's Basilica with other popes. In his will, he requested to be buried in the "true earth" and therefore, he does not have an ornate sarcophagus but an in-ground grave.
With the six consistories, Paul VI continued the internationalization policies started by Pius XII in 1946 and continued by John XXIII. In his 1976 consistory, five of twenty cardinals originated from Africa, one of them a son of a tribal chief with fifty wives. Several prominent Latin Americans like Eduardo Francisco Pironio of Argentina; Luis Aponte Martinez of Puerto Rico and Eugênio de Araújo Sales and Aloisio Lorscheider from Brazil were also elevated by him. There were voices within the Church at the time saying that the European period of the Church was coming to a close, a view shared by Britain's Cardinal Basil Hume. At the same time, the members of the College of Cardinals lost some of their previous influences, after Paul VI decreed, that not only cardinals but also bishops too may participate in committees of the Roman Curia. The age limit of eighty years imposed by the Pope, a numerical increase of Cardinals by almost 100%, and a reform of the regal vestments of the "Princes of the Church" further contributed to a service-oriented perception of Cardinals under his pontificate. The increased number of Cardinals from the Third World and the papal emphasis on related issues was nevertheless welcomed by many in Western Europe.
Paul VI did renounce many traditional symbols of the papacy and the Catholic Church; some of his changes to the papal dress were reversed by Pope Benedict XVI in the early 21st century. Refusing a Vatican army of colourful military uniforms from centuries, he got rid of them. He became the first pope to visit five continents. Paul VI systematically continued and completed the efforts of his predecessors, to turn the Euro-centric Church into a Church of the world, by integrating the bishops from all continents in its government and in the Synods which he convened. His 6 August 1967 motu proprio Pro Comperto Sane opened the Roman Curia to the bishops of the world. Until then, only Cardinals could be leading members of the Curia.
Buddhism /ˈbudɪzəm/ is a nontheistic religion[note 1] or philosophy (Sanskrit: धर्म dharma; Pali: धम्म dhamma) that encompasses a variety of traditions, beliefs and spiritual practices largely based on teachings attributed to Gautama Buddha, commonly known as the Buddha ("the awakened one"). According to Buddhist tradition, the Buddha lived and taught in the eastern part of the Indian subcontinent, present-day Nepal sometime between the 6th and 4th centuries BCE.[note 1] He is recognized by Buddhists as an awakened or enlightened teacher who shared his insights to help sentient beings end their suffering through the elimination of ignorance and craving. Buddhists believe that this is accomplished through the direct understanding and perception of dependent origination and the Four Noble Truths.
Two major extant branches of Buddhism are generally recognized by scholars: Theravada ("The School of the Elders") and Mahayana ("The Great Vehicle"). Vajrayana, a body of teachings attributed to Indian siddhas, may be viewed as a third branch or merely a part of Mahayana. Theravada has a widespread following in Sri Lanka and Southeast Asia. Mahayana which includes the traditions of Pure Land, Zen, Nichiren Buddhism, Shingon, and Tiantai (Tendai) is found throughout East Asia. Tibetan Buddhism, which preserves the Vajrayana teachings of eighth century India, is practiced in regions surrounding the Himalayas, Mongolia and Kalmykia. Buddhists number between an estimated 488 million[web 1] and 535 million, making it one of the world's major religions.
In Theravada Buddhism, the ultimate goal is the attainment of the sublime state of Nirvana, achieved by practicing the Noble Eightfold Path (also known as the Middle Way), thus escaping what is seen as a cycle of suffering and rebirth. Mahayana Buddhism instead aspires to Buddhahood via the bodhisattva path, a state wherein one remains in this cycle to help other beings reach awakening. Tibetan Buddhism aspires to Buddhahood or rainbow body.
Buddhist schools vary on the exact nature of the path to liberation, the importance and canonicity of various teachings and scriptures, and especially their respective practices. Buddhism denies a creator deity and posits that mundane deities such as Mahabrahma are misperceived to be a creator. The foundations of Buddhist tradition and practice are the Three Jewels: the Buddha, the Dharma (the teachings), and the Sangha (the community). Taking "refuge in the triple gem" has traditionally been a declaration and commitment to being on the Buddhist path, and in general distinguishes a Buddhist from a non-Buddhist. Other practices are Ten Meritorious Deeds including, giving charity to reduce the greediness; following ethical precepts; renouncing conventional living and becoming a monastic; the development of mindfulness and practice of meditation; cultivation of higher wisdom and discernment; study of scriptures; devotional practices; ceremonies; and in the Mahayana tradition, invocation of buddhas and bodhisattvas.
This narrative draws on the Nidānakathā of the Jataka tales of the Theravada, which is ascribed to Buddhaghoṣa in the 5th century CE. Earlier biographies such as the Buddhacarita, the Lokottaravādin Mahāvastu, and the Sarvāstivādin Lalitavistara Sūtra, give different accounts. Scholars are hesitant to make unqualified claims about the historical facts of the Buddha's life. Most accept that he lived, taught and founded a monastic order, but do not consistently accept all of the details contained in his biographies.
According to author Michael Carrithers, while there are good reasons to doubt the traditional account, "the outline of the life must be true: birth, maturity, renunciation, search, awakening and liberation, teaching, death." In writing her biography of the Buddha, Karen Armstrong noted, "It is obviously difficult, therefore, to write a biography of the Buddha that meets modern criteria, because we have very little information that can be considered historically sound... [but] we can be reasonably confident Siddhatta Gotama did indeed exist and that his disciples preserved the memory of his life and teachings as well as they could."[dubious – discuss]
The evidence of the early texts suggests that Siddhārtha Gautama was born in a community that was on the periphery, both geographically and culturally, of the northeastern Indian subcontinent in the fifth century BCE. It was either a small republic, in which case his father was an elected chieftain, or an oligarchy, in which case his father was an oligarch.
According to this narrative, shortly after the birth of young prince Gautama, an astrologer named Asita visited the young prince's father, Suddhodana, and prophesied that Siddhartha would either become a great king or renounce the material world to become a holy man, depending on whether he saw what life was like outside the palace walls.
Śuddhodana was determined to see his son become a king, so he prevented him from leaving the palace grounds. But at age 29, despite his father's efforts, Gautama ventured beyond the palace several times. In a series of encounters—known in Buddhist literature as the four sights—he learned of the suffering of ordinary people, encountering an old man, a sick man, a corpse and, finally, an ascetic holy man, apparently content and at peace with the world. These experiences prompted Gautama to abandon royal life and take up a spiritual quest.
Gautama first went to study with famous religious teachers of the day, and mastered the meditative attainments they taught. But he found that they did not provide a permanent end to suffering, so he continued his quest. He next attempted an extreme asceticism, which was a religious pursuit common among the śramaṇas, a religious culture distinct from the Vedic one. Gautama underwent prolonged fasting, breath-holding, and exposure to pain. He almost starved himself to death in the process. He realized that he had taken this kind of practice to its limit, and had not put an end to suffering. So in a pivotal moment he accepted milk and rice from a village girl and changed his approach. He devoted himself to anapanasati meditation, through which he discovered what Buddhists call the Middle Way (Skt. madhyamā-pratipad): a path of moderation between the extremes of self-indulgence and self-mortification.[web 2][web 3]
Gautama was now determined to complete his spiritual quest. At the age of 35, he famously sat in meditation under a Ficus religiosa tree now called the Bodhi Tree in the town of Bodh Gaya and vowed not to rise before achieving enlightenment. After many days, he finally destroyed the fetters of his mind, thereby liberating himself from the cycle of suffering and rebirth, and arose as a fully enlightened being (Skt. samyaksaṃbuddha). Soon thereafter, he attracted a band of followers and instituted a monastic order. Now, as the Buddha, he spent the rest of his life teaching the path of awakening he had discovered, traveling throughout the northeastern part of the Indian subcontinent, and died at the age of 80 (483 BCE) in Kushinagar, India. The south branch of the original fig tree available only in Anuradhapura Sri Lanka is known as Jaya Sri Maha Bodhi.
Within Buddhism, samsara is defined as the continual repetitive cycle of birth and death that arises from ordinary beings' grasping and fixating on a self and experiences. Specifically, samsara refers to the process of cycling through one rebirth after another within the six realms of existence,[note 2] where each realm can be understood as physical realm or a psychological state characterized by a particular type of suffering. Samsara arises out of avidya (ignorance) and is characterized by dukkha (suffering, anxiety, dissatisfaction). In the Buddhist view, liberation from samsara is possible by following the Buddhist path.
In Buddhism, Karma (from Sanskrit: "action, work") is the force that drives saṃsāra—the cycle of suffering and rebirth for each being. Good, skillful deeds (Pali: "kusala") and bad, unskillful (Pāli: "akusala") actions produce "seeds" in the mind that come to fruition either in this life or in a subsequent rebirth. The avoidance of unwholesome actions and the cultivation of positive actions is called sīla. Karma specifically refers to those actions of body, speech or mind that spring from mental intent (cetanā), and bring about a consequence or phala "fruit" or vipāka "result".
In Theravada Buddhism there can be no divine salvation or forgiveness for one's karma, since it is a purely impersonal process that is a part of the makeup of the universe.[citation needed] In Mahayana Buddhism, the texts of certain Mahayana sutras (such as the Lotus Sutra, the Aṅgulimālīya Sūtra and the Mahāyāna Mahāparinirvāṇa Sūtra) claim that the recitation or merely the hearing of their texts can expunge great swathes of negative karma. Some forms of Buddhism (for example, Vajrayana) regard the recitation of mantras as a means for cutting off of previous negative karma. The Japanese Pure Land teacher Genshin taught that Amitābha has the power to destroy the karma that would otherwise bind one in saṃsāra.
Rebirth refers to a process whereby beings go through a succession of lifetimes as one of many possible forms of sentient life, each running from conception to death. The doctrine of anattā (Sanskrit anātman) rejects the concepts of a permanent self or an unchanging, eternal soul, as it is called in Hinduism and Christianity. According to Buddhism there ultimately is no such thing as a self independent from the rest of the universe. Buddhists also refer to themselves as the believers of the anatta doctrine—Nairatmyavadin or Anattavadin. Rebirth in subsequent existences must be understood as the continuation of a dynamic, ever-changing process of pratītyasamutpāda ("dependent arising") determined by the laws of cause and effect (karma) rather than that of one being, reincarnating from one existence to the next.
The above are further subdivided into 31 planes of existence.[web 4] Rebirths in some of the higher heavens, known as the Śuddhāvāsa Worlds or Pure Abodes, can be attained only by skilled Buddhist practitioners known as anāgāmis (non-returners). Rebirths in the Ārūpyadhātu (formless realms) can be attained by only those who can meditate on the arūpajhānas, the highest object of meditation.
According to East Asian and Tibetan Buddhism, there is an intermediate state (Tibetan "bardo") between one life and the next. The orthodox Theravada position rejects this; however there are passages in the Samyutta Nikaya of the Pali Canon that seem to lend support to the idea that the Buddha taught of an intermediate stage between one life and the next.[page needed]
The teachings on the Four Noble Truths are regarded as central to the teachings of Buddhism, and are said to provide a conceptual framework for Buddhist thought. These four truths explain the nature of dukkha (suffering, anxiety, unsatisfactoriness), its causes, and how it can be overcome. The four truths are:[note 4]
The first truth explains the nature of dukkha. Dukkha is commonly translated as "suffering", "anxiety", "unsatisfactoriness", "unease", etc., and it is said to have the following three aspects:
The second truth is that the origin of dukkha can be known. Within the context of the four noble truths, the origin of dukkha is commonly explained as craving (Pali: tanha) conditioned by ignorance (Pali: avijja). On a deeper level, the root cause of dukkha is identified as ignorance (Pali: avijja) of the true nature of things. The third noble truth is that the complete cessation of dukkha is possible, and the fourth noble truth identifies a path to this cessation.[note 7]
The Noble Eightfold Path—the fourth of the Buddha's Noble Truths—consists of a set of eight interconnected factors or conditions, that when developed together, lead to the cessation of dukkha. These eight factors are: Right View (or Right Understanding), Right Intention (or Right Thought), Right Speech, Right Action, Right Livelihood, Right Effort, Right Mindfulness, and Right Concentration.
Ajahn Sucitto describes the path as "a mandala of interconnected factors that support and moderate each other." The eight factors of the path are not to be understood as stages, in which each stage is completed before moving on to the next. Rather, they are understood as eight significant dimensions of one's behaviour—mental, spoken, and bodily—that operate in dependence on one another; taken together, they define a complete path, or way of living.
While he searched for enlightenment, Gautama combined the yoga practice of his teacher Kalama with what later became known as "the immeasurables".[dubious – discuss] Gautama thus invented a new kind of human, one without egotism.[dubious – discuss] What Thich Nhat Hanh calls the "Four Immeasurable Minds" of love, compassion, joy, and equanimity[full citation needed] are also known as brahmaviharas, divine abodes, or simply as four immeasurables.[web 5] Pema Chödrön calls them the "four limitless ones". Of the four, mettā or loving-kindness meditation is perhaps the best known.[web 5] The Four Immeasurables are taught as a form of meditation that cultivates "wholesome attitudes towards all sentient beings."[web 6][web 7]
An important guiding principle of Buddhist practice is the Middle Way (or Middle Path), which is said to have been discovered by Gautama Buddha prior to his enlightenment. The Middle Way has several definitions:
Buddhist scholars have produced a number of intellectual theories, philosophies and world view concepts (see, for example, Abhidharma, Buddhist philosophy and Reality in Buddhism). Some schools of Buddhism discourage doctrinal study, and some regard it as essential practice.
The concept of liberation (nirvāṇa)—the goal of the Buddhist path—is closely related to overcoming ignorance (avidyā), a fundamental misunderstanding or mis-perception of the nature of reality. In awakening to the true nature of the self and all phenomena one develops dispassion for the objects of clinging, and is liberated from suffering (dukkha) and the cycle of incessant rebirths (saṃsāra). To this end, the Buddha recommended viewing things as characterized by the three marks of existence.
Impermanence (Pāli: anicca) expresses the Buddhist notion that all compounded or conditioned phenomena (all things and experiences) are inconstant, unsteady, and impermanent. Everything we can experience through our senses is made up of parts, and its existence is dependent on external conditions. Everything is in constant flux, and so conditions and the thing itself are constantly changing. Things are constantly coming into being, and ceasing to be. Since nothing lasts, there is no inherent or fixed nature to any object or experience. According to the doctrine of impermanence, life embodies this flux in the aging process, the cycle of rebirth (saṃsāra), and in any experience of loss. The doctrine asserts that because things are impermanent, attachment to them is futile and leads to suffering (dukkha).
Suffering (Pāli: दुक्ख dukkha; Sanskrit दुःख duḥkha) is also a central concept in Buddhism. The word roughly corresponds to a number of terms in English including suffering, pain, unsatisfactoriness, sorrow, affliction, anxiety, dissatisfaction, discomfort, anguish, stress, misery, and frustration. Although the term is often translated as "suffering", its philosophical meaning is more analogous to "disquietude" as in the condition of being disturbed. As such, "suffering" is too narrow a translation with "negative emotional connotations"[web 9] that can give the impression that the Buddhist view is pessimistic, but Buddhism seeks to be neither pessimistic nor optimistic, but realistic. In English-language Buddhist literature translated from Pāli, "dukkha" is often left untranslated, so as to encompass its full range of meaning.[note 8]
Not-self (Pāli: anatta; Sanskrit: anātman) is the third mark of existence. Upon careful examination, one finds that no phenomenon is really "I" or "mine"; these concepts are in fact constructed by the mind. In the Nikayas anatta is not meant as a metaphysical assertion, but as an approach for gaining release from suffering. In fact, the Buddha rejected both of the metaphysical assertions "I have a Self" and "I have no Self" as ontological views that bind one to suffering.[note 9] When asked if the self was identical with the body, the Buddha refused to answer. By analyzing the constantly changing physical and mental constituents (skandhas) of a person or object, the practitioner comes to the conclusion that neither the respective parts nor the person as a whole comprise a self.
The doctrine of pratītyasamutpāda, (Sanskrit; Pali: paticcasamuppāda; Tibetan Wylie: rten cing 'brel bar 'byung ba; Chinese: 緣起) is an important part of Buddhist metaphysics. It states that phenomena arise together in a mutually interdependent web of cause and effect. It is variously rendered into English as "dependent origination", "conditioned genesis", "dependent relationship", "dependent co-arising", "interdependent arising", or "contingency".
The best-known application of the concept of pratītyasamutpāda is the scheme of Twelve Nidānas (from Pāli "nidāna" meaning "cause, foundation, source or origin"), which explain the continuation of the cycle of suffering and rebirth (saṃsāra) in detail.[note 10]
The Twelve Nidānas describe a causal connection between the subsequent characteristics or conditions of cyclic existence, each one giving rise to the next:
Sentient beings always suffer throughout saṃsāra until they free themselves from this suffering (dukkha) by attaining Nirvana. Then the absence of the first Nidāna—ignorance—leads to the absence of the others.
Mahayana Buddhism received significant theoretical grounding from Nagarjuna (perhaps c. 150–250 CE), arguably the most influential scholar within the Mahayana tradition. Nagarjuna's primary contribution to Buddhist philosophy was the systematic exposition of the concept of śūnyatā, or "emptiness", widely attested in the Prajñāpāramitā sutras that emerged in his era. The concept of emptiness brings together other key Buddhist doctrines, particularly anatta and dependent origination, to refute the metaphysics of Sarvastivada and Sautrantika (extinct non-Mahayana schools). For Nagarjuna, it is not merely sentient beings that are empty of ātman; all phenomena (dharmas) are without any svabhava (literally "own-nature" or "self-nature"), and thus without any underlying essence; they are "empty" of being independent; thus the heterodox theories of svabhava circulating at the time were refuted on the basis of the doctrines of early Buddhism. Nagarjuna's school of thought is known as the Mādhyamaka. Some of the writings attributed to Nagarjuna made explicit references to Mahayana texts, but his philosophy was argued within the parameters set out by the agamas. He may have arrived at his positions from a desire to achieve a consistent exegesis of the Buddha's doctrine as recorded in the Canon. In the eyes of Nagarjuna the Buddha was not merely a forerunner, but the very founder of the Mādhyamaka system.
Sarvastivada teachings—which were criticized by Nāgārjuna—were reformulated by scholars such as Vasubandhu and Asanga and were adapted into the Yogacara school. While the Mādhyamaka school held that asserting the existence or non-existence of any ultimately real thing was inappropriate, some exponents of Yogacara asserted that the mind and only the mind is ultimately real (a doctrine known as cittamatra). Not all Yogacarins asserted that mind was truly existent; Vasubandhu and Asanga in particular did not.[web 11] These two schools of thought, in opposition or synthesis, form the basis of subsequent Mahayana metaphysics in the Indo-Tibetan tradition.
Besides emptiness, Mahayana schools often place emphasis on the notions of perfected spiritual insight (prajñāpāramitā) and Buddha-nature (tathāgatagarbha). There are conflicting interpretations of the tathāgatagarbha in Mahāyāna thought. The idea may be traced to Abhidharma, and ultimately to statements of the Buddha in the Nikāyas. In Tibetan Buddhism, according to the Sakya school, tathāgatagarbha is the inseparability of the clarity and emptiness of one's mind. In Nyingma, tathāgatagarbha also generally refers to inseparability of the clarity and emptiness of one's mind. According to the Gelug school, it is the potential for sentient beings to awaken since they are empty (i.e. dependently originated). According to the Jonang school, it refers to the innate qualities of the mind that expresses themselves as omniscience etc. when adventitious obscurations are removed. The "Tathāgatagarbha Sutras" are a collection of Mahayana sutras that present a unique model of Buddha-nature. Even though this collection was generally ignored in India, East Asian Buddhism provides some significance to these texts.
Nirvana (Sanskrit; Pali: "Nibbāna") means "cessation", "extinction" (of craving and ignorance and therefore suffering and the cycle of involuntary rebirths (saṃsāra)), "extinguished", "quieted", "calmed"; it is also known as "Awakening" or "Enlightenment" in the West. The term for anybody who has achieved nirvana, including the Buddha, is arahant.
Bodhi (Pāli and Sanskrit, in devanagari: बॊधि) is a term applied to the experience of Awakening of arahants. Bodhi literally means "awakening", but it is more commonly translated into English as "enlightenment". In Early Buddhism, bodhi carried a meaning synonymous to nirvana, using only some different metaphors to describe the experience, which implies the extinction of raga (greed, craving),[web 12] dosa (hate, aversion)[web 13] and moha (delusion).[web 14] In the later school of Mahayana Buddhism, the status of nirvana was downgraded in some scriptures, coming to refer only to the extinction of greed and hate, implying that delusion was still present in one who attained nirvana, and that one needed to attain bodhi to eradicate delusion:
Therefore, according to Mahayana Buddhism, the arahant has attained only nirvana, thus still being subject to delusion, while the bodhisattva not only achieves nirvana but full liberation from delusion as well. He thus attains bodhi and becomes a buddha. In Theravada Buddhism, bodhi and nirvana carry the same meaning as in the early texts, that of being freed from greed, hate and delusion.
The term parinirvana is also encountered in Buddhism, and this generally refers to the complete nirvana attained by the arahant at the moment of death, when the physical body expires.
According to Buddhist traditions a Buddha is a fully awakened being who has completely purified his mind of the three poisons of desire, aversion and ignorance. A Buddha is no longer bound by Samsara and has ended the suffering which unawakened people experience in life.
Buddhists do not consider Siddhartha Gautama to have been the only Buddha. The Pali Canon refers to many previous ones (see List of the 28 Buddhas), while the Mahayana tradition additionally has many Buddhas of celestial, rather than historical, origin (see Amitabha or Vairocana as examples, for lists of many thousands Buddha names see Taishō Shinshū Daizōkyō numbers 439–448). A common Theravada and Mahayana Buddhist belief is that the next Buddha will be one named Maitreya (Pali: Metteyya).
In Theravada doctrine, a person may awaken from the "sleep of ignorance" by directly realizing the true nature of reality; such people are called arahants and occasionally buddhas. After numerous lifetimes of spiritual striving, they have reached the end of the cycle of rebirth, no longer reincarnating as human, animal, ghost, or other being. The commentaries to the Pali Canon classify these awakened beings into three types:
Bodhi and nirvana carry the same meaning, that of being freed from craving, hate, and delusion. In attaining bodhi, the arahant has overcome these obstacles. As a further distinction, the extinction of only hatred and greed (in the sensory context) with some residue of delusion, is called anagami.
In the Mahayana, the Buddha tends not to be viewed as merely human, but as the earthly projection of a beginningless and endless, omnipresent being (see Dharmakaya) beyond the range and reach of thought. Moreover, in certain Mahayana sutras, the Buddha, Dharma and Sangha are viewed essentially as One: all three are seen as the eternal Buddha himself.
The Buddha's death is seen as an illusion, he is living on in other planes of existence, and monks are therefore permitted to offer "new truths" based on his input. Mahayana also differs from Theravada in its concept of śūnyatā (that ultimately nothing has existence), and in its belief in bodhisattvas (enlightened people who vow to continue being reborn until all beings can be enlightened).

The method of self-exertion or "self-power"—without reliance on an external force or being—stands in contrast to another major form of Buddhism, Pure Land, which is characterized by utmost trust in the salvific "other-power" of Amitabha Buddha. Pure Land Buddhism is a very widespread and perhaps the most faith-orientated manifestation of Buddhism and centres upon the conviction that faith in Amitabha Buddha and the chanting of homage to his name liberates one at death into the Blissful (安樂), Pure Land (淨土) of Amitabha Buddha. This Buddhic realm is variously construed as a foretaste of Nirvana, or as essentially Nirvana itself. The great vow of Amitabha Buddha to rescue all beings from samsaric suffering is viewed within Pure Land Buddhism as universally efficacious, if only one has faith in the power of that vow or chants his name.
Buddhists believe Gautama Buddha was the first to achieve enlightenment in this Buddha era and is therefore credited with the establishment of Buddhism. A Buddha era is the stretch of history during which people remember and practice the teachings of the earliest known Buddha. This Buddha era will end when all the knowledge, evidence and teachings of Gautama Buddha have vanished. This belief therefore maintains that many Buddha eras have started and ended throughout the course of human existence.[web 15][web 16] The Gautama Buddha, therefore, is the Buddha of this era, who taught directly or indirectly to all other Buddhas in it (see types of Buddhas).
In addition, Mahayana Buddhists believe there are innumerable other Buddhas in other universes. A Theravada commentary says that Buddhas arise one at a time in this world element, and not at all in others. The understandings of this matter reflect widely differing interpretations of basic terms, such as "world realm", between the various schools of Buddhism.
The idea of the decline and gradual disappearance of the teaching has been influential in East Asian Buddhism. Pure Land Buddhism holds that it has declined to the point where few are capable of following the path, so it may be best to rely on the power of Amitābha.
Bodhisattva means "enlightenment being", and generally refers to one who is on the path to buddhahood. Traditionally, a bodhisattva is anyone who, motivated by great compassion, has generated bodhicitta, which is a spontaneous wish to attain Buddhahood for the benefit of all sentient beings. Theravada Buddhism primarily uses the term in relation to Gautama Buddha's previous existences, but has traditionally acknowledged and respected the bodhisattva path as well.[web 17]
According to Jan Nattier, the term Mahāyāna "Great Vehicle" was originally even an honorary synonym for Bodhisattvayāna "Bodhisattva Vehicle." The Aṣṭasāhasrikā Prajñāpāramitā Sūtra, an early and important Mahayana text, contains a simple and brief definition for the term bodhisattva: "Because he has enlightenment as his aim, a bodhisattva-mahāsattva is so called."
Mahayana Buddhism encourages everyone to become bodhisattvas and to take the bodhisattva vow, where the practitioner promises to work for the complete enlightenment of all beings by practicing the six pāramitās. According to Mahayana teachings, these perfections are: dāna, śīla, kṣanti, vīrya, dhyāna, and prajñā.
A famous saying by the 8th-century Indian Buddhist scholar-saint Shantideva, which the 14th Dalai Lama often cites as his favourite verse, summarizes the Bodhisattva's intention (Bodhicitta) as follows: "For as long as space endures, and for as long as living beings remain, until then may I too abide to dispel the misery of the world."[citation needed]
Devotion is an important part of the practice of most Buddhists. Devotional practices include bowing, offerings, pilgrimage, and chanting. In Pure Land Buddhism, devotion to the Buddha Amitabha is the main practice. In Nichiren Buddhism, devotion to the Lotus Sutra is the main practice.
Buddhism traditionally incorporates states of meditative absorption (Pali: jhāna; Skt: dhyāna). The most ancient sustained expression of yogic ideas is found in the early sermons of the Buddha. One key innovative teaching of the Buddha was that meditative absorption must be combined with liberating cognition. The difference between the Buddha's teaching and the yoga presented in early Brahminic texts is striking. Meditative states alone are not an end, for according to the Buddha, even the highest meditative state is not liberating. Instead of attaining a complete cessation of thought, some sort of mental activity must take place: a liberating cognition, based on the practice of mindful awareness.
Meditation was an aspect of the practice of the yogis in the centuries preceding the Buddha. The Buddha built upon the yogis' concern with introspection and developed their meditative techniques, but rejected their theories of liberation. In Buddhism, mindfulness and clear awareness are to be developed at all times; in pre-Buddhist yogic practices there is no such injunction. A yogi in the Brahmanical tradition is not to practice while defecating, for example, while a Buddhist monastic should do so.
Religious knowledge or "vision" was indicated as a result of practice both within and outside of the Buddhist fold. According to the Samaññaphala Sutta, this sort of vision arose for the Buddhist adept as a result of the perfection of "meditation" coupled with the perfection of "discipline" (Pali sīla; Skt. śīla). Some of the Buddha's meditative techniques were shared with other traditions of his day, but the idea that ethics are causally related to the attainment of "transcendent wisdom" (Pali paññā; Skt. prajñā) was original.[web 18]
The Buddhist texts are probably the earliest describing meditation techniques. They describe meditative practices and states that existed before the Buddha as well as those first developed within Buddhism. Two Upanishads written after the rise of Buddhism do contain full-fledged descriptions of yoga as a means to liberation.
While there is no convincing evidence for meditation in pre-Buddhist early Brahminic texts, Wynne argues that formless meditation originated in the Brahminic or Shramanic tradition, based on strong parallels between Upanishadic cosmological statements and the meditative goals of the two teachers of the Buddha as recorded in the early Buddhist texts. He mentions less likely possibilities as well. Having argued that the cosmological statements in the Upanishads also reflect a contemplative tradition, he argues that the Nasadiya Sukta contains evidence for a contemplative tradition, even as early as the late Rig Vedic period.
Traditionally, the first step in most Buddhist schools requires taking refuge in the Three Jewels (Sanskrit: tri-ratna, Pāli: ti-ratana)[web 19] as the foundation of one's religious practice. The practice of taking refuge on behalf of young or even unborn children is mentioned in the Majjhima Nikaya, recognized by most scholars as an early text (cf. Infant baptism). Tibetan Buddhism sometimes adds a fourth refuge, in the lama. In Mahayana, the person who chooses the bodhisattva path makes a vow or pledge, considered the ultimate expression of compassion. In Mahayana, too, the Three Jewels are perceived as possessed of an eternal and unchanging essence and as having an irreversible effect: "The Three Jewels have the quality of excellence. Just as real jewels never change their faculty and goodness, whether praised or reviled, so are the Three Jewels (Refuges), because they have an eternal and immutable essence. These Three Jewels bring a fruition that is changeless, for once one has reached Buddhahood, there is no possibility of falling back to suffering.
According to the scriptures, Gautama Buddha presented himself as a model. The Dharma offers a refuge by providing guidelines for the alleviation of suffering and the attainment of Nirvana. The Sangha is considered to provide a refuge by preserving the authentic teachings of the Buddha and providing further examples that the truth of the Buddha's teachings is attainable.
Śīla (Sanskrit) or sīla (Pāli) is usually translated into English as "virtuous behavior", "morality", "moral discipline", "ethics" or "precept". It is an action committed through the body, speech, or mind, and involves an intentional effort. It is one of the three practices (sīla, samādhi, and paññā) and the second pāramitā. It refers to moral purity of thought, word, and deed. The four conditions of śīla are chastity, calmness, quiet, and extinguishment.
Śīla is the foundation of Samādhi/Bhāvana (Meditative cultivation) or mind cultivation. Keeping the precepts promotes not only the peace of mind of the cultivator, which is internal, but also peace in the community, which is external. According to the Law of Karma, keeping the precepts is meritorious and it acts as causes that would bring about peaceful and happy effects. Keeping these precepts keeps the cultivator from rebirth in the four woeful realms of existence.
Śīla refers to overall principles of ethical behavior. There are several levels of sīla, which correspond to "basic morality" (five precepts), "basic morality with asceticism" (eight precepts), "novice monkhood" (ten precepts) and "monkhood" (Vinaya or Patimokkha). Lay people generally undertake to live by the five precepts, which are common to all Buddhist schools. If they wish, they can choose to undertake the eight precepts, which add basic asceticism.
The precepts are not formulated as imperatives, but as training rules that laypeople undertake voluntarily to facilitate practice. In Buddhist thought, the cultivation of dana and ethical conduct themselves refine consciousness to such a level that rebirth in one of the lower heavens is likely, even if there is no further Buddhist practice. There is nothing improper or un-Buddhist about limiting one's aims to this level of attainment.
In the eight precepts, the third precept on sexual misconduct is made more strict, and becomes a precept of celibacy. The three additional precepts are:
The complete list of ten precepts may be observed by laypeople for short periods. For the complete list, the seventh precept is partitioned into two, and a tenth added:
Vinaya is the specific moral code for monks and nuns. It includes the Patimokkha, a set of 227 rules for monks in the Theravadin recension. The precise content of the vinayapitaka (scriptures on Vinaya) differs slightly according to different schools, and different schools or subschools set different standards for the degree of adherence to Vinaya. Novice-monks use the ten precepts, which are the basic precepts for monastics.
Regarding the monastic rules, the Buddha constantly reminds his hearers that it is the spirit that counts. On the other hand, the rules themselves are designed to assure a satisfying life, and provide a perfect springboard for the higher attainments. Monastics are instructed by the Buddha to live as "islands unto themselves". In this sense, living life as the vinaya prescribes it is, as one scholar puts it: "more than merely a means to an end: it is very nearly the end in itself."
In Eastern Buddhism, there is also a distinctive Vinaya and ethics contained within the Mahayana Brahmajala Sutra (not to be confused with the Pali text of that name) for Bodhisattvas, where, for example, the eating of meat is frowned upon and vegetarianism is actively encouraged (see vegetarianism in Buddhism). In Japan, this has almost completely displaced the monastic vinaya, and allows clergy to marry.
Buddhist meditation is fundamentally concerned with two themes: transforming the mind and using it to explore itself and other phenomena. According to Theravada Buddhism the Buddha taught two types of meditation, samatha meditation (Sanskrit: śamatha) and vipassanā meditation (Sanskrit: vipaśyanā). In Chinese Buddhism, these exist (translated chih kuan), but Chán (Zen) meditation is more popular. According to Peter Harvey, whenever Buddhism has been healthy, not only monks, nuns, and married lamas, but also more committed lay people have practiced meditation. According to Routledge's Encyclopedia of Buddhism, in contrast, throughout most of Buddhist history before modern times, serious meditation by lay people has been unusual. The evidence of the early texts suggests that at the time of the Buddha, many male and female lay practitioners did practice meditation, some even to the point of proficiency in all eight jhānas (see the next section regarding these).[note 11]
In the language of the Noble Eightfold Path, samyaksamādhi is "right concentration". The primary means of cultivating samādhi is meditation. Upon development of samādhi, one's mind becomes purified of defilement, calm, tranquil, and luminous.
Once the meditator achieves a strong and powerful concentration (jhāna, Sanskrit ध्यान dhyāna), his mind is ready to penetrate and gain insight (vipassanā) into the ultimate nature of reality, eventually obtaining release from all suffering. The cultivation of mindfulness is essential to mental concentration, which is needed to achieve insight.
Samatha meditation starts from being mindful of an object or idea, which is expanded to one's body, mind and entire surroundings, leading to a state of total concentration and tranquility (jhāna). There are many variations in the style of meditation, from sitting cross-legged or kneeling to chanting or walking. The most common method of meditation is to concentrate on one's breath (anapanasati), because this practice can lead to both samatha and vipassana'.
In Buddhist practice, it is said that while samatha meditation can calm the mind, only vipassanā meditation can reveal how the mind was disturbed to start with, which is what leads to insight knowledge (jñāna; Pāli ñāṇa) and understanding (prajñā Pāli paññā), and thus can lead to nirvāṇa (Pāli nibbāna). When one is in jhana, all defilements are suppressed temporarily. Only understanding (prajñā or vipassana) eradicates the defilements completely. Jhanas are also states that Arahants abide in order to rest.
In Theravāda Buddhism, the cause of human existence and suffering is identified as craving, which carries with it the various defilements. These various defilements are traditionally summed up as greed, hatred and delusion. These are believed deeply rooted afflictions of the mind that create suffering and stress. To be free from suffering and stress, these defilements must be permanently uprooted through internal investigation, analyzing, experiencing, and understanding of the true nature of those defilements by using jhāna, a technique of the Noble Eightfold Path. It then leads the meditator to realize the Four Noble Truths, Enlightenment and Nibbāna. Nibbāna is the ultimate goal of Theravadins.
Prajñā (Sanskrit) or paññā (Pāli) means wisdom that is based on a realization of dependent origination, The Four Noble Truths and the three marks of existence. Prajñā is the wisdom that is able to extinguish afflictions and bring about bodhi. It is spoken of as the principal means of attaining nirvāṇa, through its revelation of the true nature of all things as dukkha (unsatisfactoriness), anicca (impermanence) and anatta (not-self). Prajñā is also listed as the sixth of the six pāramitās of the Mahayana.
Initially, prajñā is attained at a conceptual level by means of listening to sermons (dharma talks), reading, studying, and sometimes reciting Buddhist texts and engaging in discourse. Once the conceptual understanding is attained, it is applied to daily life so that each Buddhist can verify the truth of the Buddha's teaching at a practical level. Notably, one could in theory attain Nirvana at any point of practice, whether deep in meditation, listening to a sermon, conducting the business of one's daily life, or any other activity.
Zen Buddhism (禅), pronounced Chán in Chinese, seon in Korean or zen in Japanese (derived from the Sanskrit term dhyāna, meaning "meditation") is a form of Buddhism that became popular in China, Korea and Japan and that lays special emphasis on meditation.[note 12] Zen places less emphasis on scriptures than some other forms of Buddhism and prefers to focus on direct spiritual breakthroughs to truth.
Zen Buddhism is divided into two main schools: Rinzai (臨済宗) and Sōtō (曹洞宗), the former greatly favouring the use in meditation on the koan (公案, a meditative riddle or puzzle) as a device for spiritual break-through, and the latter (while certainly employing koans) focusing more on shikantaza or "just sitting".[note 13]
Zen Buddhist teaching is often full of paradox, in order to loosen the grip of the ego and to facilitate the penetration into the realm of the True Self or Formless Self, which is equated with the Buddha himself.[note 14] According to Zen master Kosho Uchiyama, when thoughts and fixation on the little "I" are transcended, an Awakening to a universal, non-dual Self occurs: "When we let go of thoughts and wake up to the reality of life that is working beyond them, we discover the Self that is living universal non-dual life (before the separation into two) that pervades all living creatures and all existence." Thinking and thought must therefore not be allowed to confine and bind one.
Though based upon Mahayana, Tibeto-Mongolian Buddhism is one of the schools that practice Vajrayana or "Diamond Vehicle" (also referred to as Mantrayāna, Tantrayāna, Tantric Buddhism, or esoteric Buddhism). It accepts all the basic concepts of Mahāyāna, but also includes a vast array of spiritual and physical techniques designed to enhance Buddhist practice. Tantric Buddhism is largely concerned with ritual and meditative practices. One component of the Vajrayāna is harnessing psycho-physical energy through ritual, visualization, physical exercises, and meditation as a means of developing the mind. Using these techniques, it is claimed that a practitioner can achieve Buddhahood in one lifetime, or even as little as three years. In the Tibetan tradition, these practices can include sexual yoga, though only for some very advanced practitioners.
Historically, the roots of Buddhism lie in the religious thought of ancient India during the second half of the first millennium BCE. That was a period of social and religious turmoil, as there was significant discontent with the sacrifices and rituals of Vedic Brahmanism.[note 15] It was challenged by numerous new ascetic religious and philosophical groups and teachings that broke with the Brahmanic tradition and rejected the authority of the Vedas and the Brahmans.[note 16] These groups, whose members were known as shramanas, were a continuation of a non-Vedic strand of Indian thought distinct from Indo-Aryan Brahmanism.[note 17] Scholars have reasons to believe that ideas such as samsara, karma (in the sense of the influence of morality on rebirth), and moksha originated in the shramanas, and were later adopted by Brahmin orthodoxy.[note 18][note 19][note 20][note 21][note 22][note 23]
This view is supported by a study of the region where these notions originated. Buddhism arose in Greater Magadha, which stretched from Sravasti, the capital of Kosala in the north-west, to Rajagrha in the south east. This land, to the east of aryavarta, the land of the Aryas, was recognized as non-Vedic. Other Vedic texts reveal a dislike of the people of Magadha, in all probability because the Magadhas at this time were not Brahmanised.[page needed] It was not until the 2nd or 3rd centuries BCE that the eastward spread of Brahmanism into Greater Magadha became significant. Ideas that developed in Greater Magadha prior to this were not subject to Vedic influence. These include rebirth and karmic retribution that appear in a number of movements in Greater Magadha, including Buddhism. These movements inherited notions of rebirth and karmic retribution from an earlier culture[page needed]
At the same time, these movements were influenced by, and in some respects continued, philosophical thought within the Vedic tradition as reflected e.g. in the Upanishads. These movements included, besides Buddhism, various skeptics (such as Sanjaya Belatthiputta), atomists (such as Pakudha Kaccayana), materialists (such as Ajita Kesakambali), antinomians (such as Purana Kassapa); the most important ones in the 5th century BCE were the Ajivikas, who emphasized the rule of fate, the Lokayata (materialists), the Ajnanas (agnostics) and the Jains, who stressed that the soul must be freed from matter. Many of these new movements shared the same conceptual vocabulary—atman ("Self"), buddha ("awakened one"), dhamma ("rule" or "law"), karma ("action"), nirvana ("extinguishing"), samsara ("eternal recurrence") and yoga ("spiritual practice").[note 24] The shramanas rejected the Veda, and the authority of the brahmans, who claimed they possessed revealed truths not knowable by any ordinary human means. Moreover, they declared that the entire Brahmanical system was fraudulent: a conspiracy of the brahmans to enrich themselves by charging exorbitant fees to perform bogus rites and give useless advice.
A particular criticism of the Buddha was Vedic animal sacrifice.[web 18] He also mocked the Vedic "hymn of the cosmic man". However, the Buddha was not anti-Vedic, and declared that the Veda in its true form was declared by "Kashyapa" to certain rishis, who by severe penances had acquired the power to see by divine eyes. He names the Vedic rishis, and declared that the original Veda of the rishis[note 25] was altered by a few Brahmins who introduced animal sacrifices. The Buddha says that it was on this alteration of the true Veda that he refused to pay respect to the Vedas of his time. However, he did not denounce the union with Brahman,[note 26] or the idea of the self uniting with the Self. At the same time, the traditional Hindu itself gradually underwent profound changes, transforming it into what is recognized as early Hinduism.
Information of the oldest teachings may be obtained by analysis of the oldest texts. One method to obtain information on the oldest core of Buddhism is to compare the oldest extant versions of the Theravadin Pali Canon and other texts.[note 27] The reliability of these sources, and the possibility to draw out a core of oldest teachings, is a matter of dispute.[page needed][page needed][page needed][page needed] According to Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.[note 28]
A core problem in the study of early Buddhism is the relation between dhyana and insight. Schmithausen, in his often-cited article On some Aspects of Descriptions or Theories of 'Liberating Insight' and 'Enlightenment' in Early Buddhism notes that the mention of the four noble truths as constituting "liberating insight", which is attained after mastering the Rupa Jhanas, is a later addition to texts such as Majjhima Nikaya 36.[page needed]
Bruce Matthews notes that there is no cohesive presentation of karma in the Sutta Pitaka, which may mean that the doctrine was incidental to the main perspective of early Buddhist soteriology. Schmithausen is a notable scholar who has questioned whether karma already played a role in the theory of rebirth of earliest Buddhism.[page needed][note 32] According to Vetter, "the Buddha at first sought "the deathless" (amata/amrta), which is concerned with the here and now. According to Vetter, only after this realization did he become acquainted with the doctrine of rebirth." Bronkhorst disagrees, and concludes that the Buddha "introduced a concept of karma that differed considerably from the commonly held views of his time." According to Bronkhorst, not physical and mental activities as such were seen as responsible for rebirth, but intentions and desire.
According to Tilmann Vetter, the core of earliest Buddhism is the practice of dhyāna. Bronkhorst agrees that dhyana was a Buddhist invention, whereas Norman notes that "the Buddha's way to release [...] was by means of meditative practices." Discriminating insight into transiency as a separate path to liberation was a later development.
According to the Mahāsaccakasutta,[note 33] from the fourth jhana the Buddha gained bodhi. Yet, it is not clear what he was awakened to.[page needed] "Liberating insight" is a later addition to this text, and reflects a later development and understanding in early Buddhism.[page needed][page needed] The mentioning of the four truths as constituting "liberating insight" introduces a logical problem, since the four truths depict a linear path of practice, the knowledge of which is in itself not depicted as being liberating.[note 34]
Although "Nibbāna" (Sanskrit: Nirvāna) is the common term for the desired goal of this practice, many other terms can be found throughout the Nikayas, which are not specified.[note 35]
According to Vetter, the description of the Buddhist path may initially have been as simple as the term "the middle way". In time, this short description was elaborated, resulting in the description of the eightfold path.
According to both Bronkhorst and Anderson, the four truths became a substitution for prajna, or "liberating insight", in the suttas in those texts where "liberating insight" was preceded by the four jhanas. According to Bronkhorst, the four truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of "liberating insight". Gotama's teachings may have been personal, "adjusted to the need of each person."
The three marks of existence may reflect Upanishadic or other influences. K.R. Norman supposes that the these terms were already in use at the Buddha's time, and were familiair to his hearers.
The history of Indian Buddhism may be divided into five periods: Early Buddhism (occasionally called Pre-sectarian Buddhism), Nikaya Buddhism or Sectarian Buddhism: The period of the Early Buddhist schools, Early Mahayana Buddhism, Later Mahayana Buddhism, and Esoteric Buddhism (also called Vajrayana Buddhism).
Pre-sectarian Buddhism is the earliest phase of Buddhism, recognized by nearly all scholars. Its main scriptures are the Vinaya Pitaka and the four principal Nikayas or Agamas. Certain basic teachings appear in many places throughout the early texts, so most scholars conclude that Gautama Buddha must have taught something similar to the Three marks of existence, the Five Aggregates, dependent origination, karma and rebirth, the Four Noble Truths, the Noble Eightfold Path, and nirvana. Some scholars disagree, and have proposed many other theories.
According to the scriptures, soon after the parinirvāṇa (from Sanskrit: "highest extinguishment") of Gautama Buddha, the first Buddhist council was held. As with any ancient Indian tradition, transmission of teaching was done orally. The primary purpose of the assembly was to collectively recite the teachings to ensure that no errors occurred in oral transmission. In the first council, Ānanda, a cousin of the Buddha and his personal attendant, was called upon to recite the discourses (sūtras, Pāli suttas) of the Buddha, and, according to some sources, the abhidhamma. Upāli, another disciple, recited the monastic rules (vinaya). Most scholars regard the traditional accounts of the council as greatly exaggerated if not entirely fictitious.[note 36]Richard Gombrich noted Sariputta led communal recitations of the Buddha's teaching for preservation in the Buddha's lifetime in Sangiti Sutta (Digha Nikaya #33), and something similar to the First Council must have taken place to compose Buddhist scriptures.
According to most scholars, at some period after the Second Council the Sangha began to break into separate factions.[note 37] The various accounts differ as to when the actual schisms occurred. According to the Dipavamsa of the Pāli tradition, they started immediately after the Second Council, the Puggalavada tradition places it in 137 AN, the Sarvastivada tradition of Vasumitra says it was in the time of Ashoka and the Mahasanghika tradition places it much later, nearly 100 BCE.
The root schism was between the Sthaviras and the Mahāsāṅghikas. The fortunate survival of accounts from both sides of the dispute reveals disparate traditions. The Sthavira group offers two quite distinct reasons for the schism. The Dipavamsa of the Theravāda says that the losing party in the Second Council dispute broke away in protest and formed the Mahasanghika. This contradicts the Mahasanghikas' own vinaya, which shows them as on the same, winning side. The Mahāsāṅghikas argued that the Sthaviras were trying to expand the vinaya and may also have challenged what they perceived were excessive claims or inhumanly high criteria for arhatship. Both parties, therefore, appealed to tradition.
The Sthaviras gave rise to several schools, one of which was the Theravāda school. Originally, these schisms were caused by disputes over vinaya, and monks following different schools of thought seem to have lived happily together in the same monasteries, but eventually, by about 100 CE if not earlier, schisms were being caused by doctrinal disagreements too.
Following (or leading up to) the schisms, each Saṅgha started to accumulate an Abhidharma, a detailed scholastic reworking of doctrinal material appearing in the Suttas, according to schematic classifications. These Abhidharma texts do not contain systematic philosophical treatises, but summaries or numerical lists. Scholars generally date these texts to around the 3rd century BCE, 100 to 200 years after the death of the Buddha. Therefore the seven Abhidharma works are generally claimed not to represent the words of the Buddha himself, but those of disciples and great scholars.[note 38] Every school had its own version of the Abhidharma, with different theories and different texts. The different Abhidharmas of the various schools did not agree with each other. Scholars disagree on whether the Mahasanghika school had an Abhidhamma Pitaka or not.[note 38]
Several scholars have suggested that the Prajñāpāramitā sūtras, which are among the earliest Mahāyāna sūtras, developed among the Mahāsāṃghika along the Kṛṣṇa River in the Āndhra region of South India.
The earliest Mahāyāna sūtras to include the very first versions of the Prajñāpāramitā genre, along with texts concerning Akṣobhya Buddha, which were probably written down in the 1st century BCE in the south of India. Guang Xing states, "Several scholars have suggested that the Prajñāpāramitā probably developed among the Mahāsāṃghikas in southern India, in the Āndhra country, on the Kṛṣṇa River." A.K. Warder believes that "the Mahāyāna originated in the south of India and almost certainly in the Āndhra country."
Anthony Barber and Sree Padma note that "historians of Buddhist thought have been aware for quite some time that such pivotally important Mahayana Buddhist thinkers as Nāgārjuna, Dignaga, Candrakīrti, Āryadeva, and Bhavaviveka, among many others, formulated their theories while living in Buddhist communities in Āndhra." They note that the ancient Buddhist sites in the lower Kṛṣṇa Valley, including Amaravati, Nāgārjunakoṇḍā and Jaggayyapeṭa "can be traced to at least the third century BCE, if not earlier." Akira Hirakawa notes the "evidence suggests that many Early Mahayana scriptures originated in South India."
There is no evidence that Mahāyāna ever referred to a separate formal school or sect of Buddhism, but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas. Initially it was known as Bodhisattvayāna (the "Vehicle of the Bodhisattvas"). Paul Williams has also noted that the Mahāyāna never had nor ever attempted to have a separate Vinaya or ordination lineage from the early schools of Buddhism, and therefore each bhikṣu or bhikṣuṇī adhering to the Mahāyāna formally belonged to an early school. This continues today with the Dharmaguptaka ordination lineage in East Asia, and the Mūlasarvāstivāda ordination lineage in Tibetan Buddhism. Therefore Mahāyāna was never a separate rival sect of the early schools. From Chinese monks visiting India, we now know that both Mahāyāna and non-Mahāyāna monks in India often lived in the same monasteries side by side.
Much of the early extant evidence for the origins of Mahāyāna comes from early Chinese translations of Mahāyāna texts. These Mahāyāna teachings were first propagated into China by Lokakṣema, the first translator of Mahāyāna sūtras into Chinese during the 2nd century CE.[note 39] Some scholars have traditionally considered the earliest Mahāyāna sūtras to include the very first versions of the Prajñāpāramitā series, along with texts concerning Akṣobhya Buddha, which were probably composed in the 1st century BCE in the south of India.[note 40]
During the period of Late Mahayana Buddhism, four major types of thought developed: Madhyamaka, Yogacara, Tathagatagarbha, and Buddhist Logic as the last and most recent. In India, the two main philosophical schools of the Mahayana were the Madhyamaka and the later Yogacara. According to Dan Lusthaus, Madhyamaka and Yogacara have a great deal in common, and the commonality stems from early Buddhism. There were no great Indian teachers associated with tathagatagarbha thought.
Buddhism may have spread only slowly in India until the time of the Mauryan emperor Ashoka, who was a public supporter of the religion. The support of Aśoka and his descendants led to the construction of more stūpas (Buddhist religious memorials) and to efforts to spread Buddhism throughout the enlarged Maurya empire and even into neighboring lands—particularly to the Iranian-speaking regions of Afghanistan and Central Asia, beyond the Mauryas' northwest border, and to the island of Sri Lanka south of India. These two missions, in opposite directions, would ultimately lead, in the first case to the spread of Buddhism into China, and in the second case, to the emergence of Theravāda Buddhism and its spread from Sri Lanka to the coastal lands of Southeast Asia.
This period marks the first known spread of Buddhism beyond India. According to the edicts of Aśoka, emissaries were sent to various countries west of India to spread Buddhism (Dharma), particularly in eastern provinces of the neighboring Seleucid Empire, and even farther to Hellenistic kingdoms of the Mediterranean. It is a matter of disagreement among scholars whether or not these emissaries were accompanied by Buddhist missionaries.
The gradual spread of Buddhism into adjacent areas meant that it came into contact with new ethnical groups. During this period Buddhism was exposed to a variety of influences, from Persian and Greek civilization, to changing trends in non-Buddhist Indian religions—themselves influenced by Buddhism. Striking examples of this syncretistic development can be seen in the emergence of Greek-speaking Buddhist monarchs in the Indo-Greek Kingdom, and in the development of the Greco-Buddhist art of Gandhāra. A Greek king, Menander, has even been immortalized in the Buddhist canon.
The Theravada school spread south from India in the 3rd century BCE, to Sri Lanka and Thailand and Burma and later also Indonesia. The Dharmagupta school spread (also in 3rd century BCE) north to Kashmir, Gandhara and Bactria (Afghanistan).
The Silk Road transmission of Buddhism to China is most commonly thought to have started in the late 2nd or the 1st century CE, though the literary sources are all open to question.[note 41] The first documented translation efforts by foreign Buddhist monks in China were in the 2nd century CE, probably as a consequence of the expansion of the Kushan Empire into the Chinese territory of the Tarim Basin.
In the 2nd century CE, Mahayana Sutras spread to China, and then to Korea and Japan, and were translated into Chinese. During the Indian period of Esoteric Buddhism (from the 8th century onwards), Buddhism spread from India to Tibet and Mongolia.
By the late Middle Ages, Buddhism had become virtually extinct in India, although it continued to exist in surrounding countries. It is now again gaining strength worldwide. China and India are now starting to fund Buddhist shrines in various Asian countries as they compete for influence in the region.[web 20]
Formal membership varies between communities, but basic lay adherence is often defined in terms of a traditional formula in which the practitioner takes refuge in The Three Jewels: the Buddha, the Dharma (the teachings of the Buddha), and the Sangha (the Buddhist community). At the present time, the teachings of all three branches of Buddhism have spread throughout the world, and Buddhist texts are increasingly translated into local languages. While in the West Buddhism is often seen as exotic and progressive, in the East it is regarded as familiar and traditional. Buddhists in Asia are frequently well organized and well funded. In countries such as Cambodia and Bhutan, it is recognized as the state religion and receives government support. Modern influences increasingly lead to new forms of Buddhism that significantly depart from traditional beliefs and practices.
A number of modern movements or tendencies in Buddhism emerged during the second half of the 20th Century, including the Dalit Buddhist movement (also sometimes called 'neo-Buddhism'), Engaged Buddhism, and the further development of various Western Buddhist traditions.
In the second half of the 20th Century a modern movement in Nichiren Buddhism: Soka Gakkai (Value Creation Society) emerged in Japan and spread further to other countries. Soka Gakkai International (SGI) is a lay Buddhist movement linking more than 12 million people around the world, and is currently described as "the most diverse" and "the largest lay Buddhist movement in the world".[web 21]
Buddhism is practiced by an estimated 488 million,[web 1] 495 million, or 535 million people as of the 2010s, representing 7% to 8% of the world's total population.
China is the country with the largest population of Buddhists, approximately 244 million or 18.2% of its total population.[web 1] They are mostly followers of Chinese schools of Mahayana, making this the largest body of Buddhist traditions. Mahayana, also practiced in broader East Asia, is followed by over half of world Buddhists.[web 1]
According to a demographic analysis reported by Peter Harvey (2013): Mahayana has 360 million adherents; Theravada has 150 million adherents; and Vajrayana has 18,2 million adherents. Seven million additional Buddhists are found outside of Asia.
According to Johnson and Grim (2013), Buddhism has grown from a total of 138 million adherents in 1910, of which 137 million were in Asia, to 495 million in 2010, of which 487 million are in Asia. According to them, there was a fast annual growth of Buddhism in Pakistan, Saudi Arabia, Lebanon and several Western European countries (1910–2010). More recently (2000–2010), the countries with highest growth rates are Qatar, the United Arab Emirates, Iran and some African countries.
Some scholars[note 44] use other schemes. Buddhists themselves have a variety of other schemes. Hinayana (literally "lesser vehicle") is used by Mahayana followers to name the family of early philosophical schools and traditions from which contemporary Theravada emerged, but as this term is rooted in the Mahayana viewpoint and can be considered derogatory, a variety of other terms are increasingly used instead, including Śrāvakayāna, Nikaya Buddhism, early Buddhist schools, sectarian Buddhism, conservative Buddhism, mainstream Buddhism and non-Mahayana Buddhism.
Not all traditions of Buddhism share the same philosophical outlook, or treat the same concepts as central. Each tradition, however, does have its own core concepts, and some comparisons can be drawn between them. For example, according to one Buddhist ecumenical organization,[web 23] several concepts common to both major Buddhist branches:
Theravada ("Doctrine of the Elders", or "Ancient Doctrine") is the oldest surviving Buddhist school. It is relatively conservative, and generally closest to early Buddhism. The name Theravāda comes from the ancestral Sthāvirīya, one of the early Buddhist schools, from which the Theravadins claim descent. After unsuccessfully trying to modify the Vinaya, a small group of "elderly members", i.e. sthaviras, broke away from the majority Mahāsāṃghika during the Second Buddhist council, giving rise to the Sthavira sect. Sinhalese Buddhist reformers in the late nineteenth and early twentieth centuries portrayed the Pali Canon as the original version of scripture. They also emphasized Theravada being rational and scientific.
Theravāda is primarily practiced today in Sri Lanka, Burma, Laos, Thailand, Cambodia as well as small portions of China, Vietnam, Malaysia and Bangladesh. It has a growing presence in the west.
Theravadin Buddhists believe that personal effort is required to realize rebirth. Monks follow the vinaya: meditating, teaching and serving their lay communities. Laypersons can perform good actions, producing merit.
Mahayana Buddhism flourished in India from the 5th century CE onwards, during the dynasty of the Guptas. Mahāyāna centres of learning were established, the most important one being the Nālandā University in north-eastern India.
Mahayana schools recognize all or part of the Mahayana Sutras. Some of these sutras became for Mahayanists a manifestation of the Buddha himself, and faith in and veneration of those texts are stated in some sutras (e.g. the Lotus Sutra and the Mahaparinirvana Sutra) to lay the foundations for the later attainment of Buddhahood itself.
Native Mahayana Buddhism is practiced today in China, Japan, Korea, Singapore, parts of Russia and most of Vietnam (also commonly referred to as "Eastern Buddhism"). The Buddhism practiced in Tibet, the Himalayan regions, and Mongolia is also Mahayana in origin, but is discussed below under the heading of Vajrayana (also commonly referred to as "Northern Buddhism"). There are a variety of strands in Eastern Buddhism, of which "the Pure Land school of Mahayana is the most widely practised today.". In most of this area however, they are fused into a single unified form of Buddhism. In Japan in particular, they form separate denominations with the five major ones being: Nichiren, peculiar to Japan; Pure Land; Shingon, a form of Vajrayana; Tendai, and Zen. In Korea, nearly all Buddhists belong to the Chogye school, which is officially Son (Zen), but with substantial elements from other traditions.
Various classes of Vajrayana literature developed as a result of royal courts sponsoring both Buddhism and Saivism. The Mañjusrimulakalpa, which later came to classified under Kriyatantra, states that mantras taught in the Saiva, Garuda and Vaisnava tantras will be effective if applied by Buddhists since they were all taught originally by Manjushri. The Guhyasiddhi of Padmavajra, a work associated with the Guhyasamaja tradition, prescribes acting as a Saiva guru and initiating members into Saiva Siddhanta scriptures and mandalas. The Samvara tantra texts adopted the pitha list from the Saiva text Tantrasadbhava, introducing a copying error where a deity was mistaken for a place.
Buddhist scriptures and other texts exist in great variety. Different schools of Buddhism place varying levels of value on learning the various texts. Some schools venerate certain texts as religious objects in themselves, while others take a more scholastic approach. Buddhist scriptures are mainly written in Pāli, Tibetan, Mongolian, and Chinese. Some texts still exist in Sanskrit and Buddhist Hybrid Sanskrit.
Unlike many religions, Buddhism has no single central text that is universally referred to by all traditions. However, some scholars have referred to the Vinaya Pitaka and the first four Nikayas of the Sutta Pitaka as the common core of all Buddhist traditions.[page needed] This could be considered misleading, as Mahāyāna considers these merely a preliminary, and not a core, teaching. The Tibetan Buddhists have not even translated most of the āgamas (though theoretically they recognize them) and they play no part in the religious life of either clergy or laity in China and Japan. Other scholars say there is no universally accepted common core. The size and complexity of the Buddhist canons have been seen by some (including Buddhist social reformer Babasaheb Ambedkar) as presenting barriers to the wider understanding of Buddhist philosophy.
Over the years, various attempts have been made to synthesize a single Buddhist text that can encompass all of the major principles of Buddhism. In the Theravada tradition, condensed 'study texts' were created that combined popular or influential scriptures into single volumes that could be studied by novice monks. Later in Sri Lanka, the Dhammapada was championed as a unifying scripture.
Dwight Goddard collected a sample of Buddhist scriptures, with the emphasis on Zen, along with other classics of Eastern philosophy, such as the Tao Te Ching, into his 'Buddhist Bible' in the 1920s. More recently, Dr. Babasaheb Ambedkar attempted to create a single, combined document of Buddhist principles in "The Buddha and His Dhamma". Other such efforts have persisted to present day, but currently there is no single text that represents all Buddhist traditions.
The Pāli Tipitaka, which means "three baskets", refers to the Vinaya Pitaka, the Sutta Pitaka, and the Abhidhamma Pitaka. The Vinaya Pitaka contains disciplinary rules for the Buddhist monks and nuns, as well as explanations of why and how these rules were instituted, supporting material, and doctrinal clarification. The Sutta Pitaka contains discourses ascribed to Gautama Buddha. The Abhidhamma Pitaka contains material often described as systematic expositions of the Gautama Buddha's teachings.
The Pāli Tipitaka is the only early Tipitaka (Sanskrit: Tripiṭaka) to survive intact in its original language, but a number of early schools had their own recensions of the Tipitaka featuring much of the same material. We have portions of the Tipitakas of the Sārvāstivāda, Dharmaguptaka, Sammitya, Mahāsaṅghika, Kāśyapīya, and Mahīśāsaka schools, most of which survive in Chinese translation only. According to some sources, some early schools of Buddhism had five or seven pitakas.
According to the scriptures, soon after the death of the Buddha, the first Buddhist council was held; a monk named Mahākāśyapa (Pāli: Mahākassapa) presided. The goal of the council was to record the Buddha's teachings. Upāli recited the vinaya. Ānanda, the Buddha's personal attendant, was called upon to recite the dhamma. These became the basis of the Tripitaka. However, this record was initially transmitted orally in form of chanting, and was committed to text in the last century BCE. Both the sūtras and the vinaya of every Buddhist school contain a wide variety of elements including discourses on the Dharma, commentaries on other teachings, cosmological and cosmogonical texts, stories of the Gautama Buddha's previous lives, and various other subjects.
Much of the material in the Canon is not specifically "Theravadin", but is instead the collection of teachings that this school preserved from the early, non-sectarian body of teachings. According to Peter Harvey, it contains material at odds with later Theravadin orthodoxy. He states: "The Theravadins, then, may have added texts to the Canon for some time, but they do not appear to have tampered with what they already had from an earlier period."
The Mahayana sutras are a very broad genre of Buddhist scriptures that the Mahayana Buddhist tradition holds are original teachings of the Buddha. Some adherents of Mahayana accept both the early teachings (including in this the Sarvastivada Abhidharma, which was criticized by Nagarjuna and is in fact opposed to early Buddhist thought) and the Mahayana sutras as authentic teachings of Gautama Buddha, and claim they were designed for different types of persons and different levels of spiritual understanding.
The Mahayana sutras often claim to articulate the Buddha's deeper, more advanced doctrines, reserved for those who follow the bodhisattva path. That path is explained as being built upon the motivation to liberate all living beings from unhappiness. Hence the name Mahāyāna (lit., the Great Vehicle).
According to Mahayana tradition, the Mahayana sutras were transmitted in secret, came from other Buddhas or Bodhisattvas, or were preserved in non-human worlds because human beings at the time could not understand them:
Approximately six hundred Mahayana sutras have survived in Sanskrit or in Chinese or Tibetan translations. In addition, East Asian Buddhism recognizes some sutras regarded by scholars as of Chinese rather than Indian origin.
Generally, scholars conclude that the Mahayana scriptures were composed from the 1st century CE onwards: "Large numbers of Mahayana sutras were being composed in the period between the beginning of the common era and the fifth century", five centuries after the historical Gautama Buddha. Some of these had their roots in other scriptures composed in the 1st century BCE. It was not until after the 5th century CE that the Mahayana sutras started to influence the behavior of mainstream Buddhists in India: "But outside of texts, at least in India, at exactly the same period, very different—in fact seemingly older—ideas and aspirations appear to be motivating actual behavior, and old and established Hinnayana groups appear to be the only ones that are patronized and supported." These texts were apparently not universally accepted among Indian Buddhists when they appeared; the pejorative label Hinayana was applied by Mahayana supporters to those who rejected the Mahayana sutras.
Only the Theravada school does not include the Mahayana scriptures in its canon. As the modern Theravada school is descended from a branch of Buddhism that diverged and established itself in Sri Lanka prior to the emergence of the Mahayana texts, debate exists as to whether the Theravada were historically included in the hinayana designation; in the modern era, this label is seen as derogatory, and is generally avoided.
Scholar Isabelle Onians asserts that although "the Mahāyāna ... very occasionally referred contemptuously to earlier Buddhism as the Hinayāna, the Inferior Way," "the preponderance of this name in the secondary literature is far out of proportion to occurrences in the Indian texts." She notes that the term Śrāvakayāna was "the more politically correct and much more usual" term used by Mahāyānists. Jonathan Silk has argued that the term "Hinayana" was used to refer to whomever one wanted to criticize on any given occasion, and did not refer to any definite grouping of Buddhists.
Buddhism provides many opportunities for comparative study with a diverse range of subjects. For example, Buddhism's emphasis on the Middle way not only provides a unique guideline for ethics but has also allowed Buddhism to peacefully coexist with various differing beliefs, customs and institutions in countries where it has resided throughout its history. Also, its moral and spiritual parallels with other systems of thought—for example, with various tenets of Christianity—have been subjects of close study. In addition, the Buddhist concept of dependent origination has been compared to modern scientific thought, as well as Western metaphysics.
There are differences of opinion on the question of whether or not Buddhism should be considered a religion. Many sources commonly refer to Buddhism as a religion. For example:
St. John's (/ˌseɪntˈdʒɒnz/, local /ˌseɪntˈdʒɑːnz/) is the capital and largest city in Newfoundland and Labrador, Canada. St. John's was incorporated as a city in 1888, yet is considered by some to be the oldest English-founded city in North America. It is located on the eastern tip of the Avalon Peninsula on the island of Newfoundland. With a population of 214,285 as of July 1, 2015, the St. John's Metropolitan Area is the second largest Census Metropolitan Area (CMA) in Atlantic Canada after Halifax and the 20th largest metropolitan area in Canada. It is one of the world's top ten oceanside destinations, according to National Geographic Magazine. Its name has been attributed to the feast day of John the Baptist, when John Cabot was believed to have sailed into the harbour in 1497, and also to a Basque fishing town with the same name.
St. John's is one of the oldest settlements in North America, with year-round settlement beginning sometime after 1630 and seasonal habitation long before that. It is not, however, the oldest surviving English settlement in North America or Canada, having been preceded by the Cuper's Cove colony at Cupids, founded in 1610, and the Bristol's Hope colony at Harbour Grace, founded in 1618. In fact, although English fishermen had begun setting up seasonal camps in Newfoundland in the 16th Century, they were expressly forbidden by the British government, at the urging of the West Country fishing industry, from establishing permanent settlements along the English controlled coast, hence the town of St. John's was not established as a permanent community until after the 1630s at the earliest. Other permanent English settlements in the Americas that predate St. John's include: St. George's, Bermuda (1612) and Jamestown, Virginia (1607).
Sebastian Cabot declares in a handwritten Latin text in his original 1545 map, that the St. John's earned its name when he and his father, the Venetian explorer John Cabot became the first Europeans to sail into the harbour, in the morning of 24 June 1494 (against British and French historians stating 1497), the feast day of Saint John the Baptist. However, the exact locations of Cabot's landfalls are disputed. A series of expeditions to St. John's by Portuguese from the Azores took place in the early 16th century, and by 1540 French, Spanish and Portuguese ships crossed the Atlantic annually to fish the waters off the Avalon Peninsula. In the Basque Country, it is a common belief that the name of St. John's was given by Basque fishermen because the bay of St. John's is very similar to the Bay of Pasaia in the Basque Country, where one of the fishing towns is also called St. John (in Spanish, San Juan, and in Basque, Donibane).
The earliest record of the location appears as São João on a Portuguese map by Pedro Reinel in 1519. When John Rut visited St. John's in 1527 he found Norman, Breton and Portuguese ships in the harbour. On 3 August 1527, Rut wrote a letter to King Henry on the findings of his voyage to North America; this was the first known letter sent from North America. St. Jehan is shown on Nicholas Desliens' world map of 1541 and San Joham is found in João Freire's Atlas of 1546. It was during this time that Water Street was first developed, making it the oldest street in North America.[dubious – discuss]
By 1620, the fishermen of England's West Country controlled most of Newfoundland's east coast. In 1627, William Payne, called St. John's "the principal prime and chief lot in all the whole country". The population grew slowly in the 17th century and St. John's was the largest settlement in Newfoundland when English naval officers began to take censuses around 1675. The population would grow in the summers with the arrival of migratory fishermen. In 1680, fishing ships (mostly from South Devon) set up fishing rooms at St. John's, bringing hundreds of Irish men into the port to operate inshore fishing boats.
The town's first significant defences were likely erected due to commercial interests, following the temporary seizure of St. John's by the Dutch admiral Michiel de Ruyter in June 1665. The inhabitants were able to fend off a second Dutch attack in 1673, when this time it was defended by Christopher Martin, an English merchant captain. Martin landed six cannons from his vessel, the Elias Andrews, and constructed an earthen breastwork and battery near chain Rock commanding the Narrows leading into the harbour. With only twenty-three men, the valiant Martin beat off an attack by three Dutch warships. The English government planned to expand these fortifications (Fort William) in around 1689, but actual construction didn't begin until after the French admiral Pierre Le Moyne d'Iberville captured and destroyed the town in the Avalon Peninsula Campaign (1696). When 1500 English reinforcements arrived in late 1697 they found nothing but rubble where the town and fortifications had stood.
St. John's was the starting point for the first non-stop transatlantic aircraft flight, by Alcock and Brown in a modified Vickers Vimy IV bomber, in June 1919, departing from Lester's Field in St. John's and ending in a bog near Clifden, Connemara, Ireland. In July 2005, the flight was duplicated by American aviator and adventurer Steve Fossett in a replica Vickers Vimy aircraft, with St. John's International Airport substituting for Lester's Field (now an urban and residential part of the city).
St. John's, and the province as a whole, was gravely affected in the 1990s by the collapse of the Northern cod fishery, which had been the driving force of the provincial economy for hundreds of years. After a decade of high unemployment rates and depopulation, the city's proximity to the Hibernia, Terra Nova and White Rose oil fields has led to an economic boom that has spurred population growth and commercial development. As a result, the St. John's area now accounts for about half of the province's economic output.
St. John's is located along the coast of the Atlantic Ocean, on the northeast of the Avalon Peninsula in southeast Newfoundland. The city covers an area of 446.04 square kilometres (172.22 sq mi) and is the most easterly city in North America, excluding Greenland; it is 295 miles (475 km) closer to London, England than it is to Edmonton, Alberta. The city of St. John's is located at a distance by air of 3,636 kilometres (2,259 mi) from Lorient, France which lies on a nearly precisely identical latitude across the Atlantic on the French western coast. The city is the largest in the province and the second largest in the Atlantic Provinces after Halifax, Nova Scotia. Its downtown area lies to the west and north of St. John's Harbour, and the rest of the city expands from the downtown to the north, south, east and west.
St. John's has a humid continental climate (Köppen Dfb), with lower seasonal variation than normal for the latitude, which is due to Gulf Stream moderation. However, despite this maritime moderation, average January high temperatures are actually slightly colder in St. John's than it is in Kelowna, British Columbia, which is an inland city that is near the more marine air of the Pacific, demonstrating the cold nature of Eastern Canada. Mean temperatures range from −4.9 °C (23.2 °F) in February to 16.1 °C (61.0 °F) in August, showing somewhat of a seasonal lag in the climate. The city is also one of the areas of the country most prone to tropical cyclone activity, as it is bordered by the Atlantic Ocean to the east, where tropical storms (and sometimes hurricanes) travel from the United States. The city is one of the rainiest in Canada outside of coastal British Columbia. This is partly due to its propensity for tropical storm activity as well as moist, Atlantic air frequently blowing ashore and creating precipitation.
Of major Canadian cities, St. John's is the foggiest (124 days), windiest (24.3 km/h (15.1 mph) average speed), and cloudiest (1,497 hours of sunshine). St. John's experiences milder temperatures during the winter season in comparison to other Canadian cities, and has the mildest winter for any Canadian city outside of British Columbia. Precipitation is frequent and often heavy, falling year round. On average, summer is the driest season, with only occasional thunderstorm activity, and the wettest months are from October to January, with December the wettest single month, with nearly 165 millimetres of precipitation on average. This winter precipitation maximum is quite unusual for humid continental climates, which most commonly have a late spring or early summer precipitation maximum (for example, most of the Midwestern U.S.). Most heavy precipitation events in St. John's are the product of intense mid-latitude storms migrating from the Northeastern U.S. and New England states, and these are most common and intense from October to March, bringing heavy precipitation (commonly 4 to 8 centimetres of rainfall equivalent in a single storm), and strong winds. In winter, two or more types of precipitation (rain, freezing rain, sleet and snow) can fall from passage of a single storm. Snowfall is heavy, averaging nearly 335 centimetres per winter season. However, winter storms can bring changing precipitation types. Heavy snow can transition to heavy rain, melting the snow cover, and possibly back to snow or ice (perhaps briefly) all in the same storm, resulting in little or no net snow accumulation. Snow cover in St. John's is variable, and especially early in the winter season, may be slow to develop, but can extend deeply into the spring months (March, April). The St. John's area is subject to freezing rain (called "silver thaws"), the worst of which paralyzed the city over a three-day period in April 1984.
Starting as a fishing outpost for European fishermen, St. John's consisted mostly of the homes of fishermen, sheds, storage shacks, and wharves constructed out of wood. Like many other cities of the time, as the Industrial Revolution took hold and new methods and materials for construction were introduced, the landscape changed as the city grew in width and height. The Great Fire of 1892 destroyed most of the downtown core, and most residential and other wood-frame buildings date from this period.
Often compared to San Francisco due to the hilly terrain and steep maze of residential streets, housing in St. John's is typically painted in bright colours. The city council has implemented strict heritage regulations in the downtown area, including restrictions on the height of buildings. These regulations have caused much controversy over the years. With the city experiencing an economic boom a lack of hotel rooms and office space has seen proposals put forward that do not meet the current height regulations. Heritage advocates argue that the current regulations should be enforced while others believe the regulations should be relaxed to encourage economic development.
To meet the need for more office space downtown without compromising the city's heritage, the city council amended heritage regulations, which originally restricted height to 15 metres in the area of land on Water Street between Bishop's Cove and Steer's Cove, to create the "Commercial Central Retail – West Zone". The new zone will allow for buildings of greater height. A 47-metre, 12-storey office building, which includes retail space and a parking garage, was the first building to be approved in this area.
As of the 2006 Census, there were 100,646 inhabitants in St. John's itself, 151,322 in the urban area and 181,113 in the St. John's Census Metropolitan Area (CMA). Thus, St. John's is Newfoundland and Labrador's largest city and Canada's 20th largest CMA. Apart from St. John's, the CMA includes 12 other communities: the city of Mount Pearl and the towns of Conception Bay South, Paradise, Portugal Cove-St. Philip's, Torbay, Logy Bay-Middle Cove-Outer Cove, Pouch Cove, Flatrock, Bay Bulls, Witless Bay, Petty Harbour-Maddox Cove and Bauline. The population of the CMA was 192,326 as of 1 July 2010.
Predominantly Christian, the population of St. John's was once divided along sectarian (Catholic/Protestant) lines. In recent years, this sectarianism has declined significantly, and is no longer a commonly acknowledged facet of life in St. John's. St. John's is the seat of the Roman Catholic Archbishop of St. John's, and the Anglican Bishop of Eastern Newfoundland and Labrador. All major Christian sects showed a decline from 2001–2011 with a large increase in those with no religion from 3.9% to 11.1%.
St. John's economy is connected to both its role as the provincial capital of Newfoundland and Labrador and to the ocean. The civil service which is supported by the federal, provincial and municipal governments has been the key to the expansion of the city's labour force and to the stability of its economy, which supports a sizable retail, service and business sector. The provincial government is the largest employer in the city, followed by Memorial University. With the collapse of the fishing industry in Newfoundland and Labrador in the 1990s, the role of the ocean is now tied to what lies beneath it – oil and gas – as opposed to what swims in or travels across it. The city is the centre of the oil and gas industry in Eastern Canada and is one of 19 World Energy Cities. ExxonMobil Canada is headquartered in St. John's and companies such as Chevron, Husky Energy, Suncor Energy and Statoil have major regional operations in the city. Three major offshore oil developments, Hibernia, Terra Nova and White Rose, are in production off the coast of the city and a fourth development, Hebron, is expected to be producing oil by 2017.
The economy has been growing quickly in recent years. In both 2010 and 2011, the metro area's gross domestic product (GDP) led 27 other metropolitan areas in the country, according to the Conference Board of Canada, recording growth of 6.6 per cent and 5.8 per cent respectively. At $52,000 the city's per capita GDP is the second highest out of all major Canadian cities. Economic forecasts suggest that the city will continue its strong economic growth in the coming years not only in the "oceanic" industries mentioned above, but also in tourism and new home construction as the population continues to grow. In May 2011, the city's unemployment rate fell to 5.6 per cent, the second lowest unemployment rate for a major city in Canada.
The LSPU Hall is home to the Resource Centre for the Arts. The "Hall" hosts a vibrant and diverse arts community and is regarded as the backbone of artistic infrastructure and development in the downtown. The careers of many well-known Newfoundland artists were launched there including Rick Mercer, Mary Walsh, Cathy Jones, Andy Jones and Greg Thomey. The St. John's Arts and Culture Centre houses an art gallery, libraries and a 1000-seat theatre, which is the city's major venue for entertainment productions.
Pippy Park is an urban park located in the east end of the city; with over 3,400 acres (14 km2) of land, it is one of Canada's largest urban parks. The park contains a range of recreational facilities including two golf courses, Newfoundland and Labrador's largest serviced campground, walking and skiing trails as well as protected habitat for many plants and animals. Pippy Park is also home to the Fluvarium, an environmental education centre which offers a cross section view of Nagle's Hill Brook.
Bannerman Park is a Victorian-style park located near the downtown. The park was officially opened in 1891 by Sir Alexander Bannerman, Governor of the Colony of Newfoundland who donated the land to create the park. Today the park contains a public swimming pool, playground, a baseball diamond and many large open grassy areas. Bannerman Park plays host to many festivals and sporting events, most notably the Newfoundland and Labrador Folk Festival and St. John's Peace-a-chord. The park is also the finishing location for the annual Tely 10 Mile Road Race.
Signal Hill is a hill which overlooks the city of St. John's. It is the location of Cabot Tower which was built in 1897 to commemorate the 400th anniversary of John Cabot's discovery of Newfoundland, and Queen Victoria's Diamond Jubilee. The first transatlantic wireless transmission was received here by Guglielmo Marconi on 12 December 1901. Today, Signal Hill is a National Historic Site of Canada and remains incredibly popular amongst tourists and locals alike; 97% of all tourists to St. John's visit Signal Hill. Amongst its popular attractions are the Signal Hill Tattoo, showcasing the Royal Newfoundland Regiment of foot, c. 1795, and the North Head Trail which grants an impressive view of the Atlantic Ocean and the surrounding coast.
The rugby union team The Rock is the Eastern Canadian entry in the Americas Rugby Championship. The Rock play their home games at Swilers Rugby Park, as did the Rugby Canada Super League champions for 2005 and 2006, the Newfoundland Rock. The city hosted a Rugby World Cup qualifying match between Canada and the USA on 12 August 2006, where the Canadians heavily defeated the USA 56–7 to qualify for the 2007 Rugby World Cup finals in France. The 2007 age-grade Rugby Canada National Championship Festival was held in the city.
St. John's served as the capital city of the Colony of Newfoundland and the Dominion of Newfoundland before Newfoundland became Canada's tenth province in 1949. The city now serves as the capital of Newfoundland and Labrador, therefore the provincial legislature is located in the city. The Confederation Building, located on Confederation Hill, is home to the House of Assembly along with the offices for the Members of the House of Assembly (MHAs) and Ministers. The city is represented by ten MHAs, four who are members of the governing Progressive Conservative Party, three that belong to the New Democratic Party (NDP), and three that belong to the Liberal Party. Lorraine Michael, leader of the NDP since 2006, represents the district of Signal Hill-Quidi Vidi.
St. John's has traditionally been one of the safest cities in Canada to live; however, in recent years crime in the city has steadily increased. While nationally crime decreased by 4% in 2009, the total crime rate in St. John's saw an increase of 4%. During this same time violent crime in the city decreased 6%, compared to a 1% decrease nationally. In 2010 the total crime severity index for the city was 101.9, an increase of 10% from 2009 and 19.2% above the national average. The violent crime severity index was 90.1, an increase of 29% from 2009 and 1.2% above the national average. St. John's had the seventh-highest metropolitan crime index and twelfth-highest metropolitan violent crime index in the country in 2010.
St. John's is served by St. John's International Airport (YYT), located 10 minutes northwest of the downtown core. In 2011, roughly 1,400,000 passengers travelled through the airport making it the second busiest airport in Atlantic Canada in passenger volume. Regular destinations include Halifax, Montreal, Ottawa, Toronto, as well as destinations throughout the province. International locations include Dublin, London, New York City, Saint Pierre and Miquelon, Glasgow and Varadero. Scheduled service providers include Air Canada, Air Canada Jazz, Air Saint-Pierre, Air Transat, United Airlines, Porter Airlines, Provincial Airlines, Sunwing Airlines and Westjet.
St. John's is the eastern terminus of the Trans-Canada Highway, one of the longest national highways in the world. The divided highway, also known as "Outer Ring Road" in the city, runs just outside the main part of the city, with exits to Pitts Memorial Drive, Topsail Road, Team Gushue Highway, Thorburn Road, Allandale Road, Portugal Cove Road and Torbay Road, providing relatively easy access to neighbourhoods served by those streets. Pitts Memorial Drive runs from Conception Bay South, through the city of Mount Pearl and into downtown St. John's, with interchanges for Goulds, Water Street and Hamilton Avenue-New Gower Street.
Metrobus Transit is responsible for public transit in the region. Metrobus has a total of 19 routes, 53 buses and an annual ridership of 3,014,073. Destinations include the Avalon Mall, The Village Shopping Centre, Memorial University, Academy Canada, the College of the North Atlantic, the Marine Institute, the Confederation Building, downtown, Stavanger Drive Business Park, Kelsey Drive, Goulds, Kilbride, Shea Heights, the four hospitals in the city as well as other important areas in St. John's and Mount Pearl.
St. John's is served by the Eastern School District, the largest school district in Newfoundland and Labrador by student population. There are currently 36 primary, elementary and secondary schools in the city of St. John's, including three private schools. St. John's also includes one school that is part of the province-wide Conseil Scolaire Francophone (CSF), the Francophone public school district. It also contains two private schools, St. Bonaventure's College and Lakecrest Independent. Atlantic Canada's largest university, Memorial University of Newfoundland (MUN), is located in St. John's. MUN provides comprehensive education and grants degrees in several fields and its historical strengths in engineering, business, geology, and medicine, make MUN one of the top comprehensive universities in Canada. The Fisheries and Marine Institute of Memorial University of Newfoundland (MI) or simply Marine Institute, is a post-secondary ocean and marine polytechnic located in St. John's and is affiliated with Memorial University of Newfoundland. MUN also offers the lowest tuition in Canada ($2,644, per Academic Year)
CJON-DT, known on air as "NTV", is an independent station. The station sublicenses entertainment programming from Global and news programming from CTV and Global, rather than purchasing primary broadcast rights. Rogers Cable has its provincial headquarters in St. John's, and their community channel Rogers TV airs local shows such as Out of the Fog and One Chef One Critic. CBC has its Newfoundland and Labrador headquarters in the city and their television station CBNT-DT broadcasts from University Avenue.
The city is home to 15 am and FM radio stations, two of which are French-language stations. St. John's is the only Canadian city served by radio stations whose call letters do not all begin with the letter C. The ITU prefix VO was assigned to the Dominion of Newfoundland before the province joined Canadian Confederation in 1949, and three AM stations kept their existing call letters. However, other commercial radio stations in St. John's which went to air after 1949 use the same range of prefixes (CF–CK) currently in use elsewhere in Canada, with the exception of VOCM-FM, which was permitted to adopt the VOCM callsign because of its corporate association with the AM station that already bore that callsign. VO also remains in use in amateur radio.
The first known European explorer to reach Bermuda was Spanish sea captain Juan de Bermúdez in 1503, after whom the islands are named. He claimed the apparently uninhabited islands for the Spanish Empire. Paying two visits to the archipelago, Bermúdez never landed on the islands, but did create a recognisable map of the archipelago. Shipwrecked Portuguese mariners are now thought to have been responsible for the 1543 inscription in Portuguese Rock (previously called Spanish Rock). Subsequent Spanish or other European parties are believed to have released pigs there, which had become feral and abundant on the island by the time European settlement began. In 1609, the English Virginia Company, which had established Jamestown in Virginia (a term originally applied to all of the North American continent) two years earlier, permanently settled Bermuda in the aftermath of a hurricane, when the crew and passengers of the Sea Venture steered the ship onto the surrounding reef to prevent its sinking, then landed ashore.
The island was administered as an extension of Virginia by the Company until 1614. Its spin-off, the Somers Isles Company, took over in 1615 and managed the colony until 1684. At that time, the company's charter was revoked, and the English Crown took over administration. The islands became a British colony following the 1707 unification of the parliaments of Scotland and England, which created the Kingdom of Great Britain. After 1949, when Newfoundland became part of Canada, Bermuda was automatically ranked as the oldest remaining British Overseas Territory. Since the return of Hong Kong to China in 1997, it is the most populous Territory. Its first capital, St. George's, was established in 1612 and is the oldest continuously inhabited English town in the New World.
Bermuda's economy is based on offshore insurance and reinsurance, and tourism, the two largest economic sectors. Bermuda had one of the world's highest GDP per capita for most of the 20th century and several years beyond. Recently, its economic status has been affected by the global recession. It has a subtropical climate. Bermuda is the northernmost point of the Bermuda Triangle, a region of sea in which, according to legend, a number of aircraft and surface vessels have disappeared under supposedly unexplained or mysterious circumstances. The island is in the hurricane belt and prone to severe weather. However, it is somewhat protected from the full force of a hurricane by the coral reef that surrounds the island.
Bermuda is a group of low-forming volcanoes located in the Atlantic Ocean, near the western edge of the Sargasso Sea, roughly 578 nautical miles (1,070 km (665 mi)) east-southeast of Cape Hatteras on the Outer Banks of North Carolina and about 594 nautical miles (1,100 km (684 mi)) southeast of Martha's Vineyard of Massachusetts. It is 898 nautical miles (1,664 km (1,034 mi)) northeast of Miami, Florida, and 667 nautical miles (1,236 km (768 mi)) from Cape Sable Island, in Nova Scotia, Canada. The islands lie due east of Fripp Island, South Carolina, west of Portugal and north of Puerto Rico.
The archipelago is formed by high points on the rim of the caldera of a submarine volcano that forms a seamount. The volcano is one part of a range that was formed as part of the same process that formed the floor of the Atlantic, and the Mid-Atlantic Ridge. The top of the seamount has gone through periods of complete submergence, during which its limestone cap was formed by marine organisms, and during the Ice Ages the entire caldera was above sea level, forming an island of approximately two hundred square miles.
Despite the small land mass, place names are repeated; there are, for example, two islands named Long Island, three bays named Long Bay (on Somerset, Main, and Cooper's islands), two Horseshoe Bays (one in Southampton, on the Main Island, the other at Morgan's Point, formerly Tucker's Island), there are two roads through cuttings called Khyber Pass (one in Warwick, the other in St. George's Parish), and St George's Town is located on St George's Island within St George's Parish (each known as St George's). There is a Hamilton Parish in addition to the City of Hamilton (which is in Pembroke Parish).
Bermuda's pink sand beaches and clear, cerulean blue ocean waters are popular with tourists. Many of Bermuda's hotels are located along the south shore of the island. In addition to its beaches, there are a number of sightseeing attractions. Historic St George's is a designated World Heritage Site. Scuba divers can explore numerous wrecks and coral reefs in relatively shallow water (typically 30–40 ft or 9–12 m in depth), with virtually unlimited visibility. Many nearby reefs are readily accessible from shore by snorkellers, especially at Church Bay.
The only indigenous mammals of Bermuda are five species of bats, all of which are also found in the eastern United States: Lasionycteris noctivagans, Lasiurus borealis, Lasiurus cinereus, Lasiurus seminolus and Perimyotis subflavus. Other commonly known fauna of Bermuda include its national bird, the Bermuda petrel or cahow. It was rediscovered in 1951 after having been thought extinct since the 1620s. It is important as an example of a Lazarus species. The government has a programme to protect it, including restoration of a habitat area. The Bermuda rock skink was long thought to have been the only indigenous land vertebrate of Bermuda, discounting the marine turtles that lay their eggs on its beaches. Recently through genetic DNA studies, scientists have discovered that a species of turtle, the diamondback terrapin, previously thought to have been introduced, pre-dated the arrival of humans in the archipelago. As this species spends most of its time in brackish ponds, some question whether it should be classified as a land vertebrate to compete with the skink's unique status.
The island experienced large-scale immigration over the 20th century, especially after the Second World War. Bermuda has a diverse population including both those with relatively deep roots in Bermuda extending back for centuries, and newer communities whose ancestry results from recent immigration, especially from Britain, North America, the West Indies, and the Portuguese Atlantic islands (especially the Azores), although these groups are steadily merging. About 46% of the population identified themselves with Bermudian ancestry in 2010, which was a decrease from the 51% who did so in the 2000 census. Those identifying with British ancestry dropped by 1% to 11% (although those born in Britain remain the largest non-native group at 3,942 persons). The number of people born in Canada declined by 13%. Those who reported West Indian ancestry were 13%. The number of people born in the West Indies actually increased by 538. A significant segment of the population is of Portuguese ancestry (10%), the result of immigration over the past 160 years, of whom 79% have residency status.
The deeper ancestral demography of Bermuda's population has been obscured by the ethnic homogenisation of the last four centuries. There is effectively no ethnic distinction between black and white Bermudians, other than those characterising recent immigrant communities. In the 17th century, this was not so. For the first hundred years of settlement, white Protestants of English heritage were the distinct majority, with white minorities of Irish (the native language of many of whom can be assumed to have been Gaelic) and Scots sent to Bermuda after the English invasions of their homelands that followed the English Civil War. Non-white minorities included Spanish-speaking, free (indentured) blacks from the West Indies, black chattel slaves primarily captured from Spanish and Portuguese ships by Bermudian privateers, and Native Americans, primarily from the Algonquian and other tribes of the Atlantic seaboard, but possibly from as far away as Mexico. By the 19th century, the white ethnically-English Bermudians had lost their numerical advantage. Despite the banning of the importation of Irish, and the repeated attempts to force free blacks to emigrate and the owners of black slaves to export them, the merging of the various minority groups, along with some of the white English, had resulted in a new demographic group, "coloured" (which term, in Bermuda, referred to anyone not wholly of European ancestry) Bermudians, gaining a slight majority. Any child born before or since then to one coloured and one white parent has been added to the coloured statistic. Most of those historically described as "coloured" are today described as "black", or "of African heritage", which obscures their non-African heritage (those previously described as "coloured" who were not of African ancestry had been very few, though the numbers of South Asians, particularly, is now growing. The number of persons born in Asian countries doubled between the 2000 and the 2010 censuses), blacks have remained in the majority, with new white immigration from Portugal, Britain and elsewhere countered by black immigration from the West Indies.
Bermuda's modern black population contains more than one demographic group. Although the number of residents born in Africa is very small, it has tripled between 2000 and 2010 (this group also includes non-blacks). The majority of blacks in Bermuda can be termed "Bermudian blacks", whose ancestry dates back centuries between the 17th century and the end of slavery in 1834, Bermuda's black population was self-sustaining, with its growth resulting largely from natural expansion. This contrasts to the enslaved blacks of the plantation colonies, who were subjected to conditions so harsh as to drop their birth rate below the death rate, and slaveholders in the United States and the West Indies found it necessary to continue importing more enslaved blacks from Africa until the end of slavery (the same had been true for the Native Americans that the Africans had replaced on the New World plantations). The indigenous populations of many West Indian islands, and much of the South-East of what is now the United States that had survived the 16th- and 17th-century epidemics of European-introduced diseases then became the victims of large-scale slave raiding, with much of the region completely depopulated. When the supply of indigenous slaves ran out, the slaveholders looked to Africa). The ancestry of Bermuda's black population is distinguished from that of the British West Indian black population in two ways: firstly, the higher degree of European and Native American admixture; secondly, the source of the African ancestry.
In the British West Indian islands (and also in the United States), the majority of enslaved blacks brought across the Atlantic came from West Africa (roughly between modern Senegal and Ghana). Very little of Bermuda's original black emigration came from this area. The first blacks to arrive in Bermuda in any numbers were free blacks from Spanish-speaking areas of the West Indies, and most of the remainder were recently enslaved Africans captured from the Spanish and Portuguese. As Spain and Portugal sourced most of their slaves from South-West Africa (the Portuguese through ports in modern-day Angola; the Spanish purchased most of their African slaves from Portuguese traders, and from Arabs whose slave trading was centred in Zanzibar). Genetic studies have consequently shown that the African ancestry of black Bermudians (other than those resulting from recent immigration from the British West Indian islands) is largely from the a band across southern Africa, from Angola to Mozambique, which is similar to what is revealed in Latin America, but distinctly different from the blacks of the West Indies and the United States.
Most of Bermuda's black population trace some of their ancestry to Native Americans, although awareness of this is largely limited to St David's Islanders and most who have such ancestry are unaware of it. During the colonial period, hundreds of Native Americans were shipped to Bermuda. The best-known examples were the Algonquian peoples who were exiled from the southern New England colonies and sold into slavery in the 17th century, notably in the aftermaths of the Pequot and King Philip's wars.
Bermuda's culture is a mixture of the various sources of its population: Native American, Spanish-Caribbean, English, Irish, and Scots cultures were evident in the 17th century, and became part of the dominant British culture. English is the primary and official language. Due to 160 years of immigration from Portuguese Atlantic islands (primarily the Azores, though also from Madeira and the Cape Verde Islands), a portion of the population also speaks Portuguese. There are strong British influences, together with Afro-Caribbean ones.
The first notable, and historically important, book credited to a Bermudian was The History of Mary Prince, a slave narrative by Mary Prince. It is thought to have contributed to the abolition of slavery in the British Empire. Ernest Graham Ingham, an expatriate author, published his books at the turn of the 19th and 20th centuries. In the 20th century, numerous books were written and published locally, though few were directed at a wider market than Bermuda. (The latter consisted primarily of scholarly works rather than creative writing). The novelist Brian Burland (1931– 2010) achieved a degree of success and acclaim internationally. More recently, Angela Barry has won critical recognition for her published fiction.
Bermuda watercolours painted by local artists are sold at various galleries. Hand-carved cedar sculptures are another speciality. One such 7 ft (2.1 m) sculpture, created by Bermudian sculptor Chesley Trott, is installed at the airport's baggage claim area. In 2010, his sculpture The Arrival was unveiled near the bay to commemorate the freeing of slaves from the American brig Enterprise in 1835. Local artwork may also be viewed at several galleries around the island. Alfred Birdsey was one of the more famous and talented watercolourists; his impressionistic landscapes of Hamilton, St George's and the surrounding sailboats, homes, and bays of Bermuda are world-renowned.
Bermuda was discovered in 1503 by Spanish explorer Juan de Bermúdez. It is mentioned in Legatio Babylonica, published in 1511 by historian Pedro Mártir de Anglería, and was also included on Spanish charts of that year. Both Spanish and Portuguese ships used the islands as a replenishment spot to take on fresh meat and water. Legends arose of spirits and devils, now thought to have stemmed from the calls of raucous birds (most likely the Bermuda petrel, or Cahow) and the loud noise heard at night from wild hogs. Combined with the frequent storm-wracked conditions and the dangerous reefs, the archipelago became known as the Isle of Devils. Neither Spain nor Portugal tried to settle it.
It established a colony at Jamestown, Virginia, in 1607. Two years later, a flotilla of seven ships left England under the Company's Admiral, Sir George Somers, and the new Governor of Jamestown, Sir Thomas Gates, with several hundred settlers, food and supplies to relieve the colony of Jamestown. Somers had previous experience sailing with both Sir Francis Drake and Sir Walter Raleigh. The flotilla was broken up by a storm. As the flagship, the Sea Venture, was taking on water, Somers drove it onto Bermuda's reef and gained the shores safely with smaller boats – all 150 passengers and a dog survived. (William Shakespeare's play The Tempest, in which the character Ariel refers to the "still-vex'd Bermoothes" (I.ii.229), is thought to have been inspired by William Strachey's account of this shipwreck.) They stayed 10 months, starting a new settlement and building two small ships to sail to Jamestown. The island was claimed for the English Crown, and the charter of the Virginia Company was later extended to include it.
In 1610, all but three of the survivors of the Sea Venture sailed on to Jamestown. Among them was John Rolfe, whose wife and child died and were buried in Bermuda. Later in Jamestown he married Pocahontas, a daughter of the powerful Powhatan, leader of a large confederation of about 30 Algonquian-speaking tribes in coastal Virginia. In 1612, the English began intentional settlement of Bermuda with the arrival of the ship Plough. St. George's was settled that year and designated as Bermuda's first capital. It is the oldest continually inhabited English town in the New World.
Because of its limited land area, Bermuda has had difficulty with over-population. In the first two centuries of settlement, it relied on steady human emigration to keep the population manageable.[citation needed] Before the American Revolution more than ten thousand Bermudians (over half of the total population through the years) gradually emigrated, primarily to the Southern United States. As Great Britain displaced Spain as the dominant European imperial power, it opened up more land for colonial development. A steady trickle of outward migration continued. With seafaring the only real industry in the early decades, by the end of the 18th century, at least a third of the island's manpower was at sea at any one time.
In the 17th century, the Somers Isles Company suppressed shipbuilding, as it needed Bermudians to farm to generate income from the land. Agricultural production met with limited success, however. The Bermuda cedar boxes used to ship tobacco to England were reportedly worth more than their contents.[citation needed] The colony of Virginia far surpassed Bermuda in both quality and quantity of tobacco produced. Bermudians began to turn to maritime trades relatively early in the 17th century, but the Somers Isles Company used all its authority to suppress turning away from agriculture. This interference led to the islanders demanding, and receiving, the revocation of the Company's charter in 1684, and the Company was dissolved.
The end of the war, however, was to cause profound change in Bermuda, though some of those changes would take decades to crystallise. Following the war, with the buildup of Naval and military forces in Bermuda, the primary leg of the Bermudian economy became defence infrastructure. Even after tourism began later in the 19th century, Bermuda remained, in the eyes of London, a base more than a colony. The Crown strengthened its political and economic ties to Bermuda, and the colony's independence on the world stage was diminished.
The war had removed Bermuda's primary trading partners, the American colonies, from the empire, and dealt a harsh blow to Bermuda's merchant shipping trade. This also suffered due to the deforestation of Bermuda, as well as the advent of metal ships and steam propulsion, for which it did not have raw materials. During the course of the following War of 1812, the primary market for Bermuda's salt disappeared as the Americans developed their own sources. Control of the Turks had passed to the Bahamas in 1819.
The most famous escapee was the Boer prisoner of war Captain Fritz Joubert Duquesne who was serving a life sentence for "conspiracy against the British government and on (the charge of) espionage.". On the night of 25 June 1902, Duquesne slipped out of his tent, worked his way over a barbed-wire fence, swam 1.5 miles (2.4 km) past patrol boats and bright spot lights, through storm-wracked, shark infested waters, using a distant lighthouse for navigation until he arrived ashore on the main island. From there he escaped to the port of St. George's and a week later, he stowed away on a boat heading to Baltimore, Maryland. He settled in the US and later became a spy for Germany in both World Wars. In 1942, Col. Duquesne was arrested by the FBI for leading the Duquesne Spy Ring, which still to this day the largest espionage case in the history of the United States.
After several failed attempts, in 1930 the first aeroplane reached Bermuda. A Stinson Detroiter seaplane flying from New York, it had to land twice in the ocean: once because of darkness and again to refuel. Navigation and weather forecasting improved in 1933 when the Royal Air Force (then responsible for providing equipment and personnel for the Royal Navy's Fleet Air Arm) established a station at the Royal Naval Dockyard to repair (and supply replacement) float planes for the fleet. In 1936 Luft Hansa began to experiment with seaplane flights from Berlin via the Azores with continuation to New York City.
In 1937, Imperial Airways and Pan American World Airways began operating scheduled flying-boat airline services from New York and Baltimore to Darrell's Island, Bermuda. In 1948, regularly scheduled commercial airline service by land-based aeroplanes began to Kindley Field (now L.F. Wade International Airport), helping tourism to reach its peak in the 1960s–1970s. By the end of the 1970s, international business had supplanted tourism as the dominant sector of Bermuda's economy (see Economy of Bermuda).
Executive authority in Bermuda is vested in the monarch and is exercised on her behalf by the Governor. The governor is appointed by the Queen on the advice of the British Government. The current governor is George Fergusson; he was sworn in on 23 May 2012. There is also a Deputy Governor (currently David Arkley JP). Defence and foreign affairs are carried out by the United Kingdom, which also retains responsibility to ensure good government. It must approve any changes to the Constitution of Bermuda. Bermuda is classified as a British Overseas Territory, but it is the oldest British colony. In 1620, a Royal Assent granted Bermuda limited self-governance; its Parliament is the fifth oldest in the world, behind the Parliament of the United Kingdom, the Tynwald of the Isle of Man, the Althing of Iceland, and Sejm of Poland. Of these, only Bermuda's and the Isle of Man's Tynwald have been in continuous existence since 1620.
The Constitution of Bermuda came into force on 1 June 1967; it was amended in 1989 and 2003. The head of government is the premier. A cabinet is nominated by the premier and appointed officially by the governor. The legislative branch consists of a bicameral parliament modelled on the Westminster system. The Senate is the upper house, consisting of 11 members appointed by the governor on the advice of the premier and the leader of the opposition. The House of Assembly, or lower house, has 36 members, elected by the eligible voting populace in secret ballot to represent geographically defined constituencies.
There are few accredited diplomats in Bermuda. The United States maintains the largest diplomatic mission in Bermuda, comprising both the United States Consulate and the US Customs and Border Protection Services at the L.F. Wade International Airport. The current US Consul General is Robert Settje, who took office in August 2012. The United States is Bermuda's largest trading partner (providing over 71% of total imports, 85% of tourist visitors, and an estimated $163 billion of US capital in the Bermuda insurance/re-insurance industry), and an estimated 5% of Bermuda residents are US citizens, representing 14% of all foreign-born persons. The American diplomatic presence is an important element in the Bermuda political landscape.
On 11 June 2009, four Uyghurs who had been held in the United States Guantánamo Bay detention camp, in Cuba, were transferred to Bermuda. The four men were among 22 Uyghurs who claimed to be refugees, who were captured in 2001 in Pakistan after fleeing the American aerial bombardment of Afghanistan. They were accused of training to assist the Taliban's military. They were cleared as safe for release from Guantánamo in 2005 or 2006, but US domestic law prohibited deporting them back to China, their country of citizenship, because the US government determined that China was likely to violate their human rights.
Homosexuality was decriminalised in Bermuda with the passage of the Stubbs Bill in May 1994. Legislation was introduced by Private Members Bill by PLP MP Wayne Furbert to amend the Human Rights Act of Bermuda to disallow Same Sex Marriage under the Act in February 2016. The OBA government simultaneously introduced a bill to permit Civil Unions. Both measures were in response to a decision by His Hon Mr. Justice Ian Kawaley, Chief Justice of Bermuda's earlier ruling that same sex spouses of Bermuda citizens could not be denied basic Human Rights.
This is a socio-economic bloc of nations in or near the Caribbean Sea. Other outlying member states include the Co-operative Republic of Guyana and the Republic of Suriname in South America, along with Belize in Central America. The Turks and Caicos Islands, an associate member of CARICOM, and the Commonwealth of The Bahamas, a full member of CARICOM, are in the Atlantic, but near to the Caribbean. Other nearby nations or territories, such as the United States, are not members (although the US Commonwealth of Puerto Rico has observer status, and the United States Virgin Islands announced in 2007 they would seek ties with CARICOM). Bermuda, at roughly a thousand miles from the Caribbean Sea, has little trade with, and little economically in common with, the region, and joined primarily to strengthen cultural links.
Bermuda was colonised by the English as an extension of Virginia and has long had close ties with the US Atlantic Seaboard and Canadian Maritimes as well as the UK. It had a history of African slavery, although Britain abolished it decades before the US. Since the 20th century, there has been considerable immigration to Bermuda from the West Indies, as well as continued immigration from Portuguese Atlantic islands. Unlike immigrants from British colonies in the West Indies, the latter immigrants have had greater difficulty in becoming permanent residents as they lacked British citizenship, mostly spoke no English, and required renewal of work permits to remain beyond an initial period. From the 1950s onwards, Bermuda relaxed its immigration laws, allowing increased immigration from Britain and Canada. Some Black politicians accused the government of using this device to counter the West Indian immigration of previous decades.
The PLP, the party in government when the decision to join CARICOM was made, has been dominated for decades by West Indians and their descendants. (The prominent roles of West Indians among Bermuda's black politicians and labour activists predated party politics in Bermuda, as exemplified by Dr. E. F. Gordon). The late PLP leader, Dame Lois Browne-Evans, and her Trinidadian-born husband, John Evans (who co-founded the West Indian Association of Bermuda in 1976), were prominent members of this group. They have emphasised Bermuda's cultural connections with the West Indies. Many Bermudians, both black and white, who lack family connections to the West Indies have objected to this emphasis.
Once known as "the Gibraltar of the West" and "Fortress Bermuda", Bermuda today is defended by forces of the British government. For the first two centuries of settlement, the most potent armed force operating from Bermuda was its merchant shipping fleet, which turned to privateering at every opportunity. The Bermuda government maintained a local militia. After the American Revolutionary War, Bermuda was established as the Western Atlantic headquarters of the Royal Navy. Once the Royal Navy established a base and dockyard defended by regular soldiers, however, the militias were disbanded following the War of 1812. At the end of the 19th century, the colony raised volunteer units to form a reserve for the military garrison.
In May 1940, the US requested base rights in Bermuda from the United Kingdom, but British Prime Minister Winston Churchill was initially unwilling to accede to the American request without getting something in return. In September 1940, as part of the Destroyers for Bases Agreement, the UK granted the US base rights in Bermuda. Bermuda and Newfoundland were not originally included in the agreement, but both were added to it, with no war material received by the UK in exchange. One of the terms of the agreement was that the airfield the US Army built would be used jointly by the US and the UK (which it was for the duration of the war, with RAF Transport Command relocating there from Darrell's Island in 1943).
Construction began in 1941 of two airbases consisting of 5.8 km2 (2.2 sq mi) of land, largely reclaimed from the sea. For many years, Bermuda's bases were used by US Air Force transport and refuelling aircraft and by US Navy aircraft patrolling the Atlantic for enemy submarines, first German and, later, Soviet. The principal installation, Kindley Air Force Base on the eastern coast, was transferred to the US Navy in 1970 and redesignated Naval Air Station Bermuda. As a naval air station, the base continued to host both transient and deployed USN and USAF aircraft, as well as transitioning or deployed Royal Air Force and Canadian Forces aircraft.
The original NAS Bermuda on the west side of the island, a seaplane base until the mid-1960s, was designated as the Naval Air Station Bermuda Annex. It provided optional anchorage and/or dockage facilities for transiting US Navy, US Coast Guard and NATO vessels, depending on size. An additional US Navy compound known as Naval Facility Bermuda (NAVFAC Bermuda), a SOSUS station, was located to the west of the Annex near a Canadian Forces communications facility. Although leased for 99 years, US forces withdrew in 1995, as part of the wave of base closures following the end of the Cold War.
Bermudians served in the British armed forces during both World War I and World War II. After the latter, Major-General Glyn Charles Anglim Gilbert, Bermuda's highest-ranking soldier, was instrumental in developing the Bermuda Regiment. A number of other Bermudians and their descendants had preceded him into senior ranks, including Bahamian-born Admiral Lord Gambier, and Bermudian-born Royal Marines Brigadier Harvey. When promoted to Brigadier at age 39, following his wounding at the Anzio landings, Harvey became the youngest-ever Royal Marine Brigadier. The Cenotaph in front of the Cabinet Building (in Hamilton) was erected in tribute to Bermuda's Great War dead (the tribute was later extended to Bermuda's Second World War dead) and is the site of the annual Remembrance Day commemoration.
In 1970 the country switched its currency from the Bermudian pound to the Bermudian dollar, which is pegged at par with the US dollar. US notes and coins are used interchangeably with Bermudian notes and coins within the islands for most practical purposes; however, banks levy an exchange rate fee for the purchase of US dollars with Bermudian dollars. Bermudian notes carry the image of Queen Elizabeth II. The Bermuda Monetary Authority is the issuing authority for all banknotes and coins, and regulates financial institutions. The Royal Naval Dockyard Museum holds a permanent exhibition of Bermuda notes and coins.
Bermuda is an offshore financial centre, which results from its minimal standards of business regulation/laws and direct taxation on personal or corporate income. It has one of the highest consumption taxes in the world and taxes all imports in lieu of an income tax system. Bermudas's consumption tax is equivalent to local income tax to local residents and funds government and infrastructure expenditures. The local tax system depends upon import duties, payroll taxes and consumption taxes. The legal system is derived from that of the United Kingdom, with recourse to English courts of final appeal. Foreign private individuals cannot easily open bank accounts or subscribe to mobile phone or internet services.
There are four hundred securities listed on the stock exchange, of which almost three hundred are offshore funds and alternative investment structures attracted by Bermuda's regulatory environment. The Exchange specialises in listing and trading of capital market instruments such as equities, debt issues, funds (including hedge fund structures) and depository receipt programmes. The BSX is a full member of the World Federation of Exchanges and is located in an OECD member nation. It also has Approved Stock Exchange status under Australia's Foreign Investment Fund (FIF) taxation rules and Designated Investment Exchange status by the UK's Financial Services Authority.
Many sports popular today were formalised by British Public schools and universities in the 19th century. These schools produced the civil servants and military and naval officers required to build and maintain the British empire, and team sports were considered a vital tool for training their students to think and act as part of a team. Former public schoolboys continued to pursue these activities, and founded organisations such as the Football Association (FA). Today's association of football with the working classes began in 1885 when the FA changed its rules to allow professional players.
The professionals soon displaced the amateur ex-Public schoolboys. Bermuda's role as the primary Royal Navy base in the Western Hemisphere, with an army garrison to match, ensured that the naval and military officers quickly introduced the newly formalised sports to Bermuda, including cricket, football, Rugby football, and even tennis and rowing (rowing did not adapt well from British rivers to the stormy Atlantic. The officers soon switched to sail racing, founding the Royal Bermuda Yacht Club). Once these sports reached Bermuda, they were eagerly adopted by Bermudians.
Bermuda's national cricket team participated in the Cricket World Cup 2007 in the West Indies. Their most famous player is a 130 kilograms (290 lb) police officer named Dwayne Leverock. But India defeated Bermuda and set a record of 413 runs in a One-Day International (ODI). Bermuda were knocked out of the World Cup. Also very well-known is David Hemp, a former captain of Glamorgan in English first class cricket. The annual "Cup Match" cricket tournament between rival parishes St George's in the east and Somerset in the west is the occasion for a popular national holiday. This tournament began in 1872 when Captain Moresby of the Royal Navy introduced the game to Bermuda, holding a match at Somerset to mark forty years since the unjust thraldom of slavery. The East End versus West End rivalry resulted from the locations of the St. George's Garrison (the original army headquarters in Bermuda) on Barrack Hill, St. George's, and the Royal Naval Dockyard at Ireland Island. Moresby founded the Somerset Cricket Club which plays the St. George's Cricket Club in this game (the membership of both clubs has long been mostly civilian).
At the 2004 Summer Olympics, Bermuda competed in sailing, athletics, swimming, diving, triathlon and equestrian events. In those Olympics, Bermuda's Katura Horton-Perinchief made history by becoming the first black female diver to compete in the Olympic Games. Bermuda has had one Olympic medallist, Clarence Hill, who won a bronze medal in boxing. Bermuda also competed in Men's Skeleton at the 2006 Winter Olympics in Turin, Italy. Patrick Singleton placed 19th, with a final time of 1:59.81. Jillian Teceira competed in the Beijing Olympics in 2008. It is tradition for Bermuda to march in the Opening Ceremony in Bermuda shorts, regardless of the summer or winter Olympic celebration. Bermuda also competes in the biennial Island Games, which it hosted in 2013.
Bermuda has developed a proud Rugby Union community. The Bermuda Rugby Union team won the 2011 Caribbean championships, defeating Guyana in the final. They previously beat The Bahamas and Mexico to take the crown. Rugby 7's is also played, with four rounds scheduled to take place in the 2011–2012 season. The Bermuda 7's team competed in the 2011 Las Vegas 7's, defeating the Mexican team. There are four clubs on the island: (1) Police (2) Mariners (3) Teachers (4) Renegades. There is a men's and women's competition–current league champions are Police (Men) (winning the title for the first time since the 1990s) and Renegades (women's). Games are currently played at Warwick Academy. Bermuda u/19 team won the 2010 Caribbean Championships.
Some definitions of southern Europe, also known as Mediterranean Europe, include the countries of the Iberian peninsula (Spain and Portugal), the Italian peninsula, southern France and Greece. Other definitions sometimes include the Balkan countries of southeast Europe, which are geographically in the southern part of Europe, but which have different historical, political, economic, and cultural backgrounds.
Different methods can be used to define southern Europe, including its political, economic, and cultural attributes. Southern Europe can also be defined by its natural features — its geography, climate, and flora.
Southern Europe's most emblematic climate is that of the Mediterranean climate, which has become a typically known characteristic of the area. The Mediterranean climate covers much of Portugal, Spain, Southeast France, Italy, Croatia, Albania, Montenegro, Greece, the Western and Southern coastal regions of Turkey as well as the Mediterranean islands. Those areas of Mediterranean climate present similar vegetations and landscapes throughout, including dry hills, small plains, pine forests and olive trees.
Cooler climates can be found in certain parts of Southern European countries, for example within the mountain ranges of Spain and Italy. Additionally, the north coast of Spain experiences a wetter Atlantic climate.
Southern Europe's flora is that of the Mediterranean Region, one of the phytochoria recognized by Armen Takhtajan. The Mediterranean and Submediterranean climate regions in Europe are found in much of Southern Europe, mainly in Southern Portugal, most of Spain, the southern coast of France, Italy, the Croatian coast, much of Bosnia, Montenegro, Albania, Macedonia, Greece, and the Mediterranean islands.
The period known as classical antiquity began with the rise of the city-states of Ancient Greece. Greek influence reached its zenith under the expansive empire of Alexander the Great, spreading throughout Asia.
The Roman Empire came to dominate the entire Mediterranean basin in a vast empire based on Roman law and Roman legions. It promoted trade, tolerance, and Greek culture. By 300 AD the Roman Empire was divided into the Western Roman Empire based in Rome, and the Eastern Roman Empire based in Constantinople. The attacks of the Germanic peoples of northern Europe led to the Fall of the Western Roman Empire in AD 476, a date which traditionally marks the end of the classical period and the start of the Middle Ages.
During the Middle Ages, the Eastern Roman Empire survived, though modern historians refer to this state as the Byzantine Empire. In Western Europe, Germanic peoples moved into positions of power in the remnants of the former Western Roman Empire and established kingdoms and empires of their own.
The period known as the Crusades, a series of religiously motivated military expeditions originally intended to bring the Levant back into Christian rule, began. Several Crusader states were founded in the eastern Mediterranean. These were all short-lived. The Crusaders would have a profound impact on many parts of Europe. Their Sack of Constantinople in 1204 brought an abrupt end to the Byzantine Empire. Though it would later be re-established, it would never recover its former glory. The Crusaders would establish trade routes that would develop into the Silk Road and open the way for the merchant republics of Genoa and Venice to become major economic powers. The Reconquista, a related movement, worked to reconquer Iberia for Christendom.
The Late Middle Ages represented a period of upheaval in Europe. The epidemic known as the Black Death and an associated famine caused demographic catastrophe in Europe as the population plummeted. Dynastic struggles and wars of conquest kept many of the states of Europe at war for much of the period. In the Balkans, the Ottoman Empire, a Turkish state originating in Anatolia, encroached steadily on former Byzantine lands, culminating in the Fall of Constantinople in 1453.
Beginning roughly in the 14th century in Florence, and later spreading through Europe with the development of the printing press, a Renaissance of knowledge challenged traditional doctrines in science and theology, with the Arabic texts and thought bringing about rediscovery of classical Greek and Roman knowledge.
The Reconquista of Portugal and Spain led to a series of oceanic explorations resulting in the Age of Discovery that established direct links with Africa, the Americas, and Asia, while religious wars continued to be fought in Europe, which ended in 1648 with the Peace of Westphalia. The Spanish crown maintained its hegemony in Europe and was the leading power on the continent until the signing of the Treaty of the Pyrenees, which ended a conflict between Spain and France that had begun during the Thirty Years' War. An unprecedented series of major wars and political revolutions took place around Europe and indeed the world in the period between 1610 and 1700. Observers at the time, and many historians since, have argued that wars caused the revolutions. Galileo Galilei, invented the telescope and the thermometer which allowed him to observe and describe the solar system. Leonardo da Vinci painted the most famous work in the world. Guglielmo Marconi invented the radio.
European overseas expansion led to the rise of colonial empires, producing the Columbian Exchange. The combination of resource inflows from the New World and the Industrial Revolution of Great Britain, allowed a new economy based on manufacturing instead of subsistence agriculture.
The period between 1815 and 1871 saw a large number of revolutionary attempts and independence wars. Balkan nations began to regain independence from the Ottoman Empire. Italy unified into a nation state. The capture of Rome in 1870 ended the Papal temporal power. Rivalry in a scramble for empires spread in what is known as The Age of Empire.
The outbreak of World War I in 1914 was precipitated by the rise of nationalism in Southeastern Europe as the Great Powers took up sides. The Allies defeated the Central Powers in 1918. During the Paris Peace Conference the Big Four imposed their terms in a series of treaties, especially the Treaty of Versailles.
The Nazi regime under Adolf Hitler came to power in 1933, and along with Mussolini's Italy sought to gain control of the continent by the Second World War. Following the Allied victory in the Second World War, Europe was divided by the Iron Curtain. The countries in Southeastern Europe were dominated by the Soviet Union and became communist states. The major non-communist Southern European countries joined a US-led military alliance (NATO) and formed the European Economic Community amongst themselves. The countries in the Soviet sphere of influence joined the military alliance known as the Warsaw Pact and the economic bloc called Comecon. Yugoslavia was neutal.
Italy became a major industrialized country again, due to its post-war economic miracle. The European Union (EU) involved the division of powers, with taxation, health and education handled by the nation states, while the EU had charge of market rules, competition, legal standards and environmentalism. The Soviet economic and political system collapsed, leading to the end of communism in the satellite countries in 1989, and the dissolution of the Soviet Union itself in 1991. As a consequence, Europe's integration deepened, the continent became depolarised, and the European Union expanded to subsequently include many of the formerly communist European countries – Romania and Bulgaria (2007) and Croatia (2013).
The most widely spoken family of languages in southern Europe are the Romance languages, the heirs of Latin, which have spread from the Italian peninsula, and are emblematic of Southwestern Europe. (See the Latin Arch.) By far the most common romance languages in Southern Europe are: Italian, which is spoken by over 50 million people in Italy, San Marino, and the Vatican; and Spanish, which is spoken by over 40 million people in Spain and Gibraltar. Other common romance languages include: Romanian, which is spoken in Romania and Moldova; Portuguese, which is spoken in Portugal; Catalan, which is spoken in eastern Spain; and Galician, which is spoken in northwestern Spain.
The Hellenic languages or Greek language are widely spoken in Greece and in the Greek part of Cyprus. Additionally, other varieties of Greek are spoken in small communities in parts of other European counties.
Several South Slavic languages are spoken by millions of people in Southern Europe. Serbian is spoken in Serbia, Bosnia, and Croatia; Bulgarian is spoken in Bulgaria; Croatian is spoken in Croatia and Bosnia; Bosnian is spoken in Bosnia; Slovene is spoken in Slovenia; and Macedonian is spoken in Macedonia.
English is used as a second language in parts of Southern Europe. As a primary language, however, English has only a small presence in Southern Europe, only in Gibraltar (alongside Spanish) and Malta (secondary to Maltese).
There are other language groupings in Southern Europe. Albanian is spoken in Albania, Kosovo, Macedoonia, and parts of Greece. Maltese is a Semitic language that is the official language of Malta. The Basque language is spoken in the Basque Country, a region in northern Spain and southwestern France.
The predominant religion is southern Europe is Christianity. Christianity spread throughout Southern Europe during the Roman Empire, and Christianity was adopted as the official religion of the Roman Empire in the year 380 AD. Due to the historical break of the Christian Church into the western half based in Rome and the eastern half based in Constantinople, different branches of Christianity are prodominent in different parts of Europe. Christians in the western half of Southern Europe — e.g., Portugal, Spain, Italy — are generally Roman Catholic. Christians in the eastern half of Southern Europe — e.g., Greece, Macedonia — are generally Greek Orthodox.
For its official works and publications, the United Nations Organization groups countries under a classification of regions. The assignment of countries or areas to specific groupings is for statistical convenience and does not imply any assumption regarding political or other affiliation of countries or territories by the United Nations. Southern Europe, as grouped for statistical convenience by the United Nations (the sub-regions according to the UN), includes following countries and territories:
European Travel Commission divides the European region on the basis of Tourism Decision Metrics (TDM) model. Countries which belong to the Southern/Mediterranean Europe are:
The szlachta ([ˈʂlaxta] ( listen), exonym: Nobility) was a legally privileged noble class with origins in the Kingdom of Poland. It gained considerable institutional privileges between 1333 and 1370 during the reign of King Casimir III the Great.:211 In 1413, following a series of tentative personal unions between the Grand Duchy of Lithuania and the Crown Kingdom of Poland, the existing Lithuanian nobility formally joined this class.:211 As the Polish-Lithuanian Commonwealth (1569–1795) evolved and expanded in territory, its membership grew to include the leaders of Ducal Prussia, Podolian and Ruthenian lands.
The origins of the szlachta are shrouded in obscurity and mystery and have been the subject of a variety of theories.:207 Traditionally, its members were owners of landed property, often in the form of "manor farms" or so-called folwarks. The nobility negotiated substantial and increasing political and legal privileges for itself throughout its entire history until the decline of the Polish Commonwealth in the late 18th century.
During the Partitions of Poland from 1772 to 1795, its members began to lose these legal privileges and social status. From that point until 1918, the legal status of the nobility was essentially dependent upon the policies of the three partitioning powers: the Russian Empire, the Kingdom of Prussia, and the Habsburg Monarchy. The legal privileges of the szlachta were legally abolished in the Second Polish Republic by the March Constitution of 1921.
The notion that all Polish nobles were social equals, regardless of their financial status or offices held, is enshrined in a traditional Polish saying:
The term szlachta is derived from the Old High German word slahta (modern German Geschlecht), which means "(noble) family", much as many other Polish words pertaining to the nobility derive from German words—e.g., the Polish "rycerz" ("knight", cognate of the German "Ritter") and the Polish "herb" ("coat of arms", from the German "Erbe", "heritage").
Poles of the 17th century assumed that "szlachta" came from the German "schlachten" ("to slaughter" or "to butcher"); also suggestive is the German "Schlacht" ("battle"). Early Polish historians thought the term may have derived from the name of the legendary proto-Polish chief, Lech, mentioned in Polish and Czech writings.
Some powerful Polish nobles were referred to as "magnates" (Polish singular: "magnat", plural: "magnaci") and "możny" ("magnate", "oligarch"; plural: "możni"); see Magnates of Poland and Lithuania.
The Polish term "szlachta" designated the formalized, hereditary noble class of Polish-Lithuanian Commonwealth. In official Latin documents of the old Commonwealth, hereditary szlachta are referred to as "nobilitas" and are indeed the equivalent in legal status of the English nobility.
Today the word szlachta in the Polish language simply translates to "nobility". In its broadest meaning, it can also denote some non-hereditary honorary knighthoods granted today by some European monarchs. Occasionally, 19th-century non-noble landowners were referred to as szlachta by courtesy or error, when they owned manorial estates though they were not noble by birth. In the narrow sense, szlachta denotes the old-Commonwealth nobility.
In the past, a certain misconception sometimes led to the mistranslation of "szlachta" as "gentry" rather than "nobility".:206 :xvi This mistaken practice began due to the economic status of some szlachta members being inferior to that of the nobility in other European countries (see also Estates of the Realm regarding wealth and nobility). The szlachta included those almost rich and powerful enough to be magnates down to rascals with a noble lineage, no land, no castle, no money, no village, and no peasants.:xvi
As some szlachta were poorer than some non-noble gentry, some particularly impoverished szlachta were forced to become tenants of the wealthier gentry. In doing so, however, these szlachta retained all their constitutional prerogatives, as it was not wealth or lifestyle (obtainable by the gentry), but hereditary juridical status, that determined nobility.
The origins of the szlachta, while ancient, have always been considered obscure.:207 As a result, its members often referred to it as odwieczna (perennial).:207 Two popular historic theories of origin forwarded by its members and earlier historians and chroniclers involved descent from the ancient Iranian tribes known as Sarmatians or from Japheth, one of Noah's sons (by contrast, the peasantry were said to be the offspring of another son of Noah, Ham—and hence subject to bondage under the Curse of Ham—and the Jews as the offspring of Shem). Other fanciful theories included its foundation by Julius Caesar, Alexander the Great:207 or regional leaders who had not mixed their bloodlines with those of 'slaves, prisoners, and aliens'.:208
Another theory describes its derivation from a non-Slavic warrior class,:42, 64–66 forming a distinct element known as the Lechici/Lekhi (Lechitów):430 :482 within the ancient Polonic tribal groupings (Indo-European caste systems). This hypothesis states this upper class was not of Slavonic extraction:482 and was of a different origin than the Slavonic peasants (kmiecie; Latin: cmethones):430 :118 over which they ruled.:482 The Szlachta were differentiated from the rural population. The nobleman's sense of distinction led to practices that in later periods would be characterized as racism.:233 The Szlachta were noble in the Aryan sense -- "noble" in contrast to the people over whom they ruled after coming into contact with them.:482 The szlachta traced their descent from Lech/Lekh, who probably founded the Polish kingdom in about the fifth century.:482 Lechia was the name of Poland in antiquity, and the szlachta's own name for themselves was Lechici/Lekhi.:482 An exact counterpart of Szlachta society was the Meerassee system of tenure of southern India—an aristocracy of equality—settled as conquerors among a separate race.:484 The Polish state paralleled the Roman Empire in that full rights of citizenship were limited to the szlachta. The szlachta were a caste, a military caste, as in Hindu society.
The documentation regarding Raciborz and Albert's tenure is the earliest surviving of the use of the clan name and cry defining the honorable status of Polish knights. The names of knightly genealogiae only came to be associated with heraldic devices later in the Middle Ages and in the early modern period. The Polish clan name and cry ritualized the ius militare, i.e., the power to command an army; and they had been used some time before 1244 to define knightly status. (Górecki 1992, pp. 183–185).
Around the 14th century, there was little difference between knights and the szlachta in Poland. Members of the szlachta had the personal obligation to defend the country (pospolite ruszenie), thereby becoming the kingdom's most privileged social class. Inclusion in the class was almost exclusively based on inheritance.
Concerning the early Polish tribes, geography contributed to long-standing traditions. The Polish tribes were internalized and organized around a unifying religious cult, governed by the wiec, an assembly of free tribesmen. Later, when safety required power to be consolidated, an elected prince was chosen to govern. The election privilege was usually limited to elites.
The tribes were ruled by clans (ród) consisting of people related by blood or marriage and theoretically descending from a common ancestor, giving the ród/clan a highly developed sense of solidarity. (See gens.) The starosta (or starszyna) had judicial and military power over the ród/clan, although this power was often exercised with an assembly of elders. Strongholds called grόd were built where the religious cult was powerful, where trials were conducted, and where clans gathered in the face of danger. The opole was the territory occupied by a single tribe. (Manteuffel 1982, p. 44)
Mieszko I of Poland (c. 935 – 25 May 992) established an elite knightly retinue from within his army, which he depended upon for success in uniting the Lekhitic tribes and preserving the unity of his state. Documented proof exists of Mieszko I's successors utilizing such a retinue, as well.
Another class of knights were granted land by the prince, allowing them the economic ability to serve the prince militarily. A Polish nobleman living at the time prior to the 15th century was referred to as a "rycerz", very roughly equivalent to the English "knight," the critical difference being the status of "rycerz" was almost strictly hereditary; the class of all such individuals was known as the "rycerstwo". Representing the wealthier families of Poland and itinerant knights from abroad seeking their fortunes, this other class of rycerstwo, which became the szlachta/nobility ("szlachta" becomes the proper term for Polish nobility beginning about the 15th century), gradually formed apart from Mieszko I's and his successors' elite retinues. This rycerstwo/nobility obtained more privileges granting them favored status. They were absolved from particular burdens and obligations under ducal law, resulting in the belief only rycerstwo (those combining military prowess with high/noble birth) could serve as officials in state administration.
The Period of Division from, A.D., 1138 – A.D., 1314, which included nearly 200 years of feudal fragmentation and which stemmed from Bolesław III's division of Poland among his sons, was the genesis of the social structure which saw the economic elevation of the great landowning feudal nobles (możni/Magnates, both ecclesiastical and lay) from the rycerstwo they originated from. The prior social structure was one of Polish tribes united into the historic Polish nation under a state ruled by the Piast dynasty, this dynasty appearing circa 850 A.D.
Some możni (Magnates) descending from past tribal dynasties regarded themselves as co-proprietors of Piast realms, even though the Piasts attempted to deprive them of their independence. These możni (Magnates) constantly sought to undermine princely authority.:75, 76 In Gall Anonym's chronicle, there is noted the nobility's alarm when the Palatine Sieciech "elevated those of a lower class over those who were noble born" entrusting them with state offices. (Manteuffel 1982, p. 149)
In Lithuania Propria and in Samogitia prior to the creation of the Kingdom of Lithuania by Mindaugas, nobles were named die beste leuten in sources that were written in German language. In the Lithuanian language nobles were named ponai. The higher nobility were named 'kunigai' or 'kunigaikščiai' (dukes)—i.e., loanword from Scandinavic konung. They were the established local leaders and warlords. During the development of the state they gradually became subordinated to higher dukes, and later to the King of Lithuania. Because of expansion of Lithuanian duchy into lands of Ruthenia in the mid of 14th century a new term appeared to denominate nobility bajorai—from Ruthenian (modern Ukrainian and Belarusian languages) бояре. This word to this day is used in Lithuanian language to name nobility, not only for own, but also for nobility of other countries.
After the Union of Horodło the Lithuanian nobility acquired equal status with the Polish szlachta, and over time began to become more and more polonized, although they did preserve their national consciousness, and in most cases recognition of their Lithuanian family roots. In the 16th century some of the Lithuanian nobility claimed that they were of Roman extraction, and the Lithuanian language was just a morphed Latin language. This led to paradox: Polish nobility claimed own ancestry from Sarmatian tribes, but Sarmatians were considered enemies to Romans. Thus new Roman-Sarmatian theory was created. Strong cultural ties with Polish nobility led that in the 16th century the new term to name Lithuanian nobility appeared šlėkta—a direct loanword from Polish szlachta. From the view of historical truth Lithuanians also should use this term, šlėkta (szlachta), to name own nobility, but Lithuanian linguists forbade the usage of this Polish loanword. This refusal to use word szlachta (in Lithuanian text šlėkta) complicates all naming.
The process of polonization took place over a lengthy period of time. At first only the highest members of the nobility were involved, although gradually a wider group of the population was affected. The major effects on the lesser Lithuanian nobility took place after various sanctions were imposed by the Russian Empire such as removing Lithuania from the names of the Gubernyas few years after the November Uprising. After the January Uprising the sanctions went further, and Russian officials announced that "Lithuanians are Russians seduced by Poles and Catholicism" and began to intensify russification, and to ban the printing of books in the Lithuanian language.
In Ruthenia the nobility gradually gravitated its loyalty towards the multicultural and multilingual Grand Duchy of Lithuania after the principalities of Halych and Volhynia became a part of it. Many noble Ruthenian families intermarried with Lithuanian ones.
The Orthodox nobles' rights were nominally equal to those enjoyed by Polish and Lithuanian nobility, but there was a cultural pressure to convert to Catholicism, that was greatly eased in 1596 by the Union of Brest. See for example careers of Senator Adam Kisiel and Jerzy Franciszek Kulczycki.
In the Kingdom of Poland and later in the Polish-Lithuanian Commonwealth, ennoblement (nobilitacja) may be equated with an individual given legal status as a szlachta (member of the Polish nobility). Initially, this privilege could be granted by monarch, but from the 1641 onward, this right was reserved for the sejm. Most often the individual being ennobled would join an existing noble szlachta clan and assume the undifferentiated coat of arms of that clan.
According to heraldic sources total number of legal ennoblements issued between the 14th century and the mid-18th century, is estimated at approximately 800. This is an average of only about two ennoblements per year or only 0.000 000 14 – 0.000 001 of historical population. Compare: historical demography of Poland.
According to heraldic sources 1,600 is a total estimated number of all legal ennoblements throughout the history of Kingdom of Poland and Polish-Lithuanian Commonwealth from the 14th century onward (half of which were performed in the final years of the late 18th century).
In the late 14th century, in the Grand Duchy of Lithuania, Vytautas the Great reformed the Grand Duchy's army: instead of calling all men to arms, he created forces comprising professional warriors—bajorai ("nobles"; see the cognate "boyar"). As there were not enough nobles, Vytautas trained suitable men, relieving them of labor on the land and of other duties; for their military service to the Grand Duke, they were granted land that was worked by hired men (veldams). The newly formed noble families generally took up, as their family names, the Lithuanian pagan given names of their ennobled ancestors; this was the case with the Goštautai, Radvilos, Astikai, Kęsgailos and others. These families were granted their coats of arms under the Union of Horodlo (1413).
Significant legislative changes in the status of the szlachta, as defined by Robert Bideleux and Ian Jeffries, consist of its 1374 exemption from the land tax, a 1425 guarantee against the 'arbitrary arrests and/or seizure of property' of its members, a 1454 requirement that military forces and new taxes be approved by provincial Sejms, and statutes issued between 1496 and 1611 that prescribed the rights of commoners.
Nobles were born into a noble family, adopted by a noble family (this was abolished in 1633) or ennobled by a king or Sejm for various reasons (bravery in combat, service to the state, etc.—yet this was the rarest means of gaining noble status). Many nobles were, in actuality, really usurpers, being commoners, who moved into another part of the country and falsely pretended to noble status. Hundreds of such false nobles were denounced by Hieronim Nekanda Trepka in his Liber generationis plebeanorium (or Liber chamorum) in the first half of the 16th century. The law forbade non-nobles from owning nobility-estates and promised the estate to the denouncer. Trepka was an impoverished nobleman who lived a townsman life and collected hundreds of such stories hoping to take over any of such estates. It does not seem he ever succeeded in proving one at the court. Many sejms issued decrees over the centuries in an attempt to resolve this issue, but with little success. It is unknown what percentage of the Polish nobility came from the 'lower' orders of society, but most historians agree that nobles of such base origins formed a 'significant' element of the szlachta.
The Polish nobility enjoyed many rights that were not available to the noble classes of other countries and, typically, each new monarch conceded them further privileges. Those privileges became the basis of the Golden Liberty in the Polish–Lithuanian Commonwealth. Despite having a king, Poland was called the nobility's Commonwealth because the king was elected by all interested members of hereditary nobility and Poland was considered to be the property of this class, not of the king or the ruling dynasty. This state of affairs grew up in part because of the extinction of the male-line descendants of the old royal dynasty (first the Piasts, then the Jagiellons), and the selection by the nobility of the Polish king from among the dynasty's female-line descendants.
Poland's successive kings granted privileges to the nobility at the time of their election to the throne (the privileges being specified in the king-elect's Pacta conventa) and at other times in exchange for ad hoc permission to raise an extraordinary tax or a pospolite ruszenie.
In 1355 in Buda King Casimir III the Great issued the first country-wide privilege for the nobility, in exchange for their agreement that in the lack of Casimir's male heirs, the throne would pass to his nephew, Louis I of Hungary. He decreed that the nobility would no longer be subject to 'extraordinary' taxes, or use their own funds for military expeditions abroad. He also promised that during travels of the royal court, the king and the court would pay for all expenses, instead of using facilities of local nobility.
In 1374 King Louis of Hungary approved the Privilege of Koszyce (Polish: "przywilej koszycki" or "ugoda koszycka") in Košice in order to guarantee the Polish throne for his daughter Jadwiga. He broadened the definition of who was a member of the nobility and exempted the entire class from all but one tax (łanowy, which was limited to 2 grosze from łan (an old measure of land size)). In addition, the King's right to raise taxes was abolished; no new taxes could be raised without the agreement of the nobility. Henceforth, also, district offices (Polish: "urzędy ziemskie") were reserved exclusively for local nobility, as the Privilege of Koszyce forbade the king to grant official posts and major Polish castles to foreign knights. Finally, this privilege obliged the King to pay indemnities to nobles injured or taken captive during a war outside Polish borders.
In 1422 King Władysław II Jagiełło by the Privilege of Czerwińsk (Polish: "przywilej czerwiński") established the inviolability of nobles' property (their estates could not be confiscated except upon a court verdict) and ceded some jurisdiction over fiscal policy to the Royal Council (later, the Senat of Poland), including the right to mint coinage.
In 1430 with the Privileges of Jedlnia, confirmed at Kraków in 1433 (Polish: "przywileje jedlneńsko-krakowskie"), based partially on his earlier Brześć Kujawski privilege (April 25, 1425), King Władysław II Jagiełło granted the nobility a guarantee against arbitrary arrest, similar to the English Magna Carta's Habeas corpus, known from its own Latin name as "neminem captivabimus (nisi jure victum)." Henceforth no member of the nobility could be imprisoned without a warrant from a court of justice: the king could neither punish nor imprison any noble at his whim. King Władysław's quid pro quo for this boon was the nobles' guarantee that his throne would be inherited by one of his sons (who would be bound to honour the privileges theretofore granted to the nobility). On May 2, 1447 the same king issued the Wilno Privilege which gave the Lithuanian boyars the same rights as those possessed by the Polish szlachta.
In 1454 King Casimir IV granted the Nieszawa Statutes (Polish: "statuty cerkwicko-nieszawskie"), clarifying the legal basis of voivodship sejmiks (local parliaments). The king could promulgate new laws, raise taxes, or call for a levée en masse (pospolite ruszenie) only with the consent of the sejmiks, and the nobility were protected from judicial abuses. The Nieszawa Statutes also curbed the power of the magnates, as the Sejm (national parliament) received the right to elect many officials, including judges, voivods and castellans. These privileges were demanded by the szlachta as a compensation for their participation in the Thirteen Years' War.
The first "free election" (Polish: "wolna elekcja") of a king took place in 1492. (To be sure, some earlier Polish kings had been elected with help from bodies such as that which put Casimir II on the throne, thereby setting a precedent for free elections.) Only senators voted in the 1492 free election, which was won by John I Albert. For the duration of the Jagiellonian Dynasty, only members of that royal family were considered for election; later, there would be no restrictions on the choice of candidates.
On April 26, 1496 King John I Albert granted the Privilege of Piotrków (Polish: "Przywilej piotrkowski", "konstytucja piotrkowska" or "statuty piotrkowskie"), increasing the nobility's feudal power over serfs. It bound the peasant to the land, as only one son (not the eldest) was permitted to leave the village; townsfolk (Polish: "mieszczaństwo") were prohibited from owning land; and positions in the Church hierarchy could be given only to nobles.
On 23 October 1501, at Mielnik Polish–Lithuanian union was reformed at the Union of Mielnik (Polish: unia mielnicka, unia piotrkowsko-mielnicka). It was there that the tradition of the coronation Sejm (Polish: "Sejm koronacyjny") was founded. Once again the middle nobility (middle in wealth, not in rank) attempted to reduce the power of the magnates with a law that made them impeachable before the Senate for malfeasance. However the Act of Mielno (Polish: Przywilej mielnicki) of 25 October did more to strengthen the magnate dominated Senate of Poland then the lesser nobility. The nobles were given the right to disobey the King or his representatives—in the Latin, "non praestanda oboedientia"—and to form confederations, an armed rebellion against the king or state officers if the nobles thought that the law or their legitimate privileges were being infringed.
On 3 May 1505 King Alexander I Jagiellon granted the Act of "Nihil novi nisi commune consensu" (Latin: "I accept nothing new except by common consent"). This forbade the king to pass any new law without the consent of the representatives of the nobility, in Sejm and Senat assembled, and thus greatly strengthened the nobility's political position. Basically, this act transferred legislative power from the king to the Sejm. This date commonly marks the beginning of the First Rzeczpospolita, the period of a szlachta-run "Commonwealth".
About that time the "executionist movement" (Polish: "egzekucja praw"--"execution of the laws") began to take form. Its members would seek to curb the power of the magnates at the Sejm and to strengthen the power of king and country. In 1562 at the Sejm in Piotrków they would force the magnates to return many leased crown lands to the king, and the king to create a standing army (wojsko kwarciane). One of the most famous members of this movement was Jan Zamoyski. After his death in 1605, the movement lost its political force.
Until the death of Sigismund II Augustus, the last king of the Jagiellonian dynasty, monarchs could be elected from within only the royal family. However, starting from 1573, practically any Polish noble or foreigner of royal blood could become a Polish–Lithuanian monarch. Every newly elected king was supposed to sign two documents—the Pacta conventa ("agreed pacts")—a confirmation of the king's pre-election promises, and Henrican articles (artykuły henrykowskie, named after the first freely elected king, Henry of Valois). The latter document served as a virtual Polish constitution and contained the basic laws of the Commonwealth:
In 1578 king Stefan Batory created the Crown Tribunal in order to reduce the enormous pressure on the Royal Court. This placed much of the monarch's juridical power in the hands of the elected szlachta deputies, further strengthening the nobility class. In 1581 the Crown Tribunal was joined by a counterpart in Lithuania, the Lithuanian Tribunal.
For many centuries, wealthy and powerful members of the szlachta sought to gain legal privileges over their peers. Few szlachta were wealthy enough to be known as magnates (karmazyni—the "Crimsons", from the crimson colour of their boots). A proper magnate should be able to trace noble ancestors back for many generations and own at least 20 villages or estates. He should also hold a major office in the Commonwealth.
Some historians estimate the number of magnates as 1% of the number of szlachta. Out of approx. one million szlachta, tens of thousands of families, only 200–300 persons could be classed as great magnates with country-wide possessions and influence, and 30–40 of them could be viewed as those with significant impact on Poland's politics.
Magnates often received gifts from monarchs, which significantly increased their wealth. Often, those gifts were only temporary leases, which the magnates never returned (in the 16th century, the anti-magnate opposition among szlachta was known as the ruch egzekucji praw—movement for execution of the laws—which demanded that all such possessions are returned to their proper owner, the king).
One of the most important victories of the magnates was the late 16th century right to create ordynacja's (similar to majorats), which ensured that a family which gained wealth and power could more easily preserve this. Ordynacje's of families of Radziwiłł, Zamoyski, Potocki or Lubomirski often rivalled the estates of the king and were important power bases for the magnates.
The sovereignty of szlachta was ended in 1795 by Partitions of Poland, and until 1918 their legal status was dependent on policies of the Russian Empire, the Kingdom of Prussia or the Habsburg Monarchy.
In the 1840s Nicholas I reduced 64,000 szlachta to commoner status. Despite this, 62.8% of Russia's nobles were szlachta in 1858 and still 46.1% in 1897. Serfdom was abolished in Russian Poland on February 19, 1864. It was deliberately enacted in a way that would ruin the szlachta. It was the only area where peasants paid the market price in redemption for the land (the average for the empire was 34% above the market price). All land taken from Polish peasants since 1846 was to be returned without redemption payments. The ex serfs could only sell land to other peasants, not szlachta. 90% of the ex serfs in the empire who actually gained land after 1861 were in the 8 western provinces. Along with Romania, Polish landless or domestic serfs were the only ones to be given land after serfdom was abolished. All this was to punish the szlachta's role in the uprisings of 1830 and 1863. By 1864 80% of szlachta were déclassé, 1/4 petty nobles were worse off than the average serf, 48.9% of land in Russian Poland was in peasant hands, nobles still held 46%. In Second Polish Republic the privileges of the nobility were lawfully abolished by the March Constitution in 1921 and as such not granted by any future Polish law.
The Polish nobility differed in many respects from the nobility of other countries. The most important difference was that, while in most European countries the nobility lost power as the ruler strove for absolute monarchy, in Poland the reverse process occurred: the nobility actually gained power at the expense of the king, and the political system evolved into an oligarchy.
Poland's nobility were also more numerous than those of all other European countries, constituting some 10–12% of the total population of historic Polish–Lithuanian Commonwealth also some 10–12% among ethnic Poles on ethnic Polish lands (part of Commonwealth), but up to 25% of all Poles worldwide (szlachta could dispose more of resources to travels and/or conquering), while in some poorer regions (e.g., Mazowsze, the area centred on Warsaw) nearly 30%. However, according to  szlachta comprised around 8% of the total population in 1791 (up from 6.6% in the 16th century), and no more than 16% of the Roman Catholic (mostly ethnically Polish) population. It should be noted, though, that Polish szlachta usually incorporated most local nobility from the areas that were absorbed by Poland–Lithuania (Ruthenian boyars, Livonian nobles, etc.) By contrast, the nobilities of other European countries, except for Spain, amounted to a mere 1–3%, however the era of sovereign rules of Polish nobility ended earlier than in other countries (excluding France) yet in 1795 (see: Partitions of Poland), since then their legitimation and future fate depended on legislature and procedures of Russian Empire, Kingdom of Prussia or Habsburg Monarchy. Gradually their privileges were under further limitations to be completely dissolved by March Constitution of Poland in 1921.
There were a number of avenues to upward social mobility and the achievement of nobility. Poland's nobility was not a rigidly exclusive, closed class. Many low-born individuals, including townsfolk, peasants and Jews, could and did rise to official ennoblement in Polish society. Each szlachcic had enormous influence over the country's politics, in some ways even greater than that enjoyed by the citizens of modern democratic countries. Between 1652 and 1791, any nobleman could nullify all the proceedings of a given sejm (Commonwealth parliament) or sejmik (Commonwealth local parliament) by exercising his individual right of liberum veto (Latin for "I do not allow"), except in the case of a confederated sejm or confederated sejmik.
All children of the Polish nobility inherited their noble status from a noble mother and father. Any individual could attain ennoblement (nobilitacja) for special services to the state. A foreign noble might be naturalised as a Polish noble (Polish: "indygenat") by the Polish king (later, from 1641, only by a general sejm).
In theory at least, all Polish noblemen were social equals. Also in theory, they were legal peers. Those who held 'real power' dignities were more privileged but these dignities were not hereditary. Those who held honorary dignities were higher in 'ritual' hierarchy but these dignities were also granted for a lifetime. Some tenancies became hereditary and went with both privilege and titles. Nobles who were not direct barons of the Crown but held land from other lords were only peers "de iure".
Note that the Polish landed gentry (ziemianie or ziemiaństwo) was composed of any nobility that owned lands: thus of course the magnates, the middle nobility and that lesser nobility that had at least part of the village. As manorial lordships were also opened to burgesses of certain privileged royal cities, not all landed gentry had a hereditary title of nobility.
Coats of arms were very important to the Polish nobility. Its heraldic system evolved together with its neighbours in Central Europe, while differing in many ways from the heraldry of other European countries. Polish knighthood families had its counterparts, links or roots in Moravia (i.e. Poraj) and Germany (i.e. Junosza).
The most notable difference is that, contrary to other European heraldic systems, the Jews, Muslim Tatars or another minorities would be given the noble title. Also, most families sharing origin would also share a coat-of-arms. They would also share arms with families adopted into the clan (these would often have their arms officially altered upon ennoblement). Sometimes unrelated families would be falsely attributed to the clan on the basis of similarity of arms. Also often noble families claimed inaccurate clan membership. Logically, the number of coats of arms in this system was rather low and did not exceed 200 in late Middle Ages (40,000 in the late 18th century).
Also, the tradition of differentiating between the coat of arms proper and a lozenge granted to women did not develop in Poland. Usually men inherited the coat of arms from their fathers. Also, the brisure was rarely used.
The szlachta's prevalent mentality and ideology were manifested in "Sarmatism", a name derived from a myth of the szlachta's origin in the powerful ancient nation of Sarmatians. This belief system became an important part of szlachta culture and affected all aspects of their lives. It was popularized by poets who exalted traditional village life, peace and pacifism. It was also manifested in oriental-style apparel (the żupan, kontusz, sukmana, pas kontuszowy, delia); and made the scimitar-like szabla, too, a near-obligatory item of everyday szlachta apparel. Sarmatism served to integrate the multi-ethnic nobility as it created an almost nationalistic sense of unity and pride in the szlachta's "Golden Liberty" (złota wolność). Knowledge of Latin was widespread, and most szlachta freely mixed Polish and Latin vocabulary (the latter, "macaronisms"—from "macaroni") in everyday conversation.
Prior to the Reformation, the Polish nobility were mostly either Roman Catholic or Orthodox with a small group of Muslims. Many families, however, soon adopted the Reformed faiths. After the Counter-Reformation, when the Roman Catholic Church regained power in Poland, the nobility became almost exclusively Catholic, despite the fact that Roman Catholicism was not the majority religion in Commonwealth (the Catholic and Orthodox churches each accounted for some 40% of all citizens population, with the remaining 20% being Jews or members of Protestant denominations). In the 18th century, many followers of Jacob Frank joined the ranks of Jewish-descended Polish gentry. Although Jewish religion wasn't usually a pretext to block or deprive of noble status, some laws favoured religious conversion from Judaism to Christianity (see: Neophyte) by rewarding it with ennoblement.
Vacuum is space void of matter. The word stems from the Latin adjective vacuus for "vacant" or "void". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call "vacuum" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is lower than atmospheric pressure. The Latin term in vacuo is used to describe an object as being in what would otherwise be a vacuum.
The quality of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%. Much higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10−12) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm3. Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average. According to modern understanding, even if all matter could be removed from a volume, it would still not be "empty" due to vacuum fluctuations, dark energy, transiting gamma rays, cosmic rays, neutrinos, and other phenomena in quantum physics. In the electromagnetism in the 19th century, vacuum was thought to be filled with a medium called aether. In modern particle physics, the vacuum state is considered the ground state of matter.
Historically, there has been much dispute over whether such a thing as a vacuum can exist. Ancient Greek philosophers debated the existence of a vacuum, or void, in the context of atomism, which posited void and atom as the fundamental explanatory elements of physics. Following Plato, even the abstract concept of a featureless void faced considerable skepticism: it could not be apprehended by the senses, it could not, itself, provide additional explanatory power beyond the physical volume with which it was commensurate and, by definition, it was quite literally nothing at all, which cannot rightly be said to exist. Aristotle believed that no void could occur naturally, because the denser surrounding material continuum would immediately fill any incipient rarity that might give rise to a void.
In his Physics, book IV, Aristotle offered numerous arguments against the void: for example, that motion through a medium which offered no impediment could continue ad infinitum, there being no reason that something would come to rest anywhere in particular. Although Lucretius argued for the existence of vacuum in the first century BC and Hero of Alexandria tried unsuccessfully to create an artificial vacuum in the first century AD, it was European scholars such as Roger Bacon, Blasius of Parma and Walter Burley in the 13th and 14th century who focused considerable attention on these issues. Eventually following Stoic physics in this instance, scholars from the 14th century onward increasingly departed from the Aristotelian perspective in favor of a supernatural void beyond the confines of the cosmos itself, a conclusion widely acknowledged by the 17th century, which helped to segregate natural and theological concerns.
Rapid decompression can be much more dangerous than vacuum exposure itself. Even if the victim does not hold his or her breath, venting through the windpipe may be too slow to prevent the fatal rupture of the delicate alveoli of the lungs. Eardrums and sinuses may be ruptured by rapid decompression, soft tissues may bruise and seep blood, and the stress of shock will accelerate oxygen consumption leading to hypoxia. Injuries caused by rapid decompression are called barotrauma. A pressure drop of 13 kPa (100 Torr), which produces no symptoms if it is gradual, may be fatal if it occurs suddenly.
Almost two thousand years after Plato, René Descartes also proposed a geometrically based alternative theory of atomism, without the problematic nothing–everything dichotomy of void and atom. Although Descartes agreed with the contemporary position, that a vacuum does not occur in nature, the success of his namesake coordinate system and more implicitly, the spatial–corporeal component of his metaphysics would come to define the philosophically modern notion of empty space as a quantified extension of volume. By the ancient definition however, directional information and magnitude were conceptually distinct. With the acquiescence of Cartesian mechanical philosophy to the "brute fact" of action at a distance, and at length, its successful reification by force fields and ever more sophisticated geometric structure, the anachronism of empty space widened until "a seething ferment" of quantum activity in the 20th century filled the vacuum with a virtual pleroma.
In 1930, Paul Dirac proposed a model of the vacuum as an infinite sea of particles possessing negative energy, called the Dirac sea. This theory helped refine the predictions of his earlier formulated Dirac equation, and successfully predicted the existence of the positron, confirmed two years later. Werner Heisenberg's uncertainty principle formulated in 1927, predict a fundamental limit within which instantaneous position and momentum, or energy and time can be measured. This has far reaching consequences on the "emptiness" of space between particles. In the late 20th century, so-called virtual particles that arise spontaneously from empty space were confirmed.
In general relativity, a vanishing stress-energy tensor implies, through Einstein field equations, the vanishing of all the components of the Ricci tensor. Vacuum does not mean that the curvature of space-time is necessarily flat: the gravitational field can still produce curvature in a vacuum in the form of tidal forces and gravitational waves (technically, these phenomena are the components of the Weyl tensor). The black hole (with zero electric charge) is an elegant example of a region completely "filled" with vacuum, but still showing a strong curvature.
But although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. Most artificial satellites operate in this region called low Earth orbit and must fire their engines every few days to maintain orbit.[citation needed] The drag here is low enough that it could theoretically be overcome by radiation pressure on solar sails, a proposed propulsion system for interplanetary travel.[citation needed] Planets are too massive for their trajectories to be significantly affected by these forces, although their atmospheres are eroded by the solar winds.
In the medieval Middle Eastern world, the physicist and Islamic scholar, Al-Farabi (Alpharabius, 872–950), conducted a small experiment concerning the existence of vacuum, in which he investigated handheld plungers in water.[unreliable source?] He concluded that air's volume can expand to fill available space, and he suggested that the concept of perfect vacuum was incoherent. However, according to Nader El-Bizri, the physicist Ibn al-Haytham (Alhazen, 965–1039) and the Mu'tazili theologians disagreed with Aristotle and Al-Farabi, and they supported the existence of a void. Using geometry, Ibn al-Haytham mathematically demonstrated that place (al-makan) is the imagined three-dimensional void between the inner surfaces of a containing body. According to Ahmad Dallal, Abū Rayhān al-Bīrūnī also states that "there is no observable evidence that rules out the possibility of vacuum". The suction pump later appeared in Europe from the 15th century.
Medieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising. The commonly held view that nature abhorred a vacuum was called horror vacui. Speculation that even God could not create a vacuum if he wanted to was shut down[clarification needed] by the 1277 Paris condemnations of Bishop Etienne Tempier, which required there to be no restrictions on the powers of God, which led to the conclusion that God could create a vacuum if he so wished. Jean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.
In 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that teams of horses could not separate two hemispheres from which the air had been partially evacuated. Robert Boyle improved Guericke's design and with the help of Robert Hooke further developed vacuum pump technology. Thereafter, research into the partial vacuum lapsed until 1850 when August Toepler invented the Toepler Pump and Heinrich Geissler invented the mercury displacement pump in 1855, achieving a partial vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, which renewed interest in further research.
While outer space provides the most rarefied example of a naturally occurring partial vacuum, the heavens were originally thought to be seamlessly filled by a rigid indestructible material called aether. Borrowing somewhat from the pneuma of Stoic physics, aether came to be regarded as the rarefied air from which it took its name, (see Aether (mythology)). Early theories of light posited a ubiquitous terrestrial and celestial medium through which light propagated. Additionally, the concept informed Isaac Newton's explanations of both refraction and of radiant heat. 19th century experiments into this luminiferous aether attempted to detect a minute drag on the Earth's orbit. While the Earth does, in fact, move through a relatively dense medium in comparison to that of interstellar space, the drag is so minuscule that it could not be detected. In 1912, astronomer Henry Pickering commented: "While the interstellar absorbing medium may be simply the ether, [it] is characteristic of a gas, and free gaseous molecules are certainly there".
The quality of a vacuum is indicated by the amount of matter remaining in the system, so that a high quality vacuum is one with very little matter left in it. Vacuum is primarily measured by its absolute pressure, but a complete characterization requires further parameters, such as temperature and chemical composition. One of the most important parameters is the mean free path (MFP) of residual gases, which indicates the average distance that molecules will travel between collisions with each other. As the gas density decreases, the MFP increases, and when the MFP is longer than the chamber, pump, spacecraft, or other objects present, the continuum assumptions of fluid mechanics do not apply. This vacuum state is called high vacuum, and the study of fluid flows in this regime is called particle gas dynamics. The MFP of air at atmospheric pressure is very short, 70 nm, but at 100 mPa (~6997100000000000000♠1×10−3 Torr) the MFP of room temperature air is roughly 100 mm, which is on the order of everyday objects such as vacuum tubes. The Crookes radiometer turns when the MFP is larger than the size of the vanes.
The SI unit of pressure is the pascal (symbol Pa), but vacuum is often measured in torrs, named for Torricelli, an early Italian physicist (1608–1647). A torr is equal to the displacement of a millimeter of mercury (mmHg) in a manometer with 1 torr equaling 133.3223684 pascals above absolute zero pressure. Vacuum is often also measured on the barometric scale or as a percentage of atmospheric pressure in bars or atmospheres. Low vacuum is often measured in millimeters of mercury (mmHg) or pascals (Pa) below standard atmospheric pressure. "Below atmospheric" means that the absolute pressure is equal to the current atmospheric pressure.
Hydrostatic gauges (such as the mercury column manometer) consist of a vertical column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight is in equilibrium with the pressure differential between the two ends of the tube. The simplest design is a closed-end U-shaped tube, one side of which is connected to the region of interest. Any fluid can be used, but mercury is preferred for its high density and low vapour pressure. Simple hydrostatic gauges can measure pressures ranging from 1 torr (100 Pa) to above atmospheric. An important variation is the McLeod gauge which isolates a known volume of vacuum and compresses it to multiply the height variation of the liquid column. The McLeod gauge can measure vacuums as high as 10−6 torr (0.1 mPa), which is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-controlled properties. These indirect measurements must be calibrated via a direct measurement, most commonly a McLeod gauge.
Thermal conductivity gauges rely on the fact that the ability of a gas to conduct heat decreases with pressure. In this type of gauge, a wire filament is heated by running current through it. A thermocouple or Resistance Temperature Detector (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 torr to 10−3 torr, but they are sensitive to the chemical composition of the gases being measured.
Ion gauges are used in ultrahigh vacuum. They come in two types: hot cathode and cold cathode. In the hot cathode version an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10−3 torr to 10−10 torr. The principle behind cold cathode version is the same, except that electrons are produced in a discharge created by a high voltage electrical discharge. Cold cathode gauges are accurate from 10−2 torr to 10−9 torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.
Cold or oxygen-rich atmospheres can sustain life at pressures much lower than atmospheric, as long as the density of oxygen is similar to that of standard sea-level atmosphere. The colder air temperatures found at altitudes of up to 3 km generally compensate for the lower pressures there. Above this altitude, oxygen enrichment is necessary to prevent altitude sickness in humans that did not undergo prior acclimatization, and spacesuits are necessary to prevent ebullism above 19 km. Most spacesuits use only 20 kPa (150 Torr) of pure oxygen. This pressure is high enough to prevent ebullism, but decompression sickness and gas embolisms can still occur if decompression rates are not managed.
Humans and animals exposed to vacuum will lose consciousness after a few seconds and die of hypoxia within minutes, but the symptoms are not nearly as graphic as commonly depicted in media and popular culture. The reduction in pressure lowers the temperature at which blood and other body fluids boil, but the elastic pressure of blood vessels ensures that this boiling point remains above the internal body temperature of 37 °C. Although the blood will not boil, the formation of gas bubbles in bodily fluids at reduced pressures, known as ebullism, is still a concern. The gas may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Swelling and ebullism can be restrained by containment in a flight suit. Shuttle astronauts wore a fitted elastic garment called the Crew Altitude Protection Suit (CAPS) which prevents ebullism at pressures as low as 2 kPa (15 Torr). Rapid boiling will cool the skin and create frost, particularly in the mouth, but this is not a significant hazard.
In ultra high vacuum systems, some very "odd" leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the adsorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The permeability of the metallic chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.
In quantum mechanics and quantum field theory, the vacuum is defined as the state (that is, the solution to the equations of the theory) with the lowest possible energy (the ground state of the Hilbert space). In quantum electrodynamics this vacuum is referred to as 'QED vacuum' to distinguish it from the vacuum of quantum chromodynamics, denoted as QCD vacuum. QED vacuum is a state with no matter particles (hence the name), and also no photons. As described above, this state is impossible to achieve experimentally. (Even if every matter particle could somehow be removed from a volume, it would be impossible to eliminate all the blackbody photons.) Nonetheless, it provides a good model for realizable vacuum, and agrees with a number of experimental observations as described next.
QED vacuum has interesting and complex properties. In QED vacuum, the electric and magnetic fields have zero average values, but their variances are not zero. As a result, QED vacuum contains vacuum fluctuations (virtual particles that hop into and out of existence), and a finite energy called vacuum energy. Vacuum fluctuations are an essential and ubiquitous part of quantum field theory. Some experimentally verified effects of vacuum fluctuations include spontaneous emission and the Lamb shift. Coulomb's law and the electric potential in vacuum near an electric charge are modified.
Stars, planets, and moons keep their atmospheres by gravitational attraction, and as such, atmospheres have no clearly delineated boundary: the density of atmospheric gas simply decreases with distance from the object. The Earth's atmospheric pressure drops to about 6998320000000000000♠3.2×10−2 Pa at 100 kilometres (62 mi) of altitude, the Kármán line, which is a common definition of the boundary with outer space. Beyond this line, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar winds, so the definition of pressure becomes difficult to interpret. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather. Astrophysicists prefer to use number density to describe these environments, in units of particles per cubic centimetre.
Vacuum is useful in a variety of processes and devices. Its first widespread use was in the incandescent light bulb to protect the filament from chemical degradation. The chemical inertness produced by a vacuum is also useful for electron beam welding, cold welding, vacuum packing and vacuum frying. Ultra-high vacuum is used in the study of atomically clean substrates, as only a very good vacuum preserves atomic-scale clean surfaces for a reasonably long time (on the order of minutes to days). High to ultra-high vacuum removes the obstruction of air, allowing particle beams to deposit or remove materials without contamination. This is the principle behind chemical vapor deposition, physical vapor deposition, and dry etching which are essential to the fabrication of semiconductors and optical coatings, and to surface science. The reduction of convection provides the thermal insulation of thermos bottles. Deep vacuum lowers the boiling point of liquids and promotes low temperature outgassing which is used in freeze drying, adhesive preparation, distillation, metallurgy, and process purging. The electrical properties of vacuum make electron microscopes and vacuum tubes possible, including cathode ray tubes. The elimination of air friction is useful for flywheel energy storage and ultracentrifuges.
Manifold vacuum can be used to drive accessories on automobiles. The best-known application is the vacuum servo, used to provide power assistance for the brakes. Obsolete applications include vacuum-driven windscreen wipers and Autovac fuel pumps. Some aircraft instruments (Attitude Indicator (AI) and the Heading Indicator (HI)) are typically vacuum-powered, as protection against loss of all (electrically powered) instruments, since early aircraft often did not have electrical systems, and since there are two readily available sources of vacuum on a moving aircraft—the engine and an external venturi. Vacuum induction melting uses electromagnetic induction within a vacuum.
Evaporation and sublimation into a vacuum is called outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure. In man-made systems, outgassing has the same effect as a leak and can limit the achievable vacuum. Outgassing products may condense on nearby colder surfaces, which can be troublesome if they obscure optical instruments or react with other materials. This is of great concern to space missions, where an obscured telescope or solar cell can ruin an expensive mission.
To continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind positive displacement pumps, like the manual water pump for example. Inside the pump, a mechanism expands a small sealed cavity to create a vacuum. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.
The above explanation is merely a simple introduction to vacuum pumping, and is not representative of the entire range of pumps in use. Many variations of the positive displacement pump have been developed, and many other pump designs rely on fundamentally different principles. Momentum transfer pumps, which bear some similarities to dynamic pumps used at higher pressures, can achieve much higher quality vacuums than positive displacement pumps. Entrapment pumps can capture gases in a solid or absorbed state, often with no moving parts, no seals and no vibration. None of these pumps are universal; each type has important performance limitations. They all share a difficulty in pumping low molecular weight gases, especially hydrogen, helium, and neon.
The lowest pressure that can be attained in a system is also dependent on many things other than the nature of the pumps. Multiple pumps may be connected in series, called stages, to achieve higher vacuums. The choice of seals, chamber geometry, materials, and pump-down procedures will all have an impact. Collectively, these are called vacuum technique. And sometimes, the final pressure is not the only relevant characteristic. Pumping systems differ in oil contamination, vibration, preferential pumping of certain gases, pump-down speeds, intermittent duty cycle, reliability, or tolerance to high leakage rates.
Cambridge English Dictionary states that culture is, "the way of life, especially the general customs and beliefs, of a particular group of people at a particular time." Terror Management Theory posits that culture is a series of activities and worldviews that provide humans with the illusion of being individuals of value in a world meaning—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that Homo Sapiens became aware of when they acquired a larger brain.
As a defining aspect of what it means to be human, culture is a central concept in anthropology, encompassing the range of phenomena that are transmitted through social learning in human societies. The word is used in a general sense as the evolved ability to categorize and represent experiences with symbols and to act imaginatively and creatively. This ability arose with the evolution of behavioral modernity in humans around 50,000 years ago.[citation needed] This capacity is often thought to be unique to humans, although some other species have demonstrated similar, though much less complex abilities for social learning. It is also used to denote the complex networks of practices and accumulated knowledge and ideas that is transmitted through social interaction and exist in specific human groups, or cultures, using the plural form. Some aspects of human behavior, such as language, social practices such as kinship, gender and marriage, expressive forms such as art, music, dance, ritual, religion, and technologies such as cooking, shelter, clothing are said to be cultural universals, found in all human societies. The concept material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including, practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science make up the intangible cultural heritage of a society.
In the humanities, one sense of culture, as an attribute of the individual, has been the degree to which they have cultivated a particular level of sophistication, in the arts, sciences, education, or manners. The level of cultural sophistication has also sometimes been seen to distinguish civilizations from less complex societies. Such hierarchical perspectives on culture are also found in class-based distinctions between a high culture of the social elite and a low culture, popular culture or folk culture of the lower classes, distinguished by the stratified access to cultural capital. In common parlance, culture is often used to refer specifically to the symbolic markers used by ethnic groups to distinguish themselves visibly from each other such as body modification, clothing or jewelry.[dubious – discuss] Mass culture refers to the mass-produced and mass mediated forms of consumer culture that emerged in the 20th century. Some schools of philosophy, such as Marxism and critical theory, have argued that culture is often used politically as a tool of the elites to manipulate the lower classes and create a false consciousness, such perspectives common in the discipline of cultural studies. In the wider social sciences, the theoretical perspective of cultural materialism holds that human symbolic culture arises from the material conditions of human life, as humans create the conditions for physical survival, and that the basis of culture is found in evolved biological dispositions.
When used as a count noun "a culture", is the set of customs, traditions and values of a society or community, such as an ethnic group or nation. In this sense, multiculturalism is a concept that values the peaceful coexistence and mutual respect between different cultures inhabiting the same territory. Sometimes "culture" is also used to describe specific practices within a subgroup of a society, a subculture (e.g. "bro culture"), or a counter culture. Within cultural anthropology, the ideology and analytical stance of cultural relativism holds that cultures cannot easily be objectively ranked or evaluated because any evaluation is necessarily situated within the value system of a given culture.
The modern term "culture" is based on a term used by the Ancient Roman orator Cicero in his Tusculanae Disputationes, where he wrote of a cultivation of the soul or "cultura animi", using an agricultural metaphor for the development of a philosophical soul, understood teleologically as the highest possible ideal for human development. Samuel Pufendorf took over this metaphor in a modern context, meaning something similar, but no longer assuming that philosophy was man's natural perfection. His use, and that of many writers after him "refers to all the ways in which human beings overcome their original barbarism, and through artifice, become fully human".
Social conflict and the development of technologies can produce changes within a society by altering social dynamics and promoting new cultural models, and spurring or enabling generative action. These social shifts may accompany ideological shifts and other types of cultural change. For example, the U.S. feminist movement involved new practices that produced a shift in gender relations, altering both gender and economic structures. Environmental conditions may also enter as factors. For example, after tropical forests returned at the end of the last ice age, plants suitable for domestication were available, leading to the invention of agriculture, which in turn brought about many cultural innovations and shifts in social dynamics.
Cultures are externally affected via contact between societies, which may also produce—or inhibit—social shifts and changes in cultural practices. War or competition over resources may impact technological development or social dynamics. Additionally, cultural ideas may transfer from one society to another, through diffusion or acculturation. In diffusion, the form of something (though not necessarily its meaning) moves from one culture to another. For example, hamburgers, fast food in the United States, seemed exotic when introduced into China. "Stimulus diffusion" (the sharing of ideas) refers to an element of one culture leading to an invention or propagation in another. "Direct Borrowing" on the other hand tends to refer to technological or tangible diffusion from one culture to another. Diffusion of innovations theory presents a research-based model of why and when individuals and cultures adopt new ideas, practices, and products.
Immanuel Kant (1724–1804) has formulated an individualist definition of "enlightenment" similar to the concept of bildung: "Enlightenment is man's emergence from his self-incurred immaturity." He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: Sapere aude, "Dare to be wise!" In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of bildung: "For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people."
In 1795, the Prussian linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a "Germany" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as "worldview" (Weltanschauung). According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between "civilized" and "primitive" or "tribal" cultures.
In 1860, Adolf Bastian (1826–1905) argued for "the psychic unity of mankind". He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of "elementary ideas" (Elementargedanken); different cultures, or different "folk ideas" (Völkergedanken), are local modifications of the elementary ideas. This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.
In practice, culture referred to an élite ideal and was associated with such activities as art, classical music, and haute cuisine. As these forms were associated with urban life, "culture" was identified with "civilization" (from lat. civitas, city). Another facet of the Romantic movement was an interest in folklore, which led to identifying a "culture" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling social group, and low culture. In other words, the idea of "culture" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.
Matthew Arnold contrasted "culture" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted "culture" with "the state of nature". According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between "civilized" and "uncivilized." According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures is really an expression of the conflict between European elites and non-elites, some critics have argued that the distinction between civilized and uncivilized people is really an expression of the conflict between European colonial powers and their colonial subjects.
Other 19th-century critics, following Rousseau have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by "the folk", i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as "noble savages" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of the West.
Although anthropologists worldwide refer to Tylor's definition of culture, in the 20th century "culture" emerged as the central and unifying concept of American anthropology, where it most commonly refers to the universal human capacity to classify and encode human experiences symbolically, and to communicate symbolically encoded experiences socially.[citation needed] American anthropology is organized into four fields, each of which plays an important role in research on culture: biological anthropology, linguistic anthropology, cultural anthropology, and archaeology.
The sociology of culture concerns culture—usually understood as the ensemble of symbolic codes used by a society—as manifested in society. For Georg Simmel (1858–1918), culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history". Culture in the sociological field can be defined as the ways of thinking, the ways of acting, and the material objects that together shape a people's way of life. Culture can be any of two types, non-material culture or material culture. Non-material culture refers to the non physical ideas that individuals have about their culture, including values, belief system, rules, norms, morals, language, organizations, and institutions. While Material culture is the physical evidence of a culture in the objects and architecture they make, or have made. The term tends to be relevant only in archeological and anthropological studies, but it specifically means all material evidence which can be attributed to culture past or present.
Cultural sociology first emerged in Weimar Germany (1918–1933), where sociologists such as Alfred Weber used the term Kultursoziologie (cultural sociology). Cultural sociology was then "reinvented" in the English-speaking world as a product of the "cultural turn" of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may loosely be regarded as an approach incorporating cultural analysis and critical theory. Cultural sociologists tend to reject scientific methods,[citation needed] instead hermeneutically focusing on words, artifacts and symbols. "Culture" has since become an important concept across many branches of sociology, including resolutely scientific fields like social stratification and social network analysis. As a result, there has been a recent influx of quantitative sociologists to the field. Thus there is now a growing group of sociologists of culture who are, confusingly, not cultural sociologists. These scholars reject the abstracted postmodern aspects of cultural sociology, and instead look for a theoretical backing in the more scientific vein of social psychology and cognitive science. "Cultural sociology" is one of the largest sections of the American Sociological Association. The British establishment of cultural studies means the latter is often taught as a loosely distinct discipline in the UK.
The sociology of culture grew from the intersection between sociology (as shaped by early theorists like Marx, Durkheim, and Weber) with the growing discipline of anthropology, where in researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field lingers in the methods (much of cultural sociological research is qualitative), in the theories (a variety of critical approaches to sociology are central to current research communities), and in the substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.
In the United Kingdom, sociologists and other scholars influenced by Marxism, such as Stuart Hall (1932–2014) and Raymond Williams (1921–1988), developed cultural studies. Following nineteenth-century Romantics, they identified "culture" with consumption goods and leisure activities (such as art, music, film, food, sports, and clothing). Nevertheless, they saw patterns of consumption and leisure as determined by relations of production, which led them to focus on class relations and the organization of production.
In the United States, "Cultural Studies" focuses largely on the study of popular culture, that is, on the social meanings of mass-produced consumer and leisure goods. Richard Hoggart coined the term in 1964 when he founded the Birmingham Centre for Contemporary Cultural Studies or CCCS. It has since become strongly associated with Stuart Hall, who succeeded Hoggart as Director. Cultural studies in this sense, then, can be viewed as a limited concentration scoped on the intricacies of consumerism, which belongs to a wider culture sometimes referred to as "Western Civilization" or as "Globalism."
From the 1970s onward, Stuart Hall's pioneering work, along with that of his colleagues Paul Willis, Dick Hebdige, Tony Jefferson, and Angela McRobbie, created an international intellectual movement. As the field developed it began to combine political economy, communication, sociology, social theory, literary theory, media theory, film/video studies, cultural anthropology, philosophy, museum studies and art history to study cultural phenomena or cultural texts. In this field researchers often concentrate on how particular phenomena relate to matters of ideology, nationality, ethnicity, social class, and/or gender.[citation needed] Cultural studies has a concern with the meaning and practices of everyday life. These practices comprise the ways people do particular things (such as watching television, or eating out) in a given culture. This field studies the meanings and uses people attribute to various objects and practices. Specifically, culture involves those meanings and practices held independently of reason. Watching television in order to view a public perspective on a historical event should not be thought of as culture, unless referring to the medium of television itself, which may have been selected culturally; however, schoolchildren watching television after school with their friends in order to "fit in" certainly qualifies, since there is no grounded reason for one's participation in this practice. Recently, as capitalism has spread throughout the world (a process called globalization), cultural studies has begun[when?] to analyze local and global forms of resistance to Western hegemony.[citation needed] Globalization in this context can be defined as western civilization in other ways, it undermines the cultural integrity of other culture and it is therefore repressive, exploitative and harmful to most people in different places.
In the context of cultural studies, the idea of a text includes not only written language, but also films, photographs, fashion or hairstyles: the texts of cultural studies comprise all the meaningful artifacts of culture.[citation needed] Similarly, the discipline widens the concept of "culture". "Culture" for a cultural-studies researcher not only includes traditional high culture (the culture of ruling social groups) and popular culture, but also everyday meanings and practices. The last two, in fact, have become the main focus of cultural studies. A further and recent approach is comparative cultural studies, based on the disciplines of comparative literature and cultural studies.[citation needed]
Scholars in the United Kingdom and the United States developed somewhat different versions of cultural studies after the late 1970s. The British version of cultural studies had originated in the 1950s and 1960s, mainly under the influence first of Richard Hoggart, E. P. Thompson, and Raymond Williams, and later that of Stuart Hall and others at the Centre for Contemporary Cultural Studies at the University of Birmingham. This included overtly political, left-wing views, and criticisms of popular culture as "capitalist" mass culture; it absorbed some of the ideas of the Frankfurt School critique of the "culture industry" (i.e. mass culture). This emerges in the writings of early British cultural-studies scholars and their influences: see the work of (for example) Raymond Williams, Stuart Hall, Paul Willis, and Paul Gilroy.
In the United States, Lindlof and Taylor write, "Cultural studies [were] grounded in a pragmatic, liberal-pluralist tradition". The American version of cultural studies initially concerned itself more with understanding the subjective and appropriative side of audience reactions to, and uses of, mass culture; for example, American cultural-studies advocates wrote about the liberatory aspects of fandom.[citation needed] The distinction between American and British strands, however, has faded.[citation needed] Some researchers, especially in early British cultural studies, apply a Marxist model to the field. This strain of thinking has some influence from the Frankfurt School, but especially from the structuralist Marxism of Louis Althusser and others. The main focus of an orthodox Marxist approach concentrates on the production of meaning. This model assumes a mass production of culture and identifies power as residing with those producing cultural artifacts. In a Marxist view, those who control the means of production (the economic base) essentially control a culture.[citation needed] Other approaches to cultural studies, such as feminist cultural studies and later American developments of the field, distance themselves from this view. They criticize the Marxist assumption of a single, dominant meaning, shared by all, for any cultural product. The non-Marxist approaches suggest that different ways of consuming cultural artifacts affect the meaning of the product. This view comes through in the book Doing Cultural Studies: The Story of the Sony Walkman (by Paul du Gay et al.), which seeks to challenge the notion that those who produce commodities control the meanings that people attribute to them. Feminist cultural analyst, theorist and art historian Griselda Pollock contributed to cultural studies from viewpoints of art history and psychoanalysis. The writer Julia Kristeva is among influential voices at the turn of the century, contributing to cultural studies from the field of art and psychoanalytical French feminism.[citation needed]
Raimon Panikkar pointed out 29 ways in which cultural change can be brought about. Some of these are: growth, development, evolution, involution, renovation, reconception, reform, innovation, revivalism, revolution, mutation, progress, diffusion, osmosis, borrowing, eclecticism, syncretism, modernization, indigenization, and transformation. Hence Modernization could be similar or related to the enlightenment but a 'looser' term set to ideal and values that flourish. a belief in objectivity progress. Also seen as a belief in a secular society (free from religious influences) example objective and rational, science vs religion and finally been modern means not being religious.