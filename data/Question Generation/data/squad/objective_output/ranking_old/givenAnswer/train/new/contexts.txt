Heresy is any provocative belief or theory that is strongly at variance with established beliefs or customs. A heretic is a proponent of such claims or beliefs. Heresy is distinct from both apostasy, which is the explicit renunciation of one's religion, principles or cause, and blasphemy, which is an impious utterance or action concerning God or sacred things.
The term is usually used to refer to violations of important religious teachings, but is used also of views strongly opposed to any generally accepted ideas. It is used in particular in reference to Christianity, Judaism, Islam and Marxism.
In certain historical Christian, Islamic and Jewish cultures, among others, espousing ideas deemed heretical has been and in some cases still is subjected not merely to punishments such as excommunication, but even to the death penalty.
The term heresy is from Greek αἵρεσις originally meant "choice" or "thing chosen", but it came to mean the "party or school of a man's choice" and also referred to that process whereby a young person would examine various philosophies to determine how to live. The word "heresy" is usually used within a Christian, Jewish, or Islamic context, and implies slightly different meanings in each. The founder or leader of a heretical movement is called a heresiarch, while individuals who espouse heresy or commit heresy are known as heretics. Heresiology is the study of heresy.
According to Titus 3:10 a divisive person should be warned two times before separating from him. The Greek for the phrase "divisive person" became a technical term in the early Church for a type of "heretic" who promoted dissension. In contrast correct teaching is called sound not only because it builds up in the faith, but because it protects against the corrupting influence of false teachers.
The Church Fathers identified Jews and Judaism with heresy. They saw deviations from Orthodox Christianity as heresies that were essentially Jewish in spirit. Tertullian implied that it was the Jews who most inspired heresy in Christianity: "From the Jew the heretic has accepted guidance in this discussion [that Jesus was not the Christ.]" Saint Peter of Antioch referred to Christians that refused to venerate religious images as having "Jewish minds".
The use of the word "heresy" was given wide currency by Irenaeus in his 2nd century tract Contra Haereses (Against Heresies) to describe and discredit his opponents during the early centuries of the Christian community.[citation needed] He described the community's beliefs and doctrines as orthodox (from ὀρθός, orthos "straight" + δόξα, doxa "belief") and the Gnostics' teachings as heretical.[citation needed] He also pointed out the concept of apostolic succession to support his arguments.
Constantine the Great, who along with Licinius had decreed toleration of Christianity in the Roman Empire by what is commonly called the "Edict of Milan", and was the first Roman Emperor baptized, set precedents for later policy. By Roman law the Emperor was Pontifex Maximus, the high priest of the College of Pontiffs (Collegium Pontificum) of all recognized religions in ancient Rome. To put an end to the doctrinal debate initiated by Arius, Constantine called the first of what would afterwards be called the ecumenical councils and then enforced orthodoxy by Imperial authority.
The first known usage of the term in a legal context was in AD 380 by the Edict of Thessalonica of Theodosius I, which made Christianity the state church of the Roman Empire. Prior to the issuance of this edict, the Church had no state-sponsored support for any particular legal mechanism to counter what it perceived as "heresy". By this edict the state's authority and that of the Church became somewhat overlapping. One of the outcomes of this blurring of Church and state was the sharing of state powers of legal enforcement with church authorities. This reinforcement of the Church's authority gave church leaders the power to, in effect, pronounce the death sentence upon those whom the church considered heretical.
Within six years of the official criminalization of heresy by the Emperor, the first Christian heretic to be executed, Priscillian, was condemned in 386 by Roman secular officials for sorcery, and put to death with four or five followers. However, his accusers were excommunicated both by Ambrose of Milan and Pope Siricius, who opposed Priscillian's heresy, but "believed capital punishment to be inappropriate at best and usually unequivocally evil". For some years after the Reformation, Protestant churches were also known to execute those they considered heretics, including Catholics. The last known heretic executed by sentence of the Roman Catholic Church was Spanish schoolmaster Cayetano Ripoll in 1826. The number of people executed as heretics under the authority of the various "ecclesiastical authorities"[note 1] is not known.[note 2] One of the first examples of the word as translated from the Nag Hammadi's Apocalypse of Peter was" they will cleave to the name of a dead man thinking that they will become pure. But they will become greatly defiled and they will fall into the name of error and into the hands of an evil cunning man and a manifold dogma, and they will be ruled heretically".
In the Roman Catholic Church, obstinate and willful manifest heresy is considered to spiritually cut one off from the Church, even before excommunication is incurred. The Codex Justinianus (1:5:12) defines "everyone who is not devoted to the Catholic Church and to our Orthodox holy Faith" a heretic. The Church had always dealt harshly with strands of Christianity that it considered heretical, but before the 11th century these tended to centre around individual preachers or small localised sects, like Arianism, Pelagianism, Donatism, Marcionism and Montanism. The diffusion of the almost Manichaean sect of Paulicians westwards gave birth to the famous 11th and 12th century heresies of Western Europe. The first one was that of Bogomils in modern day Bosnia, a sort of sanctuary between Eastern and Western Christianity. By the 11th century, more organised groups such as the Patarini, the Dulcinians, the Waldensians and the Cathars were beginning to appear in the towns and cities of northern Italy, southern France and Flanders.
In France the Cathars grew to represent a popular mass movement and the belief was spreading to other areas. The Cathar Crusade was initiated by the Roman Catholic Church to eliminate the Cathar heresy in Languedoc. Heresy was a major justification for the Inquisition (Inquisitio Haereticae Pravitatis, Inquiry on Heretical Perversity) and for the European wars of religion associated with the Protestant Reformation.
Galileo Galilei was brought before the Inquisition for heresy, but abjured his views and was sentenced to house arrest, under which he spent the rest of his life. Galileo was found "vehemently suspect of heresy", namely of having held the opinions that the Sun lies motionless at the centre of the universe, that the Earth is not at its centre and moves, and that one may hold and defend an opinion as probable after it has been declared contrary to Holy Scripture. He was required to "abjure, curse and detest" those opinions.
Pope St. Gregory stigmatized Judaism and the Jewish People in many of his writings. He described Jews as enemies of Christ: "The more the Holy Spirit fills the world, the more perverse hatred dominates the souls of the Jews." He labeled all heresy as "Jewish", claiming that Judaism would "pollute [Catholics and] deceive them with sacrilegious seduction." The identification of Jews and heretics in particular occurred several times in Roman-Christian law,
In Eastern Christianity heresy most commonly refers to those beliefs declared heretical by the first seven Ecumenical Councils.[citation needed] Since the Great Schism and the Protestant Reformation, various Christian churches have also used the concept in proceedings against individuals and groups those churches deemed heretical. The Orthodox Church also rejects the early Christian heresies such as Arianism, Gnosticism, Origenism, Montanism, Judaizers, Marcionism, Docetism, Adoptionism, Nestorianism, Monophysitism, Monothelitism and Iconoclasm.
In his work "On the Jews and Their Lies" (1543), German Reformation leader Martin Luther claims that Jewish history was "assailed by much heresy", and that Christ the logos swept away the Jewish heresy and goes on to do so, "as it still does daily before our eyes." He stigmatizes Jewish Prayer as being "blasphemous" (sic) and a lie, and vilifies Jews in general as being spiritually "blind" and "surely possessed by all devils." Luther calls the members of the Orthodox Catholic Church "papists" and heretics, and has a special spiritual problem with Jewish circumcision.
In England, the 16th-century European Reformation resulted in a number of executions on charges of heresy. During the thirty-eight years of Henry VIII's reign, about sixty heretics, mainly Protestants, were executed and a rather greater number of Catholics lost their lives on grounds of political offences such as treason, notably Sir Thomas More and Cardinal John Fisher, for refusing to accept the king's supremacy over the Church in England. Under Edward VI, the heresy laws were repealed in 1547 only to be reintroduced in 1554 by Mary I; even so two radicals were executed in Edward's reign (one for denying the reality of the incarnation, the other for denying Christ's divinity). Under Mary, around two hundred and ninety people were burned at the stake between 1555 and 1558 after the restoration of papal jurisdiction. When Elizabeth I came to the throne, the concept of heresy was retained in theory but severely restricted by the 1559 Act of Supremacy and the one hundred and eighty or so Catholics who were executed in the forty-five years of her reign were put to death because they were considered members of "...a subversive fifth column." The last execution of a "heretic" in England occurred under James VI and I in 1612. Although the charge was technically one of "blasphemy" there was one later execution in Scotland (still at that date an entirely independent kingdom) when in 1697 Thomas Aikenhead was accused, among other things, of denying the doctrine of the Trinity.
Another example of the persecution of heretics under Protestant rule was the execution of the Boston martyrs in 1659, 1660, and 1661. These executions resulted from the actions of the Anglican Puritans, who at that time wielded political as well as ecclesiastic control in the Massachusetts Bay Colony. At the time, the colony leaders were apparently hoping to achieve their vision of a "purer absolute theocracy" within their colony .[citation needed] As such, they perceived the teachings and practices of the rival Quaker sect as heretical, even to the point where laws were passed and executions were performed with the aim of ridding their colony of such perceived "heresies".[citation needed] It should be noticed that the Eastern Orthodox and Oriental Orthodox communions generally regard the Puritans themselves as having been heterodox or heretical.
The era of mass persecution and execution of heretics under the banner of Christianity came to an end in 1826 with the last execution of a "heretic", Cayetano Ripoll, by the Catholic Inquisition.
Although less common than in earlier periods, in modern times, formal charges of heresy within Christian churches still occur. Issues in the Protestant churches have included modern biblical criticism and the nature of God. In the Catholic Church, the Congregation for the Doctrine of the Faith criticizes writings for "ambiguities and errors" without using the word "heresy".
Perhaps due to the many modern negative connotations associated with the term heretic, such as the Spanish inquisition, the term is used less often today. The subject of Christian heresy opens up broader questions as to who has a monopoly on spiritual truth, as explored by Jorge Luis Borges in the short story "The Theologians" within the compilation Labyrinths.
Ottoman Sultan Selim the Grim, regarded the Shia Qizilbash as heretics, reportedly proclaimed that "the killing of one Shiite had as much otherworldly reward as killing 70 Christians."
In some modern day nations and regions in which Sharia law is ostensibly practiced, heresy remains an offense punishable by death. One example is the 1989 fatwa issued by the government of Iran, offering a substantial bounty for anyone who succeeds in the assassination of author Salman Rushdie, whose writings were declared as heretical.
Orthodox Judaism considers views on the part of Jews who depart from traditional Jewish principles of faith heretical. In addition, the more right-wing groups within Orthodox Judaism hold that all Jews who reject the simple meaning of Maimonides's 13 principles of Jewish faith are heretics. As such, most of Orthodox Judaism considers Reform and Reconstructionist Judaism heretical movements, and regards most of Conservative Judaism as heretical. The liberal wing of Modern Orthodoxy is more tolerant of Conservative Judaism, particularly its right wing, as there is some theological and practical overlap between these groups.
The act of using Church of Scientology techniques in a form different than originally described by Hubbard is referred to within Scientology as "squirreling" and is said by Scientologists to be high treason. The Religious Technology Center has prosecuted breakaway groups that have practiced Scientology outside the official Church without authorization.
In other contexts the term does not necessarily have pejorative overtones and may even be complimentary when used, in areas where innovation is welcome, of ideas that are in fundamental disagreement with the status quo in any practice and branch of knowledge. Scientist/author Isaac Asimov considered heresy as an abstraction, Asimov's views are in Forward: The Role of the Heretic. mentioning religious, political, socioeconomic and scientific heresies. He divided scientific heretics into endoheretics (those from within the scientific community) and exoheretics (those from without). Characteristics were ascribed to both and examples of both kinds were offered. Asimov concluded that science orthodoxy defends itself well against endoheretics (by control of science education, grants and publication as examples), but is nearly powerless against exoheretics. He acknowledged by examples that heresy has repeatedly become orthodoxy.
The revisionist paleontologist Robert T. Bakker, who published his findings as The Dinosaur Heresies, treated the mainstream view of dinosaurs as dogma. "I have enormous respect for dinosaur paleontologists past and present. But on average, for the last fifty years, the field hasn't tested dinosaur orthodoxy severely enough." page 27 "Most taxonomists, however, have viewed such new terminology as dangerously destabilizing to the traditional and well-known scheme..." page 462. This book apparently influenced Jurassic Park. The illustrations by the author show dinosaurs in very active poses, in contrast to the traditional perception of lethargy. He is an example of a recent scientific endoheretic.
Immanuel Velikovsky is an example of a recent scientific exoheretic; he did not have appropriate scientific credentials or did not publish in scientific journals. While the details of his work are in scientific disrepute, the concept of catastrophic change (extinction event and punctuated equilibrium) has gained acceptance in recent decades.
The term heresy is also used as an ideological pigeonhole for contemporary writers because, by definition, heresy depends on contrasts with an established orthodoxy. For example, the tongue-in-cheek contemporary usage of heresy, such as to categorize a "Wall Street heresy" a "Democratic heresy" or a "Republican heresy," are metaphors that invariably retain a subtext that links orthodoxies in geology or biology or any other field to religion. These expanded metaphoric senses allude to both the difference between the person's views and the mainstream and the boldness of such a person in propounding these views.
Josip Broz Tito (Cyrillic: Јосип Броз Тито, pronounced [jǒsip brôːz tîto]; born Josip Broz; 7 May 1892[nb 1] – 4 May 1980) was a Yugoslav revolutionary and statesman, serving in various roles from 1943 until his death in 1980. During World War II he was the leader of the Partisans, often regarded as the most effective resistance movement in occupied Europe. While his presidency has been criticized as authoritarian, and concerns about the repression of political opponents have been raised, Tito was "seen by most as a benevolent dictator" due to his economic and diplomatic policies. He was a popular public figure both in Yugoslavia and abroad. Viewed as a unifying symbol, his internal policies maintained the peaceful coexistence of the nations of the Yugoslav federation. He gained further international attention as the chief leader of the Non-Aligned Movement, working with Jawaharlal Nehru of India, Gamal Abdel Nasser of Egypt and Sukarno of Indonesia.
He was General Secretary (later Chairman of the Presidium) of the League of Communists of Yugoslavia (1939–80), and went on to lead the World War II Yugoslav guerrilla movement, the Partisans (1941–45). After the war, he was the Prime Minister (1944–63), President (later President for Life) (1953–80) of the Socialist Federal Republic of Yugoslavia (SFRY). From 1943 to his death in 1980, he held the rank of Marshal of Yugoslavia, serving as the supreme commander of the Yugoslav military, the Yugoslav People's Army (JNA). With a highly favourable reputation abroad in both Cold War blocs, Josip Broz Tito received some 98 foreign decorations, including the Legion of Honour and the Order of the Bath.
Josip Broz was born to a Croat father and Slovene mother in the village of Kumrovec, Croatia. Drafted into military service, he distinguished himself, becoming the youngest Sergeant Major in the Austro-Hungarian Army of that time. After being seriously wounded and captured by the Imperial Russians during World War I, Josip was sent to a work camp in the Ural Mountains. He participated in the October Revolution, and later joined a Red Guard unit in Omsk. Upon his return home, Broz found himself in the newly established Kingdom of Yugoslavia, where he joined the Communist Party of Yugoslavia (KPJ).
Tito was the chief architect of the second Yugoslavia, a socialist federation that lasted from 1943 to 1991–92. Despite being one of the founders of Cominform, soon he became the first Cominform member to defy Soviet hegemony and the only one to manage to leave Cominform and begin with its own socialist program. Tito was a backer of independent roads to socialism (sometimes referred to as "national communism"). In 1951 he implemented a self-management system that differentiated Yugoslavia from other socialist countries. A turn towards a model of market socialism brought economic expansion in the 1950s and 1960s and a decline during the 1970s. His internal policies included the suppression of nationalist sentiment and the promotion of the "brotherhood and unity" of the six Yugoslav nations. After Tito's death in 1980, tensions between the Yugoslav republics emerged and in 1991 the country disintegrated and went into a series of wars and unrest that lasted the rest of the decade, and which continue to impact most of the former Yugoslav republics. He remains a very controversial figure in the Balkans.
Josip Broz was born on 7 May 1892 in Kumrovec, in the northern Croatian region of Hrvatsko Zagorje in Austria-Hungary.[nb 1] He was the seventh child of Franjo and Marija Broz. His father, Franjo Broz (26 November 1860 – 16 December 1936), was a Croat, while his mother Marija (25 March 1864 – 14 January 1918), was a Slovene. His parents were married on 21 January 1891. After spending part of his childhood years with his maternal grandfather Martin Javeršek in the Slovenian village of Podsreda, he entered primary school in 1900 at Kumrovec, he failed the 2nd grade and graduated in 1905. In 1907 he moved out of the rural environment and started working as a machinist's apprentice in Sisak. There, he became aware of the labour movement and celebrated 1 May – Labour Day for the first time. In 1910, he joined the union of metallurgy workers and at the same time the Social-Democratic Party of Croatia and Slavonia. Between 1911 and 1913, Broz worked for shorter periods in Kamnik (1911–1912, factory "Titan"), Cenkov, Munich and Mannheim, where he worked for the Benz car factory; then he went to Wiener Neustadt, Austria, and worked as a test driver for Daimler.
In the autumn of 1913, he was conscripted into the Austro-Hungarian Army. He was sent to a school for non-commissioned officers and became a sergeant, serving in the 25th Croatian Regiment based in Zagreb. In May 1914, Broz won a silver medal at an army fencing competition in Budapest. At the outbreak of World War I in 1914, he was sent to Ruma, where he was arrested for anti-war propaganda and imprisoned in the Petrovaradin fortress. In January 1915, he was sent to the Eastern Front in Galicia to fight against Russia. He distinguished himself as a capable soldier, becoming the youngest Sergeant Major in the Austro-Hungarian Army. For his bravery in the face of the enemy, he was recommended for the Silver Bravery Medal but was taken prisoner of war before it could be formally presented. On 25 March 1915, while in Bukovina, he was seriously wounded and captured by the Russians.
After 13 months at the hospital, Broz was sent to a work camp in the Ural Mountains where prisoners selected him for their camp leader. In February 1917, revolting workers broke into the prison and freed the prisoners. Broz subsequently joined a Bolshevik group. In April 1917, he was arrested again but managed to escape and participate in the July Days demonstrations in Petrograd (St. Petersburg) on 16–17 July 1917. On his way to Finland, Broz was caught and imprisoned in the Peter and Paul Fortress for three weeks. He was again sent to Kungur, but escaped from the train. He hid with a Russian family in Omsk, Siberia where he met his future wife Pelagija Belousova. After the October Revolution, he joined a Red Guard unit in Omsk. Following a White counteroffensive, he fled to Kirgiziya and subsequently returned to Omsk, where he married Belousova. In the spring of 1918, he joined the Yugoslav section of the Russian Communist Party. By June of the same year, Broz left Omsk to find work and support his family, and was employed as a mechanic near Omsk for a year.
In January 1920, Tito and his wife made a long and difficult journey home to Yugoslavia where he arrived in September. Upon his return, Broz joined the Communist Party of Yugoslavia. The CPY's influence on the political life of the Kingdom of Yugoslavia was growing rapidly. In the 1920 elections the Communists won 59 seats in the parliament and became the third strongest party. Winning numerous local elections, they gained a stronghold in the second largest city of Zagreb, electing Svetozar Delić for mayor. After the assassination of Milorad Drašković, the Yugoslav Minister of the Interior, by a young communist on 2 August 1921, the CPY was declared illegal under the Yugoslav State Security Act of 1921. During 1920 and 1921 all Communist-won mandates were nullified. Broz continued his work underground despite pressure on Communists from the government. As 1921 began he moved to Veliko Trojstvo near Bjelovar and found work as a machinist. In 1925, Broz moved to Kraljevica where he started working at a shipyard. He was elected as a union leader and a year later he led a shipyard strike. He was fired and moved to Belgrade, where he worked in a train coach factory in Smederevska Palanka. He was elected as Workers' Commissary but was fired as soon as his CPY membership was revealed. Broz then moved to Zagreb, where he was appointed secretary of Metal Workers' Union of Croatia. In 1928, he became the Zagreb Branch Secretary of the CPY. In the same year he was arrested, tried in court for his illegal communist activities, and sent to jail. During his five years at Lepoglava prison he met Moša Pijade, who became his ideological mentor. After his release, he lived incognito and assumed numerous noms de guerre, among them "Walter" and "Tito".
In 1934 the Zagreb Provincial Committee sent Tito to Vienna where all the Central Committee of the Communist Party of Yugoslavia had sought refuge. He was appointed to the Committee and started to appoint allies to him, among them Edvard Kardelj, Milovan Đilas, Aleksandar Ranković and Boris Kidrič. In 1935, Tito travelled to the Soviet Union, working for a year in the Balkans section of Comintern. He was a member of the Soviet Communist Party and the Soviet secret police (NKVD). Tito was also involved in recruiting for the Dimitrov Battalion, a group of volunteers serving in the Spanish Civil War. In 1936, the Comintern sent "Comrade Walter" (i.e. Tito) back to Yugoslavia to purge the Communist Party there. In 1937, Stalin had the Secretary-General of the CPY, Milan Gorkić, murdered in Moscow. Subsequently Tito was appointed Secretary-General of the still-outlawed CPY.
On 6 April 1941, German forces, with Hungarian and Italian assistance, launched an invasion of Yugoslavia. On 10 April 1941, Slavko Kvaternik proclaimed the Independent State of Croatia, and Tito responded by forming a Military Committee within the Central Committee of the Yugoslav Communist Party. Attacked from all sides, the armed forces of the Kingdom of Yugoslavia quickly crumbled. On 17 April 1941, after King Peter II and other members of the government fled the country, the remaining representatives of the government and military met with the German officials in Belgrade. They quickly agreed to end military resistance. On 1 May 1941, Tito issued a pamphlet calling on the people to unite in a battle against the occupation. On 27 June 1941, the Central Committee of the Communist Party of Yugoslavia appointed Tito Commander in Chief of all project national liberation military forces. On 1 July 1941, the Comintern sent precise instructions calling for immediate action.
Despite conflicts with the rival monarchic Chetnik movement, Tito's Partisans succeeded in liberating territory, notably the "Republic of Užice". During this period, Tito held talks with Chetnik leader Draža Mihailović on 19 September and 27 October 1941. It is said that Tito ordered his forces to assist escaping Jews, and that more than 2,000 Jews fought directly for Tito.
On 21 December 1941, the Partisans created the First Proletarian Brigade (commanded by Koča Popović) and on 1 March 1942, Tito created the Second Proletarian Brigade. In liberated territories, the Partisans organised People's Committees to act as civilian government. The Anti-Fascist Council of National Liberation of Yugoslavia (AVNOJ) convened in Bihać on 26–27 November 1942 and in Jajce on 29 November 1943. In the two sessions, the resistance representatives established the basis for post-war organisation of the country, deciding on a federation of the Yugoslav nations. In Jajce, a 67-member "presidency" was elected and established a nine-member National Committee of Liberation (five communist members) as a de facto provisional government. Tito was named President of the National Committee of Liberation.
With the growing possibility of an Allied invasion in the Balkans, the Axis began to divert more resources to the destruction of the Partisans main force and its high command. This meant, among other things, a concerted German effort to capture Josip Broz Tito personally. On 25 May 1944, he managed to evade the Germans after the Raid on Drvar (Operation Rösselsprung), an airborne assault outside his Drvar headquarters in Bosnia.
After the Partisans managed to endure and avoid these intense Axis attacks between January and June 1943, and the extent of Chetnik collaboration became evident, Allied leaders switched their support from Draža Mihailović to Tito. King Peter II, American President Franklin Roosevelt and British Prime Minister Winston Churchill joined Soviet Premier Joseph Stalin in officially recognising Tito and the Partisans at the Tehran Conference. This resulted in Allied aid being parachuted behind Axis lines to assist the Partisans. On 17 June 1944 on the Dalmatian island of Vis, the Treaty of Vis (Viški sporazum) was signed in an attempt to merge Tito's government (the AVNOJ) with the government in exile of King Peter II. The Balkan Air Force was formed in June 1944 to control operations that were mainly aimed at aiding his forces.
In the first post war years Tito was widely considered a communist leader very loyal to Moscow, indeed, he was often viewed as second only to Stalin in the Eastern Bloc. In fact, Stalin and Tito had an uneasy alliance from the start, with Stalin considering Tito too independent.
On 12 September 1944, King Peter II called on all Yugoslavs to come together under Tito's leadership and stated that those who did not were "traitors", by which time Tito was recognized by all Allied authorities (including the government-in-exile) as the Prime Minister of Yugoslavia, in addition to commander-in-chief of the Yugoslav forces. On 28 September 1944, the Telegraph Agency of the Soviet Union (TASS) reported that Tito signed an agreement with the Soviet Union allowing "temporary entry" of Soviet troops into Yugoslav territory which allowed the Red Army to assist in operations in the northeastern areas of Yugoslavia. With their strategic right flank secured by the Allied advance, the Partisans prepared and executed a massive general offensive which succeeded in breaking through German lines and forcing a retreat beyond Yugoslav borders. After the Partisan victory and the end of hostilities in Europe, all external forces were ordered off Yugoslav territory.
In the final days of World War II in Yugoslavia, units of the Partisans were responsible for atrocities after the repatriations of Bleiburg, and accusations of culpability were later raised at the Yugoslav leadership under Tito. At the time, Josip Broz Tito repeatedly issued calls for surrender to the retreating column, offering amnesty and attempting to avoid a disorderly surrender. On 14 May he dispatched a telegram to the supreme headquarters Slovene Partisan Army prohibiting "in the sternest language" the execution of prisoners of war and commanding the transfer of the possible suspects to a military court.
On 7 March 1945, the provisional government of the Democratic Federal Yugoslavia (Demokratska Federativna Jugoslavija, DFY) was assembled in Belgrade by Josip Broz Tito, while the provisional name allowed for either a republic or monarchy. This government was headed by Tito as provisional Yugoslav Prime Minister and included representatives from the royalist government-in-exile, among others Ivan Šubašić. In accordance with the agreement between resistance leaders and the government-in-exile, post-war elections were held to determine the form of government. In November 1945, Tito's pro-republican People's Front, led by the Communist Party of Yugoslavia, won the elections with an overwhelming majority, the vote having been boycotted by monarchists. During the period, Tito evidently enjoyed massive popular support due to being generally viewed by the populace as the liberator of Yugoslavia. The Yugoslav administration in the immediate post-war period managed to unite a country that had been severely affected by ultra-nationalist upheavals and war devastation, while successfully suppressing the nationalist sentiments of the various nations in favor of tolerance, and the common Yugoslav goal. After the overwhelming electoral victory, Tito was confirmed as the Prime Minister and the Minister of Foreign Affairs of the DFY. The country was soon renamed the Federal People's Republic of Yugoslavia (FPRY) (later finally renamed into Socialist Federal Republic of Yugoslavia, SFRY). On 29 November 1945, King Peter II was formally deposed by the Yugoslav Constituent Assembly. The Assembly drafted a new republican constitution soon afterwards.
Yugoslavia organized the Yugoslav People's Army (Jugoslavenska narodna armija, or JNA) from the Partisan movement and became the fourth strongest army in Europe at the time. The State Security Administration (Uprava državne bezbednosti/sigurnosti/varnosti, UDBA) was also formed as the new secret police, along with a security agency, the Department of People's Security (Organ Zaštite Naroda (Armije), OZNA). Yugoslav intelligence was charged with imprisoning and bringing to trial large numbers of Nazi collaborators; controversially, this included Catholic clergymen due to the widespread involvement of Croatian Catholic clergy with the Ustaša regime. Draža Mihailović was found guilty of collaboration, high treason and war crimes and was subsequently executed by firing squad in July 1946.
Prime Minister Josip Broz Tito met with the president of the Bishops' Conference of Yugoslavia, Aloysius Stepinac on 4 June 1945, two days after his release from imprisonment. The two could not reach an agreement on the state of the Catholic Church. Under Stepinac's leadership, the bishops' conference released a letter condemning alleged Partisan war crimes in September, 1945. The following year Stepinac was arrested and put on trial. In October 1946, in its first special session for 75 years, the Vatican excommunicated Tito and the Yugoslav government for sentencing Stepinac to 16 years in prison on charges of assisting Ustaše terror and of supporting forced conversions of Serbs to Catholicism. Stepinac received preferential treatment in recognition of his status and the sentence was soon shortened and reduced to house-arrest, with the option of emigration open to the archbishop. At the conclusion of the "Informbiro period", reforms rendered Yugoslavia considerably more religiously liberal than the Eastern Bloc states.
Unlike other new communist states in east-central Europe, Yugoslavia liberated itself from Axis domination with limited direct support from the Red Army. Tito's leading role in liberating Yugoslavia not only greatly strengthened his position in his party and among the Yugoslav people, but also caused him to be more insistent that Yugoslavia had more room to follow its own interests than other Bloc leaders who had more reasons (and pressures) to recognize Soviet efforts in helping them liberate their own countries from Axis control. Although Tito was formally an ally of Stalin after World War II, the Soviets had set up a spy ring in the Yugoslav party as early as 1945, giving way to an uneasy alliance.[citation needed]
In the immediate aftermath of World War II, there occurred several armed incidents between Yugoslavia and the Western Allies. Following the war, Yugoslavia acquired the Italian territory of Istria as well as the cities of Zadar and Rijeka. Yugoslav leadership was looking to incorporate Trieste into the country as well, which was opposed by the Western Allies. This led to several armed incidents, notably attacks by Yugoslav fighter planes on US transport aircraft, causing bitter criticism from the west. From 1945 to 1948, at least four US aircraft were shot down.[better source needed] Stalin was opposed to these provocations, as he felt the USSR unready to face the West in open war so soon after the losses of World War II and at the time when US had operational nuclear weapons whereas USSR had yet to conduct its first test. In addition, Tito was openly supportive of the Communist side in the Greek Civil War, while Stalin kept his distance, having agreed with Churchill not to pursue Soviet interests there, although he did support the Greek communist struggle politically, as demonstrated in several assemblies of the UN Security Council. In 1948, motivated by the desire to create a strong independent economy, Tito modeled his economic development plan independently from Moscow, which resulted in a diplomatic escalation followed by a bitter exchange of letters in which Tito affirmed that
The Soviet answer on 4 May admonished Tito and the Communist Party of Yugoslavia (CPY) for failing to admit and correct its mistakes, and went on to accuse them of being too proud of their successes against the Germans, maintaining that the Red Army had saved them from destruction. Tito's response on 17 May suggested that the matter be settled at the meeting of the Cominform to be held that June. However, Tito did not attend the second meeting of the Cominform, fearing that Yugoslavia was to be openly attacked. In 1949 the crisis nearly escalated into an armed conflict, as Hungarian and Soviet forces were massing on the northern Yugoslav frontier. On 28 June, the other member countries expelled Yugoslavia, citing "nationalist elements" that had "managed in the course of the past five or six months to reach a dominant position in the leadership" of the CPY. The assumption in Moscow was that once it was known that he had lost Soviet approval, Tito would collapse; 'I will shake my little finger and there will be no more Tito,' Stalin remarked. The expulsion effectively banished Yugoslavia from the international association of socialist states, while other socialist states of Eastern Europe subsequently underwent purges of alleged "Titoists". Stalin took the matter personally and arranged several assassination attempts on Tito, none of which succeeded. In a correspondence between the two leaders, Tito openly wrote:
One significant consequence of the tension arising between Yugoslavia and Soviet Union, was that Tito fought Yugoslav Stalinists with Stalin's methods. In other words, Aleksandar Ranković and the State Security Service (UBDA) employed the same inhumane methods against their opponents as Stalin did in the Soviet Union against his. Not every person accused of a political crime was convicted and nobody was sentenced to death for his or her pro-Soviet feelings. However this repression, which lasted until 1956, was marked by significant violations of human rights.
Tito's estrangement from the USSR enabled Yugoslavia to obtain US aid via the Economic Cooperation Administration (ECA), the same US aid institution which administered the Marshall Plan. Still, he did not agree to align with the West, which was a common consequence of accepting American aid at the time. After Stalin's death in 1953, relations with the USSR were relaxed and he began to receive aid as well from the COMECON. In this way, Tito played East-West antagonism to his advantage. Instead of choosing sides, he was instrumental in kick-starting the Non-Aligned Movement, which would function as a 'third way' for countries interested in staying outside of the East-West divide.
The event was significant not only for Yugoslavia and Tito, but also for the global development of socialism, since it was the first major split between Communist states, casting doubt on Comintern's claims for socialism to be a unified force that would eventually control the whole world, as Tito became the first (and the only successful) socialist leader to defy Stalin's leadership in the COMINFORM. This rift with the Soviet Union brought Tito much international recognition, but also triggered a period of instability often referred to as the Informbiro period. Tito's form of communism was labeled "Titoism" by Moscow, which encouraged purges against suspected "Titoites'" throughout the Eastern bloc.
On 26 June 1950, the National Assembly supported a crucial bill written by Milovan Đilas and Tito about "self-management" (samoupravljanje): a type of cooperative independent socialist experiment that introduced profit sharing and workplace democracy in previously state-run enterprises which then became the direct social ownership of the employees. On 13 January 1953, they established that the law on self-management was the basis of the entire social order in Yugoslavia. Tito also succeeded Ivan Ribar as the President of Yugoslavia on 14 January 1953. After Stalin's death Tito rejected the USSR's invitation for a visit to discuss normalization of relations between two nations. Nikita Khrushchev and Nikolai Bulganin visited Tito in Belgrade in 1955 and apologized for wrongdoings by Stalin's administration. Tito visited the USSR in 1956, which signaled to the world that animosity between Yugoslavia and USSR was easing. However, the relationship between the USSR and Yugoslavia would reach another low in the late 1960s. Commenting on the crisis, Tito concluded that:
The Tito-Stalin split had large ramifications for countries outside the USSR and Yugoslavia. It has, for example, been given as one of the reasons for the Slánský trial in Czechoslovakia, in which 14 high-level Communist officials were purged, with 11 of them being executed. Stalin put pressure on Czechoslovakia to conduct purges in order to discourage the spread of the idea of a "national path to socialism," which Tito espoused.
Under Tito's leadership, Yugoslavia became a founding member of the Non-Aligned Movement. In 1961, Tito co-founded the movement with Egypt's Gamal Abdel Nasser, India's Jawaharlal Nehru, Indonesia's Sukarno and Ghana's Kwame Nkrumah, in an action called The Initiative of Five (Tito, Nehru, Nasser, Sukarno, Nkrumah), thus establishing strong ties with third world countries. This move did much to improve Yugoslavia's diplomatic position. On 1 September 1961, Josip Broz Tito became the first Secretary-General of the Non-Aligned Movement.
Tito's foreign policy led to relationships with a variety of governments, such as exchanging visits (1954 and 1956) with Emperor Haile Selassie of Ethiopia, where a street was named in his honor.
Tito was notable for pursuing a foreign policy of neutrality during the Cold War and for establishing close ties with developing countries. Tito's strong belief in self-determination caused early rift with Stalin and consequently, the Eastern Bloc. His public speeches often reiterated that policy of neutrality and cooperation with all countries would be natural as long as these countries did not use their influence to pressure Yugoslavia to take sides. Relations with the United States and Western European nations were generally cordial.
Yugoslavia had a liberal travel policy permitting foreigners to freely travel through the country and its citizens to travel worldwide, whereas it was limited by most Communist countries. A number[quantify] of Yugoslav citizens worked throughout Western Europe. Tito met many world leaders during his rule, such as Soviet rulers Joseph Stalin, Nikita Khrushchev and Leonid Brezhnev; Egypt's Gamal Abdel Nasser, Indian politicians Jawaharlal Nehru and Indira Gandhi; British Prime Ministers Winston Churchill, James Callaghan and Margaret Thatcher; U.S. Presidents Dwight D. Eisenhower, John F. Kennedy, Richard Nixon, Gerald Ford and Jimmy Carter; other political leaders, dignitaries and heads of state that Tito met at least once in his lifetime included Che Guevara, Fidel Castro, Yasser Arafat, Willy Brandt, Helmut Schmidt, Georges Pompidou, Queen Elizabeth II, Hua Guofeng, Kim Il Sung, Sukarno, Sheikh Mujibur Rahman, Suharto, Idi Amin, Haile Selassie, Kenneth Kaunda, Gaddafi, Erich Honecker, Nicolae Ceaușescu, János Kádár and Urho Kekkonen. He also met numerous celebrities.
Tito visited India from December 22, 1954 through January 8, 1955. After his return, he removed many restrictions on churches and spiritual institutions in Yugoslavia.
Tito also developed warm relations with Burma under U Nu, travelling to the country in 1955 and again in 1959, though he didn't receive the same treatment in 1959 from the new leader, Ne Win.
Because of its neutrality, Yugoslavia would often be rare among Communist countries to have diplomatic relations with right-wing, anti-Communist governments. For example, Yugoslavia was the only communist country allowed to have an embassy in Alfredo Stroessner's Paraguay. One notable exception to Yugoslavia's neutral stance toward anti-communist countries was Chile under Pinochet; Yugoslavia was one of many countries which severed diplomatic relations with Chile after Salvador Allende was overthrown. Yugoslavia also provided military aid and arms supplies to staunchly anti-Communist regimes such as that of Guatemala under Kjell Eugenio Laugerud García.
On 7 April 1963, the country changed its official name to the Socialist Federal Republic of Yugoslavia. Reforms encouraged private enterprise and greatly relaxed restrictions on freedom of speech and religious expression. Tito subsequently went on a tour of the Americas. In Chile, two government ministers resigned over his visit to that country. In the autumn of 1960 Tito met President Dwight D. Eisenhower at the United Nations General Assembly meeting. Tito and Eisenhower discussed a range of issues from arms control to economic development. When Eisenhower remarked that Yugoslavia's neutralism was "neutral on his side", Tito replied that neutralism did not imply passivity but meant "not taking sides".
In 1966 an agreement with the Vatican, fostered in part by the death in 1960 of anti-communist archbishop of Zagreb Aloysius Stepinac and shifts in the church's approach to resisting communism originating in the Second Vatican Council, accorded new freedom to the Yugoslav Roman Catholic Church, particularly to catechize and open seminaries. The agreement also eased tensions, which had prevented the naming of new bishops in Yugoslavia since 1945. Tito's new socialism met opposition from traditional communists culminating in conspiracy headed by Aleksandar Ranković. In the same year Tito declared that Communists must henceforth chart Yugoslavia's course by the force of their arguments (implying an abandonment of Leninist orthodoxy and development of liberal Communism). The State Security Administration (UDBA) saw its power scaled back and its staff reduced to 5000.
On 1 January 1967, Yugoslavia was the first communist country to open its borders to all foreign visitors and abolish visa requirements. In the same year Tito became active in promoting a peaceful resolution of the Arab–Israeli conflict. His plan called for Arabs to recognize the state of Israel in exchange for territories Israel gained.
In 1968, Tito offered Czechoslovak leader Alexander Dubček to fly to Prague on three hours notice if Dubček needed help in facing down the Soviets. In April 1969, Tito removed generals Ivan Gošnjak and Rade Hamović in the aftermath of the invasion of Czechoslovakia due to the unpreparedness of the Yugoslav army to respond to a similar invasion of Yugoslavia.
In 1971, Tito was re-elected as President of Yugoslavia by the Federal Assembly for the sixth time. In his speech before the Federal Assembly he introduced 20 sweeping constitutional amendments that would provide an updated framework on which the country would be based. The amendments provided for a collective presidency, a 22-member body consisting of elected representatives from six republics and two autonomous provinces. The body would have a single chairman of the presidency and chairmanship would rotate among six republics. When the Federal Assembly fails to agree on legislation, the collective presidency would have the power to rule by decree. Amendments also provided for stronger cabinet with considerable power to initiate and pursue legislature independently from the Communist Party. Džemal Bijedić was chosen as the Premier. The new amendments aimed to decentralize the country by granting greater autonomy to republics and provinces. The federal government would retain authority only over foreign affairs, defense, internal security, monetary affairs, free trade within Yugoslavia, and development loans to poorer regions. Control of education, healthcare, and housing would be exercised entirely by the governments of the republics and the autonomous provinces.
Tito's greatest strength, in the eyes of the western communists, had been in suppressing nationalist insurrections and maintaining unity throughout the country. It was Tito's call for unity, and related methods, that held together the people of Yugoslavia. This ability was put to a test several times during his reign, notably during the Croatian Spring (also referred as the Masovni pokret, maspok, meaning "Mass Movement") when the government suppressed both public demonstrations and dissenting opinions within the Communist Party. Despite this suppression, much of maspok's demands were later realized with the new constitution, heavily backed by Tito himself against opposition from the Serbian branch of the party.[citation needed] On 16 May 1974, the new Constitution was passed, and the aging Tito was named president for life, a status which he would enjoy for five years.
Tito's visits to the United States avoided most of the Northeast due to large minorities of Yugoslav emigrants bitter about communism in Yugoslavia. Security for the state visits was usually high to keep him away from protesters, who would frequently burn the Yugoslav flag. During a visit to the United Nations in the late 1970s emigrants shouted "Tito murderer" outside his New York hotel, for which he protested to United States authorities.
After the constitutional changes of 1974, Tito began reducing his role in the day-to-day running of the state. He continued to travel abroad and receive foreign visitors, going to Beijing in 1977 and reconciling with a Chinese leadership that had once branded him a revisionist. In turn, Chairman Hua Guofeng visited Yugoslavia in 1979. In 1978, Tito traveled to the U.S. During the visit strict security was imposed in Washington, D.C. owing to protests by anti-communist Croat, Serb and Albanian groups.
Tito became increasingly ill over the course of 1979. During this time Vila Srna was built for his use near Morović in the event of his recovery. On 7 January and again on 11 January 1980, Tito was admitted to the Medical Centre in Ljubljana, the capital city of the SR Slovenia, with circulation problems in his legs. His left leg was amputated soon afterward due to arterial blockages and he died of gangrene at the Medical Centre Ljubljana on 4 May 1980 at 15:05, three days short of his 88th birthday. His funeral drew many world statesmen. Based on the number of attending politicians and state delegations, at the time it was the largest state funeral in history; this concentration of dignitaries would be unmatched until the funeral of Pope John Paul II in 2005 and the memorial service of Nelson Mandela in 2013. Those who attended included four kings, 31 presidents, six princes, 22 prime ministers and 47 ministers of foreign affairs. They came from both sides of the Cold War, from 128 different countries out of 154 UN members at the time.
Tito was interred in a mausoleum in Belgrade, which forms part of a memorial complex in the grounds of the Museum of Yugoslav History (formerly called "Museum 25 May" and "Museum of the Revolution"). The actual mausoleum is called House of Flowers (Kuća Cveća) and numerous people visit the place as a shrine to "better times". The museum keeps the gifts Tito received during his presidency. The collection also includes original prints of Los Caprichos by Francisco Goya, and many others. The Government of Serbia has planned to merge it into the Museum of the History of Serbia. At the time of his death, speculation began about whether his successors could continue to hold Yugoslavia together. Ethnic divisions and conflict grew and eventually erupted in a series of Yugoslav wars a decade after his death.
During his life and especially in the first year after his death, several places were named after Tito. Several of these places have since returned to their original names, such as Podgorica, formerly Titograd (though Podgorica's international airport is still identified by the code TGD), and Užice, formerly Titovo Užice, which reverted to its original name in 1992. Streets in Belgrade, the capital, have all reverted to their original pre–World War II and pre-communist names as well. In 2004, Antun Augustinčić's statue of Broz in his birthplace of Kumrovec was decapitated in an explosion. It was subsequently repaired. Twice in 2008, protests took place in Zagreb's Marshal Tito Square, organized by a group called Circle for the Square (Krug za Trg), with an aim to force the city government to rename it to its previous name, while a counter-protest by Citizens' Initiative Against Ustašism (Građanska inicijativa protiv ustaštva) accused the "Circle for the Square" of historical revisionism and neo-fascism. Croatian president Stjepan Mesić criticized the demonstration to change the name. In the Croatian coastal city of Opatija the main street (also its longest street) still bears the name of Marshal Tito, as do streets in numerous towns in Serbia, mostly in the country's north. One of the main streets in downtown Sarajevo is called Marshal Tito Street, and Tito's statue in a park in front of the university campus (ex. JNA barrack "Maršal Tito") in Marijin Dvor is a place where Bosnians and Sarajevans still today commemorate and pay tribute to Tito (image on the right). The largest Tito monument in the world, about 10 m (33 ft) high, is located at Tito Square (Slovene: Titov trg), the central square in Velenje, Slovenia. One of the main bridges in Slovenia's second largest city of Maribor is Tito Bridge (Titov most). The central square in Koper, the largest Slovenian port city, is as well named Tito Square.
Every year a "Brotherhood and Unity" relay race is organized in Montenegro, Macedonia and Serbia which ends at the "House of Flowers" in Belgrade on May 25 – the final resting place of Tito. At the same time, runners in Slovenia, Croatia and Bosnia and Herzegovina set off for Kumrovec, Tito's birthplace in northern Croatia. The relay is a left-over from Yugoslav times, when young people made a similar yearly trek on foot through Yugoslavia that ended in Belgrade with a massive celebration.
In the years following the dissolution of Yugoslavia, some historians stated that human rights were suppressed in Yugoslavia under Tito, particularly in the first decade up until the Tito-Stalin split. On 4 October 2011, the Slovenian Constitutional Court found a 2009 naming of a street in Ljubljana after Tito to be unconstitutional. While several public areas in Slovenia (named during the Yugoslav period) do already bear Tito's name, on the issue of renaming an additional street the court ruled that:
The court, however, explicitly made it clear that the purpose of the review was "not a verdict on Tito as a figure or on his concrete actions, as well as not a historical weighing of facts and circumstances". Slovenia has several streets and squares named after Tito, notably Tito Square in Velenje, incorporating a 10-meter statue.
Tito has also been named as responsible for systematic eradication of the ethnic German (Danube Swabian) population in Vojvodina by expulsions and mass executions following the collapse of the German occupation of Yugoslavia at the end of World War II, in contrast to his inclusive attitude towards other Yugoslav nationalities.
Tito carried on numerous affairs and was married several times. In 1918 he was brought to Omsk, Russia, as a prisoner of war. There he met Pelagija Belousova who was then thirteen; he married her a year later, and she moved with him to Yugoslavia. Pelagija bore him five children but only their son Žarko Leon (born 4 February, 1924) survived. When Tito was jailed in 1928, she returned to Russia. After the divorce in 1936 she later remarried.
In 1936, when Tito stayed at the Hotel Lux in Moscow, he met the Austrian comrade Lucia Bauer. They married in October 1936, but the records of this marriage were later erased.
His next relationship was with Herta Haas, whom he married in 1940. Broz left for Belgrade after the April War, leaving Haas pregnant. In May 1941, she gave birth to their son, Aleksandar "Mišo" Broz. All throughout his relationship with Haas, Tito had maintained a promiscuous life and had a parallel relationship with Davorjanka Paunović, who, under the codename "Zdenka", served as a courier in the resistance and subsequently became his personal secretary. Haas and Tito suddenly parted company in 1943 in Jajce during the second meeting of AVNOJ after she reportedly walked in on him and Davorjanka. The last time Haas saw Broz was in 1946. Davorjanka died of tuberculosis in 1946 and Tito insisted that she be buried in the backyard of the Beli Dvor, his Belgrade residence.
His best known wife was Jovanka Broz. Tito was just shy of his 59th birthday, while she was 27, when they finally married in April 1952, with state security chief Aleksandar Ranković as the best man. Their eventual marriage came about somewhat unexpectedly since Tito actually rejected her some years earlier when his confidante Ivan Krajacic brought her in originally. At that time, she was in her early 20s and Tito, objecting to her energetic personality, opted for the more mature opera singer Zinka Kunc instead. Not one to be discouraged easily, Jovanka continued working at Beli Dvor, where she managed the staff and eventually got another chance after Tito's strange relationship with Zinka failed. Since Jovanka was the only female companion he married while in power, she also went down in history as Yugoslavia's first lady. Their relationship was not a happy one, however. It had gone through many, often public, ups and downs with episodes of infidelities and even allegations of preparation for a coup d'état by the latter pair. Certain unofficial reports suggest Tito and Jovanka even formally divorced in the late 1970s, shortly before his death. However, during Tito's funeral she was officially present as his wife, and later claimed rights for inheritance. The couple did not have any children.
Tito's notable grandchildren include Aleksandra Broz, a prominent theatre director in Croatia; Svetlana Broz, a cardiologist and writer in Bosnia-Herzegovina; and Josip "Joška" Broz, Edvard Broz and Natali Klasevski, an artisan of Bosnia-Herzegovina.
As the President, Tito had access to extensive (state-owned) property associated with the office, and maintained a lavish lifestyle. In Belgrade he resided in the official residence, the Beli dvor, and maintained a separate private home. The Brijuni islands were the site of the State Summer Residence from 1949 on. The pavilion was designed by Jože Plečnik, and included a zoo. Close to 100 foreign heads of state were to visit Tito at the island residence, along with film stars such as Elizabeth Taylor, Richard Burton, Sophia Loren, Carlo Ponti, and Gina Lollobrigida.
Another residence was maintained at Lake Bled, while the grounds at Karađorđevo were the site of "diplomatic hunts". By 1974 the Yugoslav President had at his disposal 32 official residences, larger and small, the yacht Galeb ("seagull"), a Boeing 727 as the presidential airplane, and the Blue Train. After Tito's death the presidential Boeing 727 was sold to Aviogenex, the Galeb remained docked in Montenegro, while the Blue Train was stored in a Serbian train shed for over two decades. While Tito was the person who held the office of president for by far the longest period, the associated property was not private and much of it continues to be in use by Yugoslav successor states, as public property, or maintained at the disposal of high-ranking officials.
As regards knowledge of languages, Tito replied that he spoke Serbo-Croatian, German, Russian, and some English. A biographer also stated that he spoke "Serbo-Croatian ... Russian, Czech, Slovenian ... German (with a Viennese accent) ... understands and reads French and Italian ... [and] also speaks Kirghiz."
In his youth Tito attended Catholic Sunday school, and was later an altar boy. After an incident where he was slapped and shouted at by a priest when he had difficulty assisting the priest to remove his vestments, Tito would not enter a church again. As an adult, he frequently declared that he was an atheist.
Every federal unit had a town or city with historic significance from the World War II period renamed to have Tito's name included. The largest of these was Titograd, now Podgorica, the capital city of Montenegro. With the exception of Titograd, the cities were renamed simply by the addition of the adjective "Tito's" ("Titov"). The cities were:
In the years after Tito's death up to nowadays, some people have disputed his identity. Tito's personal doctor, Aleksandar Matunović, wrote a book about Tito in which he also questioned his true origin, noting that Tito's habits and lifestyle could only mean that he was from an aristocratic family. Serbian journalist Vladan Dinić (born 1949), in Tito nije tito, includes several possible alternate identities of Tito.
In 2013 a lot of media coverage was given to unclassified NSA's study in Cryptologic Spectrum that concluded that Tito did not speak the language as a native, and had features of other Slavic languages (Russian and Polish). The hypothesis that "a non-Yugoslav, perhaps a Russian or a Pole" assumed Tito's identity was included. The report also notes Draža Mihailović's impressions of Tito's Russian origins.
However, the NSA's report was completely disproved by Croatian experts. The report failed to recognize that Tito was a native speaker of the very distinctive local Kajkavian dialect of Zagorje. The acute accent, present only in Croatian dialects, which Tito is perfectly pronouncing, is the strongest proof of Tito's belonging to Kajkavian dialect.
As the Communist Party was outlawed in Yugoslavia starting on 30 December 1920, Josip Broz took on many assumed names during his activity within the Party, including "Rudi", "Walter", and "Tito." Broz himself explains:
Josip Broz Tito received a total of 119 awards and decorations from 60 countries around the world (59 countries and Yugoslavia). 21 decorations were from Yugoslavia itself, 18 having been awarded once, and the Order of the National Hero on three occasions. Of the 98 international awards and decorations, 92 were received once, and three on two occasions (Order of the White Lion, Polonia Restituta, and Karl Marx). The most notable awards included the French Legion of Honour and National Order of Merit, the British Order of the Bath, the Soviet Order of Lenin, the Japanese Order of the Chrysanthemum, the German Federal Cross of Merit, and the Order of Merit of Italy.
The decorations were seldom displayed, however. After the Tito–Stalin split of 1948 and his inauguration as president in 1953, Tito rarely wore his uniform except when present in a military function, and then (with rare exception) only wore his Yugoslav ribbons for obvious practical reasons. The awards were displayed in full number only at his funeral in 1980. Tito's reputation as one of the Allied leaders of World War II, along with his diplomatic position as the founder of the Non-Aligned Movement, was primarily the cause of the favorable international recognition.
Some of the other foreign awards and decorations of Josip Broz Tito include Order of Merit, Order of Manuel Amador Guerrero, Order of Prince Henry, Order of Independence, Order of Merit, Order of the Nile, Order of the Condor of the Andes, Order of the Star of Romania, Order of the Gold Lion of the House of Nassau, Croix de Guerre, Order of the Cross of Grunwald, Czechoslovak War Cross, Decoration of Honour for Services to the Republic of Austria, Military Order of the White Lion, Nishan-e-Pakistan, Order of Al Rafidain, Order of Carol I, Order of Georgi Dimitrov, Order of Karl Marx, Order of Manuel Amador Guerrero, Order of Michael the Brave, Order of Pahlavi, Order of Sukhbaatar, Order of Suvorov, Order of the Liberator, Order of the October Revolution, Order of the Queen of Sheba, Order of the White Rose of Finland, Partisan Cross, Royal Order of Cambodia and Star of People's Friendship and Thiri Thudhamma Thingaha.[citation needed]
A database management system (DBMS) is a computer software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, PostgreSQL, Microsoft SQL Server, Oracle, Sybase, SAP HANA, and IBM DB2. A database is not generally portable across different DBMSs, but different DBMS can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one DBMS. Database management systems are often classified according to the database model that they support; the most popular database systems since the 1980s have all supported the relational model as represented by the SQL language.[disputed – discuss] Sometimes a DBMS is loosely referred to as a 'database'.
Formally, a "database" refers to a set of related data and the way it is organized. Access to these data is usually provided by a "database management system" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. RAID is used for recovery of data if any of the disks fail. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions. from databases before the inception of Structured Query Language (SQL). The data recovered was disparate, redundant and disorderly, since there was no proper method to fetch it and arrange it in a concrete structure.[citation needed]
A DBMS has evolved into a complex software system and its development typically requires thousands of human years of development effort.[a] Some general-purpose DBMSs such as Adabas, Oracle and DB2 have been undergoing upgrades since the 1970s. General-purpose DBMSs aim to meet the needs of as many applications as possible, which adds to the complexity. However, the fact that their development cost can be spread over a large number of users means that they are often the most cost-effective approach. However, a general-purpose DBMS is not always the optimal solution: in some cases a general-purpose DBMS may introduce unnecessary overhead. Therefore, there are many examples of systems that use special-purpose databases. A common example is an email system that performs many of the functions of a general-purpose DBMS such as the insertion and deletion of messages composed of various items of data or associating messages with a particular email address; but these functions are limited to what is required to handle email and don't provide the user with all of the functionality that would be available using a general-purpose DBMS.
Many other databases have application software that accesses the database on behalf of end-users, without exposing the DBMS interface directly. Application programmers may use a wire protocol directly, or more likely through an application programming interface. Database designers and database administrators interact with the DBMS through dedicated interfaces to build and maintain the applications' databases, and thus need some more knowledge and understanding about how DBMSs operate and the DBMSs' external interfaces and tuning parameters.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2015[update] they remain dominant : IBM DB2, Oracle, MySQL and Microsoft SQL Server are the top DBMS. The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.[citation needed]
As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the "Database Task Group" within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971 the Database Task Group delivered their standard, which generally became known as the "CODASYL approach", and soon a number of commercial products based on this approach entered the market.
IBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed, and Bachman's 1973 Turing Award presentation was The Programmer as Navigator. IMS is classified[by whom?] as a hierarchical database. IDMS and Cincom Systems' TOTAL database are classified as network databases. IMS remains in use as of 2014[update].
In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to use a "table" of fixed-length records, with each table used for a different type of entity. A linked-list system would be very inefficient when storing "sparse" databases where some of the data for any one record could be left empty. The relational model solved this by splitting the data into a series of normalized tables (or relations), with optional elements being moved out of the main table to where they would take up room only if needed. Data may be freely inserted, deleted and edited in these tables, with the DBMS doing whatever maintenance needed to present a table view to the application/user.
The relational model also allowed the content of the database to evolve without constant rewriting of links and pointers. The relational part comes from entities referencing other entities in what is known as one-to-many relationship, like a traditional hierarchical model, and many-to-many relationship, like a navigational (network) model. Thus, a relational model can express both hierarchical and navigational models, as well as its native tabular model, allowing for pure or combined modeling in terms of these three models, as the application requires.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach all of this data would be placed in a single record, and unused items would simply not be placed in the database. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
Linking the information back together is the key to this system. In the relational model, some bit of information was used as a "key", uniquely defining a particular record. When information was being collected about a user, information stored in the optional tables would be found by searching for this key. For instance, if the login name of a user is unique, addresses and phone numbers for that user would be recorded with the login name as its key. This simple "re-linking" of related data back into a single collection is something that traditional computer languages are not designed for.
Just as the navigational approach would require programs to loop in order to collect records, the relational approach would require loops to collect information about any one record. Codd's solution to the necessary looping was a set-oriented language, a suggestion that would later spawn the ubiquitous SQL. Using a branch of mathematics known as tuple calculus, he demonstrated that such a system could support all the operations of normal databases (inserting, updating etc.) as well as providing a simple system for finding and returning sets of data in a single operation.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).
IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (DB2).
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff the creator of dBASE stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation." dBASE was one of the top selling software titles in the 1980s and early 1990s.
The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields. The term "object-relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object-relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object-relational mappings (ORMs) attempt to solve the same problem.

XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.
In recent years there was a high demand for massively distributed databases with high partition tolerance but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity-relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organisation, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modelling notation used to express that design.)
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like. This is often called physical database design. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
While there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different views of the company's database.
The conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often utilizing the operating systems' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or utilizing specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations it is desirable to move, migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically a DBMS vendor provides tools to help importing databases from other popular DBMSs.
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When this state is needed, i.e., when it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are utilized to restore that state.
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.
The law of the United States comprises many levels of codified and uncodified forms of law, of which the most important is the United States Constitution, the foundation of the federal government of the United States. The Constitution sets out the boundaries of federal law, which consists of acts of Congress, treaties ratified by the Senate, regulations promulgated by the executive branch, and case law originating from the federal judiciary. The United States Code is the official compilation and codification of general and permanent federal statutory law.
Federal law and treaties, so long as they are in accordance with the Constitution, preempt conflicting state and territorial laws in the 50 U.S. states and in the territories. However, the scope of federal preemption is limited because the scope of federal power is not universal. In the dual-sovereign system of American federalism (actually tripartite because of the presence of Indian reservations), states are the plenary sovereigns, each with their own constitution, while the federal sovereign possesses only the limited supreme authority enumerated in the Constitution. Indeed, states may grant their citizens broader rights than the federal Constitution as long as they do not infringe on any federal constitutional rights. Thus, most U.S. law (especially the actual "living law" of contract, tort, property, criminal, and family law experienced by the majority of citizens on a day-to-day basis) consists primarily of state law, which can and does vary greatly from one state to the next.
Notably, a statute does not disappear automatically merely because it has been found unconstitutional; it must be deleted by a subsequent statute. Many federal and state statutes have remained on the books for decades after they were ruled to be unconstitutional. However, under the principle of stare decisis, no sensible lower court will enforce an unconstitutional statute, and any court that does so will be reversed by the Supreme Court. Conversely, any court that refuses to enforce a constitutional statute (where such constitutionality has been expressly established in prior cases) will risk reversal by the Supreme Court.
Notably, the most broadly influential innovation of 20th-century American tort law was the rule of strict liability for defective products, which originated with judicial glosses on the law of warranty. In 1963, Roger J. Traynor of the Supreme Court of California threw away legal fictions based on warranties and imposed strict liability for defective products as a matter of public policy in the landmark case of Greenman v. Yuba Power Products. The American Law Institute subsequently adopted a slightly different version of the Greenman rule in Section 402A of the Restatement (Second) of Torts, which was published in 1964 and was very influential throughout the United States. Outside the U.S., the rule was adopted by the European Economic Community in the Product Liability Directive of July 1985 by Australia in July 1992 and by Japan in June 1994.
Tort law covers the entire imaginable spectrum of wrongs which humans can inflict upon each other, and of course, partially overlaps with wrongs also punishable by criminal law. Although the American Law Institute has attempted to standardize tort law through the development of several versions of the Restatement of Torts, many states have chosen to adopt only certain sections of the Restatements and to reject others. Thus, because of its immense size and diversity, American tort law cannot be easily summarized.
However, it is important to understand that despite the presence of reception statutes, much of contemporary American common law has diverged significantly from English common law. The reason is that although the courts of the various Commonwealth nations are often influenced by each other's rulings, American courts rarely follow post-Revolution Commonwealth rulings unless there is no American ruling on point, the facts and law at issue are nearly identical, and the reasoning is strongly persuasive.
The actual substance of English law was formally "received" into the United States in several ways. First, all U.S. states except Louisiana have enacted "reception statutes" which generally state that the common law of England (particularly judge-made law) is the law of the state to the extent that it is not repugnant to domestic law or indigenous conditions. Some reception statutes impose a specific cutoff date for reception, such as the date of a colony's founding, while others are deliberately vague. Thus, contemporary U.S. courts often cite pre-Revolution cases when discussing the evolution of an ancient judge-made common law principle into its modern form, such as the heightened duty of care traditionally imposed upon common carriers.
Early on, American courts, even after the Revolution, often did cite contemporary English cases. This was because appellate decisions from many American courts were not regularly reported until the mid-19th century; lawyers and judges, as creatures of habit, used English legal materials to fill the gap. But citations to English decisions gradually disappeared during the 19th century as American courts developed their own principles to resolve the legal problems of the American people. The number of published volumes of American reports soared from eighteen in 1810 to over 8,000 by 1910. By 1879 one of the delegates to the California constitutional convention was already complaining: "Now, when we require them to state the reasons for a decision, we do not mean they shall write a hundred pages of detail. We [do] not mean that they shall include the small cases, and impose on the country all this fine judicial literature, for the Lord knows we have got enough of that already."
Federal law originates with the Constitution, which gives Congress the power to enact statutes for certain limited purposes like regulating interstate commerce. The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations generally also carry the force of law under the Chevron doctrine. Many lawsuits turn on the meaning of a federal statute or regulation, and judicial interpretations of such meaning carry legal force under the principle of stare decisis.
During the 18th and 19th centuries, federal law traditionally focused on areas where there was an express grant of power to the federal government in the federal Constitution, like the military, money, foreign relations (especially international treaties), tariffs, intellectual property (specifically patents and copyrights), and mail. Since the start of the 20th century, broad interpretations of the Commerce and Spending Clauses of the Constitution have enabled federal law to expand into areas like aviation, telecommunications, railroads, pharmaceuticals, antitrust, and trademarks. In some areas, like aviation and railroads, the federal government has developed a comprehensive scheme that preempts virtually all state law, while in others, like family law, a relatively small number of federal statutes (generally covering interstate and international situations) interacts with a much larger body of state law. In areas like antitrust, trademark, and employment law, there are powerful laws at both the federal and state levels that coexist with each other. In a handful of areas like insurance, Congress has enacted laws expressly refusing to regulate them as long as the states have laws regulating them (see, e.g., the McCarran-Ferguson Act).
After the President signs a bill into law (or Congress enacts it over his veto), it is delivered to the Office of the Federal Register (OFR) of the National Archives and Records Administration (NARA) where it is assigned a law number, and prepared for publication as a slip law. Public laws, but not private laws, are also given legal statutory citation by the OFR. At the end of each session of Congress, the slip laws are compiled into bound volumes called the United States Statutes at Large, and they are known as session laws. The Statutes at Large present a chronological arrangement of the laws in the exact order that they have been enacted.
Congress often enacts statutes that grant broad rulemaking authority to federal agencies. Often, Congress is simply too gridlocked to draft detailed statutes that explain how the agency should react to every possible situation, or Congress believes the agency's technical specialists are best equipped to deal with particular fact situations as they arise. Therefore, federal agencies are authorized to promulgate regulations. Under the principle of Chevron deference, regulations normally carry the force of law as long as they are based on a reasonable interpretation of the relevant statutes.
The difficult question is whether federal judicial power extends to formulating binding precedent through strict adherence to the rule of stare decisis. This is where the act of deciding a case becomes a limited form of lawmaking in itself, in that an appellate court's rulings will thereby bind itself and lower courts in future cases (and therefore also impliedly binds all persons within the court's jurisdiction). Prior to a major change to federal court rules in 2007, about one-fifth of federal appellate cases were published and thereby became binding precedents, while the rest were unpublished and bound only the parties to each case.
As federal judge Alex Kozinski has pointed out, binding precedent as we know it today simply did not exist at the time the Constitution was framed. Judicial decisions were not consistently, accurately, and faithfully reported on both sides of the Atlantic (reporters often simply rewrote or failed to publish decisions which they disliked), and the United Kingdom lacked a coherent court hierarchy prior to the end of the 19th century. Furthermore, English judges in the eighteenth century subscribed to now-obsolete natural law theories of law, by which law was believed to have an existence independent of what individual judges said. Judges saw themselves as merely declaring the law which had always theoretically existed, and not as making the law. Therefore, a judge could reject another judge's opinion as simply an incorrect statement of the law, in the way that scientists regularly reject each other's conclusions as incorrect statements of the laws of science.
Unlike the situation with the states, there is no plenary reception statute at the federal level that continued the common law and thereby granted federal courts the power to formulate legal precedent like their English predecessors. Federal courts are solely creatures of the federal Constitution and the federal Judiciary Acts. However, it is universally accepted that the Founding Fathers of the United States, by vesting "judicial power" into the Supreme Court and the inferior federal courts in Article Three of the United States Constitution, thereby vested in them the implied judicial power of common law courts to formulate persuasive precedent; this power was widely accepted, understood, and recognized by the Founding Fathers at the time the Constitution was ratified. Several legal scholars have argued that the federal judicial power to decide "cases or controversies" necessarily includes the power to decide the precedential effect of those cases and controversies.
In turn, according to Kozinski's analysis, the contemporary rule of binding precedent became possible in the U.S. in the nineteenth century only after the creation of a clear court hierarchy (under the Judiciary Acts), and the beginning of regular verbatim publication of U.S. appellate decisions by West Publishing. The rule gradually developed, case-by-case, as an extension of the judiciary's public policy of effective judicial administration (that is, in order to efficiently exercise the judicial power). The rule of precedent is generally justified today as a matter of public policy, first, as a matter of fundamental fairness, and second, because in the absence of case law, it would be completely unworkable for every minor issue in every legal case to be briefed, argued, and decided from first principles (such as relevant statutes, constitutional provisions, and underlying public policies), which in turn would create hopeless inefficiency, instability, and unpredictability, and thereby undermine the rule of law.
Under the doctrine of Erie Railroad Co. v. Tompkins (1938), there is no general federal common law. Although federal courts can create federal common law in the form of case law, such law must be linked one way or another to the interpretation of a particular federal constitutional provision, statute, or regulation (which in turn was enacted as part of the Constitution or after). Federal courts lack the plenary power possessed by state courts to simply make up law, which the latter are able to do in the absence of constitutional or statutory provisions replacing the common law. Only in a few narrow limited areas, like maritime law, has the Constitution expressly authorized the continuation of English common law at the federal level (meaning that in those areas federal courts can continue to make law as they see fit, subject to the limitations of stare decisis).
The other major implication of the Erie doctrine is that federal courts cannot dictate the content of state law when there is no federal issue (and thus no federal supremacy issue) in a case. When hearing claims under state law pursuant to diversity jurisdiction, federal trial courts must apply the statutory and decisional law of the state in which they sit, as if they were a court of that state, even if they believe that the relevant state law is irrational or just bad public policy. And under Erie, deference is one-way only: state courts are not bound by federal interpretations of state law.
The fifty American states are separate sovereigns, with their own state constitutions, state governments, and state courts. All states have a legislative branch which enacts state statutes, an executive branch that promulgates state regulations pursuant to statutory authorization, and a judicial branch that applies, interprets, and occasionally overturns both state statutes and regulations, as well as local ordinances. They retain plenary power to make laws covering anything not preempted by the federal Constitution, federal statutes, or international treaties ratified by the federal Senate. Normally, state supreme courts are the final interpreters of state constitutions and state law, unless their interpretation itself presents a federal issue, in which case a decision may be appealed to the U.S. Supreme Court by way of a petition for writ of certiorari. State laws have dramatically diverged in the centuries since independence, to the extent that the United States cannot be regarded as one legal system as to the majority of types of law traditionally under state control, but must be regarded as 50 separate systems of tort law, family law, property law, contract law, criminal law, and so on.
Most cases are litigated in state courts and involve claims and defenses under state laws. In a 2012 report, the National Center for State Courts' Court Statistics Project found that state trial courts received 103.5 million newly filed cases in 2010, which consisted of 56.3 million traffic cases, 20.4 million criminal cases, 19.0 million civil cases, 5.9 million domestic relations cases, and 1.9 million juvenile cases. In 2010, state appellate courts received 272,795 new cases. By way of comparison, all federal district courts in 2010 together received only about 282,000 new civil cases, 77,000 new criminal cases, and 1.5 million bankruptcy cases, while federal appellate courts received 56,000 new cases.
The law of criminal procedure in the United States consists of a massive overlay of federal constitutional case law interwoven with the federal and state statutes that actually provide the foundation for the creation and operation of law enforcement agencies and prison systems as well as the proceedings in criminal trials. Due to the perennial inability of legislatures in the U.S. to enact statutes that would actually force law enforcement officers to respect the constitutional rights of criminal suspects and convicts, the federal judiciary gradually developed the exclusionary rule as a method to enforce such rights. In turn, the exclusionary rule spawned a family of judge-made remedies for the abuse of law enforcement powers, of which the most famous is the Miranda warning. The writ of habeas corpus is often used by suspects and convicts to challenge their detention, while the Civil Rights Act of 1871 and Bivens actions are used by suspects to recover tort damages for police brutality.
The law of civil procedure governs process in all judicial proceedings involving lawsuits between private parties. Traditional common law pleading was replaced by code pleading in 24 states after New York enacted the Field Code in 1850 and code pleading in turn was subsequently replaced again in most states by modern notice pleading during the 20th century. The old English division between common law and equity courts was abolished in the federal courts by the adoption of the Federal Rules of Civil Procedure in 1938; it has also been independently abolished by legislative acts in nearly all states. The Delaware Court of Chancery is the most prominent of the small number of remaining equity courts.
New York, Illinois, and California are the most significant states that have not adopted the FRCP. Furthermore, all three states continue to maintain most of their civil procedure laws in the form of codified statutes enacted by the state legislature, as opposed to court rules promulgated by the state supreme court, on the ground that the latter are undemocratic. But certain key portions of their civil procedure laws have been modified by their legislatures to bring them closer to federal civil procedure.
Generally, American civil procedure has several notable features, including extensive pretrial discovery, heavy reliance on live testimony obtained at deposition or elicited in front of a jury, and aggressive pretrial "law and motion" practice designed to result in a pretrial disposition (that is, summary judgment) or a settlement. U.S. courts pioneered the concept of the opt-out class action, by which the burden falls on class members to notify the court that they do not wish to be bound by the judgment, as opposed to opt-in class actions, where class members must join into the class. Another unique feature is the so-called American Rule under which parties generally bear their own attorneys' fees (as opposed to the English Rule of "loser pays"), though American legislators and courts have carved out numerous exceptions.
Criminal law involves the prosecution by the state of wrongful acts which are considered to be so serious that they are a breach of the sovereign's peace (and cannot be deterred or remedied by mere lawsuits between private parties). Generally, crimes can result in incarceration, but torts (see below) cannot. The majority of the crimes committed in the United States are prosecuted and punished at the state level. Federal criminal law focuses on areas specifically relevant to the federal government like evading payment of federal income tax, mail theft, or physical attacks on federal officials, as well as interstate crimes like drug trafficking and wire fraud.
Some states distinguish between two levels: felonies and misdemeanors (minor crimes). Generally, most felony convictions result in lengthy prison sentences as well as subsequent probation, large fines, and orders to pay restitution directly to victims; while misdemeanors may lead to a year or less in jail and a substantial fine. To simplify the prosecution of traffic violations and other relatively minor crimes, some states have added a third level, infractions. These may result in fines and sometimes the loss of one's driver's license, but no jail time.
Contract law covers obligations established by agreement (express or implied) between private parties. Generally, contract law in transactions involving the sale of goods has become highly standardized nationwide as a result of the widespread adoption of the Uniform Commercial Code. However, there is still significant diversity in the interpretation of other kinds of contracts, depending upon the extent to which a given state has codified its common law of contracts or adopted portions of the Restatement (Second) of Contracts.
Solar energy is radiant light and heat from the Sun harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture and artificial photosynthesis.
The Earth receives 174,000 terawatts (TW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most people around the world live in areas with insolation levels of 150 to 300 watts per square meter or 3.5 to 7.0 kWh/m2 per day.
Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anti-cyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived.
The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined,
Solar technologies are broadly characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on distance from the equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all renewable energies, other than geothermal and tidal, derive their energy from the Sun in a direct or indirect way.
Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand side technologies.
In 1897, Frank Shuman, a U.S. inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water, and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912.
Shuman built the world’s first solar thermal power station in Maadi, Egypt, between 1912 and 1913. Shuman’s plant used parabolic troughs to power a 45–52 kilowatts (60–70 hp) engine that pumped more than 22,000 litres (4,800 imp gal; 5,800 US gal) of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman’s vision and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:
Solar hot water systems use sunlight to heat water. In low geographical latitudes (below 40 degrees) from 60 to 70% of the domestic hot water use with temperatures up to 60 °C can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools.
As of 2007, the total installed capacity of solar hot water systems is approximately 154 thermal gigawatt (GWth). China is the world leader in their deployment with 70 GWth installed as of 2006 and a long-term goal of 210 GWth by 2020. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada and Australia heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GWth as of 2005.
In the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ/yr) of the energy used in commercial buildings and nearly 50% (10.1 EJ/yr) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy.
Thermal mass is any material that can be used to store heat—heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting and shading conditions. When properly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment.
A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses.
Deciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain.
Solar cookers use sunlight for cooking, drying and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of 90–150 °C (194–302 °F). Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of 315 °C (599 °F) and above but require direct light to function properly and must be repositioned to track the Sun.
Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the "right to dry" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to 22 °C (40 °F) and deliver outlet temperatures of 45–60 °C (113–140 °F). The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of 35,000 square metres (380,000 sq ft) had been installed worldwide, including an 860 m2 (9,300 sq ft) collector in Costa Rica used for drying coffee beans and a 1,300 m2 (14,000 sq ft) collector in Coimbatore, India, used for drying marigolds.
Solar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of 4,700 m2 (51,000 sq ft), could produce up to 22,700 L (5,000 imp gal; 6,000 US gal) per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications.
Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water.
Solar energy may be used in a water stabilisation pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable.
Solar power is anticipated to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16 and 11 percent to the global overall consumption, respectively.
Commercial CSP plants were first developed in the 1980s. Since 1985 the eventually 354 MW SEGS CSP installation, in the Mojave Desert of California, is the largest solar power plant in the world. Other large CSP plants include the 150 MW Solnova Solar Power Station and the 100 MW Andasol solar power station, both in Spain. The 250 MW Agua Caliente Solar Project, in the United States, and the 221 MW Charanka Solar Park in India, are the world’s largest photovoltaic plants. Solar projects exceeding 1 GW are being developed, but most of the deployed photovoltaics are in small rooftop arrays of less than 5 kW, which are grid connected using net metering and/or a feed-in tariff. In 2013 solar generated less than 1% of the worlds total grid electricity.
In the last two decades, photovoltaics (PV), also known as solar PV, has evolved from a pure niche market of small scale applications towards becoming a mainstream electricity source. A solar cell is a device that converts light directly into electricity using the photoelectric effect. The first solar cell was constructed by Charles Fritts in the 1880s. In 1931 a German engineer, Dr Bruno Lange, developed a photo cell using silver selenide in place of copper oxide. Although the prototype selenium cells converted less than 1% of incident light into electricity, both Ernst Werner von Siemens and James Clerk Maxwell recognized the importance of this discovery. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the crystalline silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%. By 2012 available efficiencies exceed 20% and the maximum efficiency of research photovoltaics is over 40%.
Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.
The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans and switchable windows can complement passive design and improve system performance.
Urban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures are a result of increased absorption of the Solar light by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white and plant trees. Using these methods, a hypothetical "cool communities" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 °C at an estimated cost of US$1 billion, giving estimated total annual benefits of US$530 million from reduced air-conditioning costs and healthcare savings.
Agriculture and horticulture seek to optimize the capture of solar energy in order to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vinters, who use the energy generated by solar panels to power grape presses.
Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today, and plastic transparent materials have also been used to similar effect in polytunnels and row covers.
Development of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over 3,021 kilometres (1,877 mi) across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was 67 kilometres per hour (42 mph) and by 2007 the winner's average speed had improved to 90.87 kilometres per hour (56.46 mph). The North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles.
In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar powered crossing of the Pacific Ocean, and the sun21 catamaran made the first solar powered crossing of the Atlantic Ocean in the winter of 2006–2007. There were plans to circumnavigate the globe in 2010.
In 1974, the unmanned AstroFlight Sunrise plane made the first solar flight. On 29 April 1979, the Solar Riser made the first flight in a solar-powered, fully controlled, man carrying flying machine, reaching an altitude of 40 feet (12 m). In 1980, the Gossamer Penguin made the first piloted flights powered solely by photovoltaics. This was quickly followed by the Solar Challenger which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the Pathfinder (1997) and subsequent designs, culminating in the Helios which set the altitude record for a non-rocket-propelled aircraft at 29,524 metres (96,864 ft) in 2001. The Zephyr, developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. As of 2015, Solar Impulse, an electric aircraft, is currently circumnavigating the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The designed allows the aircraft to remain airborne for 36 hours.
Solar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 –  the splitting of sea water providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. Another vision involves all human structures covering the earth's surface (i.e., roads, vehicles and buildings) doing photosynthesis more efficiently than plants.
Hydrogen production technologies been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (2,300–2,600 °C or 4,200–4,700 °F). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above 1,200 °C (2,200 °F). This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen.
Thermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements.
Phase change materials such as paraffin wax and Glauber's salt are another thermal storage media. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately 64 °C or 147 °F). The "Dover House" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948. Solar energy can also be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity and can deliver heat at temperatures compatible with conventional power systems. The Solar Two used this method of energy storage, allowing it to store 1.44 terajoules (400,000 kWh) in its 68 cubic metres storage tank with an annual storage efficiency of about 99%.
Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems a credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary.
Pumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator.
The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the US (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE).
Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s and growth rates have averaged 20% per year since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154 GW as of 2007.
The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces:
The International Organization for Standardization has established a number of standards relating to solar energy equipment. For example, ISO 9050 relates to glass in building while ISO 10217 relates to the materials used in solar water heaters.
It is an important source of renewable energy and its technologies are broadly characterized as either passive solar or active solar depending on the way they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air.
The large magnitude of solar energy available makes it a highly appealing source of electricity. The United Nations Development Programme in its 2000 World Energy Assessment found that the annual potential of solar energy was 1,575–49,837 exajoules (EJ). This is several times larger than the total world energy consumption, which was 559.8 EJ in 2012.
In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".
The potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limits the amount of solar energy that we can acquire.
Geography effects solar energy potential because areas that are closer to the equator have a greater amount of solar radiation. However, the use of photovoltaics that can follow the position of the sun can significantly increase the solar energy potential in areas that are farther from the equator. Time variation effects the potential of solar energy because during the nighttime there is little solar radiation on the surface of the Earth for solar panels to absorb. This limits the amount of energy that solar panels can absorb in one day. Cloud cover can effect the potential of solar panels because clouds block incoming light from the sun and reduce the light available for solar cells.
In addition, land availability has a large effect on the available solar energy because solar panels can only be set up on land that is unowned and suitable for solar panels. Roofs have been found to be a suitable place for solar cells, as many people have discovered that they can collect energy directly from their homes this way. Other areas that are suitable for solar cells are lands that are unowned by businesses where solar plants can be established.
In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year that took into account factors such as insolation, cloud cover, and the land that is usable by humans. The estimate found that solar energy has a global potential of 1,575–49,837 EJ per year (see table below).
Solar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). CSP systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. PV converts light into electric current using the photoelectric effect.
Sunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth.
A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high.
Beginning with the surge in coal use which accompanied the Industrial Revolution, energy consumption has steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum.
In 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water and concentrated solar power could provide a third of the world’s energy by 2060 if politicians commit to limiting climate change. The energy from the sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. "The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale".
The Heian period (平安時代, Heian jidai?) is the last division of classical Japanese history, running from 794 to 1185. The period is named after the capital city of Heian-kyō, or modern Kyōto. It is the period in Japanese history when Buddhism, Taoism and other Chinese influences were at their height. The Heian period is also considered the peak of the Japanese imperial court and noted for its art, especially poetry and literature. Although the Imperial House of Japan had power on the surface, the real power was in the hands of the Fujiwara clan, a powerful aristocratic family who had intermarried with the imperial family. Many emperors actually had mothers from the Fujiwara family. Heian (平安?) means "peace" in Japanese.
The Heian period was preceded by the Nara period and began in 794 A.D after the movement of the capital of Japan to Heian-kyō (present day Kyōto京都), by the 50th emperor, Emperor Kanmu. Kanmu first tried to move the capital to Nagaoka-kyō, but a series of disasters befell the city, prompting the emperor to relocate the capital a second time, to Heian. The Heian Period is considered a high point in Japanese culture that later generations have always admired. The period is also noted for the rise of the samurai class, which would eventually take power and start the feudal period of Japan.
Nominally, sovereignty lay in the emperor but in fact power was wielded by the Fujiwara nobility. However, to protect their interests in the provinces, the Fujiwara and other noble families required guards, police and soldiers. The warrior class made steady political gains throughout the Heian period. As early as 939 A.D, Taira no Masakado threatened the authority of the central government, leading an uprising in the eastern province of Hitachi, and almost simultaneously, Fujiwara no Sumitomo rebelled in the west. Still, a true military takeover of the Japanese government was centuries away, when much of the strength of the government would lie within the private armies of the shogunate.
When Emperor Kammu moved the capital to Heian-kyō (Kyōto), which remained the imperial capital for the next 1,000 years, he did so not only to strengthen imperial authority but also to improve his seat of government geopolitically. Nara was abandoned after only 70 years in part due to the ascendancy of Dōkyō and the encroaching secular power of the Buddhist institutions there. Kyōto had good river access to the sea and could be reached by land routes from the eastern provinces. The early Heian period (784–967) continued Nara culture; the Heian capital was patterned on the Chinese Tang capital at Chang'an, as was Nara, but on a larger scale than Nara. Kammu endeavoured to improve the Tang-style administrative system which was in use. Known as the ritsuryō, this system attempted to recreate the Tang imperium in Japan, despite the "tremendous differences in the levels of development between the two countries". Despite the decline of the Taika-Taihō reforms, imperial government was vigorous during the early Heian period. Indeed, Kammu's avoidance of drastic reform decreased the intensity of political struggles, and he became recognized as one of Japan's most forceful emperors.
Although Kammu had abandoned universal conscription in 792, he still waged major military offensives to subjugate the Emishi, possible descendants of the displaced Jōmon, living in northern and eastern Japan. After making temporary gains in 794, in 797 Kammu appointed a new commander, Sakanoue no Tamuramaro, under the title Sei-i Taishōgun (Barbarian-subduing generalissimo). By 801 the shogun had defeated the Emishi and had extended the imperial domains to the eastern end of Honshū. Imperial control over the provinces was tenuous at best, however. In the ninth and tenth centuries, much authority was lost to the great families, who disregarded the Chinese-style land and tax systems imposed by the government in Kyoto. Stability came to Japan, but, even though succession was ensured for the imperial family through heredity, power again concentrated in the hands of one noble family, the Fujiwara which also helped Japan develop more.
Following Kammu's death in 806 and a succession struggle among his sons, two new offices were established in an effort to adjust the Taika-Taihō administrative structure. Through the new Emperor's Private Office, the emperor could issue administrative edicts more directly and with more self-assurance than before. The new Metropolitan Police Board replaced the largely ceremonial imperial guard units. While these two offices strengthened the emperor's position temporarily, soon they and other Chinese-style structures were bypassed in the developing state. In 838 the end of the imperial-sanctioned missions to Tang China, which had begun in 630, marked the effective end of Chinese influence. Tang China was in a state of decline, and Chinese Buddhists were severely persecuted, undermining Japanese respect for Chinese institutions. Japan began to turn inward.
As the Soga clan had taken control of the throne in the sixth century, the Fujiwara by the ninth century had intermarried with the imperial family, and one of their members was the first head of the Emperor's Private Office. Another Fujiwara became regent, Sesshō for his grandson, then a minor emperor, and yet another was appointed Kampaku. Toward the end of the ninth century, several emperors tried, but failed, to check the Fujiwara. For a time, however, during the reign of Emperor Daigo (897-930), the Fujiwara regency was suspended as he ruled directly.
Nevertheless, the Fujiwara were not demoted by Daigo but actually became stronger during his reign. Central control of Japan had continued to decline, and the Fujiwara, along with other great families and religious foundations, acquired ever larger shōen and greater wealth during the early tenth century. By the early Heian period, the shōen had obtained legal status, and the large religious establishments sought clear titles in perpetuity, waiver of taxes, and immunity from government inspection of the shōen they held. Those people who worked the land found it advantageous to transfer title to shōen holders in return for a share of the harvest. People and lands were increasingly beyond central control and taxation, a de facto return to conditions before the Taika Reform.
Despite their usurpation of imperial authority, the Fujiwara presided over a period of cultural and artistic flowering at the imperial court and among the aristocracy. There was great interest in graceful poetry and vernacular literature. Two types of phonetic Japanese script: katakana, a simplified script that was developed by using parts of Chinese characters, was abbreviated to hiragana, a cursive syllabary with a distinct writing method that was uniquely Japanese. Hiragana gave written expression to the spoken word and, with it, to the rise in Japan's famous vernacular literature, much of it written by court women who had not been trained in Chinese as had their male counterparts. Three late tenth century and early eleventh century women presented their views of life and romance at the Heian court in Kagerō Nikki by "the mother of Fujiwara Michitsuna", The Pillow Book by Sei Shōnagon and The Tale of Genji by Murasaki Shikibu. Indigenous art also flourished under the Fujiwara after centuries of imitating Chinese forms. Vividly colored yamato-e, Japanese style paintings of court life and stories about temples and shrines became common in the mid- and late Heian periods, setting patterns for Japanese art to this day.
As culture flourished, so did decentralization. Whereas the first phase of shōen development in the early Heian period had seen the opening of new lands and the granting of the use of lands to aristocrats and religious institutions, the second phase saw the growth of patrimonial "house governments," as in the old clan system. (In fact, the form of the old clan system had remained largely intact within the great old centralized government.) New institutions were now needed in the face of social, economic, and political changes. The Taihō Code lapsed, its institutions relegated to ceremonial functions. Family administrations now became public institutions. As the most powerful family, the Fujiwara governed Japan and determined the general affairs of state, such as succession to the throne. Family and state affairs were thoroughly intermixed, a pattern followed among other families, monasteries, and even the imperial family. Land management became the primary occupation of the aristocracy, not so much because direct control by the imperial family or central government had declined but more from strong family solidarity and a lack of a sense of Japan as a single nation.
Under the early courts, when military conscription had been centrally controlled, military affairs had been taken out of the hands of the provincial aristocracy. But as the system broke down after 792, local power holders again became the primary source of military strength. The re-establishment of an efficient military system was made gradually through a process of trial-and-error. At that time the imperial court did not possess an army but rather relied on an organization of professional warriors composed mainly of oryoshi, which were appointed to an individual province and tsuibushi, which were appointed over imperial circuits or for specific tasks. This gave rise to the Japanese military class. Nonetheless final authority rested with the imperial court.
Shōen holders had access to manpower and, as they obtained improved military technology (such as new training methods, more powerful bows, armor, horses, and superior swords) and faced worsening local conditions in the ninth century, military service became part of shōen life. Not only the shōen but also civil and religious institutions formed private guard units to protect themselves. Gradually, the provincial upper class was transformed into a new military elite based on the ideals of the bushi (warrior) or samurai (literally, one who serves).
Bushi interests were diverse, cutting across old power structures to form new associations in the tenth century. Mutual interests, family connections, and kinship were consolidated in military groups that became part of family administration. In time, large regional military families formed around members of the court aristocracy who had become prominent provincial figures. These military families gained prestige from connections to the imperial court and court-granted military titles and access to manpower. The Fujiwara family, Taira clan, and Minamoto clan were among the most prominent families supported by the new military class.
The Fujiwara controlled the throne until the reign of Emperor Go-Sanjō (1068-1073), the first emperor not born of a Fujiwara mother since the ninth century. Go-Sanjo, determined to restore imperial control through strong personal rule, implemented reforms to curb Fujiwara influence. He also established an office to compile and validate estate records with the aim of reasserting central control. Many shōen were not properly certified, and large landholders, like the Fujiwara, felt threatened with the loss of their lands. Go-Sanjo also established the In-no-cho (ja:院庁 Office of the Cloistered Emperor), which was held by a succession of emperors who abdicated to devote themselves to behind-the-scenes governance, or insei.
The In-no-cho filled the void left by the decline of Fujiwara power. Rather than being banished, the Fujiwara were mostly retained in their old positions of civil dictator and minister of the center while being bypassed in decision making. In time, many of the Fujiwara were replaced, mostly by members of the rising Minamoto family. While the Fujiwara fell into disputes among themselves and formed northern and southern factions, the insei system allowed the paternal line of the imperial family to gain influence over the throne. The period from 1086 to 1156 was the age of supremacy of the In-no-cho and of the rise of the military class throughout the country. Military might rather than civil authority dominated the government.
A struggle for succession in the mid-twelfth century gave the Fujiwara an opportunity to regain their former power. Fujiwara no Yorinaga sided with the retired emperor in a violent battle in 1156 against the heir apparent, who was supported by the Taira and Minamoto (Hōgen Rebellion). In the end, the Fujiwara were destroyed, the old system of government supplanted, and the insei system left powerless as bushi took control of court affairs, marking a turning point in Japanese history. In 1159, the Taira and Minamoto clashed (Heiji Rebellion), and a twenty-year period of Taira ascendancy began.
Taira Kiyomori emerged as the real power in Japan following the Minamoto's destruction, and he would remain in command for the next 20 years. He gave his daughter Tokuko in marriage to the young emperor Takakura, who died at only 19, leaving their infant son Antoku to succeed to the throne. Kiyomori filled no less than 50 government posts with his relatives, rebuilt the Inland Sea, and encouraged trade with Sung China. He also took aggressive actions to safeguard his power when necessary, including the removal and exile of 45 court officials and the razing of two troublesome temples, Todai-ji and Kofuku-ji.
With Yoritomo firmly established, the bakufu system that would govern Japan for the next seven centuries was in place. He appointed military governors, or daimyos, to rule over the provinces, and stewards, or jito to supervise public and private estates. Yoritomo then turned his attention to the elimination of the powerful Fujiwara family, which sheltered his rebellious brother Yoshitsune. Three years later, he was appointed shogun in Kyoto. One year before his death in 1199, Yoritomo expelled the teenage emperor Go-Toba from the throne. Two of Go-Toba's sons succeeded him, but they would also be removed by Yoritomo's successors to the shogunate.
Buddhism began to spread throughout Japan during the Heian period, primarily through two major esoteric sects, Tendai and Shingon. Tendai originated in China and is based on the Lotus Sutra, one of the most important sutras of Mahayana Buddhism; Saichō was key to its transmission to Japan. Shingon is the Japanese transmission of the Chinese Chen Yen school. Shingon, brought to Japan by the monk Kūkai, emphasizes Esoteric Buddhism. Both Kūkai and Saichō aimed to connect state and religion and establish support from the aristocracy, leading to the notion of 'aristocratic Buddhism'. An important element of Tendai doctrine was the suggestion that enlightenment was accessible to "every creature". Saichō also sought independent ordination for Tendai monks. A close relationship developed between the Tendai monastery complex on Mount Hiei and the imperial court in its new capital at the foot of the mountain. As a result, Tendai emphasized great reverence for the emperor and the nation. Kammu himself was a notable patron of the otherworldly Tendai sect, which rose to great power over the ensuing centuries. Kūkai greatly impressed the emperors who succeeded Emperor Kammu, and also generations of Japanese, not only with his holiness but also with his poetry, calligraphy, painting, and sculpture. Shingon, through its use of "rich symbols, rituals and mandalas" held a wide-ranging appeal.
Poetry, in particular, was a staple of court life. Nobles and ladies-in-waiting were expected to be well versed in the art of writing poetry as a mark of their status. Every occasion could call for the writing of a verse, from the birth of a child to the coronation of an emperor, or even a pretty scene of nature. A well-written poem or haiku could easily make or break one's reputation, and often was a key part of social interaction.Almost as important was the choice of calligraphy, or handwriting, used. The Japanese of this period believed handwriting could reflect the condition of a person's soul: therefore, poor or hasty writing could be considered a sign of poor breeding. Whether the script was Chinese or Japanese, good writing and artistic skill was paramount to social reputation when it came to poetry. Sei Shonagon mentions in her Pillow Book that when a certain courtesan tried to ask her advice about how to write a poem to the empress Sadako, she had to politely rebuke him because his writing was so poor.
The lyrics of the modern Japanese national anthem, Kimi ga Yo, were written in the Heian period, as was The Tale of Genji by Murasaki Shikibu, one of the first novels ever written. Murasaki Shikibu's contemporary and rival Sei Shōnagon's revealing observations and musings as an attendant in the Empress' court were recorded collectively as The Pillow Book in the 990s, which revealed the quotidian capital lifestyle. The Heian period produced a flowering of poetry including works of Ariwara no Narihira, Ono no Komachi, Izumi Shikibu, Murasaki Shikibu, Saigyō and Fujiwara no Teika. The famous Japanese poem known as the Iroha (いろは), of uncertain authorship, was also written during the Heian period.
While on one hand the Heian period was an unusually long period of peace, it can also be argued that the period weakened Japan economically and led to poverty for all but a tiny few of its inhabitants. The control of rice fields provided a key source of income for families such as the Fujiwara and were a fundamental base for their power. The aristocratic beneficiaries of Heian culture, the Ryōmin (良民 "Good People") numbered about five thousand in a land of perhaps five million. One reason the samurai were able to take power was that the ruling nobility proved incompetent at managing Japan and its provinces. By the year 1000 the government no longer knew how to issue currency and money was gradually disappearing. Instead of a fully realised system of money circulation, rice was the primary unit of exchange. The lack of a solid medium of economic exchange is implicitly illustrated in novels of the time. For instance, messengers were rewarded with useful objects, e.g., an old silk kimono, rather than paid a fee.
The Fujiwara rulers failed to maintain adequate police forces, which left robbers free to prey on travelers. This is implicitly illustrated in novels by the terror that night travel inspired in the main characters. The shōen system enabled the accumulation of wealth by an aristocratic elite; the economic surplus can be linked to the cultural developments of the Heian period and the "pursuit of arts". The major Buddhist temples in Heian-kyō and Nara also made use of the shōen. The establishment of branches rurally and integration of some Shinto shrines within these temple networks reflects a greater "organizational dynamism".
The game Total War: Shogun 2 has the Rise of the Samurai expansion pack as downloadable campaign. It allows the player to make their own version of the Gempei War which happened during the Heian period. The player is able to choose one of the most powerful families of Japan at the time, the Taira, Minamoto or Fujiwara; each family fielding two branches for a total of six playable clans. The expansion pack features a different set of land units, ships and buildings and is also playable in the multiplayer modes.
Throughout its prehistory and early history, the region and its vicinity in the Yangtze region was the cradle of unique local civilizations which can be dated back to at least the 15th century BC and coinciding with the later years of the Shang and Zhou dynasties in North China. Sichuan was referred to in ancient Chinese sources as Ba-Shu (巴蜀), an abbreviation of the kingdoms of Ba and Shu which existed within the Sichuan Basin. Ba included Chongqing and the land in eastern Sichuan along the Yangtze and some tributary streams, while Shu included today's Chengdu, its surrounding plain and adjacent territories in western Sichuan.
The existence of the early state of Shu was poorly recorded in the main historical records of China. It was, however, referred to in the Book of Documents as an ally of the Zhou. Accounts of Shu exist mainly as a mixture of mythological stories and historical legends recorded in local annals such as the Chronicles of Huayang compiled in the Jin dynasty (265–420), with folk stories such as that of Emperor Duyu (杜宇) who taught the people agriculture and transformed himself into a cuckoo after his death. The existence of a highly developed civilization with an independent bronze industry in Sichuan eventually came to light with an archaeological discovery in 1986 at a small village named Sanxingdui in Guanghan, Sichuan. This site, believed to be an ancient city of Shu, was initially discovered by a local farmer in 1929 who found jade and stone artefacts. Excavations by archaeologists in the area yielded few significant finds until 1986 when two major sacrificial pits were found with spectacular bronze items as well as artefacts in jade, gold, earthenware, and stone. This and other discoveries in Sichuan contest the conventional historiography that the local culture and technology of Sichuan were undeveloped in comparison to the technologically and culturally "advanced" Yellow River valley of north-central China. The name Shu continues to be used to refer to Sichuan in subsequent periods in Chinese history up to the present day.
The rulers of the expansionist Qin dynasty, based in present-day Gansu and Shaanxi, were only the first strategists to realize that the area's military importance matched its commercial and agricultural significance. The Sichuan basin is surrounded by the Himalayas to the west, the Qin Mountains to the north, and mountainous areas of Yunnan to the south. Since the Yangtze flows through the basin and then through the perilous Yangzi Gorges to eastern and southern China, Sichuan was a staging area for amphibious military forces and a refuge for political refugees.[citation needed]
Qin armies finished their conquest of the kingdoms of Shu and Ba by 316 BC. Any written records and civil achievements of earlier kingdoms were destroyed. Qin administrators introduced improved agricultural technology. Li Bing, engineered the Dujiangyan irrigation system to control the Min River, a major tributary of the Yangtze. This innovative hydraulic system was composed of movable weirs which could be adjusted for high or low water flow according to the season, to either provide irrigation or prevent floods. The increased agricultural output and taxes made the area a source of provisions and men for Qin's unification of China.
Sichuan came under the firm control of a Chinese central government during the Sui dynasty, but it was during the subsequent Tang dynasty where Sichuan regained its previous political and cultural prominence for which it was known during the Han. Chengdu became nationally known as a supplier of armies and the home of Du Fu, who is sometimes called China's greatest poet. During the An Lushan Rebellion (755-763), Emperor Xuanzong of Tang fled from Chang'an to Sichuan. The region was torn by constant warfare and economic distress as it was besieged by the Tibetan Empire.
In the middle of the 17th century, the peasant rebel leader Zhang Xianzhong (1606–1646) from Yan'an, Shanxi Province, nicknamed Yellow Tiger, led his peasant troop from north China to the south, and conquered Sichuan. Upon capturing it, he declared himself emperor of the Daxi Dynasty (大西王朝). In response to the resistance from local elites, he massacred a large native population. As a result of the massacre as well as years of turmoil during the Ming-Qing transition, the population of Sichuan fell sharply, requiring a massive resettlement of people from the neighboring Huguang Province (modern Hubei and Hunan) and other provinces during the Qing dynasty.
In the 20th century, as Beijing, Shanghai, Nanjing, and Wuhan had all been occupied by the Japanese during the Second Sino-Japanese War, the capital of the Republic of China had been temporary relocated to Chongqing, then a major city in Sichuan. An enduring legacy of this move is that nearby inland provinces, such as Shaanxi, Gansu, and Guizhou, which previously never had modern Western-style universities, began to be developed in this regard. The difficulty of accessing the region overland from the eastern part of China and the foggy climate hindering the accuracy of Japanese bombing of the Sichuan Basin, made the region the stronghold of Chiang Kai-Shek's Kuomintang government during 1938-45, and led to the Bombing of Chongqing.
The Second Sino-Japanese War was soon followed by the resumed Chinese Civil War, and the cities of East China fell to the Communists one after another, the Kuomintang government again tried to make Sichuan its stronghold on the mainland, although it already saw some Communist activity since it was one area on the road of the Long March. Chiang Kai-Shek himself flew to Chongqing from Taiwan in November 1949 to lead the defense. But the same month Chongqing fell to the Communists, followed by Chengdu on 10 December. The Kuomintang general Wang Sheng wanted to stay behind with his troops to continue anticommunist guerilla war in Sichuan, but was recalled to Taiwan. Many of his soldiers made their way there as well, via Burma.
From 1955 until 1997 Sichuan had been China's most populous province, hitting 100 million mark shortly after the 1982 census figure of 99,730,000. This changed in 1997 when the Sub-provincial city of Chongqing as well as the three surrounding prefectures of Fuling, Wanxian, and Qianjiang were split off into the new Chongqing Municipality. The new municipality was formed to spearhead China's effort to economically develop its western provinces, as well as to coordinate the resettlement of residents from the reservoir areas of the Three Gorges Dam project.
Sichuan consists of two geographically very distinct parts. The eastern part of the province is mostly within the fertile Sichuan basin (which is shared by Sichuan with Chongqing Municipality). The western Sichuan consists of the numerous mountain ranges forming the easternmost part of the Qinghai-Tibet Plateau, which are known generically as Hengduan Mountains. One of these ranges, Daxue Mountains, contains the highest point of the province Gongga Shan, at 7,556 metres (24,790 ft) above sea level.
The Yangtze River and its tributaries flows through the mountains of western Sichuan and the Sichuan Basin; thus, the province is upstream of the great cities that stand along the Yangtze River further to the east, such as Chongqing, Wuhan, Nanjing and Shanghai. One of the major tributaries of the Yangtze within the province is the Min River of central Sichuan, which joins the Yangtze at Yibin. Sichuan's 4 main rivers, as Sichuan means literally, are Jaling Jiang, Tuo Jiang, Yalong Jiang, and Jinsha Jiang.
Due to great differences in terrain, the climate of the province is highly variable. In general it has strong monsoonal influences, with rainfall heavily concentrated in the summer. Under the Köppen climate classification, the Sichuan Basin (including Chengdu) in the eastern half of the province experiences a humid subtropical climate (Köppen Cwa or Cfa), with long, hot, humid summers and short, mild to cool, dry and cloudy winters. Consequently, it has China's lowest sunshine totals. The western region has mountainous areas producing a cooler but sunnier climate. Having cool to very cold winters and mild summers, temperatures generally decrease with greater elevation. However, due to high altitude and its inland location, many areas such as Garze County and Zoige County in Sichuan exhibit a subarctic climate (Köppen Dwc)- featuring extremely cold winters down to -30 °C and even cold summer nights. The region is geologically active with landslides and earthquakes. Average elevation ranges from 2,000 to 3,500 meters; average temperatures range from 0 to 15 °C. The southern part of the province, including Panzhihua and Xichang, has a sunny climate with short, very mild winters and very warm to hot summers.
Sichuan has been historically known as the "Province of Abundance". It is one of the major agricultural production bases of China. Grain, including rice and wheat, is the major product with output that ranked first in China in 1999. Commercial crops include citrus fruits, sugar cane, sweet potatoes, peaches and grapes. Sichuan also had the largest output of pork among all the provinces and the second largest output of silkworm cocoons in 1999. Sichuan is rich in mineral resources. It has more than 132 kinds of proven underground mineral resources including vanadium, titanium, and lithium being the largest in China. The Panxi region alone possesses 13.3% of the reserves of iron, 93% of titanium, 69% of vanadium, and 83% of the cobalt of the whole country. Sichuan also possesses China's largest proven natural gas reserves, the majority of which is transported to more developed eastern regions.
Sichuan is one of the major industrial centers of China. In addition to heavy industries such as coal, energy, iron and steel, the province has also established a light industrial sector comprising building materials, wood processing, food and silk processing. Chengdu and Mianyang are the production centers for textiles and electronics products. Deyang, Panzhihua, and Yibin are the production centers for machinery, metallurgical industries, and wine, respectively. Sichuan's wine production accounted for 21.9% of the country’s total production in 2000.
The Three Gorges Dam, the largest dam ever constructed, is being built on the Yangtze River in nearby Hubei province to control flooding in the Sichuan Basin, neighboring Yunnan province, and downstream. The plan is hailed by some as China's efforts to shift towards alternative energy sources and to further develop its industrial and commercial bases, but others have criticised it for its potentially harmful effects, such as massive resettlement of residents in the reservoir areas, loss of archeological sites, and ecological damages.
According to the Sichuan Department of Commerce, the province's total foreign trade was US$22.04 billion in 2008, with an annual increase of 53.3 percent. Exports were US$13.1 billion, an annual increase of 52.3 percent, while imports were US$8.93 billion, an annual increase of 54.7 percent. These achievements were accomplished because of significant changes in China's foreign trade policy, acceleration of the yuan's appreciation, increase of commercial incentives and increase in production costs. The 18 cities and counties witnessed a steady rate of increase. Chengdu, Suining, Nanchong, Dazhou, Ya'an, Abazhou, and Liangshan all saw an increase of more than 40 percent while Leshan, Neijiang, Luzhou, Meishan, Ziyang, and Yibin saw an increase of more than 20 percent. Foreign trade in Zigong, Panzhihua, Guang'an, Bazhong and Ganzi remained constant.
The Sichuan government raised the minimum wage in the province by 12.5 percent at the end of December 2007. The monthly minimum wage went up from 400 to 450 yuan, with a minimum of 4.9 yuan per hour for part-time work, effective 26 December 2007. The government also reduced the four-tier minimum wage structure to three. The top tier mandates a minimum of 650 yuan per month, or 7.1 yuan per hour. National law allows each province to set minimum wages independently, but with a floor of 450 yuan per month.
Chengdu Economic and Technological Development Zone (Chinese: 成都经济技术开发区; pinyin: Chéngdū jīngjì jìshù kāifā qū) was approved as state-level development zone in February 2000. The zone now has a developed area of 10.25 km2 (3.96 sq mi) and has a planned area of 26 km2 (10 sq mi). Chengdu Economic and Technological Development Zone (CETDZ) lies 13.6 km (8.5 mi) east of Chengdu, the capital city of Sichuan Province and the hub of transportation and communication in southwest China. The zone has attracted investors and developers from more than 20 countries to carry out their projects there. Industries encouraged in the zone include mechanical, electronic, new building materials, medicine and food processing.
Established in 1988, Chengdu Hi-tech Industrial Development Zone (Chinese: 成都高新技术产业开发区; pinyin: Chéngdū Gāoxīn Jìshù Chǎnyè Kāifā Qū) was approved as one of the first national hi-tech development zones in 1991. In 2000, it was open to APEC and has been recognized as a national advanced hi-tech development zone in successive assessment activities held by China's Ministry of Science and Technology. It ranks 5th among the 53 national hi-tech development zones in China in terms of comprehensive strength.
Chengdu Hi-tech Development Zone covers an area of 82.5 km2 (31.9 sq mi), consisting of the South Park and the West Park. By relying on the city sub-center, which is under construction, the South Park is focusing on creating a modernized industrial park of science and technology with scientific and technological innovation, incubation R&D, modern service industry and Headquarters economy playing leading roles. Priority has been given to the development of software industry. Located on both sides of the "Chengdu-Dujiangyan-Jiuzhaigou" golden tourism channel, the West Park aims at building a comprehensive industrial park targeting at industrial clustering with complete supportive functions. The West Park gives priority to three major industries i.e. electronic information, biomedicine and precision machinery.
Mianyang Hi-Tech Industrial Development Zone was established in 1992, with a planned area of 43 km2 (17 sq mi). The zone is situated 96 kilometers away from Chengdu, and is 8 km (5.0 mi) away from Mianyang Airport. Since its establishment, the zone accumulated 177.4 billion yuan of industrial output, 46.2 billion yuan of gross domestic product, fiscal revenue 6.768 billion yuan. There are more than 136 high-tech enterprises in the zone and they accounted for more than 90% of the total industrial output.
On 3 November 2007, the Sichuan Transportation Bureau announced that the Sui-Yu Expressway was completed after three years of construction. After completion of the Chongqing section of the road, the 36.64 km (22.77 mi) expressway connected Cheng-Nan Expressway and formed the shortest expressway from Chengdu to Chongqing. The new expressway is 50 km (31 mi) shorter than the pre-existing road between Chengdu and Chongqing; thus journey time between the two cities was reduced by an hour, now taking two and a half hours. The Sui-Yu Expressway is a four lane overpass with a speed limit of 80 km/h (50 mph). The total investment was 1.045 billion yuan.
The majority of the province's population is Han Chinese, who are found scattered throughout the region with the exception of the far western areas. Thus, significant minorities of Tibetan, Yi, Qiang and Nakhi people reside in the western portion that are impacted by inclement weather and natural disasters, environmentally fragile, and impoverished. Sichuan's capital of Chengdu is home to a large community of Tibetans, with 30,000 permanent Tibetan residents and up to 200,000 Tibetan floating population. The Eastern Lipo, included with either the Yi or the Lisu people, as well as the A-Hmao, also are among the ethnic groups of the provinces.
Sichuan was China's most populous province before Chongqing became a directly-controlled municipality; it is currently the fourth most populous, after Guangdong, Shandong and Henan. As of 1832, Sichuan was the most populous of the 18 provinces in China, with an estimated population at that time of 21 million. It was the third most populous sub-national entity in the world, after Uttar Pradesh, India and the Russian Soviet Federative Socialist Republic until 1991, when the Soviet Union was dissolved. It is also one of the only six to ever reach 100 million people (Uttar Pradesh, Russian RSFSR, Maharashtra, Sichuan, Bihar and Punjab). It is currently 10th.
Garzê Tibetan Autonomous Prefecture and Ngawa Tibetan and Qiang Autonomous Prefecture in western Sichuan are populated by Tibetans and Qiang people. Tibetans speak the Khams and Amdo Tibetan, which are Tibetic languages, as well as various Qiangic languages. The Qiang speak Qiangic languages and often Tibetic languages as well. The Yi people of Liangshan Yi Autonomous Prefecture in southern Sichuan speak the Nuosu language, which is one of the Lolo-Burmese languages; Yi is written using the Yi script, a syllabary standardized in 1974. The Southwest University for Nationalities has one of China's most prominent Tibetology departments, and the Southwest Minorities Publishing House prints literature in minority languages. In the minority inhabited regions of Sichuan, there is bi-lingual signage and public school instruction in non-Mandarin minority languages.
Oklahoma City is the capital and largest city of the state of Oklahoma. The county seat of Oklahoma County, the city ranks 27th among United States cities in population. The population grew following the 2010 Census, with the population estimated to have increased to 620,602 as of July 2014. As of 2014, the Oklahoma City metropolitan area had a population of 1,322,429, and the Oklahoma City-Shawnee Combined Statistical Area had a population of 1,459,758 (Chamber of Commerce) residents, making it Oklahoma's largest metropolitan area. Oklahoma City's city limits extend into Canadian, Cleveland, and Pottawatomie counties, though much of those areas outside of the core Oklahoma County area are suburban or rural (watershed). The city ranks as the eighth-largest city in the United States by land area (including consolidated city-counties; it is the largest city in the United States by land area whose government is not consolidated with that of a county or borough).
Oklahoma City, lying in the Great Plains region, features one of the largest livestock markets in the world. Oil, natural gas, petroleum products and related industries are the largest sector of the local economy. The city is situated in the middle of an active oil field and oil derricks dot the capitol grounds. The federal government employs large numbers of workers at Tinker Air Force Base and the United States Department of Transportation's Mike Monroney Aeronautical Center (these two sites house several offices of the Federal Aviation Administration and the Transportation Department's Enterprise Service Center, respectively).
Oklahoma City is on the I-35 Corridor and is one of the primary travel corridors into neighboring Texas and Mexico. Located in the Frontier Country region of the state, the city's northeast section lies in an ecological region known as the Cross Timbers. The city was founded during the Land Run of 1889, and grew to a population of over 10,000 within hours of its founding. The city was the scene of the April 19, 1995 bombing of the Alfred P. Murrah Federal Building, in which 168 people died. It was the deadliest terror attack in the history of the United States until the attacks of September 11, 2001, and remains the deadliest act of domestic terrorism in U.S. history.
Oklahoma City was settled on April 22, 1889, when the area known as the "Unassigned Lands" was opened for settlement in an event known as "The Land Run". Some 10,000 homesteaders settled the area that would become the capital of Oklahoma. The town grew quickly; the population doubled between 1890 and 1900. Early leaders of the development of the city included Anton Classen, John Shartel, Henry Overholser and James W. Maney.
By the time Oklahoma was admitted to the Union in 1907, Oklahoma City had surpassed Guthrie, the territorial capital, as the population center and commercial hub of the new state. Soon after, the capital was moved from Guthrie to Oklahoma City. Oklahoma City was a major stop on Route 66 during the early part of the 20th century; it was prominently mentioned in Bobby Troup's 1946 jazz classic, "(Get Your Kicks on) Route 66", later made famous by artist Nat King Cole.
Before World War II, Oklahoma City developed major stockyards, attracting jobs and revenue formerly in Chicago and Omaha, Nebraska. With the 1928 discovery of oil within the city limits (including under the State Capitol), Oklahoma City became a major center of oil production. Post-war growth accompanied the construction of the Interstate Highway System, which made Oklahoma City a major interchange as the convergence of I-35, I-40 and I-44. It was also aided by federal development of Tinker Air Force Base.
Patience Latting was elected Mayor of Oklahoma City in 1971, becoming the city's first female mayor. Latting was also the first woman to serve as mayor of a U.S. city with over 350,000 residents.
In 1993, the city passed a massive redevelopment package known as the Metropolitan Area Projects (MAPS), intended to rebuild the city's core with civic projects to establish more activities and life to downtown. The city added a new baseball park; central library; renovations to the civic center, convention center and fairgrounds; and a water canal in the Bricktown entertainment district. Water taxis transport passengers within the district, adding color and activity along the canal. MAPS has become one of the most successful public-private partnerships undertaken in the U.S., exceeding $3 billion in private investment as of 2010. As a result of MAPS, the population living in downtown housing has exponentially increased, together with demand for additional residential and retail amenities, such as grocery, services, and shops.
Since the MAPS projects' completion, the downtown area has seen continued development. Several downtown buildings are undergoing renovation/restoration. Notable among these was the restoration of the Skirvin Hotel in 2007. The famed First National Center is being renovated.
Residents of Oklahoma City suffered substantial losses on April 19, 1995 when Timothy McVeigh detonated a bomb in front of the Murrah building. The building was destroyed (the remnants of which had to be imploded in a controlled demolition later that year), more than 100 nearby buildings suffered severe damage, and 168 people were killed. The site has been commemorated as the Oklahoma City National Memorial and Museum. Since its opening in 2000, over three million people have visited. Every year on April 19, survivors, families and friends return to the memorial to read the names of each person lost.
The "Core-to-Shore" project was created to relocate I-40 one mile (1.6 km) south and replace it with a boulevard to create a landscaped entrance to the city. This also allows the central portion of the city to expand south and connect with the shore of the Oklahoma River. Several elements of "Core to Shore" were included in the MAPS 3 proposal approved by voters in late 2009.
According to the United States Census Bureau, the city has a total area of 620.34 square miles (1,606.7 km2), of which, 601.11 square miles (1,556.9 km2) of it is land and 19.23 square miles (49.8 km2) of it is water. The total area is 3.09 percent water.
Oklahoma City lies in the Sandstone Hills region of Oklahoma, known for hills of 250 to 400 feet (120 m) and two species of oak: blackjack oak (Quercus marilandica) and post oak (Q. stellata). The northeastern part of the city and its eastern suburbs fall into an ecological region known as the Cross Timbers.
The city is roughly bisected by the North Canadian River (recently renamed the Oklahoma River inside city limits). The North Canadian once had sufficient flow to flood every year, wreaking destruction on surrounding areas, including the central business district and the original Oklahoma City Zoo. In the 1940s, a dam was built on the river to manage the flood control and reduced its level. In the 1990s, as part of the citywide revitalization project known as MAPS, the city built a series of low-water dams, returning water to the portion of the river flowing near downtown. The city has three large lakes: Lake Hefner and Lake Overholser, in the northwestern quarter of the city; and the largest, Lake Stanley Draper, in the sparsely populated far southeast portion of the city.
The population density normally reported for Oklahoma City using the area of its city limits can be a bit misleading. Its urbanized zone covers roughly 244 sq mi (630 km2) resulting in a density of 2,500 per square mile (2013 est), compared with larger rural watershed areas incorporated by the city, which cover the remaining 377 sq mi (980 km2) of the city limits.
The city is bisected geographically and culturally by the North Canadian River, which basically divides North Oklahoma City and South Oklahoma City. The two halves of the city were actually founded and plotted as separate cities, but soon grew together. The north side is characterized by very diverse and fashionable urban neighborhoods near the city center and sprawling suburbs further north. South Oklahoma City is generally more blue collar working class and significantly more industrial, having grown up around the Stockyards and meat packing plants at the turn of the century, and is currently the center of the city's rapidly growing Latino community.
Downtown Oklahoma City, which has 7,600 residents, is currently seeing an influx of new private investment and large scale public works projects, which have helped to resuscitate a central business district left almost deserted by the Oil Bust of the early 1980s. The centerpiece of downtown is the newly renovated Crystal Bridge and Myriad Botanical Gardens, one of the few elements of the Pei Plan to be completed. In the next few years a massive new central park will link the gardens near the CBD and the new convention center to be built just south of it to the North Canadian River, as part of a massive works project known as Core to Shore; the new park is part of MAPS3, a collection of civic projects funded by a 1-cent temporary (seven-year) sales tax increase.
Oklahoma City has a humid subtropical climate (Köppen: Cfa), with frequent variations in weather daily and seasonally, except during the consistently hot and humid summer months. Prolonged and severe droughts (sometimes leading to wildfires in the vicinity) as well as very heavy rainfall leading to flash flooding and flooding occur with some regularity. Consistent winds, usually from the south or south-southeast during the summer, help temper the hotter weather. Consistent northerly winds during the winter can intensify cold periods. Severe ice storms and snowstorms happen sporadically during the winter.
The average temperature is 61.4 °F (16.3 °C), with the monthly daily average ranging from 39.2 °F (4.0 °C) in January to 83.0 °F (28.3 °C) in July. Extremes range from −17 °F (−27 °C) on February 12, 1899 to 113 °F (45 °C) on August 11, 1936 and August 3, 2012; the last sub-zero (°F) reading was −5 °F (−21 °C) on February 10, 2011. Temperatures reach 100 °F (38 °C) on 10.4 days of the year, 90 °F (32 °C) on nearly 70 days, and fail to rise above freezing on 8.3 days. The city receives about 35.9 inches (91.2 cm) of precipitation annually, of which 8.6 inches (21.8 cm) is snow.
Oklahoma City has a very active severe weather season from March through June, especially during April and May. Being in the center of what is colloquially referred to as Tornado Alley, it is prone to especially frequent and severe tornadoes, as well as very severe hailstorms and occasional derechoes. Tornadoes have occurred in every month of the year and a secondary smaller peak also occurs during autumn, especially October. The Oklahoma City metropolitan area is one of the most tornado-prone major cities in the world, with about 150 tornadoes striking within the city limits since 1890. Since the time weather records have been kept, Oklahoma City has been struck by thirteen violent tornadoes, eleven F/EF4s and two F/EF5. On May 3, 1999 parts of southern Oklahoma City and nearby suburban communities suffered from one of the most powerful tornadoes on record, an F5 on the Fujita scale, with wind speeds estimated by radar at 318 mph (510 km/h). On May 20, 2013, far southwest Oklahoma City, along with Newcastle and Moore, was hit again by a EF5 tornado; it was 0.5 to 1.3 miles (0.80 to 2.09 km) wide and killed 23 people. Less than two weeks later, on May 31, another outbreak affected the Oklahoma City area, including an EF1 and an EF0 within the city and a tornado several miles west of the city that was 2.6 miles (4.2 km) in width, the widest tornado ever recorded.
With 19.48 inches of rainfall, May 2015 was by far Oklahoma City's record-wettest month since record keeping began in 1890. Across Oklahoma and Texas generally, there was record flooding in the latter part of the month 
As of the 2010 census, there were 579,999 people, 230,233 households, and 144,120 families residing in the city. The population density was 956.4 inhabitants per square mile (321.9/km²). There were 256,930 housing units at an average density of 375.9 per square mile (145.1/km²).
There were 230,233 households, 29.4% of which had children under the age of 18 living with them, 43.4% were married couples living together, 13.9% had a female householder with no husband present, and 37.4% were non-families. One person households account for 30.5% of all households and 8.7% of all households had someone living alone who is 65 years of age or older. The average household size was 2.47 and the average family size was 3.11.
In the 2000 Census Oklahoma City's age composition was 25.5% under the age of 18, 10.7% from 18 to 24, 30.8% from 25 to 44, 21.5% from 45 to 64, and 11.5% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 92.7 males.
Oklahoma City has experienced significant population increases since the late 1990s. In May 2014, the U.S. Census announced Oklahoma City had an estimated population of 620,602 in 2014 and that it had grown 5.3 percent between April 2010 and June 2013. Since the official Census in 2000, Oklahoma City had grown 21 percent (a 114,470 raw increase) according to the Bureau estimates. The 2014 estimate of 620,602 is the largest population Oklahoma City has ever recorded. It is the first city in the state to record a population greater than 600,000 residents and the largest municipal population of the Great Plains region (OK, KS, NE, SD, ND).
Oklahoma City is the principal city of the eight-county Oklahoma City Metropolitan Statistical Area in Central Oklahoma and is the state's largest urbanized area. Based on population rank, the metropolitan area was the 42nd largest in the nation as of 2012.
With regards to Mexican drug cartels, Oklahoma City has traditionally been the territory of the notorious Juárez Cartel, but the Sinaloa Cartel has been reported as trying to establish a foothold in Oklahoma City. There are many rival gangs in Oklahoma City, one whose headquarters has been established in the city, the Southside Locos, traditionally known as Sureños.
Oklahoma City also has its share of very brutal crimes, particularly in the 1970s. The worst of which occurred in 1978, when six employees of a Sirloin Stockade restaurant on the city's south side were murdered execution-style in the restaurant's freezer. An intensive investigation followed, and the three individuals involved, who also killed three others in Purcell, Oklahoma, were identified. One, Harold Stafford, died in a motorcycle accident in Tulsa not long after the restaurant murders. Another, Verna Stafford, was sentenced to life without parole after being granted a new trial after she had previously been sentenced to death. Roger Dale Stafford, considered the mastermind of the murder spree, was executed by lethal injection at the Oklahoma State Penitentiary in 1995.
The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.
On April 19, 1995, the Alfred P. Murrah Federal Building was destroyed by a fertilizer bomb manufactured and detonated by Timothy McVeigh. The blast and catastrophic collapse killed 168 people and injured over 680. The blast shockwave destroyed or damaged 324 buildings within a 340-meter radius, destroyed or burned 86 cars, and shattered glass in 258 nearby buildings, causing at least an estimated $652 million worth of damage. The main suspect- Timothy McVeigh, was executed by lethal injection on June 11, 2001. It was the deadliest single domestic terrorist attack in US history, prior to 9/11.
While not in Oklahoma City proper, other large employers within the MSA region include: Tinker Air Force Base (27,000); University of Oklahoma (11,900); University of Central Oklahoma (2,900); and Norman Regional Hospital (2,800).
According to the Oklahoma City Chamber of Commerce, the metropolitan area's economic output grew by 33 percent between 2001 and 2005 due chiefly to economic diversification. Its gross metropolitan product was $43.1 billion in 2005 and grew to $61.1 billion in 2009.
In 2008, Forbes magazine named Oklahoma City the most "recession proof city in America". The magazine reported that the city had falling unemployment, one of the strongest housing markets in the country and solid growth in energy, agriculture and manufacturing. However, during the early 1980s, Oklahoma City had one of the worst job and housing markets due to the bankruptcy of Penn Square Bank in 1982 and then the post-1985 crash in oil prices.[citation needed]
Other theaters include Lyric Theatre, Jewel Box Theatre, Kirkpatrick Auditorium, the Poteet Theatre, the Oklahoma City Community College Bruce Owen Theater and the 488-seat Petree Recital Hall, at the Oklahoma City University campus. The university also opened the Wanda L Bass School of Music and auditorium in April 2006.
The Science Museum Oklahoma (formerly Kirkpatrick Science and Air Space Museum at Omniplex) houses exhibits on science, aviation, and an IMAX theater. The museum formerly housed the International Photography Hall of Fame (IPHF) that exhibits photographs and artifacts from a large collection of cameras and other artifacts preserving the history of photography. IPHF honors those who have made significant contributions to the art and/or science of photography and relocated to St. Louis, Missouri in 2013.
The Museum of Osteology houses more than 300 real animal skeletons. Focusing on the form and function of the skeletal system, this 7,000 sq ft (650 m2) museum displays hundreds of skulls and skeletons from all corners of the world. Exhibits include adaptation, locomotion, classification and diversity of the vertebrate kingdom. The Museum of Osteology is the only one of its kind in America.
The National Cowboy & Western Heritage Museum has galleries of western art and is home to the Hall of Great Western Performers. In contrast, the city will also be home to The American Indian Cultural Center and Museum that began construction in 2009 (although completion of the facility has been held up due to insufficient funding), on the south side of Interstate 40, southeast from Bricktown.
The Oklahoma City National Memorial in the northern part of Oklahoma City's downtown was created as the inscription on its eastern gate of the Memorial reads, "to honor the victims, survivors, rescuers, and all who were changed forever on April 19, 1995"; the memorial was built on the land formerly occupied by the Alfred P. Murrah Federal Building complex prior to its 1995 bombing. The outdoor Symbolic Memorial can be visited 24 hours a day for free, and the adjacent Memorial Museum, located in the former Journal Record building damaged by the bombing, can be entered for a small fee. The site is also home to the National Memorial Institute for the Prevention of Terrorism, a non-partisan, nonprofit think tank devoted to the prevention of terrorism.
The American Banjo Museum located in the Bricktown Entertainment district is dedicated to preserving and promoting the music and heritage of America's native musical instrument – the banjo. With a collection valued at $3.5 million it is truly a national treasure. An interpretive exhibits tells the evolution of the banjo from its humble roots in American slavery, to bluegrass, to folk and world music.
The Oklahoma History Center is the history museum of the state of Oklahoma. Located across the street from the governor's mansion at 800 Nazih Zuhdi Drive in northeast Oklahoma City, the museum opened in 2005 and is operated by the Oklahoma Historical Society. It preserves the history of Oklahoma from the prehistoric to the present day.
Oklahoma City is home to several professional sports teams, including the Oklahoma City Thunder of the National Basketball Association. The Thunder is the city's second "permanent" major professional sports franchise after the now-defunct AFL Oklahoma Wranglers and is the third major-league team to call the city home when considering the temporary hosting of the New Orleans/Oklahoma City Hornets for the 2005–06 and 2006–07 NBA seasons.
Other professional sports clubs in Oklahoma City include the Oklahoma City Dodgers, the Triple-A affiliate of the Los Angeles Dodgers, the Oklahoma City Energy FC of the United Soccer League, and the Crusaders of Oklahoma Rugby Football Club USA Rugby.
Chesapeake Energy Arena in downtown is the principal multipurpose arena in the city which hosts concerts, NHL exhibition games, and many of the city's pro sports teams. In 2008, the Oklahoma City Thunder became the major tenant. Located nearby in Bricktown, the Chickasaw Bricktown Ballpark is the home to the city's baseball team, the Dodgers. "The Brick", as it is locally known, is considered one of the finest minor league parks in the nation.[citation needed]
Oklahoma City is the annual host of the Big 12 Baseball Tournament, the World Cup of Softball, and the annual NCAA Women's College World Series. The city has held the 2005 NCAA Men's Basketball First and Second round and hosted the Big 12 Men's and Women's Basketball Tournaments in 2007 and 2009. The major universities in the area – University of Oklahoma, Oklahoma City University, and Oklahoma State University – often schedule major basketball games and other sporting events at Chesapeake Energy Arena and Chickasaw Bricktown Ballpark, although most home games are played at their campus stadiums.
Other major sporting events include Thoroughbred and Quarter horse racing circuits at Remington Park and numerous horse shows and equine events that take place at the state fairgrounds each year. There are numerous golf courses and country clubs spread around the city.
The state of Oklahoma hosts a highly competitive high school football culture, with many teams in the Oklahoma City metropolitan area. The Oklahoma Secondary School Activities Association (OSSAA) organizes high school football into eight distinct classes based on the size of school enrollment. Beginning with the largest, the classes are: 6A, 5A, 4A, 3A, 2A, A, B, and C. Class 6A is broken into two divisions. Oklahoma City area schools in this division include: Edmond North, Mustang, Moore, Yukon, Edmond Memorial, Edmond Santa Fe, Norman North, Westmoore, Southmoore, Putnam City North, Norman, Putnam City, Putnam City West, U.S. Grant, Midwest City.
The Oklahoma City Thunder of the National Basketball Association (NBA) has called Oklahoma City home since the 2008–09 season, when owner Clayton Bennett relocated the franchise from Seattle, Washington. The Thunder plays home games at the Chesapeake Energy Arena in downtown Oklahoma City, known affectionately in the national media as 'the Peake' and 'Loud City'. The Thunder is known by several nicknames, including "OKC Thunder" and simply "OKC", and its mascot is Rumble the Bison.
After a lackluster arrival to Oklahoma City for the 2008–09 season, the Oklahoma City Thunder secured a berth (8th) in the 2010 NBA Playoffs the next year after boasting its first 50-win season, winning two games in the first round against the Los Angeles Lakers. In 2012, Oklahoma City made it to the NBA Finals, but lost to the Miami Heat in five games. In 2013 the Thunder reached the Western Conference semifinals without All-Star guard Russell Westbrook, who was injured in their first round series against the Houston Rockets, only to lose to the Memphis Grizzlies. In 2014 Oklahoma City again reached the NBA's Western Conference Finals but eventually lost to the San Antonio Spurs in six games.
The Oklahoma City Thunder has been regarded by sports analysts as one of the elite franchises of the NBA's Western Conference and that of a media darling as the future of the league. Oklahoma City has earned Northwest Division titles every year since 2009 and has consistently improved its win record to 59-wins in 2014. The Thunder is led by first year head coach Billy Donovan and is anchored by several NBA superstars, including perennial All-Star point guard Russell Westbrook, 2014 MVP and four-time NBA scoring champion Kevin Durant, and Defensive Player of the Year nominee and shot-blocker Serge Ibaka.
In the aftermath of Hurricane Katrina, the NBA's New Orleans Hornets (now the New Orleans Pelicans) temporarily relocated to the Ford Center, playing the majority of its home games there during the 2005–06 and 2006–07 seasons. The team became the first NBA franchise to play regular-season games in the state of Oklahoma.[citation needed] The team was known as the New Orleans/Oklahoma City Hornets while playing in Oklahoma City. The team ultimately returned to New Orleans full-time for the 2007–08 season. The Hornets played their final home game in Oklahoma City during the exhibition season on October 9, 2007 against the Houston Rockets.
One of the more prominent landmarks downtown is the Crystal Bridge at the Myriad Botanical Gardens, a large downtown urban park. Designed by I. M. Pei, the Crystal Bridge is a tropical conservatory in the area. The park has an amphitheater, known as the Water Stage. In 2007, following a renovation of the stage, Oklahoma Shakespeare in the Park relocated to the Myriad Gardens. The Myriad Gardens will undergo a massive renovation in conjunction with the recently built Devon Tower directly north of it.
The Oklahoma City Zoo and Botanical Garden is home to numerous natural habitats, WPA era architecture and landscaping, and hosts major touring concerts during the summer at its amphitheater. Oklahoma City also has two amusement parks, Frontier City theme park and White Water Bay water park. Frontier City is an 'Old West'-themed amusement park. The park also features a recreation of a western gunfight at the 'OK Corral' and many shops that line the "Western" town's main street. Frontier City also hosts a national concert circuit at its amphitheater during the summer. Oklahoma City also has a combination racetrack and casino open year-round, Remington Park, which hosts both Quarter horse (March – June) and Thoroughbred (August – December) seasons.
Walking trails line Lake Hefner and Lake Overholser in the northwest part of the city and downtown at the canal and the Oklahoma River. The majority of the east shore area is taken up by parks and trails, including a new leashless dog park and the postwar-era Stars and Stripes Park. Lake Stanley Draper is the city's largest and most remote lake.
Oklahoma City has a major park in each quadrant of the city, going back to the first parks masterplan. Will Rogers Park, Lincoln Park, Trosper Park, and Woodson Park were once connected by the Grand Boulevard loop, some sections of which no longer exist. Martin Park Nature Center is a natural habitat in far northwest Oklahoma City. Will Rogers Park is home to the Lycan Conservatory, the Rose Garden, and Butterfly Garden, all built in the WPA era. Oklahoma City is home to the American Banjo Museum, which houses a large collection of highly decorated banjos from the early 20th century and exhibits on the history of the banjo and its place in American history. Concerts and lectures are also held there.
In April 2005, the Oklahoma City Skate Park at Wiley Post Park was renamed the Mat Hoffman Action Sports Park to recognize Mat Hoffman, an Oklahoma City area resident and businessman that was instrumental in the design of the skate park and is a 10-time BMX World Vert champion. In March 2009, the Mat Hoffman Action Sports Park was named by the National Geographic Society Travel Guide as one of the "Ten Best."
The City of Oklahoma City has operated under a council-manager form of city government since 1927. Mick Cornett serves as Mayor, having first been elected in 2004, and re-elected in 2006, 2010, and 2014. Eight councilpersons represent each of the eight wards of Oklahoma City. City Manager Jim Couch was appointed in late 2000. Couch previously served as assistant city manager, Metropolitan Area Projects Plan (MAPS) director and utilities director prior to his service as city manager.
The city is home to several colleges and universities. Oklahoma City University, formerly known as Epworth University, was founded by the United Methodist Church on September 1, 1904 and is renowned for its performing arts, science, mass communications, business, law, and athletic programs. OCU has its main campus in the north-central section of the city, near the city's chinatown area. OCU Law is located in the Midtown district near downtown, in the old Central High School building.
The University of Oklahoma has several institutions of higher learning in the city and metropolitan area, with OU Medicine and the University of Oklahoma Health Sciences Center campuses located east of downtown in the Oklahoma Health Center district, and the main campus located to the south in the suburb of Norman. The OU Medicine hosting the state's only Level-One trauma center. OU Health Sciences Center is one of the nation's largest independent medical centers, employing more than 12,000 people. OU is one of only four major universities in the nation to operate six medical schools.[clarification needed]
The third-largest university in the state, the University of Central Oklahoma, is located just north of the city in the suburb of Edmond. Oklahoma Christian University, one of the state's private liberal arts institutions, is located just south of the Edmond border, inside the Oklahoma City limits.
Oklahoma City Community College in south Oklahoma City is the second-largest community college in the state. Rose State College is located east of Oklahoma City in suburban Midwest City. Oklahoma State University–Oklahoma City is located in the "Furniture District" on the Westside. Northeast of the city is Langston University, the state's historically black college (HBCU). Langston also has an urban campus in the eastside section of the city. Southern Nazarene University, which was founded by the Church of the Nazarene, is a university located in suburban Bethany, which is surrounded by the Oklahoma City city limits.
Although technically not a university, the FAA's Mike Monroney Aeronautical Center has many aspects of an institution of higher learning. Its FAA Academy is accredited by the North Central Association of Colleges and Schools. Its Civil Aerospace Medical Institute (CAMI) has a medical education division responsible for aeromedical education in general as well as the education of aviation medical examiners in the U.S. and 93 other countries. In addition, The National Academy of Science offers Research Associateship Programs for fellowship and other grants for CAMI research.
Oklahoma City is home to the state's largest school district, Oklahoma City Public Schools. The district's Classen School of Advanced Studies and Harding Charter Preparatory High School rank high among public schools nationally according to a formula that looks at the number of Advanced Placement, International Baccalaureate and/or Cambridge tests taken by the school's students divided by the number of graduating seniors. In addition, OKCPS's Belle Isle Enterprise Middle School was named the top middle school in the state according to the Academic Performance Index, and recently received the Blue Ribbon School Award, in 2004 and again in 2011. KIPP Reach College Preparatory School in Oklahoma City received the 2012 National Blue Ribbon along with its school leader, Tracy McDaniel Sr., being awarded the Terrel H. Bell Award for Outstanding Leadership.
The Oklahoma School of Science and Mathematics, a school for some of the state's most gifted math and science pupils, is also located in Oklahoma City.
Oklahoma City has several public career and technology education schools associated with the Oklahoma Department of Career and Technology Education, the largest of which are Metro Technology Center and Francis Tuttle Technology Center.
Private career and technology education schools in Oklahoma City include Oklahoma Technology Institute, Platt College, Vatterott College, and Heritage College. The Dale Rogers Training Center in Oklahoma City is a nonprofit vocational training center for individuals with disabilities.
The Oklahoman is Oklahoma City's major daily newspaper and is the most widely circulated in the state. NewsOK.com is the Oklahoman's online presence. Oklahoma Gazette is Oklahoma City's independent newsweekly, featuring such staples as local commentary, feature stories, restaurant reviews and movie listings and music and entertainment. The Journal Record is the city's daily business newspaper and okcBIZ is a monthly publication that covers business news affecting those who live and work in Central Oklahoma.
There are numerous community and international newspapers locally that cater to the city's ethnic mosaic; such as The Black Chronicle, headquartered in the Eastside, the OK VIETIMES and Oklahoma Chinese Times, located in Asia District, and various Hispanic community publications. The Campus is the student newspaper at Oklahoma City University. Gay publications include The Gayly Oklahoman.
An upscale lifestyle publication called Slice Magazine is circulated throughout the metropolitan area. In addition, there is a magazine published by Back40 Design Group called The Edmond Outlook. It contains local commentary and human interest pieces direct-mailed to over 50,000 Edmond residents.
Oklahoma City was home to several pioneers in radio and television broadcasting. Oklahoma City's WKY Radio was the first radio station transmitting west of the Mississippi River and the third radio station in the United States. WKY received its federal license in 1921 and has continually broadcast under the same call letters since 1922. In 1928, WKY was purchased by E.K. Gaylord's Oklahoma Publishing Company and affiliated with the NBC Red Network; in 1949, WKY-TV (channel 4) went on the air and later became the first independently owned television station in the U.S. to broadcast in color. In mid-2002, WKY radio was purchased outright by Citadel Broadcasting, who was bought out by Cumulus Broadcasting in 2011. The Gaylord family earlier sold WKY-TV in 1976, which has gone through a succession of owners (what is now KFOR-TV is currently owned by Tribune Broadcasting as of December 2013).
The major U.S. broadcast television networks have affiliates in the Oklahoma City market (ranked 41st for television by Nielsen and 48th for radio by Arbitron, covering a 34-county area serving the central, northern-central and west-central sections Oklahoma); including NBC affiliate KFOR-TV (channel 4), ABC affiliate KOCO-TV (channel 5), CBS affiliate KWTV-DT (channel 9, the flagship of locally based Griffin Communications), PBS station KETA-TV (channel 13, the flagship of the state-run OETA member network), Fox affiliate KOKH-TV (channel 25), CW affiliate KOCB (channel 34), independent station KAUT-TV (channel 43), MyNetworkTV affiliate KSBI-TV (channel 52), and Ion Television owned-and-operated station KOPX-TV (channel 62). The market is also home to several religious stations including TBN owned-and-operated station KTBO-TV (channel 14) and Norman-based Daystar owned-and-operated station KOCM (channel 46).
Oklahoma City is protected by the Oklahoma City Fire Department (OKCFD), which employs 1015 paid, professional firefighters. The current Chief of Department is G. Keith Bryant, the department is also commanded by three Deputy Chiefs, who – along with the department chief – oversee the Operational Services, Prevention Services, and Support Services bureaus. The OKCFD currently operates out of 37 fire stations, located throughout the city in six battalions. The OKCFD also operates a fire apparatus fleet of 36 engines (including 30 paramedic engines), 13 ladders, 16 brush patrol units, six water tankers, two hazardous materials units, one Technical Rescue Unit, one Air Supply Unit, six Arson Investigation Units, and one Rehabilitation Unit. Each engine is staffed with a driver, an officer, and one to two firefighters, while each ladder company is staffed with a driver, an officer, and one firefighter. Minimum staffing per shift is 213 personnel. The Oklahoma City Fire Department responds to over 70,000 emergency calls annually.
Oklahoma City is an integral point on the United States Interstate Network, with three major interstate highways – Interstate 35, Interstate 40, and Interstate 44 – bisecting the city. Interstate 240 connects Interstate 40 and Interstate 44 in south Oklahoma City, while Interstate 235 spurs from Interstate 44 in north-central Oklahoma City into downtown.
Major state expressways through the city include Lake Hefner Parkway (SH-74), the Kilpatrick Turnpike, Airport Road (SH-152), and Broadway Extension (US-77) which continues from I-235 connecting Central Oklahoma City to Edmond. Lake Hefner Parkway runs through northwest Oklahoma City, while Airport Road runs through southwest Oklahoma City and leads to Will Rogers World Airport. The Kilpatrick Turnpike loops around north and west Oklahoma City.
Oklahoma City also has several major national and state highways within its city limits. Shields Boulevard (US-77) continues from E.K. Gaylord Boulevard in downtown Oklahoma City and runs south eventually connecting to I-35 near the suburb of Moore. Northwest Expressway (Oklahoma State Highway 3) runs from North Classen Boulevard in north-central Oklahoma City to the northwestern suburbs.
Oklahoma City is served by two primary airports, Will Rogers World Airport and the much smaller Wiley Post Airport (incidentally, the two honorees died in the same plane crash in Alaska) Will Rogers World Airport is the state's busiest commercial airport, with over 3.6 million passengers annually. Tinker Air Force Base, in southeast Oklahoma City, is the largest military air depot in the nation; a major maintenance and deployment facility for the Navy and the Air Force, and the second largest military institution in the state (after Fort Sill in Lawton).
METRO Transit is the city's public transit company. The main transfer terminal is located downtown at NW 5th Street and Hudson Avenue. METRO Transit maintains limited coverage of the city's main street grid using a hub-and-spoke system from the main terminal, making many journeys impractical due to the rather small number of bus routes offered and that most trips require a transfer downtown. The city has recognized that transit as a major issue for the rapidly growing and urbanizing city and has initiated several studies in recent times to improve upon the existing bus system starting with a plan known as the Fixed Guideway Study. This study identified several potential commuter transit routes from the suburbs into downtown OKC as well as feeder-line bus and/or rail routes throughout the city.
On December 2009, Oklahoma City voters passed MAPS 3, the $777 million (7-year 1-cent tax) initiative, which will include funding (appx $130M) for an estimated 5-to-6-mile (8.0 to 9.7 km) modern streetcar in downtown Oklahoma City and the establishment of a transit hub. It is believed the streetcar would begin construction in 2014 and be in operation around 2017.
Oklahoma City and the surrounding metropolitan area are home to a number of health care facilities and specialty hospitals. In Oklahoma City's MidTown district near downtown resides the state's oldest and largest single site hospital, St. Anthony Hospital and Physicians Medical Center.
OU Medicine, an academic medical institution located on the campus of The University of Oklahoma Health Sciences Center, is home to OU Medical Center. OU Medicine operates Oklahoma's only level-one trauma center at the OU Medical Center and the state's only level-one trauma center for children at Children's Hospital at OU Medicine, both of which are located in the Oklahoma Health Center district. Other medical facilities operated by OU Medicine include OU Physicians and OU Children's Physicians, the OU College of Medicine, the Oklahoma Cancer Center and OU Medical Center Edmond, the latter being located in the northern suburb of Edmond.
INTEGRIS Health owns several hospitals, including INTEGRIS Baptist Medical Center, the INTEGRIS Cancer Institute of Oklahoma, and the INTEGRIS Southwest Medical Center. INTEGRIS Health operates hospitals, rehabilitation centers, physician clinics, mental health facilities, independent living centers and home health agencies located throughout much of Oklahoma. INTEGRIS Baptist Medical Center was named in U.S. News & World Report's 2012 list of Best Hospitals. INTEGRIS Baptist Medical Center ranks high-performing in the following categories: Cardiology and Heart Surgery; Diabetes and Endocrinology; Ear, Nose and Throat; Gastroenterology; Geriatrics; Nephrology; Orthopedics; Pulmonology and Urology.
The Midwest Regional Medical Center located in the suburb of Midwest City; other major hospitals in the city include the Oklahoma Heart Hospital and the Mercy Health Center. There are 347 physicians for every 100,000 people in the city.
In the American College of Sports Medicine's annual ranking of the United States' 50 most populous metropolitan areas on the basis of community health, Oklahoma City took last place in 2010, falling five places from its 2009 rank of 45. The ACSM's report, published as part of its American Fitness Index program, cited, among other things, the poor diet of residents, low levels of physical fitness, higher incidences of obesity, diabetes, and cardiovascular disease than the national average, low access to recreational facilities like swimming pools and baseball diamonds, the paucity of parks and low investment by the city in their development, the high percentage of households below the poverty level, and the lack of state-mandated physical education curriculum as contributing factors.
Physically, clothing serves many purposes: it can serve as protection from the elements, and can enhance safety during hazardous activities such as hiking and cooking. It protects the wearer from rough surfaces, rash-causing plants, insect bites, splinters, thorns and prickles by providing a barrier between the skin and the environment. Clothes can insulate against cold or hot conditions. Further, they can provide a hygienic barrier, keeping infectious and toxic materials away from the body. Clothing also provides protection from harmful UV radiation.
There is no easy way to determine when clothing was first developed, but some information has been inferred by studying lice. The body louse specifically lives in clothing, and diverge from head lice about 107,000 years ago, suggesting that clothing existed at that time. Another theory is that modern humans are the only survivors of several species of primates who may have worn clothes and that clothing may have been used as long ago as 650 thousand years ago. Other louse-based estimates put the introduction of clothing at around 42,000–72,000 BP.
The most obvious function of clothing is to improve the comfort of the wearer, by protecting the wearer from the elements. In hot climates, clothing provides protection from sunburn or wind damage, while in cold climates its thermal insulation properties are generally more important. Shelter usually reduces the functional need for clothing. For example, coats, hats, gloves, and other superficial layers are normally removed when entering a warm home, particularly if one is residing or sleeping there. Similarly, clothing has seasonal and regional aspects, so that thinner materials and fewer layers of clothing are generally worn in warmer seasons and regions than in colder ones.
Clothing can and has in history been made from a very wide variety of materials. Materials have ranged from leather and furs, to woven materials, to elaborate and exotic natural and synthetic fabrics. Not all body coverings are regarded as clothing. Articles carried rather than worn (such as purses), worn on a single part of the body and easily removed (scarves), worn purely for adornment (jewelry), or those that serve a function other than protection (eyeglasses), are normally considered accessories rather than clothing, as are footwear and hats.
Clothing protects against many things that might injure the uncovered human body. Clothes protect people from the elements, including rain, snow, wind, and other weather, as well as from the sun. However, clothing that is too sheer, thin, small, tight, etc., offers less protection. Clothes also reduce risk during activities such as work or sport. Some clothing protects from specific environmental hazards, such as insects, noxious chemicals, weather, weapons, and contact with abrasive substances. Conversely, clothing may protect the environment from the clothing wearer, as with doctors wearing medical scrubs.
Humans have shown extreme inventiveness in devising clothing solutions to environmental hazards. Examples include: space suits, air conditioned clothing, armor, diving suits, swimsuits, bee-keeper gear, motorcycle leathers, high-visibility clothing, and other pieces of protective clothing. Meanwhile, the distinction between clothing and protective equipment is not always clear-cut—since clothes designed to be fashionable often have protective value and clothes designed for function often consider fashion in their design. Wearing clothes also has social implications. They cover parts of the body that social norms require to be covered, act as a form of adornment, and serve other social purposes.
Although dissertations on clothing and its function appear from the 19th century as colonising countries dealt with new environments, concerted scientific research into psycho-social, physiological and other functions of clothing (e.g. protective, cartage) occurred in the first half of the 20th century, with publications such as J. C. Flügel's Psychology of Clothes in 1930, and Newburgh's seminal Physiology of Heat Regulation and The Science of Clothing in 1949. By 1968, the field of environmental physiology had advanced and expanded significantly, but the science of clothing in relation to environmental physiology had changed little. While considerable research has since occurred and the knowledge-base has grown significantly, the main concepts remain unchanged, and indeed Newburgh's book is still cited by contemporary authors, including those attempting to develop thermoregulatory models of clothing development.
In Western societies, skirts, dresses and high-heeled shoes are usually seen as women's clothing, while neckties are usually seen as men's clothing. Trousers were once seen as exclusively male clothing, but are nowadays worn by both genders. Male clothes are often more practical (that is, they can function well under a wide variety of situations), but a wider range of clothing styles are available for females. Males are typically allowed to bare their chests in a greater variety of public places. It is generally acceptable for a woman to wear traditionally male clothing, while the converse is unusual.
In some societies, clothing may be used to indicate rank or status. In ancient Rome, for example, only senators could wear garments dyed with Tyrian purple. In traditional Hawaiian society, only high-ranking chiefs could wear feather cloaks and palaoa, or carved whale teeth. Under the Travancore Kingdom of Kerala, (India), lower caste women had to pay a tax for the right to cover their upper body. In China, before establishment of the republic, only the emperor could wear yellow. History provides many examples of elaborate sumptuary laws that regulated what people could wear. In societies without such laws, which includes most modern societies, social status is instead signaled by the purchase of rare or luxury items that are limited by cost to those with wealth or status. In addition, peer pressure influences clothing choice.
According to archaeologists and anthropologists, the earliest clothing likely consisted of fur, leather, leaves, or grass that were draped, wrapped, or tied around the body. Knowledge of such clothing remains inferential, since clothing materials deteriorate quickly compared to stone, bone, shell and metal artifacts. Archeologists have identified very early sewing needles of bone and ivory from about 30,000 BC, found near Kostenki, Russia in 1988. Dyed flax fibers that could have been used in clothing have been found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP.
Scientists are still debating when people started wearing clothes. Ralf Kittler, Manfred Kayser and Mark Stoneking, anthropologists at the Max Planck Institute for Evolutionary Anthropology, have conducted a genetic analysis of human body lice that suggests clothing originated quite recently, around 170,000 years ago. Body lice is an indicator of clothes-wearing, since most humans have sparse body hair, and lice thus require human clothing to survive. Their research suggests the invention of clothing may have coincided with the northward migration of modern Homo sapiens away from the warm climate of Africa, thought to have begun between 50,000 and 100,000 years ago. However, a second group of researchers using similar genetic methods estimate that clothing originated around 540,000 years ago (Reed et al. 2004. PLoS Biology 2(11): e340). For now, the date of the origin of clothing remains unresolved.[citation needed]
Different cultures have evolved various ways of creating clothes out of cloth. One approach simply involves draping the cloth. Many people wore, and still wear, garments consisting of rectangles of cloth wrapped to fit – for example, the dhoti for men and the sari for women in the Indian subcontinent, the Scottish kilt or the Javanese sarong. The clothes may simply be tied up, as is the case of the first two garments; or pins or belts hold the garments in place, as in the case of the latter two. The precious cloth remains uncut, and people of various sizes or the same person at different sizes can wear the garment.
By the early years of the 21st century, western clothing styles had, to some extent, become international styles. This process began hundreds of years earlier, during the periods of European colonialism. The process of cultural dissemination has perpetuated over the centuries as Western media corporations have penetrated markets throughout the world, spreading Western culture and styles. Fast fashion clothing has also become a global phenomenon. These garments are less expensive, mass-produced Western clothing. Donated used clothing from Western countries are also delivered to people in poor countries by charity organizations.
Most sports and physical activities are practiced wearing special clothing, for practical, comfort or safety reasons. Common sportswear garments include shorts, T-shirts, tennis shirts, leotards, tracksuits, and trainers. Specialized garments include wet suits (for swimming, diving or surfing), salopettes (for skiing) and leotards (for gymnastics). Also, spandex materials are often used as base layers to soak up sweat. Spandex is also preferable for active sports that require form fitting garments, such as volleyball, wrestling, track & field, dance, gymnastics and swimming.
The world of clothing is always changing, as new cultural influences meet technological innovations. Researchers in scientific labs have been developing prototypes for fabrics that can serve functional purposes well beyond their traditional roles, for example, clothes that can automatically adjust their temperature, repel bullets, project images, and generate electricity. Some practical advances already available to consumers are bullet-resistant garments made with kevlar and stain-resistant fabrics that are coated with chemical mixtures that reduce the absorption of liquids.
Though mechanization transformed most aspects of human industry by the mid-20th century, garment workers have continued to labor under challenging conditions that demand repetitive manual labor. Mass-produced clothing is often made in what are considered by some to be sweatshops, typified by long work hours, lack of benefits, and lack of worker representation. While most examples of such conditions are found in developing countries, clothes made in industrialized nations may also be manufactured similarly, often staffed by undocumented immigrants.[citation needed]
Outsourcing production to low wage countries like Bangladesh, China, India and Sri Lanka became possible when the Multi Fibre Agreement (MFA) was abolished. The MFA, which placed quotas on textiles imports, was deemed a protectionist measure.[citation needed] Globalization is often quoted as the single most contributing factor to the poor working conditions of garment workers. Although many countries recognize treaties like the International Labor Organization, which attempt to set standards for worker safety and rights, many countries have made exceptions to certain parts of the treaties or failed to thoroughly enforce them. India for example has not ratified sections 87 and 92 of the treaty.[citation needed]
The use of animal fur in clothing dates to prehistoric times. It is currently associated in developed countries with expensive, designer clothing, although fur is still used by indigenous people in arctic zones and higher elevations for its warmth and protection. Once uncontroversial, it has recently been the focus of campaigns on the grounds that campaigners consider it cruel and unnecessary. PETA, along with other animal rights and animal liberation groups have called attention to fur farming and other practices they consider cruel.
Many kinds of clothing are designed to be ironed before they are worn to remove wrinkles. Most modern formal and semi-formal clothing is in this category (for example, dress shirts and suits). Ironed clothes are believed to look clean, fresh, and neat. Much contemporary casual clothing is made of knit materials that do not readily wrinkle, and do not require ironing. Some clothing is permanent press, having been treated with a coating (such as polytetrafluoroethylene) that suppresses wrinkles and creates a smooth appearance without ironing.
A resin used for making non-wrinkle shirts releases formaldehyde, which could cause contact dermatitis for some people; no disclosure requirements exist, and in 2008 the U.S. Government Accountability Office tested formaldehyde in clothing and found that generally the highest levels were in non-wrinkle shirts and pants. In 1999, a study of the effect of washing on the formaldehyde levels found that after 6 months after washing, 7 of 27 shirts had levels in excess of 75 ppm, which is a safe limit for direct skin exposure.
In past times, mending was an art. A meticulous tailor or seamstress could mend rips with thread raveled from hems and seam edges so skillfully that the tear was practically invisible. When the raw material – cloth – was worth more than labor, it made sense to expend labor in saving it. Today clothing is considered a consumable item. Mass-manufactured clothing is less expensive than the labor required to repair it. Many people buy a new piece of clothing rather than spend time mending. The thrifty still replace zippers and buttons and sew up ripped hems.
The term high definition once described a series of television systems originating from August 1936; however, these systems were only high definition when compared to earlier systems that were based on mechanical systems with as few as 30 lines of resolution. The ongoing competition between companies and nations to create true "HDTV" spanned the entire 20th century, as each new system became more HD than the last.In the beginning of the 21st century, this race has continued with 4k, 5k and current 8K systems.
The British high-definition TV service started trials in August 1936 and a regular service on 2 November 1936 using both the (mechanical) Baird 240 line sequential scan (later to be inaccurately rechristened 'progressive') and the (electronic) Marconi-EMI 405 line interlaced systems. The Baird system was discontinued in February 1937. In 1938 France followed with their own 441-line system, variants of which were also used by a number of other countries. The US NTSC 525-line system joined in 1941. In 1949 France introduced an even higher-resolution standard at 819 lines, a system that should have been high definition even by today's standards, but was monochrome only and the technical limitations of the time prevented it from achieving the definition of which it should have been capable. All of these systems used interlacing and a 4:3 aspect ratio except the 240-line system which was progressive (actually described at the time by the technically correct term "sequential") and the 405-line system which started as 5:4 and later changed to 4:3. The 405-line system adopted the (at that time) revolutionary idea of interlaced scanning to overcome the flicker problem of the 240-line with its 25 Hz frame rate. The 240-line system could have doubled its frame rate but this would have meant that the transmitted signal would have doubled in bandwidth, an unacceptable option as the video baseband bandwidth was required to be not more than 3 MHz.
Colour broadcasts started at similarly higher resolutions, first with the US NTSC color system in 1953, which was compatible with the earlier monochrome systems and therefore had the same 525 lines of resolution. European standards did not follow until the 1960s, when the PAL and SECAM color systems were added to the monochrome 625 line broadcasts.
The Nippon Hōsō Kyōkai (NHK, the Japan Broadcasting Corporation) began conducting research to "unlock the fundamental mechanism of video and sound interactions with the five human senses" in 1964, after the Tokyo Olympics. NHK set out to create an HDTV system that ended up scoring much higher in subjective tests than NTSC's previously dubbed "HDTV". This new system, NHK Color, created in 1972, included 1125 lines, a 5:3 aspect ratio and 60 Hz refresh rate. The Society of Motion Picture and Television Engineers (SMPTE), headed by Charles Ginsburg, became the testing and study authority for HDTV technology in the international theater. SMPTE would test HDTV systems from different companies from every conceivable perspective, but the problem of combining the different formats plagued the technology for many years.
There were four major HDTV systems tested by SMPTE in the late 1970s, and in 1979 an SMPTE study group released A Study of High Definition Television Systems:
Since the formal adoption of digital video broadcasting's (DVB) widescreen HDTV transmission modes in the early 2000s; the 525-line NTSC (and PAL-M) systems, as well as the European 625-line PAL and SECAM systems, are now regarded as standard definition television systems.
In 1949, France started its transmissions with an 819 lines system (with 737 active lines). The system was monochrome only, and was used only on VHF for the first French TV channel. It was discontinued in 1983.
In 1958, the Soviet Union developed Тransformator (Russian: Трансформатор, meaning Transformer), the first high-resolution (definition) television system capable of producing an image composed of 1,125 lines of resolution aimed at providing teleconferencing for military command. It was a research project and the system was never deployed by either the military or consumer broadcasting.
In 1979, the Japanese state broadcaster NHK first developed consumer high-definition television with a 5:3 display aspect ratio. The system, known as Hi-Vision or MUSE after its Multiple sub-Nyquist sampling encoding for encoding the signal, required about twice the bandwidth of the existing NTSC system but provided about four times the resolution (1080i/1125 lines). Satellite test broadcasts started in 1989, with regular testing starting in 1991 and regular broadcasting of BS-9ch commencing on November 25, 1994, which featured commercial and NHK programming.
In 1981, the MUSE system was demonstrated for the first time in the United States, using the same 5:3 aspect ratio as the Japanese system. Upon visiting a demonstration of MUSE in Washington, US President Ronald Reagan was impressed and officially declared it "a matter of national interest" to introduce HDTV to the US.
Several systems were proposed as the new standard for the US, including the Japanese MUSE system, but all were rejected by the FCC because of their higher bandwidth requirements. At this time, the number of television channels was growing rapidly and bandwidth was already a problem. A new standard had to be more efficient, needing less bandwidth for HDTV than the existing NTSC.
The limited standardization of analog HDTV in the 1990s did not lead to global HDTV adoption as technical and economic constraints at the time did not permit HDTV to use bandwidths greater than normal television.
Early HDTV commercial experiments, such as NHK's MUSE, required over four times the bandwidth of a standard-definition broadcast. Despite efforts made to reduce analog HDTV to about twice the bandwidth of SDTV, these television formats were still distributable only by satellite.
In addition, recording and reproducing an HDTV signal was a significant technical challenge in the early years of HDTV (Sony HDVS). Japan remained the only country with successful public broadcasting of analog HDTV, with seven broadcasters sharing a single channel.
Since 1972, International Telecommunication Union's radio telecommunications sector (ITU-R) had been working on creating a global recommendation for Analog HDTV. These recommendations, however, did not fit in the broadcasting bands which could reach home users. The standardization of MPEG-1 in 1993 also led to the acceptance of recommendations ITU-R BT.709. In anticipation of these standards the Digital Video Broadcasting (DVB) organisation was formed, an alliance of broadcasters, consumer electronics manufacturers and regulatory bodies. The DVB develops and agrees upon specifications which are formally standardised by ETSI.
DVB created first the standard for DVB-S digital satellite TV, DVB-C digital cable TV and DVB-T digital terrestrial TV. These broadcasting systems can be used for both SDTV and HDTV. In the US the Grand Alliance proposed ATSC as the new standard for SDTV and HDTV. Both ATSC and DVB were based on the MPEG-2 standard, although DVB systems may also be used to transmit video using the newer and more efficient H.264/MPEG-4 AVC compression standards. Common for all DVB standards is the use of highly efficient modulation techniques for further reducing bandwidth, and foremost for reducing receiver-hardware and antenna requirements.
In 1983, the International Telecommunication Union's radio telecommunications sector (ITU-R) set up a working party (IWP11/6) with the aim of setting a single international HDTV standard. One of the thornier issues concerned a suitable frame/field refresh rate, the world already having split into two camps, 25/50 Hz and 30/60 Hz, largely due to the differences in mains frequency. The IWP11/6 working party considered many views and throughout the 1980s served to encourage development in a number of video digital processing areas, not least conversion between the two main frame/field rates using motion vectors, which led to further developments in other areas. While a comprehensive HDTV standard was not in the end established, agreement on the aspect ratio was achieved.
Initially the existing 5:3 aspect ratio had been the main candidate but, due to the influence of widescreen cinema, the aspect ratio 16:9 (1.78) eventually emerged as being a reasonable compromise between 5:3 (1.67) and the common 1.85 widescreen cinema format. An aspect ratio of 16:9 was duly agreed upon at the first meeting of the IWP11/6 working party at the BBC's Research and Development establishment in Kingswood Warren. The resulting ITU-R Recommendation ITU-R BT.709-2 ("Rec. 709") includes the 16:9 aspect ratio, a specified colorimetry, and the scan modes 1080i (1,080 actively interlaced lines of resolution) and 1080p (1,080 progressively scanned lines). The British Freeview HD trials used MBAFF, which contains both progressive and interlaced content in the same encoding.
It also includes the alternative 1440×1152 HDMAC scan format. (According to some reports, a mooted 750-line (720p) format (720 progressively scanned lines) was viewed by some at the ITU as an enhanced television format rather than a true HDTV format, and so was not included, although 1920×1080i and 1280×720p systems for a range of frame and field rates were defined by several US SMPTE standards.)
HDTV technology was introduced in the United States in the late 1980s and made official in 1993 by the Digital HDTV Grand Alliance, a group of television, electronic equipment, communications companies consisting of AT&T Bell Labs, General Instrument, Philips, Sarnoff, Thomson, Zenith and the Massachusetts Institute of Technology. Field testing of HDTV at 199 sites in the United States was completed August 14, 1994. The first public HDTV broadcast in the United States occurred on July 23, 1996 when the Raleigh, North Carolina television station WRAL-HD began broadcasting from the existing tower of WRAL-TV southeast of Raleigh, winning a race to be first with the HD Model Station in Washington, D.C., which began broadcasting July 31, 1996 with the callsign WHD-TV, based out of the facilities of NBC owned and operated station WRC-TV. The American Advanced Television Systems Committee (ATSC) HDTV system had its public launch on October 29, 1998, during the live coverage of astronaut John Glenn's return mission to space on board the Space Shuttle Discovery. The signal was transmitted coast-to-coast, and was seen by the public in science centers, and other public theaters specially equipped to receive and display the broadcast.
The first HDTV transmissions in Europe, albeit not direct-to-home, began in 1990, when the Italian broadcaster RAI used the HD-MAC and MUSE HDTV technologies to broadcast the 1990 FIFA World Cup. The matches were shown in 8 cinemas in Italy and 2 in Spain. The connection with Spain was made via the Olympus satellite link from Rome to Barcelona and then with a fiber optic connection from Barcelona to Madrid. After some HDTV transmissions in Europe the standard was abandoned in the mid-1990s.
The first regular broadcasts started on January 1, 2004 when the Belgian company Euro1080 launched the HD1 channel with the traditional Vienna New Year's Concert. Test transmissions had been active since the IBC exhibition in September 2003, but the New Year's Day broadcast marked the official launch of the HD1 channel, and the official start of direct-to-home HDTV in Europe.
Euro1080, a division of the former and now bankrupt Belgian TV services company Alfacam, broadcast HDTV channels to break the pan-European stalemate of "no HD broadcasts mean no HD TVs bought means no HD broadcasts ..." and kick-start HDTV interest in Europe. The HD1 channel was initially free-to-air and mainly comprised sporting, dramatic, musical and other cultural events broadcast with a multi-lingual soundtrack on a rolling schedule of 4 or 5 hours per day.
These first European HDTV broadcasts used the 1080i format with MPEG-2 compression on a DVB-S signal from SES's Astra 1H satellite. Euro1080 transmissions later changed to MPEG-4/AVC compression on a DVB-S2 signal in line with subsequent broadcast channels in Europe.
Despite delays in some countries, the number of European HD channels and viewers has risen steadily since the first HDTV broadcasts, with SES's annual Satellite Monitor market survey for 2010 reporting more than 200 commercial channels broadcasting in HD from Astra satellites, 185 million HD capable TVs sold in Europe (£60 million in 2010 alone), and 20 million households (27% of all European digital satellite TV homes) watching HD satellite broadcasts (16 million via Astra satellites).
In December 2009 the United Kingdom became the first European country to deploy high definition content using the new DVB-T2 transmission standard, as specified in the Digital TV Group (DTG) D-book, on digital terrestrial television.
The Freeview HD service currently contains 10 HD channels (as of December 2013[update]) and was rolled out region by region across the UK in accordance with the digital switchover process, finally being completed in October 2012. However, Freeview HD is not the first HDTV service over digital terrestrial television in Europe;
If all three parameters are used, they are specified in the following form: [frame size][scanning system][frame or field rate] or [frame size]/[frame or field rate][scanning system].[citation needed] Often, frame size or frame rate can be dropped if its value is implied from context. In this case, the remaining numeric parameter is specified first, followed by the scanning system.
For example, 1920×1080p25 identifies progressive scanning format with 25 frames per second, each frame being 1,920 pixels wide and 1,080 pixels high. The 1080i25 or 1080i50 notation identifies interlaced scanning format with 25 frames (50 fields) per second, each frame being 1,920 pixels wide and 1,080 pixels high. The 1080i30 or 1080i60 notation identifies interlaced scanning format with 30 frames (60 fields) per second, each frame being 1,920 pixels wide and 1,080 pixels high. The 720p60 notation identifies progressive scanning format with 60 frames per second, each frame being 720 pixels high; 1,280 pixels horizontally are implied.
50 Hz systems support three scanning rates: 50i, 25p and 50p. 60 Hz systems support a much wider set of frame rates: 59.94i, 60i, 23.976p, 24p, 29.97p, 30p, 59.94p and 60p. In the days of standard definition television, the fractional rates were often rounded up to whole numbers, e.g. 23.976p was often called 24p, or 59.94i was often called 60i. 60 Hz high definition television supports both fractional and slightly different integer rates, therefore strict usage of notation is required to avoid ambiguity. Nevertheless, 29.97i/59.94i is almost universally called 60i, likewise 23.976p is called 24p.
For the commercial naming of a product, the frame rate is often dropped and is implied from context (e.g., a 1080i television set). A frame rate can also be specified without a resolution. For example, 24p means 24 progressive scan frames per second, and 50i means 25 interlaced frames per second.
There is no single standard for HDTV color support. Colors are typically broadcast using a (10-bits per channel) YUV color space but, depending on the underlying image generating technologies of the receiver, are then subsequently converted to a RGB color space using standardized algorithms. When transmitted directly through the Internet, the colors are typically pre-converted to 8-bit RGB channels for additional storage savings with the assumption that it will only be viewed only on a (sRGB) computer screen. As an added benefit to the original broadcasters, the losses of the pre-conversion essentially make these files unsuitable for professional TV re-broadcasting.
At a minimum, HDTV has twice the linear resolution of standard-definition television (SDTV), thus showing greater detail than either analog television or regular DVD. The technical standards for broadcasting HDTV also handle the 16:9 aspect ratio images without using letterboxing or anamorphic stretching, thus increasing the effective image resolution.
A very high resolution source may require more bandwidth than available in order to be transmitted without loss of fidelity. The lossy compression that is used in all digital HDTV storage and transmission systems will distort the received picture, when compared to the uncompressed source.
The optimum format for a broadcast depends upon the type of videographic recording medium used and the image's characteristics. For best fidelity to the source the transmitted field ratio, lines, and frame rate should match those of the source.
PAL, SECAM and NTSC frame rates technically apply only to analogue standard definition television, not to digital or high definition broadcasts. However, with the roll out of digital broadcasting, and later HDTV broadcasting, countries retained their heritage systems. HDTV in former PAL and SECAM countries operates at a frame rate of 25/50 Hz, while HDTV in former NTSC countries operates at 30/60 Hz.
Standard 35mm photographic film used for cinema projection has a much higher image resolution than HDTV systems, and is exposed and projected at a rate of 24 frames per second (frame/s). To be shown on standard television, in PAL-system countries, cinema film is scanned at the TV rate of 25 frame/s, causing a speedup of 4.1 percent, which is generally considered acceptable. In NTSC-system countries, the TV scan rate of 30 frame/s would cause a perceptible speedup if the same were attempted, and the necessary correction is performed by a technique called 3:2 Pulldown: Over each successive pair of film frames, one is held for three video fields (1/20 of a second) and the next is held for two video fields (1/30 of a second), giving a total time for the two frames of 1/12 of a second and thus achieving the correct average film frame rate.
Non-cinematic HDTV video recordings intended for broadcast are typically recorded either in 720p or 1080i format as determined by the broadcaster. 720p is commonly used for Internet distribution of high-definition video, because most computer monitors operate in progressive-scan mode. 720p also imposes less strenuous storage and decoding requirements compared to both 1080i and 1080p. 1080p/24, 1080i/30, 1080i/25, and 720p/30 is most often used on Blu-ray Disc.
In the US, residents in the line of sight of television station broadcast antennas can receive free, over the air programming with a television set with an ATSC tuner (most sets sold since 2009 have this). This is achieved with a TV aerial, just as it has been since the 1940s except now the major network signals are broadcast in high definition (ABC, Fox, and Ion Television broadcast at 720p resolution; CBS, My Network TV, NBC, PBS, and The CW at 1080i). As their digital signals more efficiently use the broadcast channel, many broadcasters are adding multiple channels to their signals. Laws about antennas were updated before the change to digital terrestrial broadcasts. These new laws prohibit home owners' associations and city government from banning the installation of antennas.
Additionally, cable-ready TV sets can display HD content without using an external box. They have a QAM tuner built-in and/or a card slot for inserting a CableCARD.
High-definition image sources include terrestrial broadcast, direct broadcast satellite, digital cable, IPTV (including GoogleTV Roku boxes and AppleTV or built into "Smart Televisions"), Blu-ray video disc (BD), and internet downloads.
Sony's PlayStation 3 has extensive HD compatibility because of its built in Blu-ray disc based player, so does Microsoft's Xbox 360 with the addition of Netflix and Windows Media Center HTPC streaming capabilities, and the Zune marketplace where users can rent or purchase digital HD content. Recently, Nintendo released a next generation high definition gaming platform, The Wii U, which includes TV remote control features in addition to IPTV streaming features like Netflix. The HD capabilities of the consoles has influenced some developers to port games from past consoles onto the PS3, Xbox 360 and Wii U, often with remastered or upscaled graphics.
HDTV can be recorded to D-VHS (Digital-VHS or Data-VHS), W-VHS (analog only), to an HDTV-capable digital video recorder (for example DirecTV's high-definition Digital video recorder, Sky HD's set-top box, Dish Network's VIP 622 or VIP 722 high-definition Digital video recorder receivers, or TiVo's Series 3 or HD recorders), or an HDTV-ready HTPC. Some cable boxes are capable of receiving or recording two or more broadcasts at a time in HDTV format, and HDTV programming, some included in the monthly cable service subscription price, some for an additional fee, can be played back with the cable company's on-demand feature.
The massive amount of data storage required to archive uncompressed streams meant that inexpensive uncompressed storage options were not available to the consumer. In 2008, the Hauppauge 1212 Personal Video Recorder was introduced. This device accepts HD content through component video inputs and stores the content in MPEG-2 format in a .ts file or in a Blu-ray compatible format .m2ts file on the hard drive or DVD burner of a computer connected to the PVR through a USB 2.0 interface. More recent systems are able to record a broadcast high definition program in its 'as broadcast' format or transcode to a format more compatible with Blu-ray.
Analog tape recorders with bandwidth capable of recording analog HD signals, such as W-VHS recorders, are no longer produced for the consumer market and are both expensive and scarce in the secondary market.
In the United States, as part of the FCC's plug and play agreement, cable companies are required to provide customers who rent HD set-top boxes with a set-top box with "functional" FireWire (IEEE 1394) on request. None of the direct broadcast satellite providers have offered this feature on any of their supported boxes, but some cable TV companies have. As of July 2004[update], boxes are not included in the FCC mandate. This content is protected by encryption known as 5C. This encryption can prevent duplication of content or simply limit the number of copies permitted, thus effectively denying most if not all fair use of the content.
Norfolk Island (i/ˈnɔːrfək ˈaɪlənd/; Norfuk: Norf'k Ailen) is a small island in the Pacific Ocean located between Australia, New Zealand and New Caledonia, 1,412 kilometres (877 mi) directly east of mainland Australia's Evans Head, and about 900 kilometres (560 mi) from Lord Howe Island. The island is part of the Commonwealth of Australia. Together with two neighbouring islands, it forms one of Australia's external territories. It has 1,796 inhabitants living on a total area of about 35 km2 (14 sq mi). Its capital is Kingston.
Norfolk Island was colonised by East Polynesians but was long unpeopled when it was settled by Great Britain as part of its settlement of Australia from 1788. The island served as a convict penal settlement from 6 March 1788 until 5 May 1855, except for an 11-year hiatus between 15 February 1814 and 6 June 1825, when it lay abandoned. On 8 June 1856, permanent civilian residence on the island began when it was settled from Pitcairn Island. In 1913, the UK handed Norfolk over to Australia to administer as an external territory.
Sir John Call argued the advantages of Norfolk Island in that it was uninhabited and that New Zealand flax grew there. In 1786 the British government included Norfolk Island as an auxiliary settlement, as proposed by John Call, in its plan for colonisation of New South Wales. The decision to settle Norfolk Island was taken due to Empress Catherine II of Russia's decision to restrict sales of hemp. Practically all the hemp and flax required by the Royal Navy for cordage and sailcloth was imported from Russia.
As early as 1794, Lieutenant-Governor of New South Wales Francis Grose suggested its closure as a penal settlement, as it was too remote and difficult for shipping and too costly to maintain. The first group of people left in February 1805, and by 1808 only about 200 remained, forming a small settlement until the remnants were removed in 1813. A small party remained to slaughter stock and destroy all buildings, so that there would be no inducement for anyone, especially from other European powers, to visit and lay claim to the place. From 15 February 1814 to 6 June 1825 the island was abandoned.
In 1824 the British government instructed the Governor of New South Wales Thomas Brisbane to occupy Norfolk Island as a place to send "the worst description of convicts". Its remoteness, previously seen as a disadvantage, was now viewed as an asset for the detention of recalcitrant male prisoners. The convicts detained have long been assumed to be a hardcore of recidivists, or 'doubly-convicted capital respites' – that is, men transported to Australia who committed fresh colonial crimes for which they were sentenced to death, and were spared the gallows on condition of life at Norfolk Island. However, a recent study has demonstrated, utilising a database of 6,458 Norfolk Island convicts, that the reality was somewhat different: more than half were detained at Norfolk Island without ever receiving a colonial conviction, and only 15% had been reprieved from a death sentence. Furthermore, the overwhelming majority of convicts sent to Norfolk Island had committed non-violent property sentences, and the average length of detention was three years.
On 8 June 1856, the next settlement began on Norfolk Island. These were the descendants of Tahitians and the HMS Bounty mutineers, including those of Fletcher Christian. They resettled from the Pitcairn Islands, which had become too small for their growing population. On 3 May 1856, 193 persons left Pitcairn Islands aboard the "Morayshire". On 8 June 194 persons arrived, a baby having been born in transit. The Pitcairners occupied many of the buildings remaining from the penal settlements, and gradually established traditional farming and whaling industries on the island. Although some families decided to return to Pitcairn in 1858 and 1863, the island's population continued to grow. They accepted additional settlers, who often arrived with whaling fleets.
After the creation of the Commonwealth of Australia in 1901, Norfolk Island was placed under the authority of the new Commonwealth government to be administered as an external territory. During World War II, the island became a key airbase and refuelling depot between Australia and New Zealand, and New Zealand and the Solomon Islands. The airstrip was constructed by Australian, New Zealand and United States servicemen during 1942. Since Norfolk Island fell within New Zealand's area of responsibility it was garrisoned by a New Zealand Army unit known as N Force at a large Army camp which had the capacity to house a 1,500 strong force. N Force relieved a company of the Second Australian Imperial Force. The island proved too remote to come under attack during the war and N Force left the island in February 1944.
Financial problems and a reduction in tourism led to Norfolk Island's administration appealing to the Australian federal government for assistance in 2010. In return, the islanders were to pay income tax for the first time but would be eligible for greater welfare benefits. However, by May 2013 agreement had not been reached and islanders were having to leave to find work and welfare. An agreement was finally signed in Canberra on 12 March 2015 to replace self-government with a local council but against the wishes of the Norfolk Island government. A majority of Norfolk Islanders have objected to the Australian plan to make changes to Norfolk Island without first consulting them and allowing their say with 68% of voters against forced changes.
Norfolk Island is located in the South Pacific Ocean, east of the Australian mainland. Norfolk Island is the main island of the island group the territory encompasses and is located at 29°02′S 167°57′E﻿ / ﻿29.033°S 167.950°E﻿ / -29.033; 167.950. It has an area of 34.6 square kilometres (13.4 sq mi), with no large-scale internal bodies of water and 32 km (20 mi) of coastline. The island's highest point is Mount Bates (319 metres (1,047 feet) above sea level), located in the northwest quadrant of the island. The majority of the terrain is suitable for farming and other agricultural uses. Phillip Island, the second largest island of the territory, is located at 29°07′S 167°57′E﻿ / ﻿29.117°S 167.950°E﻿ / -29.117; 167.950, seven kilometres (4.3 miles) south of the main island.
The coastline of Norfolk Island consists, to varying degrees, of cliff faces. A downward slope exists towards Slaughter Bay and Emily Bay, the site of the original colonial settlement of Kingston. There are no safe harbour facilities on Norfolk Island, with loading jetties existing at Kingston and Cascade Bay. All goods not domestically produced are brought in by ship, usually to Cascade Bay. Emily Bay, protected from the Pacific Ocean by a small coral reef, is the only safe area for recreational swimming, although surfing waves can be found at Anson and Ball Bays.
Norfolk Island has 174 native plants; 51 of them are endemic. At least 18 of the endemic species are rare or threatened. The Norfolk Island palm (Rhopalostylis baueri) and the smooth tree-fern (Cyathea brownii), the tallest tree-fern in the world, are common in the Norfolk Island National Park but rare elsewhere on the island. Before European colonization, most of Norfolk Island was covered with subtropical rain forest, the canopy of which was made of Araucaria heterophylla (Norfolk Island pine) in exposed areas, and the palm Rhopalostylis baueri and tree ferns Cyathea brownii and C. australis in moister protected areas. The understory was thick with lianas and ferns covering the forest floor. Only one small tract (5 km2) of rainforest remains, which was declared as the Norfolk Island National Park in 1986.
As a relatively small and isolated oceanic island, Norfolk has few land birds but a high degree of endemicity among them. Many of the endemic species and subspecies have become extinct as a result of massive clearance of the island's native vegetation of subtropical rainforest for agriculture, hunting and persecution as agricultural pests. The birds have also suffered from the introduction of mammals such as rats, cats, pigs and goats, as well as from introduced competitors such as common blackbirds and crimson rosellas.
The Norfolk Island Group Nepean Island is also home to breeding seabirds. The providence petrel was hunted to local extinction by the beginning of the 19th century, but has shown signs of returning to breed on Phillip Island. Other seabirds breeding there include the white-necked petrel, Kermadec petrel, wedge-tailed shearwater, Australasian gannet, red-tailed tropicbird and grey ternlet. The sooty tern (known locally as the whale bird) has traditionally been subject to seasonal egg harvesting by Norfolk Islanders.
Cetaceans were historically abundant around the island as commercial hunts on the island was operating until 1956. Today, numbers of larger whales have disappeared, but even today many species such humpback whale, minke whale, sei whale, and dolphins can be observed close to shores, and scientific surveys have been conducted regularly. Southern right whales were once regular migrants to the Norfolk hence naming the island as the "Middle ground" by whalers, but had been severely depleted by historical hunts, and further by illegal Soviet and Japan whaling, resulting in none of very few, if remnants still live, right whales in these regions along with Lord Howe Island.
Sixty-two percent of islanders are Christians. After the death of the first chaplain Rev G. H. Nobbs in 1884, a Methodist church was formed and in 1891 a Seventh-day Adventist congregation led by one of Nobbs' sons. Some unhappiness with G. H. Nobbs, the more organised and formal ritual of the Church of England service arising from the influence of the Melanesian Mission, decline in spirituality, the influence of visiting American whalers, literature sent by Christians overseas impressed by the Pitcairn story, and the adoption of Seventh-day Adventism by the descendants of the mutineers still on Pitcairn, all contributed to these developments. The Roman Catholic Church began work in 1957 and in the late 1990s a group left the former Methodist (then Uniting Church) and formed a charismatic fellowship. In 2011, 34 percent of the ordinary residents identified as Anglican, 13 percent as Uniting Church, 12 percent as Roman Catholic and three percent as Seventh-day Adventist. Nine percent were from other religions. Twenty four percent had no religion, and seven percent did not indicate a religion. Typical ordinary congregations in any church do not exceed 30 local residents as of 2010[update]. The three older denominations have good facilities. Ministers are usually short-term visitors.
Islanders speak both English and a creole language known as Norfuk, a blend of 18th-century English and Tahitian. The Norfuk language is decreasing in popularity as more tourists travel to the island and more young people leave for work and study reasons; however, there are efforts to keep it alive via dictionaries and the renaming of some tourist attractions to their Norfuk equivalents. In 2004 an act of the Norfolk Island Assembly made it a co-official language of the island. The act is long-titled: "An Act to recognise the Norfolk Island Language (Norf'k) as an official language of Norfolk Island." The "language known as 'Norf'k'" is described as the language "that is spoken by descendants of the first free settlers of Norfolk Island who were descendants of the settlers of Pitcairn Island". The act recognises and protects use of the language but does not require it; in official use, it must be accompanied by an accurate translation into English. 32% of the total population reported speaking a language other than English in the 2011 census, and just under three-quarters of the ordinarily resident population could speak Norfuk.
Norfolk Island is the only non-mainland Australian territory to have achieved self-governance. The Norfolk Island Act 1979, passed by the Parliament of Australia in 1979, is the Act under which the island was governed until the passing of the Norfolk Island Legislation Amendment Act 2015. The Australian government maintains authority on the island through an Administrator, currently Gary Hardgrave. From 1979 to 2015, a Legislative Assembly was elected by popular vote for terms of not more than three years, although legislation passed by the Australian Parliament could extend its laws to the territory at will, including the power to override any laws made by the assembly.
The Assembly consisted of nine seats, with electors casting nine equal votes, of which no more than two could be given to any individual candidate. It is a method of voting called a "weighted first past the post system". Four of the members of the Assembly formed the Executive Council, which devised policy and acted as an advisory body to the Administrator. The last Chief Minister of Norfolk Island was Lisle Snell. Other ministers included: Minister for Tourism, Industry and Development; Minister for Finance; Minister for Cultural Heritage and Community Services; and Minister for Environment.
Disagreements over the island's relationship with Australia were put in sharper relief by a 2006 review undertaken by the Australian government. Under the more radical of two models proposed in the review, the island's legislative assembly would have been reduced to the status of a local council. However, in December 2006, citing the "significant disruption" that changes to the governance would impose on the island's economy, the Australian government ended the review leaving the existing governance arrangements unaltered.
It was announced on 19 March 2015 that self-governance for the island would be revoked by the Commonwealth and replaced by a local council with the state of New South Wales providing services to the island. A reason given was that the island had never gained self-sufficiency and was being heavily subsidised by the Commonwealth, by $12.5 million in 2015 alone. It meant that residents would have to start paying Australian income tax, but they would also be covered by Australian welfare schemes such as Centrelink and Medicare.
The Norfolk Island Legislative Assembly decided to hold a referendum on the proposal. On 8 May 2015, voters were asked if Norfolk Islanders should freely determine their political status and their economic, social and cultural development, and to "be consulted at referendum or plebiscite on the future model of governance for Norfolk Island before such changes are acted upon by the Australian parliament". 68% out of 912 voters voted in favour. The Norfolk Island Chief Minister, Lisle Snell, said that "the referendum results blow a hole in Canberra's assertion that the reforms introduced before the Australian Parliament that propose abolishing the Legislative Assembly and Norfolk Island Parliament were overwhelmingly supported by the people of Norfolk Island".
Norfolk Island was originally a colony acquired by settlement but was never within the British Settlements Act. It was accepted as a territory of Australia, separate from any state, by the Norfolk Island Act 1913 (Cth), passed under the territories power (Constitution section 122) and made effective in 1914. In 1976 the High Court of Australia held unanimously that Norfolk Island is a part of the Commonwealth. Again, in 2007 the High Court of Australia affirmed the validity of legislation that made Australian citizenship a necessary qualification for voting for, and standing for election to, the Legislative Assembly of Norfolk Island.
The island is subject to separate immigration controls from the remainder of Australia. Until recently immigration to Norfolk Island even by other Australian citizens was heavily restricted. In 2012, immigration controls were relaxed with the introduction of an Unrestricted Entry Permit for all Australian and New Zealand citizens upon arrival and the option to apply for residency; the only criteria are to pass a police check and be able to pay into the local health scheme. From 1 July 2016, the Australian migration system will replace the immigration arrangements currently maintained by the Norfolk Island Government.
Australian citizens and residents from other parts of the nation now have automatic right of residence on the island after meeting these criteria (Immigration (Amendment No. 2) Act 2012). Australian citizens must carry either a passport or a Document of Identity to travel to Norfolk Island. Citizens of all other nations must carry a passport to travel to Norfolk Island even if arriving from other parts of Australia. Holders of Australian visas who travel to Norfolk Island have departed the Australian Migration Zone. Unless they hold a multiple-entry visa, the visa will have ceased; in which case they will require another visa to re-enter mainland Australia.
Non-Australian citizens who are Australian permanent residents should be aware that during their stay on Norfolk Island they are "outside of Australia" for the purposes of the Migration Act. This means that not only will they need a still-valid migrant visa or Resident return visa to return from Norfolk Island to the mainland, but also the time spent in Norfolk Island will not be counted for satisfying the residence requirement for obtaining a Resident return visa in the future. On the other hand, as far as Australian nationality law is concerned, Norfolk Island is a part of Australia, and any time spent by an Australian permanent resident on Norfolk Island will count as time spent in Australia for the purpose of applying for Australian citizenship.
Norfolk Island Hospital is the only medical centre on the island. Medicare and the Pharmaceutical Benefits Scheme do not cover Norfolk Island. All visitors to Norfolk Island, including Australians, are recommended to purchase travel insurance. Although the hospital can perform minor surgery, serious medical conditions are not permitted to be treated on the island and patients are flown back to mainland Australia. Air charter transport can cost in the order of A$30,000. For serious emergencies, medical evacuations are provided by the Royal Australian Air Force. The island has one ambulance staffed by St John Ambulance Australia volunteers.
The Australian government controls the exclusive economic zone (EEZ) and revenue from it extending 200 nautical miles (370 km) around Norfolk Island (roughly 428,000km2) and territorial sea claims to three nautical miles (6 km) from the island. There is a strong belief on the island that some of the revenue generated from Norfolk's EEZ should be available to providing services such as health and infrastructure on the island, which the island has been responsible for, similar to how the Northern Territory is able to access revenue from their mineral resources. The exclusive economic zone provides the islanders with fish, its only major natural resource. Norfolk Island has no direct control over any marine areas but has an agreement with the Commonwealth through the Australian Fisheries Management Authority (AFMA) to fish "recreationally" in a small section of the EEZ known locally as "the Box". While there is speculation that the zone may include oil and gas deposits, this is not proven. There are no major arable lands or permanent farmlands, though about 25 per cent of the island is a permanent pasture. There is no irrigated land. The island uses the Australian dollar as its currency.
Residents of Norfolk Island do not pay Australian federal taxes, creating a tax haven for locals and visitors alike. Because there is no income tax, the island's legislative assembly raises money through an import duty, fuel levy, medicare levy, GST of 12% and local/international phone calls. In a move that apparently surprised many islanders the Chief Minister of Norfolk Island, David Buffett, announced on 6 November 2010 that the island would voluntarily surrender its tax free status in return for a financial bailout from the federal government to cover significant debts. The introduction of income taxation will now come into effect on July 1, 2016, with a variation of opinion on the island about these changes but with many understanding that for the island's governance to continue there is a need to pay into the commonwealth revenue pool so that the island can have assistance in supporting its delivery of State government responsibilities such as health, education, medicare, and infrastructure. Prior to these reforms residents of Norfolk Island were not entitled to social services. It appears that the reforms do extend to companies and trustees and not only individuals.
As of 2004[update], 2532 telephone main lines are in use, a mix of analog (2500) and digital (32) circuits. Satellite communications services are planned.[citation needed] There is one locally based radio station (Radio Norfolk 89.9FM), broadcasting on both AM and FM frequencies. There is also one TV station, Norfolk TV, featuring local programming, plus transmitters for Australian channels ABC, SBS, Imparja Television and Southern Cross Television. The Internet country code top-level domain (ccTLD) is .nf.
There are no railways, waterways, ports or harbours on the island. Loading jetties are located at Kingston and Cascade, but ships cannot get close to either of them. When a supply ship arrives, it is emptied by whaleboats towed by launches, five tonnes at a time. Which jetty is used depends on the prevailing weather on the day. The jetty on the leeward side of the island is often used. If the wind changes significantly during unloading/loading, the ship will move around to the other side. Visitors often gather to watch the activity when a supply ship arrives.
Daylight saving time (DST) or summer time is the practice of advancing clocks during summer months by one hour so that in the evening daylight is experienced an hour longer, while sacrificing normal sunrise times. Typically, regions with summer time adjust clocks forward one hour close to the start of spring and adjust them backward in the autumn to standard time.
New Zealander George Hudson proposed the modern idea of daylight saving in 1895. Germany and Austria-Hungary organized the first nationwide implementation, starting on 30 April 1916. Many countries have used it at various times since then, particularly since the energy crisis of the 1970s.
The practice has received both advocacy and criticism. Putting clocks forward benefits retailing, sports, and other activities that exploit sunlight after working hours, but can cause problems for evening entertainment and for other activities tied to sunlight, such as farming. Although some early proponents of DST aimed to reduce evening use of incandescent lighting, which used to be a primary use of electricity, modern heating and cooling usage patterns differ greatly and research about how DST affects energy use is limited or contradictory.
DST clock shifts sometimes complicate timekeeping and can disrupt travel, billing, record keeping, medical devices, heavy equipment, and sleep patterns. Computer software can often adjust clocks automatically, but policy changes by various jurisdictions of the dates and timings of DST may be confusing.
Industrialized societies generally follow a clock-based schedule for daily activities that do not change throughout the course of the year. The time of day that individuals begin and end work or school, and the coordination of mass transit, for example, usually remain constant year-round. In contrast, an agrarian society's daily routines for work and personal conduct are more likely governed by the length of daylight hours and by solar time, which change seasonally because of the Earth's axial tilt. North and south of the tropics daylight lasts longer in summer and shorter in winter, the effect becoming greater as one moves away from the tropics.
By synchronously resetting all clocks in a region to one hour ahead of Standard Time (one hour "fast"), individuals who follow such a year-round schedule will wake an hour earlier than they would have otherwise; they will begin and complete daily work routines an hour earlier, and they will have available to them an extra hour of daylight after their workday activities. However, they will have one less hour of daylight at the start of each day, making the policy less practical during winter.
While the times of sunrise and sunset change at roughly equal rates as the seasons change, proponents of Daylight Saving Time argue that most people prefer a greater increase in daylight hours after the typical "nine-to-five" workday. Supporters have also argued that DST decreases energy consumption by reducing the need for lighting and heating, but the actual effect on overall energy use is heavily disputed.
The manipulation of time at higher latitudes (for example Iceland, Nunavut or Alaska) has little impact on daily life, because the length of day and night changes more extremely throughout the seasons (in comparison to other latitudes), and thus sunrise and sunset times are significantly out of sync with standard working hours regardless of manipulations of the clock. DST is also of little use for locations near the equator, because these regions see only a small variation in daylight in the course of the year.
Although they did not fix their schedules to the clock in the modern sense, ancient civilizations adjusted daily schedules to the sun more flexibly than modern DST does, often dividing daylight into twelve hours regardless of day length, so that each daylight hour was longer during summer. For example, Roman water clocks had different scales for different months of the year: at Rome's latitude the third hour from sunrise, hora tertia, started by modern standards at 09:02 solar time and lasted 44 minutes at the winter solstice, but at the summer solstice it started at 06:58 and lasted 75 minutes. After ancient times, equal-length civil hours eventually supplanted unequal, so civil time no longer varies by season. Unequal hours are still used in a few traditional settings, such as some Mount Athos monasteries and all Jewish ceremonies.
During his time as an American envoy to France, Benjamin Franklin, publisher of the old English proverb, "Early to bed, and early to rise, makes a man healthy, wealthy and wise", anonymously published a letter suggesting that Parisians economize on candles by rising earlier to use morning sunlight. This 1784 satire proposed taxing shutters, rationing candles, and waking the public by ringing church bells and firing cannons at sunrise. Despite common misconception, Franklin did not actually propose DST; 18th-century Europe did not even keep precise schedules. However, this soon changed as rail and communication networks came to require a standardization of time unknown in Franklin's day.
Modern DST was first proposed by the New Zealand entomologist George Hudson, whose shift-work job gave him leisure time to collect insects, and led him to value after-hours daylight. In 1895 he presented a paper to the Wellington Philosophical Society proposing a two-hour daylight-saving shift, and after considerable interest was expressed in Christchurch, he followed up in an 1898 paper. Many publications credit DST's proposal to the prominent English builder and outdoorsman William Willett, who independently conceived DST in 1905 during a pre-breakfast ride, when he observed with dismay how many Londoners slept through a large part of a summer's day. An avid golfer, he also disliked cutting short his round at dusk. His solution was to advance the clock during the summer months, a proposal he published two years later. The proposal was taken up by the Liberal Member of Parliament (MP) Robert Pearce, who introduced the first Daylight Saving Bill to the House of Commons on 12 February 1908. A select committee was set up to examine the issue, but Pearce's bill did not become law, and several other bills failed in the following years. Willett lobbied for the proposal in the UK until his death in 1915.
Starting on 30 April 1916, Germany and its World War I ally Austria-Hungary were the first to use DST (German: Sommerzeit) as a way to conserve coal during wartime. Britain, most of its allies, and many European neutrals soon followed suit. Russia and a few other countries waited until the next year and the United States adopted it in 1918.
Broadly speaking, Daylight Saving Time was abandoned in the years after the war (with some notable exceptions including Canada, the UK, France, and Ireland for example). However, it was brought back for periods of time in many different places during the following decades, and commonly during the Second World War. It became widely adopted, particularly in North America and Europe starting in the 1970s as a result of the 1970s energy crisis.
Since then, the world has seen many enactments, adjustments, and repeals. For specific details, an overview is available at Daylight saving time by country.
In the case of the United States where a one-hour shift occurs at 02:00 local time, in spring the clock jumps forward from the last moment of 01:59 standard time to 03:00 DST and that day has 23 hours, whereas in autumn the clock jumps backward from the last moment of 01:59 DST to 01:00 standard time, repeating that hour, and that day has 25 hours. A digital display of local time does not read 02:00 exactly at the shift to summer time, but instead jumps from 01:59:59.9 forward to 03:00:00.0.
Clock shifts are usually scheduled near a weekend midnight to lessen disruption to weekday schedules. A one-hour shift is customary, but Australia's Lord Howe Island uses a half-hour shift. Twenty-minute and two-hour shifts have been used in the past.
Coordination strategies differ when adjacent time zones shift clocks. The European Union shifts all at once, at 01:00 UTC or 02:00 CET or 03:00 EET; for example, Eastern European Time is always one hour ahead of Central European Time. Most of North America shifts at 02:00 local time, so its zones do not shift at the same time; for example, Mountain Time is temporarily (for one hour) zero hours ahead of Pacific Time, instead of one hour ahead, in the autumn and two hours, instead of one, ahead of Pacific Time in the spring. In the past, Australian districts went even further and did not always agree on start and end dates; for example, in 2008 most DST-observing areas shifted clocks forward on October 5 but Western Australia shifted on October 26. In some cases only part of a country shifts; for example, in the US, Hawaii and most of Arizona do not observe DST.
Start and end dates vary with location and year. Since 1996 European Summer Time has been observed from the last Sunday in March to the last Sunday in October; previously the rules were not uniform across the European Union. Starting in 2007, most of the United States and Canada observe DST from the second Sunday in March to the first Sunday in November, almost two-thirds of the year. The 2007 US change was part of the Energy Policy Act of 2005; previously, from 1987 through 2006, the start and end dates were the first Sunday in April and the last Sunday in October, and Congress retains the right to go back to the previous dates now that an energy-consumption study has been done. Proponents for permanently retaining November as the month for ending DST point to Halloween as a reason to delay the change in order to allow extra daylight for the evening of October 31.
Beginning and ending dates are roughly the reverse in the southern hemisphere. For example, mainland Chile observed DST from the second Saturday in October to the second Saturday in March, with transitions at 24:00 local time. The time difference between the United Kingdom and mainland Chile could therefore be five hours during the Northern summer, three hours during the Southern summer and four hours a few weeks per year because of mismatch of changing dates.
DST is generally not observed near the equator, where sunrise times do not vary enough to justify it. Some countries observe it only in some regions; for example, southern Brazil observes it while equatorial Brazil does not. Only a minority of the world's population uses DST because Asia and Africa generally do not observe it.
Daylight saving has caused controversy since it began. Winston Churchill argued that it enlarges "the opportunities for the pursuit of health and happiness among the millions of people who live in this country" and pundits have dubbed it "Daylight Slaving Time". Historically, retailing, sports, and tourism interests have favored daylight saving, while agricultural and evening entertainment interests have opposed it, and its initial adoption had been prompted by energy crisis and war.
The fate of Willett's 1907 proposal illustrates several political issues involved. The proposal attracted many supporters, including Balfour, Churchill, Lloyd George, MacDonald, Edward VII (who used half-hour DST at Sandringham), the managing director of Harrods, and the manager of the National Bank. However, the opposition was stronger: it included Prime Minister H. H. Asquith, Christie (the Astronomer Royal), George Darwin, Napier Shaw (director of the Meteorological Office), many agricultural organizations, and theatre owners. After many hearings the proposal was narrowly defeated in a Parliament committee vote in 1909. Willett's allies introduced similar bills every year from 1911 through 1914, to no avail. The US was even more skeptical: Andrew Peters introduced a DST bill to the US House of Representatives in May 1909, but it soon died in committee.
After Germany led the way with starting DST (German: Sommerzeit) during World War I on 30 April 1916 together with its allies to alleviate hardships from wartime coal shortages and air raid blackouts, the political equation changed in other countries; the United Kingdom used DST first on 21 May 1916. US retailing and manufacturing interests led by Pittsburgh industrialist Robert Garland soon began lobbying for DST, but were opposed by railroads. The US's 1917 entry to the war overcame objections, and DST was established in 1918.
The war's end swung the pendulum back. Farmers continued to dislike DST, and many countries repealed it after the war. Britain was an exception: it retained DST nationwide but over the years adjusted transition dates for several reasons, including special rules during the 1920s and 1930s to avoid clock shifts on Easter mornings. The US was more typical: Congress repealed DST after 1919. President Woodrow Wilson, like Willett an avid golfer, vetoed the repeal twice but his second veto was overridden. Only a few US cities retained DST locally thereafter, including New York so that its financial exchanges could maintain an hour of arbitrage trading with London, and Chicago and Cleveland to keep pace with New York. Wilson's successor Warren G. Harding opposed DST as a "deception". Reasoning that people should instead get up and go to work earlier in the summer, he ordered District of Columbia federal employees to start work at 08:00 rather than 09:00 during summer 1922. Some businesses followed suit though many others did not; the experiment was not repeated.
The history of time in the United States includes DST during both world wars, but no standardization of peacetime DST until 1966. In May 1965, for two weeks, St. Paul, Minnesota and Minneapolis, Minnesota were on different times, when the capital city decided to join most of the nation by starting Daylight Saving Time while Minneapolis opted to follow the later date set by state law. In the mid-1980s, Clorox (parent of Kingsford Charcoal) and 7-Eleven provided the primary funding for the Daylight Saving Time Coalition behind the 1987 extension to US DST, and both Idaho senators voted for it based on the premise that during DST fast-food restaurants sell more French fries, which are made from Idaho potatoes.
In 1992, after a three-year trial of daylight saving in Queensland, Australia, a referendum on daylight saving was held and defeated with a 54.5% 'no' vote – with regional and rural areas strongly opposed, while those in the metropolitan south-east were in favor. In 2005, the Sporting Goods Manufacturers Association and the National Association of Convenience Stores successfully lobbied for the 2007 extension to US DST. In December 2008, the Daylight Saving for South East Queensland (DS4SEQ) political party was officially registered in Queensland, advocating the implementation of a dual-time zone arrangement for Daylight Saving in South East Queensland while the rest of the state maintains standard time. DS4SEQ contested the March 2009 Queensland State election with 32 candidates and received one percent of the statewide primary vote, equating to around 2.5% across the 32 electorates contested. After a three-year trial, more than 55% of Western Australians voted against DST in 2009, with rural areas strongly opposed. On 14 April 2010, after being approached by the DS4SEQ political party, Queensland Independent member Peter Wellington, introduced the Daylight Saving for South East Queensland Referendum Bill 2010 into Queensland Parliament, calling for a referendum to be held at the next State election on the introduction of daylight saving into South East Queensland under a dual-time zone arrangement. The Bill was defeated in Queensland Parliament on 15 June 2011.
In the UK the Royal Society for the Prevention of Accidents supports a proposal to observe SDST's additional hour year-round, but is opposed in some industries, such as postal workers and farmers, and particularly by those living in the northern regions of the UK.
In some Muslim countries DST is temporarily abandoned during Ramadan (the month when no food should be eaten between sunrise and sunset), since the DST would delay the evening dinner. Ramadan took place in July and August in 2012. This concerns at least Morocco and Palestine, although Iran keeps DST during Ramadan. Most Muslim countries do not use DST, partially for this reason.
The 2011 declaration by Russia that it would not turn its clocks back and stay in DST all year long was subsequently followed by a similar declaration from Belarus. The plan generated widespread complaints due to the dark of wintertime morning, and thus was abandoned in 2014. The country changed its clocks to Standard Time on 26 October 2014 - and intends to stay there permanently.
Proponents of DST generally argue that it saves energy, promotes outdoor leisure activity in the evening (in summer), and is therefore good for physical and psychological health, reduces traffic accidents, reduces crime, or is good for business. Groups that tend to support DST are urban workers, retail businesses, outdoor sports enthusiasts and businesses, tourism operators, and others who benefit from increased light during the evening in summer.
Opponents argue that actual energy savings are inconclusive, that DST increases health risks such as heart attack, that DST can disrupt morning activities, and that the act of changing clocks twice a year is economically and socially disruptive and cancels out any benefit. Farmers have tended to oppose DST.
Common agreement about the day's layout or schedule confers so many advantages that a standard DST schedule has generally been chosen over ad hoc efforts to get up earlier. The advantages of coordination are so great that many people ignore whether DST is in effect by altering their nominal work schedules to coordinate with television broadcasts or daylight. DST is commonly not observed during most of winter, because its mornings are darker; workers may have no sunlit leisure time, and children may need to leave for school in the dark. Since DST is applied to many varying communities, its effects may be very different depending on their culture, light levels, geography, and climate; that is why it is hard to make generalized conclusions about the absolute effects of the practice. Some areas may adopt DST simply as a matter of coordination with others rather than for any direct benefits.
DST's potential to save energy comes primarily from its effects on residential lighting, which consumes about 3.5% of electricity in the United States and Canada. Delaying the nominal time of sunset and sunrise reduces the use of artificial light in the evening and increases it in the morning. As Franklin's 1784 satire pointed out, lighting costs are reduced if the evening reduction outweighs the morning increase, as in high-latitude summer when most people wake up well after sunrise. An early goal of DST was to reduce evening usage of incandescent lighting, which used to be a primary use of electricity. Although energy conservation remains an important goal, energy usage patterns have greatly changed since then, and recent research is limited and reports contradictory results. Electricity use is greatly affected by geography, climate, and economics, making it hard to generalize from single studies.
Several studies have suggested that DST increases motor fuel consumption. The 2008 DOE report found no significant increase in motor gasoline consumption due to the 2007 United States extension of DST.
Retailers, sporting goods makers, and other businesses benefit from extra afternoon sunlight, as it induces customers to shop and to participate in outdoor afternoon sports. In 1984, Fortune magazine estimated that a seven-week extension of DST would yield an additional $30 million for 7-Eleven stores, and the National Golf Foundation estimated the extension would increase golf industry revenues $200 million to $300 million. A 1999 study estimated that DST increases the revenue of the European Union's leisure sector by about 3%.
Conversely, DST can adversely affect farmers, parents of young children, and others whose hours are set by the sun and they have traditionally opposed the practice, although some farmers are neutral. One reason why farmers oppose DST is that grain is best harvested after dew evaporates, so when field hands arrive and leave earlier in summer their labor is less valuable. Dairy farmers are another group who complain of the change. Their cows are sensitive to the timing of milking, so delivering milk earlier disrupts their systems. Today some farmers' groups are in favor of DST.
Changing clocks and DST rules has a direct economic cost, entailing extra work to support remote meetings, computer applications and the like. For example, a 2007 North American rule change cost an estimated $500 million to $1 billion, and Utah State University economist William F. Shughart II has estimated the lost opportunity cost at around $1.7 billion USD. Although it has been argued that clock shifts correlate with decreased economic efficiency, and that in 2000 the daylight-saving effect implied an estimated one-day loss of $31 billion on US stock exchanges, the estimated numbers depend on the methodology. The results have been disputed, and the original authors have refuted the points raised by disputers.
In 1975 the US DOT conservatively identified a 0.7% reduction in traffic fatalities during DST, and estimated the real reduction at 1.5% to 2%, but the 1976 NBS review of the DOT study found no differences in traffic fatalities. In 1995 the Insurance Institute for Highway Safety estimated a reduction of 1.2%, including a 5% reduction in crashes fatal to pedestrians. Others have found similar reductions. Single/Double Summer Time (SDST), a variant where clocks are one hour ahead of the sun in winter and two in summer, has been projected to reduce traffic fatalities by 3% to 4% in the UK, compared to ordinary DST. However, accidents do increase by as much as 11% during the two weeks that follow the end of British Summer Time. It is not clear whether sleep disruption contributes to fatal accidents immediately after the spring clock shifts. A correlation between clock shifts and traffic accidents has been observed in North America and the UK but not in Finland or Sweden. If this effect exists, it is far smaller than the overall reduction in traffic fatalities. A 2009 US study found that on Mondays after the switch to DST, workers sleep an average of 40 minutes less, and are injured at work more often and more severely.
In the 1970s the US Law Enforcement Assistance Administration (LEAA) found a reduction of 10% to 13% in Washington, D.C.'s violent crime rate during DST. However, the LEAA did not filter out other factors, and it examined only two cities and found crime reductions only in one and only in some crime categories; the DOT decided it was "impossible to conclude with any confidence that comparable benefits would be found nationwide". Outdoor lighting has a marginal and sometimes even contradictory influence on crime and fear of crime.
In several countries, fire safety officials encourage citizens to use the two annual clock shifts as reminders to replace batteries in smoke and carbon monoxide detectors, particularly in autumn, just before the heating and candle season causes an increase in home fires. Similar twice-yearly tasks include reviewing and practicing fire escape and family disaster plans, inspecting vehicle lights, checking storage areas for hazardous materials, reprogramming thermostats, and seasonal vaccinations. Locations without DST can instead use the first days of spring and autumn as reminders.
DST has mixed effects on health. In societies with fixed work schedules it provides more afternoon sunlight for outdoor exercise. It alters sunlight exposure; whether this is beneficial depends on one's location and daily schedule, as sunlight triggers vitamin D synthesis in the skin, but overexposure can lead to skin cancer. DST may help in depression by causing individuals to rise earlier, but some argue the reverse. The Retinitis Pigmentosa Foundation Fighting Blindness, chaired by blind sports magnate Gordon Gund, successfully lobbied in 1985 and 2005 for US DST extensions.
Clock shifts were found to increase the risk of heart attack by 10 percent, and to disrupt sleep and reduce its efficiency. Effects on seasonal adaptation of the circadian rhythm can be severe and last for weeks. A 2008 study found that although male suicide rates rise in the weeks after the spring transition, the relationship weakened greatly after adjusting for season. A 2008 Swedish study found that heart attacks were significantly more common the first three weekdays after the spring transition, and significantly less common the first weekday after the autumn transition. The government of Kazakhstan cited health complications due to clock shifts as a reason for abolishing DST in 2005. In March 2011, Dmitri Medvedev, president of Russia, claimed that "stress of changing clocks" was the motivation for Russia to stay in DST all year long. Officials at the time talked about an annual increase in suicides.
An unexpected adverse effect of daylight saving time may lie in the fact that an extra part of morning rush hour traffic occurs before dawn and traffic emissions then cause higher air pollution than during daylight hours.
DST's clock shifts have the obvious disadvantage of complexity. People must remember to change their clocks; this can be time-consuming, particularly for mechanical clocks that cannot be moved backward safely. People who work across time zone boundaries need to keep track of multiple DST rules, as not all locations observe DST or observe it the same way. The length of the calendar day becomes variable; it is no longer always 24 hours. Disruption to meetings, travel, broadcasts, billing systems, and records management is common, and can be expensive. During an autumn transition from 02:00 to 01:00, a clock reads times from 01:00:00 through 01:59:59 twice, possibly leading to confusion.
Damage to a German steel facility occurred during a DST transition in 1993, when a computer timing system linked to a radio time synchronization signal allowed molten steel to cool for one hour less than the required duration, resulting in spattering of molten steel when it was poured. Medical devices may generate adverse events that could harm patients, without being obvious to clinicians responsible for care. These problems are compounded when the DST rules themselves change; software developers must test and perhaps modify many programs, and users must install updates and restart applications. Consumers must update devices such as programmable thermostats with the correct DST rules, or manually adjust the devices' clocks. A common strategy to resolve these problems in computer systems is to express time using the Coordinated Universal Time (UTC) rather than the local time zone. For example, Unix-based computer systems use the UTC-based Unix time internally.
Some clock-shift problems could be avoided by adjusting clocks continuously or at least more gradually—for example, Willett at first suggested weekly 20-minute transitions—but this would add complexity and has never been implemented.
DST inherits and can magnify the disadvantages of standard time. For example, when reading a sundial, one must compensate for it along with time zone and natural discrepancies. Also, sun-exposure guidelines such as avoiding the sun within two hours of noon become less accurate when DST is in effect.
As explained by Richard Meade in the English Journal of the (American) National Council of Teachers of English, the form daylight savings time (with an "s") was already in 1978 much more common than the older form daylight saving time in American English ("the change has been virtually accomplished"). Nevertheless, even dictionaries such as Merriam-Webster's, American Heritage, and Oxford, which describe actual usage instead of prescribing outdated usage (and therefore also list the newer form), still list the older form first. This is because the older form is still very common in print and preferred by many editors. ("Although daylight saving time is considered correct, daylight savings time (with an "s") is commonly used.") The first two words are sometimes hyphenated (daylight-saving[s] time). Merriam-Webster's also lists the forms daylight saving (without "time"), daylight savings (without "time"), and daylight time.
In Britain, Willett's 1907 proposal used the term daylight saving, but by 1911 the term summer time replaced daylight saving time in draft legislation. Continental Europe uses similar phrases, such as Sommerzeit in Germany, zomertijd in Dutch-speaking regions, kesäaika in Finland, horario de verano or hora de verano in Spain and heure d'été in France, whereas in Italy the term is ora legale, that is, legal time (legally enforced time) as opposed to "ora solare", solar time, in winter.
The name of local time typically changes when DST is observed. American English replaces standard with daylight: for example, Pacific Standard Time (PST) becomes Pacific Daylight Time (PDT). In the United Kingdom, the standard term for UK time when advanced by one hour is British Summer Time (BST), and British English typically inserts summer into other time zone names, e.g. Central European Time (CET) becomes Central European Summer Time (CEST).
The North American mnemonic "spring forward, fall back" (also "spring ahead ...", "spring up ...", and "... fall behind") helps people remember which direction to shift clocks.
Changes to DST rules cause problems in existing computer installations. For example, the 2007 change to DST rules in North America required many computer systems to be upgraded, with the greatest impact on email and calendaring programs; the upgrades consumed a significant effort by corporate information technologists.
Some applications standardize on UTC to avoid problems with clock shifts and time zone differences. Likewise, most modern operating systems internally handle and store all times as UTC and only convert to local time for display.
However, even if UTC is used internally, the systems still require information on time zones to correctly calculate local time where it is needed. Many systems in use today base their date/time calculations from data derived from the IANA time zone database also known as zoneinfo.
The IANA time zone database maps a name to the named location's historical and predicted clock shifts. This database is used by many computer software systems, including most Unix-like operating systems, Java, and the Oracle RDBMS; HP's "tztab" database is similar but incompatible. When temporal authorities change DST rules, zoneinfo updates are installed as part of ordinary system maintenance. In Unix-like systems the TZ environment variable specifies the location name, as in TZ=':America/New_York'. In many of those systems there is also a system-wide setting that is applied if the TZ environment variable isn't set: this setting is controlled by the contents of the /etc/localtime file, which is usually a symbolic link or hard link to one of the zoneinfo files. Internal time is stored in timezone-independent epoch time; the TZ is used by each of potentially many simultaneous users and processes to independently localize time display.
Older or stripped-down systems may support only the TZ values required by POSIX, which specify at most one start and end rule explicitly in the value. For example, TZ='EST5EDT,M3.2.0/02:00,M11.1.0/02:00' specifies time for the eastern United States starting in 2007. Such a TZ value must be changed whenever DST rules change, and the new value applies to all years, mishandling some older timestamps.
As with zoneinfo, a user of Microsoft Windows configures DST by specifying the name of a location, and the operating system then consults a table of rule sets that must be updated when DST rules change. Procedures for specifying the name and updating the table vary with release. Updates are not issued for older versions of Microsoft Windows. Windows Vista supports at most two start and end rules per time zone setting. In a Canadian location observing DST, a single Vista setting supports both 1987–2006 and post-2006 time stamps, but mishandles some older time stamps. Older Microsoft Windows systems usually store only a single start and end rule for each zone, so that the same Canadian setting reliably supports only post-2006 time stamps.
These limitations have caused problems. For example, before 2005, DST in Israel varied each year and was skipped some years. Windows 95 used rules correct for 1995 only, causing problems in later years. In Windows 98, Microsoft marked Israel as not having DST, forcing Israeli users to shift their computer clocks manually twice a year. The 2005 Israeli Daylight Saving Law established predictable rules using the Jewish calendar but Windows zone files could not represent the rules' dates in a year-independent way. Partial workarounds, which mishandled older time stamps, included manually switching zone files every year and a Microsoft tool that switches zones automatically. In 2013, Israel standardized its daylight saving time according to the Gregorian calendar.
Microsoft Windows keeps the system real-time clock in local time. This causes several problems, including compatibility when multi booting with operating systems that set the clock to UTC, and double-adjusting the clock when multi booting different Windows versions, such as with a rescue boot disk. This approach is a problem even in Windows-only systems: there is no support for per-user timezone settings, only a single system-wide setting. In 2008 Microsoft hinted that future versions of Windows will partially support a Windows registry entry RealTimeIsUniversal that had been introduced many years earlier, when Windows NT supported RISC machines with UTC clocks, but had not been maintained. Since then at least two fixes related to this feature have been published by Microsoft.
The NTFS file system used by recent versions of Windows stores the file with a UTC time stamp, but displays it corrected to local—or seasonal—time. However, the FAT filesystem commonly used on removable devices stores only the local time. Consequently, when a file is copied from the hard disk onto separate media, its time will be set to the current local time. If the time adjustment is changed, the timestamps of the original file and the copy will be different. The same effect can be observed when compressing and uncompressing files with some file archivers. It is the NTFS file that changes seen time. This effect should be kept in mind when trying to determine if a file is a duplicate of another, although there are other methods of comparing files for equality (such as using a checksum algorithm).
A move to "permanent daylight saving time" (staying on summer hours all year with no time shifts) is sometimes advocated, and has in fact been implemented in some jurisdictions such as Argentina, Chile, Iceland, Singapore, Uzbekistan and Belarus. Advocates cite the same advantages as normal DST without the problems associated with the twice yearly time shifts. However, many remain unconvinced of the benefits, citing the same problems and the relatively late sunrises, particularly in winter, that year-round DST entails. Russia switched to permanent DST from 2011 to 2014, but the move proved unpopular because of the late sunrises in winter, so the country switched permanently back to "standard" or "winter" time in 2014.
Xinjiang, China; Argentina; Chile; Iceland; Russia and other areas skew time zones westward, in effect observing DST year-round without complications from clock shifts. For example, Saskatoon, Saskatchewan, is at 106°39′ W longitude, slightly west of center of the idealized Mountain Time Zone (105° W), but the time in Saskatchewan is Central Standard Time (90° W) year-round, so Saskatoon is always about 67 minutes ahead of mean solar time, thus effectively observing daylight saving time year-round. Conversely, northeast India and a few other areas skew time zones eastward, in effect observing negative DST. The United Kingdom and Ireland experimented with year-round DST from 1968 to 1971 but abandoned it because of its unpopularity, particularly in northern regions.
Western France, Spain, and other areas skew time zones and shift clocks, in effect observing DST in winter with an extra hour in summer. Nome, Alaska, is at 165°24′ W longitude, which is just west of center of the idealized Samoa Time Zone (165° W), but Nome observes Alaska Time (135° W) with DST, so it is slightly more than two hours ahead of the sun in winter and three in summer. Double daylight saving time has been used on occasion; for example, it was used in some European countries during and shortly after World War II when it was referred to as "Double Summer Time". See British Double Summer Time and Central European Midsummer Time for details.
Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expression, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through paralanguage. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion.
Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out "noise", i.e. similar molecules without biotic content.
Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
Effective verbal or spoken communication is dependent on a number of factors and cannot be fully isolated from other important interpersonal skills such as non-verbal communication, listening skills and clarification. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word "language" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages. Communication is two-way process not merely one-way.
Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.
The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication.
The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:
In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of "communication noise" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized "nervous system" of plants. The original meaning of the word "neuron" in Greek is "vegetable fiber" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed]
Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997).
Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.
Slavs are the largest Indo-European ethno-linguistic group in Europe. They inhabit Central Europe, Eastern Europe, Southeast Europe, North Asia and Central Asia. Slavs speak Indo-European Slavic languages and share, to varying degrees, some cultural traits and historical backgrounds. From the early 6th century they spread to inhabit most of Central and Eastern Europe and Southeast Europe, whilst Slavic mercenaries fighting for the Byzantines and Arabs settled Asia Minor and even as far as Syria. The East Slavs colonised Siberia and Central Asia.[better source needed] Presently over half of Europe's territory is inhabited by Slavic-speaking communities, but every Slavic ethnicity has emigrated to other continents.
Present-day Slavic people are classified into West Slavic (chiefly Poles, Czechs and Slovaks), East Slavic (chiefly Russians, Belarusians, and Ukrainians), and South Slavic (chiefly Serbs, Bulgarians, Croats, Bosniaks, Macedonians, Slovenes, and Montenegrins), though sometimes the West Slavs and East Slavs are combined into a single group known as North Slavs. For a more comprehensive list, see the ethnocultural subdivisions. Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual ethnic groups themselves – are varied, ranging from a sense of connection to mutual feelings of hostility.
The Slavic autonym is reconstructed in Proto-Slavic as *Slověninъ, plural *Slověne. The oldest documents written in Old Church Slavonic and dating from the 9th century attest Словѣне Slověne to describe the Slavs. Other early Slavic attestations include Old East Slavic Словѣнѣ Slověně for "an East Slavic group near Novgorod." However, the earliest written references to the Slavs under this name are in other languages. In the 6th century AD Procopius, writing in Byzantine Greek, refers to the Σκλάβοι Sklaboi, Σκλαβηνοί Sklabēnoi, Σκλαυηνοί Sklauenoi, Σθλαβηνοί Sthlabenoi, or Σκλαβῖνοι Sklabinoi, while his contemporary Jordanes refers to the Sclaveni in Latin.
The Slavic autonym *Slověninъ is usually considered (e.g. by Roman Jakobson) a derivation from slovo "word", originally denoting "people who speak (the same language)," i.e. people who understand each other, in contrast to the Slavic word denoting "foreign people" – němci, meaning "mumbling, murmuring people" (from Slavic *němъ – "mumbling, mute").
The word slovo ("word") and the related slava ("fame") and slukh ("hearing") originate from the Proto-Indo-European root *ḱlew- ("be spoken of, fame"), cognate with Ancient Greek κλῆς (klês - "famous"), whence the name Pericles, and Latin clueo ("be called"), and English loud.
The English word Slav could be derived from the Middle English word sclave, which was borrowed from Medieval Latin sclavus or slavus, itself a borrowing and Byzantine Greek σκλάβος sklábos "slave," which was in turn apparently derived from a misunderstanding of the Slavic autonym (denoting a speaker of their own languages). The Byzantine term Sklavinoi was loaned into Arabic as Saqaliba صقالبة (sing. Saqlabi صقلبي) by medieval Arab historiographers. However, the origin of this word is disputed.
Alternative proposals for the etymology of *Slověninъ propounded by some scholars have much less support. Lozinski argues that the word *slava once had the meaning of worshipper, in this context meaning "practicer of a common Slavic religion," and from that evolved into an ethnonym. S.B. Bernstein speculates that it derives from a reconstructed Proto-Indo-European *(s)lawos, cognate to Ancient Greek λαός laós "population, people," which itself has no commonly accepted etymology. Meanwhile, others have pointed out that the suffix -enin indicates a man from a certain place, which in this case should be a place called Slova or Slava, possibly a river name. The Old East Slavic Slavuta for the Dnieper River was argued by Henrich Bartek (1907–1986) to be derived from slova and also the origin of Slovene.
The earliest mentions of Slavic raids across the lower River Danube may be dated to the first half of the 6th century, yet no archaeological evidence of a Slavic settlement in the Balkans could be securely dated before c. 600 AD.
The Slavs under name of the Antes and the Sclaveni make their first appearance in Byzantine records in the early 6th century. Byzantine historiographers under Justinian I (527–565), such as Procopius of Caesarea, Jordanes and Theophylact Simocatta describe tribes of these names emerging from the area of the Carpathian Mountains, the lower Danube and the Black Sea, invading the Danubian provinces of the Eastern Empire.
Procopius wrote in 545 that "the Sclaveni and the Antae actually had a single name in the remote past; for they were both called Spori in olden times." He describes their social structure and beliefs:
Jordanes tells us that the Sclaveni had swamps and forests for their cities. Another 6th-century source refers to them living among nearly impenetrable forests, rivers, lakes, and marshes.
Menander Protector mentions a Daurentius (577–579) that slew an Avar envoy of Khagan Bayan I. The Avars asked the Slavs to accept the suzerainty of the Avars, he however declined and is reported as saying: "Others do not conquer our land, we conquer theirs – so it shall always be for us".
The relationship between the Slavs and a tribe called the Veneti east of the River Vistula in the Roman period is uncertain. The name may refer both to Balts and Slavs.
According to eastern homeland theory, prior to becoming known to the Roman world, Slavic-speaking tribes were part of the many multi-ethnic confederacies of Eurasia – such as the Sarmatian, Hun and Gothic empires. The Slavs emerged from obscurity when the westward movement of Germans in the 5th and 6th centuries CE (thought to be in conjunction with the movement of peoples from Siberia and Eastern Europe: Huns, and later Avars and Bulgars) started the great migration of the Slavs, who settled the lands abandoned by Germanic tribes fleeing the Huns and their allies: westward into the country between the Oder and the Elbe-Saale line; southward into Bohemia, Moravia, much of present-day Austria, the Pannonian plain and the Balkans; and northward along the upper Dnieper river. Perhaps some Slavs migrated with the movement of the Vandals to Iberia and north Africa.
Around the 6th century, Slavs appeared on Byzantine borders in great numbers.[page needed] The Byzantine records note that grass would not regrow in places where the Slavs had marched through, so great were their numbers. After a military movement even the Peloponnese and Asia Minor were reported to have Slavic settlements. This southern movement has traditionally been seen as an invasive expansion. By the end of the 6th century, Slavs had settled the Eastern Alps regions.
When their migratory movements ended, there appeared among the Slavs the first rudiments of state organizations, each headed by a prince with a treasury and a defense force. Moreover, it was the beginnings of class differentiation, and nobles pledged allegiance either to the Frankish/ Holy Roman Emperors or the Byzantine Emperors.
In the 7th century, the Frankish merchant Samo, who supported the Slavs fighting their Avar rulers, became the ruler of the first known Slav state in Central Europe, which, however, most probably did not outlive its founder and ruler. This provided the foundation for subsequent Slavic states to arise on the former territory of this realm with Carantania being the oldest of them. Very old also are the Principality of Nitra and the Moravian principality (see under Great Moravia). In this period, there existed central Slavic groups and states such as the Balaton Principality, but the subsequent expansion of the Magyars, as well as the Germanisation of Austria, separated the northern and southern Slavs. The First Bulgarian Empire was founded in 681, the Slavic language Old Bulgarian became the main and official of the empire in 864. Bulgaria was instrumental in the spread of Slavic literacy and Christianity to the rest of the Slavic world.
As of 1878, there were only three free Slavic states in the world: the Russian Empire, Serbia and Montenegro. Bulgaria was also free but was de jure vassal to the Ottoman Empire until official independence was declared in 1908. In the entire Austro-Hungarian Empire of approximately 50 million people, about 23 million were Slavs. The Slavic peoples who were, for the most part, denied a voice in the affairs of the Austro-Hungarian Empire, were calling for national self-determination. During World War I, representatives of the Czechs, Slovaks, Poles, Serbs, Croats, and Slovenes set up organizations in the Allied countries to gain sympathy and recognition. In 1918, after World War I ended, the Slavs established such independent states as Czechoslovakia, the Second Polish Republic, and the State of Slovenes, Croats and Serbs.
During World War II, Hitler's Generalplan Ost (general plan for the East) entailed killing, deporting, or enslaving the Slavic and Jewish population of occupied Eastern Europe to create Lebensraum (living space) for German settlers. The Nazi Hunger Plan and Generalplan Ost would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the deaths of an estimated 19.3 million civilians and prisoners of war.
The first half of the 20th century in Russia and the Soviet Union was marked by a succession of wars, famines and other disasters, each accompanied by large-scale population losses. Stephen J. Lee estimates that, by the end of World War II in 1945, the Russian population was about 90 million fewer than it could have been otherwise.
Because of the vastness and diversity of the territory occupied by Slavic people, there were several centers of Slavic consolidation. In the 19th century, Pan-Slavism developed as a movement among intellectuals, scholars, and poets, but it rarely influenced practical politics and did not find support in some nations that had Slavic origins. Pan-Slavism became compromised when the Russian Empire started to use it as an ideology justifying its territorial conquests in Central Europe as well as subjugation of other ethnic groups of Slavic origins such as Poles and Ukrainians, and the ideology became associated with Russian imperialism. The common Slavic experience of communism combined with the repeated usage of the ideology by Soviet propaganda after World War II within the Eastern bloc (Warsaw Pact) was a forced high-level political and economic hegemony of the USSR dominated by Russians. A notable political union of the 20th century that covered most South Slavs was Yugoslavia, but it ultimately broke apart in the 1990s along with the Soviet Union.
The word "Slavs" was used in the national anthem of the Slovak Republic (1939–1945), Yugoslavia (1943–1992) and the Federal Republic of Yugoslavia (1992–2003), later Serbia and Montenegro (2003–2006).
Former Soviet states, as well as countries that used to be satellite states or territories of the Warsaw Pact, have numerous minority Slavic populations, many of whom are originally from the Russian SFSR, Ukrainian SSR and Byelorussian SSR. As of now, Kazakhstan has the largest Slavic minority population with most being Russians (Ukrainians, Belarusians and Poles are present as well but in much smaller numbers).
Pan-Slavism, a movement which came into prominence in the mid-19th century, emphasized the common heritage and unity of all the Slavic peoples. The main focus was in the Balkans where the South Slavs had been ruled for centuries by other empires: the Byzantine Empire, Austria-Hungary, the Ottoman Empire, and Venice. The Russian Empire used Pan-Slavism as a political tool; as did the Soviet Union, which gained political-military influence and control over most Slavic-majority nations between 1945 and 1948 and retained a hegemonic role until the period 1989–1991.
Slavic studies began as an almost exclusively linguistic and philological enterprise. As early as 1833, Slavic languages were recognized as Indo-European.
Slavic standard languages which are official in at least one country: Belarusian, Bosnian, Bulgarian, Croatian, Czech, Macedonian, Montenegrin, Polish, Russian, Serbian, Slovak, Slovene, and Ukrainian. The alphabet depends on what religion is usual for the respective Slavic ethnic groups. The Orthodox use the Cyrillic alphabet and the Roman Catholics use Latin alphabet, the Bosniaks who are Muslims also use the Latin. Few Greek Roman and Roman Catholics use the Cyrillic alphabet however. The Serbian language and Montenegrin language uses both Cyrillic and Latin alphabets. There is also a Latin script to write in Belarusian, called the Lacinka alphabet.
Proto-Slavic, the supposed ancestor language of all Slavic languages, is a descendant of common Proto-Indo-European, via a Balto-Slavic stage in which it developed numerous lexical and morphophonological isoglosses with the Baltic languages. In the framework of the Kurgan hypothesis, "the Indo-Europeans who remained after the migrations [from the steppe] became speakers of Balto-Slavic".
Proto-Slavic, sometimes referred to as Common Slavic or Late Proto-Slavic, is defined as the last stage of the language preceding the geographical split of the historical Slavic languages. That language was uniform, and on the basis of borrowings from foreign languages and Slavic borrowings into other languages, cannot be said to have any recognizable dialects, suggesting a comparatively compact homeland. Slavic linguistic unity was to some extent visible as late as Old Church Slavonic manuscripts which, though based on local Slavic speech of Thessaloniki, could still serve the purpose of the first common Slavic literary language.
The pagan Slavic populations were Christianized between the 6th and 10th centuries. Orthodox Christianity is predominant in the East and South Slavs, while Roman Catholicism is predominant in West Slavs and the western South Slavs. The religious borders are largely comparable to the East–West Schism which began in the 11th century. The majority of contemporary Slavic populations who profess a religion are Orthodox, followed by Catholic, while a small minority are Protestant. There are minor Slavic Muslim groups. Religious delineations by nationality can be very sharp; usually in the Slavic ethnic groups the vast majority of religious people share the same religion. Some Slavs are atheist or agnostic: only 19% of Czechs professed belief in god/s in the 2005 Eurobarometer survey.
Slavs are customarily divided along geographical lines into three major subgroups: West Slavs, East Slavs, and South Slavs, each with a different and a diverse background based on unique history, religion and culture of particular Slavic groups within them. Apart from prehistorical archaeological cultures, the subgroups have had notable cultural contact with non-Slavic Bronze- and Iron Age civilisations.
^1 Also considered part of Rusyns
^2 Considered transitional between Ukrainians and Belarusians
^3 The ethnic affiliation of the Lemkos has become an ideological conflict. It has been alleged that among the Lemkos the idea of "Carpatho-Ruthenian" nation is supported only by Lemkos residing in Transcarpathia and abroad
^4 Most inhabitants of historic Moravia considered themselves as Czechs but significant amount declared their Moravian nationality, different from that Czech (although people from Bohemia and Moravia use the same official language).
^5 Also considered Poles.
^6 There are sources that show Silesians as part of the Poles. Parts of the southmost population of Upper Silesia is sometimes considered Czech (controversial).
^7 A census category recognized as an ethnic group. Most Slavic Muslims (especially in Bosnia, Croatia, Montenegro and Serbia) now opt for Bosniak ethnicity, but some still use the "Muslim" designation. Bosniak and Muslim are considered two ethnonyms for a single ethnicity and the terms may even be used interchangeably. However, a small number of people within Bosnia and Herzegovina declare themselves Bosniak but are not necessarily Muslim by faith.
^8 This identity continues to be used by a minority throughout the former Yugoslav republics. The nationality is also declared by diasporans living in the USA and Canada. There are a multitude of reasons as to why people prefer this affiliation, some published on the article.
^9 Sub-groups of Croats include Bunjevci (in Bačka), Šokci (in Slavonia and Vojvodina), Janjevci (in Kosovo), Burgenland Croats (in Austria), Bosniaks (in Hungary), Molise Croats (in Italy), Krashovans (in Romania), Moravian Croats (in the Czech Republic)
^10 Sub-groups of Slovenes include Prekmurians, Hungarian Slovenes, Carinthian Slovenes, Venetian Slovenes, Resians, and the extinct Carantanians and Somogy Slovenes.
Note: Besides ethnic groups, Slavs often identify themselves with the local geographical region in which they live. Some of the major regional South Slavic groups include: Zagorci in northern Croatia, Istrijani in westernmost Croatia, Dalmatinci in southern Croatia, Boduli in Adriatic islands, Vlaji in hinterland of Dalmatia, Slavonci in eastern Croatia, Bosanci in Bosnia, Hercegovci in Herzegovina, Krajišnici in western Bosnia, but is more commonly used to refer to the Serbs of Croatia, most of whom are descendants of the Grenzers, and continued to live in the area which made up the Military Frontier until the Croatian war of independence, Semberci in northeast Bosnia, Srbijanci in Serbia proper, Šumadinci in central Serbia, Vojvođani in northern Serbia, Sremci in Syrmia, Bačvani in northwest Vojvodina, Banaćani in Banat, Sandžaklije (Muslims in Serbia/Montenegro border), Kosovci in Kosovo, Bokelji in southwest Montenegro, Trakiytsi in Upper Thracian Lowlands, Dobrudzhantsi in north-east Bulgarian region, Balkandzhii in Central Balkan Mountains, Miziytsi in north Bulgarian region, Warmiaks and Masurians in north-east Polish regions Warmia and Mazuria, Pirintsi in Blagoevgrad Province, Ruptsi in the Rhodopes etc.
The modern Slavic peoples carry a variety of mitochondrial DNA haplogroups and Y-chromosome DNA haplogroups. Yet two paternal haplogroups predominate: R1a1a [M17] and I2a2a [L69.2=T/S163.2]. The frequency of Haplogroup R1a ranges from 63.39% in the Sorbs, through 56.4% in Poland, 54% in Ukraine, 52% in Russia, Belarus, to 15.2% in Republic of Macedonia, 14.7% in Bulgaria and 12.1% in Herzegovina. The correlation between R1a1a [M17] and the speakers of Indo-European languages, particularly those of Eastern Europe (Russian) and Central and Southern Asia, was noticed in the late 1990s. From this Spencer Wells and colleagues, following the Kurgan hypothesis, deduced that R1a1a arose on the Pontic-Caspian steppe.
Specific studies of Slavic genetics followed. In 2007 Rębała and colleagues studied several Slavic populations with the aim of localizing the Proto-Slavic homeland. The significant findings of this study are that:
Marcin Woźniak and colleagues (2010) searched for specifically Slavic sub-group of R1a1a [M17]. Working with haplotypes, they found a pattern among Western Slavs which turned out to correspond to a newly discovered marker, M458, which defines subclade R1a1a7. This marker correlates remarkably well with the distribution of Slavic-speakers today. The team led by Peter Underhill, which discovered M458, did not consider the possibility that this was a Slavic marker, since they used the "evolutionary effective" mutation rate, which gave a date far too old to be Slavic. Woźniak and colleagues pointed out that the pedigree mutation rate, giving a later date, is more consistent with the archaeological record.
Pomors are distinguished by the presence of Y Haplogroup N among them. Postulated to originate from southeast Asia, it is found at high rates in Uralic peoples. Its presence in Pomors (called "Northern Russians" in the report) attests to the non-Slavic tribes (mixing with Finnic tribes of northern Eurasia). Autosomally, Russians are generally similar to populations in central-eastern Europe but some northern Russians are intermediate to Finno-Ugric groups.
On the other hand, I2a1b1 (P41.2) is typical of the South Slavic populations, being highest in Bosnia-Herzegovina (>50%). Haplogroup I2a2 is also commonly found in north-eastern Italians. There is also a high concentration of I2a2a in the Moldavian region of Romania, Moldova and western Ukraine. According to original studies, Hg I2a2 was believed to have arisen in the west Balkans sometime after the LGM, subsequently spreading from the Balkans through Central Russian Plain. Recently, Ken Nordtvedt has split I2a2 into two clades – N (northern) and S (southern), in relation where they arose compared to Danube river. He proposes that N is slightly older than S. He recalculated the age of I2a2 to be ~ 2550 years and proposed that the current distribution is explained by a Slavic expansion from the area north-east of the Carpathians.
In 2008, biochemist Boris Arkadievich Malyarchuk (Russian: Борис Аркадьевич Малярчук) et al. of the Institute of Biological Problems of the North, Russian Academy of Sciences, Magadan, Russia, used a sample (n=279) of Czech individuals to determine the frequency of "Mongoloid" "mtDNA lineages". Malyarchuk found Czech mtDNA lineages were typical of "Slavic populations" with "1.8%" Mongoloid mtDNA lineage. Malyarchuk added that "Slavic populations" "almost always" contain Mongoloid mtDNA lineage. Malyarchuk said the Mongoloid component of Slavic people was partially added before the split of "Balto-Slavics" in 2,000–3,000 BC with additional Mongoloid mixture occurring among Slavics in the last 4,000 years. Malyarchuk said the "Russian population" was developed by the "assimilation of the indigenous pre-Slavic population of Eastern Europe by true Slavs" with additional "assimilation of Finno-Ugric populations" and "long-lasting" interactions with the populations of "Siberia" and "Central Asia". Malyarchuk said that other Slavs "Mongoloid component" was increased during the waves of migration from "steppe populations (Huns, Avars, Bulgars and Mongols)", especially the decay of the "Avar Khaganate".
DNA samples from 1228 Russians show that the Y chromosomes analyzed, all except 20 (1.6%) fall into seven major haplogroups all characteristic to West Eurasian populations. Taken together, they account for 95% of the total Russian Y chromosomal pool. Only (0.7%) fell into haplogroups that are specific to East and South Asian populations. Mitochondrial DNA (mtDNA) examined in Poles and Russians revealed the presence of all major European haplogroups, which were characterized by similar patterns of distribution in Poles and Russians. An analysis of the DNA did not reveal any specific combinations of unique mtDNA haplotypes and their subclusters. The DNA clearly shows that both Poles and Russians are not different from the neighbouring European populations.
Throughout their history, Slavs came into contact with non-Slavic groups. In the postulated homeland region (present-day Ukraine), they had contacts with the Iranic Sarmatians and the Germanic Goths. After their subsequent spread, they began assimilating non-Slavic peoples. For example, in the Balkans, there were Paleo-Balkan peoples, such as Romanized and Hellenized (Jireček Line) Illyrians, Thracians and Dacians, as well as Greeks and Celtic Scordisci. Over time, due to the larger number of Slavs, most descendants of the indigenous populations of the Balkans were Slavicized. The Thracians and Illyrians vanished from the population during this period – although the modern Albanian nation claims descent from the Illyrians. Exceptions are Greece, where the lesser numbered Slavs scattered there came to be Hellenized (aided in time by more Greeks returning to Greece in the 9th century and the role of the church and administration) and Romania where Slavic people settled en route for present-day Greece, Republic of Macedonia, Bulgaria and East Thrace whereby the Slavic population had come to assimilate. Bulgars were also assimilated by local Slavs but their ruling status and subsequent land cast the nominal legacy of Bulgarian country and people onto all future generations. The Romance speakers within the fortified Dalmatian cities managed to retain their culture and language for a long time, as Dalmatian Romance was spoken until the high Middle Ages. However, they too were eventually assimilated into the body of Slavs.
In the Western Balkans, South Slavs and Germanic Gepids intermarried with Avar invaders, eventually producing a Slavicized population.[citation needed] In Central Europe, the Slavs intermixed with Germanic and Celtic, while the eastern Slavs encountered Uralic and Scandinavian peoples. Scandinavians (Varangians) and Finnic peoples were involved in the early formation of the Rus' state but were completely Slavicized after a century. Some Finno-Ugric tribes in the north were also absorbed into the expanding Rus population. At the time of the Magyar migration, the present-day Hungary was inhabited by Slavs, numbering about 200,000, and by Romano-Dacians who were either assimilated or enslaved by the Magyars. In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchaks and the Pechenegs, caused a massive migration of East Slavic populations to the safer, heavily forested regions of the north. In the Middle Ages, groups of Saxon ore miners settled in medieval Bosnia, Serbia and Bulgaria where they were Slavicized.
Polabian Slavs (Wends) settled in parts of England (Danelaw), apparently as Danish allies. Polabian-Pomeranian Slavs are also known to have even settled on Norse age Iceland. Saqaliba refers to the Slavic mercenaries and slaves in the medieval Arab world in North Africa, Sicily and Al-Andalus. Saqaliba served as caliph's guards. In the 12th century, there was intensification of Slavic piracy in the Baltics. The Wendish Crusade was started against the Polabian Slavs in 1147, as a part of the Northern Crusades. Niklot, pagan chief of the Slavic Obodrites, began his open resistance when Lothar III, Holy Roman Emperor, invaded Slavic lands. In August 1160 Niklot was killed and German colonization (Ostsiedlung) of the Elbe-Oder region began. In Hanoverian Wendland, Mecklenburg-Vorpommern and Lusatia invaders started germanization. Early forms of germanization were described by German monks: Helmold in the manuscript Chronicon Slavorum and Adam of Bremen in Gesta Hammaburgensis ecclesiae pontificum. The Polabian language survived until the beginning of the 19th century in what is now the German state of Lower Saxony. In Eastern Germany, around 20% of Germans have Slavic paternal ancestry. Similarly, in Germany, around 20% of the foreign surnames are of Slavic origin.
Cossacks, although Slavic-speaking and Orthodox Christians, came from a mix of ethnic backgrounds, including Tatars and other Turks. Many early members of the Terek Cossacks were Ossetians.
The Gorals of southern Poland and northern Slovakia are partially descended from Romance-speaking Vlachs who migrated into the region from the 14th to 17th centuries and were absorbed into the local population. The population of Moravian Wallachia also descend of this population.
Conversely, some Slavs were assimilated into other populations. Although the majority continued south, attracted by the riches of the territory which would become Bulgaria, a few remained in the Carpathian basin and were ultimately assimilated into the Magyar or Romanian population. There is a large number of river names and other placenames of Slavic origin in Romania.[better source needed]
Somalis (Somali: Soomaali, Arabic: صومال‎) are an ethnic group inhabiting the Horn of Africa (Somali Peninsula). The overwhelming majority of Somalis speak the Somali language, which is part of the Cushitic branch of the Afro-Asiatic family. They are predominantly Sunni Muslim. Ethnic Somalis number around 16-20 million and are principally concentrated in Somalia (around 12.3 million), Ethiopia (4.6 million), Kenya (2.4 million), and Djibouti (464,600), with many also residing in parts of the Middle East, North America and Europe.
Irir Samaale, the oldest common ancestor of several Somali clans, is generally regarded as the source of the ethnonym Somali. The name "Somali" is, in turn, held to be derived from the words soo and maal, which together mean "go and milk" — a reference to the ubiquitous pastoralism of the Somali people. Another plausible etymology proposes that the term Somali is derived from the Arabic for "wealthy" (dhawamaal), again referring to Somali riches in livestock.
An Ancient Chinese document from the 9th century referred to the northern Somali coast — which was then called "Berbera" by Arab geographers in reference to the region's "Berber" (Cushitic) inhabitants — as Po-pa-li. The first clear written reference of the sobriquet Somali, however, dates back to the 15th century. During the wars between the Sultanate of Ifat based at Zeila and the Solomonic Dynasty, the Abyssinian Emperor had one of his court officials compose a hymn celebrating a military victory over the Sultan of Ifat's eponymous troops.
Ancient rock paintings in Somalia which date back to 5000 years have been found in the northern part of the country, depicting early life in the territory. The most famous of these is the Laas Geel complex, which contains some of the earliest known rock art on the African continent and features many elaborate pastoralist sketches of animal and human figures. In other places, such as the northern Dhambalin region, a depiction of a man on a horse is postulated as being one of the earliest known examples of a mounted huntsman.
Inscriptions have been found beneath many of the rock paintings, but archaeologists have so far been unable to decipher this form of ancient writing. During the Stone age, the Doian culture and the Hargeisan culture flourished here with their respective industries and factories.
The oldest evidence of burial customs in the Horn of Africa comes from cemeteries in Somalia dating back to 4th millennium BC. The stone implements from the Jalelo site in northern Somalia are said to be the most important link in evidence of the universality in palaeolithic times between the East and the West.
In antiquity, the ancestors of the Somali people were an important link in the Horn of Africa connecting the region's commerce with the rest of the ancient world. Somali sailors and merchants were the main suppliers of frankincense, myrrh and spices, items which were considered valuable luxuries by the Ancient Egyptians, Phoenicians, Mycenaeans and Babylonians.
According to most scholars, the ancient Land of Punt and its inhabitants formed part of the ethnogenesis of the Somali people. The ancient Puntites were a nation of people that had close relations with Pharaonic Egypt during the times of Pharaoh Sahure and Queen Hatshepsut. The pyramidal structures, temples and ancient houses of dressed stone littered around Somalia are said to date from this period.
In the classical era, several ancient city-states such as Opone, Essina, Sarapion, Nikon, Malao, Damo and Mosylon near Cape Guardafui, which competed with the Sabaeans, Parthians and Axumites for the wealthy Indo-Greco-Roman trade, also flourished in Somalia.
The birth of Islam on the opposite side of Somalia's Red Sea coast meant that Somali merchants, sailors and expatriates living in the Arabian Peninsula gradually came under the influence of the new religion through their converted Arab Muslim trading partners. With the migration of fleeing Muslim families from the Islamic world to Somalia in the early centuries of Islam and the peaceful conversion of the Somali population by Somali Muslim scholars in the following centuries, the ancient city-states eventually transformed into Islamic Mogadishu, Berbera, Zeila, Barawa and Merca, which were part of the Berberi civilization. The city of Mogadishu came to be known as the City of Islam, and controlled the East African gold trade for several centuries.
The Sultanate of Ifat, led by the Walashma dynasty with its capital at Zeila, ruled over parts of what is now eastern Ethiopia, Djibouti, and northern Somalia. The historian al-Umari records that Ifat was situated near the Red Sea coast, and states its size as 15 days travel by 20 days travel. Its army numbered 15,000 horsemen and 20,000 foot soldiers. Al-Umari also credits Ifat with seven "mother cities": Belqulzar, Kuljura, Shimi, Shewa, Adal, Jamme and Laboo.
In the Middle Ages, several powerful Somali empires dominated the regional trade including the Ajuran Sultanate, which excelled in hydraulic engineering and fortress building, the Sultanate of Adal, whose general Ahmad ibn Ibrahim al-Ghazi (Ahmed Gurey) was the first commander to use cannon warfare on the continent during Adal's conquest of the Ethiopian Empire, and the Sultanate of the Geledi, whose military dominance forced governors of the Omani empire north of the city of Lamu to pay tribute to the Somali Sultan Ahmed Yusuf.
In the late 19th century, after the Berlin conference had ended, European empires sailed with their armies to the Horn of Africa. The imperial clouds wavering over Somalia alarmed the Dervish leader Mohammed Abdullah Hassan, who gathered Somali soldiers from across the Horn of Africa and began one of the longest anti-colonial wars ever. The Dervish State successfully repulsed the British empire four times and forced it to retreat to the coastal region. As a result of its successes against the British, the Dervish State received support from the Ottoman and German empires. The Turks also named Hassan Emir of the Somali nation, and the Germans promised to officially recognize any territories the Dervishes were to acquire. After a quarter of a century of holding the British at bay, the Dervishes were finally defeated in 1920, when Britain for the first time in Africa used airplanes to bomb the Dervish capital of Taleex. As a result of this bombardment, former Dervish territories were turned into a protectorate of Britain. Italy similarly faced the same opposition from Somali Sultans and armies and did not acquire full control of parts of modern Somalia until the Fascist era in late 1927. This occupation lasted till 1941 and was replaced by a British military administration.
Following World War II, Britain retained control of both British Somaliland and Italian Somaliland as protectorates. In 1945, during the Potsdam Conference, the United Nations granted Italy trusteeship of Italian Somaliland, but only under close supervision and on the condition — first proposed by the Somali Youth League (SYL) and other nascent Somali political organizations, such as Hizbia Digil Mirifle Somali (HDMS) and the Somali National League (SNL) — that Somalia achieve independence within ten years. British Somaliland remained a protectorate of Britain until 1960.
To the extent that Italy held the territory by UN mandate, the trusteeship provisions gave the Somalis the opportunity to gain experience in political education and self-government. These were advantages that British Somaliland, which was to be incorporated into the new Somali state, did not have. Although in the 1950s British colonial officials attempted, through various administrative development efforts, to make up for past neglect, the protectorate stagnated. The disparity between the two territories in economic development and political experience would cause serious difficulties when it came time to integrate the two parts. Meanwhile, in 1948, under pressure from their World War II allies and to the dismay of the Somalis, the British "returned" the Haud (an important Somali grazing area that was presumably 'protected' by British treaties with the Somalis in 1884 and 1886) and the Ogaden to Ethiopia, based on a treaty they signed in 1897 in which the British ceded Somali territory to the Ethiopian Emperor Menelik in exchange for his help against plundering by Somali clans. Britain included the proviso that the Somali nomads would retain their autonomy, but Ethiopia immediately claimed sovereignty over them. This prompted an unsuccessful bid by Britain in 1956 to buy back the Somali lands it had turned over. Britain also granted administration of the almost exclusively Somali-inhabited Northern Frontier District (NFD) to Kenyan nationalists despite an informal plebiscite demonstrating the overwhelming desire of the region's population to join the newly formed Somali Republic.
A referendum was held in neighboring Djibouti (then known as French Somaliland) in 1958, on the eve of Somalia's independence in 1960, to decide whether or not to join the Somali Republic or to remain with France. The referendum turned out in favour of a continued association with France, largely due to a combined yes vote by the sizable Afar ethnic group and resident Europeans. There was also widespread vote rigging, with the French expelling thousands of Somalis before the referendum reached the polls. The majority of those who voted no were Somalis who were strongly in favour of joining a united Somalia, as had been proposed by Mahmoud Harbi, Vice President of the Government Council. Harbi was killed in a plane crash two years later. Djibouti finally gained its independence from France in 1977, and Hassan Gouled Aptidon, a Somali who had campaigned for a yes vote in the referendum of 1958, eventually wound up as Djibouti's first president (1977–1991).
British Somaliland became independent on 26 June 1960 as the State of Somaliland, and the Trust Territory of Somalia (the former Italian Somaliland) followed suit five days later. On 1 July 1960, the two territories united to form the Somali Republic, albeit within boundaries drawn up by Italy and Britain. A government was formed by Abdullahi Issa Mohamud and Muhammad Haji Ibrahim Egal other members of the trusteeship and protectorate governments, with Haji Bashir Ismail Yusuf as President of the Somali National Assembly, Aden Abdullah Osman Daar as the President of the Somali Republic and Abdirashid Ali Shermarke as Prime Minister (later to become President from 1967 to 1969). On 20 July 1961 and through a popular referendum, the people of Somalia ratified a new constitution, which was first drafted in 1960. In 1967, Muhammad Haji Ibrahim Egal became Prime Minister, a position to which he was appointed by Shermarke. Egal would later become the President of the autonomous Somaliland region in northwestern Somalia.
On 15 October 1969, while paying a visit to the northern town of Las Anod, Somalia's then President Abdirashid Ali Shermarke was shot dead by one of his own bodyguards. His assassination was quickly followed by a military coup d'état on 21 October 1969 (the day after his funeral), in which the Somali Army seized power without encountering armed opposition — essentially a bloodless takeover. The putsch was spearheaded by Major General Mohamed Siad Barre, who at the time commanded the army.
Alongside Barre, the Supreme Revolutionary Council (SRC) that assumed power after President Sharmarke's assassination was led by Lieutenant Colonel Salaad Gabeyre Kediye and Chief of Police Jama Korshel. The SRC subsequently renamed the country the Somali Democratic Republic, dissolved the parliament and the Supreme Court, and suspended the constitution.
The revolutionary army established large-scale public works programs and successfully implemented an urban and rural literacy campaign, which helped dramatically increase the literacy rate. In addition to a nationalization program of industry and land, the new regime's foreign policy placed an emphasis on Somalia's traditional and religious links with the Arab world, eventually joining the Arab League (AL) in 1974. That same year, Barre also served as chairman of the Organization of African Unity (OAU), the predecessor of the African Union (AU).
Somali people in the Horn of Africa are divided among different countries (Somalia, Djibouti, Ethiopia, and northeastern Kenya) that were artificially and some might say arbitrarily partitioned by the former imperial powers. Pan-Somalism is an ideology that advocates the unification of all ethnic Somalis once part of Somali empires such as the Ajuran Empire, the Adal Sultanate, the Gobroon Dynasty and the Dervish State under one flag and one nation. The Siad Barre regime actively promoted Pan-Somalism, which eventually led to the Ogaden War between Somalia on one side, and Ethiopia, Cuba and the Soviet Union on the other.
According to Y chromosome studies by Sanchez et al. (2005), Cruciani et al. (2004, 2007), the Somalis are paternally closely related to other Afro-Asiatic-speaking groups in Northeast Africa. Besides comprising the majority of the Y-DNA in Somalis, the E1b1b1a (formerly E3b1a) haplogroup also makes up a significant proportion of the paternal DNA of Ethiopians, Sudanese, Egyptians, Berbers, North African Arabs, as well as many Mediterranean populations. Sanchez et al. (2005) observed the M78 subclade of E1b1b in about 77% of their Somali male samples. According to Cruciani et al. (2007), the presence of this subhaplogroup in the Horn region may represent the traces of an ancient migration from Egypt/Libya. After haplogroup E1b1b, the second most frequently occurring Y-DNA haplogroup among Somalis is the West Asian haplogroup T (M70). It is observed in slightly more than 10% of Somali males. Haplogroup T, like haplogroup E1b1b, is also typically found among populations of Northeast Africa, North Africa, the Near East and the Mediterranean.
According to mtDNA studies by Holden (2005) and Richards et al. (2006), a significant proportion of the maternal lineages of Somalis consists of the M1 haplogroup. This mitochondrial clade is common among Ethiopians and North Africans, particularly Egyptians and Algerians. M1 is believed to have originated in Asia, where its parent M clade represents the majority of mtDNA lineages. This haplogroup is also thought to possibly correlate with the Afro-Asiatic language family:
According to an autosomal DNA study by Hodgson et al. (2014), the Afro-Asiatic languages were likely spread across Africa and the Near East by an ancestral population(s) carrying a newly identified non-African genetic component, which the researchers dub the "Ethio-Somali". This Ethio-Somali component is today most common among Afro-Asiatic-speaking populations in the Horn of Africa. It reaches a frequency peak among ethnic Somalis, representing the majority of their ancestry. The Ethio-Somali component is most closely related to the Maghrebi non-African genetic component, and is believed to have diverged from all other non-African ancestries at least 23,000 years ago. On this basis, the researchers suggest that the original Ethio-Somali carrying population(s) probably arrived in the pre-agricultural period from the Near East, having crossed over into northeastern Africa via the Sinai Peninsula. The population then likely split into two branches, with one group heading westward toward the Maghreb and the other moving south into the Horn.
The analysis of HLA antigens has also helped clarify the possible background of the Somali people, as the distribution of haplotype frequencies vary among population groups. According to Mohamoud et al. (2006):
The history of Islam in Somalia is as old as the religion itself. The early persecuted Muslims fled to various places in the region, including the city of Zeila in modern-day northern Somalia, so as to seek protection from the Quraysh. Somalis were among the first populations on the continent to embrace Islam. With very few exceptions, Somalis are entirely Muslims, the majority belonging to the Sunni branch of Islam and the Shafi`i school of Islamic jurisprudence, although a few are also adherents of the Shia Muslim denomination.
Qur'anic schools (also known as dugsi) remain the basic system of traditional religious instruction in Somalia. They provide Islamic education for children, thereby filling a clear religious and social role in the country. Known as the most stable local, non-formal system of education providing basic religious and moral instruction, their strength rests on community support and their use of locally made and widely available teaching materials. The Qur'anic system, which teaches the greatest number of students relative to other educational sub-sectors, is oftentimes the only system accessible to Somalis in nomadic as compared to urban areas. A study from 1993 found, among other things, that "unlike in primary schools where gender disparity is enormous, around 40 per cent of Qur'anic school pupils are girls; but the teaching staff have minimum or no qualification necessary to ensure intellectual development of children." To address these concerns, the Somali government on its own part subsequently established the Ministry of Endowment and Islamic Affairs, under which Qur'anic education is now regulated.
In the Somali diaspora, multiple Islamic fundraising events are held every year in cities like Birmingham, London, Toronto and Minneapolis, where Somali scholars and professionals give lectures and answer questions from the audience. The purpose of these events is usually to raise money for new schools or universities in Somalia, to help Somalis that have suffered as a consequence of floods and/or droughts, or to gather funds for the creation of new mosques like the Abuubakar-As-Saddique Mosque, which is currently undergoing construction in the Twin cities.
In addition, the Somali community has produced numerous important Muslim figures over the centuries, many of whom have significantly shaped the course of Islamic learning and practice in the Horn of Africa, the Arabian Peninsula and well beyond.
The clan groupings of the Somali people are important social units, and clan membership plays a central part in Somali culture and politics. Clans are patrilineal and are often divided into sub-clans, sometimes with many sub-divisions. The tombs of the founders of the Darod, Dir and Isaaq major clans as well as the Abgaal sub-clan of the Hawiye are all located in northern Somalia. Tradition holds this general area as an ancestral homeland of the Somali people.
Somali society is traditionally ethnically endogamous. So to extend ties of alliance, marriage is often to another ethnic Somali from a different clan. Thus, for example, a recent study observed that in 89 marriages contracted by men of the Dhulbahante clan, 55 (62%) were with women of Dhulbahante sub-clans other than those of their husbands; 30 (33.7%) were with women of surrounding clans of other clan families (Isaaq, 28; Hawiye, 3); and 3 (4.3%) were with women of other clans of the Darod clan family (Majerteen 2, Ogaden 1).
In 1975, the most prominent government reforms regarding family law in a Muslim country were set in motion in the Somali Democratic Republic, which put women and men, including husbands and wives, on complete equal footing. The 1975 Somali Family Law gave men and women equal division of property between the husband and wife upon divorce and the exclusive right to control by each spouse over his or her personal property.
Somalis constitute the largest ethnic group in Somalia, at approximately 85% of the nation's inhabitants. They are traditionally nomads, but since the late 20th century, many have moved to urban areas. While most Somalis can be found in Somalia proper, large numbers also live in Ethiopia, Djibouti, Kenya, Yemen, the Middle East, South Asia and Europe due to their seafaring tradition.
Civil strife in the early 1990s greatly increased the size of the Somali diaspora, as many of the best educated Somalis left for the Middle East, Europe and North America. In Canada, the cities of Toronto, Ottawa, Calgary, Edmonton, Montreal, Vancouver, Winnipeg and Hamilton all harbor Somali populations. Statistics Canada's 2006 census ranks people of Somali descent as the 69th largest ethnic group in Canada.
While the distribution of Somalis per country in Europe is hard to measure because the Somali community on the continent has grown so quickly in recent years, an official 2010 estimate reported 108,000 Somalis living in the United Kingdom. Somalis in Britain are largely concentrated in the cities of London, Sheffield, Bristol, Birmingham, Cardiff, Liverpool, Manchester, Leeds, and Leicester, with London alone accounting for roughly 78% of Britain's Somali population. There are also significant Somali communities in Sweden: 57,906 (2014); the Netherlands: 37,432 (2014); Norway: 38,413 (2015); Denmark: 18,645 (2014); and Finland: 16,721 (2014).
In the United States, Minneapolis, Saint Paul, Columbus, San Diego, Seattle, Washington, D.C., Houston, Atlanta, Los Angeles, Portland, Denver, Nashville, Green Bay, Lewiston, Portland, Maine and Cedar Rapids have the largest Somali populations.
An estimated 20,000 Somalis emigrated to the U.S. state of Minnesota some ten years ago and the Twin Cities (Minneapolis and Saint Paul) now have the highest population of Somalis in North America. The city of Minneapolis hosts hundreds of Somali-owned and operated businesses offering a variety of products, including leather shoes, jewelry and other fashion items, halal meat, and hawala or money transfer services. Community-based video rental stores likewise carry the latest Somali films and music. The number of Somalis has especially surged in the Cedar-Riverside area of Minneapolis.
There is a sizable Somali community in the United Arab Emirates. Somali-owned businesses line the streets of Deira, the Dubai city centre, with only Iranians exporting more products from the city at large. Internet cafés, hotels, coffee shops, restaurants and import-export businesses are all testimony to the Somalis' entrepreneurial spirit. Star African Air is also one of three Somali-owned airlines which are based in Dubai.
Besides their traditional areas of inhabitation in Greater Somalia, a Somali community mainly consisting of entrepreneurs, academics, and students also exists in Egypt. In addition, there is an historical Somali community in the general Sudan area. Primarily concentrated in the north and Khartoum, the expatriate community mainly consists of students as well as some businesspeople. More recently, Somali entrepreneurs have established themselves in Kenya, investing over $1.5 billion in the Somali enclave of Eastleigh alone. In South Africa, Somali businesspeople also provide most of the retail trade in informal settlements around the Western Cape province.
The Somali language is a member of the Cushitic branch of the Afro-Asiatic family. Its nearest relatives are the Afar and Saho languages. Somali is the best documented of the Cushitic languages, with academic studies of it dating from before 1900.
The exact number of speakers of Somali is unknown. One source estimates that there are 7.78 million speakers of Somali in Somalia itself and 12.65 million speakers globally. The Somali language is spoken by ethnic Somalis in Greater Somalia and the Somali diaspora.
Somali dialects are divided into three main groups: Northern, Benaadir and Maay. Northern Somali (or Northern-Central Somali) forms the basis for Standard Somali. Benaadir (also known as Coastal Somali) is spoken on the Benadir coast from Adale to south of Merca, including Mogadishu, as well as in the immediate hinterland. The coastal dialects have additional phonemes which do not exist in Standard Somali. Maay is principally spoken by the Digil and Mirifle (Rahanweyn) clans in the southern areas of Somalia.
A number of writing systems have been used over the years for transcribing the language. Of these, the Somali alphabet is the most widely used, and has been the official writing script in Somalia since the government of former President of Somalia Mohamed Siad Barre formally introduced it in October 1972. The script was developed by the Somali linguist Shire Jama Ahmed specifically for the Somali language, and uses all letters of the English Latin alphabet except p, v and z. Besides Ahmed's Latin script, other orthographies that have been used for centuries for writing Somali include the long-established Arabic script and Wadaad's writing. Indigenous writing systems developed in the twentieth century include the Osmanya, Borama and Kaddare scripts, which were invented by Osman Yusuf Kenadid, Abdurahman Sheikh Nuur and Hussein Sheikh Ahmed Kaddare, respectively.
In addition to Somali, Arabic, which is also an Afro-Asiatic tongue, is an official national language in both Somalia and Djibouti. Many Somalis speak it due to centuries-old ties with the Arab world, the far-reaching influence of the Arabic media, and religious education. Somalia and Djibouti are also both members of the Arab League.
The culture of Somalia is an amalgamation of traditions developed independently and through interaction with neighbouring and far away civilizations, such as other parts of Northeast Africa, the Arabian Peninsula, India and Southeast Asia.
The textile-making communities in Somalia are a continuation of an ancient textile industry, as is the culture of wood carving, pottery and monumental architecture that dominates Somali interiors and landscapes. The cultural diffusion of Somali commercial enterprise can be detected in its cuisine, which contains Southeast Asian influences. Due to the Somali people's passionate love for and facility with poetry, Somalia has often been referred to by scholars as a "Nation of Poets" and a "Nation of Bards" including, among others, the Canadian novelist Margaret Laurence.
All of these traditions, including festivals, martial arts, dress, literature, sport and games such as Shax, have immensely contributed to the enrichment of Somali heritage.
Somalis have a rich musical heritage centered on traditional Somali folklore. Most Somali songs are pentatonic. That is, they only use five pitches per octave in contrast to a heptatonic (seven note) scale, such as the major scale. At first listen, Somali music might be mistaken for the sounds of nearby regions such as Ethiopia, Sudan or Arabia, but it is ultimately recognizable by its own unique tunes and styles. Somali songs are usually the product of collaboration between lyricists (midho), songwriters (lahan) and singers ('odka or "voice").
Growing out of the Somali people's rich storytelling tradition, the first few feature-length Somali films and cinematic festivals emerged in the early 1960s, immediately after independence. Following the creation of the Somali Film Agency (SFA) regulatory body in 1975, the local film scene began to expand rapidly. The Somali filmmaker Ali Said Hassan concurrently served as the SFA's representative in Rome. In the 1970s and early 1980s, popular musicals known as riwaayado were the main driving force behind the Somali movie industry. Epic and period films as well as international co-productions followed suit, facilitated by the proliferation of video technology and national television networks. Said Salah Ahmed during this period directed his first feature film, The Somali Darwish (The Somalia Dervishes), devoted to the Dervish State. In the 1990s and 2000s, a new wave of more entertainment-oriented movies emerged. Referred to as Somaliwood, this upstart, youth-based cinematic movement has energized the Somali film industry and in the process introduced innovative storylines, marketing strategies and production techniques. The young directors Abdisalam Aato of Olol Films and Abdi Malik Isak are at the forefront of this quiet revolution.
Somali art is the artistic culture of the Somali people, both historic and contemporary. These include artistic traditions in pottery, music, architecture, wood carving and other genres. Somali art is characterized by its aniconism, partly as a result of the vestigial influence of the pre-Islamic mythology of the Somalis coupled with their ubiquitous Muslim beliefs. However, there have been cases in the past of artistic depictions representing living creatures, such as certain ancient rock paintings in northern Somalia, the golden birds on the Mogadishan canopies, and the plant decorations on religious tombs in southern Somalia. More typically, intricate patterns and geometric designs, bold colors and monumental architecture were the norm.
Additionally, henna is an important part of Somali culture. It is worn by Somali women on their hands, arms, feet and neck during weddings, Eid, Ramadan, and other festive occasions. Somali henna designs are similar to those in the Arabian peninsula, often featuring flower motifs and triangular shapes. The palm is also frequently decorated with a dot of henna and the fingertips are dipped in the dye. Henna parties are usually held before the wedding takes place. Somali women have likewise traditionally applied kohl (kuul) to their eyes. Usage of the eye cosmetic in the Horn region is believed to date to the ancient Land of Punt.
Football is the most popular sport amongst Somalis. Important competitions are the Somalia League and Somalia Cup. The multi-ethnic Ocean Stars, Somalia's national team, first participated at the Olympic Games in 1972 and has sent athletes to compete in most Summer Olympic Games since then. The equally diverse Somali beach soccer team also represents the country in international beach soccer competitions. In addition, several international footballers such as Mohammed Ahamed Jama, Liban Abdi, Ayub Daud and Abdisalam Ibrahim have played in European top divisions.
The FIBA Africa Championship 1981 was hosted by Somalia from 15–23 December 1981. The games were played in Mogadishu, and the Somali national team received the bronze prize. Abdi Bile won the 1500 m event at the World Championships in 1987, running the fastest final 800 m of any 1,500 meter race in history. He was a two-time Olympian (1984 and 1996) and dominated the event in the late 1980s. Hussein Ahmed Salah, a Somalia-born former long-distance runner from Djibouti, won a bronze medal in the marathon at the 1988 Summer Olympics. He also won silver medals in this event at the 1987 and 1991 World Championships, as well as the 1985 IAAF World Marathon Cup. Mo Farah is a double Olympic gold medal winner and world champion, and holds the European track record for 10,000 metres, the British road record for 10,000 metres, the British indoor record in the 3000 metres, the British track record for 5000 metres and the European indoor record for 5000 metres. Mohammed Ahmed (athlete) is a Canadian long-distance runner who represented Canada in the 10,000 meter races at the 2012 Summer Olympics and the 2013 World Championships in Athletics.
In the martial arts, Faisal Jeylani Aweys and Mohamed Deq Abdulle also took home a silver medal and fourth place, respectively, at the 2013 Open World Taekwondo Challenge Cup in Tongeren. The Somali National Olympic committee has devised a special support program to ensure continued success in future tournaments. Additionally, Mohamed Jama has won both world and European titles in K1 and Thai Boxing.
When not dressed in Westernized clothing such as jeans and t-shirts, Somali men typically wear the macawis, which is a sarong-like garment worn around the waist. On their heads, they often wrap a colorful turban or wear the koofiyad, an embroidered fez.
Due to Somalia's proximity to and close ties with the Arabian Peninsula, many Somali men also wear the jellabiya (jellabiyad or qamiis in Somali), a long white garment common in the Arab world.
During regular, day-to-day activities, Somali women usually wear the guntiino, a long stretch of cloth tied over the shoulder and draped around the waist. It is usually made out of alandi, which is a textile common in the Horn region and some parts of North Africa. The garment can be worn in different styles. It can also be made with other fabrics, including white cloth with gold borders. For more formal settings, such as at weddings or religious celebrations like Eid, women wear the dirac. It is a long, light, diaphanous voile dress made of silk, chiffon, taffeta or saree fabric. The gown is worn over a full-length half-slip and a brassiere. Known as the gorgorad, the underskirt is made out of silk and serves as a key part of the overall outfit. The dirac is usually sparkly and very colorful, the most popular styles being those with gilded borders or threads. The fabric is typically acquired from Somali clothing stores in tandem with the gorgorad. In the past, dirac fabric was also frequently purchased from South Asian merchandisers.
Married women tend to sport headscarves referred to as shaash. They also often cover their upper body with a shawl, which is known as garbasaar. Unmarried or young women, however, do not always cover their heads. Traditional Arabian garb, such as the jilbab and abaya, is also commonly worn.
Additionally, Somali women have a long tradition of wearing gold jewelry, particularly bangles. During weddings, the bride is frequently adorned in gold. Many Somali women by tradition also wear gold necklaces and anklets.
The Somali flag is an ethnic flag conceived to represent ethnic Somalis. It was created in 1954 by the Somali scholar Mohammed Awale Liban, after he had been selected by the labour trade union of the Trust Territory of Somalia to come up with a design. Upon independence in 1960, the flag was adopted as the national flag of the nascent Somali Republic. The five-pointed Star of Unity in the flag's center represents the Somali ethnic group inhabiting the five territories in Greater Somalia.
Somali cuisine varies from region to region and consists of a fusion of diverse culinary influences. It is the product of Somalia's rich tradition of trade and commerce. Despite the variety, there remains one thing that unites the various regional cuisines: all food is served halal. There are therefore no pork dishes, alcohol is not served, nothing that died on its own is eaten, and no blood is incorporated.
Qado or lunch is often elaborate. Varieties of bariis (rice), the most popular probably being basmati, usually serve as the main dish. Spices like cumin, cardamom, cloves, cinnamon, and garden sage are used to aromatize these different rice delicacies. Somalis eat dinner as late as 9 pm. During Ramadan, supper is often served after Tarawih prayers; sometimes as late as 11 pm.
Xalwo (halva) is a popular confection eaten during festive occasions, such as Eid celebrations or wedding receptions. It is made from sugar, corn starch, cardamom powder, nutmeg powder and ghee. Peanuts are also sometimes added to enhance texture and flavor. After meals, homes are traditionally perfumed using frankincense (lubaan) or incense (cuunsi), which is prepared inside an incense burner referred to as a dabqaad.
Somali scholars have for centuries produced many notable examples of Islamic literature ranging from poetry to Hadith. With the adoption of the Latin alphabet in 1972 to transcribe the Somali language, numerous contemporary Somali authors have also released novels, some of which have gone on to receive worldwide acclaim. Of these modern writers, Nuruddin Farah is probably the most celebrated. Books such as From a Crooked Rib and Links are considered important literary achievements, works which have earned Farah, among other accolades, the 1998 Neustadt International Prize for Literature. Farah Mohamed Jama Awl is another prominent Somali writer who is perhaps best known for his Dervish era novel, Ignorance is the enemy of love. Young upstart Nadifa Mohamed was also awarded the 2010 Betty Trask Prize. Additionally, Mohamed Ibrahim Warsame 'Hadrawi' is considered by many to be the greatest living Somali poet, and several of his works have been translated internationally.
Somalis for centuries have practiced a form of customary law, which they call Xeer. Xeer is a polycentric legal system where there is no monopolistic agent that determines what the law should be or how it should be interpreted.
The Xeer legal system is assumed to have developed exclusively in the Horn of Africa since approximately the 7th century. There is no evidence that it developed elsewhere or was greatly influenced by any foreign legal system. The fact that Somali legal terminology is practically devoid of loan words from foreign languages suggests that Xeer is truly indigenous.
The Xeer legal system also requires a certain amount of specialization of different functions within the legal framework. Thus, one can find odayal (judges), xeer boggeyaal (jurists), guurtiyaal (detectives), garxajiyaal (attorneys), murkhaatiyal (witnesses) and waranle (police officers) to enforce the law.
Somali architecture is a rich and diverse tradition of engineering and designing. It involves multiple different construction types, such as stone cities, castles, citadels, fortresses, mosques, mausoleums, towers, tombs, tumuli, cairns, megaliths, menhirs, stelae, dolmens, stone circles, monuments, temples, enclosures, cisterns, aqueducts, and lighthouses. Spanning the ancient, medieval and early modern periods in Greater Somalia, it also includes the fusion of Somalo-Islamic architecture with Western designs in contemporary times.
In ancient Somalia, pyramidical structures known in Somali as taalo were a popular burial style, with hundreds of these dry stone monuments scattered around the country today. Houses were built of dressed stone similar to the ones in Ancient Egypt. There are also examples of courtyards and large stone walls enclosing settlements, such as the Wargaade Wall.
The peaceful introduction of Islam in the early medieval era of Somalia's history brought Islamic architectural influences from Arabia and Persia. This had the effect of stimulating a shift in construction from drystone and other related materials to coral stone, sundried bricks, and the widespread use of limestone in Somali architecture. Many of the new architectural designs, such as mosques, were built on the ruins of older structures. This practice would continue over and over again throughout the following centuries.
The scholarly term for research concerning Somalis and Greater Somalia is known as Somali Studies. It consists of several disciplines such as anthropology, sociology, linguistics, historiography and archaeology. The field draws from old Somali chronicles, records and oral literature, in addition to written accounts and traditions about Somalis from explorers and geographers in the Horn of Africa and the Middle East. Since 1980, prominent Somalist scholars from around the world have also gathered annually to hold the International Congress of Somali Studies.
Around 1300, centuries of prosperity and growth in Europe came to a halt. A series of famines and plagues, including the Great Famine of 1315–1317 and the Black Death, reduced the population to around half of what it was before the calamities. Along with depopulation came social unrest and endemic warfare. France and England experienced serious peasant uprisings, such as the Jacquerie and the Peasants' Revolt, as well as over a century of intermittent conflict in the Hundred Years' War. To add to the many problems of the period, the unity of the Catholic Church was shattered by the Western Schism. Collectively these events are sometimes called the Crisis of the Late Middle Ages.
Despite these crises, the 14th century was also a time of great progress in the arts and sciences. Following a renewed interest in ancient Greek and Roman texts that took root in the High Middle Ages, the Italian Renaissance began. The absorption of Latin texts had started before the Renaissance of the 12th century through contact with Arabs during the Crusades, but the availability of important Greek texts accelerated with the capture of Constantinople by the Ottoman Turks, when many Byzantine scholars had to seek refuge in the West, particularly Italy.
Combined with this influx of classical ideas was the invention of printing which facilitated dissemination of the printed word and democratized learning. These two things would later lead to the Protestant Reformation. Toward the end of the period, an era of discovery began (Age of Discovery). The rise of the Ottoman Empire, culminating in the Fall of Constantinople in 1453, eroded the last remnants of the Byzantine Empire and cut off trading possibilities with the east. Europeans were forced to seek new trading routes, leading to the expedition of Columbus to the Americas in 1492, and Vasco da Gama’s circumnavigation of India and Africa in 1498. Their discoveries strengthened the economy and power of European nations.
The term "Late Middle Ages" refers to one of the three periods of the Middle Ages, along with the Early Middle Ages and the High Middle Ages. Leonardo Bruni was the first historian to use tripartite periodization in his History of the Florentine People (1442). Flavio Biondo used a similar framework in Decades of History from the Deterioration of the Roman Empire (1439–1453). Tripartite periodization became standard after the German historian Christoph Cellarius published Universal History Divided into an Ancient, Medieval, and New Period (1683).
As economic and demographic methods were applied to the study of history, the trend was increasingly to see the late Middle Ages as a period of recession and crisis. Belgian historian Henri Pirenne continued the subdivision of Early, High, and Late Middle Ages in the years around World War I. Yet it was his Dutch colleague, Johan Huizinga, who was primarily responsible for popularising the pessimistic view of the Late Middle Ages, with his book The Autumn of the Middle Ages (1919). To Huizinga, whose research focused on France and the Low Countries rather than Italy, despair and decline were the main themes, not rebirth.
Modern historiography on the period has reached a consensus between the two extremes of innovation and crisis. It is now (generally) acknowledged that conditions were vastly different north and south of the Alps, and "Late Middle Ages" is often avoided entirely within Italian historiography. The term "Renaissance" is still considered useful for describing certain intellectual, cultural, or artistic developments, but not as the defining feature of an entire European historical epoch. The period from the early 14th century up until – and sometimes including – the 16th century, is rather seen as characterised by other trends: demographic and economic decline followed by recovery, the end of western religious unity and the subsequent emergence of the nation state, and the expansion of European influence onto the rest of the world.
After the failed union of Sweden and Norway of 1319–1365, the pan-Scandinavian Kalmar Union was instituted in 1397. The Swedes were reluctant members of the Danish-dominated union from the start. In an attempt to subdue the Swedes, King Christian II of Denmark had large numbers of the Swedish aristocracy killed in the Stockholm Bloodbath of 1520. Yet this measure only led to further hostilities, and Sweden broke away for good in 1523. Norway, on the other hand, became an inferior party of the union and remained united with Denmark until 1814.
Bohemia prospered in the 14th century, and the Golden Bull of 1356 made the king of Bohemia first among the imperial electors, but the Hussite revolution threw the country into crisis. The Holy Roman Empire passed to the Habsburgs in 1438, where it remained until its dissolution in 1806. Yet in spite of the extensive territories held by the Habsburgs, the Empire itself remained fragmented, and much real power and influence lay with the individual principalities. In addition, financial institutions, such as the Hanseatic League and the Fugger family, held great power, on both economic and a political levels.
Louis did not leave a son as heir after his death in 1382. Instead, he named as his heir the young prince Sigismund of Luxemburg, who was 11 years old. The Hungarian nobility did not accept his claim, and the result was an internal war. Sigismund eventually achieved total control of Hungary and established his court in Buda and Visegrád. Both palaces were rebuilt and improved, and were considered the richest of the time in Europe. Inheriting the throne of Bohemia and the Holy Roman Empire, Sigismund continued conducting his politics from Hungary, but he was kept busy fighting the Hussites and the Ottoman Empire, which was becoming a menace to Europe in the beginning of the 15th century.
The Bulgarian Empire was in decline by the 14th century, and the ascendancy of Serbia was marked by the Serbian victory over the Bulgarians in the Battle of Velbazhd in 1330. By 1346, the Serbian king Stefan Dušan had been proclaimed emperor. Yet Serbian dominance was short-lived; the Serbian army led by the Lazar Hrebljevanovic was defeated by the Ottomans at the Battle of Kosovo in 1389, where most of the Serbian nobility was killed and the south of the country came under Ottoman occupation, as much of southern Bulgaria had become Ottoman territory in 1371. Northern remnants of Bulgaria were finally conquered by 1396, Serbia fell in 1459, Bosnia in 1463, and Albania was finally subordinated in 1479 only a few years after the death of Skanderbeg. Belgrade, an Hungarian domain at the time, was the last large Balkan city to fall under Ottoman rule, in 1521. By the end of the medieval period, the entire Balkan peninsula was annexed by, or became vassal to, the Ottomans.
Avignon was the seat of the papacy from 1309 to 1376. With the return of the Pope to Rome in 1378, the Papal State developed into a major secular power, culminating in the morally corrupt papacy of Alexander VI. Florence grew to prominence amongst the Italian city-states through financial business, and the dominant Medici family became important promoters of the Renaissance through their patronage of the arts. Other city states in northern Italy also expanded their territories and consolidated their power, primarily Milan and Venice. The War of the Sicilian Vespers had by the early 14th century divided southern Italy into an Aragon Kingdom of Sicily and an Anjou Kingdom of Naples. In 1442, the two kingdoms were effectively united under Aragonese control.
The 1469 marriage of Isabella I of Castile and Ferdinand II of Aragon and the 1479 death of John II of Aragon led to the creation of modern-day Spain. In 1492, Granada was captured from the Moors, thereby completing the Reconquista. Portugal had during the 15th century – particularly under Henry the Navigator – gradually explored the coast of Africa, and in 1498, Vasco da Gama found the sea route to India. The Spanish monarchs met the Portuguese challenge by financing the expedition of Christopher Columbus to find a western sea route to India, leading to the discovery of the Americas in 1492.
Around 1300–1350 the Medieval Warm Period gave way to the Little Ice Age. The colder climate resulted in agricultural crises, the first of which is known as the Great Famine of 1315-1317. The demographic consequences of this famine, however, were not as severe as the plagues that occurred later in the century, particularly the Black Death. Estimates of the death rate caused by this epidemic range from one third to as much as sixty percent. By around 1420, the accumulated effect of recurring plagues and famines had reduced the population of Europe to perhaps no more than a third of what it was a century earlier. The effects of natural disasters were exacerbated by armed conflicts; this was particularly the case in France during the Hundred Years' War.
As the European population was severely reduced, land became more plentiful for the survivors, and labour consequently more expensive. Attempts by landowners to forcibly reduce wages, such as the English 1351 Statute of Laborers, were doomed to fail. These efforts resulted in nothing more than fostering resentment among the peasantry, leading to rebellions such as the French Jacquerie in 1358 and the English Peasants' Revolt in 1381. The long-term effect was the virtual end of serfdom in Western Europe. In Eastern Europe, on the other hand, landowners were able to exploit the situation to force the peasantry into even more repressive bondage.
Up until the mid-14th century, Europe had experienced steadily increasing urbanisation. Cities were also decimated by the Black Death, but the role of urban areas as centres of learning, commerce and government ensured continued growth. By 1500, Venice, Milan, Naples, Paris and Constantinople each probably had more than 100,000 inhabitants. Twenty-two other cities were larger than 40,000; most of these were in Italy and the Iberian peninsula, but there were also some in France, the Empire, the Low Countries, plus London in England.
Changes also took place within the recruitment and composition of armies. The use of the national or feudal levy was gradually replaced by paid troops of domestic retinues or foreign mercenaries. The practice was associated with Edward III of England and the condottieri of the Italian city-states. All over Europe, Swiss soldiers were in particularly high demand. At the same time, the period also saw the emergence of the first permanent armies. It was in Valois France, under the heavy demands of the Hundred Years' War, that the armed forces gradually assumed a permanent nature.
Parallel to the military developments emerged also a constantly more elaborate chivalric code of conduct for the warrior class. This new-found ethos can be seen as a response to the diminishing military role of the aristocracy, and gradually it became almost entirely detached from its military origin. The spirit of chivalry was given expression through the new (secular) type of chivalric orders; the first of these was the Order of St. George, founded by Charles I of Hungary in 1325, while the best known was probably the English Order of the Garter, founded by Edward III in 1348.
The French crown's increasing dominance over the Papacy culminated in the transference of the Holy See to Avignon in 1309. When the Pope returned to Rome in 1377, this led to the election of different popes in Avignon and Rome, resulting in the Papal Schism (1378–1417). The Schism divided Europe along political lines; while France, her ally Scotland and the Spanish kingdoms supported the Avignon Papacy, France's enemy England stood behind the Pope in Rome, together with Portugal, Scandinavia and most of the German princes.
Though many of the events were outside the traditional time-period of the Middle Ages, the end of the unity of the Western Church (the Protestant Reformation), was one of the distinguishing characteristics of the medieval period. The Catholic Church had long fought against heretic movements, but during the Late Middle Ages, it started to experience demands for reform from within. The first of these came from Oxford professor John Wycliffe in England. Wycliffe held that the Bible should be the only authority in religious questions, and he spoke out against transubstantiation, celibacy and indulgences. In spite of influential supporters among the English aristocracy, such as John of Gaunt, the movement was not allowed to survive. Though Wycliffe himself was left unmolested, his supporters, the Lollards, were eventually suppressed in England.
The marriage of Richard II of England to Anne of Bohemia established contacts between the two nations and brought Lollard ideas to her homeland. The teachings of the Czech priest Jan Hus were based on those of John Wycliffe, yet his followers, the Hussites, were to have a much greater political impact than the Lollards. Hus gained a great following in Bohemia, and in 1414, he was requested to appear at the Council of Constance to defend his cause. When he was burned as a heretic in 1415, it caused a popular uprising in the Czech lands. The subsequent Hussite Wars fell apart due to internal quarrels and did not result in religious or national independence for the Czechs, but both the Catholic Church and the German element within the country were weakened.
Martin Luther, a German monk, started the German Reformation by posting 95 theses on the castle church of Wittenberg on October 31, 1517. The immediate provocation spurring this act was Pope Leo X’s renewal of the indulgence for the building of the new St. Peter's Basilica in 1514. Luther was challenged to recant his heresy at the Diet of Worms in 1521. When he refused, he was placed under the ban of the Empire by Charles V. Receiving the protection of Frederick the Wise, he was then able to translate the Bible into German.
In the late 13th and early 14th centuries, a process took place – primarily in Italy but partly also in the Empire – that historians have termed a 'commercial revolution'. Among the innovations of the period were new forms of partnership and the issuing of insurance, both of which contributed to reducing the risk of commercial ventures; the bill of exchange and other forms of credit that circumvented the canonical laws for gentiles against usury, and eliminated the dangers of carrying bullion; and new forms of accounting, in particular double-entry bookkeeping, which allowed for better oversight and accuracy.
With the financial expansion, trading rights became more jealously guarded by the commercial elite. Towns saw the growing power of guilds, while on a national level special companies would be granted monopolies on particular trades, like the English wool Staple. The beneficiaries of these developments would accumulate immense wealth. Families like the Fuggers in Germany, the Medicis in Italy, the de la Poles in England, and individuals like Jacques Coeur in France would help finance the wars of kings, and achieve great political influence in the process.
Though there is no doubt that the demographic crisis of the 14th century caused a dramatic fall in production and commerce in absolute terms, there has been a vigorous historical debate over whether the decline was greater than the fall in population. While the older orthodoxy held that the artistic output of the Renaissance was a result of greater opulence, more recent studies have suggested that there might have been a so-called 'depression of the Renaissance'. In spite of convincing arguments for the case, the statistical evidence is simply too incomplete for a definite conclusion to be made.
The predominant school of thought in the 13th century was the Thomistic reconciliation of the teachings of Aristotle with Christian theology. The Condemnation of 1277, enacted at the University of Paris, placed restrictions on ideas that could be interpreted as heretical; restrictions that had implication for Aristotelian thought. An alternative was presented by William of Ockham, who insisted that the world of reason and the world of faith had to be kept apart. Ockham introduced the principle of parsimony – or Occam's razor – whereby a simple theory is preferred to a more complex one, and speculation on unobservable phenomena is avoided.
This new approach liberated scientific speculation from the dogmatic restraints of Aristotelian science, and paved the way for new approaches. Particularly within the field of theories of motion great advances were made, when such scholars as Jean Buridan, Nicole Oresme and the Oxford Calculators challenged the work of Aristotle. Buridan developed the theory of impetus as the cause of the motion of projectiles, which was an important step towards the modern concept of inertia. The works of these scholars anticipated the heliocentric worldview of Nicolaus Copernicus.
Certain technological inventions of the period – whether of Arab or Chinese origin, or unique European innovations – were to have great influence on political and social developments, in particular gunpowder, the printing press and the compass. The introduction of gunpowder to the field of battle affected not only military organisation, but helped advance the nation state. Gutenberg's movable type printing press made possible not only the Reformation, but also a dissemination of knowledge that would lead to a gradually more egalitarian society. The compass, along with other innovations such as the cross-staff, the mariner's astrolabe, and advances in shipbuilding, enabled the navigation of the World Oceans, and the early phases of colonialism. Other inventions had a greater impact on everyday life, such as eyeglasses and the weight-driven clock.
The period saw several important technical innovations, like the principle of linear perspective found in the work of Masaccio, and later described by Brunelleschi. Greater realism was also achieved through the scientific study of anatomy, championed by artists like Donatello. This can be seen particularly well in his sculptures, inspired by the study of classical models. As the centre of the movement shifted to Rome, the period culminated in the High Renaissance masters da Vinci, Michelangelo and Raphael.
The ideas of the Italian Renaissance were slow to cross the Alps into northern Europe, but important artistic innovations were made also in the Low Countries. Though not – as previously believed – the inventor of oil painting, Jan van Eyck was a champion of the new medium, and used it to create works of great realism and minute detail. The two cultures influenced each other and learned from each other, but painting in the Netherlands remained more focused on textures and surfaces than the idealized compositions of Italy.
Dante Alighieri's Divine Comedy, written in the early 14th century, merged a medieval world view with classical ideals. Another promoter of the Italian language was Boccaccio with his Decameron. The application of the vernacular did not entail a rejection of Latin, and both Dante and Boccaccio wrote prolifically in Latin as well as Italian, as would Petrarch later (whose Canzoniere also promoted the vernacular and whose contents are considered the first modern lyric poems). Together the three poets established the Tuscan dialect as the norm for the modern Italian language.
Music was an important part of both secular and spiritual culture, and in the universities it made up part of the quadrivium of the liberal arts. From the early 13th century, the dominant sacred musical form had been the motet; a composition with text in several parts. From the 1330s and onwards, emerged the polyphonic style, which was a more complex fusion of independent voices. Polyphony had been common in the secular music of the Provençal troubadours. Many of these had fallen victim to the 13th-century Albigensian Crusade, but their influence reached the papal court at Avignon.
The main representatives of the new style, often referred to as ars nova as opposed to the ars antiqua, were the composers Philippe de Vitry and Guillaume de Machaut. In Italy, where the Provençal troubadours had also found refuge, the corresponding period goes under the name of trecento, and the leading composers were Giovanni da Cascia, Jacopo da Bologna and Francesco Landini. Prominent reformer of Orthodox Church music from the first half of 14th century was John Kukuzelis; he also introduced a system of notation widely used in the Balkans in the following centuries.
Morality plays emerged as a distinct dramatic form around 1400 and flourished until 1550. The most interesting morality play is The Castle of Perseverance which depicts mankind's progress from birth to death. However, the most famous morality play and perhaps best known medieval drama is Everyman. Everyman receives Death's summons, struggles to escape and finally resigns himself to necessity. Along the way, he is deserted by Kindred, Goods, and Fellowship - only Good Deeds goes with him to the grave.
At the end of the Late Middle Ages, professional actors began to appear in England and Europe. Richard III and Henry VII both maintained small companies of professional actors. Their plays were performed in the Great Hall of a nobleman's residence, often with a raised platform at one end for the audience and a "screen" at the other for the actors. Also important were Mummers' plays, performed during the Christmas season, and court masques. These masques were especially popular during the reign of Henry VIII who had a House of Revels built and an Office of Revels established in 1545.
The end of medieval drama came about due to a number of factors, including the weakening power of the Catholic Church, the Protestant Reformation and the banning of religious plays in many countries. Elizabeth I forbid all religious plays in 1558 and the great cycle plays had been silenced by the 1580s. Similarly, religious plays were banned in the Netherlands in 1539, the Papal States in 1547 and in Paris in 1548. The abandonment of these plays destroyed the international theatre that had thereto existed and forced each country to develop its own form of drama. It also allowed dramatists to turn to secular subjects and the reviving interest in Greek and Roman theatre provided them with the perfect opportunity.
After the end of the late Middle Ages period, the Renaissance would spread unevenly over continental Europe from the southern European region. The intellectual transformation of the Renaissance is viewed as a bridge between the Middle Ages and the Modern era. Europeans would later begin an era of world discovery. Combined with the influx of classical ideas was the invention of printing which facilitated dissemination of the printed word and democratized learning. These two things would lead to the Protestant Reformation. Europeans also discovered new trading routes, as was the case with Columbus’s travel to the Americas in 1492, and Vasco da Gama’s circumnavigation of Africa and India in 1498. Their discoveries strengthened the economy and power of European nations.
At the end of the 15th century the Ottoman Empire advanced all over Southeastern Europe, eventually conquering the Byzantine Empire and extending control over the Balkan states. Hungary was the last bastion of the Latin Christian world in the East, and fought to keep its rule over a period of two centuries. After the tragic death of the young king Vladislaus I of Hungary during the Battle of Varna in 1444 against the Ottomans, the Kingdom was placed in the hands of count John Hunyadi, who became Hungary's regent-governor (1446–1453). Hunyadi was considered one of the most relevant military figures of the 15th century: Pope Pius II awarded him the title of Athleta Christi or Champion of Christ for being the only hope of resisting the Ottomans from advancing to Central and Western Europe.
Hunyadi succeeded during the Siege of Belgrade in 1456 against the Ottomans, the biggest victory against that empire in decades. This battle became a real Crusade against the Muslims, as the peasants were motivated by the Franciscan monk Saint John of Capistrano, who came from Italy predicating Holy War. The effect that it created in that time was one of the main factors that helped in achieving the victory. However the premature death of the Hungarian Lord left Pannonia defenseless and in chaos. In an extremely unusual event for the Middle Ages, Hunyadi's son, Matthias, was elected as King of Hungary by the nobility. For the first time, a member of an aristocratic family (and not from a royal family) was crowned.
King Matthias Corvinus of Hungary (1458–1490) was one of the most prominent figures of the period, directing campaigns to the West, conquering Bohemia in answer to the Pope's call for help against the Hussite Protestants. Also, in resolving political hostilities with the German emperor Frederick III of Habsburg, he invaded his western domains. Matthew organized the Black Army of mercenary soldiers; it was considered as the biggest army of its time. Using this powerful tool, the Hungarian king led wars against the Turkish armies and stopped the Ottomans during his reign. After the death of Matthew, and with end of the Black Army, the Ottoman Empire grew in strength and Central Europe was defenseless. At the Battle of Mohács, the forces of the Ottoman Empire annihilated the Hungarian army and Louis II of Hungary drowned in the Csele Creek while trying to escape. The leader of the Hungarian army, Pál Tomori, also died in the battle. This is considered to be one of the final battles of Medieval times.
The changes brought about by these developments have led many scholars to view this period as the end of the Middle Ages and beginning of modern history and early modern Europe. However, the division is somewhat artificial, since ancient learning was never entirely absent from European society. As a result there was developmental continuity between the ancient age (via classical antiquity) and the modern age. Some historians, particularly in Italy, prefer not to speak of the late Middle Ages at all, but rather see the high period of the Middle Ages transitioning to the Renaissance and the modern era.
Diarrhea, also spelled diarrhoea, is the condition of having at least three loose or liquid bowel movements each day. It often lasts for a few days and can result in dehydration due to fluid loss. Signs of dehydration often begin with loss of the normal stretchiness of the skin and irritable behaviour. This can progress to decreased urination, loss of skin color, a fast heart rate, and a decrease in responsiveness as it becomes more severe. Loose but non-watery stools in babies who are breastfed, however, may be normal.
The most common cause is an infection of the intestines due to either a virus, bacteria, or parasite; a condition known as gastroenteritis. These infections are often acquired from food or water that has been contaminated by stool, or directly from another person who is infected. It may be divided into three types: short duration watery diarrhea, short duration bloody diarrhea, and if it lasts for more than two weeks, persistent diarrhea. The short duration watery diarrhea may be due to an infection by cholera, although this is rare in the developed world. If blood is present it is also known as dysentery. A number of non-infectious causes may also result in diarrhea, including hyperthyroidism, lactose intolerance, inflammatory bowel disease, a number of medications, and irritable bowel syndrome. In most cases, stool cultures are not required to confirm the exact cause.
Prevention of infectious diarrhea is by improved sanitation, clean drinking water, and hand washing with soap. Breastfeeding for at least six months is also recommended as is vaccination against rotavirus. Oral rehydration solution (ORS), which is clean water with modest amounts of salts and sugar, is the treatment of choice. Zinc tablets are also recommended. These treatments have been estimated to have saved 50 million children in the past 25 years. When people have diarrhea it is recommended that they continue to eat healthy food and babies continue to be breastfed. If commercial ORS are not available, homemade solutions may be used. In those with severe dehydration, intravenous fluids may be required. Most cases; however, can be managed well with fluids by mouth. Antibiotics, while rarely used, may be recommended in a few cases such as those who have bloody diarrhea and a high fever, those with severe diarrhea following travelling, and those who grow specific bacteria or parasites in their stool. Loperamide may help decrease the number of bowel movement but is not recommended in those with severe disease.
About 1.7 to 5 billion cases of diarrhea occur per year. It is most common in developing countries, where young children get diarrhea on average three times a year. Total deaths from diarrhea are estimated at 1.26 million in 2013 – down from 2.58 million in 1990. In 2012, it is the second most common cause of deaths in children younger than five (0.76 million or 11%). Frequent episodes of diarrhea are also a common cause of malnutrition and the most common cause in those younger than five years of age. Other long term problems that can result include stunted growth and poor intellectual development.
Secretory diarrhea means that there is an increase in the active secretion, or there is an inhibition of absorption. There is little to no structural damage. The most common cause of this type of diarrhea is a cholera toxin that stimulates the secretion of anions, especially chloride ions. Therefore, to maintain a charge balance in the lumen, sodium is carried with it, along with water. In this type of diarrhea intestinal fluid secretion is isotonic with plasma even during fasting. It continues even when there is no oral food intake.
Osmotic diarrhea occurs when too much water is drawn into the bowels. If a person drinks solutions with excessive sugar or excessive salt, these can draw water from the body into the bowel and cause osmotic diarrhea. Osmotic diarrhea can also be the result of maldigestion (e.g., pancreatic disease or Coeliac disease), in which the nutrients are left in the lumen to pull in water. Or it can be caused by osmotic laxatives (which work to alleviate constipation by drawing water into the bowels). In healthy individuals, too much magnesium or vitamin C or undigested lactose can produce osmotic diarrhea and distention of the bowel. A person who has lactose intolerance can have difficulty absorbing lactose after an extraordinarily high intake of dairy products. In persons who have fructose malabsorption, excess fructose intake can also cause diarrhea. High-fructose foods that also have a high glucose content are more absorbable and less likely to cause diarrhea. Sugar alcohols such as sorbitol (often found in sugar-free foods) are difficult for the body to absorb and, in large amounts, may lead to osmotic diarrhea. In most of these cases, osmotic diarrhea stops when offending agent (e.g. milk, sorbitol) is stopped.
Inflammatory diarrhea occurs when there is damage to the mucosal lining or brush border, which leads to a passive loss of protein-rich fluids and a decreased ability to absorb these lost fluids. Features of all three of the other types of diarrhea[clarification needed] can be found in this type of diarrhea. It can be caused by bacterial infections, viral infections, parasitic infections, or autoimmune problems such as inflammatory bowel diseases. It can also be caused by tuberculosis, colon cancer, and enteritis.[citation needed]
Diarrheal disease may have a negative impact on both physical fitness and mental development. "Early childhood malnutrition resulting from any cause reduces physical fitness and work productivity in adults," and diarrhea is a primary cause of childhood malnutrition. Further, evidence suggests that diarrheal disease has significant impacts on mental development and health; it has been shown that, even when controlling for helminth infection and early breastfeeding, children who had experienced severe diarrhea had significantly lower scores on a series of tests of intelligence.
Another possible cause of diarrhea is irritable bowel syndrome (IBS), which usually presents with abdominal discomfort relieved by defecation and unusual stool (diarrhea or constipation) for at least 3 days a week over the previous 3 months. Symptoms of diarrhea-predominant IBS can be managed through a combination of dietary changes, soluble fiber supplements, and/or medications such as loperamide or codeine. About 30% of patients with diarrhea-predominant IBS have bile acid malabsorption diagnosed with an abnormal SeHCAT test.
Poverty is a good indicator of the rate of infectious diarrhea in a population. This association does not stem from poverty itself, but rather from the conditions under which impoverished people live. The absence of certain resources compromises the ability of the poor to defend themselves against infectious diarrhea. "Poverty is associated with poor housing, crowding, dirt floors, lack of access to clean water or to sanitary disposal of fecal waste (sanitation), cohabitation with domestic animals that may carry human pathogens, and a lack of refrigerated storage for food, all of which increase the frequency of diarrhea... Poverty also restricts the ability to provide age-appropriate, nutritionally balanced diets or to modify diets when diarrhea develops so as to mitigate and repair nutrient losses. The impact is exacerbated by the lack of adequate, available, and affordable medical care."
Proper nutrition is important for health and functioning, including the prevention of infectious diarrhea. It is especially important to young children who do not have a fully developed immune system. Zinc deficiency, a condition often found in children in developing countries can, even in mild cases, have a significant impact on the development and proper functioning of the human immune system. Indeed, this relationship between zinc deficiency reduced immune functioning corresponds with an increased severity of infectious diarrhea. Children who have lowered levels of zinc have a greater number of instances of diarrhea, severe diarrhea, and diarrhea associated with fever. Similarly, vitamin A deficiency can cause an increase in the severity of diarrheal episodes, however there is some discrepancy when it comes to the impact of vitamin A deficiency on the rate of disease. While some argue that a relationship does not exist between the rate of disease and vitamin A status, others suggest an increase in the rate associated with deficiency. Given that estimates suggest 127 million preschool children worldwide are vitamin A deficient, this population has the potential for increased risk of disease contraction.
According to two researchers, Nesse and Williams, diarrhea may function as an evolved expulsion defense mechanism. As a result, if it is stopped, there might be a delay in recovery. They cite in support of this argument research published in 1973 that found that treating Shigella with the anti-diarrhea drug (Co-phenotrope, Lomotil) caused people to stay feverish twice as long as those not so treated. The researchers indeed themselves observed that: "Lomotil may be contraindicated in shigellosis. Diarrhea may represent a defense mechanism".
Basic sanitation techniques can have a profound effect on the transmission of diarrheal disease. The implementation of hand washing using soap and water, for example, has been experimentally shown to reduce the incidence of disease by approximately 42–48%. Hand washing in developing countries, however, is compromised by poverty as acknowledged by the CDC: "Handwashing is integral to disease prevention in all parts of the world; however, access to soap and water is limited in a number of less developed countries. This lack of access is one of many challenges to proper hygiene in less developed countries." Solutions to this barrier require the implementation of educational programs that encourage sanitary behaviours.
Given that water contamination is a major means of transmitting diarrheal disease, efforts to provide clean water supply and improved sanitation have the potential to dramatically cut the rate of disease incidence. In fact, it has been proposed that we might expect an 88% reduction in child mortality resulting from diarrheal disease as a result of improved water sanitation and hygiene. Similarly, a meta-analysis of numerous studies on improving water supply and sanitation shows a 22–27% reduction in disease incidence, and a 21–30% reduction in mortality rate associated with diarrheal disease.
Immunization against the pathogens that cause diarrheal disease is a viable prevention strategy, however it does require targeting certain pathogens for vaccination. In the case of Rotavirus, which was responsible for around 6% of diarrheal episodes and 20% of diarrheal disease deaths in the children of developing countries, use of a Rotavirus vaccine in trials in 1985 yielded a slight (2-3%) decrease in total diarrheal disease incidence, while reducing overall mortality by 6-10%. Similarly, a Cholera vaccine showed a strong reduction in morbidity and mortality, though the overall impact of vaccination was minimal as Cholera is not one of the major causative pathogens of diarrheal disease. Since this time, more effective vaccines have been developed that have the potential to save many thousands of lives in developing nations, while reducing the overall cost of treatment, and the costs to society.
Dietary deficiencies in developing countries can be combated by promoting better eating practices. Supplementation with vitamin A and/or zinc. Zinc supplementation proved successful showing a significant decrease in the incidence of diarrheal disease compared to a control group. The majority of the literature suggests that vitamin A supplementation is advantageous in reducing disease incidence. Development of a supplementation strategy should take into consideration the fact that vitamin A supplementation was less effective in reducing diarrhea incidence when compared to vitamin A and zinc supplementation, and that the latter strategy was estimated to be significantly more cost effective.
In many cases of diarrhea, replacing lost fluid and salts is the only treatment needed. This is usually by mouth – oral rehydration therapy – or, in severe cases, intravenously. Diet restrictions such as the BRAT diet are no longer recommended. Research does not support the limiting of milk to children as doing so has no effect on duration of diarrhea. To the contrary, WHO recommends that children with diarrhea continue to eat as sufficient nutrients are usually still absorbed to support continued growth and weight gain, and that continuing to eat also speeds up recovery of normal intestinal functioning. CDC recommends that children and adults with cholera also continue to eat.
Oral rehydration solution (ORS) (a slightly sweetened and salty water) can be used to prevent dehydration. Standard home solutions such as salted rice water, salted yogurt drinks, vegetable and chicken soups with salt can be given. Home solutions such as water in which cereal has been cooked, unsalted soup, green coconut water, weak tea (unsweetened), and unsweetened fresh fruit juices can have from half a teaspoon to full teaspoon of salt (from one-and-a-half to three grams) added per liter. Clean plain water can also be one of several fluids given. There are commercial solutions such as Pedialyte, and relief agencies such as UNICEF widely distribute packets of salts and sugar. A WHO publication for physicians recommends a homemade ORS consisting of one liter water with one teaspoon salt (3 grams) and two tablespoons sugar (18 grams) added (approximately the "taste of tears"). Rehydration Project recommends adding the same amount of sugar but only one-half a teaspoon of salt, stating that this more dilute approach is less risky with very little loss of effectiveness. Both agree that drinks with too much sugar or salt can make dehydration worse.
Drinks especially high in simple sugars, such as soft drinks and fruit juices, are not recommended in children under 5 years of age as they may increase dehydration. A too rich solution in the gut draws water from the rest of the body, just as if the person were to drink sea water. Plain water may be used if more specific and effective ORT preparations are unavailable or are not palatable. Additionally, a mix of both plain water and drinks perhaps too rich in sugar and salt can alternatively be given to the same person, with the goal of providing a medium amount of sodium overall. A nasogastric tube can be used in young children to administer fluids if warranted.
WHO recommends a child with diarrhea continue to be fed. Continued feeding speeds the recovery of normal intestinal function. In contrast, children whose food is restricted have diarrhea of longer duration and recover intestinal function more slowly. A child should also continue to be breastfed. The WHO states "Food should never be withheld and the child's usual foods should not be diluted. Breastfeeding should always be continued." And in the specific example of cholera, CDC also makes the same recommendation. In young children who are not breast-fed and live in the developed world, a lactose-free diet may be useful to speed recovery.
While antibiotics are beneficial in certain types of acute diarrhea, they are usually not used except in specific situations. There are concerns that antibiotics may increase the risk of hemolytic uremic syndrome in people infected with Escherichia coli O157:H7. In resource-poor countries, treatment with antibiotics may be beneficial. However, some bacteria are developing antibiotic resistance, particularly Shigella. Antibiotics can also cause diarrhea, and antibiotic-associated diarrhea is the most common adverse effect of treatment with general antibiotics.
Hindu philosophy refers to a group of darśanas (philosophies, world views, teachings) that emerged in ancient India. The mainstream Hindu philosophy includes six systems (ṣaḍdarśana) – Samkhya, Yoga, Nyaya, Vaisheshika, Mimamsa and Vedanta. These are also called the Astika (orthodox) philosophical traditions and are those that accept the Vedas as authoritative, important source of knowledge.[note 1][note 2] Ancient and medieval India was also the source of philosophies that share philosophical concepts but rejected the Vedas, and these have been called nāstika (heterodox or non-orthodox) Indian philosophies. Nāstika Indian philosophies include Buddhism, Jainism, Cārvāka, Ājīvika, and others.
Scholars have debated the relationship and differences within āstika philosophies and with nāstika philosophies, starting with the writings of Indologists and Orientalists of the 18th and 19th centuries, which were themselves derived from limited availability of Indian literature and medieval doxographies. The various sibling traditions included in Hindu philosophies are diverse, and they are united by shared history and concepts, same textual resources, similar ontological and soteriological focus, and cosmology. While Buddhism and Jainism are considered distinct philosophies and religions, some heterodox traditions such as Cārvāka are often considered as distinct schools within Hindu philosophy.
Hindu philosophy also includes several sub-schools of theistic philosophies that integrate ideas from two or more of the six orthodox philosophies, such as the realism of the Nyāya, the naturalism of the Vaiśeṣika, the dualism of the Sāṅkhya, the monism and knowledge of Self as essential to liberation of Advaita, the self-discipline of yoga and the asceticism and elements of theistic ideas. Examples of such schools include Pāśupata Śaiva, Śaiva siddhānta, Pratyabhijña, Raseśvara and Vaiṣṇava. Some sub-schools share Tantric ideas with those found in some Buddhist traditions. The ideas of these sub-schools are found in the Puranas and Āgamas.
Ancient and medieval Hindu texts identify six pramāṇas as correct means of accurate knowledge and truths: pratyakṣa (perception), anumāṇa (inference), upamāṇa (comparison and analogy), arthāpatti (postulation, derivation from circumstances), anupalabdi (non-perception, negative/cognitive proof) and śabda (word, testimony of past or present reliable experts) Each of these are further categorized in terms of conditionality, completeness, confidence and possibility of error, by each school . The various schools vary on how many of these six are valid paths of knowledge. For example, the Cārvāka nāstika philosophy holds that only one (perception) is an epistemically reliable means of knowledge, the Samkhya school holds three are (perception, inference and testimony), while the Mīmāṃsā and Advaita schools hold all six are epistemically useful and reliable means to knowledge.
Samkhya school espouses dualism between consciousness and matter. It regards the universe as consisting of two realities; Puruṣa (consciousness) and prakriti (matter). Jiva (a living being) is that state in which puruṣa is bonded to prakriti in some form. This fusion, state the Samkhya scholars, led to the emergence of buddhi (awareness, intellect) and ahankara (individualized ego consciousness, “I-maker”). The universe is described by this school as one created by Purusa-Prakriti entities infused with various permutations and combinations of variously enumerated elements, senses, feelings, activity and mind.
Samkhya philosophy includes a theory of gunas (qualities, innate tendencies, psyche). Guna, it states, are of three types: Sattva being good, compassionate, illuminating, positive, and constructive; Rajas guna is one of activity, chaotic, passion, impulsive, potentially good or bad; and Tamas being the quality of darkness, ignorance, destructive, lethargic, negative. Everything, all life forms and human beings, state Samkhya scholars, have these three gunas, but in different proportions. The interplay of these gunas defines the character of someone or something, of nature and determines the progress of life. Samkhya theorises a pluralism of souls (Jeevatmas) who possess consciousness, but denies the existence of Ishvara (God). Classical Samkhya is considered an atheist / non-theistic Hindu philosophy.
In Indian philosophy, Yoga is among other things, the name of one of the six āstika philosophical schools. The Yoga philosophical system is closely allied with the dualism premises of Samkhya school. The Yoga school accepts the Samkhya psychology and metaphysics, but is considered theistic because it accepts the concept of "personal god", unlike Samkhya. The epistemology of the Yoga school, like the Sāmkhya school, relies on three of six prāmaṇas as the means of gaining reliable knowledge: pratyakṣa (perception), anumāṇa (inference) and śabda (āptavacana, word/testimony of reliable sources).
The Yoga school builds on the Samkhya school theory that jñāna (knowledge) is a sufficient means to moksha. It suggests that systematic techniques/practice (personal experimentation) combined with Samkhya's approach to knowledge is the path to moksha. Yoga shares several central ideas with Advaita Vedanta, with the difference that Yoga is a form of experimental mysticism while Advaita Vedanta is a form of monistic personalism. Like Advaita Vedanta, the Yoga school of Hindu philosophy states that liberation/freedom in this life is achievable, and this occurs when an individual fully understands and realizes the equivalence of Atman (soul, self) and Brahman.
The Vaiśeṣika philosophy is a naturalist school; it is a form of atomism in natural philosophy. It postulated that all objects in the physical universe are reducible to paramāṇu (atoms), and one's experiences are derived from the interplay of substance (a function of atoms, their number and their spatial arrangements), quality, activity, commonness, particularity and inherence. Knowledge and liberation was achievable by complete understanding of the world of experience, according to Vaiśeṣika school . The Vaiśeṣika darśana is credited to Kaṇāda Kaśyapa from the second half of the first millennium BCE. The foundational text, the Vaiśeṣika Sūtra, opens as follows,
Vaiśeṣika metaphysical premises are founded on a form of atomism, that the reality is composed of four substances (earth, water, air, fire). Each of these four are of two types: atomic (paramāṇu) and composite. An atom is, according to Vaiśeṣika scholars, that which is indestructible (anitya), indivisible, and has a special kind of dimension, called “small” (aṇu). A composite, in this philosophy, is defined to be anything which is divisible into atoms. Whatever human beings perceive is composite, while atoms are invisible. The Vaiśeṣikas stated that size, form, truths and everything that human beings experience as a whole is a function of atoms, their number and their spatial arrangements, their guṇa (quality), karma (activity), sāmānya (commonness), viśeṣa (particularity) and amavāya (inherence, inseparable connectedness of everything).
In its metaphysics, Nyāya school is closer to the Vaiśeṣika school than others. It holds that human suffering results from mistakes/defects produced by activity under wrong knowledge (notions and ignorance). Moksha (liberation), it states, is gained through right knowledge. This premise led Nyāya to concern itself with epistemology, that is the reliable means to gain correct knowledge and to remove wrong notions. False knowledge is not merely ignorance to Naiyayikas, it includes delusion. Correct knowledge is discovering and overcoming one's delusions, and understanding true nature of soul, self and reality. The Nyāya Sūtras begin:
The Mīmāṃsā school has several subschools defined by epistemology. The Prābhākara subschool of Mīmāṃsā considered five epistemically reliable means to gaining knowledge: pratyakṣa (perception), anumāṇa (inference), upamāṇa (comparison and analogy), arthāpatti (postulation, derivation from circumstances), and śabda (word, testimony of past or present reliable experts). The Kumārila Bhaṭṭa sub-school of Mīmāṃsā added sixth to its canon of reliable epistemology - anupalabdi (non-perception, negative/cognitive proof).
The metaphysics in Mīmāṃsā school consists of both atheistic and theistic doctrines and the school showed little interest in systematic examination of the existence of God. Rather, it held that the soul is eternal omnipresent, inherently active spiritual essence, then focussed on the epistemology and metaphysics of dharma. To them, dharma meant rituals and duties, not devas (gods), because devas existed only in name. The Mīmāṃsākas held that the Vedas are "eternal authorless infallible", that Vedic vidhi (injunctions) and mantras in rituals are prescriptive karya (actions), and the rituals are of primary importance and merit. They considered the Upanishads and other self-knowledge, spirituality-related texts to be of secondary importance, a philosophical view that the Vedanta school disagreed with.
Mīmāṃsā gave rise to the study of philology and the philosophy of language. While their deep analysis of language and linguistics influenced other schools, their views were not shared by others. Mīmāṃsākas considered the purpose and power of language was to clearly prescribe the proper, correct and right. In contrast, Vedantins extended the scope and value of language as a tool to also describe, develop and derive. Mīmāṃsākas considered orderly, law-driven, procedural life as central purpose and noblest necessity of dharma and society, and divine (theistic) sustenance means to that end. The Mimamsa school was influential and foundational to the Vedanta school, with the difference that Mīmāṃsā school developed and emphasized karmakāṇḍa (that part of the śruti which relates to ceremonial acts and sacrificial rites, the early parts of the Vedas), while the Vedanta school developed and emphasized jñānakāṇḍa (that portion of the Vedas which relates to knowledge of monism, the latter parts of the Vedas).
The Vedānta school built upon the teachings of the Upanishads and Brahma Sutras from the first millennium BCE and is the most developed and well-known of the Hindu schools. The epistemology of the Vedantins included, depending on the sub-school, five or six methods as proper and reliable means of gaining any form of knowledge: pratyakṣa (perception), anumāṇa (inference), upamāṇa (comparison and analogy), arthāpatti (postulation, derivation from circumstances), anupalabdi (non-perception, negative/cognitive proof) and śabda (word, testimony of past or present reliable experts). Each of these have been further categorized in terms of conditionality, completeness, confidence and possibility of error, by each sub-school of Vedanta.
The emergence of Vedanta school represented a period when a more knowledge-centered understanding began to emerge. These focussed on jnana (knowledge) driven aspects of the Vedic religion and the Upanishads. This included metaphysical concepts such as ātman and Brahman, and emphasized meditation, self-discipline, self-knowledge and abstract spirituality, rather than ritualism. The Upanishads were variously interpreted by ancient and medieval era Vedanta scholars. Consequently, the Vedanta separated into many sub-schools, ranging from theistic dualism to non-theistic monism, each interpreting the texts in its own way and producing its own series of sub-commentaries.
Advaita literally means "not two, sole, unity". It is a sub-school of Vedanta, and asserts spiritual and universal non-dualism. Its metaphysics is a form of absolute monism, that is all ultimate reality is interconnected oneness. This is the oldest and most widely acknowledged Vedantic school. The foundational texts of this school are the Brahma Sutras and the early Upanishads from the 1st millennium BCE. Its first great consolidator was the 8th century scholar Adi Shankara, who continued the line of thought of the Upanishadic teachers, and that of his teacher's teacher Gaudapada. He wrote extensive commentaries on the major Vedantic scriptures and is celebrated as one of the major Hindu philosophers from whose doctrines the main currents of modern Indian thought are derived.
According to this school of Vedanta, all reality is Brahman, and there exists nothing whatsoever which is not Brahman. Its metaphysics includes the concept of māyā and ātman. Māyā connotes "that which exists, but is constantly changing and thus is spiritually unreal". The empirical reality is considered as always changing and therefore "transitory, incomplete, misleading and not what it appears to be". The concept of ātman is of soul, self within each person, each living being. Advaita Vedantins assert that ātman is same as Brahman, and this Brahman is within each human being and all life, all living beings are spiritually interconnected, and there is oneness in all of existence. They hold that dualities and misunderstanding of māyā as the spiritual reality that matters is caused by ignorance, and are the cause of sorrow, suffering. Jīvanmukti (liberation during life) can be achieved through Self-knowledge, the understanding that ātman within is same as ātman in another person and all of Brahman – the eternal, unchanging, entirety of cosmic principles and true reality.
Ramanuja (c. 1037–1137) was the foremost proponent of the philosophy of Viśiṣṭādvaita or qualified non-dualism. Viśiṣṭādvaita advocated the concept of a Supreme Being with essential qualities or attributes. Viśiṣṭādvaitins argued against the Advaitin conception of Brahman as an impersonal empty oneness. They saw Brahman as an eternal oneness, but also as the source of all creation, which was omnipresent and actively involved in existence. To them the sense of subject-object perception was illusory and a sign of ignorance. However, the individual's sense of self was not a complete illusion since it was derived from the universal beingness that is Brahman. Ramanuja saw Vishnu as a personification of Brahman.
Dvaita Vedanta is a dualistic interpretation of the Vedas, espouses dualism by theorizing the existence of two separate realities. The first and the only independent reality, states the Dvaita school, is that of Vishnu or Brahman. Vishnu is the supreme Self, in a manner similar to monotheistic God in other major religions. The distinguishing factor of Dvaita philosophy, as opposed to monistic Advaita Vedanta, is that God takes on a personal role and is seen as a real eternal entity that governs and controls the universe. Like Vishishtadvaita Vedanta subschool, Dvaita philosophy also embraced Vaishnavism, with the metaphysical concept of Brahman in the Vedas identified with Vishnu and the one and only Supreme Being. However, unlike Vishishtadvaita which envisions ultimate qualified nondualism, the dualism of Dvaita was permanent.
Dvaitādvaita was proposed by Nimbarka, a 13th-century Vaishnava Philosopher from the Andhra region. According to this philosophy there are three categories of existence: Brahman, soul, and matter. Soul and matter are different from Brahman in that they have attributes and capacities different from Brahman. Brahman exists independently, while soul and matter are dependent. Thus soul and matter have an existence that is separate yet dependent. Further, Brahman is a controller, the soul is the enjoyer, and matter the thing enjoyed. Also, the highest object of worship is Krishna and his consort Radha, attended by thousands of gopis; of the Vrindavan; and devotion consists in self-surrender.
Early history of Shaivism is difficult to determine. However, the Śvetāśvatara Upanishad (400 – 200 BCE) is considered to be the earliest textual exposition of a systematic philosophy of Shaivism. Shaivism is represented by various philosophical schools, including non-dualist (abheda), dualist (bheda), and non-dualist-with-dualist (bhedābheda) perspectives. Vidyaranya in his works mentions three major schools of Shaiva thought— Pashupata Shaivism, Shaiva Siddhanta and Pratyabhijña (Kashmir Shaivism).
Pāśupata Shaivism (Pāśupata, "of Paśupati") is the oldest of the major Shaiva schools. The philosophy of Pashupata sect was systematized by Lakulish in the 2nd century CE. Paśu in Paśupati refers to the effect (or created world), the word designates that which is dependent on something ulterior. Whereas, Pati means the cause (or principium), the word designates the Lord, who is the cause of the universe, the pati, or the ruler. Pashupatas disapproved of Vaishnava theology, known for its doctrine servitude of souls to the Supreme Being, on the grounds that dependence upon anything could not be the means of cessation of pain and other desired ends. They recognised that those depending upon another and longing for independence will not be emancipated because they still depend upon something other than themselves. According to Pāśupatas, soul possesses the attributes of the Supreme Deity when it becomes liberated from the 'germ of every pain'.
Pāśupatas divided the created world into the insentient and the sentient. The insentient was the unconscious and thus dependent on the sentient or conscious. The insentient was further divided into effects and causes. The effects were of ten kinds, the earth, four elements and their qualities, colour etc. The causes were of thirteen kinds, the five organs of cognition, the five organs of action, the three internal organs, intellect, the ego principle and the cognising principle. These insentient causes were held responsible for the illusive identification of Self with non-Self. Salvation in Pāśupata involved the union of the soul with God through the intellect.
Even though, both Kashmir Shaivism and Advaita Vedanta are non-dual philosophies which give primacy to Universal Consciousness (Chit or Brahman), in Kashmir Shavisim, as opposed to Advaita, all things are a manifestation of this Consciousness. This implies that from the point of view of Kashmir Shavisim, the phenomenal world (Śakti) is real, and it exists and has its being in Consciousness (Chit). Whereas, Advaita holds that Brahman is inactive (niṣkriya) and the phenomenal world is an illusion (māyā). The objective of human life, according to Kashmir Shaivism, is to merge in Shiva or Universal Consciousness, or to realize one's already existing identity with Shiva, by means of wisdom, yoga and grace.
Recent developments in LEDs permit them to be used in environmental and task lighting. LEDs have many advantages over incandescent light sources including lower energy consumption, longer lifetime, improved physical robustness, smaller size, and faster switching. Light-emitting diodes are now used in applications as diverse as aviation lighting, automotive headlamps, advertising, general lighting, traffic signals, camera flashes and lighted wallpaper. As of 2015[update], LEDs powerful enough for room lighting remain somewhat more expensive, and require more precise current and heat management, than compact fluorescent lamp sources of comparable output.
Electroluminescence as a phenomenon was discovered in 1907 by the British experimenter H. J. Round of Marconi Labs, using a crystal of silicon carbide and a cat's-whisker detector. Soviet inventor Oleg Losev reported creation of the first LED in 1927. His research was distributed in Soviet, German and British scientific journals, but no practical use was made of the discovery for several decades. Kurt Lehovec, Carl Accardo and Edward Jamgochian, explained these first light-emitting diodes in 1951 using an apparatus employing SiC crystals with a current source of battery or pulse generator and with a comparison to a variant, pure, crystal in 1953.
In 1957, Braunstein further demonstrated that the rudimentary devices could be used for non-radio communication across a short distance. As noted by Kroemer Braunstein".. had set up a simple optical communications link: Music emerging from a record player was used via suitable electronics to modulate the forward current of a GaAs diode. The emitted light was detected by a PbS diode some distance away. This signal was fed into an audio amplifier, and played back by a loudspeaker. Intercepting the beam stopped the music. We had a great deal of fun playing with this setup." This setup presaged the use of LEDs for optical communication applications.
In September 1961, while working at Texas Instruments in Dallas, Texas, James R. Biard and Gary Pittman discovered near-infrared (900 nm) light emission from a tunnel diode they had constructed on a GaAs substrate. By October 1961, they had demonstrated efficient light emission and signal coupling between a GaAs p-n junction light emitter and an electrically-isolated semiconductor photodetector. On August 8, 1962, Biard and Pittman filed a patent titled "Semiconductor Radiant Diode" based on their findings, which described a zinc diffused p–n junction LED with a spaced cathode contact to allow for efficient emission of infrared light under forward bias. After establishing the priority of their work based on engineering notebooks predating submissions from G.E. Labs, RCA Research Labs, IBM Research Labs, Bell Labs, and Lincoln Lab at MIT, the U.S. patent office issued the two inventors the patent for the GaAs infrared (IR) light-emitting diode (U.S. Patent US3293513), the first practical LED. Immediately after filing the patent, Texas Instruments (TI) began a project to manufacture infrared diodes. In October 1962, TI announced the first LED commercial product (the SNX-100), which employed a pure GaAs crystal to emit a 890 nm light output. In October 1963, TI announced the first commercial hemispherical LED, the SNX-110.
The first visible-spectrum (red) LED was developed in 1962 by Nick Holonyak, Jr., while working at General Electric Company. Holonyak first reported his LED in the journal Applied Physics Letters on the December 1, 1962. M. George Craford, a former graduate student of Holonyak, invented the first yellow LED and improved the brightness of red and red-orange LEDs by a factor of ten in 1972. In 1976, T. P. Pearsall created the first high-brightness, high-efficiency LEDs for optical fiber telecommunications by inventing new semiconductor materials specifically adapted to optical fiber transmission wavelengths.
The first commercial LEDs were commonly used as replacements for incandescent and neon indicator lamps, and in seven-segment displays, first in expensive equipment such as laboratory and electronics test equipment, then later in such appliances as TVs, radios, telephones, calculators, as well as watches (see list of signal uses). Until 1968, visible and infrared LEDs were extremely costly, in the order of US$200 per unit, and so had little practical use. The Monsanto Company was the first organization to mass-produce visible LEDs, using gallium arsenide phosphide (GaAsP) in 1968 to produce red LEDs suitable for indicators. Hewlett Packard (HP) introduced LEDs in 1968, initially using GaAsP supplied by Monsanto. These red LEDs were bright enough only for use as indicators, as the light output was not enough to illuminate an area. Readouts in calculators were so small that plastic lenses were built over each digit to make them legible. Later, other colors became widely available and appeared in appliances and equipment. In the 1970s commercially successful LED devices at less than five cents each were produced by Fairchild Optoelectronics. These devices employed compound semiconductor chips fabricated with the planar process invented by Dr. Jean Hoerni at Fairchild Semiconductor. The combination of planar processing for chip fabrication and innovative packaging methods enabled the team at Fairchild led by optoelectronics pioneer Thomas Brandt to achieve the needed cost reductions. These methods continue to be used by LED producers.
The first high-brightness blue LED was demonstrated by Shuji Nakamura of Nichia Corporation in 1994 and was based on InGaN. In parallel, Isamu Akasaki and Hiroshi Amano in Nagoya were working on developing the important GaN nucleation on sapphire substrates and the demonstration of p-type doping of GaN. Nakamura, Akasaki and Amano were awarded the 2014 Nobel prize in physics for their work. In 1995, Alberto Barbieri at the Cardiff University Laboratory (GB) investigated the efficiency and reliability of high-brightness LEDs and demonstrated a "transparent contact" LED using indium tin oxide (ITO) on (AlGaInP/GaAs).
The attainment of high efficiency in blue LEDs was quickly followed by the development of the first white LED. In this device a Y
3Al
5O
12:Ce (known as "YAG") phosphor coating on the emitter absorbs some of the blue emission and produces yellow light through fluorescence. The combination of that yellow with remaining blue light appears white to the eye. However using different phosphors (fluorescent materials) it also became possible to instead produce green and red light through fluorescence. The resulting mixture of red, green and blue is not only perceived by humans as white light but is superior for illumination in terms of color rendering, whereas one cannot appreciate the color of red or green objects illuminated only by the yellow (and remaining blue) wavelengths from the YAG phosphor.
A P-N junction can convert absorbed light energy into a proportional electric current. The same process is reversed here (i.e. the P-N junction emits light when electrical energy is applied to it). This phenomenon is generally called electroluminescence, which can be defined as the emission of light from a semi-conductor under the influence of an electric field. The charge carriers recombine in a forward-biased P-N junction as the electrons cross from the N-region and recombine with the holes existing in the P-region. Free electrons are in the conduction band of energy levels, while holes are in the valence energy band. Thus the energy level of the holes will be lesser than the energy levels of the electrons. Some portion of the energy must be dissipated in order to recombine the electrons and the holes. This energy is emitted in the form of heat and light.
In September 2003, a new type of blue LED was demonstrated by Cree that consumes 24 mW at 20 milliamperes (mA). This produced a commercially packaged white light giving 65 lm/W at 20 mA, becoming the brightest white LED commercially available at the time, and more than four times as efficient as standard incandescents. In 2006, they demonstrated a prototype with a record white LED luminous efficacy of 131 lm/W at 20 mA. Nichia Corporation has developed a white LED with luminous efficacy of 150 lm/W at a forward current of 20 mA. Cree's XLamp XM-L LEDs, commercially available in 2011, produce 100 lm/W at their full power of 10 W, and up to 160 lm/W at around 2 W input power. In 2012, Cree announced a white LED giving 254 lm/W, and 303 lm/W in March 2014. Practical general lighting needs high-power LEDs, of one watt or more. Typical operating currents for such devices begin at 350 mA.
The most common symptom of LED (and diode laser) failure is the gradual lowering of light output and loss of efficiency. Sudden failures, although rare, can also occur. Early red LEDs were notable for their short service life. With the development of high-power LEDs the devices are subjected to higher junction temperatures and higher current densities than traditional devices. This causes stress on the material and may cause early light-output degradation. To quantitatively classify useful lifetime in a standardized manner it has been suggested to use L70 or L50, which are the runtimes (typically given in thousands of hours) at which a given LED reaches 70% and 50% of initial light output, respectively.
Since LED efficacy is inversely proportional to operating temperature, LED technology is well suited for supermarket freezer lighting. Because LEDs produce less waste heat than incandescent lamps, their use in freezers can save on refrigeration costs as well. However, they may be more susceptible to frost and snow buildup than incandescent lamps, so some LED lighting systems have been designed with an added heating circuit. Additionally, research has developed heat sink technologies that will transfer heat produced within the junction to appropriate areas of the light fixture.
The first blue-violet LED using magnesium-doped gallium nitride was made at Stanford University in 1972 by Herb Maruska and Wally Rhines, doctoral students in materials science and engineering. At the time Maruska was on leave from RCA Laboratories, where he collaborated with Jacques Pankove on related work. In 1971, the year after Maruska left for Stanford, his RCA colleagues Pankove and Ed Miller demonstrated the first blue electroluminescence from zinc-doped gallium nitride, though the subsequent device Pankove and Miller built, the first actual gallium nitride light-emitting diode, emitted green light. In 1974 the U.S. Patent Office awarded Maruska, Rhines and Stanford professor David Stevenson a patent for their work in 1972 (U.S. Patent US3819974 A) and today magnesium-doping of gallium nitride continues to be the basis for all commercial blue LEDs and laser diodes. These devices built in the early 1970s had too little light output to be of practical use and research into gallium nitride devices slowed. In August 1989, Cree introduced the first commercially available blue LED based on the indirect bandgap semiconductor, silicon carbide (SiC). SiC LEDs had very low efficiency, no more than about 0.03%, but did emit in the blue portion of the visible light spectrum.[citation needed]
In the late 1980s, key breakthroughs in GaN epitaxial growth and p-type doping ushered in the modern era of GaN-based optoelectronic devices. Building upon this foundation, Dr. Moustakas at Boston University patented a method for producing high-brightness blue LEDs using a new two-step process. Two years later, in 1993, high-brightness blue LEDs were demonstrated again by Shuji Nakamura of Nichia Corporation using a gallium nitride growth process similar to Dr. Moustakas's. Both Dr. Moustakas and Mr. Nakamura were issued separate patents, which confused the issue of who was the original inventor (partly because although Dr. Moustakas invented his first, Dr. Nakamura filed first).[citation needed] This new development revolutionized LED lighting, making high-power blue light sources practical, leading to the development of technologies like BlueRay, as well as allowing the bright high resolution screens of modern tablets and phones.[citation needed]
Nakamura was awarded the 2006 Millennium Technology Prize for his invention. Nakamura, Hiroshi Amano and Isamu Akasaki were awarded the Nobel Prize in Physics in 2014 for the invention of the blue LED. In 2015, a US court ruled that three companies (i.e. the litigants who had not previously settled out of court) that had licensed Mr. Nakamura's patents for production in the United States had infringed Dr. Moustakas's prior patent, and order them to pay licensing fees of not less than 13 million USD.
By the late 1990s, blue LEDs became widely available. They have an active region consisting of one or more InGaN quantum wells sandwiched between thicker layers of GaN, called cladding layers. By varying the relative In/Ga fraction in the InGaN quantum wells, the light emission can in theory be varied from violet to amber. Aluminium gallium nitride (AlGaN) of varying Al/Ga fraction can be used to manufacture the cladding and quantum well layers for ultraviolet LEDs, but these devices have not yet reached the level of efficiency and technological maturity of InGaN/GaN blue/green devices. If un-alloyed GaN is used in this case to form the active quantum well layers, the device will emit near-ultraviolet light with a peak wavelength centred around 365 nm. Green LEDs manufactured from the InGaN/GaN system are far more efficient and brighter than green LEDs produced with non-nitride material systems, but practical devices still exhibit efficiency too low for high-brightness applications.[citation needed]
With nitrides containing aluminium, most often AlGaN and AlGaInN, even shorter wavelengths are achievable. Ultraviolet LEDs in a range of wavelengths are becoming available on the market. Near-UV emitters at wavelengths around 375–395 nm are already cheap and often encountered, for example, as black light lamp replacements for inspection of anti-counterfeiting UV watermarks in some documents and paper currencies. Shorter-wavelength diodes, while substantially more expensive, are commercially available for wavelengths down to 240 nm. As the photosensitivity of microorganisms approximately matches the absorption spectrum of DNA, with a peak at about 260 nm, UV LED emitting at 250–270 nm are to be expected in prospective disinfection and sterilization devices. Recent research has shown that commercially available UVA LEDs (365 nm) are already effective disinfection and sterilization devices. UV-C wavelengths were obtained in laboratories using aluminium nitride (210 nm), boron nitride (215 nm) and diamond (235 nm).
White light can be formed by mixing differently colored lights; the most common method is to use red, green, and blue (RGB). Hence the method is called multi-color white LEDs (sometimes referred to as RGB LEDs). Because these need electronic circuits to control the blending and diffusion of different colors, and because the individual color LEDs typically have slightly different emission patterns (leading to variation of the color depending on direction) even if they are made as a single unit, these are seldom used to produce white lighting. Nonetheless, this method has many applications because of the flexibility of mixing different colors, and in principle, this mechanism also has higher quantum efficiency in producing white light.[citation needed]
There are several types of multi-color white LEDs: di-, tri-, and tetrachromatic white LEDs. Several key factors that play among these different methods, include color stability, color rendering capability, and luminous efficacy. Often, higher efficiency will mean lower color rendering, presenting a trade-off between the luminous efficacy and color rendering. For example, the dichromatic white LEDs have the best luminous efficacy (120 lm/W), but the lowest color rendering capability. However, although tetrachromatic white LEDs have excellent color rendering capability, they often have poor luminous efficacy. Trichromatic white LEDs are in between, having both good luminous efficacy (>70 lm/W) and fair color rendering capability.
Multi-color LEDs offer not merely another means to form white light but a new means to form light of different colors. Most perceivable colors can be formed by mixing different amounts of three primary colors. This allows precise dynamic color control. As more effort is devoted to investigating this method, multi-color LEDs should have profound influence on the fundamental method that we use to produce and control light color. However, before this type of LED can play a role on the market, several technical problems must be solved. These include that this type of LED's emission power decays exponentially with rising temperature, resulting in a substantial change in color stability. Such problems inhibit and may preclude industrial use. Thus, many new package designs aimed at solving this problem have been proposed and their results are now being reproduced by researchers and scientists.
This method involves coating LEDs of one color (mostly blue LEDs made of InGaN) with phosphors of different colors to form white light; the resultant LEDs are called phosphor-based or phosphor-converted white LEDs (pcLEDs). A fraction of the blue light undergoes the Stokes shift being transformed from shorter wavelengths to longer. Depending on the color of the original LED, phosphors of different colors can be employed. If several phosphor layers of distinct colors are applied, the emitted spectrum is broadened, effectively raising the color rendering index (CRI) value of a given LED.
Phosphor-based LED efficiency losses are due to the heat loss from the Stokes shift and also other phosphor-related degradation issues. Their luminous efficacies compared to normal LEDs depend on the spectral distribution of the resultant light output and the original wavelength of the LED itself. For example, the luminous efficacy of a typical YAG yellow phosphor based white LED ranges from 3 to 5 times the luminous efficacy of the original blue LED because of the human eye's greater sensitivity to yellow than to blue (as modeled in the luminosity function). Due to the simplicity of manufacturing the phosphor method is still the most popular method for making high-intensity white LEDs. The design and production of a light source or light fixture using a monochrome emitter with phosphor conversion is simpler and cheaper than a complex RGB system, and the majority of high-intensity white LEDs presently on the market are manufactured using phosphor light conversion.
Among the challenges being faced to improve the efficiency of LED-based white light sources is the development of more efficient phosphors. As of 2010, the most efficient yellow phosphor is still the YAG phosphor, with less than 10% Stoke shift loss. Losses attributable to internal optical losses due to re-absorption in the LED chip and in the LED packaging itself account typically for another 10% to 30% of efficiency loss. Currently, in the area of phosphor LED development, much effort is being spent on optimizing these devices to higher light output and higher operation temperatures. For instance, the efficiency can be raised by adapting better package design or by using a more suitable type of phosphor. Conformal coating process is frequently used to address the issue of varying phosphor thickness.
White LEDs can also be made by coating near-ultraviolet (NUV) LEDs with a mixture of high-efficiency europium-based phosphors that emit red and blue, plus copper and aluminium-doped zinc sulfide (ZnS:Cu, Al) that emits green. This is a method analogous to the way fluorescent lamps work. This method is less efficient than blue LEDs with YAG:Ce phosphor, as the Stokes shift is larger, so more energy is converted to heat, but yields light with better spectral characteristics, which render color better. Due to the higher radiative output of the ultraviolet LEDs than of the blue ones, both methods offer comparable brightness. A concern is that UV light may leak from a malfunctioning light source and cause harm to human eyes or skin.
A new style of wafers composed of gallium-nitride-on-silicon (GaN-on-Si) is being used to produce white LEDs using 200-mm silicon wafers. This avoids the typical costly sapphire substrate in relatively small 100- or 150-mm wafer sizes. The sapphire apparatus must be coupled with a mirror-like collector to reflect light that would otherwise be wasted. It is predicted that by 2020, 40% of all GaN LEDs will be made with GaN-on-Si. Manufacturing large sapphire material is difficult, while large silicon material is cheaper and more abundant. LED companies shifting from using sapphire to silicon should be a minimal investment.
Quantum dots (QD) are semiconductor nanocrystals that possess unique optical properties. Their emission color can be tuned from the visible throughout the infrared spectrum. This allows quantum dot LEDs to create almost any color on the CIE diagram. This provides more color options and better color rendering than white LEDs since the emission spectrum is much narrower, characteristic of quantum confined states. There are two types of schemes for QD excitation. One uses photo excitation with a primary light source LED (typically blue or UV LEDs are used). The other is direct electrical excitation first demonstrated by Alivisatos et al.
The structure of QD-LEDs used for the electrical-excitation scheme is similar to basic design of OLEDs. A layer of quantum dots is sandwiched between layers of electron-transporting and hole-transporting materials. An applied electric field causes electrons and holes to move into the quantum dot layer and recombine forming an exciton that excites a QD. This scheme is commonly studied for quantum dot display. The tunability of emission wavelengths and narrow bandwidth is also beneficial as excitation sources for fluorescence imaging. Fluorescence near-field scanning optical microscopy (NSOM) utilizing an integrated QD-LED has been demonstrated.
High-power LEDs (HP-LEDs) or high-output LEDs (HO-LEDs) can be driven at currents from hundreds of mA to more than an ampere, compared with the tens of mA for other LEDs. Some can emit over a thousand lumens. LED power densities up to 300 W/cm2 have been achieved. Since overheating is destructive, the HP-LEDs must be mounted on a heat sink to allow for heat dissipation. If the heat from a HP-LED is not removed, the device will fail in seconds. One HP-LED can often replace an incandescent bulb in a flashlight, or be set in an array to form a powerful LED lamp.
LEDs have been developed by Seoul Semiconductor that can operate on AC power without the need for a DC converter. For each half-cycle, part of the LED emits light and part is dark, and this is reversed during the next half-cycle. The efficacy of this type of HP-LED is typically 40 lm/W. A large number of LED elements in series may be able to operate directly from line voltage. In 2009, Seoul Semiconductor released a high DC voltage LED, named as 'Acrich MJT', capable of being driven from AC power with a simple controlling circuit. The low-power dissipation of these LEDs affords them more flexibility than the original AC LED design.
Alphanumeric LEDs are available in seven-segment, starburst and dot-matrix format. Seven-segment displays handle all numbers and a limited set of letters. Starburst displays can display all letters. Dot-matrix displays typically use 5x7 pixels per character. Seven-segment LED displays were in widespread use in the 1970s and 1980s, but rising use of liquid crystal displays, with their lower power needs and greater display flexibility, has reduced the popularity of numeric and alphanumeric LED displays.
Digital-RGB LEDs are RGB LEDs that contain their own "smart" control electronics. In addition to power and ground, these provide connections for data-in, data-out, and sometimes a clock or strobe signal. These are connected in a daisy chain, with the data in of the first LED sourced by a microprocessor, which can control the brightness and color of each LED independently of the others. They are used where a combination of maximum control and minimum visible electronics are needed such as strings for Christmas and LED matrices. Some even have refresh rates in the kHz range, allowing for basic video applications.
An LED filament consists of multiple LED dice connected in series on a common longitudinal substrate that form a thin rod reminiscent of a traditional incandescent filament. These are being used as a low cost decorative alternative for traditional light bulbs that are being phased out in many countries. The filaments require a rather high voltage to light to nominal brightness, allowing them to work efficiently and simply with mains voltages. Often a simple rectifier and capacitive current limiting are employed to create a low-cost replacement for a traditional light bulb without the complexity of creating a low voltage, high current converter which is required by single die LEDs. Usually they are packaged in a sealed enclosure with a shape similar to lamps they were designed to replace (e.g. a bulb), and filled with inert nitrogen or carbon dioxide gas to remove heat efficiently.
The current–voltage characteristic of an LED is similar to other diodes, in that the current is dependent exponentially on the voltage (see Shockley diode equation). This means that a small change in voltage can cause a large change in current. If the applied voltage exceeds the LED's forward voltage drop by a small amount, the current rating may be exceeded by a large amount, potentially damaging or destroying the LED. The typical solution is to use constant-current power supplies to keep the current below the LED's maximum current rating. Since most common power sources (batteries, mains) are constant-voltage sources, most LED fixtures must include a power converter, at least a current-limiting resistor. However, the high resistance of three-volt coin cells combined with the high differential resistance of nitride-based LEDs makes it possible to power such an LED from such a coin cell without an external resistor.
The vast majority of devices containing LEDs are "safe under all conditions of normal use", and so are classified as "Class 1 LED product"/"LED Klasse 1". At present, only a few LEDs—extremely bright LEDs that also have a tightly focused viewing angle of 8° or less—could, in theory, cause temporary blindness, and so are classified as "Class 2". The opinion of the French Agency for Food, Environmental and Occupational Health & Safety (ANSES) of 2010, on the health issues concerning LEDs, suggested banning public use of lamps which were in the moderate Risk Group 2, especially those with a high blue component in places frequented by children. In general, laser safety regulations—and the "Class 1", "Class 2", etc. system—also apply to LEDs.
While LEDs have the advantage over fluorescent lamps that they do not contain mercury, they may contain other hazardous metals such as lead and arsenic. Regarding the toxicity of LEDs when treated as waste, a study published in 2011 stated: "According to federal standards, LEDs are not hazardous except for low-intensity red LEDs, which leached Pb [lead] at levels exceeding regulatory limits (186 mg/L; regulatory limit: 5). However, according to California regulations, excessive levels of copper (up to 3892 mg/kg; limit: 2500), lead (up to 8103 mg/kg; limit: 1000), nickel (up to 4797 mg/kg; limit: 2000), or silver (up to 721 mg/kg; limit: 500) render all except low-intensity yellow LEDs hazardous."
One-color light is well suited for traffic lights and signals, exit signs, emergency vehicle lighting, ships' navigation lights or lanterns (chromacity and luminance standards being set under the Convention on the International Regulations for Preventing Collisions at Sea 1972, Annex I and the CIE) and LED-based Christmas lights. In cold climates, LED traffic lights may remain snow-covered. Red or yellow LEDs are used in indicator and alphanumeric displays in environments where night vision must be retained: aircraft cockpits, submarine and ship bridges, astronomy observatories, and in the field, e.g. night time animal watching and military field use.
Because of their long life, fast switching times, and their ability to be seen in broad daylight due to their high output and focus, LEDs have been used in brake lights for cars' high-mounted brake lights, trucks, and buses, and in turn signals for some time, but many vehicles now use LEDs for their rear light clusters. The use in brakes improves safety, due to a great reduction in the time needed to light fully, or faster rise time, up to 0.5 second faster[citation needed] than an incandescent bulb. This gives drivers behind more time to react. In a dual intensity circuit (rear markers and brakes) if the LEDs are not pulsed at a fast enough frequency, they can create a phantom array, where ghost images of the LED will appear if the eyes quickly scan across the array. White LED headlamps are starting to be used. Using LEDs has styling advantages because LEDs can form much thinner lights than incandescent lamps with parabolic reflectors.
Assistive listening devices in many theaters and similar spaces use arrays of infrared LEDs to send sound to listeners' receivers. Light-emitting diodes (as well as semiconductor lasers) are used to send data over many types of fiber optic cable, from digital audio over TOSLINK cables to the very high bandwidth fiber links that form the Internet backbone. For some time, computers were commonly equipped with IrDA interfaces, which allowed them to send and receive data to nearby machines via infrared.
In the US, one kilowatt-hour (3.6 MJ) of electricity currently causes an average 1.34 pounds (610 g) of CO
2 emission. Assuming the average light bulb is on for 10 hours a day, a 40-watt bulb will cause 196 pounds (89 kg) of CO
2 emission per year. The 6-watt LED equivalent will only cause 30 pounds (14 kg) of CO
2 over the same time span. A building’s carbon footprint from lighting can therefore be reduced by 85% by exchanging all incandescent bulbs for new LEDs if a building previously used only incandescent bulbs.
Machine vision systems often require bright and homogeneous illumination, so features of interest are easier to process. LEDs are often used for this purpose, and this is likely to remain one of their major uses until the price drops low enough to make signaling and illumination uses more widespread. Barcode scanners are the most common example of machine vision, and many low cost products use red LEDs instead of lasers. Optical computer mice are an example of LEDs in machine vision, as it is used to provide an even light source on the surface for the miniature camera within the mouse. LEDs constitute a nearly ideal light source for machine vision systems for several reasons:
The light from LEDs can be modulated very quickly so they are used extensively in optical fiber and free space optics communications. This includes remote controls, such as for TVs, VCRs, and LED Computers, where infrared LEDs are often used. Opto-isolators use an LED combined with a photodiode or phototransistor to provide a signal path with electrical isolation between two circuits. This is especially useful in medical equipment where the signals from a low-voltage sensor circuit (usually battery-powered) in contact with a living organism must be electrically isolated from any possible electrical failure in a recording or monitoring device operating at potentially dangerous voltages. An optoisolator also allows information to be transferred between circuits not sharing a common ground potential.
Many sensor systems rely on light as the signal source. LEDs are often ideal as a light source due to the requirements of the sensors. LEDs are used as motion sensors, for example in optical computer mice. The Nintendo Wii's sensor bar uses infrared LEDs. Pulse oximeters use them for measuring oxygen saturation. Some flatbed scanners use arrays of RGB LEDs rather than the typical cold-cathode fluorescent lamp as the light source. Having independent control of three illuminated colors allows the scanner to calibrate itself for more accurate color balance, and there is no need for warm-up. Further, its sensors only need be monochromatic, since at any one time the page being scanned is only lit by one color of light. Since LEDs can also be used as photodiodes, they can be used for both photo emission and detection. This could be used, for example, in a touchscreen that registers reflected light from a finger or stylus. Many materials and biological systems are sensitive to, or dependent on, light. Grow lights use LEDs to increase photosynthesis in plants, and bacteria and viruses can be removed from water and other substances using UV LEDs for sterilization.
LEDs have also been used as a medium-quality voltage reference in electronic circuits. The forward voltage drop (e.g. about 1.7 V for a normal red LED) can be used instead of a Zener diode in low-voltage regulators. Red LEDs have the flattest I/V curve above the knee. Nitride-based LEDs have a fairly steep I/V curve and are useless for this purpose. Although LED forward voltage is far more current-dependent than a Zener diode, Zener diodes with breakdown voltages below 3 V are not widely available.
With an estimated population of 1,381,069 as of July 1, 2014, San Diego is the eighth-largest city in the United States and second-largest in California. It is part of the San Diego–Tijuana conurbation, the second-largest transborder agglomeration between the US and a bordering country after Detroit–Windsor, with a population of 4,922,723 people. San Diego is the birthplace of California and is known for its mild year-round climate, natural deep-water harbor, extensive beaches, long association with the United States Navy and recent emergence as a healthcare and biotechnology development center.
Historically home to the Kumeyaay people, San Diego was the first site visited by Europeans on what is now the West Coast of the United States. Upon landing in San Diego Bay in 1542, Juan Rodríguez Cabrillo claimed the entire area for Spain, forming the basis for the settlement of Alta California 200 years later. The Presidio and Mission San Diego de Alcalá, founded in 1769, formed the first European settlement in what is now California. In 1821, San Diego became part of the newly-independent Mexico, which reformed as the First Mexican Republic two years later. In 1850, it became part of the United States following the Mexican–American War and the admission of California to the union.
The first European to visit the region was Portuguese-born explorer Juan Rodríguez Cabrillo sailing under the flag of Castile. Sailing his flagship San Salvador from Navidad, New Spain, Cabrillo claimed the bay for the Spanish Empire in 1542, and named the site 'San Miguel'. In November 1602, Sebastián Vizcaíno was sent to map the California coast. Arriving on his flagship San Diego, Vizcaíno surveyed the harbor and what are now Mission Bay and Point Loma and named the area for the Catholic Saint Didacus, a Spaniard more commonly known as San Diego de Alcalá. On November 12, 1602, the first Christian religious service of record in Alta California was conducted by Friar Antonio de la Ascensión, a member of Vizcaíno's expedition, to celebrate the feast day of San Diego.
In May 1769, Gaspar de Portolà established the Fort Presidio of San Diego on a hill near the San Diego River. It was the first settlement by Europeans in what is now the state of California. In July of the same year, Mission San Diego de Alcalá was founded by Franciscan friars under Junípero Serra. By 1797, the mission boasted the largest native population in Alta California, with over 1,400 neophytes living in and around the mission proper. Mission San Diego was the southern anchor in California of the historic mission trail El Camino Real. Both the Presidio and the Mission are National Historic Landmarks.
In 1821, Mexico won its independence from Spain, and San Diego became part of the Mexican territory of Alta California. In 1822, Mexico began attempting to extend its authority over the coastal territory of Alta California. The fort on Presidio Hill was gradually abandoned, while the town of San Diego grew up on the level land below Presidio Hill. The Mission was secularized by the Mexican government in 1833, and most of the Mission lands were sold to wealthy Californio settlers. The 432 residents of the town petitioned the governor to form a pueblo, and Juan María Osuna was elected the first alcalde ("municipal magistrate"), defeating Pío Pico in the vote. (See, List of pre-statehood mayors of San Diego.) However, San Diego had been losing population throughout the 1830s and in 1838 the town lost its pueblo status because its size dropped to an estimated 100 to 150 residents. Beyond town Mexican land grants expanded the number of California ranchos that modestly added to the local economy.
In 1846, the United States went to war against Mexico and sent a naval and land expedition to conquer Alta California. At first they had an easy time of it capturing the major ports including San Diego, but the Californios in southern Alta California struck back. Following the successful revolt in Los Angeles, the American garrison at San Diego was driven out without firing a shot in early October 1846. Mexican partisans held San Diego for three weeks until October 24, 1846, when the Americans recaptured it. For the next several months the Americans were blockaded inside the pueblo. Skirmishes occurred daily and snipers shot into the town every night. The Californios drove cattle away from the pueblo hoping to starve the Americans and their Californio supporters out. On December 1 the Americans garrison learned that the dragoons of General Stephen W. Kearney were at Warner's Ranch. Commodore Robert F. Stockton sent a mounted force of fifty under Captain Archibald Gillespie to march north to meet him. Their joint command of 150 men, returning to San Diego, encountered about 93 Californios under Andrés Pico. In the ensuing Battle of San Pasqual, fought in the San Pasqual Valley which is now part of the city of San Diego, the Americans suffered their worst losses in the campaign. Subsequently a column led by Lieutenant Gray arrived from San Diego, rescuing Kearny's battered and blockaded command.
Stockton and Kearny went on to recover Los Angeles and force the capitulation of Alta California with the "Treaty of Cahuenga" on January 13, 1847. As a result of the Mexican–American War of 1846–48, the territory of Alta California, including San Diego, was ceded to the United States by Mexico, under the terms of the Treaty of Guadalupe Hidalgo in 1848. The Mexican negotiators of that treaty tried to retain San Diego as part of Mexico, but the Americans insisted that San Diego was "for every commercial purpose of nearly equal importance to us with that of San Francisco," and the Mexican-American border was eventually established to be one league south of the southernmost point of San Diego Bay, so as to include the entire bay within the United States.
The state of California was admitted to the United States in 1850. That same year San Diego was designated the seat of the newly established San Diego County and was incorporated as a city. Joshua H. Bean, the last alcalde of San Diego, was elected the first mayor. Two years later the city was bankrupt; the California legislature revoked the city's charter and placed it under control of a board of trustees, where it remained until 1889. A city charter was re-established in 1889 and today's city charter was adopted in 1931.
The original town of San Diego was located at the foot of Presidio Hill, in the area which is now Old Town San Diego State Historic Park. The location was not ideal, being several miles away from navigable water. In 1850, William Heath Davis promoted a new development by the Bay shore called "New San Diego", several miles south of the original settlement; however, for several decades the new development consisted only a few houses, a pier and an Army depot. In the late 1860s, Alonzo Horton promoted a move to the bayside area, which he called "New Town" and which became Downtown San Diego. Horton promoted the area heavily, and people and businesses began to relocate to New Town because of its location on San Diego Bay convenient to shipping. New Town soon eclipsed the original settlement, known to this day as Old Town, and became the economic and governmental heart of the city. Still, San Diego remained a relative backwater town until the arrival of a railroad connection in 1878.
In the early part of the 20th century, San Diego hosted two World's Fairs: the Panama-California Exposition in 1915 and the California Pacific International Exposition in 1935. Both expositions were held in Balboa Park, and many of the Spanish/Baroque-style buildings that were built for those expositions remain to this day as central features of the park. The buildings were intended to be temporary structures, but most remained in continuous use until they progressively fell into disrepair. Most were eventually rebuilt, using castings of the original façades to retain the architectural style. The menagerie of exotic animals featured at the 1915 exposition provided the basis for the San Diego Zoo. During the 1950s there was a citywide festival called Fiesta del Pacifico highlighting the area's Spanish and Mexican past. In the 2010s there was a proposal for a large-scale celebration of the 100th anniversary of Balboa Park, but the plans were abandoned when the organization tasked with putting on the celebration went out of business.
The southern portion of the Point Loma peninsula was set aside for military purposes as early as 1852. Over the next several decades the Army set up a series of coastal artillery batteries and named the area Fort Rosecrans. Significant U.S. Navy presence began in 1901 with the establishment of the Navy Coaling Station in Point Loma, and expanded greatly during the 1920s. By 1930, the city was host to Naval Base San Diego, Naval Training Center San Diego, San Diego Naval Hospital, Camp Matthews, and Camp Kearny (now Marine Corps Air Station Miramar). The city was also an early center for aviation: as early as World War I, San Diego was proclaiming itself "The Air Capital of the West". The city was home to important airplane developers and manufacturers like Ryan Airlines (later Ryan Aeronautical), founded in 1925, and Consolidated Aircraft (later Convair), founded in 1923. Charles A. Lindbergh's plane The Spirit of St. Louis was built in San Diego in 1927 by Ryan Airlines.
During World War II, San Diego became a major hub of military and defense activity, due to the presence of so many military installations and defense manufacturers. The city's population grew rapidly during and after World War II, more than doubling between 1930 (147,995) and 1950 (333,865). During the final months of the war, the Japanese had a plan to target multiple U.S. cities for biological attack, starting with San Diego. The plan was called "Operation Cherry Blossoms at Night" and called for kamikaze planes filled with fleas infected with plague (Yersinia pestis) to crash into civilian population centers in the city, hoping to spread plague in the city and effectively kill tens of thousands of civilians. The plan was scheduled to launch on September 22, 1945, but was not carried out because Japan surrendered five weeks earlier.
From the start of the 20th century through the 1970s, the American tuna fishing fleet and tuna canning industry were based in San Diego, "the tuna capital of the world". San Diego's first tuna cannery was founded in 1911, and by the mid-1930s the canneries employed more than 1,000 people. A large fishing fleet supported the canneries, mostly staffed by immigrant fishermen from Japan, and later from the Portuguese Azores and Italy whose influence is still felt in neighborhoods like Little Italy and Point Loma. Due to rising costs and foreign competition, the last of the canneries closed in the early 1980s.
The city lies on approximately 200 deep canyons and hills separating its mesas, creating small pockets of natural open space scattered throughout the city and giving it a hilly geography. Traditionally, San Diegans have built their homes and businesses on the mesas, while leaving the urban canyons relatively wild. Thus, the canyons give parts of the city a segmented feel, creating gaps between otherwise proximate neighborhoods and contributing to a low-density, car-centered environment. The San Diego River runs through the middle of San Diego from east to west, creating a river valley which serves to divide the city into northern and southern segments. The river used to flow into San Diego Bay and its fresh water was the focus of the earliest Spanish explorers.[citation needed] Several reservoirs and Mission Trails Regional Park also lie between and separate developed areas of the city.
Downtown San Diego is located on San Diego Bay. Balboa Park encompasses several mesas and canyons to the northeast, surrounded by older, dense urban communities including Hillcrest and North Park. To the east and southeast lie City Heights, the College Area, and Southeast San Diego. To the north lies Mission Valley and Interstate 8. The communities north of the valley and freeway, and south of Marine Corps Air Station Miramar, include Clairemont, Kearny Mesa, Tierrasanta, and Navajo. Stretching north from Miramar are the northern suburbs of Mira Mesa, Scripps Ranch, Rancho Peñasquitos, and Rancho Bernardo. The far northeast portion of the city encompasses Lake Hodges and the San Pasqual Valley, which holds an agricultural preserve. Carmel Valley and Del Mar Heights occupy the northwest corner of the city. To their south are Torrey Pines State Reserve and the business center of the Golden Triangle. Further south are the beach and coastal communities of La Jolla, Pacific Beach, Mission Beach, and Ocean Beach. Point Loma occupies the peninsula across San Diego Bay from downtown. The communities of South San Diego, such as San Ysidro and Otay Mesa, are located next to the Mexico–United States border, and are physically separated from the rest of the city by the cities of National City and Chula Vista. A narrow strip of land at the bottom of San Diego Bay connects these southern neighborhoods with the rest of the city.
The development of skyscrapers over 300 feet (91 m) in San Diego is attributed to the construction of the El Cortez Hotel in 1927, the tallest building in the city from 1927 to 1963. As time went on multiple buildings claimed the title of San Diego's tallest skyscraper, including the Union Bank of California Building and Symphony Towers. Currently the tallest building in San Diego is One America Plaza, standing 500 feet (150 m) tall, which was completed in 1991. The downtown skyline contains no super-talls, as a regulation put in place by the Federal Aviation Administration in the 1970s set a 500 feet (152 m) limit on the height of buildings due to the proximity of San Diego International Airport. An iconic description of the skyline includes its skyscrapers being compared to the tools of a toolbox.
San Diego is one of the top-ten best climates in the Farmers' Almanac and is one of the two best summer climates in America as scored by The Weather Channel. Under the Köppen–Geiger climate classification system, the San Diego area has been variously categorized as having either a semi-arid climate (BSh in the original classification and BSkn in modified Köppen classification) or a Mediterranean climate (Csa and Csb). San Diego's climate is characterized by warm, dry summers and mild winters with most of the annual precipitation falling between December and March. The city has a mild climate year-round, with an average of 201 days above 70 °F (21 °C) and low rainfall (9–13 inches [230–330 mm] annually). Dewpoints in the summer months range from 57.0 °F (13.9 °C) to 62.4 °F (16.9 °C).
The climate in San Diego, like most of Southern California, often varies significantly over short geographical distances resulting in microclimates. In San Diego, this is mostly because of the city's topography (the Bay, and the numerous hills, mountains, and canyons). Frequently, particularly during the "May gray/June gloom" period, a thick "marine layer" cloud cover will keep the air cool and damp within a few miles of the coast, but will yield to bright cloudless sunshine approximately 5–10 miles (8.0–16.1 km) inland. Sometimes the June gloom can last into July, causing cloudy skies over most of San Diego for the entire day. Even in the absence of June gloom, inland areas tend to experience much more significant temperature variations than coastal areas, where the ocean serves as a moderating influence. Thus, for example, downtown San Diego averages January lows of 50 °F (10 °C) and August highs of 78 °F (26 °C). The city of El Cajon, just 10 miles (16 km) inland from downtown San Diego, averages January lows of 42 °F (6 °C) and August highs of 88 °F (31 °C).
Rainfall along the coast averages about 10 inches (250 mm) of precipitation annually. The average (mean) rainfall is 10.65 inches (271 mm) and the median is 9.6 inches (240 mm). Most of the rainfall occurs during the cooler months. The months of December through March supply most of the rain, with February the only month averaging 2 inches (51 mm) or more of rain. The months of May through September tend to be almost completely dry. Though there are few wet days per month during the rainy period, rainfall can be heavy when it does fall. Rainfall is usually greater in the higher elevations of San Diego; some of the higher elevation areas of San Diego can receive 11–15 inches (280–380 mm) of rain a year. Variability of rainfall can be extreme: in the wettest years of 1883/1884 and 1940/1941 more than 24 inches (610 mm) fell in the city, whilst in the driest years as little as 3.2 inches (80 mm) has fallen for a full year. The wettest month on record has been December 1921 with 9.21 inches (234 mm).
Like most of southern California, the majority of San Diego's current area was originally occupied by chaparral, a plant community made up mostly of drought-resistant shrubs. The endangered Torrey pine has the bulk of its population in San Diego in a stretch of protected chaparral along the coast. The steep and varied topography and proximity to the ocean create a number of different habitats within the city limits, including tidal marsh and canyons. The chaparral and coastal sage scrub habitats in low elevations along the coast are prone to wildfire, and the rates of fire have increased in the 20th century, due primarily to fires starting near the borders of urban and wild areas.
San Diego County has one of the highest counts of animal and plant species that appear on the endangered species list among counties in the United States. Because of its diversity of habitat and its position on the Pacific Flyway, San Diego County has recorded the presence of 492 bird species, more than any other region in the country. San Diego always scores very high in the number of bird species observed in the annual Christmas Bird Count, sponsored by the Audubon Society, and it is known as one of the "birdiest" areas in the United States.
San Diego and its backcountry are subject to periodic wildfires. In October 2003, San Diego was the site of the Cedar Fire, which has been called the largest wildfire in California over the past century. The fire burned 280,000 acres (1,100 km2), killed 15 people, and destroyed more than 2,200 homes. In addition to damage caused by the fire, smoke resulted in a significant increase in emergency room visits due to asthma, respiratory problems, eye irritation, and smoke inhalation; the poor air quality caused San Diego County schools to close for a week. Wildfires four years later destroyed some areas, particularly within the communities of Rancho Bernardo, Rancho Santa Fe, and Ramona.
The city had a population of 1,307,402 according to the 2010 census, distributed over a land area of 372.1 square miles (963.7 km2). The urban area of San Diego extends beyond the administrative city limits and had a total population of 2,956,746, making it the third-largest urban area in the state, after that of the Los Angeles metropolitan area and San Francisco metropolitan area. They, along with the Riverside–San Bernardino, form those metropolitan areas in California larger than the San Diego metropolitan area, with a total population of 3,095,313 at the 2010 census.
As of the Census of 2010, there were 1,307,402 people living in the city of San Diego. That represents a population increase of just under 7% from the 1,223,400 people, 450,691 households, and 271,315 families reported in 2000. The estimated city population in 2009 was 1,306,300. The population density was 3,771.9 people per square mile (1,456.4/km2). The racial makeup of San Diego was 45.1% White, 6.7% African American, 0.6% Native American, 15.9% Asian (5.9% Filipino, 2.7% Chinese, 2.5% Vietnamese, 1.3% Indian, 1.0% Korean, 0.7% Japanese, 0.4% Laotian, 0.3% Cambodian, 0.1% Thai). 0.5% Pacific Islander (0.2% Guamanian, 0.1% Samoan, 0.1% Native Hawaiian), 12.3% from other races, and 5.1% from two or more races. The ethnic makeup of the city was 28.8% Hispanic or Latino (of any race); 24.9% of the total population were Mexican American, and 0.6% were Puerto Rican.
As of January 1, 2008 estimates by the San Diego Association of Governments revealed that the household median income for San Diego rose to $66,715, up from $45,733, and that the city population rose to 1,336,865, up 9.3% from 2000. The population was 45.3% non-Hispanic whites, down from 78.9% in 1970, 27.7% Hispanics, 15.6% Asians/Pacific Islanders, 7.1% blacks, 0.4% American Indians, and 3.9% from other races. Median age of Hispanics was 27.5 years, compared to 35.1 years overall and 41.6 years among non-Hispanic whites; Hispanics were the largest group in all ages under 18, and non-Hispanic whites constituted 63.1% of population 55 and older.
The U.S. Census Bureau reported that in 2000, 24.0% of San Diego residents were under 18, and 10.5% were 65 and over. As of 2011[update] the median age was 35.6; more than a quarter of residents were under age 20 and 11% were over age 65. Millennials (ages 18 through 34) constitute 27.1% of San Diego's population, the second-highest percentage in a major U.S. city. The San Diego County regional planning agency, SANDAG, provides tables and graphs breaking down the city population into 5-year age groups.
In 2000, the median income for a household in the city was $45,733, and the median income for a family was $53,060. Males had a median income of $36,984 versus $31,076 for females. The per capita income for the city was $23,609. According to Forbes in 2005, San Diego was the fifth wealthiest U.S. city but about 10.6% of families and 14.6% of the population were below the poverty line, including 20.0% of those under age 18 and 7.6% of those age 65 or over. Nonetheless, San Diego was rated the fifth-best place to live in the United States in 2006 by Money magazine.
Tourism is a major industry owing to the city's climate, its beaches, and numerous tourist attractions such as Balboa Park, Belmont amusement park, San Diego Zoo, San Diego Zoo Safari Park, and SeaWorld San Diego. San Diego's Spanish and Mexican heritage is reflected in the many historic sites across the city, such as Mission San Diego de Alcala and Old Town San Diego State Historic Park. Also, the local craft brewing industry attracts an increasing number of visitors for "beer tours" and the annual San Diego Beer Week in November; San Diego has been called "America's Craft Beer Capital."
The city shares a 15-mile (24 km) border with Mexico that includes two border crossings. San Diego hosts the busiest international border crossing in the world, in the San Ysidro neighborhood at the San Ysidro Port of Entry. A second, primarily commercial border crossing operates in the Otay Mesa area; it is the largest commercial crossing on the California-Baja California border and handles the third-highest volume of trucks and dollar value of trade among all United States-Mexico land crossings.
San Diego hosts several major producers of wireless cellular technology. Qualcomm was founded and is headquartered in San Diego, and is one of the largest private-sector employers in San Diego. Other wireless industry manufacturers headquartered here include Nokia, LG Electronics, Kyocera International., Cricket Communications and Novatel Wireless. The largest software company in San Diego is security software company Websense Inc. San Diego also has the U.S. headquarters for the Slovakian security company ESET. San Diego has been designated as an iHub Innovation Center for collaboration potentially between wireless and life sciences.
The presence of the University of California, San Diego and other research institutions has helped to fuel biotechnology growth. In 2013, San Diego has the second-largest biotech cluster in the United States, below the Boston area and above the San Francisco Bay Area. There are more than 400 biotechnology companies in the area. In particular, the La Jolla and nearby Sorrento Valley areas are home to offices and research facilities for numerous biotechnology companies. Major biotechnology companies like Illumina and Neurocrine Biosciences are headquartered in San Diego, while many biotech and pharmaceutical companies have offices or research facilities in San Diego. San Diego is also home to more than 140 contract research organizations (CROs) that provide a variety of contract services for pharmaceutical and biotechnology companies.
Many popular museums, such as the San Diego Museum of Art, the San Diego Natural History Museum, the San Diego Museum of Man, the Museum of Photographic Arts, and the San Diego Air & Space Museum are located in Balboa Park, which is also the location of the San Diego Zoo. The Museum of Contemporary Art San Diego (MCASD) is located in La Jolla and has a branch located at the Santa Fe Depot downtown. The downtown branch consists of two building on two opposite streets. The Columbia district downtown is home to historic ship exhibits belonging to the San Diego Maritime Museum, headlined by the Star of India, as well as the unrelated San Diego Aircraft Carrier Museum featuring the USS Midway aircraft carrier.
The San Diego Symphony at Symphony Towers performs on a regular basis and is directed by Jahja Ling. The San Diego Opera at Civic Center Plaza, directed by Ian Campbell, was ranked by Opera America as one of the top 10 opera companies in the United States. Old Globe Theatre at Balboa Park produces about 15 plays and musicals annually. The La Jolla Playhouse at UCSD is directed by Christopher Ashley. Both the Old Globe Theatre and the La Jolla Playhouse have produced the world premieres of plays and musicals that have gone on to win Tony Awards or nominations on Broadway. The Joan B. Kroc Theatre at Kroc Center's Performing Arts Center is a 600-seat state-of-the-art theatre that hosts music, dance, and theatre performances. The San Diego Repertory Theatre at the Lyceum Theatres in Horton Plaza produces a variety of plays and musicals. Hundreds of movies and a dozen TV shows have been filmed in San Diego, a tradition going back as far as 1898.
The San Diego Surf of the American Basketball Association is located in the city. The annual Farmers Insurance Open golf tournament (formerly the Buick Invitational) on the PGA Tour occurs at Torrey Pines Golf Course. This course was also the site of the 2008 U.S. Open Golf Championship. The San Diego Yacht Club hosted the America's Cup yacht races three times during the period 1988 to 1995. The amateur beach sport Over-the-line was invented in San Diego, and the annual world Over-the-line championships are held at Mission Bay every year.
The city is governed by a mayor and a 9-member city council. In 2006, the city's form of government changed from a council–manager government to a strong mayor government. The change was brought about by a citywide vote in 2004. The mayor is in effect the chief executive officer of the city, while the council is the legislative body. The City of San Diego is responsible for police, public safety, streets, water and sewer service, planning and zoning, and similar services within its borders. San Diego is a sanctuary city, however, San Diego County is a participant of the Secure Communities program. As of 2011[update], the city had one employee for every 137 residents, with a payroll greater than $733 million.
The members of the city council are each elected from single member districts within the city. The mayor and city attorney are elected directly by the voters of the entire city. The mayor, city attorney, and council members are elected to four-year terms, with a two-term limit. Elections are held on a non-partisan basis per California state law; nevertheless, most officeholders do identify themselves as either Democrats or Republicans. In 2007, registered Democrats outnumbered Republicans by about 7 to 6 in the city, and Democrats currently (as of 2015[update]) hold a 5-4 majority in the city council. The current mayor, Kevin Faulconer, is a Republican.
In 2005 two city council members, Ralph Inzunza and Deputy Mayor Michael Zucchet – who briefly took over as acting mayor when Murphy resigned – were convicted of extortion, wire fraud, and conspiracy to commit wire fraud for taking campaign contributions from a strip club owner and his associates, allegedly in exchange for trying to repeal the city's "no touch" laws at strip clubs. Both subsequently resigned. Inzunza was sentenced to 21 months in prison. In 2009, a judge acquitted Zucchet on seven out of the nine counts against him, and granted his petition for a new trial on the other two charges; the remaining charges were eventually dropped.
In July 2013, three former supporters of Mayor Bob Filner asked him to resign because of allegations of repeated sexual harassment. Over the ensuing six weeks, 18 women came forward to publicly claim that Filner had sexually harassed them, and multiple individuals and groups called for him to resign. On August 19 Filner and city representatives entered a mediation process, as a result of which Filner agreed to resign, effective August 30, 2013, while the city agreed to limit his legal and financial exposure. Filner subsequently pleaded guilty to one felony count of false imprisonment and two misdemeanor battery charges, and was sentenced to house arrest and probation.
San Diego was ranked as the 20th-safest city in America in 2013 by Business Insider. According to Forbes magazine, San Diego was the ninth-safest city in the top 10 list of safest cities in the U.S. in 2010. Like most major cities, San Diego had a declining crime rate from 1990 to 2000. Crime in San Diego increased in the early 2000s. In 2004, San Diego had the sixth lowest crime rate of any U.S. city with over half a million residents. From 2002 to 2006, the crime rate overall dropped 0.8%, though not evenly by category. While violent crime decreased 12.4% during this period, property crime increased 1.1%. Total property crimes per 100,000 people were lower than the national average in 2008.
San Diego's first television station was KFMB, which began broadcasting on May 16, 1949. Since the Federal Communications Commission (FCC) licensed seven television stations in Los Angeles, two VHF channels were available for San Diego because of its relative proximity to the larger city. In 1952, however, the FCC began licensing UHF channels, making it possible for cities such as San Diego to acquire more stations. Stations based in Mexico (with ITU prefixes of XE and XH) also serve the San Diego market. Television stations today include XHTJB 3 (Once TV), XETV 6 (CW), KFMB 8 (CBS), KGTV 10 (ABC), XEWT 12 (Televisa Regional), KPBS 15 (PBS), KBNT-CD 17 (Univision), XHTIT-TDT 21 (Azteca 7), XHJK-TDT 27 (Azteca 13), XHAS 33 (Telemundo), K35DG-D 35 (UCSD-TV), KDTF-LD 51 (Telefutura), KNSD 39 (NBC), KZSD-LP 41 (Azteca America), KSEX-CD 42 (Infomercials), XHBJ-TDT 45 (Gala TV), XHDTV 49 (MNTV), KUSI 51 (Independent), XHUAA-TDT 57 (Canal de las Estrellas), and KSWB-TV 69 (Fox). San Diego has an 80.6 percent cable penetration rate.
Due to the ratio of U.S. and Mexican-licensed stations, San Diego is the largest media market in the United States that is legally unable to support a television station duopoly between two full-power stations under FCC regulations, which disallow duopolies in metropolitan areas with fewer than nine full-power television stations and require that there must be eight unique station owners that remain once a duopoly is formed (there are only seven full-power stations on the California side of the San Diego-Tijuana market).[citation needed] Though the E. W. Scripps Company owns KGTV and KZSD-LP, they are not considered a duopoly under the FCC's legal definition as common ownership between full-power and low-power television stations in the same market is permitted regardless to the number of stations licensed to the area. As a whole, the Mexico side of the San Diego-Tijuana market has two duopolies and one triopoly (Entravision Communications owns both XHAS-TV and XHDTV-TV, Azteca owns XHJK-TV and XHTIT-TV, and Grupo Televisa owns XHUAA-TV and XHWT-TV along with being the license holder for XETV-TV, which is run by California-based subsidiary Bay City Television).
The radio stations in San Diego include nationwide broadcaster, Clear Channel Communications; CBS Radio, Midwest Television, Lincoln Financial Media, Finest City Broadcasting, and many other smaller stations and networks. Stations include: KOGO AM 600, KFMB AM 760, KCEO AM 1000, KCBQ AM 1170, K-Praise, KLSD AM 1360 Air America, KFSD 1450 AM, KPBS-FM 89.5, Channel 933, Star 94.1, FM 94/9, FM News and Talk 95.7, Q96 96.1, KyXy 96.5, Free Radio San Diego (AKA Pirate Radio San Diego) 96.9FM FRSD, KSON 97.3/92.1, KXSN 98.1, Jack-FM 100.7, 101.5 KGB-FM, KLVJ 102.1, Rock 105.3, and another Pirate Radio station at 106.9FM, as well as a number of local Spanish-language radio stations.
With the automobile being the primary means of transportation for over 80 percent of its residents, San Diego is served by a network of freeways and highways. This includes Interstate 5, which runs south to Tijuana and north to Los Angeles; Interstate 8, which runs east to Imperial County and the Arizona Sun Corridor; Interstate 15, which runs northeast through the Inland Empire to Las Vegas and Salt Lake City; and Interstate 805, which splits from I-5 near the Mexican border and rejoins I-5 at Sorrento Valley.
Major state highways include SR 94, which connects downtown with I-805, I-15 and East County; SR 163, which connects downtown with the northeast part of the city, intersects I-805 and merges with I-15 at Miramar; SR 52, which connects La Jolla with East County through Santee and SR 125; SR 56, which connects I-5 with I-15 through Carmel Valley and Rancho Peñasquitos; SR 75, which spans San Diego Bay as the San Diego-Coronado Bridge, and also passes through South San Diego as Palm Avenue; and SR 905, which connects I-5 and I-805 to the Otay Mesa Port of Entry.
San Diego's roadway system provides an extensive network of routes for travel by bicycle. The dry and mild climate of San Diego makes cycling a convenient and pleasant year-round option. At the same time, the city's hilly, canyon-like terrain and significantly long average trip distances—brought about by strict low-density zoning laws—somewhat restrict cycling for utilitarian purposes. Older and denser neighborhoods around the downtown tend to be utility cycling oriented. This is partly because of the grid street patterns now absent in newer developments farther from the urban core, where suburban style arterial roads are much more common. As a result, a vast majority of cycling-related activities are recreational. Testament to San Diego's cycling efforts, in 2006, San Diego was rated as the best city for cycling for U.S. cities with a population over 1 million.
San Diego is served by the San Diego Trolley light rail system, by the SDMTS bus system, and by Coaster and Amtrak Pacific Surfliner commuter rail; northern San Diego county is also served by the Sprinter light rail line. The Trolley primarily serves downtown and surrounding urban communities, Mission Valley, east county, and coastal south bay. A planned Mid-Coast extension of the Trolley will operate from Old Town to University City and the University of California, San Diego along the I-5 Freeway, with planned operation by 2018. The Amtrak and Coaster trains currently run along the coastline and connect San Diego with Los Angeles, Orange County, Riverside, San Bernardino, and Ventura via Metrolink and the Pacific Surfliner. There are two Amtrak stations in San Diego, in Old Town and the Santa Fe Depot downtown. San Diego transit information about public transportation and commuting is available on the Web and by dialing "511" from any phone in the area.
The city's primary commercial airport is the San Diego International Airport (SAN), also known as Lindbergh Field. It is the busiest single-runway airport in the United States. It served over 17 million passengers in 2005, and is dealing with an increasingly larger number every year. It is located on San Diego Bay three miles (4.8 km) from downtown. San Diego International Airport maintains scheduled flights to the rest of the United States including Hawaii, as well as to Mexico, Canada, Japan, and the United Kingdom. It is operated by an independent agency, the San Diego Regional Airport Authority. In addition, the city itself operates two general-aviation airports, Montgomery Field (MYF) and Brown Field (SDM). By 2015, the Tijuana Cross-border Terminal in Otay Mesa will give direct access to Tijuana International Airport, with passengers walking across the U.S.–Mexico border on a footbridge to catch their flight on the Mexican side.
Numerous regional transportation projects have occurred in recent years to mitigate congestion in San Diego. Notable efforts are improvements to San Diego freeways, expansion of San Diego Airport, and doubling the capacity of the cruise ship terminal of the port. Freeway projects included expansion of Interstates 5 and 805 around "The Merge," a rush-hour spot where the two freeways meet. Also, an expansion of Interstate 15 through the North County is underway with the addition of high-occupancy-vehicle (HOV) "managed lanes". There is a tollway (The South Bay Expressway) connecting SR 54 and Otay Mesa, near the Mexican border. According to a 2007 assessment, 37 percent of streets in San Diego were in acceptable driving condition. The proposed budget fell $84.6 million short of bringing the city's streets to an acceptable level. Port expansions included a second cruise terminal on Broadway Pier which opened in 2010. Airport projects include expansion of Terminal 2, currently under construction and slated for completion in summer 2013.
Spectre (2015) is the twenty-fourth James Bond film produced by Eon Productions. It features Daniel Craig in his fourth performance as James Bond, and Christoph Waltz as Ernst Stavro Blofeld, with the film marking the character's re-introduction into the series. It was directed by Sam Mendes as his second James Bond film following Skyfall, and was written by John Logan, Neal Purvis, Robert Wade and Jez Butterworth. It is distributed by Metro-Goldwyn-Mayer and Columbia Pictures. With a budget around $245 million, it is the most expensive Bond film and one of the most expensive films ever made.
The story sees Bond pitted against the global criminal organisation Spectre, marking the group's first appearance in an Eon Productions film since 1971's Diamonds Are Forever,[N 2] and tying Craig's series of films together with an overarching storyline. Several recurring James Bond characters, including M, Q and Eve Moneypenny return, with the new additions of Léa Seydoux as Dr. Madeleine Swann, Dave Bautista as Mr. Hinx, Andrew Scott as Max Denbigh and Monica Bellucci as Lucia Sciarra.
Spectre was released on 26 October 2015 in the United Kingdom on the same night as the world premiere at the Royal Albert Hall in London, followed by a worldwide release. It was released in the United States on 6 November 2015. It became the second James Bond film to be screened in IMAX venues after Skyfall, although it was not filmed with IMAX cameras. Spectre received mixed reviews upon its release; although criticised for its length, lack of screen time for new characters, and writing, it received praise for its action sequences and cinematography. The theme song, "Writing's on the Wall", received mixed reviews, particularly compared to the previous theme; nevertheless, it won the Golden Globe for Best Original Song and was nominated for the Academy Award in the same category. As of 20 February 2016[update], Spectre has grossed over $879 million worldwide.
Following Garreth Mallory's promotion to M, on a mission in Mexico City unofficially ordered by a posthumous message from the previous M, 007 James Bond kills three men plotting a terrorist bombing during the Day of the Dead and gives chase to Marco Sciarra, an assassin who survived the attack. In the ensuing struggle, Bond steals his ring, which is emblazoned with a stylised octopus, and then kills Sciarra by kicking him out of a helicopter. Upon returning to London, Bond is indefinitely suspended from field duty by M, who is in the midst of a power struggle with C, the head of the privately-backed Joint Intelligence Service, consisting of the recently merged MI5 and MI6. C campaigns for Britain to form alongside 8 other countries "Nine Eyes ", a global surveillance and intelligence co-operation initiative between nine member states, and uses his influence to close down the '00' section, believing it to be outdated.
Bond disobeys M's order and travels to Rome to attend Sciarra's funeral. That evening he visits Sciarra's widow Lucia, who tells him about Spectre, a criminal organisation to which her husband belonged. Bond infiltrates a Spectre meeting, where he identifies the leader, Franz Oberhauser. When Oberhauser addresses Bond by name, he escapes and is pursued by Mr. Hinx, a Spectre assassin. Moneypenny informs Bond that the information he collected leads to Mr. White, former member of Quantum, a subsidiary of Spectre. Bond asks her to investigate Oberhauser, who was presumed dead years earlier.
Bond travels to Austria to find White, who is dying of thallium poisoning. He admits to growing disenchanted with Quantum and tells Bond to find and protect his daughter, Dr. Madeline Swann, who will take him to L'Américain; this will in turn lead him to Spectre. White then commits suicide. Bond locates Swann at the Hoffler Klinik, but she is abducted by Hinx. Bond rescues her and the two meet Q, who discovers that Sciarra's ring links Oberhauser to Bond's previous missions, identifying Le Chiffre, Dominic Greene and Raoul Silva as Spectre agents. Swann reveals that L'Américain is a hotel in Tangier.
The two travel to the hotel and discover White's secret room where they find co-ordinates pointing to Oberhauser's operations base in the desert. They travel by train to the nearest station, but are once again confronted by Hinx; they engage in a fight throughout the train in which Mr Hinx is eventually thrown off the train by Bond with Swann's assistance. After arriving at the station, Bond and Swann are escorted to Oberhauser's base. There, he reveals that Spectre has been staging terrorist attacks around the world, creating a need for the Nine Eyes programme. In return Spectre will be given unlimited access to intelligence gathered by Nine Eyes. Bond is tortured as Oberhauser discusses their shared history: after the younger Bond was orphaned, Oberhauser's father, Hannes, became his temporary guardian. Believing that Bond supplanted his role as son, Oberhauser killed his father and staged his own death, subsequently adopting the name Ernst Stavro Blofeld and going on to form Spectre. Bond and Swann escape, destroying the base in the process, leaving Blofeld to apparently die during the explosion.
Bond and Swann return to London where they meet M, Bill Tanner, Q, and Moneypenny; they intend to arrest C and stop Nine Eyes from going online. Swann leaves Bond, telling him she cannot be part of a life involving espionage, and is subsequently kidnapped. On the way, the group is ambushed and Bond is kidnapped, but the rest still proceed with the plan. After Q succeeds in preventing the Nine Eyes from going online, a brief struggle between M and C ends with the latter falling to his death. Meanwhile, Bond is taken to the old MI6 building, which is scheduled for demolition, and frees himself. Moving throughout the ruined labyrinth, he encounters a disfigured Blofeld, who tells him that he has three minutes to escape the building before explosives are detonated or die trying to save Swann. Bond finds Swann and the two escape by boat as the building collapses. Bond shoots down Blofeld's helicopter, which crashes onto Westminster Bridge. As Blofeld crawls away from the wreckage, Bond confronts him but ultimately leaves him to be arrested by M. Bond leaves the bridge with Swann.
The ownership of the Spectre organisation—originally stylised "SPECTRE" as an acronym of SPecial Executive for Counter-intelligence, Terrorism, Revenge and Extortion—and its characters, had been at the centre of long-standing litigation starting in 1961 between Ian Fleming and Kevin McClory over the film rights to the novel Thunderball. The dispute began after Fleming incorporated elements of an undeveloped film script written by McClory and screenwriter Jack Whittingham—including characters and plot points—into Thunderball, which McClory contested in court, claiming ownership over elements of the novel. In 1963, Fleming settled out of court with McClory, in an agreement which awarded McClory the film rights. This enabled him to become a producer for the 1965 film Thunderball—with Albert R. Broccoli and Harry Saltzman as executive producers—and the non-Eon film Never Say Never Again, an updated remake of Thunderball, in 1983.[N 3] A second remake, entitled Warhead 2000 A.D., was planned for production and release in the 1990s before being abandoned. Under the terms of the 1963 settlement, the literary rights stayed with Fleming, allowing the Spectre organisation and associated characters to continue appearing in print.
In November 2013 MGM and the McClory estate formally settled the issue with Danjaq, LLC—sister company of Eon Productions—with MGM acquiring the full copyright film rights to the concept of Spectre and all of the characters associated with it. With the acquisition of the film rights and the organisation's re-introduction to the series' continuity, the SPECTRE acronym was discarded and the organisation reimagined as "Spectre".
In November 2014, Sony Pictures Entertainment was targeted by hackers who released details of confidential e-mails between Sony executives regarding several high-profile film projects. Included within these were several memos relating to the production of Spectre, claiming that the film was over budget, detailing early drafts of the script written by John Logan, and expressing Sony's frustration with the project. Eon Productions later issued a statement confirming the leak of what they called "an early version of the screenplay".
Despite being an original story, Spectre draws on Ian Fleming's source material, most notably in the character of Franz Oberhauser, played by Christoph Waltz. Oberhauser shares his name with Hannes Oberhauser, a background character in the short story "Octopussy" from the Octopussy and The Living Daylights collection, and who is named in the film as having been a temporary legal guardian of a young Bond in 1983. Similarly, Charmian Bond is shown to have been his full-time guardian, observing the back story established by Fleming. With the acquisition of the rights to Spectre and its associated characters, screenwriters Neal Purvis and Robert Wade revealed that the film would provide a minor retcon to the continuity of the previous films, with the Quantum organisation alluded to in Casino Royale and introduced in Quantum of Solace reimagined as a division within Spectre rather than an independent organisation.
Further references to Fleming's material can be found throughout the film; an MI6 safehouse is called "Hildebrand Rarities and Antiques", a reference to the short story "The Hildebrand Rarity" from the For Your Eyes Only short story collection.[citation needed] Bond's torture by Blofeld mirrors his torture by the title character of Kingsley Amis' continuation novel Colonel Sun.[citation needed]
The main cast was revealed in December 2014 at the 007 Stage at Pinewood Studios. Daniel Craig returned for his fourth appearance as James Bond, while Ralph Fiennes, Naomie Harris and Ben Whishaw reprised their roles as M, Eve Moneypenny and Q respectively, having been established in Skyfall. Rory Kinnear also reprised his role as Bill Tanner in his third appearance in the series.
Christoph Waltz was cast in the role of Franz Oberhauser, though he refused to comment on the nature of the part. It was later revealed with the film's release that he is Ernst Stavro Blofeld. Dave Bautista was cast as Mr. Hinx after producers sought an actor with a background in contact sports. After casting Bérénice Lim Marlohe, a relative newcomer, as Sévérine in Skyfall, Mendes consciously sought out a more experienced actor for the role of Madeleine Swann, ultimately casting Léa Seydoux in the role. Monica Bellucci joined the cast as Lucia Sciarra, becoming, at the age of fifty, the oldest actress to be cast as a Bond girl. In a separate interview with Danish website Euroman, Jesper Christensen revealed he would be reprising his role as Mr. White from Casino Royale and Quantum of Solace. Christensen's character was reportedly killed off in a scene intended to be used as an epilogue to Quantum of Solace, before it was removed from the final cut of the film, enabling his return in Spectre.
In addition to the principal cast, Alessandro Cremona was cast as Marco Sciarra, Stephanie Sigman was cast as Estrella, and Detlef Bothe was cast as a villain for scenes shot in Austria. In February 2015 over fifteen hundred extras were hired for the pre-title sequence set in Mexico, though they were duplicated in the film, giving the effect of around ten thousand extras.
In March 2013 Mendes said he would not return to direct the next film in the series, then known as Bond 24; he later recanted and announced that he would return, as he found the script and the plans for the long-term future of the franchise appealing. In directing Skyfall and Spectre, Mendes became the first director to oversee two consecutive Bond films since John Glen directed The Living Daylights and Licence to Kill in 1987 and 1989. Skyfall writer John Logan resumed his role of scriptwriter, collaborating with Neal Purvis and Robert Wade, who returned for their sixth Bond film.[N 4] The writer Jez Butterworth also worked on the script, alongside Mendes and Craig. Dennis Gassner returned as the film's production designer, while cinematographer Hoyte van Hoytema took over from Roger Deakins. In July 2015 Mendes noted that the combined crew of Spectre numbered over one thousand, making it a larger production than Skyfall. Craig is listed as co-producer.
Mendes revealed that production would begin on 8 December 2014 at Pinewood Studios, with filming taking seven months. Mendes also confirmed several filming locations, including London, Mexico City and Rome. Van Hoytema shot the film on Kodak 35 mm film stock. Early filming took place at Pinewood Studios, and around London, with scenes variously featuring Craig and Harris at Bond's flat, and Craig and Kinnear travelling down the River Thames.
Filming started in Austria in December 2014, with production taking in the area around Sölden—including the Ötztal Glacier Road, Rettenbach glacier and the adjacent ski resort and cable car station—and Obertilliach and Lake Altaussee, before concluding in February 2015. Scenes filmed in Austria centred on the Ice Q Restaurant, standing in for the fictional Hoffler Klinik, a private medical clinic in the Austrian Alps. Filming included an action scene featuring a Land Rover Defender Bigfoot and a Range Rover Sport. Production was temporarily halted first by an injury to Craig, who sprained his knee whilst shooting a fight scene, and later by an accident involving a filming vehicle that saw three crew members injured, at least one of them seriously.
Filming temporarily returned to England to shoot scenes at Blenheim Palace in Oxfordshire, which stood in for a location in Rome, before moving on to the city itself for a five-week shoot across the city, with locations including the Ponte Sisto bridge and the Roman Forum. The production faced opposition from a variety of special interest groups and city authorities, who were concerned about the potential for damage to historical sites around the city, and problems with graffiti and rubbish appearing in the film. A car chase scene set along the banks of the Tiber River and through the streets of Rome featured an Aston Martin DB10 and a Jaguar C-X75. The C-X75 was originally developed as a hybrid electric vehicle with four independent electric engines powered by two jet turbines, before the project was cancelled. The version used for filming was converted to use a conventional internal combustion engine, to minimise the potential for disruption from mechanical problems with the complex hybrid system. The C-X75s used for filming were developed by the engineering division of Formula One racing team Williams, who built the original C-X75 prototype for Jaguar.
With filming completed in Rome, production moved to Mexico City in late March to shoot the film's opening sequence, with scenes to include the Day of the Dead festival filmed in and around the Zócalo and the Centro Histórico district. The planned scenes required the city square to be closed for filming a sequence involving a fight aboard a Messerschmitt-Bölkow-Blohm Bo 105 helicopter flown by stunt pilot Chuck Aaron, which called for modifications to be made to several buildings to prevent damage. This particular scene in Mexico required 1,500 extras, 10 giant skeletons and 250,000 paper flowers. Reports in the Mexican media added that the film's second unit would move to Palenque in the state of Chiapas, to film aerial manoeuvres considered too dangerous to shoot in an urban area.
Following filming in Mexico, and during a scheduled break, Craig was flown to New York to undergo minor surgery to fix his knee injury. It was reported that filming was not affected and he had returned to filming at Pinewood Studios as planned on 22 April.
A brief shoot at London's City Hall was filmed on 18 April 2015, while Mendes was on location. On 17 May 2015 filming took place on the Thames in London. Stunt scenes involving Craig and Seydoux on a speedboat as well as a low flying helicopter near Westminster Bridge were shot at night, with filming temporarily closing both Westminster and Lambeth Bridges. Scenes were also shot on the river near MI6's headquarters at Vauxhall Cross. The crew returned to the river less than a week later to film scenes solely set on Westminster Bridge. The London Fire Brigade was on set to simulate rain as well as monitor smoke used for filming. Craig, Seydoux, and Waltz, as well as Harris and Fiennes, were seen being filmed. Prior to this, scenes involving Fiennes were shot at a restaurant in Covent Garden. Filming then took place in Trafalgar Square. In early June, the crew, as well as Craig, Seydoux, and Waltz, returned to the Thames for a final time to continue filming scenes previously shot on the river.
After wrapping up in England, production travelled to Morocco in June, with filming taking place in Oujda, Tangier and Erfoud, after preliminary work was completed by the production's second unit. An explosion filmed in Morocco holds a Guinness World Record for the "Largest film stunt explosion" in cinematic history, with the record credited to production designer Chris Corbould. Principal photography concluded on 5 July 2015. A wrap-up party for Spectre was held in commemoration before entering post-production. Filming took 128 days.
Whilst filming in Mexico City, speculation in the media claimed that the script had been altered to accommodate the demands of Mexican authorities—reportedly influencing details of the scene and characters, casting choices, and modifying the script in order to portray the country in a "positive light"—in order to secure tax concessions and financial support worth up to $20 million for the film. This was denied by producer Michael G. Wilson, who stated that the scene had always been intended to be shot in Mexico as production had been attracted to the imagery of the Day of the Dead, and that the script had been developed from there. Production of Skyfall had previously faced similar problems while attempting to secure permits to shoot the film's pre-title sequence in India before moving to Istanbul.
Thomas Newman returned as Spectre's composer. Rather than composing the score once the film had moved into post-production, Newman worked during filming. The theatrical trailer released in July 2015 contained a rendition of John Barry's On Her Majesty's Secret Service theme. Mendes revealed that the final film would have more than one hundred minutes of music. The soundtrack album was released on 23 October 2015 in the UK and 6 November 2015 in the USA on the Decca Records label.
In September 2015 it was announced that Sam Smith and regular collaborator Jimmy Napes had written the film's title theme, "Writing's on the Wall", with Smith performing it for the film. Smith said the song came together in one session and that he and Napes wrote it in under half an hour before recording a demo. Satisfied with the quality, the demo was used in the final release.
The song was released as a digital download on 25 September 2015. It received mixed reviews from critics and fans, particularly in comparison to Adele's "Skyfall". The mixed reception to the song led to Shirley Bassey trending on Twitter on the day it was released. It became the first Bond theme to reach number one in the UK Singles Chart. The English band Radiohead also composed a song for the film, which went unused.
During the December 2014 press conference announcing the start of filming, Aston Martin and Eon unveiled the new DB10 as the official car for the film. The DB10 was designed in collaboration between Aston Martin and the filmmakers, with only 10 being produced especially for Spectre as a celebration of the 50th anniversary of the company's association with the franchise. Only eight of those 10 were used for the film, however; the remaining two were used for promotional work. After modifying the Jaguar C-X75 for the film, Williams F1 carried the 007 logo on their cars at the 2015 Mexican Grand Prix, with the team playing host to the cast and crew ahead of the Mexican premiere of the film.
To promote the film, production continued the trend established during Skyfall's production of releasing still images of clapperboards and video blogs on Eon's official social media accounts.
On 13 March 2015, several members of the cast and crew, including Craig, Whishaw, Wilson and Mendes, as well as previous James Bond actor, Sir Roger Moore, appeared in a sketch written by David Walliams and the Dawson Brothers for Comic Relief's Red Nose Day on BBC One. In the sketch, they film a behind-the-scenes mockumentary on the filming of Spectre. The first teaser trailer for Spectre was released worldwide in March 2015, followed by the theatrical trailer in July and the final trailer in October.
Spectre had its world premiere in London on 26 October 2015 at the Royal Albert Hall, the same day as its general release in the United Kingdom and Republic of Ireland. Following the announcement of the start of filming, Paramount Pictures brought forward the release of Mission: Impossible – Rogue Nation to avoid competing with Spectre. In March 2015 IMAX corporation announced that Spectre would be screened in its cinemas, following Skyfall's success with the company. In the UK it received a wider release than Skyfall, with a minimum of 647 cinemas including 40 IMAX screens, compared to Skyfall's 587 locations and 21 IMAX screens.
As of 21 February 2016[update] Spectre has grossed $879.3 million worldwide; $138.1 million of the takings have been generated from the UK market and $199.8 million from North America.
In the United Kingdom, the film grossed £4.1 million ($6.4 million) from its Monday preview screenings. It grossed £6.3 million ($9.2 million) on its opening day and then £5.7 million ($8.8 million) on Wednesday, setting UK records for both days. In the film's first seven days it grossed £41.7 million ($63.8 million), breaking the UK record for highest first-week opening, set by Harry Potter and the Prisoner of Azkaban's £23.88 million ($36.9 million) in 2004. Its Friday–Saturday gross was £20.4 million ($31.2 million) compared to Skyfall's £20.1 million ($31 million). The film also broke the record for the best per-screen opening average with $110,000, a record previously held by The Dark Knight with $100,200. It has grossed a total of $136.3 million there. In the U.K., it surpassed Avatar to become the country's highest-grossing IMAX release ever with $10.09 million.
Spectre opened in Germany with $22.45 million (including previews), which included a new record for the biggest Saturday of all time, Australia with $8.7 million (including previews) and South Korea opened to $8.2 million (including previews). Despite the 13 November Paris attacks, which led to numerous theaters being closed down, the film opened with $14.6 million (including $2 million in previews) in France. In Mexico, where part of the film was shot, it debuted with more than double that of Skyfall with $4.5 million. It also bested its predecessor's opening in various Nordic regions where MGM is distributing, such as in Finland ($2.66 million) and Norway ($2.91 million), and in other markets like Denmark ($4.2 million), the Netherlands ($3.38 million), and Sweden ($3.1 million). In India, it opened at No. 1 with $4.8 million which is 4% above the opening of Skyfall. It topped the German-speaking Switzerland box office for four weeks and in the Netherlands, it has held the No. 1 spot for seven weeks straight where it has topped Minions to become the top movie of the year. The top earning markets are Germany ($70.3 million) and France ($38.8 million). In Paris, it has the second highest ticket sales of all time with $4.1 million tickets sold only behind Spider-Man 3 which sold over $6.32 million tickets in 2007.
In the United States and Canada, the film opened on 6 November 2015, and in its opening weekend, was originally projected to gross $70–75 million from 3,927 screens, the widest release for a Bond film. However, after grossing $5.25 million from its early Thursday night showings and $28 million on its opening day, weekend projections were increased to $75–80 million. The film ended up grossing $70.4 million in its opening weekend (about $20 million less than Skyfall's $90.6 million debut, including IMAX previews), but nevertheless finished first at the box office. IMAX generated $9.1 million for Spectre at 374 screens, premium large format made $8 million from 429 cinemas, reaping 11% of the film's opening, which means that Spectre earned $17.1 million (23%) of its opening weekend total in large-format venues. Cinemark XD generated $1.85 million in 112 XD locations.
In China, it opened on 12 November and earned $15 million on its opening day, which is the second biggest 2D single day gross for a Hollywood film behind the $18.5 million opening day of Mission: Impossible – Rogue Nation and occupying 43% of all available screens which included $790,000 in advance night screenings. Through its opening weekend, it earned $48.1 million from 14,700 screens which is 198% ahead of Skyfall, a new record for a Hollywood 2D opening. IMAX contributed $4.6 million on 246 screens, also a new record for a three-day opening for a November release (breaking Interstellar's record). In its second weekend, it added $12.1 million falling precipitously by 75% which is the second worst second weekend drop for any major Hollywood release in China of 2015. It grossed a total of $84.7 million there after four weekends. Albeit a strong opening it failed to attain the $100 million mark as projected.
Spectre has received mixed reviews, with many reviewers either giving the film highly positive or highly negative feedback. Many critics praised the film's opening scene, action sequences, stuntwork, cinematography and performances from the cast. In some early reviews, the film received favourable comparisons with its predecessor, Skyfall. Rotten Tomatoes sampled 274 reviews and judged 64% of the critiques to be positive, saying that the film "nudges Daniel Craig's rebooted Bond closer to the glorious, action-driven spectacle of earlier entries, although it's admittedly reliant on established 007 formula." On Metacritic, the film has a rating of 60 out of 100, based on 48 critics, indicating "mixed or average reviews". Audiences polled by CinemaScore gave the film an average grade of "A−" on an A+ to F scale.
Prior to its UK release, Spectre mostly received positive reviews. Mark Kermode, writing in The Guardian, gave the film four out of five stars, observing that the film did not live up to the standard set by Skyfall, but was able to tap into audience expectations. Writing in the same publication, Peter Bradshaw gave the film a full five stars, calling it "inventive, intelligent and complex", and singling out Craig's performance as the film's highlight. In another five star review, The Daily Telegraph's Robbie Collin described Spectre as "a swaggering show of confidence'", lauding it as "a feat of pure cinematic necromancy." In an otherwise positive, but overall less enthusiastic review, IGN's Chris Tilly considered Spectre "solid if unspectacular", and gave the film a 7.2 score (out of a possible 10), saying that "the film falls frustratingly short of greatness."
Critical appraisal of the film was mixed in the United States. In a lukewarm review for RogerEbert.com, Matt Zoller Seitz gave the film 2.5 stars out of 4, describing Spectre as inconsistent and unable to capitalise on its potential. Kenneth Turan, reviewing the film for Los Angeles Times, concluded that Spectre "comes off as exhausted and uninspired". Manohla Dargis of The New York Times panned the film as having "nothing surprising" and sacrificing its originality for the sake of box office returns. Forbes' Scott Mendelson also heavily criticised the film, denouncing Spectre as "the worst 007 movie in 30 years". Darren Franich of Entertainment Weekly viewed Spectre as "an overreaction to our current blockbuster moment", aspiring "to be a serialized sequel" and proving "itself as a Saga". While noting that "[n]othing that happens in Spectre holds up to even minor logical scrutiny", he had "come not to bury Spectre, but to weirdly praise it. Because the final act of the movie is so strange, so willfully obtuse, that it deserves extra attention." In a positive review Rolling Stone, Peter Travers gave the film 3.5 stars out of 4, describing "The 24th movie about the British MI6 agent with a license to kill is party time for Bond fans, a fierce, funny, gorgeously produced valentine to the longest-running franchise in movies". Other positive reviews from Mick LaSalle from the San Francisco Chronicle, gave it a perfect 100 score, stating: “One of the great satisfactions of Spectre is that, in addition to all the stirring action, and all the timely references to a secret organization out to steal everyone’s personal information, we get to believe in Bond as a person.” Stephen Whitty from the New York Daily News, gave it an 80 grade, saying: “Craig is cruelly efficient. Dave Bautista makes a good, Oddjob-like assassin. And while Lea Seydoux doesn’t leave a huge impression as this film’s “Bond girl,” perhaps it’s because we’ve already met — far too briefly — the hypnotic Monica Bellucci, as the first real “Bond woman” since Diana Rigg.” Richard Roeper from the Chicago Sun-Times, gave it a 75 grade. He stated: “This is the 24th Bond film and it ranks solidly in the middle of the all-time rankings, which means it’s still a slick, beautifully photographed, action-packed, international thriller with a number of wonderfully, ludicrously entertaining set pieces, a sprinkling of dry wit, myriad gorgeous women and a classic psycho-villain who is clearly out of his mind but seems to like it that way.” Michael Phillips over at the Chicago Tribune, gave it a 75 grade. He stated: “For all its workmanlike devotion to out-of-control helicopters, “Spectre” works best when everyone’s on the ground, doing his or her job, driving expensive fast cars heedlessly, detonating the occasional wisecrack, enjoying themselves and their beautiful clothes.” Guy Lodge from Variety, gave it a 70 score, stating: “What’s missing is the unexpected emotional urgency of “Skyfall,” as the film sustains its predecessor’s nostalgia kick with a less sentimental bent.”
Christopher Orr, writing in The Atlantic, also criticised the film, saying that Spectre "backslides on virtually every [aspect]". Lawrence Toppman of The Charlotte Observer called Craig's performance "Bored, James Bored." Alyssa Rosenberg, writing for The Washington Post, stated that the film turned into "a disappointingly conventional Bond film."
In India, it was reported that the Indian Central Board of Film Certification (CBFC) censored kissing scenes featuring Monica Bellucci, Daniel Craig, and Léa Seydoux. They also muted all profanity. This prompted criticism of the board online, especially on Twitter.
A sequel to Spectre will begin development in spring 2016. Sam Mendes has stated he will not return to direct the next 007 film. Christoph Waltz has signed on for two more films in the series, but his return depends on whether or not Craig will again portray Bond.
Tucson (/ˈtuːsɒn/ /tuːˈsɒn/) is a city and the county seat of Pima County, Arizona, United States, and home to the University of Arizona. The 2010 United States Census put the population at 520,116, while the 2013 estimated population of the entire Tucson metropolitan statistical area (MSA) was 996,544. The Tucson MSA forms part of the larger Tucson-Nogales combined statistical area (CSA), with a total population of 980,263 as of the 2010 Census. Tucson is the second-largest populated city in Arizona behind Phoenix, both of which anchor the Arizona Sun Corridor. The city is  located 108 miles (174 km) southeast of Phoenix and 60 mi (97 km) north of the U.S.-Mexico border. Tucson is the 33rd largest city and the 59th largest metropolitan area in the United States. Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname Optics Valley.
Tucson was probably first visited by Paleo-Indians, known to have been in southern Arizona about 12,000 years ago. Recent archaeological excavations near the Santa Cruz River have located a village site dating from 2100 BC.[citation needed] The floodplain of the Santa Cruz River was extensively farmed during the Early Agricultural period, circa 1200 BC to AD 150. These people constructed irrigation canals and grew corn, beans, and other crops while gathering wild plants and hunting. The Early Ceramic period occupation of Tucson saw the first extensive use of pottery vessels for cooking and storage. The groups designated as the Hohokam lived in the area from AD 600 to 1450 and are known for their vast irrigation canal systems and their red-on-brown pottery.[citation needed]
Jesuit missionary Eusebio Francisco Kino visited the Santa Cruz River valley in 1692, and founded the Mission San Xavier del Bac in 1700 about 7 mi (11 km) upstream from the site of the settlement of Tucson. A separate Convento settlement was founded downstream along the Santa Cruz River, near the base of what is now "A" mountain. Hugo O'Conor, the founding father of the city of Tucson, Arizona authorized the construction of a military fort in that location, Presidio San Agustín del Tucsón, on August 20, 1775 (near the present downtown Pima County Courthouse). During the Spanish period of the presidio, attacks such as the Second Battle of Tucson were repeatedly mounted by Apaches. Eventually the town came to be called "Tucson" and became a part of Sonora after Mexico gained independence from Spain in 1821.
Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854. Tucson became a part of the United States of America, although the American military did not formally take over control until March 1856. In 1857 Tucson became a stage station on the San Antonio-San Diego Mail Line and in 1858 became 3rd division headquarters of the Butterfield Overland Mail until the line shut down in March 1861. The Overland Mail Corporation attempted to continue running, however following the Bascom Affair, devastating Apache attacks on the stations and coaches ended operations in August 1861.[citation needed]
From 1877 to 1878, the area suffered a rash of stagecoach robberies. Most notable, however, were the two holdups committed by masked road-agent William Whitney Brazelton. Brazelton held up two stages in the summer of 1878 near Point of Mountain Station approximately 17 mi (27 km) northwest of Tucson. John Clum, of Tombstone, Arizona fame was one of the passengers. Brazelton was eventually tracked down and killed on Monday August 19, 1878, in a mesquite bosque along the Santa Cruz River 3 miles (5 km) south of Tucson by Pima County Sheriff Charles A. Shibell and his citizen's posse. Brazelton had been suspected of highway robbery not only in the Tucson area, but also in the Prescott region and Silver City, New Mexico area as well. Brazelton's crimes prompted John J. Valentine, Sr. of Wells, Fargo & Co. to send special agent and future Pima County sheriff Bob Paul to investigate. Fort Lowell, then east of Tucson, was established to help protect settlers from Apache attacks. In 1882, Frank Stilwell was implicated in the murder of Morgan Earp by Cowboy Pete Spence's wife, Marietta, at the coroner's inquest on Morgan Earp's shooting. The coroner's jury concluded that Spence, Stilwell, Frederick Bode, and Florentino "Indian Charlie" Cruz were the prime suspects in the assassination of Morgan Earp. :250 Deputy U.S. Marshal Wyatt Earp gathered a few trusted friends and accompanied Virgil Earp and his family as they traveled to Benson for a train ride to California. They found Stilwell lying in wait for Virgil in the Tucson station and killed him on the tracks. After killing Stilwell, Wyatt deputized others and rode on a vendetta, killing three more cowboys over the next few days before leaving the state.
By 1900, 7,531 people lived in the city. The population increased gradually to 13,913 in 1910. At about this time, the U.S. Veterans Administration had begun construction on the present Veterans Hospital. Many veterans who had been gassed in World War I and were in need of respiratory therapy began coming to Tucson after the war, due to the clean dry air. Over the following years the city continued to grow, with the population increasing to 20,292 in 1920 and 36,818 in 1940. In 2006 the population of Pima County, in which Tucson is located, passed one million while the City of Tucson's population was 535,000.
The city's elevation is 2,643 ft (806 m) above sea level (as measured at the Tucson International Airport). Tucson is situated on an alluvial plain in the Sonoran desert, surrounded by five minor ranges of mountains: the Santa Catalina Mountains and the Tortolita Mountains to the north, the Santa Rita Mountains to the south, the Rincon Mountains to the east, and the Tucson Mountains to the west. The high point of the Santa Catalina Mountains is 9,157 ft (2,791 m) Mount Lemmon, the southernmost ski destination in the continental U.S., while the Tucson Mountains include 4,687 ft (1,429 m) Wasson Peak. The highest point in the area is Mount Wrightson, found in the Santa Rita Mountains at 9,453 ft (2,881 m) above sea level.
Tucson is located 118 mi (190 km) southeast of Phoenix and 60 mi (97 km) north of the United States - Mexico border. The 2010 United States Census puts the city's population at 520,116 with a metropolitan area population at 980,263. In 2009, Tucson ranked as the 32nd largest city and 52nd largest metropolitan area in the United States. A major city in the Arizona Sun Corridor, Tucson is the largest city in southern Arizona, the second largest in the state after Phoenix. It is also the largest city in the area of the Gadsden Purchase. As of 2015, The Greater Tucson Metro area has exceeded a population of 1 million.
Interstate 10, which runs southeast to northwest through town, connects Tucson to Phoenix to the northwest on the way to its western terminus in Santa Monica, California, and to Las Cruces, New Mexico and El Paso, Texas toward its eastern terminus in Jacksonville, Florida. I-19 runs south from Tucson toward Nogales and the U.S.-Mexico border. I-19 is the only Interstate highway that uses "kilometer posts" instead of "mileposts", although the speed limits are marked in miles per hour instead of kilometers per hour.
At the end of the first decade of the 21st century, downtown Tucson underwent a revitalization effort by city planners and the business community. The primary project was Rio Nuevo, a large retail and community center that has been stalled in planning for more than ten years. Downtown is generally regarded as the area bordered by 17th Street to the south, I-10 to the west, and 6th Street to the north, and Toole Avenue and the Union Pacific (formerly Southern Pacific) railroad tracks, site of the historic train depot and "Locomotive #1673", built in 1900. Downtown is divided into the Presidio District, the Barrio Viejo, and the Congress Street Arts and Entertainment District. Some authorities include the 4th Avenue shopping district, which is set just northeast of the rest of downtown and connected by an underpass beneath the UPRR tracks.
As one of the oldest parts of town, Central Tucson is anchored by the Broadway Village shopping center designed by local architect Josias Joesler at the intersection of Broadway Boulevard and Country Club Road. The 4th Avenue Shopping District between downtown and the University and the Lost Barrio just East of downtown, also have many unique and popular stores. Local retail business in Central Tucson is densely concentrated along Fourth Avenue and the Main Gate Square on University Boulevard near the UA campus. The El Con Mall is also located in the eastern part of midtown.
Tucson's largest park, Reid Park, is located in midtown and includes Reid Park Zoo and Hi Corbett Field. Speedway Boulevard, a major east-west arterial road in central Tucson, was named the "ugliest street in America" by Life magazine in the early 1970s, quoting Tucson Mayor James Corbett. Despite this, Speedway Boulevard was awarded "Street of the Year" by Arizona Highways in the late 1990s. According to David Leighton, historical writer for the Arizona Daily Star newspaper, Speedway Boulevard derives its name from an old horse racetrack, known as "The Harlem River Speedway," more commonly called "The Speedway," in New York City. The street was called "The Speedway," from 1904 to about 1906 before the word "The" was taken out.
Central Tucson is bicycle-friendly. To the east of the University of Arizona, Third Street is bike-only except for local traffic and passes by the historic homes of the Sam Hughes neighborhood. To the west, E. University Boulevard leads to the Fourth Avenue Shopping District. To the North, N. Mountain Avenue has a full bike-only lane for half of the 3.5 miles (5.6 km) to the Rillito River Park bike and walk multi-use path. To the south, N. Highland Avenue leads to the Barraza-Aviation Parkway bicycle path.
South Tucson is actually the name of an independent, incorporated town of 1 sq mi (2.6 km2), completely surrounded by the city of Tucson, sitting just south of downtown. South Tucson has a colorful, dynamic history. It was first incorporated in 1936, and later reincorporated in 1940. The population consists of about 83% Mexican-American and 10% Native American residents. South Tucson is widely known for its many Mexican restaurants and the architectural styles which include bright outdoor murals, many of which have been painted over due to city policy.
A combination of urban and suburban development, the West Side is generally defined as the area west of I-10. Western Tucson encompasses the banks of the Santa Cruz River and the foothills of the Tucson Mountains, and includes the International Wildlife Museum, Sentinel Peak, and the Marriott Starr Pass Resort & Spa, located in the wealthy enclave known as Starr Pass. Moving past the Tucson Mountains, travelers find themselves in the area commonly referred to as "west of" Tucson or "Old West Tucson". A large undulating plain extending south into the Altar Valley, rural residential development predominates, but here you will also find major attractions including Saguaro National Park West, the Arizona-Sonora Desert Museum, and the Old Tucson Studios movie set/theme park.
On Sentinel Peak (also known as "'A' Mountain"), just west of downtown, there is a giant "A" in honor of the University of Arizona. Starting in about 1916, a yearly tradition developed for freshmen to whitewash the "A", which was visible for miles. However, at the beginning of the Iraq War, anti-war activists painted it black. This was followed by a paint scuffle where the "A" was painted various colors until the city council intervened. It is now red, white and blue except when it is white or another color decided by a biennial election. Because of the three-color paint scheme often used, the shape of the A can be vague and indistinguishable from the rest of the peak. The top of Sentinel Peak, which is accessible by road, offers an outstanding scenic view of the city looking eastward. A parking lot located near the summit of Sentinel Peak was formerly a popular place to watch sunsets or view the city lights at night.
Also on the north side is the suburban community of Catalina Foothills, located in the foothills of the Santa Catalina Mountains just north of the city limits. This community includes among the area's most expensive homes, sometimes multimillion-dollar estates. The Foothills area is generally defined as north of River Road, east of Oracle Road, and west of Sabino Creek. Some of the Tucson area's major resorts are located in the Catalina Foothills, including the Hacienda Del Sol, Westin La Paloma Resort, Loews Ventana Canyon Resort and Canyon Ranch Resort. La Encantada, an upscale outdoor shopping mall, is also in the Foothills.
The expansive area northwest of the city limits is diverse, ranging from the rural communities of Catalina and parts of the town of Marana, the small suburb of Picture Rocks, the affluent town of Oro Valley in the western foothills of the Santa Catalina Mountains, and residential areas in the northeastern foothills of the Tucson Mountains. Continental Ranch (Marana), Dove Mountain (Marana), and Rancho Vistoso (Oro Valley) are all masterplanned communities located in the Northwest, where thousands of residents live.
The community of Casas Adobes is also on the Northwest Side, with the distinction of being Tucson's first suburb, established in the late 1940s. Casas Adobes is centered on the historic Casas Adobes Plaza (built in 1948). Casas Adobes is also home to Tohono Chul Park (a nature preserve) near the intersection of North Oracle Road and West Ina Road. The attempted assassination of Representative Gabrielle Giffords, and the murders of chief judge for the U.S. District Court for Arizona, John Roll and five other people on January 8, 2011, occurred at the La Toscana Village in Casas Adobes. The Foothills Mall is also located on the northwest side in Casas Adobes.
East Tucson is relatively new compared to other parts of the city, developed between the 1950s and the 1970s,[citation needed] with developments such as Desert Palms Park. It is generally classified as the area of the city east of Swan Road, with above-average real estate values relative to the rest of the city. The area includes urban and suburban development near the Rincon Mountains. East Tucson includes Saguaro National Park East. Tucson's "Restaurant Row" is also located on the east side, along with a significant corporate and financial presence. Restaurant Row is sandwiched by three of Tucson's storied Neighborhoods: Harold Bell Wright Estates, named after the famous author's ranch which occupied some of that area prior to the depression; the Tucson Country Club (the third to bear the name Tucson Country Club), and the Dorado Country Club. Tucson's largest office building is 5151 East Broadway in east Tucson, completed in 1975. The first phases of Williams Centre, a mixed-use, master-planned development on Broadway near Craycroft Road, were opened in 1987. Park Place, a recently renovated shopping center, is also located along Broadway (west of Wilmot Road).
Near the intersection of Craycroft and Ft. Lowell Roads are the remnants of the Historic Fort Lowell. This area has become one of Tucson's iconic neighborhoods. In 1891, the Fort was abandoned and much of the interior was stripped of their useful components and it quickly fell into ruin. In 1900, three of the officer buildings were purchased for use as a sanitarium. The sanitarium was then sold to Harvey Adkins in 1928. The Bolsius family Pete, Nan and Charles Bolsius purchased and renovated surviving adobe buildings of the Fort – transforming them into spectacular artistic southwestern architectural examples. Their woodwork, plaster treatment and sense of proportion drew on their Dutch heritage and New Mexican experience. Other artists and academics throughout the middle of the 20th century, including: Win Ellis, Jack Maul, Madame Cheruy, Giorgio Belloli, Charels Bode, Veronica Hughart, Edward and Rosamond Spicer, Hazel Larson Archer and Ruth Brown, renovated adobes, built homes and lived in the area. The artist colony attracted writers and poets including beat generation Alan Harrington and Jack Kerouac whose visit is documented in his iconic book On the Road. This rural pocket in the middle of the city is listed on the National Register of Historic Places. Each year in February the neighborhood celebrates its history in the City Landmark it owns and restored the San Pedro Chapel.
Southeast Tucson continues to experience rapid residential development. The area includes Davis-Monthan Air Force Base. The area is considered to be south of Golf Links Road. It is the home of Santa Rita High School, Chuck Ford Park (Lakeside Park), Lakeside Lake, Lincoln Park (upper and lower), The Lakecrest Neighborhoods, and Pima Community College East Campus. The Atterbury Wash with its access to excellent bird watching is also located in the Southeast Tucson area. The suburban community of Rita Ranch houses many of the military families from Davis-Monthan, and is near the southeastern-most expansion of the current city limits. Close by Rita Ranch and also within the city limits lies Civano, a planned development meant to showcase ecologically sound building practices and lifestyles.
Catalina Highway stretches 25 miles (40 km) and the entire mountain range is one of Tucson's most popular vacation spots for cycling, hiking, rock climbing, camping, birding, and wintertime snowboarding and skiing. Near the top of Mt. Lemmon is the town of Summerhaven. In Summerhaven, visitors will find log houses and cabins, a general store, and various shops, as well as numerous hiking trails. Near Summerhaven is the road to Ski Valley which hosts a ski lift, several runs, a giftshop, and nearby restaurant.
Tucson has a desert climate (Köppen BWh), with two major seasons, summer and winter; plus three minor seasons: fall, spring, and the monsoon. Tucson averages 11.8 inches (299.7 mm) of precipitation per year, more than most other locations with desert climates, but it still qualifies due to its high evapotranspiration; in other words, it experiences a high net loss of water. A similar scenario is seen in Alice Springs, Australia, which averages 11 inches (279.4 mm) a year, but has a desert climate.
The monsoon can begin any time from mid-June to late July, with an average start date around July 3. It typically continues through August and sometimes into September. During the monsoon, the humidity is much higher than the rest of the year. It begins with clouds building up from the south in the early afternoon followed by intense thunderstorms and rainfall, which can cause flash floods. The evening sky at this time of year is often pierced with dramatic lightning strikes. Large areas of the city do not have storm sewers, so monsoon rains flood the main thoroughfares, usually for no longer than a few hours. A few underpasses in Tucson have "feet of water" scales painted on their supports to discourage fording by automobiles during a rainstorm. Arizona traffic code Title 28-910, the so-called "Stupid Motorist Law", was instituted in 1995 to discourage people from entering flooded roadways. If the road is flooded and a barricade is in place, motorists who drive around the barricade can be charged up to $2000 for costs involved in rescuing them. Despite all warnings and precautions, however, three Tucson drivers have drowned between 2004 and 2010.
Winters in Tucson are mild relative to other parts of the United States. Daytime highs in the winter range between 64 and 75 °F (18 and 24 °C), with overnight lows between 30 and 44 °F (−1 and 7 °C). Tucson typically averages one hard freeze per winter season, with temperatures dipping to the mid or low-20s (−7 to −4 °C), but this is typically limited to only a very few nights. Although rare, snow has been known to fall in Tucson, usually a light dusting that melts within a day. The most recent snowfall was on February 20, 2013 when 2.0 inches of snow blanketed the city, the largest snowfall since 1987.
At the University of Arizona, where records have been kept since 1894, the record maximum temperature was 115 °F (46 °C) on June 19, 1960, and July 28, 1995, and the record minimum temperature was 6 °F (−14 °C) on January 7, 1913. There are an average of 150.1 days annually with highs of 90 °F (32 °C) or higher and an average of 26.4 days with lows reaching or below the freezing mark. Average annual precipitation is 11.15 in (283 mm). There is an average of 49 days with measurable precipitation. The wettest year was 1905 with 24.17 in (614 mm) and the driest year was 1924 with 5.07 in (129 mm). The most precipitation in one month was 7.56 in (192 mm) in July 1984. The most precipitation in 24 hours was 4.16 in (106 mm) on October 1, 1983. Annual snowfall averages 0.7 in (1.8 cm). The most snow in one year was 7.2 in (18 cm) in 1987. The most snow in one month was 6.0 in (15 cm) in January 1898 and March 1922.
At the airport, where records have been kept since 1930, the record maximum temperature was 117 °F (47 °C) on June 26, 1990, and the record minimum temperature was 16 °F (−9 °C) on January 4, 1949. There is an average of 145.0 days annually with highs of 90 °F (32 °C) or higher and an average of 16.9 days with lows reaching or below the freezing mark. Measurable precipitation falls on an average of 53 days. The wettest year was 1983 with 21.86 in (555 mm) of precipitation, and the driest year was 1953 with 5.34 in (136 mm). The most rainfall in one month was 7.93 in (201 mm) in August 1955. The most rainfall in 24 hours was 3.93 in (100 mm) on July 29, 1958. Snow at the airport averages only 1.1 in (2.8 cm) annually. The most snow received in one year was 8.3 in (21 cm) and the most snow in one month was 6.8 in (17 cm) in December 1971.
As of the census of 2010, there were 520,116 people, 229,762 households, and 112,455 families residing in the city. The population density was 2,500.1 inhabitants per square mile (965.3/km²). There were 209,609 housing units at an average density of 1,076.7 per square mile (415.7/km²). The racial makeup of the city was 69.7% White (down from 94.8% in 1970), 5.0% Black or African-American, 2.7% Native American, 2.9% Asian, 0.2% Pacific Islander, 16.9% from other races, and 3.8% from two or more races. Hispanic or Latino of any race were 41.6% of the population. Non-Hispanic Whites were 47.2% of the population in 2010, down from 72.8% in 1970.
Much of Tucson's economic development has been centered on the development of the University of Arizona, which is currently the second largest employer in the city. Davis-Monthan Air Force Base, located on the southeastern edge of the city, also provides many jobs for Tucson residents. Its presence, as well as the presence of the US Army Intelligence Center (Fort Huachuca, the largest employer in the region in nearby Sierra Vista), has led to the development of a significant number of high-tech industries, including government contractors, in the area. The city of Tucson is also a major hub for the Union Pacific Railroad's Sunset Route that links the Los Angeles ports with the South/Southeast regions of the country.
The City of Tucson, Pima County, the State of Arizona, and the private sector have all made commitments to create a growing, healthy economy[citation needed] with advanced technology industry sectors as its foundation. Raytheon Missile Systems (formerly Hughes Aircraft Co.), Texas Instruments, IBM, Intuit Inc., Universal Avionics, Honeywell Aerospace, Sunquest Information Systems, Sanofi-Aventis, Ventana Medical Systems, Inc., and Bombardier Aerospace all have a significant presence in Tucson. Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname "Optics Valley".
Since 2009, the Tucson Festival of Books has been held annually over a two-day period in March at the University of Arizona. By 2010 it had become the fourth largest book festival in the United States, with 450 authors and 80,000 attendees. In addition to readings and lectures, it features a science fair, varied entertainment, food, and exhibitors ranging from local retailers and publishers to regional and national nonprofit organizations. In 2011, the Festival began presenting a Founder's Award; recipients include Elmore Leonard and R.L. Stine.
For the past 25 years, the Tucson Folk Festival has taken place the first Saturday and Sunday of May in downtown Tucson's El Presidio Park. In addition to nationally known headline acts each evening, the Festival highlights over 100 local and regional musicians on five stages is one of the largest free festivals in the country. All stages are within easy walking distance. Organized by the Tucson Kitchen Musicians Association, volunteers make this festival possible. KXCI 91.3-FM, Arizona's only community radio station, is a major partner, broadcasting from the Plaza Stage throughout the weekend. In addition, there are numerous workshops, events for children, sing-alongs, and a popular singer/songwriter contest. Musicians typically play 30-minute sets, supported by professional audio staff volunteers. A variety of food and crafts are available at the festival, as well as local micro-brews. All proceeds from sales go to fund future festivals.
Another popular event held in February, which is early spring in Tucson, is the Fiesta de los Vaqueros, or rodeo week, founded by winter visitor, Leighton Kramer. While at its heart the Fiesta is a sporting event, it includes what is billed as "the world's largest non-mechanized parade". The Rodeo Parade is a popular event as most schools give two rodeo days off instead of Presidents Day. The exception to this is Presidio High (a non-public charter school), which doesn't get either. Western wear is seen throughout the city as corporate dress codes are cast aside during the Fiesta. The Fiesta de los Vaqueros marks the beginning of the rodeo season in the United States.
The Procession, held at sundown, consists of a non-motorized parade through downtown Tucson featuring many floats, sculptures, and memorials, in which the community is encouraged to participate. The parade is followed by performances on an outdoor stage, culminating in the burning of an urn in which written prayers have been collected from participants and spectators. The event is organized and funded by the non-profit arts organization Many Mouths One Stomach, with the assistance of many volunteers and donations from the public and local businesses.
The accomplished and awarded writers (poets, novelists, dramatists, nonfiction writers) who have lived in Tucson include Edward Abbey, Erskine Caldwell, Barbara Kingsolver and David Foster Wallace. Some were associated with the University of Arizona, but many were independent writers who chose to make Tucson their home. The city is particularly active in publishing and presenting contemporary innovative poetry in various ways. Examples are the Chax Press, a publisher of poetry books in trade and book arts editions, and the University of Arizona Poetry Center, which has a sizable poetry library and presents readings, conferences, and workshops.
Tucson is commonly known as "The Old Pueblo". While the exact origin of this nickname is uncertain, it is commonly traced back to Mayor R. N. "Bob" Leatherwood. When rail service was established to the city on March 20, 1880, Leatherwood celebrated the fact by sending telegrams to various leaders, including the President of the United States and the Pope, announcing that the "ancient and honorable pueblo" of Tucson was now connected by rail to the outside world. The term became popular with newspaper writers who often abbreviated it as "A. and H. Pueblo". This in turn transformed into the current form of "The Old Pueblo".
The University of Arizona Wildcats sports teams, most notably the men's basketball and women's softball teams have strong local interest. The men's basketball team, formerly coached by Hall of Fame head coach Lute Olson and currently coached by Sean Miller, has made 25 straight NCAA Tournaments and won the 1997 National Championship. Arizona's Softball team has reached the NCAA National Championship game 12 times and has won 8 times, most recently in 2007. The university's swim teams have gained international recognition, with swimmers coming from as far as Japan and Africa to train with the coach Frank Busch who has also worked with the U.S. Olympic swim team for a number of years. Both men and women's swim teams recently[when?] won the NCAA National Championships.
The Tucson Padres played at Kino Veterans Memorial Stadium from 2011 to 2013. They served as the AAA affiliate of the San Diego Padres. The team, formerly known as the Portland Beavers, was temporarily relocated to Tucson from Portland while awaiting the building of a new stadium in Escondido. Legal issues derailed the plans to build the Escondido stadium, so they moved to El Paso, Texas for the 2014 season. Previously, the Tucson Sidewinders, a triple-A affiliate of the Arizona Diamondbacks, won the Pacific Coast League championship and unofficial AAA championship in 2006. The Sidewinders played in Tucson Electric Park and were in the Pacific Conference South of the PCL. The Sidewinders were sold in 2007 and moved to Reno, Nevada after the 2008 season. They now compete as the Reno Aces.
Tracks include Tucson Raceway Park and Rillito Downs. Tucson Raceway Park hosts NASCAR-sanctioned auto racing events and is one of only two asphalt short tracks in Arizona. Rillito Downs is an in-town destination on weekends in January and February each year. This historic track held the first organized quarter horse races in the world, and they are still racing there. The racetrack is threatened by development. The Moltacqua racetrack, was another historic horse racetrack located on what is now Sabino Canyon Road and Vactor Ranch Trail, but it no longer exists.
The League of American Bicyclists gave Tucson a gold rating for bicycle friendliness in late April 2007. Tucson hosts the largest perimeter cycling event in the United States. The ride called "El Tour de Tucson" happens in November on the Saturday before Thanksgiving. El Tour de Tucson produced and promoted by Perimeter Bicycling has as many as 10,000 participants from all over the world, annually. Tucson is one of only nine cities in the U.S. to receive a gold rating or higher for cycling friendliness from the League of American Bicyclists. The city is known for its winter cycling opportunities. Both road and mountain biking are popular in and around Tucson with trail areas including Starr Pass and Fantasy Island.
In general, Tucson and Pima County support the Democratic Party, as opposed the state's largest metropolitan area, Phoenix, which usually supports the Republican Party. Congressional redistricting in 2013, following the publication of the 2010 Census, divided the Tucson area into three Federal Congressional districts (the first, second and third of Arizona). The city center is in the 3rd District, represented by Raul Grijalva, a Democrat, since 2003, while the more affluent residential areas to the south and east are in the 2nd District, represented by Republican Martha McSally since 2015, and the exurbs north and west between Tucson and Phoenix in the 3rd District are represented by Democrat Ann Kirkpatrick since 2008. The United States Postal Service operates post offices in Tucson. The Tucson Main Post Office is located at 1501 South Cherrybell Stravenue.
Both the council members and the mayor serve four-year terms; none face term limits. Council members are nominated by their wards via a ward-level primary held in September. The top vote-earners from each party then compete at-large for their ward's seat on the November ballot. In other words, on election day the whole city votes on all the council races up for that year. Council elections are severed: Wards 1, 2, and 4 (as well as the mayor) are up for election in the same year (most recently 2011), while Wards 3, 5, and 6 share another year (most recently 2013).
Tucson is known for being a trailblazer in voluntary partial publicly financed campaigns. Since 1985, both mayoral and council candidates have been eligible to receive matching public funds from the city. To become eligible, council candidates must receive 200 donations of $10 or more (300 for a mayoral candidate). Candidates must then agree to spending limits equal to 33¢ for every registered Tucson voter, or $79,222 in 2005 (the corresponding figures for mayor are 64¢ per registered voter, or $142,271 in 2003). In return, candidates receive matching funds from the city at a 1:1 ratio of public money to private donations. The only other limitation is that candidates may not exceed 75% of the limit by the date of the primary. Many cities, such as San Francisco and New York City, have copied this system, albeit with more complex spending and matching formulas.
Tucson has one daily newspaper, the morning Arizona Daily Star. Wick Communications publishes the daily legal paper The Daily Territorial, while Boulder, Colo.-based 10/13 Communications publishes Tucson Weekly (an "alternative" publication), Inside Tucson Business and the Explorer. TucsonSentinel.com is a nonprofit independent online news organization. Tucson Lifestyle Magazine, Lovin' Life News, DesertLeaf, and Zócalo Magazine are monthly publications covering arts, architecture, decor, fashion, entertainment, business, history, and other events. The Arizona Daily Wildcat is the University of Arizona's student newspaper, and the Aztec News is the Pima Community College student newspaper. The New Vision is the newspaper for the Roman Catholic Diocese of Tucson, and the Arizona Jewish Post is the newspaper of the Jewish Federation of Southern Arizona.
The Tucson metro area is served by many local television stations and is the 68th largest designated market area (DMA) in the U.S. with 433,310 homes (0.39% of the total U.S.). It is limited to the three counties of southeastern Arizona (Pima, Santa Cruz, and Cochise) The major television networks serving Tucson are: KVOA 4 (NBC), KGUN 9 (ABC), KMSB-TV 11 (Fox), KOLD-TV 13 (CBS), KTTU 18 (My Network TV) and KWBA 58 (The CW). KUAT-TV 6 is a PBS affiliate run by the University of Arizona (as is sister station KUAS 27).
Tucson's primary electrical power source is a coal and natural gas power-plant managed by Tucson Electric Power that is situated within the city limits on the south-western boundary of Davis-Monthan Air-force base adjacent to Interstate-10. The air pollution generated has raised some concerns as the Sundt operating station has been online since 1962 as is exempt from many pollution standards and controls due to its age. Solar has been gaining ground in Tucson with its ideal over 300 days of sunshine climate. Federal, state, and even local utility credits and incentives have also enticed residents to equip homes with solar systems. Davis-Monthan AFB has a 3.3 Megawatt (MW) ground-mounted solar photovoltaic (PV) array and a 2.7 MW rooftop-mounted PV array, both of which are located in the Base Housing area. The base will soon have the largest solar-generating capacity in the United States Department of Defense after awarding a contract on September 10, 2010, to SunEdison to construct a 14.5 MW PV field on the northwestern side of the base.
Perhaps the biggest sustainability problem in Tucson, with its high desert climate, is potable water supply. The state manages all water in Arizona through its Arizona Department of Water Resources (ADWR). The primary consumer of water is Agriculture (including golf courses), which consumes about 69% of all water. Municipal (which includes residential use) accounts for about 25% of use. Energy consumption and availability is another sustainability issue. However, with over 300 days of full sun a year, Tucson has demonstrated its potential to be an ideal solar energy producer.
In an effort to conserve water, Tucson is recharging groundwater supplies by running part of its share of CAP water into various open portions of local rivers to seep into their aquifer. Additional study is scheduled to determine the amount of water that is lost through evaporation from the open areas, especially during the summer. The City of Tucson already provides reclaimed water to its inhabitants, but it is only used for "applications such as irrigation, dust control, and industrial uses." These resources have been in place for more than 27 years, and deliver to over 900 locations.
To prevent further loss of groundwater, Tucson has been involved in water conservation and groundwater preservation efforts, shifting away from its reliance on a series of Tucson area wells in favor of conservation, consumption-based pricing for residential and commercial water use, and new wells in the more sustainable Avra Valley aquifer, northwest of the city. An allocation from the Central Arizona Project Aqueduct (CAP), which passes more than 300 mi (480 km) across the desert from the Colorado River, has been incorporated into the city's water supply, annually providing over 20 million gallons of "recharged" water which is pumped into the ground to replenish water pumped out. Since 2001, CAP water has allowed the city to remove or turn off over 80 wells.
Tucson's Sun Tran bus system serves greater Tucson with standard, express, regional shuttle, and on-demand shuttle bus service. It was awarded Best Transit System in 1988 and 2005. A 3.9-mile streetcar line, Sun Link, connects the University of Arizona campus with 4th Avenue, downtown, and the Mercado District west of Interstate 10 and the Santa Cruz River. Ten-minute headway passenger service began July 25, 2014. The streetcar utilizes Sun Tran's card payment and transfer system, connecting with the University of Arizona's CatTran shuttles, Amtrak, and Greyhound intercity bus service.
Cycling is popular in Tucson due to its flat terrain and dry climate. Tucson and Pima County maintain an extensive network of marked bike routes, signal crossings, on-street bike lanes, mountain-biking trails, and dedicated shared-use paths. The Loop is a network of seven linear parks comprising over 100 mi (160 km) of paved, vehicle-free trails that encircles the majority of the city with links to Marana and Oro Valley. The Tucson-Pima County Bicycle Advisory Committee (TPCBAC) serves in an advisory capacity to local governments on issues relating to bicycle recreation, transportation, and safety. Tucson was awarded a gold rating for bicycle-friendliness by the League of American Bicyclists in 2006.
The Alps (/ælps/; Italian: Alpi [ˈalpi]; French: Alpes [alp]; German: Alpen [ˈʔalpm̩]; Slovene: Alpe [ˈáːlpɛ]) are the highest and most extensive mountain range system that lies entirely in Europe, stretching approximately 1,200 kilometres (750 mi) across eight Alpine countries: Austria, France, Germany, Italy, Liechtenstein, Monaco, Slovenia, and Switzerland. The Caucasus Mountains are higher, and the Urals longer, but both lie partly in Asia. The mountains were formed over tens of millions of years as the African and Eurasian tectonic plates collided. Extreme shortening caused by the event resulted in marine sedimentary rocks rising by thrusting and folding into high mountain peaks such as Mont Blanc and the Matterhorn. Mont Blanc spans the French–Italian border, and at 4,810 m (15,781 ft) is the highest mountain in the Alps. The Alpine region area contains about a hundred peaks higher than 4,000 m (13,123 ft), known as the "four-thousanders".
The altitude and size of the range affects the climate in Europe; in the mountains precipitation levels vary greatly and climatic conditions consist of distinct zones. Wildlife such as ibex live in the higher peaks to elevations of 3,400 m (11,155 ft), and plants such as Edelweiss grow in rocky areas in lower elevations as well as in higher elevations. Evidence of human habitation in the Alps goes back to the Paleolithic era.
A mummified man, determined to be 5,000 years old, was discovered on a glacier at the Austrian–Italian border in 1991. By the 6th century BC, the Celtic La Tène culture was well established. Hannibal famously crossed the Alps with a herd of elephants, and the Romans had settlements in the region. In 1800 Napoleon crossed one of the mountain passes with an army of 40,000. The 18th and 19th centuries saw an influx of naturalists, writers, and artists, in particular the Romantics, followed by the golden age of alpinism as mountaineers began to ascend the peaks. In World War II, Adolf Hitler kept a base of operation in the Bavarian Alps throughout the war.
The Alpine region has a strong cultural identity. The traditional culture of farming, cheesemaking, and woodworking still exists in Alpine villages, although the tourist industry began to grow early in the 20th century and expanded greatly after World War II to become the dominant industry by the end of the century. The Winter Olympic Games have been hosted in the Swiss, French, Italian, Austrian and German Alps. At present the region is home to 14 million people and has 120 million annual visitors.
The English word Alps derives from the Latin Alpes (through French). Maurus Servius Honoratus, an ancient commentator of Virgil, says in his commentary (A. X 13) that all high mountains are called Alpes by Celts. The term may be common to Italo-Celtic, because the Celtic languages have terms for high mountains derived from alp.
This may be consistent with the theory that in Greek Alpes is a name of non-Indo-European origin (which is common for prominent mountains and mountain ranges in the Mediterranean region). According to the Old English Dictionary, the Latin Alpes might possibly derive from a pre-Indo-European word *alb "hill"; "Albania" is a related derivation. Albania, a name not native to the region known as the country of Albania, has been used as a name for a number of mountainous areas across Europe. In Roman times, "Albania" was a name for the eastern Caucasus, while in the English language "Albania" (or "Albany") was occasionally used as a name for Scotland.
It's likely[weasel words] that alb ("white") and albus have common origins deriving from the association of the tops of tall mountains or steep hills with snow.
In modern languages the term alp, alm, albe or alpe refers to a grazing pastures in the alpine regions below the glaciers, not the peaks. An alp refers to a high mountain pasture where cows are taken to be grazed during the summer months and where hay barns can be found, and the term "the Alps", referring to the mountains, is a misnomer. The term for the mountain peaks varies by nation and language: words such as horn, kogel, gipfel, spitz, and berg are used in German speaking regions: mont, pic, dent and aiguille in French speaking regions; and monte, picco or cima in Italian speaking regions.
The Alps are a crescent shaped geographic feature of central Europe that ranges in a 800 km (500 mi) arc from east to west and is 200 km (120 mi) in width. The mean height of the mountain peaks is 2.5 km (1.6 mi). The range stretches from the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland. The range continues toward Vienna in Austria, and east to the Adriatic Sea and into Slovenia. To the south it dips into northern Italy and to the north extends to the south border of Bavaria in Germany. In areas like Chiasso, Switzerland, and Neuschwanstein, Bavaria, the demarcation between the mountain range and the flatlands are clear; in other places such as Geneva, the demarcation is less clear. The countries with the greatest alpine territory are Switzerland, France, Austria and Italy.
The highest portion of the range is divided by the glacial trough of the Rhone valley, with the Pennine Alps from Mont Blanc to the Matterhorn and Monte Rosa on the southern side, and the Bernese Alps on the northern. The peaks in the easterly portion of the range, in Austria and Slovenia, are smaller than those in the central and western portions.
The variances in nomenclature in the region spanned by the Alps makes classification of the mountains and subregions difficult, but a general classification is that of the Eastern Alps and Western Alps with the divide between the two occurring in eastern Switzerland according to geologist Stefan Schmid, near the Splügen Pass.
The highest peaks of the Western Alps and Eastern Alps, respectively, are Mont Blanc, at 4,810 m (15,780 ft) and Piz Bernina at 4,049 metres (13,284 ft). The second-highest major peaks are Monte Rosa at 4,634 m (15,200 ft) and Ortler at 3,905 m (12,810 ft), respectively
Series of lower mountain ranges run parallel to the main chain of the Alps, including the French Prealps in France and the Jura Mountains in Switzerland and France. The secondary chain of the Alps follows the watershed from the Mediterranean Sea to the Wienerwald, passing over many of the highest and most well-known peaks in the Alps. From the Colle di Cadibona to Col de Tende it runs westwards, before turning to the northwest and then, near the Colle della Maddalena, to the north. Upon reaching the Swiss border, the line of the main chain heads approximately east-northeast, a heading it follows until its end near Vienna.
The Alps have been crossed for war and commerce, and by pilgrims, students and tourists. Crossing routes by road, train or foot are known as passes, and usually consist of depressions in the mountains in which a valley leads from the plains and hilly pre-mountainous zones. In the medieval period hospices were established by religious orders at the summits of many of the main passes. The most important passes are the Col de l'Iseran (the highest), the Brenner Pass, the Mont-Cenis, the Great St. Bernard Pass, the Col de Tende, the Gotthard Pass, the Semmering Pass, and the Stelvio Pass.
Crossing the Italian-Austrian border, the Brenner Pass separates the Ötztal Alps and Zillertal Alps and has been in use as a trading route since the 14th century. The lowest of the Alpine passes at 985 m (3,232 ft), the Semmering crosses from Lower Austria to Styria; since the 12th century when a hospice was built there it has seen continuous use. A railroad with a tunnel 1 mile (1.6 km) long was built along the route of the pass in the mid-19th century. With a summit of 2,469 m (8,100 ft), the Great St. Bernard Pass is one of the highest in the Alps, crossing the Italian-Swiss border east of the Pennine Alps along the flanks of Mont Blanc. The pass was used by Napoleon Bonaparte to cross 40,000 troops in 1800. The Saint Gotthard Pass crosses from Central Switzerland to Ticino; in the late 19th century the 14 km (9 mi) long Saint Gotthard Tunnel was built connecting Lucerne in Switzerland, with Milan in Italy. The Mont Cenis pass has been a major commercial road between Western Europe and Italy. Now the pass has been supplanted by the Fréjus Road and Rail tunnel. At 2,756 m (9,042 ft), the Stelvio Pass in northern Italy is one of the highest of the Alpine passes; the road was built in the 1820s. The highest pass in the alps is the col de l'Iseran in Savoy (France) at 2,770 m (9,088 ft).
Important geological concepts were established as naturalists began studying the rock formations of the Alps in the 18th century. In the mid-19th century the now defunct theory of geosynclines was used to explain the presence of "folded" mountain chains but by the mid-20th century the theory of plate tectonics became widely accepted.
The formation of the Alps (the Alpine orogeny) was an episodic process that began about 300 million years ago. In the Paleozoic Era the Pangaean supercontinent consisted of a single tectonic plate; it broke into separate plates during the Mesozoic Era and the Tethys sea developed between Laurasia and Gondwana during the Jurassic Period. The Tethys was later squeezed between colliding plates causing the formation of mountain ranges called the Alpide belt, from Gibraltar through the Himalayas to Indonesia—a process that began at the end of the Mesozoic and continues into the present. The formation of the Alps was a segment of this orogenic process, caused by the collision between the African and the Eurasian plates that began in the late Cretaceous Period.
Under extreme compressive stresses and pressure, marine sedimentary rocks were uplifted, creating characteristic recumbent folds, or nappes, and thrust faults. As the rising peaks underwent erosion, a layer of marine flysch sediments was deposited in the foreland basin, and the sediments became involved in younger nappes (folds) as the orogeny progressed. Coarse sediments from the continual uplift and erosion were later deposited in foreland areas as molasse. The molasse regions in Switzerland and Bavaria were well-developed and saw further upthrusting of flysch.
The Alpine orogeny occurred in ongoing cycles through to the Paleogene causing differences in nappe structures, with a late-stage orogeny causing the development of the Jura Mountains. A series of tectonic events in the Triassic, Jurassic and Cretaceous periods caused different paleogeographic regions. The Alps are subdivided by different lithology (rock composition) and nappe structure according to the orogenic events that affected them. The geological subdivision differentiates the Western, Eastern Alps and Southern Alps: the Helveticum in the north, the Penninicum and Austroalpine system in the center and, south of the Periadriatic Seam, the Southern Alpine system.
According to geologist Stefan Schmid, because the Western Alps underwent a metamorphic event in the Cenozoic Era while the Austroalpine peaks underwent an event in the Cretaceous Period, the two areas show distinct differences in nappe formations. Flysch deposits in the Southern Alps of Lombardy probably occurred in the Cretaceous or later.
Peaks in France, Italy and Switzerland lie in the "Houillière zone", which consists of basement with sediments from the Mesozoic Era. High "massifs" with external sedimentary cover are more common in the Western Alps and were affected by Neogene Period thin-skinned thrusting whereas the Eastern Alps have comparatively few high peaked massifs. Similarly the peaks in Switzerland extending to western Austria (Helvetic nappes) consist of thin-skinned sedimentary folding that detached from former basement rock.
In simple terms the structure of the Alps consists of layers of rock of European, African and oceanic (Tethyan) origin. The bottom nappe structure is of continental European origin, above which are stacked marine sediment nappes, topped off by nappes derived from the African plate. The Matterhorn is an example of the ongoing orogeny and shows evidence of great folding. The tip of the mountain consists of gneisses from the African plate; the base of the peak, below the glaciated area, consists of European basement rock. The sequence of Tethyan marine sediments and their oceanic basement is sandwiched between rock derived from the African and European plates.
The core regions of the Alpine orogenic belt have been folded and fractured in such a manner that erosion created the characteristic steep vertical peaks of the Swiss Alps that rise seemingly straight out of the foreland areas. Peaks such as Mont Blanc, the Matterhorn, and high peaks in the Pennine Alps, the Briançonnais, and Hohe Tauern consist of layers of rock from the various orogenies including exposures of basement rock.
The Union Internationale des Associations d'Alpinisme (UIAA) has defined a list of 82 "official" Alpine summits that reach at least 4,000 m (13,123 ft). The list includes not only mountains, but also subpeaks with little prominence that are considered important mountaineering objectives. Below are listed the 22 "four-thousanders" with at least 500 m (1,640 ft) of prominence.
While Mont Blanc was first climbed in 1786, most of the Alpine four-thousanders were climbed during the first half of the 19th century; the ascent of the Matterhorn in 1865 marked the end of the golden age of alpinism. Karl Blodig (1859–1956) was among the first to successfully climb all the major 4,000 m peaks. He completed his series of ascents in 1911.
The first British Mont Blanc ascent was in 1788; the first female ascent in 1819. By the mid-1850s Swiss mountaineers had ascended most of the peaks and were eagerly sought as mountain guides. Edward Whymper reached the top of the Matterhorn in 1865 (after seven attempts), and in 1938 the last of the six great north faces of the Alps was climbed with the first ascent of the Eiger Nordwand (north face of the Eiger).
The Alps are a source of minerals that have been mined for thousands of years. In the 8th to 6th centuries BC during the Hallstatt culture, Celtic tribes mined copper; later the Romans mined gold for coins in the Bad Gastein area. Erzberg in Styria furnishes high-quality iron ore for the steel industry. Crystals are found throughout much of the Alpine region such as cinnabar, amethyst, and quartz. The cinnabar deposits in Slovenia are a notable source of cinnabar pigments.
Alpine crystals have been studied and collected for hundreds of years, and began to be classified in the 18th century. Leonhard Euler studied the shapes of crystals, and by the 19th century crystal hunting was common in Alpine regions. David Friedrich Wiser amassed a collection of 8000 crystals that he studied and documented. In the 20th century Robert Parker wrote a well-known work about the rock crystals of the Swiss Alps; at the same period a commission was established to control and standardize the naming of Alpine minerals.
In the Miocene Epoch the mountains underwent severe erosion because of glaciation, which was noted in the mid-19th century by naturalist Louis Agassiz who presented a paper proclaiming the Alps were covered in ice at various intervals—a theory he formed when studying rocks near his Neuchâtel home which he believed originated to the west in the Bernese Oberland. Because of his work he came to be known as the "father of the ice-age concept" although other naturalists before him put forth similar ideas.
Agassiz studied glacier movement in the 1840s at the Unteraar Glacier where he found the glacier moved 100 m (328 ft) per year, more rapidly in the middle than at the edges. His work was continued by other scientists and now a permanent laboratory exists inside a glacier under the Jungfraujoch, devoted exclusively to the study of Alpine glaciers.
Glaciers pick up rocks and sediment with them as they flow. This causes erosion and the formation of valleys over time. The Inn valley is an example of a valley carved by glaciers during the ice ages with a typical terraced structure caused by erosion. Eroded rocks from the most recent ice age lie at the bottom of the valley while the top of the valley consists of erosion from earlier ice ages. Glacial valleys have characteristically steep walls (reliefs); valleys with lower reliefs and talus slopes are remnants of glacial troughs or previously infilled valleys. Moraines, piles of rock picked up during the movement of the glacier, accumulate at edges, center and the terminus of glaciers.
Alpine glaciers can be straight rivers of ice, long sweeping rivers, spread in a fan-like shape (Piedmont glaciers), and curtains of ice that hang from vertical slopes of the mountain peaks. The stress of the movement causes the ice to break and crack loudly, perhaps explaining why the mountains were believed to be home to dragons in the medieval period. The cracking creates unpredictable and dangerous crevasses, often invisible under new snowfall, which cause the greatest danger to mountaineers.
Glaciers end in ice caves (the Rhone Glacier), by trailing into a lake or river, or by shedding snowmelt on a meadow. Sometimes a piece of glacier will detach or break resulting in flooding, property damage and loss of life. In the 17th century about 2500 people were killed by an avalanche in a village on the French-Italian border; in the 19th century 120 homes in a village near Zermatt were destroyed by an avalanche.
High levels of precipitation cause the glaciers to descend to permafrost levels in some areas whereas in other, more arid regions, glaciers remain above about the 3,500 m (11,483 ft) level. The 1,817 square kilometres (702 sq mi) of the Alps covered by glaciers in 1876 had shrunk to 1,342 km2 (518 sq mi) by 1973, resulting in decreased river run-off levels. Forty percent of the glaciation in Austria has disappeared since 1850, and 30% of that in Switzerland.
The Alps provide lowland Europe with drinking water, irrigation, and hydroelectric power. Although the area is only about 11 percent of the surface area of Europe, the Alps provide up to 90 percent of water to lowland Europe, particularly to arid areas and during the summer months. Cities such as Milan depend on 80 percent of water from Alpine runoff. Water from the rivers is used in over 500 hydroelectricity power plants, generating as much as 2900 kilowatts of electricity.
Major European rivers flow from Switzerland, such as the Rhine, the Rhone, the Inn, the Ticino and the Po, all of which have headwaters in the Alps and flow into neighbouring countries, finally emptying into the North Sea, the Mediterranean Sea, the Adriatic Sea and the Black Sea. Other rivers such as the Danube have major tributaries flowing into them that originate in the Alps. The Rhone is second to the Nile as a freshwater source to the Mediterranean Sea; the river begins as glacial meltwater, flows into Lake Geneva, and from there to France where one of its uses is to cool nuclear power plants. The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country. Tributary valleys, some of which are complicated, channel water to the main valleys which can experience flooding during the snow melt season when rapid runoff causes debris torrents and swollen rivers.
The rivers form lakes, such as Lake Geneva, a crescent shaped lake crossing the Swiss border with Lausanne on the Swiss side and the town of Evian-les-Bains on the French side. In Germany, the medieval St. Bartholomew's chapel was built on the south side of the Königssee, accessible only by boat or by climbing over the abutting peaks.
Scientists have been studying the impact of climate change and water use. For example, each year more water is diverted from rivers for snowmaking in the ski resorts, the effect of which is yet unknown. Furthermore, the decrease of glaciated areas combined with a succession of winters with lower-than-expected precipitation may have a future impact on the rivers in the Alps as well as an effect on the water availability to the lowlands.
The Alps are a classic example of what happens when a temperate area at lower altitude gives way to higher-elevation terrain. Elevations around the world that have cold climates similar to those of the polar regions have been called Alpine. A rise from sea level into the upper regions of the atmosphere causes the temperature to decrease (see adiabatic lapse rate). The effect of mountain chains on prevailing winds is to carry warm air belonging to the lower region into an upper zone, where it expands in volume at the cost of a proportionate loss of temperature, often accompanied by precipitation in the form of snow or rain. The height of the Alps is sufficient to divide the weather patterns in Europe into a wet north and a dry south because moisture is sucked from the air as it flows over the high peaks.
The severe weather in the Alps has been studied since the 18th century; particularly the weather patterns such as the seasonal foehn wind. Numerous weather stations were placed in the mountains early in the early 20th century, providing continuous data for climatologists. Some of the valleys are quite arid such as the Aosta valley in Italy, the Maurienne in France, the Valais in Switzerland, and northern Tyrol.
The areas that are not arid and receive high precipitation experience periodic flooding from rapid snowmelt and runoff. The mean precipitation in the Alps ranges from a low of 2,600 mm (100 in) per year to 3,600 mm (140 in) per year, with the higher levels occurring at high altitudes. At altitudes between 1,000 and 3,000 m (3,281 and 9,843 ft), snowfall begins in November and accumulates through to April or May when the melt begins. Snow lines vary from 2,400 to 3,000 m (7,874 to 9,843 ft), above which the snow is permanent and the temperatures hover around the freezing point even July and August. High-water levels in streams and rivers peak in June and July when the snow is still melting at the higher altitudes.
The Alps are split into five climatic zones, each with different vegetation. The climate, plant life and animal life vary among the different sections or zones of the mountains. The lowest zone is the colline zone, which exists between 500 and 1,000 m (1,640 and 3,281 ft), depending on the location. The montane zone extends from 800 to 1,700 m (2,625 to 5,577 ft), followed by the sub-Alpine zone from 1,600 to 2,400 m (5,249 to 7,874 ft). The Alpine zone, extending from tree line to snow line, is followed by the glacial zone, which covers the glaciated areas of the mountain. Climatic conditions show variances within the same zones; for example, weather conditions at the head of a mountain valley, extending directly from the peaks, are colder and more severe than those at the mouth of a valley which tend to be less severe and receive less snowfall.
Various models of climate change have been projected into the 22nd century for the Alps, with an expectation that a trend toward increased temperatures will have an effect on snowfall, snowpack, glaciation, and river runoff.
Thirteen thousand species of plants have been identified in the Alpine regions. Alpine plants are grouped by habitat and soil type which can be limestone or non-calcerous. The habitats range from meadows, bogs, woodland (deciduous and coniferous) areas to soilless scree and moraines, and rock faces and ridges. A natural vegetation limit with altitude is given by the presence of the chief deciduous trees—oak, beech, ash and sycamore maple. These do not reach exactly to the same elevation, nor are they often found growing together; but their upper limit corresponds accurately enough to the change from a temperate to a colder climate that is further proved by a change in the presence of wild herbaceous vegetation. This limit usually lies about 1,200 m (3,940 ft) above the sea on the north side of the Alps, but on the southern slopes it often rises to 1,500 m (4,920 ft), sometimes even to 1,700 m (5,580 ft).
Above the forestry, there is often a band of short pine trees (Pinus mugo), which is in turn superseded by Alpenrosen, dwarf shrubs, typically Rhododendron ferrugineum (on acid soils) or Rhododendron hirsutum (on alkaline soils). Although the Alpenrose prefers acidic soil, the plants are found throughout the region. Above the tree line is the area defined as "alpine" where in the alpine meadow plants are found that have adapted well to harsh conditions of cold temperatures, aridity, and high altitudes. The alpine area fluctuates greatly because of regional fluctuations in tree lines.
Alpine plants such the Alpine gentian grow in abundance in areas such as the meadows above the Lauterbrunnental. Gentians are named after the Illyrian king Gentius, and 40 species of the early-spring blooming flower grow in the Alps, in a range of 1,500 to 2,400 m (4,921 to 7,874 ft). Writing about the gentians in Switzerland D. H. Lawrence described them as "darkening the day-time, torch-like with the smoking blueness of Pluto's gloom." Gentians tend to "appear" repeatedly as the spring blooming takes place at progressively later dates, moving from the lower altitude to the higher altitude meadows where the snow melts much later than in the valleys. On the highest rocky ledges the spring flowers bloom in the summer.
At these higher altitudes, the plants tend to form isolated cushions. In the Alps, several species of flowering plants have been recorded above 4,000 m (13,120 ft), including Ranunculus glacialis, Androsace alpina and Saxifraga biflora. The Eritrichium nanum, commonly known as the King of the Alps, is the most elusive of the alpine flowers, growing on rocky ridges at 2,600 to 3,750 m (8,530 to 12,303 ft). Perhaps the best known of the alpine plants is the Edelweiss which grows in rocky areas and can be found at altitudes as low as 1,400 m (4,593 ft) and as high as 3,400 m (11,155 ft). The plants that grow at the highest altitudes have adapted to conditions by specialization such as growing in rock screes that give protection from winds.
The extreme and stressful climatic conditions give way to the growth of plant species with secondary metabolites important for medicinal purposes. Origanum vulgare, Prunella vulgaris, Solanum nigrum and Urtica dioica are some of the more useful medicinal species found in the Alps.
Human interference has nearly exterminated the trees in many areas, and, except for the beech forests of the Austrian Alps, forests of deciduous trees are rarely found after the extreme deforestation between the 17th and 19th centuries. The vegetation has changed since the second half of the 20th century, as the high alpine meadows cease to be harvested for hay or used for grazing which eventually might result in a regrowth of forest. In some areas the modern practice of building ski runs by mechanical means has destroyed the underlying tundra from which the plant life cannot recover during the non-skiing months, whereas areas that still practice a natural piste type of ski slope building preserve the fragile underlayers.
The Alps are a habitat for 30,000 species of wildlife, ranging from the tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.
The largest mammal to live in the highest altitudes are the alpine ibex, which have been sighted as high as 3,000 m (9,843 ft). The ibex live in caves and descend to eat the succulent alpine grasses. Classified as antelopes, chamois are smaller than ibex and found throughout the Alps, living above the tree line and are common in the entire alpine range. Areas of the eastern Alps are still home to brown bears. In Switzerland the canton of Bern was named for the bears but the last bear is recorded as having been killed in 1792 above Kleine Scheidegg by three hunters from Grindelwald.
Many rodents such as voles live underground. Marmots live almost exclusively above the tree line as high as 2,700 m (8,858 ft). They hibernate in large groups to provide warmth, and can be found in all areas of the Alps, in large colonies they build beneath the alpine pastures. Golden eagles and bearded vultures are the largest birds to be found in the Alps; they nest high on rocky ledges and can be found at altitudes of 2,400 m (7,874 ft). The most common bird is the alpine chough which can be found scavenging at climber's huts or at the Jungfraujoch, a high altitude tourist destination.
Reptiles such as adders and vipers live up to the snow line; because they cannot bear the cold temperatures they hibernate underground and soak up the warmth on rocky ledges. The high-altitude Alpine salamanders have adapted to living above the snow line by giving birth to fully developed young rather than laying eggs. Brown trout can be found in the streams up to the snow line. Molluscs such as the wood snail live up the snow line. Popularly gathered as food, the snails are now protected.
A number of species of moths live in the Alps, some of which are believed to have evolved in the same habitat up to 120 million years ago, long before the Alps were created. Blue moths can commonly be seen drinking from the snow melt; some species of blue moths fly as high as 1,800 m (5,906 ft). The butterflies tend to be large, such as those from the swallowtail Parnassius family, with a habitat that ranges to 1,800 m (5,906 ft). Twelve species of beetles have habitats up to the snow line; the most beautiful and formerly collected for its colours but now protected is the Rosalia alpina. Spiders, such as the large wolf spider, live above the snow line and can be seen as high as 400 m (1,312 ft). Scorpions can be found in the Italian Alps.
Some of the species of moths and insects show evidence of having been indigenous to the area from as long ago as the Alpine orogeny. In Emosson in Valais, Switzerland, dinosaur tracks were found in the 1970s, dating probably from the Triassic Period.
About 10,000 years ago, when the ice melted after the last glacial period, late Paleolithic communities were established along the lake shores and in cave systems. Evidence of human habitation has been found in caves near Vercors, close to Grenoble; in Austria the Mondsee culture shows evidence of houses built on piles to keep them dry. Standing stones have been found in Alpine areas of France and Italy. The rock drawings in Valcamonica are more than 5000 years old; more than 200,000 drawings and etchings have been identified at the site.
In 1991 a mummy of a neolithic body, known as Ötzi the Iceman, was discovered by hikers on the Similaun glacier. His clothing and gear indicate that he lived in an alpine farming community, while the location and manner of his death - an arrowhead was discovered in his shoulder - suggests he was travelling from one place to another. Analysis of the mitochondrial DNA of Ötzi, has shown that he belongs to the K1 subclade which cannot be categorized into any of the three modern branches of that subclade. The new subclade has provisionally been named K1ö for Ötzi.
Celtic tribes settled in Switzerland between 1000 to 1500 BC. The Raetians lived in the eastern regions, while the west was occupied by the Helvetii and the Allobrogi settled in the Rhone valley and in Savoy. Among the many substances Celtic tribes mined was salt in areas such as Salzburg in Austria where evidence of the Hallstatt culture was found by a mine manager in the 19th century. By the 6th century BC the La Tène culture was well established in the region, and became known for high quality decorated weapons and jewelry. The Celts were the most widespread of the mountain tribes—they had warriors that were strong, tall and fair skinned skilled with iron weapons, which gave them an advantage in warfare.
During the Second Punic War in 218 BC, the Carthaginian general Hannibal probably crossed the Alps with an army numbering 38,000 infantry, 8,000 cavalry, and 37 war elephants. This was one of the most celebrated achievements of any military force in ancient warfare, although no evidence exists of the actual crossing or the place of crossing. The Romans, however, had built roads along the mountain passes, which continued to be used through the medieval period to cross the mountains and Roman road markers can still be found on the mountain passes.
The Roman expansion brought the defeat of the Allobrogi in 121 BC and during the Gallic Wars in 58 BC Julius Caesar overcame the Helvetii. The Rhaetians continued to resist but were eventually conquered when the Romans turned northward to the Danube valley in Austria and defeated the Brigantes. The Romans built settlements in the Alps; towns such as Aosta (named for Augustus) in Italy, Martigny and Lausanne in Switzerland, and Partenkirchen in Bavaria show remains of Roman baths, villas, arenas and temples. Much of the Alpine region was gradually settled by Germanic tribes, (Lombards, Alemanni, Bavarii, and Franks) from the 6th to the 13th centuries mixing with the local Celtic tribes.
Christianity was established in the region by the Romans, and saw the establishment of monasteries and churches in the high regions. The Frankish expansion of the Carolingian Empire and the Bavarian expansion in the eastern Alps introduced feudalism and the building of castles to support the growing number of dukedoms and kingdoms. Castello del Buonconsiglio in Trento, Italy, still has intricate frescoes, excellent examples of Gothic art, in a tower room. In Switzerland, Château de Chillon is preserved as an example of medieval architecture.
Much of the medieval period was a time of power struggles between competing dynasties such as the House of Savoy, the Visconti in northern Italy and the House of Habsburg in Austria and Slovenia. In 1291 to protect themselves from incursions by the Habsburgs, four cantons in the middle of Switzerland drew up a charter that is considered to be a declaration of independence from neighboring kingdoms. After a series of battles fought in the 13th, 14th and 15th centuries, more cantons joined the confederacy and by the 16th century Switzerland was well-established as a separate state.
During the Napoleonic Wars in the late 18th century and early 19th century, Napoleon annexed territory formerly controlled by the Habsburgs and Savoys. In 1798 he established the Helvetic Republic in Switzerland; two years later he led an army across the St. Bernard pass and conquered almost all of the Alpine regions.
After the fall of Napoléon, many alpine countries developed heavy protections to prevent any new invasion. Thus, Savoy built a series of fortifications in the Maurienne valley in order to protect the major alpine passes, such as the col du Mont-Cenis that was even crossed by, Charlemagne and his father to defeat the Lombarts. The later indeed became very popular after the construction of a paved road ordered by Napoléon Bonaparte. The Barrière de l'Esseillon is a serie of forts with heavy batteries, built on a cliff with a perfect view on the valley, a gorge on one side and steep mountains on the other side.
In the 19th century, the monasteries built in the high Alps during the medieval period to shelter travelers and as places of pilgrimage, became tourist destinations. The Benedictines had built monasteries in Lucerne, Switzerland, and Oberammergau; the Cistercians in the Tyrol and at Lake Constance; and the Augustinians had abbeys in the Savoy and one in the center of Interlaken, Switzerland. The Great St Bernard Hospice, built in the 9th or 10th centuries, at the summit of the Great Saint Bernard Pass was shelter for travelers and place for pilgrims since its inception; by the 19th century it became a tourist attraction with notable visitors such as author Charles Dickens and mountaineer Edward Whymper.
Radiocarbon dated charcoal placed around 50,000 years ago was found in the Drachloch (Dragon's Hole) cave above the village of Vattis in the canton of St. Gallen, proving that the high peaks were visited by prehistoric people. Seven bear skulls from the cave may have been buried by the same prehistoric people. The peaks, however, were mostly ignored except for a few notable examples, and long left to the exclusive attention of the people of the adjoining valleys. The mountain peaks were seen as terrifying, the abode of dragons and demons, to the point that people blindfolded themselves to cross the Alpine passes. The glaciers remained a mystery and many still believed the highest areas to be inhabited by dragons.
Charles VII of France ordered his chamberlain to climb Mont Aiguille in 1356. The knight reached the summit of Rocciamelone where he left a bronze triptych of three crosses, a feat which he conducted with the use of ladders to traverse the ice. In 1492 Antoine de Ville climbed Mont Aiguille, without reaching the summit, an experience he described as "horrifying and terrifying." Leonardo da Vinci was fascinated by variations of light in the higher altitudes, and climbed a mountain—scholars are uncertain which one; some believe it may have been Monte Rosa. From his description of a "blue like that of a gentian" sky it is thought that he reached a significantly high altitude. In the 18th century four Chamonix man almost made the summit of Mont Blanc but were overcome by altitude sickness and snowblindness.
Conrad Gessner was the first naturalist to ascend the mountains in the 16th century, to study them, writing that in the mountains he found the "theatre of the Lord". By the 19th century more naturalists began to arrive to explore, study and conquer the high peaks; they were followed by artists, writers and painters. Two men who first explored the regions of ice and snow were Horace-Bénédict de Saussure (1740–1799) in the Pennine Alps, and the Benedictine monk of Disentis Placidus a Spescha (1752–1833). Born in Geneva, Saussure was enamored with the mountains from an early age; he left a law career to become a naturalist and spent many years trekking through the Bernese Oberland, the Savoy, the Piedmont and Valais, studying the glaciers and the geology, as he became an early proponent of the theory of rock upheaval. Saussure, in 1787, was a member of the third ascent of Mont Blanc—today the summits of all the peaks have been climbed.
Jean-Jacques Rousseau was the first of many to present the Alps as a place of allure and beauty, banishing the prevalent conception of the mountains as a hellish wasteland inhabited by demons. Rousseau's conception of alpine purity was later emphasized with the publication of Albrecht von Haller's poem Die Alpen that described the mountains as an area of mythical purity. Late in the 18th century the first wave of Romantics such as Goethe and Turner came to admire the scenery; Wordsworth visited the area in 1790, writing of his experiences in The Prelude. Schiller later wrote the play William Tell romanticising Swiss independence. After the end of the Napoleonic Wars, the Alpine countries began to see an influx of poets, artists, and musicians, as visitors came to experience the sublime effects of monumental nature.
In 1816 Byron, Percy Bysshe Shelley and his wife Mary Shelley visited Geneva and all three were inspired by the scenery in their writings. During these visits Shelley wrote the poem "Mont Blanc", Byron wrote "The Prisoner of Chillon" and the dramatic poem Manfred, and Mary Shelley, who found the scenery overwhelming, conceived the idea for the novel Frankenstein in her villa on the shores of Lake Geneva in the midst of a thunderstorm. When Coleridge travelled to Chamonix, he declaimed, in defiance of Shelley, who had signed himself "Atheos" in the guestbook of the Hotel de Londres near Montenvers, "Who would be, who could be an atheist in this valley of wonders". By the mid-19th century scientists began to arrive en masse to study the geology and ecology of the region.
Austrian-born Adolf Hitler had a lifelong romantic fascination with the Alps and by the 1930s established a home in the Obersalzberg region outside of Berchtesgaden. His first visit to the area was in 1923 and he maintained a strong tie there until the end of his life. At the end of World War II the US Army occupied Obersalzberg, to prevent Hitler from retreating with the Wehrmacht into the mountains.
By 1940 the Third Reich had occupied many of the Alpine countries. Austria underwent a political coup that made it part of the Third Reich; France had been invaded and Italy was a fascist regime. Switzerland was the only country to luckily avoid invasion. The Swiss Confederate mobilized its troops—the country follows the doctrine of "armed neutrality" with all males required to have military training—a number that General Eisenhower estimated to be about 850,000. The Swiss commanders wired the infrastructure leading into the country, and threatening to destroy bridges, railway tunnels and passes in the event of a Nazi invasion, and then they retreated to the heart of the mountain peaks where conditions were harsher and a military invasion would involve difficult and protracted battles.
Ski troops were trained for the war, and battles were waged in mountainous areas such as the battle at Riva Ridge in Italy, where the American 10th Mountain Division encountered heavy resistance in February 1945. At the end of the war, a substantial amount of Nazi plunder was found stored in Austria, where Hitler had hoped to retreat as the war drew to a close. The salt mines surrounding the Altaussee area, where American troops found 75 kilos of gold coins stored in a single mine, were used to store looted art, jewels, and currency; vast quantities of looted art were found and returned to the owners.
The population of the region is 14 million spread across eight countries. On the rim of the mountains, on the plateaus and the plains the economy consists of manufacturing and service jobs whereas in the higher altitudes and in the mountains farming is still essential to the economy. Farming and forestry continue to be mainstays of Alpine culture, industries that provide for export to the cities and maintain the mountain ecology.
Much of the Alpine culture is unchanged since the medieval period when skills that guaranteed survival in the mountain valleys and in the highest villages became mainstays, leading to strong traditions of carpentry, woodcarving, baking and pastry-making, and cheesemaking.
Farming had been a traditional occupation for centuries, although it became less dominant in the 20th century with the advent of tourism. Grazing and pasture land are limited because of the steep and rocky topography of the Alps. In mid-June cows are moved to the highest pastures close to the snowline, where they are watched by herdsmen who stay in the high altitudes often living in stone huts or wooden barns during the summers. Villagers celebrate the day the cows are herded up to the pastures and again when they return in mid-September. The Alpanschluss or Désalpes ("coming down from the alps") is celebrated by decorating the cows with garlands and enormous cowbells while the farmers dress in traditional costumes.
Cheesemaking is an ancient tradition in most Alpine countries. A wheel of cheese from the Emmental in Switzerland can weigh up to 45 kg (100 lb), and the Beaufort in Savoy can weight up to 70 kilograms (150 lb). Owners of the cows traditionally receive from the cheesemakers a portion in relation to the proportion of the cows' milk from the summer months in the high alps. Haymaking is an important farming activity in mountain villages which has become somewhat mechanized in recent years, although the slopes are so steep that usually scythes are necessary to cut the grass. Hay is normally brought in twice a year, often also on festival days. Alpine festivals vary from country to country and often include the display of local costumes such as dirndl and trachten, the playing of Alpenhorns, wrestling matches, some pagan traditions such as Walpurgis Night and, in many areas, Carnival is celebrated before Lent.
In the high villages people live in homes built according to medieval designs that withstand cold winters. The kitchen is separated from the living area (called the stube, the area of the home heated by a stove), and second-floor bedrooms benefit from rising heat. The typical Swiss chalet originated in the Bernese Oberland. Chalets often face south or downhill, and are built of solid wood, with a steeply gabled roof to allow accumulated snow to slide off easily. Stairs leading to upper levels are sometimes built on the outside, and balconies are sometimes enclosed.
Food is passed from the kitchen to the stube, where the dining room table is placed. Some meals are communal, such as fondue, where a pot is set in the middle of the table for each person to dip into. Other meals are still served in a traditional manner on carved wooden plates. Furniture has been traditionally elaborately carved and in many Alpine countries carpentry skills are passed from generation to generation.
Roofs are traditionally constructed from Alpine rocks such as pieces of schist, gneiss or slate. Such chalets are typically found in the higher parts of the valleys, as in the Maurienne valley in Savoy, where the amount of snow during the cold months is important. The inclination of the roof cannot exceed 40%, allowing the snow to stay on top, thereby functioning as insulation from the cold. In the lower areas where the forests are widespread, wooden tiles are traditionally used. Commonly made of Norway spruce, they are called "tavaillon". The Alpine regions are multicultural and linguistically diverse. Dialects are common, and vary from valley to valley and region to region. In the Slavic Alps alone 19 dialects have been identified. Some of the French dialects spoken in the French, Swiss and Italian alps of Aosta Valley derive from Arpitan, while the southern part of the western range is related to Old Provençal; the German dialects derive from Germanic tribal languages. Romansh, spoken by two percent of the population in southeast Switzerland, is an ancient Rhaeto-Romanic language derived from Latin, remnants of ancient Celtic languages and perhaps Etruscan.
At present the Alps are one of the more popular tourist destinations in the world with many resorts such Oberstdorf, in Bavaria, Saalbach in Austria, Davos in Switzerland, Chamonix in France, and Cortina d'Ampezzo in Italy recording more than a million annual visitors. With over 120 million visitors a year tourism is integral to the Alpine economy with much it coming from winter sports although summer visitors are an important component of the tourism industry.
The tourism industry began in the early 19th century when foreigners visited the Alps, traveled to the bases of the mountains to enjoy the scenery, and stayed at the spa-resorts. Large hotels were built during the Belle Époque; cog-railways, built early in the 20th century, brought tourists to ever higher elevations, with the Jungfraubahn terminating at the Jungfraujoch, well above the eternal snow-line, after going through a tunnel in Eiger. During this period winter sports were slowly introduced: in 1882 the first figure skating championship was held in St. Moritz, and downhill skiing became a popular sport with English visitors early in the 20th century, as the first ski-lift was installed in 1908 above Grindelwald.
In the first half of the 20th century the Olympic Winter Games were held three times in Alpine venues: the 1924 Winter Olympics in Chamonix, France; the 1928 Winter Olympics in St. Moritz, Switzerland; and the 1936 Winter Olympics in Garmisch-Partenkirchen, Germany. During World War II the winter games were canceled but after that time the Winter Games have been held in St. Moritz (1948), Cortina d'Ampezzo (1956), Innsbruck, Austria (1964 and 1976), Grenoble, France, (1968), Albertville, France, (1992), and Torino (2006). In 1930 the Lauberhorn Rennen (Lauberhorn Race), was run for the first time on the Lauberhorn above Wengen; the equally demanding Hahnenkamm was first run in the same year in Kitzbühl, Austria. Both races continue to be held each January on successive weekends. The Lauberhorn is the more strenuous downhill race at 4.5 km (2.8 mi) and poses danger to racers who reach 130 km/h (81 mph) within seconds of leaving the start gate.
During the post-World War I period ski-lifts were built in Swiss and Austrian towns to accommodate winter visitors, but summer tourism continued to be important; by the mid-20th century the popularity of downhill skiing increased greatly as it became more accessible and in the 1970s several new villages were built in France devoted almost exclusively to skiing, such as Les Menuires. Until this point Austria and Switzerland had been the traditional and more popular destinations for winter sports, but by the end of the 20th century and into the early 21st century, France, Italy and the Tyrol began to see increases in winter visitors. From 1980 to the present, ski-lifts have been modernized and snow-making machines installed at many resorts, leading to concerns regarding the loss of traditional Alpine culture and questions regarding sustainable development as the winter ski industry continues to develop quickly and the number of summer tourists decline.
The region is serviced by 4,200 km (2,600 mi) of roads used by 6 million vehicles. Train travel is well established in the Alps, with, for instance 120 km (75 mi) of track for every 1,000 km2 (390 sq mi) in a country such as Switzerland. Most of Europe's highest railways are located there. Moreover, plans are underway to build a 57 km (35 mi)-long sub-alpine tunnel connecting the older Lötschberg and Gotthard tunnels built in the 19th century.
Some high mountain villages, such as Avoriaz (in France), Wengen, and Zermatt (in Switzerland) are accessible only by cable car or cog-rail trains, and are car free. Other villages in the Alps are considering becoming car free zones or limiting the number of cars for reasons of sustainability of the fragile Alpine terrain.
The lower regions and larger towns of the Alps are well-served by motorways and main roads, but higher mountain passes and byroads, which are amongst the highest in Europe, can be treacherous even in summer due to steep slopes. Many passes are closed in winter. A multitude of airports around the Alps (and some within), as well as long-distance rail links from all neighbouring countries, afford large numbers of travellers easy access from abroad.
Immunology is a branch of biomedical science that covers the study of immune systems in all organisms. It charts, measures, and contextualizes the: physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (such as autoimmune diseases, hypersensitivities, immune deficiency, and transplant rejection); the physical, chemical and physiological characteristics of the components of the immune system in vitro, in situ, and in vivo. Immunology has applications in numerous disciplines of medicine, particularly in the fields of organ transplantation, oncology, virology, bacteriology, parasitology, psychiatry, and dermatology.
Prior to the designation of immunity from the etymological root immunis, which is Latin for "exempt"; early physicians characterized organs that would later be proven as essential components of the immune system. The important lymphoid organs of the immune system are the thymus and bone marrow, and chief lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and liver. When health conditions worsen to emergency status, portions of immune system organs including the thymus, spleen, bone marrow, lymph nodes and other lymphatic tissues can be surgically excised for examination while patients are still alive.
Many components of the immune system are typically cellular in nature and not associated with any specific organ; but rather are embedded or circulating in various tissues located throughout the body.
Classical immunology ties in with the fields of epidemiology and medicine. It studies the relationship between the body systems, pathogens, and immunity. The earliest written mention of immunity can be traced back to the plague of Athens in 430 BCE. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. Many other ancient societies have references to this phenomenon, but it was not until the 19th and 20th centuries before the concept developed into scientific theory.
The study of the molecular and cellular components that comprise the immune system, including their function and interaction, is the central science of immunology. The immune system has been divided into a more primitive innate immune system and, in vertebrates, an acquired or adaptive immune system. The latter is further divided into humoral (or antibody) and cell-mediated components.
The humoral (antibody) response is defined as the interaction between antibodies and antigens. Antibodies are specific proteins released from a certain class of immune cells known as B lymphocytes, while antigens are defined as anything that elicits the generation of antibodies ("anti"body "gen"erators). Immunology rests on an understanding of the properties of these two biological entities and the cellular response to both.
Immunological research continues to become more specialized, pursuing non-classical models of immunity and functions of cells, organs and systems not previously associated with the immune system (Yemeserach 2010).
Clinical immunology is the study of diseases caused by disorders of the immune system (failure, aberrant action, and malignant growth of the cellular elements of the system). It also involves diseases of other systems, where immune reactions play a part in the pathology and clinical features.
Other immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.
The most well-known disease that affects the immune system itself is AIDS, an immunodeficiency characterized by the suppression of CD4+ ("helper") T cells, dendritic cells and macrophages by the Human Immunodeficiency Virus (HIV).
The body’s capability to react to antigen depends on a person's age, antigen type, maternal factors and the area where the antigen is presented. Neonates are said to be in a state of physiological immunodeficiency, because both their innate and adaptive immunological responses are greatly suppressed. Once born, a child’s immune system responds favorably to protein antigens while not as well to glycoproteins and polysaccharides. In fact, many of the infections acquired by neonates are caused by low virulence organisms like Staphylococcus and Pseudomonas. In neonates, opsonic activity and the ability to activate the complement cascade is very limited. For example, the mean level of C3 in a newborn is approximately 65% of that found in the adult. Phagocytic activity is also greatly impaired in newborns. This is due to lower opsonic activity, as well as diminished up-regulation of integrin and selectin receptors, which limit the ability of neutrophils to interact with adhesion molecules in the endothelium. Their monocytes are slow and have a reduced ATP production, which also limits the newborn's phagocytic activity. Although, the number of total lymphocytes is significantly higher than in adults, the cellular and humoral immunity is also impaired. Antigen-presenting cells in newborns have a reduced capability to activate T cells. Also, T cells of a newborn proliferate poorly and produce very small amounts of cytokines like IL-2, IL-4, IL-5, IL-12, and IFN-g which limits their capacity to activate the humoral response as well as the phagocitic activity of macrophage. B cells develop early during gestation but are not fully active.
Maternal factors also play a role in the body’s immune response. At birth, most of the immunoglobulin present is maternal IgG. Because IgM, IgD, IgE and IgA don’t cross the placenta, they are almost undetectable at birth. Some IgA is provided by breast milk. These passively-acquired antibodies can protect the newborn for up to 18 months, but their response is usually short-lived and of low affinity. These antibodies can also produce a negative response. If a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response. Passively acquired maternal antibodies can suppress the antibody response to active immunization. Similarly the response of T-cells to vaccination differs in children compared to adults, and vaccines that induce Th1 responses in adults do not readily elicit these same responses in neonates. Between six to nine months after birth, a child’s immune system begins to respond more strongly to glycoproteins, but there is usually no marked improvement in their response to polysaccharides until they are at least one year old. This can be the reason for distinct time frames found in vaccination schedules.
During adolescence, the human body undergoes various physical, physiological and immunological changes triggered and mediated by hormones, of which the most significant in females is 17-β-oestradiol (an oestrogen) and, in males, is testosterone. Oestradiol usually begins to act around the age of 10 and testosterone some months later. There is evidence that these steroids act directly not only on the primary and secondary sexual characteristics but also have an effect on the development and regulation of the immune system, including an increased risk in developing pubescent and post-pubescent autoimmunity. There is also some evidence that cell surface receptors on B cells and macrophages may detect sex hormones in the system.
Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between "cellular" and "humoral" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held by Robert Koch and Emil von Behring, among others, stated that the active immune agents were soluble components (molecules) found in the organism’s “humors” rather than its cells.
In the mid-1950s, Frank Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: "self" constituents (constituents of the body) do not trigger destructive immune responses, while "nonself" entities (e.g., pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex "two-signal" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.
Bioscience is the overall major in which undergraduate students who are interested in general well-being take in college. Immunology is a branch of bioscience for undergraduate programs but the major gets specified as students move on for graduate program in immunology. The aim of immunology is to study the health of humans and animals through effective yet consistent research, (AAAAI, 2013). The most important thing about being immunologists is the research because it is the biggest portion of their jobs.
Most graduate immunology schools follow the AAI courses immunology which are offered throughout numerous schools in the United States. For example, in New York State, there are several universities that offer the AAI courses immunology: Albany Medical College, Cornell University, Icahn School of Medicine at Mount Sinai, New York University Langone Medical Center, University at Albany (SUNY), University at Buffalo (SUNY), University of Rochester Medical Center and Upstate Medical University (SUNY). The AAI immunology courses include an Introductory Course and an Advance Course. The Introductory Course is a course that gives students an overview of the basics of immunology.
In addition, this Introductory Course gives students more information to complement general biology or science training. It also has two different parts: Part I is an introduction to the basic principles of immunology and Part II is a clinically-oriented lecture series. On the other hand, the Advanced Course is another course for those who are willing to expand or update their understanding of immunology. It is advised for students who want to attend the Advanced Course to have a background of the principles of immunology. Most schools require students to take electives in other to complete their degrees. A Master’s degree requires two years of study following the attainment of a bachelor's degree. For a doctoral programme it is required to take two additional years of study.

A gene is a locus (or region) of DNA that encodes a functional RNA or protein product, and is the molecular unit of heredity.:Glossary The transmission of genes to an organism's offspring is the basis of the inheritance of phenotypic traits. Most biological traits are under the influence of polygenes (many different genes) as well as the gene–environment interactions. Some genetic traits are instantly visible, such as eye colour or number of limbs, and some are not, such as blood type, risk for specific diseases, or the thousands of basic biochemical processes that comprise life.
Genes can acquire mutations in their sequence, leading to different variants, known as alleles, in the population. These alleles encode slightly different versions of a protein, which cause different phenotype traits. Colloquial usage of the term "having a gene" (e.g., "good genes," "hair colour gene") typically refers to having a different allele of the gene. Genes evolve due to natural selection or survival of the fittest of the alleles.
The concept of a gene continues to be refined as new phenomena are discovered. For example, regulatory regions of a gene can be far removed from its coding regions, and coding regions can be split into several exons. Some viruses store their genome in RNA instead of DNA and some gene products are functional non-coding RNAs. Therefore, a broad, modern working definition of a gene is any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product or by regulation of gene expression.
The existence of discrete inheritable units was first suggested by Gregor Mendel (1822–1884). From 1857 to 1864, he studied inheritance patterns in 8000 common edible pea plants, tracking distinct traits from parent to offspring. He described these mathematically as 2n combinations where n is the number of differing characteristics in the original peas. Although he did not use the term gene, he explained his results in terms of discrete inherited units that give rise to observable physical characteristics. This description prefigured the distinction between genotype (the genetic material of an organism) and phenotype (the visible traits of that organism). Mendel was also the first to demonstrate independent assortment, the distinction between dominant and recessive traits, the distinction between a heterozygote and homozygote, and the phenomenon of discontinuous inheritance.
Prior to Mendel's work, the dominant theory of heredity was one of blending inheritance, which suggested that each parent contributed fluids to the fertilisation process and that the traits of the parents blended and mixed to produce the offspring. Charles Darwin developed a theory of inheritance he termed pangenesis, which used the term gemmule to describe hypothetical particles that would mix during reproduction. Although Mendel's work was largely unrecognized after its first publication in 1866, it was 'rediscovered' in 1900 by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak, who claimed to have reached similar conclusions in their own research.
The word gene is derived (via pangene) from the Ancient Greek word γένος (génos) meaning "race, offspring". Gene was coined in 1909 by Danish botanist Wilhelm Johannsen to describe the fundamental physical and functional unit of heredity, while the related word genetics was first used by William Bateson in 1905.
Advances in understanding genes and inheritance continued throughout the 20th century. Deoxyribonucleic acid (DNA) was shown to be the molecular repository of genetic information by experiments in the 1940s to 1950s. The structure of DNA was studied by Rosalind Franklin using X-ray crystallography, which led James D. Watson and Francis Crick to publish a model of the double-stranded DNA molecule whose paired nucleotide bases indicated a compelling hypothesis for the mechanism of genetic replication. Collectively, this body of research established the central dogma of molecular biology, which states that proteins are translated from RNA, which is transcribed from DNA. This dogma has since been shown to have exceptions, such as reverse transcription in retroviruses. The modern study of genetics at the level of DNA is known as molecular genetics.
In 1972, Walter Fiers and his team at the University of Ghent were the first to determine the sequence of a gene: the gene for Bacteriophage MS2 coat protein. The subsequent development of chain-termination DNA sequencing in 1977 by Frederick Sanger improved the efficiency of sequencing and turned it into a routine laboratory tool. An automated version of the Sanger method was used in early phases of the Human Genome Project.
The theories developed in the 1930s and 1940s to integrate molecular genetics with Darwinian evolution are called the modern evolutionary synthesis, a term introduced by Julian Huxley. Evolutionary biologists subsequently refined this concept, such as George C. Williams' gene-centric view of evolution. He proposed an evolutionary concept of the gene as a unit of natural selection with the definition: "that which segregates and recombines with appreciable frequency.":24 In this view, the molecular gene transcribes as a unit, and the evolutionary gene inherits as a unit. Related ideas emphasizing the centrality of genes in evolution were popularized by Richard Dawkins.
The vast majority of living organisms encode their genes in long strands of DNA (deoxyribonucleic acid). DNA consists of a chain made from four types of nucleotide subunits, each composed of: a five-carbon sugar (2'-deoxyribose), a phosphate group, and one of the four bases adenine, cytosine, guanine, and thymine.:2.1
Two chains of DNA twist around each other to form a DNA double helix with the phosphate-sugar backbone spiralling around the outside, and the bases pointing inwards with adenine base pairing to thymine and guanine to cytosine. The specificity of base pairing occurs because adenine and thymine align form two hydrogen bonds, whereas cytosine and guanine form three hydrogen bonds. The two strands in a double helix must therefore be complementary, with their sequence of bases matching such that the adenines of one strand are paired with the thymines of the other strand, and so on.:4.1
Due to the chemical composition of the pentose residues of the bases, DNA strands have directionality. One end of a DNA polymer contains an exposed hydroxyl group on the deoxyribose; this is known as the 3' end of the molecule. The other end contains an exposed phosphate group; this is the 5' end. The two strands of a double-helix run in opposite directions. Nucleic acid synthesis, including DNA replication and transcription occurs in the 5'→3' direction, because new nucleotides are added via a dehydration reaction that uses the exposed 3' hydroxyl as a nucleophile.:27.2
The expression of genes encoded in DNA begins by transcribing the gene into RNA, a second type of nucleic acid that is very similar to DNA, but whose monomers contain the sugar ribose rather than deoxyribose. RNA also contains the base uracil in place of thymine. RNA molecules are less stable than DNA and are typically single-stranded. Genes that encode proteins are composed of a series of three-nucleotide sequences called codons, which serve as the "words" in the genetic "language". The genetic code specifies the correspondence during protein translation between codons and amino acids. The genetic code is nearly the same for all known organisms.:4.1
The total complement of genes in an organism or cell is known as its genome, which may be stored on one or more chromosomes. A chromosome consists of a single, very long DNA helix on which thousands of genes are encoded.:4.2 The region of the chromosome at which a particular gene is located is called its locus. Each locus contains one allele of a gene; however, members of a population may have different alleles at the locus, each with a slightly different gene sequence.
The majority of eukaryotic genes are stored on a set of large, linear chromosomes. The chromosomes are packed within the nucleus in complex with storage proteins called histones to form a unit called a nucleosome. DNA packaged and condensed in this way is called chromatin.:4.2 The manner in which DNA is stored on the histones, as well as chemical modifications of the histone itself, regulate whether a particular region of DNA is accessible for gene expression. In addition to genes, eukaryotic chromosomes contain sequences involved in ensuring that the DNA is copied without degradation of end regions and sorted into daughter cells during cell division: replication origins, telomeres and the centromere.:4.2 Replication origins are the sequence regions where DNA replication is initiated to make two copies of the chromosome. Telomeres are long stretches of repetitive sequence that cap the ends of the linear chromosomes and prevent degradation of coding and regulatory regions during DNA replication. The length of the telomeres decreases each time the genome is replicated and has been implicated in the aging process. The centromere is required for binding spindle fibres to separate sister chromatids into daughter cells during cell division.:18.2
Prokaryotes (bacteria and archaea) typically store their genomes on a single large, circular chromosome. Similarly, some eukaryotic organelles contain a remnant circular chromosome with a small number of genes.:14.4 Prokaryotes sometimes supplement their chromosome with additional small circles of DNA called plasmids, which usually encode only a few genes and are transferable between individuals. For example, the genes for antibiotic resistance are usually encoded on bacterial plasmids and can be passed between individual cells, even those of different species, via horizontal gene transfer.
Whereas the chromosomes of prokaryotes are relatively gene-dense, those of eukaryotes often contain regions of DNA that serve no obvious function. Simple single-celled eukaryotes have relatively small amounts of such DNA, whereas the genomes of complex multicellular organisms, including humans, contain an absolute majority of DNA without an identified function. This DNA has often been referred to as "junk DNA". However, more recent analyses suggest that, although protein-coding DNA makes up barely 2% of the human genome, about 80% of the bases in the genome may be expressed, so the term "junk DNA" may be a misnomer.
The structure of a gene consists of many elements of which the actual protein coding sequence is often only a small part. These include DNA regions that are not transcribed as well as untranslated regions of the RNA.
Firstly, flanking the open reading frame, all genes contain a regulatory sequence that is required for their expression. In order to be expressed, genes require a promoter sequence. The promoter is recognized and bound by transcription factors and RNA polymerase to initiate transcription.:7.1 A gene can have more than one promoter, resulting in messenger RNAs (mRNA) that differ in how far they extend in the 5' end. Promoter regions have a consensus sequence, however highly transcribed genes have "strong" promoter sequences that bind the transcription machinery well, whereas others have "weak" promoters that bind poorly and initiate transcription less frequently.:7.2 Eukaryotic promoter regions are much more complex and difficult to identify than prokaryotic promoters.:7.3
Additionally, genes can have regulatory regions many kilobases upstream or downstream of the open reading frame. These act by binding to transcription factors which then cause the DNA to loop so that the regulatory sequence (and bound transcription factor) become close to the RNA polymerase binding site. For example, enhancers increase transcription by binding an activator protein which then helps to recruit the RNA polymerase to the promoter; conversely silencers bind repressor proteins and make the DNA less available for RNA polymerase.
The transcribed pre-mRNA contains untranslated regions at both ends which contain a ribosome binding site, terminator and start and stop codons. In addition, most eukaryotic open reading frames contain untranslated introns which are removed before the exons are translated. The sequences at the ends of the introns, dictate the splice sites to generate the final mature mRNA which encodes the protein or RNA product.
Many prokaryotic genes are organized into operons, with multiple protein-coding sequences that are transcribed as a unit. The products of operon genes typically have related functions and are involved in the same regulatory network.:7.3
Defining exactly what section of a DNA sequence comprises a gene is difficult. Regulatory regions of a gene such as enhancers do not necessarily have to be close to the coding sequence on the linear molecule because the intervening DNA can be looped out to bring the gene and its regulatory region into proximity. Similarly, a gene's introns can be much larger than its exons. Regulatory regions can even be on entirely different chromosomes and operate in trans to allow regulatory regions on one chromosome to come in contact with target genes on another chromosome.
Early work in molecular genetics suggested the model that one gene makes one protein. This model has been refined since the discovery of genes that can encode multiple proteins by alternative splicing and coding sequences split in short section across the genome whose mRNAs are concatenated by trans-splicing.
A broad operational definition is sometimes used to encompass the complexity of these diverse phenomena, where a gene is defined as a union of genomic sequences encoding a coherent set of potentially overlapping functional products. This definition categorizes genes by their functional products (proteins or RNA) rather than their specific DNA loci, with regulatory elements classified as gene-associated regions.
In all organisms, two steps are required to read the information encoded in a gene's DNA and produce the protein it specifies. First, the gene's DNA is transcribed to messenger RNA (mRNA).:6.1 Second, that mRNA is translated to protein.:6.2 RNA-coding genes must still go through the first step, but are not translated into protein. The process of producing a biologically functional molecule of either RNA or protein is called gene expression, and the resulting molecule is called a gene product.
The nucleotide sequence of a gene's DNA specifies the amino acid sequence of a protein through the genetic code. Sets of three nucleotides, known as codons, each correspond to a specific amino acid.:6 Additionally, a "start codon", and three "stop codons" indicate the beginning and end of the protein coding region. There are 64 possible codons (four possible nucleotides at each of three positions, hence 43 possible codons) and only 20 standard amino acids; hence the code is redundant and multiple codons can specify the same amino acid. The correspondence between codons and amino acids is nearly universal among all known living organisms.
Transcription produces a single-stranded RNA molecule known as messenger RNA, whose nucleotide sequence is complementary to the DNA from which it was transcribed.:6.1 The mRNA acts as an intermediate between the DNA gene and its final protein product. The gene's DNA is used as a template to generate a complementary mRNA. The mRNA matches the sequence of the gene's DNA coding strand because it is synthesised as the complement of the template strand. Transcription is performed by an enzyme called an RNA polymerase, which reads the template strand in the 3' to 5' direction and synthesizes the RNA from 5' to 3'. To initiate transcription, the polymerase first recognizes and binds a promoter region of the gene. Thus, a major mechanism of gene regulation is the blocking or sequestering the promoter region, either by tight binding by repressor molecules that physically block the polymerase, or by organizing the DNA so that the promoter region is not accessible.:7
In prokaryotes, transcription occurs in the cytoplasm; for very long transcripts, translation may begin at the 5' end of the RNA while the 3' end is still being transcribed. In eukaryotes, transcription occurs in the nucleus, where the cell's DNA is stored. The RNA molecule produced by the polymerase is known as the primary transcript and undergoes post-transcriptional modifications before being exported to the cytoplasm for translation. One of the modifications performed is the splicing of introns which are sequences in the transcribed region that do not encode protein. Alternative splicing mechanisms can result in mature transcripts from the same gene having different sequences and thus coding for different proteins. This is a major form of regulation in eukaryotic cells and also occurs in some prokaryotes.:7.5
Translation is the process by which a mature mRNA molecule is used as a template for synthesizing a new protein.:6.2 Translation is carried out by ribosomes, large complexes of RNA and protein responsible for carrying out the chemical reactions to add new amino acids to a growing polypeptide chain by the formation of peptide bonds. The genetic code is read three nucleotides at a time, in units called codons, via interactions with specialized RNA molecules called transfer RNA (tRNA). Each tRNA has three unpaired bases known as the anticodon that are complementary to the codon it reads on the mRNA. The tRNA is also covalently attached to the amino acid specified by the complementary codon. When the tRNA binds to its complementary codon in an mRNA strand, the ribosome attaches its amino acid cargo to the new polypeptide chain, which is synthesized from amino terminus to carboxyl terminus. During and after synthesis, most new proteins must folds to their active three-dimensional structure before they can carry out their cellular functions.:3
Genes are regulated so that they are expressed only when the product is needed, since expression draws on limited resources.:7 A cell regulates its gene expression depending on its external environment (e.g. available nutrients, temperature and other stresses), its internal environment (e.g. cell division cycle, metabolism, infection status), and its specific role if in a multicellular organism. Gene expression can be regulated at any step: from transcriptional initiation, to RNA processing, to post-translational modification of the protein. The regulation of lactose metabolism genes in E. coli (lac operon) was the first such mechanism to be described in 1961.
A typical protein-coding gene is first copied into RNA as an intermediate in the manufacture of the final protein product.:6.1 In other cases, the RNA molecules are the actual functional products, as in the synthesis of ribosomal RNA and transfer RNA. Some RNAs known as ribozymes are capable of enzymatic function, and microRNA has a regulatory role. The DNA sequences from which such RNAs are transcribed are known as non-coding RNA genes.
Some viruses store their entire genomes in the form of RNA, and contain no DNA at all. Because they use RNA to store genes, their cellular hosts may synthesize their proteins as soon as they are infected and without the delay in waiting for transcription. On the other hand, RNA retroviruses, such as HIV, require the reverse transcription of their genome from RNA into DNA before their proteins can be synthesized. RNA-mediated epigenetic inheritance has also been observed in plants and very rarely in animals.
Organisms inherit their genes from their parents. Asexual organisms simply inherit a complete copy of their parent's genome. Sexual organisms have two copies of each chromosome because they inherit one complete set from each parent.:1
According to Mendelian inheritance, variations in an organism's phenotype (observable physical and behavioral characteristics) are due in part to variations in its genotype (particular set of genes). Each gene specifies a particular trait with different sequence of a gene (alleles) giving rise to different phenotypes. Most eukaryotic organisms (such as the pea plants Mendel worked on) have two alleles for each trait, one inherited from each parent.:20
Alleles at a locus may be dominant or recessive; dominant alleles give rise to their corresponding phenotypes when paired with any other allele for the same trait, whereas recessive alleles give rise to their corresponding phenotype only when paired with another copy of the same allele. For example, if the allele specifying tall stems in pea plants is dominant over the allele specifying short stems, then pea plants that inherit one tall allele from one parent and one short allele from the other parent will also have tall stems. Mendel's work demonstrated that alleles assort independently in the production of gametes, or germ cells, ensuring variation in the next generation. Although Mendelian inheritance remains a good model for many traits determined by single genes (including a number of well-known genetic disorders) it does not include the physical processes of DNA replication and cell division.
The growth, development, and reproduction of organisms relies on cell division, or the process by which a single cell divides into two usually identical daughter cells. This requires first making a duplicate copy of every gene in the genome in a process called DNA replication.:5.2 The copies are made by specialized enzymes known as DNA polymerases, which "read" one strand of the double-helical DNA, known as the template strand, and synthesize a new complementary strand. Because the DNA double helix is held together by base pairing, the sequence of one strand completely specifies the sequence of its complement; hence only one strand needs to be read by the enzyme to produce a faithful copy. The process of DNA replication is semiconservative; that is, the copy of the genome inherited by each daughter cell contains one original and one newly synthesized strand of DNA.:5.2
After DNA replication is complete, the cell must physically separate the two copies of the genome and divide into two distinct membrane-bound cells.:18.2 In prokaryotes (bacteria and archaea) this usually occurs via a relatively simple process called binary fission, in which each circular genome attaches to the cell membrane and is separated into the daughter cells as the membrane invaginates to split the cytoplasm into two membrane-bound portions. Binary fission is extremely fast compared to the rates of cell division in eukaryotes. Eukaryotic cell division is a more complex process known as the cell cycle; DNA replication occurs during a phase of this cycle known as S phase, whereas the process of segregating chromosomes and splitting the cytoplasm occurs during M phase.:18.1
The duplication and transmission of genetic material from one generation of cells to the next is the basis for molecular inheritance, and the link between the classical and molecular pictures of genes. Organisms inherit the characteristics of their parents because the cells of the offspring contain copies of the genes in their parents' cells. In asexually reproducing organisms, the offspring will be a genetic copy or clone of the parent organism. In sexually reproducing organisms, a specialized form of cell division called meiosis produces cells called gametes or germ cells that are haploid, or contain only one copy of each gene.:20.2 The gametes produced by females are called eggs or ova, and those produced by males are called sperm. Two gametes fuse to form a diploid fertilized egg, a single cell that has two sets of genes, with one copy of each gene from the mother and one from the father.:20
During the process of meiotic cell division, an event called genetic recombination or crossing-over can sometimes occur, in which a length of DNA on one chromatid is swapped with a length of DNA on the corresponding sister chromatid. This has no effect if the alleles on the chromatids are the same, but results in reassortment of otherwise linked alleles if they are different.:5.5 The Mendelian principle of independent assortment asserts that each of a parent's two genes for each trait will sort independently into gametes; which allele an organism inherits for one trait is unrelated to which allele it inherits for another trait. This is in fact only true for genes that do not reside on the same chromosome, or are located very far from one another on the same chromosome. The closer two genes lie on the same chromosome, the more closely they will be associated in gametes and the more often they will appear together; genes that are very close are essentially never separated because it is extremely unlikely that a crossover point will occur between them. This is known as genetic linkage.
DNA replication is for the most part extremely accurate, however errors (mutations) do occur.:7.6 The error rate in eukaryotic cells can be as low as 10−8 per nucleotide per replication, whereas for some RNA viruses it can be as high as 10−3. This means that each generation, each human genome accumulates 1–2 new mutations. Small mutations can be caused by DNA replication and the aftermath of DNA damage and include point mutations in which a single base is altered and frameshift mutations in which a single base is inserted or deleted. Either of these mutations can change the gene by missense (change a codon to encode a different amino acid) or nonsense (a premature stop codon). Larger mutations can be caused by errors in recombination to cause chromosomal abnormalities including the duplication, deletion, rearrangement or inversion of large sections of a chromosome. Additionally, the DNA repair mechanisms that normally revert mutations can introduce errors when repairing the physical damage to the molecule is more important than restoring an exact copy, for example when repairing double-strand breaks.:5.4
When multiple different alleles for a gene are present in a species's population it is called polymorphic. Most different alleles are functionally equivalent, however some alleles can give rise to different phenotypic traits. A gene's most common allele is called the wild type, and rare alleles are called mutants. The genetic variation in relative frequencies of different alleles in a population is due to both natural selection and genetic drift. The wild-type allele is not necessarily the ancestor of less common alleles, nor is it necessarily fitter.
Most mutations within genes are neutral, having no effect on the organism's phenotype (silent mutations). Some mutations do not change the amino acid sequence because multiple codons encode the same amino acid (synonymous mutations). Other mutations can be neutral if they lead to amino acid sequence changes, but the protein still functions similarly with the new amino acid (e.g. conservative mutations). Many mutations, however, are deleterious or even lethal, and are removed from populations by natural selection. Genetic disorders are the result of deleterious mutations and can be due to spontaneous mutation in the affected individual, or can be inherited. Finally, a small fraction of mutations are beneficial, improving the organism's fitness and are extremely important for evolution, since their directional selection leads to adaptive evolution.:7.6
Genes with a most recent common ancestor, and thus a shared evolutionary ancestry, are known as homologs. These genes appear either from gene duplication within an organism's genome, where they are known as paralogous genes, or are the result of divergence of the genes after a speciation event, where they are known as orthologous genes,:7.6 and often perform the same or similar functions in related organisms. It is often assumed that the functions of orthologous genes are more similar than those of paralogous genes, although the difference is minimal.
The relationship between genes can be measured by comparing the sequence alignment of their DNA.:7.6 The degree of sequence similarity between homologous genes is called conserved sequence. Most changes to a gene's sequence do not affect its function and so genes accumulate mutations over time by neutral molecular evolution. Additionally, any selection on a gene will cause its sequence to diverge at a different rate. Genes under stabilizing selection are constrained and so change more slowly whereas genes under directional selection change sequence more rapidly. The sequence differences between genes can be used for phylogenetic analyses to study how those genes have evolved and how the organisms they come from are related.
The most common source of new genes in eukaryotic lineages is gene duplication, which creates copy number variation of an existing gene in the genome. The resulting genes (paralogs) may then diverge in sequence and in function. Sets of genes formed in this way comprise a gene family. Gene duplications and losses within a family are common and represent a major source of evolutionary biodiversity. Sometimes, gene duplication may result in a nonfunctional copy of a gene, or a functional copy may be subject to mutations that result in loss of function; such nonfunctional genes are called pseudogenes.:7.6
De novo or "orphan" genes, whose sequence shows no similarity to existing genes, are extremely rare. Estimates of the number of de novo genes in the human genome range from 18 to 60. Such genes are typically shorter and simpler in structure than most eukaryotic genes, with few if any introns. Two primary sources of orphan protein-coding genes are gene duplication followed by extremely rapid sequence change, such that the original relationship is undetectable by sequence comparisons, and formation through mutation of "cryptic" transcription start sites that introduce a new open reading frame in a region of the genome that did not previously code for a protein.
Horizontal gene transfer refers to the transfer of genetic material through a mechanism other than reproduction. This mechanism is a common source of new genes in prokaryotes, sometimes thought to contribute more to genetic variation than gene duplication. It is a common means of spreading antibiotic resistance, virulence, and adaptive metabolic functions. Although horizontal gene transfer is rare in eukaryotes, likely examples have been identified of protist and alga genomes containing genes of bacterial origin.
The genome size, and the number of genes it encodes varies widely between organisms. The smallest genomes occur in viruses (which can have as few as 2 protein-coding genes), and viroids (which act as a single non-coding RNA gene). Conversely, plants can have extremely large genomes, with rice containing >46,000 protein-coding genes. The total number of protein-coding genes (the Earth's proteome) is estimated to be 5 million sequences.
Although the number of base-pairs of DNA in the human genome has been known since the 1960s, the estimated number of genes has changed over time as definitions of genes, and methods of detecting them have been refined. Initial theoretical predictions of the number of human genes were as high as 2,000,000. Early experimental measures indicated there to be 50,000–100,000 transcribed genes (expressed sequence tags). Subsequently, the sequencing in the Human Genome Project indicated that many of these transcripts were alternative variants of the same genes, and the total number of protein-coding genes was revised down to ~20,000 with 13 genes encoded on the mitochondrial genome. Of the human genome, only 1–2% consists of protein-coding genes, with the remainder being 'noncoding' DNA such as introns, retrotransposons, and noncoding RNAs.
Essential genes are the set of genes thought to be critical for an organism's survival. This definition assumes the abundant availability of all relevant nutrients and the absence of environmental stress. Only a small portion of an organism's genes are essential. In bacteria, an estimated 250–400 genes are essential for Escherichia coli and Bacillus subtilis, which is less than 10% of their genes. Half of these genes are orthologs in both organisms and are largely involved in protein synthesis. In the budding yeast Saccharomyces cerevisiae the number of essential genes is slightly higher, at 1000 genes (~20% of their genes). Although the number is more difficult to measure in higher eukaryotes, mice and humans are estimated to have around 2000 essential genes (~10% of their genes).
Housekeeping genes are critical for carrying out basic cell functions and so are expressed at a relatively constant level (constitutively). Since their expression is constant, housekeeping genes are used as experimental controls when analysing gene expression. Not all essential genes are housekeeping genes since some essential genes are developmentally regulated or expressed at certain times during the organism's life cycle.
Gene nomenclature has been established by the HUGO Gene Nomenclature Committee (HGNC) for each known human gene in the form of an approved gene name and symbol (short-form abbreviation), which can be accessed through a database maintained by HGNC. Symbols are chosen to be unique, and each gene has only one symbol (although approved symbols sometimes change). Symbols are preferably kept consistent with other members of a gene family and with homologs in other species, particularly the mouse due to its role as a common model organism.
Genetic engineering is the modification of an organism's genome through biotechnology. Since the 1970s, a variety of techniques have been developed to specifically add, remove and edit genes in an organism. Recently developed genome engineering techniques use engineered nuclease enzymes to create targeted DNA repair in a chromosome to either disrupt or edit a gene when the break is repaired. The related term synthetic biology is sometimes used to refer to extensive genetic engineering of an organism.
Genetic engineering is now a routine research tool with model organisms. For example, genes are easily added to bacteria and lineages of knockout mice with a specific gene's function disrupted are used to investigate that gene's function. Many organisms have been genetically modified for applications in agriculture, industrial biotechnology, and medicine.
For multicellular organisms, typically the embryo is engineered which grows into the adult genetically modified organism. However, the genomes of cells in an adult organism can be edited using gene therapy techniques to treat genetic diseases.
The console was first officially announced at E3 2005, and was released at the end of 2006. It was the first console to use Blu-ray Disc as its primary storage medium. The console was the first PlayStation to integrate social gaming services, included it being the first to introduce Sony's social gaming service, PlayStation Network, and its remote connectivity with PlayStation Portable and PlayStation Vita, being able to remote control the console from the devices. In September 2009, the Slim model of the PlayStation 3 was released, being lighter and thinner than the original version, which notably featured a redesigned logo and marketing design, as well as a minor start-up change in software. A Super Slim variation was then released in late 2012, further refining and redesigning the console. As of March 2016, PlayStation 3 has sold 85 million units worldwide. Its successor, the PlayStation 4, was released later in November 2013.
Sony officially unveiled PlayStation 3 (then marketed as PLAYSTATION 3) to the public on May 16, 2005, at E3 2005, along with a 'boomerang' shaped prototype design of the Sixaxis controller. A functional version of the system was not present there, nor at the Tokyo Game Show in September 2005, although demonstrations (such as Metal Gear Solid 4: Guns of the Patriots) were held at both events on software development kits and comparable personal computer hardware. Video footage based on the predicted PlayStation 3 specifications was also shown (notably a Final Fantasy VII tech demo).
The initial prototype shown in May 2005 featured two HDMI ports, three Ethernet ports and six USB ports; however, when the system was shown again a year later at E3 2006, these were reduced to one HDMI port, one Ethernet port and four USB ports, presumably to cut costs. Two hardware configurations were also announced for the console: a 20 GB model and a 60 GB model, priced at US$499 (€499) and US$599 (€599), respectively. The 60 GB model was to be the only configuration to feature an HDMI port, Wi-Fi internet, flash card readers and a chrome trim with the logo in silver. Both models were announced for a simultaneous worldwide release: November 11, 2006, for Japan and November 17, 2006, for North America and Europe.
On September 6, 2006, Sony announced that PAL region PlayStation 3 launch would be delayed until March 2007, because of a shortage of materials used in the Blu-ray drive. At the Tokyo Game Show on September 22, 2006, Sony announced that it would include an HDMI port on the 20 GB system, but a chrome trim, flash card readers, silver logo and Wi-Fi would not be included. Also, the launch price of the Japanese 20 GB model was reduced by over 20%, and the 60 GB model was announced for an open pricing scheme in Japan. During the event, Sony showed 27 playable PS3 games running on final hardware.
The console was originally planned for a global release through November, but at the start of September the release in Europe and the rest of the world was delayed until March. With it being a somewhat last-minute delay, some companies had taken deposits for pre-orders, at which Sony informed customers that they were eligible for full refunds or could continue the pre-order. On January 24, 2007, Sony announced that PlayStation 3 would go on sale on March 23, 2007, in Europe, Australia, the Middle East, Africa and New Zealand. The system sold about 600,000 units in its first two days. On March 7, 2007, the 60 GB PlayStation 3 launched in Singapore with a price of S$799. The console was launched in South Korea on June 16, 2007, as a single version equipped with an 80 GB hard drive and IPTV.
Following speculation that Sony was working on a 'slim' model, Sony officially announced the PS3 CECH-2000 model on August 18, 2009, at the Sony Gamescom press conference. New features included a slimmer form factor, decreased power consumption, and a quieter cooling system. It was released in major territories by September 2009. As part of the release for the slim model, the console logo ceased using the "Spider-Man font" (the same font used for the title of Sony's Spider-Man 3) and the capitalized PLAYSTATION 3. It instead reverted to a more traditional PlayStation- and PlayStation 2-like 'PlayStation 3' logo with "PS3" imprinted on the console. Along with the redesigning of the console and logo, the boot screen of all consoles changed from "Sony Computer Entertainment" to "PS3 PlayStation 3", with a new chime and the game start splash screen being dropped. The cover art and packaging of games was also changed.
In September 2012 at the Tokyo Game Show, Sony announced that a new, slimmer PS3 redesign (CECH-4000) was due for release in late 2012 and that it would be available with either a 250 GB or 500 GB hard drive. Three versions Super Slim model were revealed: one with a 500 GB hard drive, a second with a 250 GB hard drive which is not available in PAL regions, and a third with a 12 GB flash storage that was only available in PAL regions. The storage of 12 GB model is upgradable with an official standalone 250 GB hard drive. A vertical stand was also released for the model. In the United Kingdom, the 500 GB model was released on September 28, 2012; and the 12 GB model was released on October 12, 2012. In the United States, the PS3 Super Slim was first released as a bundled console. The 250 GB was model was bundled with Game of the Year edition of Uncharted 3: Drake's Deception and released on September 25, 2012; and the 500 GB model was bundled with Assassin's Creed III and released on October 30, 2012. In Japan, the black colored Super Slim model was released on October 4, 2012; and the white colored Super Slim model was released on November 22, 2012. The Super Slim model is 20 percent smaller and 25 percent lighter than the Slim model and features a manual sliding disc cover instead of a motorized slot-loading disc cover of the Slim model. The white colored Super Slim model was released in the United States on January 27, 2013 as part of the Instant Game Collection Bundle. The Garnet Red and Azurite Blue colored models were launched in Japan on February 28, 2013. The Garnet Red version was released in North America on March 12, 2013 as part of the God of War: Ascension bundle with 500 GB storage and contained God of War: Ascension as well as the God of War Saga. The Azurite Blue model was released as a GameStop exclusive with 250GB storage.
PlayStation 3 launched in North America with 14 titles, with another three being released before the end of 2006. After the first week of sales it was confirmed that Resistance: Fall of Man from Insomniac Games was the top-selling launch game in North America. The game was heavily praised by numerous video game websites, including GameSpot and IGN, both of whom awarded it their PlayStation 3 Game of the Year award for 2006. Some titles missed the launch window and were delayed until early 2007, such as The Elder Scrolls IV: Oblivion, F.E.A.R. and Sonic the Hedgehog. During the Japanese launch, Ridge Racer 7 was the top-selling game, while Mobile Suit Gundam: Crossfire also fared well in sales, both of which were offerings from Namco Bandai Games. PlayStation 3 launched in Europe with 24 titles, including ones that were not offered in North American and Japanese launches, such as Formula One Championship Edition, MotorStorm and Virtua Fighter 5. Resistance: Fall of Man and MotorStorm were the most successful titles of 2007, and both games subsequently received sequels in the form of Resistance 2 and MotorStorm: Pacific Rift.
At E3 2007, Sony was able to show a number of their upcoming video games for PlayStation 3, including Heavenly Sword, Lair, Ratchet & Clank Future: Tools of Destruction, Warhawk and Uncharted: Drake's Fortune; all of which were released in the third and fourth quarters of 2007. They also showed off a number of titles that were set for release in 2008 and 2009; most notably Killzone 2, Infamous, Gran Turismo 5 Prologue, LittleBigPlanet and SOCOM: U.S. Navy SEALs Confrontation. A number of third-party exclusives were also shown, including the highly anticipated Metal Gear Solid 4: Guns of the Patriots, alongside other high-profile third-party titles such as Grand Theft Auto IV, Call of Duty 4: Modern Warfare, Assassin's Creed, Devil May Cry 4 and Resident Evil 5. Two other important titles for PlayStation 3, Final Fantasy XIII and Final Fantasy Versus XIII, were shown at TGS 2007 in order to appease the Japanese market.
Sony have since launched their budget range of PlayStation 3 titles, known as the Greatest Hits range in North America, the Platinum range in Europe and Australia and The Best range in Japan. Among the titles available in the budget range include Resistance: Fall of Man, MotorStorm, Uncharted: Drakes Fortune, Rainbow Six: Vegas, Call Of Duty 3, Assassin's Creed and Ninja Gaiden Sigma. As of October 2009 Metal Gear Solid 4: Guns of the Patriots, Ratchet & Clank Future: Tools of Destruction, Devil May Cry 4, Army of Two, Battlefield: Bad Company and Midnight Club: Los Angeles have also joined the list.
In December 2008, the CTO of Blitz Games announced that it would bring stereoscopic 3D gaming and movie viewing to Xbox 360 and PlayStation 3 with its own technology. This was first demonstrated publicly on PS3 using Sony's own technology in January 2009 at the Consumer Electronics Show. Journalists were shown Wipeout HD and Gran Turismo 5 Prologue in 3D as a demonstration of how the technology might work if it is implemented in the future. Firmware update 3.30 officially allowed PS3 titles to be played in 3D, requiring a compatible display for use. System software update 3.50 prepared it for 3D films. While the game itself must be programmed to take advantage of the 3D technology, titles may be patched to add in the functionality retroactively. Titles with such patches include Wipeout HD, Pain, and Super Stardust HD.
PS3's hardware has also been used to build supercomputers for high-performance computing. Fixstars Solutions sells a version of Yellow Dog Linux for PlayStation 3 (originally sold by Terra Soft Solutions). RapidMind produced a stream programming package for PS3, but were acquired by Intel in 2009. Also, on January 3, 2007, Dr. Frank Mueller, Associate Professor of Computer science at NCSU, clustered 8 PS3s. Mueller commented that the 256 MB of system RAM is a limitation for this particular application and is considering attempting to retrofit more RAM. Software includes: Fedora Core 5 Linux ppc64, MPICH2, OpenMP v 2.5, GNU Compiler Collection and CellSDK 1.1. As a more cost-effective alternative to conventional supercomputers, the U.S. military has purchased clusters of PS3 units for research purposes. Retail PS3 Slim units cannot be used for supercomputing, because PS3 Slim lacks the ability to boot into a third-party OS.
PlayStation 3 uses the Cell microprocessor, designed by Sony, Toshiba and IBM, as its CPU, which is made up of one 3.2 GHz PowerPC-based "Power Processing Element" (PPE) and eight Synergistic Processing Elements (SPEs). The eighth SPE is disabled to improve chip yields. Only six of the seven SPEs are accessible to developers as the seventh SPE is reserved by the console's operating system. Graphics processing is handled by the NVIDIA RSX 'Reality Synthesizer', which can produce resolutions from 480i/576i SD up to 1080p HD. PlayStation 3 has 256 MB of XDR DRAM main memory and 256 MB of GDDR3 video memory for the RSX.
At its press conference at the 2007 Tokyo Game Show, Sony announced DualShock 3 (trademarked DUALSHOCK 3), a PlayStation 3 controller with the same function and design as Sixaxis, but with vibration capability included. Hands-on accounts describe the controller as being noticeably heavier than the standard Sixaxis controller and capable of vibration forces comparable to DualShock 2. It was released in Japan on November 11, 2007; in North America on April 5, 2008; in Australia on April 24, 2008; in New Zealand on May 9, 2008; in mainland Europe on July 2, 2008, and in the United Kingdom and Ireland on July 4, 2008.
The standard PlayStation 3 version of the XrossMediaBar (pronounced Cross Media Bar, or abbreviated XMB) includes nine categories of options. These are: Users, Settings, Photo, Music, Video, TV/Video Services, Game, Network, PlayStation Network and Friends (similar to the PlayStation Portable media bar). TheTV/Video Services category is for services like Netflix and/or if PlayTV or torne is installed; the first category in this section is "My Channels", which lets users download various streaming services, including Sony's own streaming services Crackle and PlayStation Vue. By default, the What's New section of PlayStation Network is displayed when the system starts up. PS3 includes the ability to store various master and secondary user profiles, manage and explore photos with or without a musical slide show, play music and copy audio CD tracks to an attached data storage device, play movies and video files from the hard disk drive, an optical disc (Blu-ray Disc or DVD-Video) or an optional USB mass storage or Flash card, compatibility for a USB keyboard and mouse and a web browser supporting compatible-file download function. Additionally, UPnP media will appear in the respective audio/video/photo categories if a compatible media server or DLNA server is detected on the local network. The Friends menu allows mail with emoticon and attached picture features and video chat which requires an optional PlayStation Eye or EyeToy webcam. The Network menu allows online shopping through the PlayStation Store and connectivity to PlayStation Portable via Remote Play.
PlayStation 3 console protects certain types of data and uses digital rights management to limit the data's use. Purchased games and content from the PlayStation Network store are governed by PlayStation's Network Digital Rights Management (NDRM). The NDRM allows users to access the data from up to 2 different PlayStation 3's that have been activated using a user's PlayStation Network ID. PlayStation 3 also limits the transfer of copy protected videos downloaded from its store to other machines and states that copy protected video "may not restore correctly" following certain actions after making a backup such as downloading a new copy protected movie.
Photo Gallery is an optional application to view, create and group photos from PS3, which is installed separately from the system software at 105 MB. It was introduced in system software version 2.60 and provides a range of tools for sorting through and displaying the system's pictures. The key feature of this application is that it can organize photos into groups according to various criteria. Notable categorizations are colors, ages, or facial expressions of the people in the photos. Slideshows can be viewed with the application, along with music and playlists. The software was updated with the release of system software version 3.40 allowing users to upload and browse photos on Facebook and Picasa.
Since June 2009 VidZone has offered a free music video streaming service in Europe, Australia and New Zealand. In October 2009, Sony Computer Entertainment and Netflix announced that the Netflix streaming service would also be available on PlayStation 3 in the United States. A paid Netflix subscription was required for the service. The service became available in November 2009. Initially users had to use a free Blu-ray disc to access the service; however, in October 2010 the requirement to use a disc to gain access was removed.
The 'OtherOS' functionality was not present in the updated PS Slim models, and the feature was subsequently removed from previous versions of the PS3 as part of the machine's firmware update version 3.21 which was released on April 1, 2010; Sony cited security concerns as the rationale. The firmware update 3.21 was mandatory for access to the PlayStation Network. The removal caused some controversy; as the update removed officially advertised features from already sold products, and gave rise to several class action lawsuits aimed at making Sony return the feature or provide compensation.
On March 1, 2010 (UTC), many of the original "fat" PlayStation 3 models worldwide were experiencing errors related to their internal system clock. The error had many symptoms. Initially, the main problem seemed to be the inability to connect to the PlayStation Network. However, the root cause of the problem was unrelated to the PlayStation Network, since even users who had never been online also had problems playing installed offline games (which queried the system timer as part of startup) and using system themes. At the same time many users noted that the console's clock had gone back to December 31, 1999. The event was nicknamed the ApocalyPS3, a play on the word apocalypse and PS3, the abbreviation for the PlayStation 3 console.
Sony confirmed that there was an error and stated that they were narrowing down the issue and were continuing to work to restore service. By March 2 (UTC), 2010, owners of original PS3 models could connect to PSN successfully and the clock no longer showed December 31, 1999. Sony stated that the affected models incorrectly identified 2010 as a leap year, because of a bug in the BCD method of storing the date. However, for some users, the hardware's operating system clock (mainly updated from the internet and not associated with the internal clock) needed to be updated manually or by re-syncing it via the internet.
PlayStation Portable can connect with PlayStation 3 in many ways, including in-game connectivity. For example, Formula One Championship Edition, a racing game, was shown at E3 2006 using a PSP as a real-time rear-view mirror. In addition, users are able to download original PlayStation format games from the PlayStation Store, transfer and play them on PSP as well as PS3 itself. It is also possible to use the Remote Play feature to play these and some PlayStation Network games, remotely on PSP over a network or internet connection.
PlayStation Network is the unified online multiplayer gaming and digital media delivery service provided by Sony Computer Entertainment for PlayStation 3 and PlayStation Portable, announced during the 2006 PlayStation Business Briefing meeting in Tokyo. The service is always connected, free, and includes multiplayer support. The network enables online gaming, the PlayStation Store, PlayStation Home and other services. PlayStation Network uses real currency and PlayStation Network Cards as seen with the PlayStation Store and PlayStation Home.
PlayStation Plus (commonly abbreviated PS+ and occasionally referred to as PSN Plus) is a premium PlayStation Network subscription service that was officially unveiled at E3 2010 by Jack Tretton, President and CEO of SCEA. Rumors of such service had been in speculation since Kaz Hirai's announcement at TGS 2009 of a possible paid service for PSN but with the current PSN service still available. Launched alongside PS3 firmware 3.40 and PSP firmware 6.30 on June 29, 2010, the paid-for subscription service provides users with enhanced services on the PlayStation Network, on top of the current PSN service which is still available with all of its features. These enhancements include the ability to have demos, game and system software updates download automatically to PlayStation 3. Subscribers also get early or exclusive access to some betas, game demos, premium downloadable content and other PlayStation Store items. North American users also get a free subscription to Qore. Users may choose to purchase either a one-year or a three-month subscription to PlayStation Plus.
The PlayStation Store is an online virtual market available to users of Sony's PlayStation 3 (PS3) and PlayStation Portable (PSP) game consoles via the PlayStation Network. The Store offers a range of downloadable content both for purchase and available free of charge. Available content includes full games, add-on content, playable demos, themes and game and movie trailers. The service is accessible through an icon on the XMB on PS3 and PSP. The PS3 store can also be accessed on PSP via a Remote Play connection to PS3. The PSP store is also available via the PC application, Media Go. As of September 24, 2009, there have been over 600 million downloads from the PlayStation Store worldwide.
What's New was announced at Gamescom 2009 and was released on September 1, 2009, with PlayStation 3 system software 3.0. The feature was to replace the existing [Information Board], which displayed news from the PlayStation website associated with the user's region. The concept was developed further into a major PlayStation Network feature, which interacts with the [Status Indicator] to display a ticker of all content, excluding recently played content (currently in North America and Japan only).
The system displays the What's New screen by default instead of the [Games] menu (or [Video] menu, if a movie was inserted) when starting up. What's New has four sections: "Our Pick", "Recently Played", latest information and new content available in PlayStation Store. There are four kinds of content the What's New screen displays and links to, on the sections. "Recently Played" displays the user's recently played games and online services only, whereas, the other sections can contain website links, links to play videos and access to selected sections of the PlayStation Store.
PlayStation Home is a virtual 3D social networking service for the PlayStation Network. Home allows users to create a custom avatar, which can be groomed realistically. Users can edit and decorate their personal apartments, avatars or club houses with free, premium or won content. Users can shop for new items or win prizes from PS3 games, or Home activities. Users interact and connect with friends and customise content in a virtual world. Home also acts as a meeting place for users that want to play multiplayer games with others.
Life with PlayStation, released on September 18, 2008 to succeed Folding@home, was retired November 6, 2012. Life with PlayStation used virtual globe data to display news and information by city. Along with Folding@home functionality, the application provided access to three other information "channels", the first being the Live Channel offering news headlines and weather which were provided by Google News, The Weather Channel, the University of Wisconsin–Madison Space Science and Engineering Center, among other sources. The second channel was the World Heritage channel which offered historical information about historical sites. The third channel was the United Village channel. United Village was designed to share information about communities and cultures worldwide. An update allowed video and photo viewing in the application. The fourth channel was the USA exclusive PlayStation Network Game Trailers Channel for direct streaming of game trailers.
On April 20, 2011, Sony shut down the PlayStation Network and Qriocity for a prolonged interval, revealing on April 23 that this was due to "an external intrusion on our system". Sony later revealed that the personal information of 77 million users might have been taken, including: names; addresses; countries; email addresses; birthdates; PSN/Qriocity logins, passwords and handles/PSN online IDs. They also stated that it was possible that users' profile data, including purchase history and billing address, and PlayStation Network/Qriocity password security answers may have been obtained. There was no evidence that any credit card data had been taken, but the possibility could not be ruled out, and Sony advised customers that their credit card data may have been obtained. Additionally, the credit card numbers were encrypted and Sony never collected the three digit CVC or CSC number from the back of the credit cards which is required for authenticating some transactions. In response to the incident, Sony announced a "Welcome Back" program, 30 days free membership of PlayStation Plus for all PSN members, two free downloadable PS3 games, and a free one-year enrollment in an identity theft protection program.
Although its PlayStation predecessors had been very dominant against the competition and were hugely profitable for Sony, PlayStation 3 had an inauspicious start, and Sony chairman and CEO Sir Howard Stringer initially could not convince investors of a turnaround in its fortunes. The PS3 lacked the unique gameplay of the more affordable Wii which became that generation's most successful console in terms of units sold. Furthermore, PS3 had to compete directly with Xbox 360 which had a market head start, and as a result the platform no longer had exclusive titles that the PS2 enjoyed such as the Grand Theft Auto and Final Fantasy series (regarding cross-platform games, Xbox 360 versions were generally considered superior in 2006, although by 2008 the PS3 versions had reached parity or surpassed), and it took longer than expected for PS3 to enjoy strong sales and close the gap with Xbox 360. Sony also continued to lose money on each PS3 sold through 2010, although the redesigned "slim" PS3 has cut these losses since then.
PlayStation 3's initial production cost is estimated by iSuppli to have been US$805.85 for the 20 GB model and US$840.35 for the 60 GB model. However, they were priced at US$499 and US$599 respectively, meaning that units may have been sold at an estimated loss of $306 or $241 depending on model, if the cost estimates were correct, and thus may have contributed to Sony's games division posting an operating loss of ¥232.3 billion (US$1.97 billion) in the fiscal year ending March 2007. In April 2007, soon after these results were published, Ken Kutaragi, President of Sony Computer Entertainment, announced plans to retire. Various news agencies, including The Times and The Wall Street Journal reported that this was due to poor sales, while SCEI maintains that Kutaragi had been planning his retirement for six months prior to the announcement.
In January 2008, Kaz Hirai, CEO of Sony Computer Entertainment, suggested that the console may start making a profit by early 2009, stating that, "the next fiscal year starts in April and if we can try to achieve that in the next fiscal year that would be a great thing" and that "[profitability] is not a definite commitment, but that is what I would like to try to shoot for". However, market analysts Nikko Citigroup have predicted that PlayStation 3 could be profitable by August 2008. In a July 2008 interview, Hirai stated that his objective is for PlayStation 3 to sell 150 million units by its ninth year, surpassing PlayStation 2's sales of 140 million in its nine years on the market. In January 2009 Sony announced that their gaming division was profitable in Q3 2008.
Since the system's launch, production costs have been reduced significantly as a result of phasing out the Emotion Engine chip and falling hardware costs. The cost of manufacturing Cell microprocessors has fallen dramatically as a result of moving to the 65 nm production process, and Blu-ray Disc diodes have become cheaper to manufacture. As of January 2008, each unit cost around $400 to manufacture; by August 2009, Sony had reduced costs by a total of 70%, meaning it only costs Sony around $240 per unit.
Critical and commercial reception to PS3 improved over time, after a series of price revisions, Blu-ray's victory over HD DVD, and the release of several well received titles. Ars Technica's original launch review gave PS3 only a 6/10, but second review of the console in June 2008 rated it a 9/10. In September 2009, IGN named PlayStation 3 the 15th best gaming console of all time, behind both of its competitors: Wii (10th) and Xbox 360 (6th). However, PS3 has won IGN's "Console Showdown"—based on which console offers the best selection of games released during each year—in three of the four years since it began (2008, 2009 and 2011, with Xbox winning in 2010). IGN judged PlayStation 3 to have the best game line-up of 2008, based on their review scores in comparison to those of Wii and Xbox 360. In a comparison piece by PC mag's Will Greenwald in June 2012, PS3 was selected as an overall better console compared to Xbox 360. Pocket-lint said of the console "The PS3 has always been a brilliant games console," and that "For now, this is just about the best media device for the money."
PS3 was given the number-eight spot on PC World magazine's list of "The Top 21 Tech Screwups of 2006", where it was criticized for being "Late, Expensive and Incompatible". GamesRadar ranked PS3 as the top item in a feature on game-related PR disasters, asking how Sony managed to "take one of the most anticipated game systems of all time and — within the space of a year — turn it into a hate object reviled by the entire internet", but added that despite its problems the system has "untapped potential". Business Week summed up the general opinion by stating that it was "more impressed with what [the PlayStation 3] could do than with what it currently does".
Developers also found the machine difficult to program for. In 2007, Gabe Newell of Valve said "The PS3 is a total disaster on so many levels, I think it's really clear that Sony lost track of what customers and what developers wanted". He continued "I'd say, even at this late date, they should just cancel it and do a do over. Just say, 'This was a horrible disaster and we're sorry and we're going to stop selling this and stop trying to convince people to develop for it'". Doug Lombardi VP of Marketing for Valve has since stated that they are interested in developing for the console and are looking to hire talented PS3 programmers for future projects. He later restated Valve's position, "Until we have the ability to get a PS3 team together, until we find the people who want to come to Valve or who are at Valve who want to work on that, I don't really see us moving to that platform". At Sony's E3 2010 press conference, Newell made a live appearance to recant his previous statements, citing Sony's move to make the system more developer friendly, and to announce that Valve would be developing Portal 2 for the system. He also claimed that the inclusion of Steamworks (Valve's system to automatically update their software independently) would help to make the PS3 version of Portal 2 the best console version on the market.
Activision Blizzard CEO Bobby Kotick has criticized PS3's high development costs and inferior attach rate and return to that of Xbox 360 and Wii. He believes these factors are pushing developers away from working on the console. In an interview with The Times Kotick stated "I'm getting concerned about Sony; the PlayStation 3 is losing a bit of momentum and they don't make it easy for me to support the platform." He continued, "It's expensive to develop for the console, and the Wii and the Xbox are just selling better. Games generate a better return on invested capital (ROIC) on the Xbox than on the PlayStation." Kotick also claimed that Activision Blizzard may stop supporting the system if the situation is not addressed. "[Sony has] to cut the [PS3's retail] price, because if they don't, the attach rates are likely to slow. If we are being realistic, we might have to stop supporting Sony." Kotick received heavy criticism for the statement, notably from developer Bioware who questioned the wisdom of the threatened move, and referred to the statement as "silly."
Despite the initial negative press, several websites have given the system very good reviews mostly regarding its hardware. CNET United Kingdom praised the system saying, "the PS3 is a versatile and impressive piece of home-entertainment equipment that lives up to the hype [...] the PS3 is well worth its hefty price tag." CNET awarded it a score of 8.8 out of 10 and voted it as its number one "must-have" gadget, praising its robust graphical capabilities and stylish exterior design while criticizing its limited selection of available games. In addition, both Home Theater Magazine and Ultimate AV have given the system's Blu-ray playback very favorable reviews, stating that the quality of playback exceeds that of many current standalone Blu-ray Disc players.
The PlayStation 3 Slim received extremely positive reviews as well as a boost in sales; less than 24 hours after its announcement, PS3 Slim took the number-one bestseller spot on Amazon.com in the video games section for fifteen consecutive days. It regained the number-one position again one day later. PS3 Slim also received praise from PC World giving it a 90 out of 100 praising its new repackaging and the new value it brings at a lower price as well as praising its quietness and the reduction in its power consumption. This is in stark contrast to the original PS3's launch in which it was given position number-eight on their "The Top 21 Tech Screwups of 2006" list.
CNET awarded PS3 Slim four out of five stars praising its Blu-ray capabilities, 120 GB hard drive, free online gaming service and more affordable pricing point, but complained about the lack of backward compatibility for PlayStation 2 games. TechRadar gave PS3 Slim four and a half stars out of five praising its new smaller size and summed up its review stating "Over all, the PS3 Slim is a phenomenal piece of kit. It's amazing that something so small can do so much". However, they criticized the exterior design and the build quality in relation to the original model.
The Super Slim model of PS3 has received positive reviews. Gaming website Spong praised the new Super Slim's quietness, stating "The most noticeable noise comes when the drive seeks a new area of the disc, such as when starting to load a game, and this occurs infrequently." They added that the fans are quieter than that of Slim, and went on to praise the new smaller, lighter size. Criticism was placed on the new disc loader, stating: "The cover can be moved by hand if you wish, there's also an eject button to do the work for you, but there is no software eject from the triangle button menus in the Xross Media Bar (XMB) interface. In addition, you have to close the cover by hand, which can be a bit fiddly if it's upright, and the PS3 won't start reading a disc unless you do [close the cover]." They also said there is no real drop in retail price.
Tech media website CNET gave new Super Slim 4 out of 5 stars ("Excellent"), saying "The Super Slim PlayStation 3 shrinks a powerful gaming machine into an even tinier package while maintaining the same features as its predecessors: a great gaming library and a strong array of streaming services [...]", whilst also criticising the "cheap" design and disc-loader, stating: "Sometimes [the cover] doesn't catch and you feel like you're using one of those old credit card imprinter machines. In short, it feels cheap. You don't realize how convenient autoloading disc trays are until they're gone. Whether it was to cut costs or save space, this move is ultimately a step back." The criticism also was due to price, stating the cheapest Super Slim model was still more expensive than the cheapest Slim model, and that the smaller size and bigger hard drive shouldn't be considered an upgrade when the hard drive on a Slim model is easily removed and replaced. They did praise that the hard drive of the Super Slim model is "the easiest yet. Simply sliding off the side panel reveals the drive bay, which can quickly be unscrewed." They also stated that whilst the Super Slim model is not in any way an upgrade, it could be an indicator as to what's to come. "It may not be revolutionary, but the Super Slim PS3 is the same impressive machine in a much smaller package. There doesn't seem to be any reason for existing PS3 owners to upgrade, but for the prospective PS3 buyer, the Super Slim is probably the way to go if you can deal with not having a slot-loading disc drive."
Technology magazine T3 gave the Super Slim model a positive review, stating the console is almost "nostalgic" in the design similarities to the original "fat" model, "While we don’t know whether it will play PS3 games or Blu-ray discs any differently yet, the look and feel of the new PS3 Slim is an obvious homage to the original PS3, minus the considerable excess weight. Immediately we would be concerned about the durability of the top loading tray that feels like it could be yanked straight out off the console, but ultimately it all feels like Sony's nostalgic way of signing off the current generation console in anticipation for the PS4."
While there is some international commonality in the way political parties are recognized, and in how they operate, there are often many differences, and some are significant. Many political parties have an ideological core, but some do not, and many represent very different ideologies than they did when first founded. In democracies, political parties are elected by the electorate to run a government. Many countries have numerous powerful political parties, such as Germany and India and some nations have one-party systems, such as China. The United States is a two-party system, with its two most powerful parties being the Democratic Party and the Republican Party.
The first political factions, cohering around a basic, if fluid, set of principles emerged from the Exclusion Crisis and Glorious Revolution in late-17th-century England. The Whigs supported Protestant constitutional monarchy against absolute rule and the Tories, originating in the Royalist (or "Cavalier") faction of the English Civil War, were conservative royalist supporters of a strong monarchy as a counterbalance to the republican tendencies of Whigs, who were the dominant political faction for most of the first half of the 18th century; they supported the Hanoverian succession of 1715 against the Jacobite supporters of the deposed Roman Catholic Stuart dynasty and were able to purge Tory politicians from important government positions after the failed Jacobite rising of 1715. The leader of the Whigs was Robert Walpole, who maintained control of the government in the period 1721–1742; his protégé was Henry Pelham (1743–1754).
As the century wore on, the factions slowly began to adopt more coherent political tendencies as the interests of their power bases began to diverge. The Whig party's initial base of support from the great aristocratic families, widened to include the emerging industrial interests and wealthy merchants. As well as championing constitutional monarchy with strict limits on the monarch's power, the Whigs adamantly opposed a Catholic king as a threat to liberty, and believed in extending toleration to nonconformist Protestants, or dissenters. A major influence on the Whigs were the liberal political ideas of John Locke, and the concepts of universal rights employed by Locke and Algernon Sidney.
Although the Tories were dismissed from office for half a century, for most of this period (at first under the leadership of Sir William Wyndham), the Tories retained party cohesion, with occasional hopes of regaining office, particularly at the accession of George II (1727) and the downfall of the ministry of Sir Robert Walpole in 1742. They acted as a united, though unavailing, opposition to Whig corruption and scandals. At times they cooperated with the "Opposition Whigs", Whigs who were in opposition to the Whig government; however, the ideological gap between the Tories and the Opposition Whigs prevented them from coalescing as a single party. They finally regained power with the accession of George III in 1760 under Lord Bute.
When they lost power, the old Whig leadership dissolved into a decade of factional chaos with distinct "Grenvillite", "Bedfordite", "Rockinghamite", and "Chathamite" factions successively in power, and all referring to themselves as "Whigs". Out of this chaos, the first distinctive parties emerged. The first such party was the Rockingham Whigs under the leadership of Charles Watson-Wentworth and the intellectual guidance of the political philosopher Edmund Burke. Burke laid out a philosophy that described the basic framework of the political party as "a body of men united for promoting by their joint endeavours the national interest, upon some particular principle in which they are all agreed". As opposed to the instability of the earlier factions, which were often tied to a particular leader and could disintegrate if removed from power, the party was centred around a set of core principles and remained out of power as a united opposition to government.
The modern Conservative Party was created out of the 'Pittite' Tories of the early 19th century. In the late 1820s disputes over political reform broke up this grouping. A government led by the Duke of Wellington collapsed amidst dire election results. Following this disaster Robert Peel set about assembling a new coalition of forces. Peel issued the Tamworth Manifesto in 1834 which set out the basic principles of Conservatism; – the necessity in specific cases of reform in order to survive, but an opposition to unnecessary change, that could lead to "a perpetual vortex of agitation". Meanwhile, the Whigs, along with free trade Tory followers of Robert Peel, and independent Radicals, formed the Liberal Party under Lord Palmerston in 1859, and transformed into a party of the growing urban middle-class, under the long leadership of William Ewart Gladstone.
Although the Founding Fathers of the United States did not originally intend for American politics to be partisan, early political controversies in the 1790s over the extent of federal government powers saw the emergence of two proto-political parties- the Federalist Party and the Democratic-Republican Party, which were championed by Framers Alexander Hamilton and James Madison, respectively. However, a consensus reached on these issues ended party politics in 1816 for a decade, a period commonly known as the Era of Good Feelings.
At the same time, the political party reached its modern form, with a membership disciplined through the use of a party whip and the implementation of efficient structures of control. The Home Rule League Party, campaigning for Home Rule for Ireland in the British Parliament was fundamentally changed by the great Irish political leader Charles Stewart Parnell in the 1880s. In 1882, he changed his party's name to the Irish Parliamentary Party and created a well-organized grass roots structure, introducing membership to replace "ad hoc" informal groupings. He created a new selection procedure to ensure the professional selection of party candidates committed to taking their seats, and in 1884 he imposed a firm 'party pledge' which obliged MPs to vote as a bloc in parliament on all occasions. The creation of a strict party whip and a formal party structure was unique at the time. His party's efficient structure and control contrasted with the loose rules and flexible informality found in the main British parties; – they soon came to model themselves on the Parnellite model.
A political party is typically led by a party leader (the most powerful member and spokesperson representing the party), a party secretary (who maintains the daily work and records of party meetings), party treasurer (who is responsible for membership dues) and party chair (who forms strategies for recruiting and retaining party members, and also chairs party meetings). Most of the above positions are also members of the party executive, the leading organization which sets policy for the entire party at the national level. The structure is far more decentralized in the United States because of the separation of powers, federalism and the multiplicity of economic interests and religious sects. Even state parties are decentralized as county and other local committees are largely independent of state central committees. The national party leader in the U.S. will be the president, if the party holds that office, or a prominent member of Congress in opposition (although a big-state governor may aspire to that role). Officially, each party has a chairman for its national committee who is a prominent spokesman, organizer and fund-raiser, but without the status of prominent elected office holders.
When the party is represented by members in the lower house of parliament, the party leader simultaneously serves as the leader of the parliamentary group of that full party representation; depending on a minimum number of seats held, Westminster-based parties typically allow for leaders to form frontbench teams of senior fellow members of the parliamentary group to serve as critics of aspects of government policy. When a party becomes the largest party not part of the Government, the party's parliamentary group forms the Official Opposition, with Official Opposition frontbench team members often forming the Official Opposition Shadow cabinet. When a party achieves enough seats in an election to form a majority, the party's frontbench becomes the Cabinet of government ministers.
The freedom to form, declare membership in, or campaign for candidates from a political party is considered a measurement of a state's adherence to liberal democracy as a political value. Regulation of parties may run from a crackdown on or repression of all opposition parties, a norm for authoritarian governments, to the repression of certain parties which hold or promote ideals which run counter to the general ideology of the state's incumbents (or possess membership by-laws which are legally unenforceable).
Furthermore, in the case of far-right, far-left and regionalism parties in the national parliaments of much of the European Union, mainstream political parties may form an informal cordon sanitarian which applies a policy of non-cooperation towards those "Outsider Parties" present in the legislature which are viewed as 'anti-system' or otherwise unacceptable for government. Cordon Sanitarian, however, have been increasingly abandoned over the past two decades in multi-party democracies as the pressure to construct broad coalitions in order to win elections – along with the increased willingness of outsider parties themselves to participate in government – has led to many such parties entering electoral and government coalitions.
In a nonpartisan system, no official political parties exist, sometimes reflecting legal restrictions on political parties. In nonpartisan elections, each candidate is eligible for office on his or her own merits. In nonpartisan legislatures, there are no typically formal party alignments within the legislature. The administration of George Washington and the first few sessions of the United States Congress were nonpartisan. Washington also warned against political parties during his Farewell Address. In the United States, the unicameral legislature of Nebraska is nonpartisan but is elected and votes on informal party lines. In Canada, the territorial legislatures of the Northwest Territories and Nunavut are nonpartisan. In New Zealand, Tokelau has a nonpartisan parliament. Many city and county governments[vague] are nonpartisan. Nonpartisan elections and modes of governance are common outside of state institutions. Unless there are legal prohibitions against political parties, factions within nonpartisan systems often evolve into political parties.
In one-party systems, one political party is legally allowed to hold effective power. Although minor parties may sometimes be allowed, they are legally required to accept the leadership of the dominant party. This party may not always be identical to the government, although sometimes positions within the party may in fact be more important than positions within the government. North Korea and China are examples; others can be found in Fascist states, such as Nazi Germany between 1934 and 1945. The one-party system is thus usually equated with dictatorships and tyranny.
In dominant-party systems, opposition parties are allowed, and there may be even a deeply established democratic tradition, but other parties are widely considered to have no real chance of gaining power. Sometimes, political, social and economic circumstances, and public opinion are the reason for others parties' failure. Sometimes, typically in countries with less of an established democratic tradition, it is possible the dominant party will remain in power by using patronage and sometimes by voting fraud. In the latter case, the definition between dominant and one-party system becomes rather blurred. Examples of dominant party systems include the People's Action Party in Singapore, the African National Congress in South Africa, the Cambodian People's Party in Cambodia, the Liberal Democratic Party in Japan, and the National Liberation Front in Algeria. One-party dominant system also existed in Mexico with the Institutional Revolutionary Party until the 1990s, in the southern United States with the Democratic Party from the late 19th century until the 1970s, in Indonesia with the Golkar from the early 1970s until 1998.
The United States has become essentially a two-party system. Since a conservative (such as the Republican Party) and liberal (such as the Democratic Party) party has usually been the status quo within American politics. The first parties were called Federalist and Republican, followed by a brief period of Republican dominance before a split occurred between National Republicans and Democratic Republicans. The former became the Whig Party and the latter became the Democratic Party. The Whigs survived only for two decades before they split over the spread of slavery, those opposed becoming members of the new Republican Party, as did anti-slavery members of the Democratic Party. Third parties (such as the Libertarian Party) often receive little support and are very rarely the victors in elections. Despite this, there have been several examples of third parties siphoning votes from major parties that were expected to win (such as Theodore Roosevelt in the election of 1912 and George Wallace in the election of 1968). As third party movements have learned, the Electoral College's requirement of a nationally distributed majority makes it difficult for third parties to succeed. Thus, such parties rarely win many electoral votes, although their popular support within a state may tip it toward one party or the other. Wallace had weak support outside the South. More generally, parties with a broad base of support across regions or among economic and other interest groups, have a great chance of winning the necessary plurality in the U.S.'s largely single-member district, winner-take-all elections. The tremendous land area and large population of the country are formidable challenges to political parties with a narrow appeal.
The UK political system, while technically a multi-party system, has functioned generally as a two-party (sometimes called a "two-and-a-half party") system; since the 1920s the two largest political parties have been the Conservative Party and the Labour Party. Before the Labour Party rose in British politics the Liberal Party was the other major political party along with the Conservatives. Though coalition and minority governments have been an occasional feature of parliamentary politics, the first-past-the-post electoral system used for general elections tends to maintain the dominance of these two parties, though each has in the past century relied upon a third party to deliver a working majority in Parliament. (A plurality voting system usually leads to a two-party system, a relationship described by Maurice Duverger and known as Duverger's Law.) There are also numerous other parties that hold or have held a number of seats in Parliament.
More commonly, in cases where there are three or more parties, no one party is likely to gain power alone, and parties work with each other to form coalition governments. This has been an emerging trend in the politics of the Republic of Ireland since the 1980s and is almost always the case in Germany on national and state level, and in most constituencies at the communal level. Furthermore, since the forming of the Republic of Iceland there has never been a government not led by a coalition (usually of the Independence Party and one other (often the Social Democratic Alliance). A similar situation exists in the Republic of Ireland; since 1989, no one party has held power on its own. Since then, numerous coalition governments have been formed. These coalitions have been exclusively led by one of either Fianna Fáil or Fine Gael. Political change is often easier with a coalition government than in one-party or two-party dominant systems.[dubious – discuss] If factions in a two-party system are in fundamental disagreement on policy goals, or even principles, they can be slow to make policy changes, which appears to be the case now in the U.S. with power split between Democrats and Republicans. Still coalition governments struggle, sometimes for years, to change policy and often fail altogether, post World War II France and Italy being prime examples. When one party in a two-party system controls all elective branches, however, policy changes can be both swift and significant. Democrats Woodrow Wilson, Franklin Roosevelt and Lyndon Johnson were beneficiaries of such fortuitous circumstances, as were Republicans as far removed in time as Abraham Lincoln and Ronald Reagan. Barack Obama briefly had such an advantage between 2009 and 2011.
Political parties, still called factions by some, especially those in the governmental apparatus, are lobbied vigorously by organizations, businesses and special interest groups such as trade unions. Money and gifts-in-kind to a party, or its leading members, may be offered as incentives. Such donations are the traditional source of funding for all right-of-centre cadre parties. Starting in the late 19th century these parties were opposed by the newly founded left-of-centre workers' parties. They started a new party type, the mass membership party, and a new source of political fundraising, membership dues.
From the second half of the 20th century on parties which continued to rely on donations or membership subscriptions ran into mounting problems. Along with the increased scrutiny of donations there has been a long-term decline in party memberships in most western democracies which itself places more strains on funding. For example, in the United Kingdom and Australia membership of the two main parties in 2006 is less than an 1/8 of what it was in 1950, despite significant increases in population over that period.
In the United Kingdom, it has been alleged that peerages have been awarded to contributors to party funds, the benefactors becoming members of the House of Lords and thus being in a position to participate in legislating. Famously, Lloyd George was found to have been selling peerages. To prevent such corruption in the future, Parliament passed the Honours (Prevention of Abuses) Act 1925 into law. Thus the outright sale of peerages and similar honours became a criminal act. However, some benefactors are alleged to have attempted to circumvent this by cloaking their contributions as loans, giving rise to the 'Cash for Peerages' scandal.
There are two broad categories of public funding, direct, which entails a monetary transfer to a party, and indirect, which includes broadcasting time on state media, use of the mail service or supplies. According to the Comparative Data from the ACE Electoral Knowledge Network, out of a sample of over 180 nations, 25% of nations provide no direct or indirect public funding, 58% provide direct public funding and 60% of nations provide indirect public funding. Some countries provide both direct and indirect public funding to political parties. Funding may be equal for all parties or depend on the results of previous elections or the number of candidates participating in an election. Frequently parties rely on a mix of private and public funding and are required to disclose their finances to the Election management body.
In fledgling democracies funding can also be provided by foreign aid. International donors provide financing to political parties in developing countries as a means to promote democracy and good governance. Support can be purely financial or otherwise. Frequently it is provided as capacity development activities including the development of party manifestos, party constitutions and campaigning skills. Developing links between ideologically linked parties is another common feature of international support for a party. Sometimes this can be perceived as directly supporting the political aims of a political party, such as the support of the US government to the Georgian party behind the Rose Revolution. Other donors work on a more neutral basis, where multiple donors provide grants in countries accessible by all parties for various aims defined by the recipients. There have been calls by leading development think-tanks, such as the Overseas Development Institute, to increase support to political parties as part of developing the capacity to deal with the demands of interest-driven donors to improve governance.
Green is the color for green parties, Islamist parties, Nordic agrarian parties and Irish republican parties. Orange is sometimes a color of nationalism, such as in the Netherlands, in Israel with the Orange Camp or with Ulster Loyalists in Northern Ireland; it is also a color of reform such as in Ukraine. In the past, Purple was considered the color of royalty (like white), but today it is sometimes used for feminist parties. White also is associated with nationalism. "Purple Party" is also used as an academic hypothetical of an undefined party, as a Centrist party in the United States (because purple is created from mixing the main parties' colors of red and blue) and as a highly idealistic "peace and love" party—in a similar vein to a Green Party, perhaps. Black is generally associated with fascist parties, going back to Benito Mussolini's blackshirts, but also with Anarchism. Similarly, brown is sometimes associated with Nazism, going back to the Nazi Party's tan-uniformed storm troopers.
Political color schemes in the United States diverge from international norms. Since 2000, red has become associated with the right-wing Republican Party and blue with the left-wing Democratic Party. However, unlike political color schemes of other countries, the parties did not choose those colors; they were used in news coverage of 2000 election results and ensuing legal battle and caught on in popular usage. Prior to the 2000 election the media typically alternated which color represented which party each presidential election cycle. The color scheme happened to get inordinate attention that year, so the cycle was stopped lest it cause confusion the following election.
During the 19th and 20th century, many national political parties organized themselves into international organizations along similar policy lines. Notable examples are The Universal Party, International Workingmen's Association (also called the First International), the Socialist International (also called the Second International), the Communist International (also called the Third International), and the Fourth International, as organizations of working class parties, or the Liberal International (yellow), Hizb ut-Tahrir, Christian Democratic International and the International Democrat Union (blue). Organized in Italy in 1945, the International Communist Party, since 1974 headquartered in Florence has sections in six countries.[citation needed] Worldwide green parties have recently established the Global Greens. The Universal Party, The Socialist International, the Liberal International, and the International Democrat Union are all based in London. Some administrations (e.g. Hong Kong) outlaw formal linkages between local and foreign political organizations, effectively outlawing international political parties.
French political scientist Maurice Duverger drew a distinction between cadre parties and mass parties. Cadre parties were political elites that were concerned with contesting elections and restricted the influence of outsiders, who were only required to assist in election campaigns. Mass parties tried to recruit new members who were a source of party income and were often expected to spread party ideology as well as assist in elections.Socialist parties are examples of mass parties, while the British Conservative Party and the German Christian Democratic Union are examples of hybrid parties. In the United States, where both major parties were cadre parties, the introduction of primaries and other reforms has transformed them so that power is held by activists who compete over influence and nomination of candidates.
Historically, the channel's programming consisted mainly of featured classic theatrically released feature films from the Turner Entertainment film library – which comprises films from Warner Bros. Pictures (covering films released before 1950) and Metro-Goldwyn-Mayer (covering films released before May 1986). However, TCM now has licensing deals with other Hollywood film studios as well as its Time Warner sister company, Warner Bros. (which now controls the Turner Entertainment library and its own later films), and occasionally shows more recent films. Turner Classic Movies is a dedicated film channel and is available in United States, United Kingdom, France (TCM Cinéma), Spain (TCM España), Nordic countries, Middle East and Africa.
In 1986, eight years before the launch of Turner Classic Movies, Ted Turner acquired the Metro-Goldwyn-Mayer film studio for $1.5 billion. Concerns over Turner Entertainment's corporate debt load resulted in Turner selling the studio that October back to Kirk Kerkorian, from whom Turner had purchased the studio less than a year before. As part of the deal, Turner Entertainment retained ownership of MGM's library of films released up to May 9, 1986. Turner Broadcasting System was split into two companies; Turner Broadcasting System and Metro-Goldwyn-Mayer and reincorporated as MGM/UA Communications Co.
The film library of Turner Entertainment would serve as the base form of programming for TCM upon the network's launch. Before the creation of Turner Classic Movies, films from Turner's library of movies aired on the Turner Broadcasting System's advertiser-supported cable network TNT – along with colorized versions of black-and-white classics such as The Maltese Falcon. After the library was acquired, MGM/UA signed a deal with Turner to continue distributing the pre-May 1986 MGM and to begin distributing the pre-1950 Warner Bros. film libraries for video release (the rest of the library went to Turner Home Entertainment).
At the time of its launch, TCM was available to approximately one million cable television subscribers. The network originally served as a competitor to AMC – which at the time was known as "American Movie Classics" and maintained a virtually identical format to TCM, as both networks largely focused on films released prior to 1970 and aired them in an uncut, uncolorized, and commercial-free format. AMC had broadened its film content to feature colorized and more recent films by 2002 and abandoned its commercial-free format, leaving TCM as the only movie-oriented cable channel to devote its programming entirely to classic films without commercial interruption.
In 1996, Turner Broadcasting System merged with Time Warner, which besides placing Turner Classic Movies and Warner Bros. Entertainment under the same corporate umbrella, also gave TCM access to Warner Bros.' library of films released after 1949 (which itself includes other acquired entities such as the Lorimar, Saul Zaentz and National General Pictures libraries); incidentally, TCM had already been running select Warner Bros. film titles through a licensing agreement with the studio that was signed prior to the launch of the channel. In March 1999, MGM paid Warner Bros. and gave up the home video rights to the MGM/UA films owned by Turner to Warner Home Video.
In 2000, TCM started the annual Young Composers Film Competition, inviting aspiring composers to participate in a judged competition that offers the winner of each year's competition the opportunity to score a restored, feature-length silent film as a grand prize, mentored by a well-known composer, with the new work subsequently premiering on the network. As of 2006, films that have been rescored include the 1921 Rudolph Valentino film Camille, two Lon Chaney films: 1921's The Ace of Hearts and 1928's Laugh, Clown, Laugh, and Greta Garbo's 1926 film The Temptress.
In 2008, TCM won a Peabody Award for excellence in broadcasting. In April 2010, Turner Classic Movies held the first TCM Classic Film Festival, an event – now held annually – at the Grauman's Chinese Theater and the Grauman's Egyptian Theater in Hollywood. Hosted by Robert Osborne, the four-day long annual festival celebrates Hollywood and its movies, and features celebrity appearances, special events, and screenings of around 50 classic movies including several newly restored by the Film Foundation, an organization devoted to preserving Hollywood's classic film legacy.
Turner Classic Movies essentially operates as a commercial-free service, with the only advertisements on the network being shown between features – which advertise TCM products, network promotions for upcoming special programs and the original trailers for films that are scheduled to be broadcast on TCM (particularly those that will air during the primetime hours), and featurettes about classic film actors and actresses. In addition to this, extended breaks between features are filled with theatrically released movie trailers and classic short subjects – from series such as The Passing Parade, Crime Does Not Pay, Pete Smith Specialties, and Robert Benchley – under the banner name TCM Extras (formerly One Reel Wonders). In 2007, some of the short films featured on TCM were made available for streaming on TCM's website. Partly to allow these interstitials, Turner Classic Movies schedules its feature films either at the top of the hour or at :15, :30 or :45 minutes past the hour, instead of in timeslots of varying five-minute increments.
TCM's film content has remained mostly uncut and uncolorized (with films natively filmed or post-produced in the format being those only ones presented in color), depending upon the original content of movies, particularly movies released after the 1968 implementation of the Motion Picture Association of America's ratings system and the concurrent disestablishment of the Motion Picture Production Code. Because of this, TCM is formatted similarly to a premium channel with certain films – particularly those made from the 1960s onward – sometimes featuring nudity, sexual content, violence and/or strong profanity; the network also features rating bumpers prior to the start of a program (most programs on TCM, especially films, are rated for content using the TV Parental Guidelines, in lieu of the MPAA's rating system).
The network's programming season runs from February until the following March of each year when a retrospective of Oscar-winning and Oscar-nominated movies is shown, called 31 Days of Oscar. As a result of its devoted format to classic feature films, viewers that are interested in tracing the career development of actresses such as Barbara Stanwyck or Greta Garbo or actors like Cary Grant or Humphrey Bogart have the unique opportunity to see most of the films that were made during their careers, from beginning to end. Turner Classic Movies presents many of its features in their original aspect ratio (widescreen or full screen) whenever possible – widescreen films broadcast on TCM are letterboxed on the network's standard definition feed. TCM also regularly presents widescreen presentations of films not available in the format on any home video release.
TCM's library of films spans several decades of cinema and includes thousands of film titles. Besides its deals to broadcast film releases from Metro-Goldwyn-Mayer and Warner Bros. Entertainment, Turner Classic Movies also maintains movie licensing rights agreements with Universal Studios, Paramount Pictures, 20th Century Fox, Walt Disney Studios (primarily film content from Walt Disney Pictures, as well as most of the Selznick International Pictures library), Sony Pictures Entertainment (primarily film content from Columbia Pictures), StudioCanal, and Janus Films.
Most Paramount sound releases made prior to 1950 are owned by EMKA, Ltd./NBCUniversal Television Distribution, while Paramount (currently owned by Viacom) holds on to most of its post-1949 releases, which are distributed for television by Trifecta Entertainment & Media. Columbia's film output is owned by Sony (through Sony Pictures Television); distribution of 20th Century Fox's film library is handled for television by its 21st Century Fox subsidiary 20th Television, and the Walt Disney Studios (owned by The Walt Disney Company) has its library film output handled for television by Disney-ABC Domestic Television. Classic films released by 20th Century Fox, Paramount Pictures, Universal Studios, and Columbia Pictures are licensed individually for broadcast on Turner Classic Movies.
Most feature movies shown during the prime time and early overnight hours (8:00 p.m. to 2:30 a.m. Eastern Time) are presented by film historian Robert Osborne (who has been with the network since its 1994 launch, except for a five-month medical leave from July to December 2011, when guest hosts presented each night's films) on Sunday through Wednesday evenings – with Osborne only presenting primetime films on weekends – and Ben Mankiewicz presenting only late evening films on Thursdays, and the "Silent Sunday Nights" and "TCM Imports" blocks on Sundays.
TCM regularly airs a "Star of the Month" throughout the year on Wednesdays starting at 8:00 p.m. Eastern Time, in which most, if not all, feature films from a classic film star are shown during that night's schedule. Hosted by Robert Osbourne, the network also marks the occurrence of a film actor's birthday (either antemortem or posthumously) or recent death with day- or evening-long festivals showcasting several of that artist's best, earliest or least-known pictures; by effect, marathons scheduled in honor of an actor's passing (which are scheduled within a month after their death) pre-empt films originally scheduled to air on that date. TCM also features a monthly program block called the "TCM Guest Programmer", in which Osborne is joined by celebrity guests responsible for choosing that evening's films (examples of such programmers during 2012 include Jules Feiffer, Anthony Bourdain, Debra Winger, Ellen Barkin, Spike Lee, Regis Philbin and Jim Lehrer); an offshoot of this block featuring Turner Classic Movies employees aired during February 2011.
Turner Classic Movies also airs regularly scheduled weekly film blocks, which are periodically preempted for special themed month-long or seasonal scheduling events, such as the "31 Days of Oscar" film series in the month preceding the Academy Awards and the month-long "Summer Under the Stars" in August; all featured programming has their own distinctive feature presentation bumper for the particular scheduled presentation. The Essentials, currently hosted by Osborne and Sally Field as of 2015[update], is a weekly film showcase airing on Saturday evenings (with a replay on the following Sunday at 6:00 p.m. Eastern Time), which spotlights a different movie and contains a special introduction and post-movie discussion.
The channel also broadcasts two movie blocks during the late evening hours each Sunday: "Silent Sunday Nights", which features silent films from the United States and abroad, usually in the latest restored version and often with new musical scores; and "TCM Imports" (which previously ran on Saturdays until the early 2000s[specify]), a weekly presentation of films originally released in foreign countries. TCM Underground – which debuted in October 2006 – is a Friday late night block which focuses on cult films, the block was originally hosted by rocker/filmmaker Rob Zombie until December 2006 (though as of 2014[update], it is the only regular film presentation block on the channel that does not have a host).
Each August, Turner Classic Movies suspends its regular schedule for a special month of film marathons called "Summer Under the Stars", which features entire daily schedules devoted to the work of a particular actor, with movies and specials that pertain to the star of the day. In the summer of 2007, the channel debuted "Funday Night at the Movies", a block hosted by actor Tom Kenny (best known as the voice of SpongeBob SquarePants). This summer block featured classic feature films (such as The Wizard of Oz, Sounder, Bringing Up Baby, Singin' in the Rain, Mr. Smith Goes to Washington, The Adventures of Robin Hood and 20,000 Leagues Under the Sea) aimed at introducing these movies to new generations of children and their families.
"Funday Night at the Movies" was replaced in 2008 by "Essentials Jr.", a youth-oriented version of its weekly series The Essentials (originally hosted by actors Abigail Breslin and Chris O'Donnell, then by John Lithgow from 2009 to 2011, and then by Bill Hader starting with the 2011 season), which included such family-themed films as National Velvet, Captains Courageous and Yours, Mine and Ours, as well as more eclectic selections as Sherlock, Jr., The Music Box, Harvey, Mutiny on the Bounty and The Man Who Knew Too Much.
In addition to films, Turner Classic Movies also airs original content, mostly documentaries about classic movie personalities, the world of filmmaking and particularly notable films. An occasional month-long series, Race and Hollywood, showcases films by and about people of non-white races, featuring discussions of how these pictures influenced white people's image of said races, as well as how people of those races viewed themselves. Previous installments have included "Asian Images on Film" in 2008, "Native American Images on Film" in 2010, "Black Images on Film" in 2006 "Latino Images on Film" in 2009 and "Arab Images on Film" in 2011. The network aired the film series Screened Out (which explored the history and depiction of homosexuality in film) in 2007 and Religion on Film (focusing on the role of religion in cinematic works) in 2005. In 2011, TCM debuted a new series entitled AFI's Master Class: The Art of Collaboration.
In December 1994, TCM debuted "TCM Remembers", a tribute to recently deceased notable film personalities (including actors, producers, composers, directors, writers and cinematographers) that occasionally airs during promotional breaks between films. The segments appear in two forms: individual tributes and a longer end-of-year compilation. Following the recent death of an especially famous classic film personality (usually an actor, producer, filmmaker or director), the segment will feature a montage of select shots of the deceased's work. Every December, a longer, more inclusive "TCM Remembers" interstitial is produced that honors all of the noted film personalities who died during the past year, interspersed with scenes from settings such as an abandoned drive-in (2012) or a theatre which is closing down and is being dismantled (2013). Since 2001, the soundtracks for these clipreels have been introspective melodies by indie artists such as Badly Drawn Boy (2007) or Steve Earle (2009).
The TCM Vault Collection consists of several different DVD collections of rare classic films that have been licensed, remastered and released by Turner Classic Movies (through corporate sister Warner Home Video). These boxed set releases are of films by notable actors, directors or studios that were previously unreleased on DVD or VHS. The sets often include bonus discs including documentaries and shorts from the TCM library. The initial batch of DVDs are printed in limited quantities and subsequent batches are made-on-demand (MOD).
In October 2015, TCM announced the launch of the TCM Wineclub, in which they teamed up with Laithwaite to provide a line of mail-order wines from famous vineyards such as famed writer-director-producer Francis Ford Coppola's winery. Wines are available in 3 month subscriptions, and can be selected as reds, whites, or a mixture of both. From the wines chosen, TCM also includes recommended movies to watch with each, such as a "True Grit" wine, to be paired with the John Wayne film of the same name.
Turner Classic Movies is available in many other countries around the world. In Canada, TCM began to be carried on Shaw Cable and satellite provider Shaw Direct in 2005. Rogers Cable started offering TCM in December 2006 as a free preview for subscribers of its digital cable tier, and was added to its analogue tier in February 2007. While the schedule for the Canadian feed is generally the same as that of the U.S. network, some films are replaced for broadcast in Canada due to rights issues and other reasons. Other versions of TCM are available in Australia, France, Middle East, South Africa, Cyprus, Spain, Asia, Latin America, Scandinavia, the United Kingdom, Ireland and Malta. The UK version operates two channels, including a spinoff called TCM 2.
According to the canonical gospels, Jesus, whom Christians believe to be the Son of God as well as the Messiah (Christ), was arrested, tried, and sentenced by Pontius Pilate to be scourged, and finally crucified by the Romans. Jesus was stripped of his clothing and offered wine mixed with gall to drink, before being crucified. He was then hung for six hours (according to Mark's Gospel) between two convicted thieves. During this time, the soldiers affixed a sign to the top of the cross stating "Jesus of Nazareth, King of the Jews" in three languages. They then divided his garments among them, but cast lots for his seamless robe. After Jesus' death they pierced his side with a spear to be certain that he had died. The Bible records seven statements that Jesus made while he was on the cross, as well as several supernatural events that occurred.
The baptism of Jesus and his crucifixion are considered to be two historically certain facts about Jesus. James Dunn states that these "two facts in the life of Jesus command almost universal assent" and "rank so high on the 'almost impossible to doubt or deny' scale of historical facts" that they are often the starting points for the study of the historical Jesus. Bart Ehrman states that the crucifixion of Jesus on the orders of Pontius Pilate is the most certain element about him. John Dominic Crossan states that the crucifixion of Jesus is as certain as any historical fact can be. Eddy and Boyd state that it is now "firmly established" that there is non-Christian confirmation of the crucifixion of Jesus. Craig Blomberg states that most scholars in the third quest for the historical Jesus consider the crucifixion indisputable. Christopher M. Tuckett states that, although the exact reasons for the death of Jesus are hard to determine, one of the indisputable facts about him is that he was crucified.
Although almost all ancient sources relating to crucifixion are literary, the 1968 archeological discovery just northeast of Jerusalem of the body of a crucified man dated to the 1st century provided good confirmatory evidence that crucifixions occurred during the Roman period roughly according to the manner in which the crucifixion of Jesus is described in the gospels. The crucified man was identified as Yehohanan ben Hagkol and probably died about 70 AD, around the time of the Jewish revolt against Rome. The analyses at the Hadassah Medical School estimated that he died in his late 20s. Another relevant archaeological find, which also dates to the 1st century AD, is an unidentified heel bone with a spike discovered in a Jerusalem gravesite, now held by the Israel Antiquities Authority and displayed in the Israel Museum.
The earliest detailed accounts of the death of Jesus are contained in the four canonical gospels. There are other, more implicit references in the New Testament epistles. In the synoptic gospels, Jesus predicts his death in three separate episodes. All four Gospels conclude with an extended narrative of Jesus' arrest, trial, crucifixion, burial, and accounts of resurrection. In each Gospel these five events in the life of Jesus are treated with more intense detail than any other portion of that Gospel's narrative. Scholars note that the reader receives an almost hour-by-hour account of what is happening.:p.91
Combining statements in the canonical Gospels produces the following account: Jesus was arrested in Gethsemane following the Last Supper with the Twelve Apostles, and then stood trial before the Sanhedrin (a Jewish judicial body), Pontius Pilate (a Roman authority in Judaea), and Herod Antipas (king of Judea, appointed by Rome), before being handed over for crucifixion by the chief priests of the Jews. After being flogged, Jesus was mocked by Roman soldiers as the "King of the Jews", clothed in a purple robe, crowned with thorns, beaten and spat on. Jesus then had to make his way to the place of his crucifixion.
Once at Golgotha, Jesus was offered wine mixed with gall to drink. Matthew's and Mark's Gospels record that he refused this. He was then crucified and hung between two convicted thieves. According to some translations from the original Greek, the thieves may have been bandits or Jewish rebels. According to Mark's Gospel, he endured the torment of crucifixion for some six hours from the third hour, at approximately 9 am, until his death at the ninth hour, corresponding to about 3 pm. The soldiers affixed a sign above his head stating "Jesus of Nazareth, King of the Jews" in three languages, divided his garments and cast lots for his seamless robe. The Roman soldiers did not break Jesus' legs, as they did to the other two men crucified (breaking the legs hastened the crucifixion process), as Jesus was dead already. Each gospel has its own account of Jesus' last words, seven statements altogether. In the Synoptic Gospels, various supernatural events accompany the crucifixion, including darkness, an earthquake, and (in Matthew) the resurrection of saints. Following Jesus' death, his body was removed from the cross by Joseph of Arimathea and buried in a rock-hewn tomb, with Nicodemus assisting.
There are several details that are only found in one of the gospel accounts. For instance, only Matthew's gospel mentions an earthquake, resurrected saints who went to the city and that Roman soldiers were assigned to guard the tomb, while Mark is the only one to state the actual time of the crucifixion (the third hour, or 9 am) and the centurion's report of Jesus' death. The Gospel of Luke's unique contributions to the narrative include Jesus' words to the women who were mourning, one criminal's rebuke of the other, the reaction of the multitudes who left "beating their breasts", and the women preparing spices and ointments before resting on the Sabbath. John is also the only one to refer to the request that the legs be broken and the soldier's subsequent piercing of Jesus' side (as fulfillment of Old Testament prophecy), as well as that Nicodemus assisted Joseph with burial.
According to the First Epistle to the Corinthians (1 Cor. 15:4), Jesus was raised from the dead ("on the third day" counting the day of crucifixion as the first) and according to the canonical Gospels, appeared to his disciples on different occasions before ascending to heaven. The account given in Acts of the Apostles, which says Jesus remained with the apostles for forty days, appears to differ from the account in the Gospel of Luke, which makes no clear distinction between the events of Easter Sunday and the Ascension. However, most biblical scholars agree that St. Luke also wrote the Acts of the Apostles as a follow-up volume to his Gospel account, and the two works must be considered as a whole.
In Mark, Jesus is crucified along with two rebels, and the day goes dark for three hours. Jesus calls out to God, then gives a shout and dies. The curtain of the Temple is torn in two. Matthew follows Mark, adding an earthquake and the resurrection of saints. Luke also follows Mark, though he describes the rebels as common criminals, one of whom defends Jesus, who in turn promises that he (Jesus) and the criminal will be together in paradise. Luke portrays Jesus as impassive in the face of his crucifixion. John includes several of the same elements as those found in Mark, though they are treated differently.
An early non-Christian reference to the crucifixion of Jesus is likely to be Mara Bar-Serapion's letter to his son, written sometime after AD 73 but before the 3rd century AD. The letter includes no Christian themes and the author is presumed to be a pagan. The letter refers to the retributions that followed the unjust treatment of three wise men: Socrates, Pythagoras, and "the wise king" of the Jews. Some scholars see little doubt that the reference to the execution of the "king of the Jews" is about the crucifixion of Jesus, while others place less value in the letter, given the possible ambiguity in the reference.
The consensus of modern scholarship is that the New Testament accounts represent a crucifixion occurring on a Friday, but a Thursday or Wednesday crucifixion have also been proposed. Some scholars explain a Thursday crucifixion based on a "double sabbath" caused by an extra Passover sabbath falling on Thursday dusk to Friday afternoon, ahead of the normal weekly Sabbath. Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday. Others have countered by saying that this ignores the Jewish idiom by which a "day and night" may refer to any part of a 24-hour period, that the expression in Matthew is idiomatic, not a statement that Jesus was 72 hours in the tomb, and that the many references to a resurrection on the third day do not require three literal nights.
In Mark 15:25 crucifixion takes place at the third hour (9 a.m.) and Jesus' death at the ninth hour (3 p.m.). However, in John 19:14 Jesus is still before Pilate at the sixth hour. Scholars have presented a number of arguments to deal with the issue, some suggesting a reconciliation, e.g., based on the use of Roman timekeeping in John but not in Mark, yet others have rejected the arguments. Several notable scholars have argued that the modern precision of marking the time of day should not be read back into the gospel accounts, written at a time when no standardization of timepieces, or exact recording of hours and minutes was available, and time was often approximated to the closest three-hour period.
Luke's gospel also describes an interaction between Jesus and the women among the crowd of mourners following him, quoting Jesus as saying "Daughters of Jerusalem, do not weep for me, but weep for yourselves and for your children. For behold, the days are coming when they will say, 'Blessed are the barren and the wombs that never bore and the breasts that never nursed!' Then they will begin to say to the mountains, 'Fall on us,' and to the hills, 'Cover us.' For if they do these things when the wood is green, what will happen when it is dry?"[Lk. 23:28-31]
Calvary as an English name for the place is derived from the Latin word for skull (calvaria), which is used in the Vulgate translation of "place of a skull", the explanation given in all four Gospels of the Aramaic word Gûlgaltâ which was the name of the place where Jesus was crucified. The text does not indicate why it was so designated, but several theories have been put forward. One is that as a place of public execution, Calvary may have been strewn with the skulls of abandoned victims (which would be contrary to Jewish burial traditions, but not Roman). Another is that Calvary is named after a nearby cemetery (which is consistent with both of the proposed modern sites). A third is that the name was derived from the physical contour, which would be more consistent with the singular use of the word, i.e., the place of "a skull". While often referred to as "Mount Calvary", it was more likely a small hill or rocky knoll.
The Gospel of Matthew describes many women at the crucifixion, some of whom are named in the Gospels. Apart from these women, the three Synoptic Gospels speak of the presence of others: "the chief priests, with the scribes and elders"; two robbers crucified, one on Jesus' right and one on his left, whom the Gospel of Luke presents as the penitent thief and the impenitent thief; "the soldiers", "the centurion and those who were with him, keeping watch over Jesus"; passers-by; "bystanders", "the crowds that had assembled for this spectacle"; and "his acquaintances"
Whereas most Christians believe the gibbet on which Jesus was executed was the traditional two-beamed cross, the Jehovah's Witnesses hold the view that a single upright stake was used. The Greek and Latin words used in the earliest Christian writings are ambiguous. The Koine Greek terms used in the New Testament are stauros (σταυρός) and xylon (ξύλον). The latter means wood (a live tree, timber or an object constructed of wood); in earlier forms of Greek, the former term meant an upright stake or pole, but in Koine Greek it was used also to mean a cross. The Latin word crux was also applied to objects other than a cross.
However, early Christian writers who speak of the shape of the particular gibbet on which Jesus died invariably describe it as having a cross-beam. For instance, the Epistle of Barnabas, which was certainly earlier than 135, and may have been of the 1st century AD, the time when the gospel accounts of the death of Jesus were written, likened it to the letter T (the Greek letter tau, which had the numeric value of 300), and to the position assumed by Moses in Exodus 17:11–12. Justin Martyr (100–165) explicitly says the cross of Christ was of two-beam shape: "That lamb which was commanded to be wholly roasted was a symbol of the suffering of the cross which Christ would undergo. For the lamb, which is roasted, is roasted and dressed up in the form of the cross. For one spit is transfixed right through from the lower parts up to the head, and one across the back, to which are attached the legs of the lamb." Irenaeus, who died around the end of the 2nd century, speaks of the cross as having "five extremities, two in length, two in breadth, and one in the middle, on which [last] the person rests who is fixed by the nails."
The assumption of the use of a two-beamed cross does not determine the number of nails used in the crucifixion and some theories suggest three nails while others suggest four nails. However, throughout history larger numbers of nails have been hypothesized, at times as high as 14 nails. These variations are also present in the artistic depictions of the crucifixion. In the Western Church, before the Renaissance usually four nails would be depicted, with the feet side by side. After the Renaissance most depictions use three nails, with one foot placed on the other. Nails are almost always depicted in art, although Romans sometimes just tied the victims to the cross. The tradition also carries to Christian emblems, e.g. the Jesuits use three nails under the IHS monogram and a cross to symbolize the crucifixion.
The placing of the nails in the hands, or the wrists is also uncertain. Some theories suggest that the Greek word cheir (χειρ) for hand includes the wrist and that the Romans were generally trained to place nails through Destot's space (between the capitate and lunate bones) without fracturing any bones. Another theory suggests that the Greek word for hand also includes the forearm and that the nails were placed near the radius and ulna of the forearm. Ropes may have also been used to fasten the hands in addition to the use of nails.
Another issue has been the use of a hypopodium as a standing platform to support the feet, given that the hands may not have been able to support the weight. In the 17th century Rasmus Bartholin considered a number of analytical scenarios of that topic. In the 20th century, forensic pathologist Frederick Zugibe performed a number of crucifixion experiments by using ropes to hang human subjects at various angles and hand positions. His experiments support an angled suspension, and a two-beamed cross, and perhaps some form of foot support, given that in an Aufbinden form of suspension from a straight stake (as used by the Nazis in the Dachau concentration camp during World War II), death comes rather quickly.
The only words of Jesus on the cross in the Mark and Matthew accounts, this is a quotation of Psalm 22. Since other verses of the same Psalm are cited in the crucifixion accounts, it is often considered a literary and theological creation. Geza Vermes, however, points out that the verse is cited in Aramaic rather than the Hebrew in which it usually would have been recited, and suggests that by the time of Jesus, this phrase had become a proverbial saying in common usage. Compared to the accounts in the other Gospels, which he describes as 'theologically correct and reassuring', he considers this phrase 'unexpected, disquieting and in consequence more probable'. He describes it as bearing 'all the appearances of a genuine cry'. Raymond Brown likewise comments that he finds 'no persuasive argument against attributing to the Jesus of Mark/Matt the literal sentiment of feeling forsaken expressed in the Psalm quote'.
Some Christian writers considered the possibility that pagan commentators may have mentioned this event, mistaking it for a solar eclipse - although this would have been impossible during the Passover, which takes place at the full moon. Christian traveller and historian Sextus Julius Africanus and Christian theologian Origen refer to Greek historian Phlegon, who lived in the 2nd century AD, as having written "with regard to the eclipse in the time of Tiberius Caesar, in whose reign Jesus appears to have been crucified, and the great earthquakes which then took place"
Sextus Julius Africanus further refers to the writings of historian Thallus: "This darkness Thallus, in the third book of his History, calls, as appears to me without reason, an eclipse of the sun. For the Hebrews celebrate the passover on the 14th day according to the moon, and the passion of our Saviour falls on the day before the passover; but an eclipse of the sun takes place only when the moon comes under the sun." Christian apologist Tertullian believed the event was documented in the Roman archives.
Colin Humphreys and W. G. Waddington of Oxford University considered the possibility that a lunar, rather than solar, eclipse might have taken place. They concluded that such an eclipse would have been visible, for thirty minutes, from Jerusalem and suggested the gospel reference to a solar eclipse was the result of a scribe wrongly amending a text. Historian David Henige dismisses this explanation as 'indefensible' and astronomer Bradley Schaefer points out that the lunar eclipse would not have been visible during daylight hours.
Modern biblical scholarship treats the account in the synoptic gospels as a literary creation by the author of the Mark Gospel, amended in the Luke and Matthew accounts, intended to heighten the importance of what they saw as a theologically significant event, and not intended to be taken literally. This image of darkness over the land would have been understood by ancient readers, a typical element in the description of the death of kings and other major figures by writers such as Philo, Dio Cassius, Virgil, Plutarch and Josephus. Géza Vermes describes the darkness account as typical of "Jewish eschatological imagery of the day of the Lord", and says that those interpreting it as a datable eclipse are "barking up the wrong tree".
In his book The Crucifixion of Jesus, physician and forensic pathologist Frederick Zugibe studied the likely circumstances of the death of Jesus in great detail. Zugibe carried out a number of experiments over several years to test his theories while he was a medical examiner. These studies included experiments in which volunteers with specific weights were hanging at specific angles and the amount of pull on each hand was measured, in cases where the feet were also secured or not. In these cases the amount of pull and the corresponding pain was found to be significant.
Christians believe that Jesus’ death was instrumental in restoring humankind to relationship with God. Christians believe that through faith in Jesus’ substitutionary death and triumphant resurrection people are reunited with God and receive new joy and power in this life as well as eternal life in heaven after the body’s death. Thus the crucifixion of Jesus along with his resurrection restores access to a vibrant experience of God’s presence, love and grace as well as the confidence of eternal life.
In Johannine "agent Christology" the submission of Jesus to crucifixion is a sacrifice made as an agent of God or servant of God, for the sake of eventual victory. This builds on the salvific theme of the Gospel of John which begins in John 1:29 with John the Baptist's proclamation: "The Lamb of God who takes away the sins of the world". Further reinforcement of the concept is provided in Revelation 21:14 where the "lamb slain but standing" is the only one worthy of handling the scroll (i.e. the book) containing the names of those who are to be saved.
Paul's Christology has a specific focus on the death and resurrection of Jesus. For Paul, the crucifixion of Jesus is directly related to his resurrection and the term "the cross of Christ" used in Galatians 6:12 may be viewed as his abbreviation of the message of the gospels. For Paul, the crucifixion of Jesus was not an isolated event in history, but a cosmic event with significant eschatological consequences, as in 1 Corinthians 2:8. In the Pauline view, Jesus, obedient to the point of death (Philippians 2:8) died "at the right time" (Romans 4:25) based on the plan of God. For Paul the "power of the cross" is not separable from the Resurrection of Jesus.
John Calvin supported the "agent of God" Christology and argued that in his trial in Pilate's Court Jesus could have successfully argued for his innocence, but instead submitted to crucifixion in obedience to the Father. This Christological theme continued into the 20th century, both in the Eastern and Western Churches. In the Eastern Church Sergei Bulgakov argued that the crucifixion of Jesus was "pre-eternally" determined by the Father before the creation of the world, to redeem humanity from the disgrace caused by the fall of Adam. In the Western Church, Karl Rahner elaborated on the analogy that the blood of the Lamb of God (and the water from the side of Jesus) shed at the crucifixion had a cleansing nature, similar to baptismal water.
Jesus' death and resurrection underpin a variety of theological interpretations as to how salvation is granted to humanity. These interpretations vary widely in how much emphasis they place on the death of Jesus as compared to his words. According to the substitutionary atonement view, Jesus' death is of central importance, and Jesus willingly sacrificed himself as an act of perfect obedience as a sacrifice of love which pleased God. By contrast the moral influence theory of atonement focuses much more on the moral content of Jesus' teaching, and sees Jesus' death as a martyrdom. Since the Middle Ages there has been conflict between these two views within Western Christianity. Evangelical Protestants typically hold a substitutionary view and in particular hold to the theory of penal substitution. Liberal Protestants typically reject substitutionary atonement and hold to the moral influence theory of atonement. Both views are popular within the Roman Catholic church, with the satisfaction doctrine incorporated into the idea of penance.
The presence of the Virgin Mary under the cross[Jn. 19:26-27] has in itself been the subject of Marian art, and well known Catholic symbolism such as the Miraculous Medal and Pope John Paul II's Coat of Arms bearing a Marian Cross. And a number of Marian devotions also involve the presence of the Virgin Mary in Calvary, e.g., Pope John Paul II stated that "Mary was united to Jesus on the Cross". Well known works of Christian art by masters such as Raphael (e.g., the Mond Crucifixion), and Caravaggio (e.g., his Entombment) depict the Virgin Mary as part of the crucifixion scene.
A fleet carrier is intended to operate with the main fleet and usually provides an offensive capability. These are the largest carriers capable of fast speeds. By comparison, escort carriers were developed to provide defense for convoys of ships. They were smaller and slower with lower numbers of aircraft carried. Most were built from mercantile hulls or, in the case of merchant aircraft carriers, were bulk cargo ships with a flight deck added on top. Light aircraft carriers were carriers that were fast enough to operate with the fleet but of smaller size with reduced aircraft capacity. Soviet aircraft carriers now in use by Russia are actually called heavy aviation cruisers, these ships while sized in the range of large fleet carriers were designed to deploy alone or with escorts and provide both strong defensive weaponry and heavy offensive missiles equivalent to a guided missile cruiser in addition to supporting fighters and helicopters.
This new-found importance of naval aviation forced nations to create a number of carriers, in efforts to provide air superiority cover for every major fleet in order to ward off enemy aircraft. This extensive usage required the construction of several new 'light' carriers. Escort aircraft carriers, such as USS Bogue, were sometimes purpose-built, but most were converted from merchant ships as a stop-gap measure to provide anti-submarine air support for convoys and amphibious invasions. Following this concept, light aircraft carriers built by the US, such as USS Independence, represented a larger, more "militarized" version of the escort carrier. Although with similar complement to Escort carriers, they had the advantage of speed from their converted cruiser hulls. The UK 1942 Design Light Fleet Carrier was designed for building quickly by civilian shipyards and with an expected service life of about 3 years. They served the Royal Navy during the war and was the hull design chosen for nearly all aircraft carrier equipped navies after the war until the 1980s. Emergencies also spurred the creation or conversion of highly unconventional aircraft carriers. CAM ships, were cargo-carrying merchant ships that could launch (but not retrieve) a single fighter aircraft from a catapult to defend the convoy from long range German aircraft.
Speaking in St. Petersburg, Russia on 30 June 2011, the head of Russia's United Shipbuilding Corporation said his company expected to begin design work for a new carrier in 2016, with a goal of beginning construction in 2018 and having the carrier achieve initial operational capability by 2023. Several months later, on 3 November 2011 the Russian newspaper Izvestiya reported that the naval building plan now included (first) the construction of a new shipyard capable of building large hull ships, after which Moscow will build two (80,000 tons full load each) nuclear-powered aircraft carriers by 2027. The spokesperson said one carrier would be assigned to the Russian Navy's Northern Fleet at Murmansk, and the second would be stationed with the Pacific Fleet at Vladivostok.
As "runways at sea", aircraft carriers have a flat-top flight deck, which launches and recovers aircraft. Aircraft launch forward, into the wind, and are recovered from astern. The flight deck is where the most notable differences between a carrier and a land runway are found. Creating such a surface at sea poses constraints on the carrier – for example, the fact that it is a ship means that a full-length runway would be costly to construct and maintain. This affects take-off procedure, as a shorter runway length of the deck requires that aircraft accelerate more quickly to gain lift. This either requires a thrust boost, a vertical component to its velocity, or a reduced take-off load (to lower mass). The differing types of deck configuration, as above, influence the structure of the flight deck. The form of launch assistance a carrier provides is strongly related to the types of aircraft embarked and the design of the carrier itself.
Since the early 1950s on conventional carriers it has been the practice to recover aircraft at an angle to port of the axial line of the ship. The primary function of this angled deck is to allow aircraft that miss the arresting wires, referred to as a bolter, to become airborne again without the risk of hitting aircraft parked forward. The angled deck allows the installation of one or two "waist" catapults in addition to the two bow cats. An angled deck also improves launch and recovery cycle flexibility with the option of simultaneous launching and recovery of aircraft.
Another deck structure that can be seen is a ski-jump ramp at the forward end of the flight deck. This was first developed to help launch STOVL aircraft take off at far higher weights than is possible with a vertical or rolling takeoff on flat decks. Originally developed by the Royal Navy, it since has been adopted by many navies for smaller carriers. A ski-jump ramp works by converting some of the forward rolling movement of the aircraft into vertical velocity and is sometimes combined with the aiming of jet thrust partly downwards. This allows heavily loaded and fueled aircraft a few more precious seconds to attain sufficient air velocity and lift to sustain normal flight. Without a ski-jump launching fully loaded and fueled aircraft such as the Harrier would not be possible on a smaller flat deck ship before either stalling out or crashing directly into the sea.
An aircraft carrier is a warship that serves as a seagoing airbase, equipped with a full-length flight deck and facilities for carrying, arming, deploying, and recovering aircraft. Typically, it is the capital ship of a fleet, as it allows a naval force to project air power worldwide without depending on local bases for staging aircraft operations. Aircraft carriers are expensive to build and are critical assets. Aircraft carriers have evolved from converted cruisers to nuclear-powered warships that carry numerous fighter planes, strike aircraft, helicopters, and other types of aircraft.
The 1903 advent of heavier-than-air fixed-wing aircraft was closely followed in 1910 by the first experimental take-off of an airplane, made from the deck of a United States Navy vessel (cruiser USS Birmingham), and the first experimental landings were conducted in 1911. On 9 May 1912 the first airplane take-off from a ship underway was made from the deck of the British Royal Navy's HMS Hibernia. Seaplane tender support ships came next, with the French Foudre of 1911. In September 1914 the Imperial Japanese Navy Wakamiya conducted the world's first successful ship-launched air raid: on 6 September 1914 a Farman aircraft launched by Wakamiya attacked the Austro-Hungarian cruiser SMS Kaiserin Elisabeth and the German gunboat Jaguar in Kiaochow Bay off Tsingtao; neither was hit. The first carrier-launched airstrike was the Tondern Raid in July 1918. Seven Sopwith Camels launched from the converted battlecruiser HMS Furious damaged the German airbase at Tønder and destroyed two zeppelins.
Modern navies that operate such aircraft carriers treat them as the capital ship of the fleet, a role previously held by the battleship. This change took place during World War II in response to air power becoming a significant factor in warfare, driven by the superior range, flexibility and effectiveness of carrier-launched aircraft. Following the war, carrier operations continued to increase in size and importance. Supercarriers, displacing 75,000 tonnes or greater, have become the pinnacle of carrier development. Some are powered by nuclear reactors and form the core of a fleet designed to operate far from home. Amphibious assault ships, such as USS Tarawa and HMS Ocean, serve the purpose of carrying and landing Marines, and operate a large contingent of helicopters for that purpose. Also known as "commando carriers" or "helicopter carriers", many have the capability to operate VSTOL aircraft.
The Royal Australian Navy is in the process of procuring two Canberra-class LHD's, the first of which was commissioned in November 2015, while the second is expected to enter service in 2016. The ships will be the largest in Australian naval history. Their primary roles are to embark, transport and deploy an embarked force and to carry out or support humanitarian assistance missions. The LHD is capable of launching multiple helicopters at one time while maintaining an amphibious capability of 1,000 troops and their supporting vehicles (tanks, armoured personnel carriers etc.). The Australian Defence Minister has publicly raised the possibility of procuring F-35B STOVL aircraft for the carrier, stating that it "has been on the table since day one and stating the LHD's are "STOVL capable".
The British Royal Navy is constructing two new larger STOVL aircraft carriers, the Queen Elizabeth class, to replace the three Invincible-class carriers. The ships will be named HMS Queen Elizabeth and HMS Prince of Wales. They will be able to operate up to 40 aircraft in peace time with a tailored group of up to 50, and will have a displacement of 70,600 tonnes. The ships are due to become operational from 2020. Their primary aircraft complement will be made up of F-35B Lightning IIs, and their ship's company will number around 680 with the total complement rising to about 1,600 when the air group is embarked. Defensive weapons will include the Phalanx Close-In Weapons System for anti-aircraft and anti-missile defence; also 30 mm Automated Small Calibre Guns and miniguns for use against fast attack craft. The two ships will be the largest warships ever built for the Royal Navy.
The constraints of constructing a flight deck affect the role of a given carrier strongly, as they influence the weight, type, and configuration of the aircraft that may be launched. For example, assisted launch mechanisms are used primarily for heavy aircraft, especially those loaded with air-to-ground weapons. CATOBAR is most commonly used on USN supercarriers as it allows the deployment of heavy jets with full loadouts, especially on ground-attack missions. STOVL is used by other navies because it is cheaper to operate and still provides good deployment capability for fighter aircraft.
On the recovery side of the flight deck, the adaptation to the aircraft loadout is mirrored. Non-VTOL or conventional aircraft cannot decelerate on their own, and almost all carriers using them must have arrested-recovery systems (-BAR, e.g. CATOBAR or STOBAR) to recover their aircraft. Aircraft that are landing extend a tailhook that catches on arrestor wires stretched across the deck to bring themselves to a stop in a short distance. Post-WWII Royal Navy research on safer CATOBAR recovery eventually led to universal adoption of a landing area angled off axis to allow aircraft who missed the arresting wires to "bolt" and safely return to flight for another landing attempt rather than crashing into aircraft on the forward deck.
Key personnel involved in the flight deck include the shooters, the handler, and the air boss. Shooters are naval aviators or Naval Flight Officers and are responsible for launching aircraft. The handler works just inside the island from the flight deck and is responsible for the movement of aircraft before launching and after recovery. The "air boss" (usually a commander) occupies the top bridge (Primary Flight Control, also called primary or the tower) and has the overall responsibility for controlling launch, recovery and "those aircraft in the air near the ship, and the movement of planes on the flight deck, which itself resembles a well-choreographed ballet." The captain of the ship spends most of his time one level below primary on the Navigation Bridge. Below this is the Flag Bridge, designated for the embarked admiral and his staff.
The disadvantage of the ski-jump is the penalty it exacts on aircraft size, payload, and fuel load (and thus range); heavily laden aircraft can not launch using a ski-jump because their high loaded weight requires either a longer takeoff roll than is possible on a carrier deck, or assistance from a catapult or JATO rocket. For example, the Russian Su-33 is only able to launch from the carrier Admiral Kuznetsov with a minimal armament and fuel load. Another disadvantage is on mixed flight deck operations where helicopters are also present such as a US Landing Helicopter Dock or Landing Helicopter Assault amphibious assault ship a ski jump is not included as this would eliminate one or more helicopter landing areas, this flat deck limits the loading of Harriers but is somewhat mitigated by the longer rolling start provided by a long flight deck compared to many STOVL carriers.
One STOBAR carrier: Liaoning was originally built as the 57,000 tonne Soviet Admiral Kuznetsov-class carrier Varyag and was later purchased as a stripped hulk by China in 1998 on the pretext of use as a floating casino, then partially rebuilt and towed to China for completion. Liaoning was commissioned on 25 September 2012, and began service for testing and training. On 24 or 25 November 2012, Liaoning successfully launched and recovered several Shenyang J-15 jet fighter aircraft. She is classified as a training ship, intended to allow the navy to practice with carrier usage. On 26 December 2012, the People's Daily reported that it will take four to five years for Liaoning to reach full capacity, mainly due to training and coordination which will take significant amount of time for Chinese PLA Navy to complete as this is the first aircraft carrier in their possession. As it is a training ship, Liaoning is not assigned to any of China's operation fleets.
India started the construction of a 40,000-tonne, 260-metre-long (850 ft) Vikrant-class aircraft carrier in 2009. The new carrier will operate MiG-29K and naval HAL Tejas aircraft along with the Indian-made helicopter HAL Dhruv. The ship will be powered by four gas-turbine engines and will have a range of 8,000 nautical miles (15,000 kilometres), carrying 160 officers, 1,400 sailors, and 30 aircraft. The carrier is being constructed by Cochin Shipyard. The ship was launched in August 2013 and is scheduled for commissioning in 2018.
Carriers have evolved since their inception in the early twentieth century from wooden vessels used to deploy balloons to nuclear-powered warships that carry dozens of aircraft, including fighter jets and helicopters. As of 3 March 2016, there are thirty-seven active aircraft carriers in the world within twelve navies. The United States Navy has 10 large nuclear-powered carriers (known as supercarriers, carrying up to 90 aircraft each), the largest carriers in the world; the total deckspace is over twice that of all other nations' combined. As well as the supercarrier fleet, the US Navy has nine amphibious assault ships used primarily for helicopters (sometimes called helicopter carriers); these can also carry up to 25 fighter jets, and in some cases, are as large as some other nations' fixed-wing carriers.
There is no single definition of an "aircraft carrier", and modern navies use several variants of the type. These variants are sometimes categorized as sub-types of aircraft carriers, and sometimes as distinct types of naval aviation-capable ships. Aircraft carriers may be classified according to the type of aircraft they carry and their operational assignments. Admiral Sir Mark Stanhope, former head of the Royal Navy, has said that "To put it simply, countries that aspire to strategic international influence have aircraft carriers".
The aircraft carrier dramatically changed naval combat in World War II, because air power was becoming a significant factor in warfare. The advent of aircraft as focal weapons was driven by the superior range, flexibility and effectiveness of carrier-launched aircraft. They had higher range and precision than naval guns, making them highly effective. The versatility of the carrier was demonstrated in November 1940 when HMS Illustrious launched a long-range strike on the Italian fleet at their base in Taranto, signalling the beginning of the effective and highly mobile aircraft strikes. This operation incapacitated three of the six battleships at a cost of two torpedo bombers. World War II in the Pacific Ocean involved clashes between aircraft carrier fleets. The 1941 Japanese surprise attack on Pearl Harbor was a clear illustration of the power projection capability afforded by a large force of modern carriers. Concentrating six carriers in a single unit turned naval history about, as no other nation had fielded anything comparable. However, the vulnerability of carriers compared to traditional battleships when forced into a gun-range encounter was quickly illustrated by the sinking of HMS Glorious by German battleships during the Norwegian campaign in 1940.
The development of flattop vessels produced the first large fleet ships. In 1918, HMS Argus became the world's first carrier capable of launching and recovering naval aircraft. As a result of the Washington Naval Treaty of 1922, which limited the construction of new heavy surface combat ships, most early aircraft carriers were conversions of ships that were laid down (or had served) as different ship types: cargo ships, cruisers, battlecruisers, or battleships. These conversions gave rise to the Lexington-class aircraft carriers (1927), Akagi and Courageous class. Specialist carrier evolution was well underway, with several navies ordering and building warships that were purposefully designed to function as aircraft carriers by the mid-1920s, resulting in the commissioning of ships such as Hōshō (1922), HMS Hermes (1924), and Béarn (1927). During World War II, these ships would become known as fleet carriers.[citation needed]
In December 2009, then Indian Navy chief Admiral Nirmal Kumar Verma said at his maiden navy week press conference that concepts currently being examined by the Directorate of Naval Design for the second indigenous aircraft carrier (IAC-2), are for a conventionally powered carrier displacing over 50,000 tons and equipped with steam catapults (rather than the ski-jump on the Gorshkov/Vikramaditya and the IAC) to launch fourth-generation aircraft. Later on in August 2013 Vice Admiral RK Dhowan, while talking about the detailed study underway on the IAC-II project, said that nuclear propulsion was also being considered. The navy also evaluated the Electromagnetic Aircraft Launch System (EMALS), which is being used by the US Navy in their latest Gerald R. Ford-class aircraft carriers. General Atomics, the developer of the EMALS, was cleared by the US government to give a technical demonstration to Indian Navy officers, who were impressed by the new capabilities of the system. The EMALS enables launching varied aircraft including unmanned combat air vehicles (UCAV). The aim is to have a total of three aircraft carriers in service, with two fully operational carriers and the third in refit.
In August 2013, a launching ceremony for Japan's largest military ship since World War II was held in Yokohama. The 820-foot-long (250 m), 19,500-ton flattop Izumo was deployed in March 2015. The ship is able to carry up to 14 helicopters; however, only seven ASW helicopters and two SAR helicopters were planned for the initial aircraft complement. For other operations, 400 troops and fifty 3.5 t trucks (or equivalent equipment) can also be carried. The flight deck has five helicopter landing spots that allow simultaneous landings or take-offs. The ship is equipped with two Phalanx CIWS and two SeaRAM for its defense. The destroyers of this class were initially intended to replace the two ships of the Shirane class, which were originally scheduled to begin decommissioning in FY2014.
The current US fleet of Nimitz-class carriers will be followed into service (and in some cases replaced) by the ten-ship Gerald R. Ford class. It is expected that the ships will be more automated in an effort to reduce the amount of funding required to staff, maintain and operate its supercarriers. The main new features are implementation of Electromagnetic Aircraft Launch System (EMALS) (which replace the old steam catapults) and unmanned aerial vehicles. With the deactivation of USS Enterprise in December 2012 (decommissioning scheduled for 2016), the U.S. fleet comprises 10 active supercarriers. On 24 July 2007, the House Armed Services Seapower subcommittee recommended seven or eight new carriers (one every four years). However, the debate has deepened over budgeting for the $12–14.5 billion (plus $12 billion for development and research) for the 100,000 ton Gerald R. Ford-class carrier (estimated service 2016) compared to the smaller $2 billion 45,000 ton America-class amphibious assault ships able to deploy squadrons of F-35B of which one is already active, another is under construction and nine more are planned.
If the aircraft are VTOL-capable or helicopters, they do not need to decelerate and hence there is no such need. The arrested-recovery system has used an angled deck since the 1950s because, in case the aircraft does not catch the arresting wire, the short deck allows easier take off by reducing the number of objects between the aircraft and the end of the runway. It also has the advantage of separating the recovery operation area from the launch area. Helicopters and aircraft capable of vertical or short take-off and landing (V/STOL) usually recover by coming abreast the carrier on the port side and then using their hover capability to move over the flight deck and land vertically without the need for arresting gear.
The superstructure of a carrier (such as the bridge, flight control tower) are concentrated in a relatively small area called an island, a feature pioneered on the HMS Hermes in 1923. While the island is usually built on the starboard side of the fight deck, the Japanese aircraft carriers Akagi and Hiryū had their islands built on the port side. Very few carriers have been designed or built without an island. The flush deck configuration proved to have significant drawbacks, primary of which was management of the exhaust from the power plant. Fumes coming across the deck were a major issue in USS Langley. In addition, lack of an island meant difficulties managing the flight deck, performing air traffic control, a lack of radar housing placements and problems with navigating and controlling the ship itself.
1 CATOBAR carrier: Charles de Gaulle is a 42,000 tonne nuclear-powered aircraft carrier, commissioned in 2001 and is the flagship of the French Navy (Marine Nationale). The ship carries a complement of Dassault-Breguet Super Étendard, Dassault Rafale M and E‑2C Hawkeye aircraft, EC725 Caracal and AS532 Cougar helicopters for combat search and rescue, as well as modern electronics and Aster missiles. It is a CATOBAR-type carrier that uses two 75 m C13‑3 steam catapults of a shorter version of the catapult system installed on the U.S. Nimitz-class carriers, one catapult at the bow and one across the front of the landing area.
With the deactivation of USS Enterprise in December 2012, the U.S. fleet comprises 10 supercarriers. The House Armed Services Seapower subcommittee on 24 July 2007, recommended seven or maybe eight new carriers (one every four years). However, the debate has deepened over budgeting for the $12–14.5 billion (plus $12 billion for development and research) for the 100,000 ton Gerald R. Ford-class carrier (estimated service 2016) compared to the smaller $2 billion 45,000 ton America-class amphibious assault ships, which are able to deploy squadrons of F-35Bs. The first of this class, USS America, is now in active service with another, USS Tripoli, under construction and 9 more are planned.
Since World War II, aircraft carrier designs have increased in size to accommodate a steady increase in aircraft size. The large, modern Nimitz class of US carriers has a displacement nearly four times that of the World War II–era USS Enterprise, yet its complement of aircraft is roughly the same—a consequence of the steadily increasing size and weight of military aircraft over the years. Today's aircraft carriers are so expensive that nations which operate them risk significant political, economic, and military impact if a carrier is lost, or even used in conflict.
Conventional ("tailhook") aircraft rely upon a landing signal officer (LSO, radio call sign paddles) to monitor the aircraft's approach, visually gauge glideslope, attitude, and airspeed, and transmit that data to the pilot. Before the angled deck emerged in the 1950s, LSOs used colored paddles to signal corrections to the pilot (hence the nickname). From the late 1950s onward, visual landing aids such as Optical Landing System have provided information on proper glide slope, but LSOs still transmit voice calls to approaching pilots by radio.
Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight. As catapults are unnecessary, carriers with this arrangement reduce weight, complexity, and space needed for complex steam or electromagnetic launching equipment, vertical landing aircraft also remove the need for arresting cables and related hardware. Russian, Chinese, and future Indian carriers include a ski-jump ramp for launching lightly loaded conventional fighter aircraft but recover using traditional carrier arresting cables and a tailhook on their aircraft.
One CATOBAR carrier: São Paulo is a Clemenceau-class aircraft carrier currently in service with the Brazilian Navy. São Paulo was first commissioned in 1963 by the French Navy as Foch and was transferred in 2000 to Brazil, where she became the new flagship of the Brazilian Navy. During the period from 2005–2010, São Paulo underwent extensive modernization. At the end of 2010, sea trials began, and as of 2011[update] São Paulo had been evaluated by the CIASA (Inspection Commission and Training Advisory). She was expected to rejoin the fleet in late 2013, but suffered another major fire in 2012.
1 STOBAR carrier: Admiral Flota Sovetskovo Soyuza Kuznetsov: 55,000 tonne Admiral Kuznetsov-class STOBAR aircraft carrier. Launched in 1985 as Tbilisi, renamed and operational from 1995. Without catapults she can launch and recover lightly fueled naval fighters for air defense or anti-ship missions but not heavy conventional bombing strikes.[citation needed] Officially designated an aircraft carrying cruiser, she is unique in carrying a heavy cruiser's complement of defensive weapons and large P-700 Granit offensive missiles. The P-700 systems will be removed in the coming refit to enlarge her below decks aviation facilities as well as upgrading her defensive systems.
The Royal Navy is constructing two new larger STOVL aircraft carriers, the Queen Elizabeth class, to replace the three now retired Invincible-class carriers. The ships are HMS Queen Elizabeth and HMS Prince of Wales. They will be able to operate up to 40 aircraft on peace time operations with a tailored group of up to 50, and will have a displacement of 70,600 tonnes. HMS Queen Elizabeth is projected to commission in 2017 followed by Prince of Wales in about 2020. The ships are due to become operational starting in 2020. Their primary aircraft complement will be made up of F-35B Lightning IIs, and their ship's company will number around 680 with the total complement rising to about 1600 when the air group is embarked. The two ships will be the largest warships ever built for the Royal Navy.
The Ministry of Defence (MoD) is the British government department responsible for implementing the defence policy set by Her Majesty's Government, and is the headquarters of the British Armed Forces.
The MoD states that its principal objectives are to defend the United Kingdom of Great Britain and Northern Ireland and its interests and to strengthen international peace and stability. With the collapse of the Soviet Union and the end of the Cold War, the MoD does not foresee any short-term conventional military threat; rather, it has identified weapons of mass destruction, international terrorism, and failed and failing states as the overriding threats to Britain's interests. The MoD also manages day-to-day running of the armed forces, contingency planning and defence procurement.
During the 1920s and 1930s, British civil servants and politicians, looking back at the performance of the state during World War I, concluded that there was a need for greater co-ordination between the three Services that made up the armed forces of the United Kingdom—the British Army, the Royal Navy, and the Royal Air Force. The formation of a united ministry of defence was rejected by David Lloyd George's coalition government in 1921; but the Chiefs of Staff Committee was formed in 1923, for the purposes of inter-Service co-ordination. As rearmament became a concern during the 1930s, Stanley Baldwin created the position of Minister for Coordination of Defence. Lord Chatfield held the post until the fall of Neville Chamberlain's government in 1940; his success was limited by his lack of control over the existing Service departments and his limited political influence.
Winston Churchill, on forming his government in 1940, created the office of Minister of Defence to exercise ministerial control over the Chiefs of Staff Committee and to co-ordinate defence matters. The post was held by the Prime Minister of the day until Clement Attlee's government introduced the Ministry of Defence Act of 1946. The new ministry was headed by a Minister of Defence who possessed a seat in the Cabinet. The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
From 1946 to 1964 five Departments of State did the work of the modern Ministry of Defence: the Admiralty, the War Office, the Air Ministry, the Ministry of Aviation, and an earlier form of the Ministry of Defence. These departments merged in 1964; the defence functions of the Ministry of Aviation Supply merged into the Ministry of Defence in 1971.
The Ministers and Chiefs of the Defence Staff are supported by a number of civilian, scientific and professional military advisors. The Permanent Under-Secretary of State for Defence (generally known as the Permanent Secretary) is the senior civil servant at the MoD. His or her role is to ensure the MoD operates effectively as a department of the government.
The current Chief of the Defence Staff, the professional head of the British Armed Forces, is General Sir Nicholas Houghton, late Green Howards. He is supported by the Vice Chief of the Defence Staff, by the professional heads of the three services of HM Armed Forces and by the Commander of Joint Forces Command.
There are also three Deputy Chiefs of the Defence Staff with particular remits, Deputy Chief of the Defence Staff (Capability), Deputy CDS (Personnel and Training) and Deputy CDS (Operations). The Surgeon General, represents the Defence Medical Services on the Defence Staff, and is the clinical head of that service.
Additionally, there are a number of Assistant Chiefs of Defence Staff, including the Assistant Chief of the Defence Staff (Reserves and Cadets) and the Defence Services Secretary in the Royal Household of the Sovereign of the United Kingdom, who is also the Assistant Chief of Defence Staff (Personnel).
The 1998 Strategic Defence Review and the 2003 Delivering Security in a Changing World White Paper outlined the following posture for the British Armed Forces:
The MoD has since been regarded as a leader in elaborating the post-Cold War organising concept of "defence diplomacy". As a result of the Strategic Defence and Security Review 2010, Prime Minister David Cameron signed a 50-year treaty with French President Nicolas Sarkozy that would have the two countries co-operate intensively in military matters. The UK is establishing air and naval bases in the Persian Gulf, located in the UAE and Bahrain. A presence in Oman is also being considered.
The Strategic Defence and Security Review 2015 included £178 billion investment in new equipment and capabilities. The review set a defence policy with four primary missions for the Armed Forces:
Following the end of the Cold War, the threat of direct conventional military confrontation with other states has been replaced by terrorism. Sir Richard Dannatt predicted British forces to be involved in combating "predatory non-state actors" for the foreseeable future, in what he called an "era of persistent conflict". He told the Chatham House think tank that the fight against al-Qaeda and other militant Islamist groups was "probably the fight of our generation".
Dannatt criticised a remnant "Cold War mentality", with military expenditures based on retaining a capability against a direct conventional strategic threat; He said currently only 10% of the MoD's equipment programme budget between 2003 and 2018 was to be invested in the "land environment"—at a time when Britain was engaged in land-based wars in Afghanistan and Iraq.
The Defence Committee—Third Report "Defence Equipment 2009" cites an article from the Financial Times website stating that the Chief of Defence Materiel, General Sir Kevin O’Donoghue, had instructed staff within Defence Equipment and Support (DE&S) through an internal memorandum to reprioritize the approvals process to focus on supporting current operations over the next three years; deterrence related programmes; those that reflect defence obligations both contractual or international; and those where production contracts are already signed. The report also cites concerns over potential cuts in the defence science and technology research budget; implications of inappropriate estimation of Defence Inflation within budgetary processes; underfunding in the Equipment Programme; and a general concern over striking the appropriate balance over a short-term focus (Current Operations) and long-term consequences of failure to invest in the delivery of future UK defence capabilities on future combatants and campaigns. The then Secretary of State for Defence, Bob Ainsworth MP, reinforced this reprioritisation of focus on current operations and had not ruled out "major shifts" in defence spending. In the same article the First Sea Lord and Chief of the Naval Staff, Admiral Sir Mark Stanhope, Royal Navy, acknowledged that there was not enough money within the defence budget and it is preparing itself for tough decisions and the potential for cutbacks. According to figures published by the London Evening Standard the defence budget for 2009 is "more than 10% overspent" (figures cannot be verified) and the paper states that this had caused Gordon Brown to say that the defence spending must be cut. The MoD has been investing in IT to cut costs and improve services for its personnel.
The Ministry of Defence is one of the United Kingdom's largest landowners, owning 227,300 hectares of land and foreshore (either freehold or leasehold) at April 2014, which was valued at "about £20 billion". The MoD also has "rights of access" to a further 222,000 hectares. In total, this is about 1.8% of the UK land mass. The total annual cost to support the defence estate is "in excess of £3.3 billion".
The defence estate is divided as training areas & ranges (84.0%), research & development (5.4%), airfields (3.4%), barracks & camps (2.5%), storage & supply depots (1.6%), and other (3.0%). These are largely managed by the Defence Infrastructure Organisation.
The headquarters of the MoD are in Whitehall and are now known as Main Building. This structure is neoclassical in style and was originally built between 1938 and 1959 to designs by Vincent Harris to house the Air Ministry and the Board of Trade. The northern entrance in Horse Guards Avenue is flanked by two monumental statues, Earth and Water, by Charles Wheeler. Opposite stands the Gurkha Monument, sculpted by Philip Jackson and unveiled in 1997 by Queen Elizabeth II. Within it is the Victoria Cross and George Cross Memorial, and nearby are memorials to the Fleet Air Arm and RAF (to its east, facing the riverside). A major refurbishment of the building was completed under a PFI contract by Skanska in 2004.
Henry VIII's wine cellar at the Palace of Whitehall, built in 1514–1516 for Cardinal Wolsey, is in the basement of Main Building, and is used for entertainment. The entire vaulted brick structure of the cellar was encased in steel and concrete and relocated nine feet to the west and nearly 19 feet (5.8 m) deeper in 1949, when construction was resumed at the site after World War II. This was carried out without any significant damage to the structure.
The most notable fraud conviction was that of Gordon Foxley, head of defence procurement at the Ministry of Defence from 1981 to 1984. Police claimed he received at least £3.5m in total in corrupt payments, such as substantial bribes from overseas arms contractors aiming to influence the allocation of contracts.
A government report covered by the Guardian in 2002 indicates that between 1940 and 1979, the Ministry of Defence "turned large parts of the country into a giant laboratory to conduct a series of secret germ warfare tests on the public" and many of these tests "involved releasing potentially dangerous chemicals and micro-organisms over vast swaths of the population without the public being told." The Ministry of Defence claims that these trials were to simulate germ warfare and that the tests were harmless. Still, families who have been in the area of many of the tests are experiencing children with birth defects and physical and mental handicaps and many are asking for a public inquiry. According to the report these tests affected estimated millions of people including one period between 1961 and 1968 where "more than a million people along the south coast of England, from Torquay to the New Forest, were exposed to bacteria including e.coli and bacillus globigii, which mimics anthrax." Two scientists commissioned by the Ministry of Defence stated that these trials posed no risk to the public. This was confirmed by Sue Ellison, a representative of Porton Down who said that the results from these trials "will save lives, should the country or our forces face an attack by chemical and biological weapons." Asked whether such tests are still being carried out, she said: "It is not our policy to discuss ongoing research." It is unknown whether or not the harmlessness of the trials was known at the time of their occurrence.
The MoD has been criticised for an ongoing fiasco, having spent £240m on eight Chinook HC3 helicopters which only started to enter service in 2010, years after they were ordered in 1995 and delivered in 2001. A National Audit Office report reveals that the helicopters have been stored in air conditioned hangars in Britain since their 2001[why?] delivery, while troops in Afghanistan have been forced to rely on helicopters which are flying with safety faults. By the time the Chinooks are airworthy, the total cost of the project could be as much as £500m.
In April 2008, a £90m contract was signed with Boeing for a "quick fix" solution, so they can fly by 2010: QinetiQ will downgrade the Chinooks—stripping out some of their more advanced equipment.
In October 2009, the MoD was heavily criticized for withdrawing the bi-annual non-operational training £20m budget for the volunteer Territorial Army (TA), ending all non-operational training for 6 months until April 2010. The government eventually backed down and restored the funding. The TA provides a small percentage of the UK's operational troops. Its members train on weekly evenings and monthly weekends, as well as two-week exercises generally annually and occasionally bi-annually for troops doing other courses. The cuts would have meant a significant loss of personnel and would have had adverse effects on recruitment.
In 2013 it was found that the Ministry of Defence had overspent on its equipment budget by £6.5bn on orders that could take up to 39 years to fulfil. The Ministry of Defence has been criticised in the past for poor management and financial control, investing in projects that have taken up to 10 and even as much as 15 years to be delivered.
The phrase "51st state" can be used in a positive sense, meaning that a region or territory is so aligned, supportive, and conducive with the United States, that it is like a U.S. state. It can also be used in a pejorative sense, meaning an area or region is perceived to be under excessive American cultural or military influence or control. In various countries around the world, people who believe their local or national culture has become too Americanized sometimes use the term "51st state" in reference to their own countries.
Under Article IV, Section Three of the United States Constitution, which outlines the relationship among the states, Congress has the power to admit new states to the union. The states are required to give "full faith and credit" to the acts of each other's legislatures and courts, which is generally held to include the recognition of legal contracts, marriages, and criminal judgments. The states are guaranteed military and civil defense by the federal government, which is also obliged by Article IV, Section Four, to "guarantee to every state in this union a republican form of government".
Puerto Rico has been discussed as a potential 51st state of the United States. In a 2012 status referendum a majority of voters, 54%, expressed dissatisfaction with the current political relationship. In a separate question, 61% of voters supported statehood (excluding the 26% of voters who left this question blank). On December 11, 2012, Puerto Rico's legislature resolved to request that the President and the U.S. Congress act on the results, end the current form of territorial status and begin the process of admitting Puerto Rico to the Union as a state.
Since 1898, Puerto Rico has had limited representation in the Congress in the form of a Resident Commissioner, a nonvoting delegate. The 110th Congress returned the Commissioner's power to vote in the Committee of the Whole, but not on matters where the vote would represent a decisive participation. Puerto Rico has elections on the United States presidential primary or caucus of the Democratic Party and the Republican Party to select delegates to the respective parties' national conventions although presidential electors are not granted on the Electoral College. As American citizens, Puerto Ricans can vote in U.S. presidential elections, provided they reside in one of the 50 states or the District of Columbia and not in Puerto Rico itself.
Residents of Puerto Rico pay U.S. federal taxes: import/export taxes, federal commodity taxes, social security taxes, therefore contributing to the American Government. Most Puerto Rico residents do not pay federal income tax but do pay federal payroll taxes (Social Security and Medicare). However, federal employees, those who do business with the federal government, Puerto Rico–based corporations that intend to send funds to the U.S. and others do pay federal income taxes. Puerto Ricans may enlist in the U.S. military. Puerto Ricans have participated in all American wars since 1898; 52 Puerto Ricans had been killed in the Iraq War and War in Afghanistan by November 2012.
Puerto Rico has been under U.S. sovereignty for over a century when it was ceded to the U.S. by Spain following the end of the Spanish–American War, and Puerto Ricans have been U.S. citizens since 1917. The island's ultimate status has not been determined as of 2012[update], its residents do not have voting representation in their federal government. Puerto Rico has limited representation in the U.S. Congress in the form of a Resident Commissioner, a delegate with limited no voting rights. Like the states, Puerto Rico has self-rule, a republican form of government organized pursuant to a constitution adopted by its people, and a bill of rights.
This constitution was created when the U.S. Congress directed local government to organize a constitutional convention to write the Puerto Rico Constitution in 1951. The acceptance of that constitution by Puerto Rico's electorate, the U.S. Congress, and the U.S. president occurred in 1952. In addition, the rights, privileges and immunities attendant to United States citizens are "respected in Puerto Rico to the same extent as though Puerto Rico were a state of the union" through the express extension of the Privileges and Immunities Clause of the U.S. Constitution by the U.S. Congress in 1948.
Puerto Rico is designated in its constitution as the "Commonwealth of Puerto Rico". The Constitution of Puerto Rico which became effective in 1952 adopted the name of Estado Libre Asociado (literally translated as "Free Associated State"), officially translated into English as Commonwealth, for its body politic. The island is under the jurisdiction of the Territorial Clause of the U.S. Constitution, which has led to doubts about the finality of the Commonwealth status for Puerto Rico. In addition, all people born in Puerto Rico become citizens of the U.S. at birth (under provisions of the Jones–Shafroth Act in 1917), but citizens residing in Puerto Rico cannot vote for president nor for full members of either house of Congress. Statehood would grant island residents full voting rights at the Federal level. The Puerto Rico Democracy Act (H.R. 2499) was approved on April 29, 2010, by the United States House of Representatives 223–169, but was not approved by the Senate before the end of the 111th Congress. It would have provided for a federally sanctioned self-determination process for the people of Puerto Rico. This act would provide for referendums to be held in Puerto Rico to determine the island's ultimate political status. It had also been introduced in 2007.
In November 2012, a referendum resulted in 54 percent of respondents voting to reject the current status under the territorial clause of the U.S. Constitution, while a second question resulted in 61 percent of voters identifying statehood as the preferred alternative to the current territorial status. The 2012 referendum was by far the most successful referendum for statehood advocates and support for statehood has risen in each successive popular referendum. However, more than one in four voters abstained from answering the question on the preferred alternative status. Statehood opponents have argued that the statehood option garnered only 45 percent of the votes if abstentions are included. If abstentions are considered, the result of the referendum is much closer to 44 percent for statehood, a number that falls under the 50 percent majority mark.
The Washington Post, The New York Times and the Boston Herald have published opinion pieces expressing support for the statehood of Puerto Rico. On November 8, 2012, Washington, D.C. newspaper The Hill published an article saying that Congress will likely ignore the results of the referendum due to the circumstances behind the votes. and U.S. Congressman Luis Gutiérrez U.S. Congresswoman Nydia Velázquez, both of Puerto Rican ancestry, agreed with the The Hill 's statements. Shortly after the results were published Puerto Rico-born U.S. Congressman José Enrique Serrano commented "I was particularly impressed with the outcome of the 'status' referendum in Puerto Rico. A majority of those voting signaled the desire to change the current territorial status. In a second question an even larger majority asked to become a state. This is an earthquake in Puerto Rican politics. It will demand the attention of Congress, and a definitive answer to the Puerto Rican request for change. This is a history-making moment where voters asked to move forward."
Several days after the referendum, the Resident Commissioner Pedro Pierluisi, Governor Luis Fortuño, and Governor-elect Alejandro García Padilla wrote separate letters to the President of the United States Barack Obama addressing the results of the voting. Pierluisi urged Obama to begin legislation in favor of the statehood of Puerto Rico, in light of its win in the referendum. Fortuño urged him to move the process forward. García Padilla asked him to reject the results because of their ambiguity. The White House stance related to the November 2012 plebiscite was that the results were clear, the people of Puerto Rico want the issue of status resolved, and a majority chose statehood in the second question. Former White House director of Hispanic media stated, "Now it is time for Congress to act and the administration will work with them on that effort, so that the people of Puerto Rico can determine their own future."
On May 15, 2013, Resident Commissioner Pierluisi introduced H.R. 2000 to Congress to "set forth the process for Puerto Rico to be admitted as a state of the Union," asking for Congress to vote on ratifying Puerto Rico as the 51st state. On February 12, 2014, Senator Martin Heinrich introduced a bill in the US Senate. The bill would require a binding referendum to be held in Puerto Rico asking whether the territory wants to be admitted as a state. In the event of a yes vote, the president would be asked to submit legislation to Congress to admit Puerto Rico as a state.
Washington, D.C. is often mentioned as a candidate for statehood. In Federalist No. 43 of The Federalist Papers, James Madison considered the implications of the definition of the "seat of government" found in the United States Constitution. Although he noted potential conflicts of interest, and the need for a "municipal legislature for local purposes," Madison did not address the district's role in national voting. Legal scholars disagree on whether a simple act of Congress can admit the District as a state, due to its status as the seat of government of the United States, which Article I, Section 8 of the Constitution requires to be under the exclusive jurisdiction of Congress; depending on the interpretation of this text, admission of the full District as a state may require a Constitutional amendment, which is much more difficult to enact. However, the Constitution does not set a minimum size for the District. Its size has already changed once before, when Virginia reclaimed the portion of the District south of the Potomac. So the constitutional requirement for a federal district can be satisfied by reducing its size to the small central core of government buildings and monuments, giving the rest of the territory to the new state.
Washington, D.C. residents who support the statehood movement sometimes use a shortened version of the Revolutionary War protest motto "No taxation without representation", omitting the initial "No", denoting their lack of Congressional representation; the phrase is now printed on newly issued Washington, D.C. license plates (although a driver may choose to have the Washington, D.C. website address instead). President Bill Clinton's presidential limousine had the "Taxation without representation" license plate late in his term, while President George W. Bush had the vehicle's plates changed shortly after beginning his term in office. President Barack Obama had the license plates changed back to the protest style at the beginning of his second term.
This position was carried by the D.C. Statehood Party, a political party; it has since merged with the local Green Party affiliate to form the D.C. Statehood Green Party. The nearest this movement ever came to success was in 1978, when Congress passed the District of Columbia Voting Rights Amendment. Two years later in 1980, local citizens passed an initiative calling for a constitutional convention for a new state. In 1982, voters ratified the constitution of the state, which was to be called New Columbia. The drive for statehood stalled in 1985, however, when the Washington, D.C. Voting Rights Amendment failed because not enough states ratified the amendment within the seven-year span specified.
Other less likely contenders are Guam and the United States Virgin Islands, both of which are unincorporated organized territories of the United States. Also, the Northern Mariana Islands and American Samoa, an unorganized, unincorporated territory, could both attempt to gain statehood. Some proposals call for the Virgin Islands to be admitted with Puerto Rico as one state (often known as the proposed "Commonwealth of Prusvi", for Puerto Rico/U.S. Virgin Islands, or as "Puerto Virgo"), and for the amalgamation of U.S. territories or former territories in the Pacific Ocean, in the manner of the "Greater Hawaii" concept of the 1960s. Guam and the Northern Mariana Islands would be admitted as one state, along with Palau, the Federated States of Micronesia, and the Marshall Islands (although these latter three entities are now separate sovereign nations, which have Compact of Free Association relationships with the United States). Such a state would have a population of 412,381 (slightly lower than Wyoming's population) and a land area of 911.82 square miles (2,361.6 km2) (slightly smaller than Rhode Island). American Samoa could possibly be part of such a state, increasing the population to 467,900 and the area to 988.65 square miles (2,560.6 km2). Radio Australia, in late May 2008, issued signs of Guam and the Northern Mariana Islands becoming one again and becoming the 51st state.
The Philippines has had small grassroots movements for U.S. statehood. Originally part of the platform of the Progressive Party, then known as the Federalista Party, the party dropped it in 1907, which coincided with the name change. As recently as 2004, the concept of the Philippines becoming a U.S. state has been part of a political platform in the Philippines. Supporters of this movement include Filipinos who believe that the quality of life in the Philippines would be higher and that there would be less poverty there if the Philippines were an American state or territory. Supporters also include Filipinos that had fought as members of the United States Armed Forces in various wars during the Commonwealth period.
In Canada, "the 51st state" is a phrase generally used in such a way as to imply that if a certain political course is taken, Canada's destiny will be as little more than a part of the United States. Examples include the Canada-United States Free Trade Agreement in 1988, the debate over the creation of a common defense perimeter, and as a potential consequence of not adopting proposals intended to resolve the issue of Quebec sovereignty, the Charlottetown Accord in 1992 and the Clarity Act in 1999.
The phrase is usually used in local political debates, in polemic writing or in private conversations. It is rarely used by politicians themselves in a public context, although at certain times in Canadian history political parties have used other similarly loaded imagery. In the 1988 federal election, the Liberals asserted that the proposed Free Trade Agreement amounted to an American takeover of Canada—notably, the party ran an ad in which Progressive Conservative (PC) strategists, upon the adoption of the agreement, slowly erased the Canada-U.S. border from a desktop map of North America. Within days, however, the PCs responded with an ad which featured the border being drawn back on with a permanent marker, as an announcer intoned "Here's where we draw the line."
The implication has historical basis and dates to the breakup of British America during the American Revolution. The colonies that had confederated to form the United States invaded Canada (at the time a term referring specifically to the modern-day provinces of Quebec and Ontario, which had only been in British hands since 1763) at least twice, neither time succeeding in taking control of the territory. The first invasion was during the Revolution, under the assumption that French-speaking Canadians' presumed hostility towards British colonial rule combined with the Franco-American alliance would make them natural allies to the American cause; the Continental Army successfully recruited two Canadian regiments for the invasion. That invasion's failure forced the members of those regiments into exile, and they settled mostly in upstate New York. The Articles of Confederation, written during the Revolution, included a provision for Canada to join the United States, should they ever decide to do so, without needing to seek U.S. permission as other states would. The United States again invaded Canada during the War of 1812, but this effort was made more difficult due to the large number of Loyalist Americans that had fled to what is now Ontario and still resisted joining the republic. The Hunter Patriots in the 1830s and the Fenian raids after the American Civil War were private attacks on Canada from the U.S. Several U.S. politicians in the 19th century also spoke in favour of annexing Canada.
In the late 1940s, during the last days of the Dominion of Newfoundland (at the time a dominion-dependency in the Commonwealth and independent of Canada), there was mainstream support, although not majority, for Newfoundland to form an economic union with the United States, thanks to the efforts of the Economic Union Party and significant U.S. investment in Newfoundland stemming from the U.S.-British alliance in World War II. The movement ultimately failed when, in a 1948 referendum, voters narrowly chose to confederate with Canada (the Economic Union Party supported an independent "responsible government" that they would then push toward their goals).
In the United States, the term "the 51st state" when applied to Canada can serve to highlight the similarities and close relationship between the United States and Canada. Sometimes the term is used disparagingly, intended to deride Canada as an unimportant neighbor. In the Quebec general election, 1989, the political party Parti 51 ran 11 candidates on a platform of Quebec seceding from Canada to join the United States (with its leader, André Perron, claiming Quebec could not survive as an independent nation). The party attracted just 3,846 votes across the province, 0.11% of the total votes cast. In comparison, the other parties in favour of sovereignty of Quebec in that election got 40.16% (PQ) and 1.22% (NPDQ).
Due to geographical proximity of the Central American countries to the U.S. which has powerful military, economic, and political influences, there were several movements and proposals by the United States during the 19th and 20th centuries to annex some or all of the Central American republics (Costa Rica, El Salvador, Guatemala, Honduras with the formerly British-ruled Bay Islands, Nicaragua, Panama which had the U.S.-ruled Canal Zone territory from 1903 to 1979, and formerly British Honduras or Belize since 1981). However, the U.S. never acted on these proposals from some U.S. politicians; some of which were never delivered or considered seriously. In 2001, El Salvador adopted the U.S. dollar as its currency, while Panama has used it for decades due to its ties to the Canal Zone.
Cuba, like many Spanish territories, wanted to break free from Spain. A pro-independence movement in Cuba was supported by the U.S., and Cuban guerrilla leaders wanted annexation to the United States, but Cuban revolutionary leader José Martí called for Cuban nationhood. When the U.S. battleship Maine sank in Havana Harbor, the U.S. blamed Spain and the Spanish–American War broke out in 1898. After the U.S. won, Spain relinquished claim of sovereignty over territories, including Cuba. The U.S. administered Cuba as a protectorate until 1902. Several decades later in 1959, the corrupt Cuban government of U.S.-backed Fulgencio Batista was overthrown by Fidel Castro. Castro installed a Marxist–Leninist government allied with the Soviet Union, which has been in power ever since.
Several websites assert that Israel is the 51st state due to the annual funding and defense support it receives from the United States. An example of this concept can be found in 2003 when Martine Rothblatt published a book called Two Stars for Peace that argued for the addition of Israel and the Palestinian territories surrounding it as the 51st state in the Union. The American State of Canaan, is a book published by Prof. Alfred de Grazia, political science and sociologist, in March 2009, proposing the creation of the 51st and 52nd states from Israel and the Palestinian territories.
In Article 3 of the Treaty of San Francisco between the Allied Powers and Japan, which came into force in April 1952, the U.S. put the outlying islands of the Ryukyus, including the island of Okinawa—home to over 1 million Okinawans related to the Japanese—and the Bonin Islands, the Volcano Islands, and Iwo Jima into U.S. trusteeship. All these trusteeships were slowly returned to Japanese rule. Okinawa was returned on May 15, 1972, but the U.S. stations troops in the island's bases as a defense for Japan.
In 2010 there was an attempt to register a 51st State Party with the New Zealand Electoral Commission. The party advocates New Zealand becoming the 51st state of the United States of America. The party's secretary is Paulus Telfer, a former Christchurch mayoral candidate. On February 5, 2010, the party applied to register a logo with the Electoral Commission. The logo – a US flag with 51 stars – was rejected by the Electoral Commission on the grounds that it was likely to cause confusion or mislead electors. As of 2014[update], the party remains unregistered and cannot appear on a ballot.
Albania has often been called the 51st state for its perceived strongly pro-American positions, mainly because of the United States' policies towards it. In reference to President George W. Bush's 2007 European tour, Edi Rama, Tirana's mayor and leader of the opposition Socialists, said: "Albania is for sure the most pro-American country in Europe, maybe even in the world ... Nowhere else can you find such respect and hospitality for the President of the United States. Even in Michigan, he wouldn't be as welcome." At the time of ex-Secretary of State James Baker's visit in 1992, there was even a move to hold a referendum declaring the country as the 51st American state. In addition to Albania, Kosovo which is predominately Albanian is seen as a 51st state due to the heavily presence and influence of the United States. The US has had troops and the largest base outside US territory, Camp Bondsteel in the territory since 1999.
During World War II, when Denmark was occupied by Nazi Germany, the United States briefly controlled Greenland for battlefields and protection. In 1946, the United States offered to buy Greenland from Denmark for $100 million ($1.2 billion today) but Denmark refused to sell it. Several politicians and others have in recent years argued that Greenland could hypothetically be in a better financial situation as a part of the United States; for instance mentioned by professor Gudmundur Alfredsson at University of Akureyri in 2014. One of the actual reasons behind US interest in Greenland could be the vast natural resources of the island. According to Wikileaks, the U.S. appears to be highly interested in investing in the resource base of the island and in tapping the vast expected hydrocarbons off the Greenlandic coast.
Poland has historically been staunchly pro-American, dating back to General Tadeusz Kościuszko and Casimir Pulaski's involvement in the American Revolution. This pro-American stance was reinforced following favorable American intervention in World War I (leading to the creation of an independent Poland) and the Cold War (culminating in a Polish state independent of Soviet influence). Poland contributed a large force to the "Coalition of the Willing" in Iraq. A quote referring to Poland as "the 51st state" has been attributed to James Pavitt, then Central Intelligence Agency Deputy Director for Operations, especially in connection to extraordinary rendition.
The Party of Reconstruction in Sicily, which claimed 40,000 members in 1944, campaigned for Sicily to be admitted as a U.S. state. This party was one of several Sicilian separatist movements active after the downfall of Italian Fascism. Sicilians felt neglected or underrepresented by the Italian government after the annexation of 1861 that ended the rule of the Kingdom of the Two Sicilies based in Naples. The large population of Sicilians in America and the American-led Allied invasion of Sicily in July–August 1943 may have contributed to the sentiment.
There are four categories of terra nullius, land that is unclaimed by any state: the small unclaimed territory of Bir Tawil between Egypt and Sudan, Antarctica, the oceans, and celestial bodies such as the Moon or Mars. In the last three of these, international treaties (the Antarctic Treaty, the United Nations Convention on the Law of the Sea, and the Outer Space Treaty respectively) prevent colonization and potential statehood of any of these uninhabited (and, given current technology, not permanently inhabitable) territories.
In a career spanning more than four decades, Spielberg's films have covered many themes and genres. Spielberg's early science-fiction and adventure films were seen as archetypes of modern Hollywood blockbuster filmmaking. In later years, his films began addressing humanistic issues such as the Holocaust (in Schindler's List), the transatlantic slave trade (in Amistad), war (in Empire of the Sun, Saving Private Ryan, War Horse and Bridge of Spies) and terrorism (in Munich). His other films include Close Encounters of the Third Kind, the Indiana Jones film series, and A.I. Artificial Intelligence.
Spielberg was born in Cincinnati, Ohio, to an Orthodox Jewish family. His mother, Leah (Adler) Posner (born 1920), was a restaurateur and concert pianist, and his father, Arnold Spielberg (born 1917), was an electrical engineer involved in the development of computers. His paternal grandparents were immigrants from Ukraine who settled in Cincinnati in the first decade of the 1900s. In 1950, his family moved to Haddon Township, New Jersey when his father took a job with RCA. Three years later, the family moved to Phoenix, Arizona.:548 Spielberg attended Hebrew school from 1953 to 1957, in classes taught by Rabbi Albert L. Lewis.
Spielberg won the Academy Award for Best Director for Schindler's List (1993) and Saving Private Ryan (1998). Three of Spielberg's films—Jaws (1975), E.T. the Extra-Terrestrial (1982), and Jurassic Park (1993)—achieved box office records, originated and came to epitomize the blockbuster film. The unadjusted gross of all Spielberg-directed films exceeds $9 billion worldwide, making him the highest-grossing director in history. His personal net worth is estimated to be more than $3 billion. He has been associated with composer John Williams since 1974, who composed music for all save five of Spielberg's feature films.
As a child, Spielberg faced difficulty reconciling being an Orthodox Jew with the perception of him by other children he played with. "It isn't something I enjoy admitting," he once said, "but when I was seven, eight, nine years old, God forgive me, I was embarrassed because we were Orthodox Jews. I was embarrassed by the outward perception of my parents' Jewish practices. I was never really ashamed to be Jewish, but I was uneasy at times." Spielberg also said he suffered from acts of anti-Semitic prejudice and bullying: "In high school, I got smacked and kicked around. Two bloody noses. It was horrible."
In 1958, he became a Boy Scout and fulfilled a requirement for the photography merit badge by making a nine-minute 8 mm film entitled The Last Gunfight. Years later, Spielberg recalled to a magazine interviewer, "My dad's still-camera was broken, so I asked the scoutmaster if I could tell a story with my father's movie camera. He said yes, and I got an idea to do a Western. I made it and got my merit badge. That was how it all started." At age thirteen, while living in Phoenix, Spielberg won a prize for a 40-minute war film he titled Escape to Nowhere, using a cast composed of other high school friends. That motivated him to make 15 more amateur 8mm films.:548 In 1963, at age sixteen, Spielberg wrote and directed his first independent film, a 140-minute science fiction adventure called Firelight, which would later inspire Close Encounters. The film was made for $500, most of which came from his father, and was shown in a local cinema for one evening, which earned back its cost.
While still a student, he was offered a small unpaid intern job at Universal Studios with the editing department. He was later given the opportunity to make a short film for theatrical release, the 26-minute, 35mm, Amblin', which he wrote and directed. Studio vice president Sidney Sheinberg was impressed by the film, which had won a number of awards, and offered Spielberg a seven-year directing contract. It made him the youngest director ever to be signed for a long-term deal with a major Hollywood studio.:548 He subsequently dropped out of college to begin professionally directing TV productions with Universal.
His first professional TV job came when he was hired to direct one of the segments for the 1969 pilot episode of Night Gallery. The segment, "Eyes," starred Joan Crawford; she and Spielberg were reportedly close friends until her death. The episode is unusual in his body of work, in that the camerawork is more highly stylized than his later, more "mature" films. After this, and an episode of Marcus Welby, M.D., Spielberg got his first feature-length assignment: an episode of The Name of the Game called "L.A. 2017". This futuristic science fiction episode impressed Universal Studios and they signed him to a short contract. He did another segment on Night Gallery and did some work for shows such as Owen Marshall: Counselor at Law and The Psychiatrist, before landing the first series episode of Columbo (previous episodes were actually TV films).
Based on the strength of his work, Universal signed Spielberg to do four TV films. The first was a Richard Matheson adaptation called Duel. The film is about a psychotic Peterbilt 281 tanker truck driver who chases the terrified driver (Dennis Weaver) of a small Plymouth Valiant and tries to run him off the road. Special praise of this film by the influential British critic Dilys Powell was highly significant to Spielberg's career. Another TV film (Something Evil) was made and released to capitalize on the popularity of The Exorcist, then a major best-selling book which had not yet been released as a film. He fulfilled his contract by directing the TV film-length pilot of a show called Savage, starring Martin Landau. Spielberg's debut full-length feature film was The Sugarland Express, about a married couple who are chased by police as the couple tries to regain custody of their baby. Spielberg's cinematography for the police chase was praised by reviewers, and The Hollywood Reporter stated that "a major new director is on the horizon.":223 However, the film fared poorly at the box office and received a limited release.
Studio producers Richard D. Zanuck and David Brown offered Spielberg the director's chair for Jaws, a thriller-horror film based on the Peter Benchley novel about an enormous killer shark. Spielberg has often referred to the gruelling shoot as his professional crucible. Despite the film's ultimate, enormous success, it was nearly shut down due to delays and budget over-runs. But Spielberg persevered and finished the film. It was an enormous hit, winning three Academy Awards (for editing, original score and sound) and grossing more than $470 million worldwide at the box office. It also set the domestic record for box office gross, leading to what the press described as "Jawsmania.":248 Jaws made Spielberg a household name and one of America's youngest multi-millionaires, allowing him a great deal of autonomy for his future projects.:250 It was nominated for Best Picture and featured Spielberg's first of three collaborations with actor Richard Dreyfuss.
Rejecting offers to direct Jaws 2, King Kong and Superman, Spielberg and actor Richard Dreyfuss re-convened to work on a film about UFOs, which became Close Encounters of the Third Kind (1977). One of the rare films both written and directed by Spielberg, Close Encounters was a critical and box office hit, giving Spielberg his first Best Director nomination from the Academy as well as earning six other Academy Awards nominations. It won Oscars in two categories (Cinematography, Vilmos Zsigmond, and a Special Achievement Award for Sound Effects Editing, Frank E. Warner). This second blockbuster helped to secure Spielberg's rise. His next film, 1941, a big-budgeted World War II farce, was not nearly as successful and though it grossed over $92.4 million worldwide (and did make a small profit for co-producing studios Columbia and Universal) it was seen as a disappointment, mainly with the critics.
Spielberg then revisited his Close Encounters project and, with financial backing from Columbia Pictures, released Close Encounters: The Special Edition in 1980. For this, Spielberg fixed some of the flaws he thought impeded the original 1977 version of the film and also, at the behest of Columbia, and as a condition of Spielberg revising the film, shot additional footage showing the audience the interior of the mothership seen at the end of the film (a decision Spielberg would later regret as he felt the interior of the mothership should have remained a mystery). Nevertheless, the re-release was a moderate success, while the 2001 DVD release of the film restored the original ending.
Next, Spielberg teamed with Star Wars creator and friend George Lucas on an action adventure film, Raiders of the Lost Ark, the first of the Indiana Jones films. The archaeologist and adventurer hero Indiana Jones was played by Harrison Ford (whom Lucas had previously cast in his Star Wars films as Han Solo). The film was considered an homage to the cliffhanger serials of the Golden Age of Hollywood. It became the biggest film at the box office in 1981, and the recipient of numerous Oscar nominations including Best Director (Spielberg's second nomination) and Best Picture (the second Spielberg film to be nominated for Best Picture). Raiders is still considered a landmark example of the action-adventure genre. The film also led to Ford's casting in Ridley Scott's Blade Runner.
His next directorial feature was the Raiders prequel Indiana Jones and the Temple of Doom. Teaming up once again with Lucas and Ford, the film was plagued with uncertainty for the material and script. This film and the Spielberg-produced Gremlins led to the creation of the PG-13 rating due to the high level of violence in films targeted at younger audiences. In spite of this, Temple of Doom is rated PG by the MPAA, even though it is the darkest and, possibly, most violent Indy film. Nonetheless, the film was still a huge blockbuster hit in 1984. It was on this project that Spielberg also met his future wife, actress Kate Capshaw.
In 1985, Spielberg released The Color Purple, an adaptation of Alice Walker's Pulitzer Prize-winning novel of the same name, about a generation of empowered African-American women during depression-era America. Starring Whoopi Goldberg and future talk-show superstar Oprah Winfrey, the film was a box office smash and critics hailed Spielberg's successful foray into the dramatic genre. Roger Ebert proclaimed it the best film of the year and later entered it into his Great Films archive. The film received eleven Academy Award nominations, including two for Goldberg and Winfrey. However, much to the surprise of many, Spielberg did not get a Best Director nomination.
In 1987, as China began opening to Western capital investment, Spielberg shot the first American film in Shanghai since the 1930s, an adaptation of J. G. Ballard's autobiographical novel Empire of the Sun, starring John Malkovich and a young Christian Bale. The film garnered much praise from critics and was nominated for several Oscars, but did not yield substantial box office revenues. Reviewer Andrew Sarris called it the best film of the year and later included it among the best films of the decade. Spielberg was also a co-producer of the 1987 film *batteries not included.
After two forays into more serious dramatic films, Spielberg then directed the third Indiana Jones film, 1989's Indiana Jones and the Last Crusade. Once again teaming up with Lucas and Ford, Spielberg also cast actor Sean Connery in a supporting role as Indy's father. The film earned generally positive reviews and was another box office success, becoming the highest grossing film worldwide that year; its total box office receipts even topped those of Tim Burton's much-anticipated film Batman, which had been the bigger hit domestically. Also in 1989, he re-united with actor Richard Dreyfuss for the romantic comedy-drama Always, about a daredevil pilot who extinguishes forest fires. Spielberg's first romantic film, Always was only a moderate success and had mixed reviews.
Spielberg's next film, Schindler's List, was based on the true story of Oskar Schindler, a man who risked his life to save 1,100 Jews from the Holocaust. Schindler's List earned Spielberg his first Academy Award for Best Director (it also won Best Picture). With the film a huge success at the box office, Spielberg used the profits to set up the Shoah Foundation, a non-profit organization that archives filmed testimony of Holocaust survivors. In 1997, the American Film Institute listed it among the 10 Greatest American Films ever Made (#9) which moved up to (#8) when the list was remade in 2007.
His next theatrical release in that same year was the World War II film Saving Private Ryan, about a group of U.S. soldiers led by Capt. Miller (Tom Hanks) sent to bring home a paratrooper whose three older brothers were killed in the same twenty-four hours, June 5–6, of the Normandy landing. The film was a huge box office success, grossing over $481 million worldwide and was the biggest film of the year at the North American box office (worldwide it made second place after Michael Bay's Armageddon). Spielberg won his second Academy Award for his direction. The film's graphic, realistic depiction of combat violence influenced later war films such as Black Hawk Down and Enemy at the Gates. The film was also the first major hit for DreamWorks, which co-produced the film with Paramount Pictures (as such, it was Spielberg's first release from the latter that was not part of the Indiana Jones series). Later, Spielberg and Tom Hanks produced a TV mini-series based on Stephen Ambrose's book Band of Brothers. The ten-part HBO mini-series follows Easy Company of the 101st Airborne Division's 506th Parachute Infantry Regiment. The series won a number of awards at the Golden Globes and the Emmys.
Spielberg and actor Tom Cruise collaborated for the first time for the futuristic neo-noir Minority Report, based upon the science fiction short story written by Philip K. Dick about a Washington D.C. police captain in the year 2054 who has been foreseen to murder a man he has not yet met. The film received strong reviews with the review tallying website Rotten Tomatoes giving it a 92% approval rating, reporting that 206 out of the 225 reviews they tallied were positive. The film earned over $358 million worldwide. Roger Ebert, who named it the best film of 2002, praised its breathtaking vision of the future as well as for the way Spielberg blended CGI with live-action.
Also in 2005, Spielberg directed a modern adaptation of War of the Worlds (a co-production of Paramount and DreamWorks), based on the H. G. Wells book of the same name (Spielberg had been a huge fan of the book and the original 1953 film). It starred Tom Cruise and Dakota Fanning, and, as with past Spielberg films, Industrial Light & Magic (ILM) provided the visual effects. Unlike E.T. and Close Encounters of the Third Kind, which depicted friendly alien visitors, War of the Worlds featured violent invaders. The film was another huge box office smash, grossing over $591 million worldwide.
Spielberg's film Munich, about the events following the 1972 Munich Massacre of Israeli athletes at the Olympic Games, was his second film essaying Jewish relations in the world (the first being Schindler's List). The film is based on Vengeance, a book by Canadian journalist George Jonas. It was previously adapted into the 1986 made-for-TV film Sword of Gideon. The film received strong critical praise, but underperformed at the U.S. and world box-office; it remains one of Spielberg's most controversial films to date. Munich received five Academy Awards nominations, including Best Picture, Film Editing, Original Music Score (by John Williams), Best Adapted Screenplay, and Best Director for Spielberg. It was Spielberg's sixth Best Director nomination and fifth Best Picture nomination.
In June 2006, Steven Spielberg announced he would direct a scientifically accurate film about "a group of explorers who travel through a worm hole and into another dimension", from a treatment by Kip Thorne and producer Lynda Obst. In January 2007, screenwriter Jonathan Nolan met with them to discuss adapting Obst and Thorne's treatment into a narrative screenplay. The screenwriter suggested the addition of a "time element" to the treatment's basic idea, which was welcomed by Obst and Thorne. In March of that year, Paramount hired Nolan, as well as scientists from Caltech, forming a workshop to adapt the treatment under the title Interstellar. The following July, Kip Thorne said there was a push by people for him to portray himself in the film. Spielberg later abandoned Interstellar, which was eventually directed by Christopher Nolan.
In early 2009, Spielberg shot the first film in a planned trilogy of motion capture films based on The Adventures of Tintin, written by Belgian artist Hergé, with Peter Jackson. The Adventures of Tintin: The Secret of the Unicorn, was not released until October 2011, due to the complexity of the computer animation involved. The world premiere took place on October 22, 2011 in Brussels, Belgium. The film was released in North American theaters on December 21, 2011, in Digital 3D and IMAX. It received generally positive reviews from critics, and grossed over $373 million worldwide. The Adventures of Tintin won the award for Best Animated Feature Film at the Golden Globe Awards that year. It is the first non-Pixar film to win the award since the category was first introduced. Jackson has been announced to direct the second film.
Spielberg followed with War Horse, shot in England in the summer of 2010. It was released just four days after The Adventures of Tintin, on December 25, 2011. The film, based on the novel of the same name written by Michael Morpurgo and published in 1982, follows the long friendship between a British boy and his horse Joey before and during World War I – the novel was also adapted into a hit play in London which is still running there, as well as on Broadway. The film was released and distributed by Disney, with whom DreamWorks made a distribution deal in 2009. War Horse received generally positive reviews from critics, and was nominated for six Academy Awards, including Best Picture.
Spielberg next directed the historical drama film Lincoln, starring Daniel Day-Lewis as United States President Abraham Lincoln and Sally Field as Mary Todd Lincoln. Based on Doris Kearns Goodwin's bestseller Team of Rivals: The Political Genius of Abraham Lincoln, the film covered the final four months of Lincoln's life. Written by Tony Kushner, the film was shot in Richmond, Virginia, in late 2011, and was released in the United States by Disney in November 2012. The film's international distribution was handled by 20th Century Fox. Upon release, Lincoln received widespread critical acclaim, and was nominated for twelve Academy Awards (the most of any film that year) including Best Picture and Best Director for Spielberg. It won the award for Best Production Design and Day-Lewis won the Academy Award for Best Actor for his portrayal of Lincoln, becoming the first three time winner in that category as well as the first to win for a performance directed by Spielberg.
Spielberg directed 2015's Bridge of Spies, a Cold War thriller based on the 1960 U-2 incident, and focusing on James B. Donovan's negotiations with the Soviets for the release of pilot Gary Powers after his aircraft was shot down over Soviet territory. The film starred Tom Hanks as Donovan, as well as Mark Rylance, Amy Ryan, and Alan Alda, with a script by the Coen brothers. The film was shot from September to December 2014 on location in New York City, Berlin and Wroclaw, Poland (which doubled for East Berlin), and was released by Disney on October 16, 2015. Bridge of Spies received positive reviews from critics, and was nominated for six Academy Awards, including Best Picture.
Since the mid-1980s, Spielberg has increased his role as a film producer. He headed up the production team for several cartoons, including the Warner Bros. hits Tiny Toon Adventures, Animaniacs, Pinky and the Brain, Toonsylvania, and Freakazoid!, for which he collaborated with Jean MacCurdy and Tom Ruegger. Due to his work on these series, in the official titles, most of them say, "Steven Spielberg presents" as well as making numerous cameos on the shows. Spielberg also produced the Don Bluth animated features, An American Tail and The Land Before Time, which were released by Universal Studios. He also served as one of the executive producers of Who Framed Roger Rabbit and its three related shorts (Tummy Trouble, Roller Coaster Rabbit, Trail Mix-Up), which were all released by Disney, under both the Walt Disney Pictures and the Touchstone Pictures banners. He was furthermore, for a short time, the executive producer of the long-running medical drama ER. In 1989, he brought the concept of The Dig to LucasArts. He contributed to the project from that time until 1995 when the game was released. He also collaborated with software publishers Knowledge Adventure on the multimedia game Steven Spielberg's Director's Chair, which was released in 1996. Spielberg appears, as himself, in the game to direct the player. The Spielberg name provided branding for a Lego Moviemaker kit, the proceeds of which went to the Starbright Foundation.
Spielberg served as an uncredited executive producer on The Haunting, The Prince of Egypt, Just Like Heaven, Shrek, Road to Perdition, and Evolution. He served as an executive producer for the 1997 film Men in Black, and its sequels, Men in Black II and Men in Black III. In 2005, he served as a producer of Memoirs of a Geisha, an adaptation of the novel by Arthur Golden, a film to which he was previously attached as director. In 2006, Spielberg co-executive produced with famed filmmaker Robert Zemeckis a CGI children's film called Monster House, marking their eighth collaboration since 1990's Back to the Future Part III. He also teamed with Clint Eastwood for the first time in their careers, co-producing Eastwood's Flags of Our Fathers and Letters from Iwo Jima with Robert Lorenz and Eastwood himself. He earned his twelfth Academy Award nomination for the latter film as it was nominated for Best Picture. Spielberg served as executive producer for Disturbia and the Transformers live action film with Brian Goldner, an employee of Hasbro. The film was directed by Michael Bay and written by Roberto Orci and Alex Kurtzman, and Spielberg continued to collaborate on the sequels, Transformers: Revenge of the Fallen and Transformers: Dark of the Moon. In 2011, he produced the J. J. Abrams science fiction thriller film Super 8 for Paramount Pictures.
Other major television series Spielberg produced were Band of Brothers, Taken and The Pacific. He was an executive producer on the critically acclaimed 2005 TV miniseries Into the West which won two Emmy awards, including one for Geoff Zanelli's score. For his 2010 miniseries The Pacific he teamed up once again with co-producer Tom Hanks, with Gary Goetzman also co-producing'. The miniseries is believed to have cost $250 million and is a 10-part war miniseries centered on the battles in the Pacific Theater during World War II. Writer Bruce McKenna, who penned several installments of (Band of Brothers), was the head writer.
In 2011, Spielberg launched Falling Skies, a science fiction television series, on the TNT network. He developed the series with Robert Rodat and is credited as an executive producer. Spielberg is also producing the Fox TV series Terra Nova. Terra Nova begins in the year 2149 when all life on the planet Earth is threatened with extinction resulting in scientists opening a door that allows people to travel back 85 million years to prehistoric times. Spielberg also produced The River, Smash, Under the Dome, Extant and The Whispers, as well as a TV adaptation of Minority Report.
Apart from being an ardent gamer Spielberg has had a long history of involvement in video games. He has been giving thanks to his games of his division DreamWorks Interactive most notable as Someone's in the Kitchen with script written by Animaniacs' Paul Rugg, Goosebumps: Escape from HorrorLand, The Neverhood (all in 1996), Skullmonkeys, Dilbert's Desktop Games, Goosebumps: Attack of the Mutant (all 1997), Boombots (1999), T'ai Fu: Wrath of the Tiger (1999), and Clive Barker's Undying (2001). In 2005 the director signed with Electronic Arts to collaborate on three games including an action game and an award winning puzzle game for the Wii called Boom Blox (and its 2009 sequel: Boom Blox Bash Party). Previously, he was involved in creating the scenario for the adventure game The Dig. In 1996, Spielberg worked on and shot original footage for a movie-making simulation game called Steven Spielberg's Director's Chair. He is the creator of the Medal of Honor series by Electronic Arts. He is credited in the special thanks section of the 1998 video game Trespasser. In 2013, Spielberg has announced he is collaborating with 343 Industries for a live-action TV show of Halo.
Spielberg has filmed and is currently in post-production on an adaptation of Roald Dahl's celebrated children's story The BFG. Spielberg's DreamWorks bought the rights in 2010, originally intending John Madden to direct. The film was written by E.T. screenwriter Melissa Mathison and is co-produced by Walt Disney Pictures, marking the first Disney-branded film to be directed by Spielberg. The BFG is set to premiere out of competition at the Cannes Film Festival in May 2016, before its wide release in the US on July 1, 2016.
After completing filming on Ready Player One, while it is in its lengthy, effects-heavy post-production, he will film his long-planned adaptation of David Kertzer's acclaimed The Kidnapping of Edgardo Mortara. The book follows the true story of a young Jewish boy in 1858 Italy who was secretly baptized by a family servant and then kidnapped from his family by the Papal States, where he was raised and trained as a priest, causing international outrage and becoming a media sensation. First announced in 2014, the book has been adapted by Tony Kushner and the film will again star Mark Rylance, as Pope Pius IX. It will be filmed in early 2017 for release at the end of that year, before Ready Player One is completed and released in 2018.
Spielberg was scheduled to shoot a $200 million adaptation of Daniel H. Wilson's novel Robopocalypse, adapted for the screen by Drew Goddard. The film would follow a global human war against a robot uprising about 15–20 years in the future. Like Lincoln, it was to be released by Disney in the United States and Fox overseas. It was set for release on April 25, 2014, with Anne Hathaway and Chris Hemsworth set to star, but Spielberg postponed production indefinitely in January 2013, just before it had been set to begin.
Spielberg's films often deal with several recurring themes. Most of his films deal with ordinary characters searching for or coming in contact with extraordinary beings or finding themselves in extraordinary circumstances. In an AFI interview in August 2000 Spielberg commented on his interest in the possibility of extra terrestrial life and how it has influenced some of his films. Spielberg described himself as feeling like an alien during childhood, and his interest came from his father, a science fiction fan, and his opinion that aliens would not travel light years for conquest, but instead curiosity and sharing of knowledge.
A strong consistent theme in his family-friendly work is a childlike, even naïve, sense of wonder and faith, as attested by works such as Close Encounters of the Third Kind, E.T. the Extra-Terrestrial, Hook, A.I. Artificial Intelligence and The BFG. According to Warren Buckland, these themes are portrayed through the use of low height camera tracking shots, which have become one of Spielberg's directing trademarks. In the cases when his films include children (E.T. the Extra-Terrestrial, Empire of the Sun, Jurassic Park, etc.), this type of shot is more apparent, but it is also used in films like Munich, Saving Private Ryan, The Terminal, Minority Report, and Amistad. If one views each of his films, one will see this shot utilized by the director, notably the water scenes in Jaws are filmed from the low-angle perspective of someone swimming. Another child oriented theme in Spielberg's films is that of loss of innocence and coming-of-age. In Empire of the Sun, Jim, a well-groomed and spoiled English youth, loses his innocence as he suffers through World War II China. Similarly, in Catch Me If You Can, Frank naively and foolishly believes that he can reclaim his shattered family if he accumulates enough money to support them.
The most persistent theme throughout his films is tension in parent-child relationships. Parents (often fathers) are reluctant, absent or ignorant. Peter Banning in Hook starts off in the beginning of the film as a reluctant married-to-his-work parent who through the course of the film regains the respect of his children. The notable absence of Elliott's father in E.T., is the most famous example of this theme. In Indiana Jones and the Last Crusade, it is revealed that Indy has always had a very strained relationship with his father, who is a professor of medieval literature, as his father always seemed more interested in his work, specifically in his studies of the Holy Grail, than in his own son, although his father does not seem to realize or understand the negative effect that his aloof nature had on Indy (he even believes he was a good father in the sense that he taught his son "self reliance," which is not how Indy saw it). Even Oskar Schindler, from Schindler's List, is reluctant to have a child with his wife. Munich depicts Avner as a man away from his wife and newborn daughter. There are of course exceptions; Brody in Jaws is a committed family man, while John Anderton in Minority Report is a shattered man after the disappearance of his son. This theme is arguably the most autobiographical aspect of Spielberg's films, since Spielberg himself was affected by his parents' divorce as a child and by the absence of his father. Furthermore, to this theme, protagonists in his films often come from families with divorced parents, most notably E.T. the Extra-Terrestrial (protagonist Elliot's mother is divorced) and Catch Me If You Can (Frank Abagnale's mother and father split early on in the film). Little known also is Tim in Jurassic Park (early in the film, another secondary character mentions Tim and Lex's parents' divorce). The family often shown divided is often resolved in the ending as well. Following this theme of reluctant fathers and father figures, Tim looks to Dr. Alan Grant as a father figure. Initially, Dr. Grant is reluctant to return those paternal feelings to Tim. However, by the end of the film, he has changed, and the kids even fall asleep with their heads on his shoulders.
In terms of casting and production itself, Spielberg has a known penchant for working with actors and production members from his previous films. For instance, he has cast Richard Dreyfuss in several films: Jaws, Close Encounters of the Third Kind, and Always. Aside from his role as Indiana Jones, Spielberg also cast Harrison Ford as a headteacher in E.T. the Extra-Terrestrial (though the scene was ultimately cut). Although Spielberg directed veteran voice actor Frank Welker only once (in Raiders of the Lost Ark, for which he voiced many of the animals), Welker has lent his voice in a number of productions Spielberg has executive produced from Gremlins to its sequel Gremlins 2: The New Batch, as well as The Land Before Time, Who Framed Roger Rabbit, and television shows such as Tiny Toons, Animaniacs, and SeaQuest DSV. Spielberg has used Tom Hanks on several occasions and has cast him in Saving Private Ryan, Catch Me If You Can, The Terminal, and Bridge of Spies. Spielberg has collaborated with Tom Cruise twice on Minority Report and War of the Worlds, and cast Shia LaBeouf in five films: Transformers, Eagle Eye, Indiana Jones and the Kingdom of the Crystal Skull, Transformers: Revenge of the Fallen, and Transformers: Dark of the Moon.
Spielberg prefers working with production members with whom he has developed an existing working relationship. An example of this is his production relationship with Kathleen Kennedy who has served as producer on all his major films from E.T. the Extra-Terrestrial to the recent Lincoln. For cinematography, Allen Daviau, a childhood friend and cinematographer, shot the early Spielberg film Amblin and most of his films up to Empire of the Sun; Janusz Kamiński who has shot every Spielberg film since Schindler's List (see List of film director and cinematographer collaborations); and the film editor Michael Kahn who has edited every film directed by Spielberg from Close Encounters to Munich (except E.T. the Extra-Terrestrial). Most of the DVDs of Spielberg's films have documentaries by Laurent Bouzereau.
A famous example of Spielberg working with the same professionals is his long-time collaboration with John Williams and the use of his musical scores in all of his films since The Sugarland Express (except Bridge of Spies, The Color Purple and Twilight Zone: The Movie). One of Spielberg's trademarks is his use of music by Williams to add to the visual impact of his scenes and to try and create a lasting picture and sound of the film in the memories of the film audience. These visual scenes often uses images of the sun (e.g. Empire of the Sun, Saving Private Ryan, the final scene of Jurassic Park, and the end credits of Indiana Jones and the Last Crusade (where they ride into the sunset)), of which the last two feature a Williams score at that end scene. Spielberg is a contemporary of filmmakers George Lucas, Francis Ford Coppola, Martin Scorsese, John Milius, and Brian De Palma, collectively known as the "Movie Brats". Aside from his principal role as a director, Spielberg has acted as a producer for a considerable number of films, including early hits for Joe Dante and Robert Zemeckis. Spielberg has often never worked with the same screenwriter in his films, beside Tony Kushner and David Koepp, who have written a few of his films more than once.
Spielberg first met actress Amy Irving in 1976 at the suggestion of director Brian De Palma, who knew he was looking for an actress to play in Close Encounters. After meeting her, Spielberg told his co-producer Julia Phillips, "I met a real heartbreaker last night.":293 Although she was too young for the role, she and Spielberg began dating and she eventually moved in to what she described as his "bachelor funky" house.:294 They lived together for four years, but the stresses of their professional careers took a toll on their relationship. Irving wanted to be certain that whatever success she attained as an actress would be her own: "I don't want to be known as Steven's girlfriend," she said, and chose not to be in any of his films during those years.:295
As a result, they broke up in 1979, but remained close friends. Then in 1984 they renewed their romance, and in November 1985, they married, already having had a son, Max Samuel. After three and a half years of marriage, however, many of the same competing stresses of their careers caused them to divorce in 1989. They agreed to maintain homes near each other as to facilitate the shared custody and parenting of their son.:403 Their divorce was recorded as the third most costly celebrity divorce in history.
In 2002, Spielberg was one of eight flagbearers who carried the Olympic Flag into Rice-Eccles Stadium at the Opening Ceremonies of the 2002 Winter Olympic Games in Salt Lake City. In 2006, Premiere listed him as the most powerful and influential figure in the motion picture industry. Time listed him as one of the 100 Most Important People of the Century. At the end of the 20th century, Life named him the most influential person of his generation. In 2009, Boston University presented him an honorary Doctor of Humane Letters degree.
According to Forbes' Most Influential Celebrities 2014 list, Spielberg was listed as the most influential celebrity in America. The annual list is conducted by E-Poll Market Research and it gave more than 6,600 celebrities on 46 different personality attributes a score representing "how that person is perceived as influencing the public, their peers, or both." Spielberg received a score of 47, meaning 47% of the US believes he is influential. Gerry Philpott, president of E-Poll Market Research, supported Spielberg's score by stating, "If anyone doubts that Steven Spielberg has greatly influenced the public, think about how many will think for a second before going into the water this summer."
A collector of film memorabilia, Spielberg purchased a balsa Rosebud sled from Citizen Kane (1941) in 1982. He bought Orson Welles's own directorial copy of the script for the radio broadcast The War of the Worlds (1938) in 1994. Spielberg has purchased Academy Award statuettes being sold on the open market and donated them to the Academy of Motion Picture Arts and Sciences, to prevent their further commercial exploitation. His donations include the Oscars that Bette Davis received for Dangerous (1935) and Jezebel (1938), and Clark Gable's Oscar for It Happened One Night (1934).
Since playing Pong while filming Jaws in 1974, Spielberg has been an avid video gamer. Spielberg played many of LucasArts adventure games, including the first Monkey Island games. He owns a Wii, a PlayStation 3, a PSP, and Xbox 360, and enjoys playing first-person shooters such as the Medal of Honor series and Call of Duty 4: Modern Warfare. He has also criticized the use of cut scenes in games, calling them intrusive, and feels making story flow naturally into the gameplay is a challenge for future game developers.
Drawing from his own experiences in Scouting, Spielberg helped the Boy Scouts of America develop a merit badge in cinematography in order to help promote filmmaking as a marketable skill. The badge was launched at the 1989 National Scout Jamboree, which Spielberg attended, and where he personally counseled many boys in their work on requirements. That same year, 1989, saw the release of Indiana Jones and the Last Crusade. The opening scene shows a teenage Indiana Jones in scout uniform bearing the rank of a Life Scout. Spielberg stated he made Indiana Jones a Boy Scout in honor of his experience in Scouting. For his career accomplishments, service to others, and dedication to a new merit badge Spielberg was awarded the Distinguished Eagle Scout Award.
In 2004 he was admitted as knight of the Légion d'honneur by president Jacques Chirac. On July 15, 2006, Spielberg was also awarded the Gold Hugo Lifetime Achievement Award at the Summer Gala of the Chicago International Film Festival, and also was awarded a Kennedy Center honour on December 3. The tribute to Spielberg featured a short, filmed biography narrated by Tom Hanks and included thank-yous from World War II veterans for Saving Private Ryan, as well as a performance of the finale to Leonard Bernstein's Candide, conducted by John Williams (Spielberg's frequent composer).[citation needed]
The Science Fiction Hall of Fame inducted Spielberg in 2005, the first year it considered non-literary contributors. In November 2007, he was chosen for a Lifetime Achievement Award to be presented at the sixth annual Visual Effects Society Awards in February 2009. He was set to be honored with the Cecil B. DeMille Award at the January 2008 Golden Globes; however, the new, watered-down format of the ceremony resulting from conflicts in the 2007–08 writers strike, the HFPA postponed his honor to the 2009 ceremony. In 2008, Spielberg was awarded the Légion d'honneur.
In psychology, memory is the process in which information is encoded, stored, and retrieved. Encoding allows information from the outside world to be sensed in the form of chemical and physical stimuli. In the first stage the information must be changed so that it may be put into the encoding process. Storage is the second memory stage or process. This entails that information is maintained over short periods of time. Finally the third process is the retrieval of information that has been stored. Such information must be located and returned to the consciousness. Some retrieval attempts may be effortless due to the type of information, and other attempts to remember stored information may be more demanding for various reasons.
Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent a visual code. Conrad (1964) found that test subjects had more difficulty recalling collections of letters that were acoustically similar (e.g. E, P, D). Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad's (1964) study, however, deals with the encoding of written text; thus, while memory of written language may rely on acoustic components, generalisations to all forms of memory cannot be made.
Short-term memory is also known as working memory. Short-term memory allows recall for a period of several seconds to a minute without rehearsal. Its capacity is also very limited: George A. Miller (1956), when working at Bell Laboratories, conducted experiments showing that the store of short-term memory was 7±2 items (the title of his famous paper, "The magical number 7±2"). Modern estimates of the capacity of short-term memory are lower, typically of the order of 4–5 items; however, memory capacity can be increased through a process called chunking. For example, in recalling a ten-digit telephone number, a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456) and lastly a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This may be reflected in some countries in the tendency to display telephone numbers as several chunks of two to four numbers.
The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration, which means that information is not retained indefinitely. By contrast, long-term memory can store much larger quantities of information for potentially unlimited duration (sometimes a whole life span). Its capacity is immeasurable. For example, given a random seven-digit number we may remember it for only a few seconds before forgetting, suggesting it was stored in our short-term memory. On the other hand, we can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory.
The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient 'KF' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was brain damaged, displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Interestingly, visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory.
Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. Without the hippocampus, new memories are unable to be stored into long-term memory, as learned from patient Henry Molaison after removal of both his hippocampi, and there will be a very short attention span. Furthermore, it may be involved in changing neural connections for a period of three months or more after the initial learning.
Sensory memory holds sensory information less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is the example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to "see" more than they can actually report. The first experiments exploring this form of sensory memory were conducted by George Sperling (1963) using the "partial report paradigm". Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments,Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the "whole report" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal.
While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966) discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, "which attempts to capture information such as 'what', 'when' and 'where'". With episodic memory, individuals are able to recall specific events such as birthday parties and weddings.
Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children’s memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants’ recognition memory and their recall memory. Habituation and operant conditioning techniques have been used to assess infants’ recognition memory and the deferred and elicited imitation techniques have been used to assess infants’ recall memory.
Another major way to distinguish different memory functions is whether the content to be remembered is in the past, retrospective memory, or in the future, prospective memory. Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or remembering to remember (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory.
Hebb distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of cortisol or epinephrine help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause memory loss and problems with memory storage. This memory loss includes, retrograde amnesia which is the loss of memory for events that occurred shortly before the time of brain damage.
One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved.
Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal representation implies that such definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called engram or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007).
In contrast, procedural memory (or implicit memory) is not based on the conscious recall of information, but on implicit learning. It can best be summarized as remember how to do something. Procedural memory is primarily employed in learning motor skills and should be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition - no new explicit memories have been formed, but one is unconsciously accessing aspects of those previous experiences. Procedural memory involved in motor learning depends on the cerebellum and basal ganglia.
The working memory model explains many practical observations, such as why it is easier to do two different tasks (one verbal and one visual) than two similar tasks (e.g., two visual), and the aforementioned word-length effect. However, the concept of a central executive as noted here has been criticised as inadequate and vague.[citation needed] Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics.
One of the key concerns of older adults is the experience of memory loss, especially as it is one of the hallmark symptoms of Alzheimer's disease. However, memory loss is qualitatively different in normal aging from the kind of memory loss associated with a diagnosis of Alzheimer's (Budson & Price, 2005). Research has revealed that individuals’ performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information; source memory tasks that require them to remember the specific circumstances or context in which they learned information; and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example.
It should be noted that although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order - that is, recalling step 1 and then step 2. In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence. Researchers have suggested that these age differences are probably due to the fact that the dentate gyrus of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months.
Declarative memory can be further sub-divided into semantic memory, concerning principles and facts taken independent of context; and episodic memory, concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract knowledge about the world, such as "Paris is the capital of France". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal associations of a particular place or time. Episodic memories often reflect the "firsts" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one's life that can be remembered clearly. Autobiographical memory - memory for particular events within one's own life - is generally viewed as either equivalent to, or a subset of, episodic memory. Visual memory is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a mental image. Visual memory can result in priming and it is assumed some kind of perceptual representational system underlies this phenomenon.[citation needed]
Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats. An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or ‘Socially Evaluated Cold Pressor Test’) for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process.
Interference can hamper memorization and retrieval. There is retroactive interference, when learning new information makes it harder to recall old information and proactive interference, where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer.
Up until the middle of the 1980s it was assumed that infants could not encode, retain, and retrieve information. A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay. Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months. In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it.
Brain areas involved in the neuroanatomy of memory such as the hippocampus, the amygdala, the striatum, or the mammillary bodies are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and declarative learning, while the amygdala is thought to be involved in emotional memory. Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway traveling through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart, learning, as solely dependent on specific brain regions. Learning and memory are attributed to changes in neuronal synapses, thought to be mediated by long-term potentiation and long-term depression.
The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they have often difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life.
Stressful life experiences may be a cause of memory loss as a person ages. Glucocorticoids that are released during stress damage neurons that are located in the hippocampal region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The CA1 neurons found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of glutamate. This high level of extracellular glutamate allow calcium to enter NMDA receptors which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind. This directly relates to traumatic events in one's past such as kidnappings, being prisoners of war or sexual abuse as a child.
Sleep does not affect acquisition or recall while one is awake. Therefore, sleep has the greatest effect on memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain’s abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). This process implicates that memories are reactivated during sleep, but that the process doesn’t enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. When you are sleeping, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When you do not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. Therefore, it is important to get the proper amount of sleep so that memory can function at the highest level. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, suggesting that new memories may be solidified through such rehearsal.
A UCLA research study published in the June 2006 issue of the American Journal of Geriatric Psychiatry found that people can improve cognitive function and brain efficiency through simple lifestyle changes such as incorporating memory exercises, healthy eating, physical fitness and stress reduction into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance. Eight subjects were asked to follow a "brain healthy" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long term follow up was conducted, it is therefore unclear if this intervention has lasting effects on memory.
Much of the current knowledge of memory has come from studying memory disorders, particularly amnesia. Loss of memory is known as amnesia. Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus. There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain's memory systems, and thus hypothesize their function in the normally working brain. Other neurological disorders such as Alzheimer's disease and Parkinson's disease can also affect memory and cognition. Hyperthymesia, or hyperthymesic syndrome, is a disorder that affects an individual's autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored. Korsakoff's syndrome, also known as Korsakoff's psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex.
Physical exercise, particularly continuous aerobic exercises such as running, cycling and swimming, has many cognitive benefits and effects on the brain. Influences on the brain include increases in neurotransmitter levels, improved oxygen and nutrient delivery, and increased neurogenesis in the hippocampus. The effects of exercise on memory have important implications for improving children's academic performance, maintaining mental abilities in old age, and the prevention and potential cure of neurological diseases.
However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated. Seventy-two healthy female and male university students, randomly assigned to the SECPT stress test or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game "Concentration" or "Memory". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar.
Interestingly, research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996) asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events.
Although people often think that memory operates like recording equipment, it is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime. In fact, research has revealed that our memories are constructed. People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974) in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, "How fast were the cars going when they smashed into each other?" gave higher estimates than those who were asked, "How fast were the cars going when they hit each other?" Furthermore, when asked a week later whether they have seen broken glass in the film, those who had been asked the question with smashed were twice more likely to report that they have seen broken glass than those who had been asked the question with hit. There was no broken glass depicted in the film. Thus, the wording of the questions distorted viewers’ memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with smashed recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.
Memorization is a method of learning that allows an individual to recall information verbatim. Rote learning is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as Cosmos Rossellius using visual alphabets. The spacing effect shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is cramming: an intensive memorization in a short period of time. Also relevant is the Zeigarnik effect which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called Method of loci uses spatial memory to memorize non-spatial information.
Estonia (i/ɛˈstoʊniə/; Estonian: Eesti [ˈeːsti]), officially the Republic of Estonia (Estonian: Eesti Vabariik), is a country in the Baltic region of Northern Europe. It is bordered to the north by the Gulf of Finland, to the west by the Baltic Sea, to the south by Latvia (343 km), and to the east by Lake Peipus and Russia (338.6 km). Across the Baltic Sea lies Sweden in the west and Finland in the north. The territory of Estonia consists of a mainland and 2,222 islands and islets in the Baltic Sea, covering 45,339 km2 (17,505 sq mi) of land, and is influenced by a humid continental climate.
After centuries of Danish, Swedish and German rule the native Estonians started to yearn for independence during the period of national awakening while being governed by the Russian Empire. Established on 24 February 1918, the Republic of Estonia came into existence towards the end of World War I. During World War II, Estonia was then occupied by the Soviet Union in 1940, then Nazi Germany a year later and again in 1944 establishing the Estonian Soviet Socialist Republic. In 1988, during the Singing Revolution, the Estonian SSR issued the Estonian Sovereignty Declaration to defy against the illegal Soviet rule. Estonia then restored its independence during the 1991 coup by the Soviets on the night of 20 August 1991.
A developed country with an advanced, high-income economy and high living standards, Estonia ranks very high in the Human Development Index, and performs favourably in measurements of economic freedom, civil liberties, education, and press freedom (third in the world in 2012). Estonia has been among the fastest growing economies in the European Union and is a part of the World Trade Organization and the Nordic Investment Bank. Estonia is often described as one of the most internet-focused countries in Europe.
In the first centuries AD, political and administrative subdivisions began to emerge in Estonia. Two larger subdivisions appeared: the province (Estonian: kihelkond) and the land (Estonian: maakond). Several elderships or villages made up a province. Nearly all provinces had at least one fortress. The king or other highest administrative official elder directed the defense of the local area. By the thirteenth century Estonia consisted of the following provinces: Revala, Harjumaa, Saaremaa, Hiiumaa, Läänemaa, Alempois, Sakala, Ugandi, Jogentagana, Soopoolitse, Vaiga, Mõhu, Nurmekund, Järvamaa and Virumaa.
The Oeselians or Osilians (Estonian saarlased; singular: saarlane) were a historical subdivision of Estonians inhabiting Saaremaa (Danish: Øsel; German: Ösel; Swedish: Ösel), an Estonian island in the Baltic Sea. They were first mentioned as early as the second century BC in Ptolemy's Geography III. The Oeselians were known in the Old Norse Icelandic Sagas and in Heimskringla as Víkingr frá Esthland (Estonian Vikings). Their sailing vessels were called pirate ships by Henry of Latvia in his Latin chronicles written at the beginning of the 13th century.
Perhaps the most famous raid by Oeselian pirates occurred in 1187, with the attack on the Swedish town of Sigtuna by Finnic raiders from Couronia and Oesel. Among the casualties of this raid was the Swedish archbishop Johannes. The city remained occupied for some time, contributing to its decline as a center of commerce in the 13th century and the rise of Uppsala, Visby, Kalmar and Stockholm. The Livonian Chronicle describes the Oeselians as using two kinds of ships, the piratica and the liburna. The former was a warship, the latter mainly a merchant ship. A piratica could carry approximately 30 men and had a high prow shaped like a dragon or a snakehead and a rectangular sail. Viking-age treasures from Estonia mostly contain silver coins and bars. Saaremaa has the richest finds of Viking treasures after Gotland in Sweden. This strongly suggests that Estonia was an important transit country during the Viking era.
The superior god of Oeselians as described by Henry of Latvia was called Tharapita. According to the legend in the chronicle Tharapita was born on a forested mountain in Virumaa (Latin: Vironia), mainland Estonia from where he flew to Oesel, Saaremaa The name Taarapita has been interpreted as "Taara, help!"/"Thor, help!" (Taara a(v)ita in Estonian) or "Taara keeper"/"Thor keeper" (Taara pidaja) Taara is associated with the Scandinavian god Thor. The story of Tharapita's or Taara's flight from Vironia to Saaremaa has been associated with a major meteor disaster estimated to have happened in 660 ± 85 BC that formed Kaali crater in Saaremaa.
The capital of Danish Estonia (Danish: Hertugdømmet Estland) was Reval (Tallinn), founded at the place of Lyndanisse after the invasion of 1219. The Danes built the fortress of Castrum Danorum at Toompea Hill. Estonians still call their capital "Tallinn", which according to legend derives from Taani linna (meaning Danish town or castle). Reval was granted Lübeck city rights (1248) and joined the Hanseatic League. Even today, Danish influence can be seen in heraldic symbols. The Danish cross is on the city of Tallinn's coat of arms, and Estonia's coat of arms displays three lions similar to those found on the Danish coat of arms.
On St. George's Night (Estonian: Jüriöö ülestõus) 23 April 1343, the indigenous Estonian population in the Duchy of Estonia, the Bishopric of Ösel-Wiek and the insular territories of the State of the Teutonic Order tried to rid themselves of the Danish and German rulers and landlords, who had conquered the country in the 13th century during the Livonian crusade, and to eradicate the non-indigenous Christian religion. After initial success the revolt was ended by the invasion of the Teutonic Order. In 1346 the Duchy of Estonia was sold for 19,000 Köln marks to the Teutonic Order by the King of Denmark. The shift of sovereignty from Denmark to the State of the Teutonic Order took place on 1 November 1346.
From 1228, after of the Livonian Crusade, through the 1560s, Estonia was part of Terra Mariana, established on 2 February 1207 as a principality of the Holy Roman Empire and proclaimed by Pope Innocent III in 1215 as subject to the Holy See. The southern parts of the country were conquered by Livonian Brothers of the Sword who joined the Teutonic Order in 1237 and became its branch known as the Livonian Order. The Duchy of Estonia was created out of the northern parts of the country and was a direct dominion of the King of Denmark from 1219 until 1346, when it was sold to the Teutonic Order and became part of the Ordenstaat. In 1343, the people of northern Estonia and Saaremaa rebelled against German rule in the St. George's Night Uprising, which was put down by 1345. The unsuccessful rebellion led to a consolidation of power for the Baltic German minority. For the subsequent centuries they remained the ruling elite in both cities and in the countryside.
After the decline of the Teutonic Order following its defeat in the Battle of Grunwald in 1410, and the defeat of the Livonian Order in the Battle of Swienta on 1 September 1435, the Livonian Confederation Agreement was signed on 4 December 1435. The Livonian Confederation ceased to exist during the Livonian War (1558–82). The wars had reduced the Estonian population from about 250–300,000 people before the Livonian War to 120–140,000 in the 1620s. The Grand Duchy of Moscow and Tsardom of Russia also attempted invasions in 1481 and 1558, both of which were unsuccessful .
The Reformation in Europe officially began in 1517 with Martin Luther (1483–1546) and his 95 Theses. The Reformation greatly changed the Baltic region. Its ideas came quickly to the Livonian Confederation and by the 1520s were widespread. Language, education, religion and politics were transformed. Church services were now conducted in the vernacular instead of in Latin, previously used. During the Livonian War in 1561, northern Estonia submitted to Swedish control. In the 1560s two voivodeships of present-day southern Estonia, Dorpat Voivodeship (Tartu region) and Parnawa Voivodeship (Pärnu region), became the autonomous Duchy of Livonia within the Polish-Lithuanian Commonwealth, under joint control of the Polish Crown and the Grand Duchy. In 1629, mainland Estonia came entirely under Swedish rule. Estonia was administratively divided between the provinces of Estonia in the north and Livonia in southern Estonia and northern Latvia. This division persisted until the early twentieth century.
As a result of the abolition of serfdom and the availability of education to the native Estonian-speaking population, an active Estonian nationalist movement developed in the 19th century.[citation needed] It began on a cultural level, resulting in the establishment of Estonian language literature, theatre and professional music and led on to the formation of the Estonian national identity and the Age of Awakening. Among the leaders of the movement were Johann Voldemar Jannsen, Jakob Hurt and Carl Robert Jakobson.
On 14 June, while the world's attention was focused on the fall of Paris to Nazi Germany a day earlier, the Soviet military blockade on Estonia went into effect, two Soviet bombers downed the Finnish passenger aeroplane "Kaleva" flying from Tallinn to Helsinki carrying three diplomatic pouches from the US delegations in Tallinn, Riga and Helsinki. On 16 June, the Soviet Union invaded Estonia. The Red Army exited from their military bases in Estonia on 17 June. The following day, some 90,000 additional troops entered the country. In the face of overwhelming Soviet force, the Estonian government capitulated on 17 June 1940 to avoid bloodshed.
Most of the Estonian Defence Forces surrendered according to the orders of the Estonian government, believing that resistance was useless and were disarmed by the Red Army. Only the Estonian Independent Signal Battalion showed resistance to Red Army and Communist militia "People's Self-Defence" units in front of the XXI Grammar School in Tallinn on 21 June. As the Red Army brought in additional reinforcements supported by six armoured fighting vehicles, the battle lasted several hours until sundown. Finally the military resistance was ended with negotiations and the Independent Signal Battalion surrendered and was disarmed. There were two dead Estonian servicemen, Aleksei Männikus and Johannes Mandre, and several wounded on the Estonian side and about ten killed and more wounded on the Soviet side.
On 6 August 1940, Estonia was annexed by the Soviet Union as the Estonian SSR. The provisions in the Estonian constitution requiring a popular referendum to decide on joining a supra-national body were ignored. Instead the vote to join the Soviet Union was taken by those elected in the elections held the previous month. Additionally those who had failed to do their "political duty" of voting Estonia into the USSR, specifically those who had failed to have their passports stamped for voting, were condemned to death by Soviet tribunals. The repressions followed with the mass deportations carried out by the Soviets in Estonia on 14 June 1941. Many of the country's political and intellectual leaders were killed or deported to remote areas of the USSR by the Soviet authorities in 1940–1941. Repressive actions were also taken against thousands of ordinary people.
After Germany invaded the Soviet Union on 22 June 1941, the Wehrmacht crossed the Estonian southern border on 7 July. The Red Army retreated behind the Pärnu River – Emajõgi line on 12 July. At the end of July the Germans resumed their advance in Estonia working in tandem with the Estonian Forest Brothers. Both German troops and Estonian partisans took Narva on 17 August and the Estonian capital Tallinn on 28 August. After the Soviets were driven out from Estonia, German troops disarmed all the partisan groups.
Although initially the Germans were welcomed by most Estonians as liberators from the USSR and its oppressions, and hopes were raised for the restoration of the country's independence, it was soon realised that the Nazis were but another occupying power. The Germans used Estonia's resources for their war effort; for the duration of the occupation Estonia was incorporated into the German province of Ostland. The Germans and their collaborators also carried out The Holocaust in Estonia in which they established a network of concentration camps and murdered thousands of Estonian Jews and Estonian Gypsies, other Estonians, non-Estonian Jews, and Soviet prisoners of war.
Some Estonians, unwilling to side directly with the Nazis, joined the Finnish Army (which was allied with the Nazis) to fight against the Soviet Union. The Finnish Infantry Regiment 200 (Estonian: soomepoisid) was formed out of Estonian volunteers in Finland. Although many Estonians were recruited into the German armed forces (including Estonian Waffen-SS), the majority of them did so only in 1944 when the threat of a new invasion of Estonia by the Red Army had become imminent. In January 1944 Estonia was again facing the prospect of invasion from the Red Army and the last legitimate prime minister of the Republic of Estonia (according to the Constitution of the Republic of Estonia) delivered a radio address asking all able-bodied men born from 1904 through 1923 to report for military service. The call resulted in around 38,000 new enlistments and several thousand Estonians who had joined the Finnish Army came back to join the newly formed Territorial Defense Force, assigned to defend Estonia against the Soviet advance. It was hoped[by whom?] that by engaging in such a war Estonia would be able to attract Western support for Estonian independence.
In the face of the country being re-occupied by the Red Army, tens of thousands of Estonians (including a majority of the education, culture, science, political and social specialists) chose to either retreat with the Germans or flee to Finland or Sweden where they sought refuge in other western countries, often by refugee ships such as the SS Walnut. On 12 January 1949, the Soviet Council of Ministers issued a decree "on the expulsion and deportation" from Baltic states of "all kulaks and their families, the families of bandits and nationalists", and others.
Half the deported perished, and the other half were not allowed to return until the early 1960s (years after Stalin's death).[citation needed] The activities of Soviet forces in 1940–41 and after reoccupation sparked a guerrilla war against Soviet authorities in Estonia by the Forest Brothers, who consisted mostly of Estonian veterans of the German and Finnish armies and some civilians. This conflict continued into the early 1950s. Material damage caused by the world war and the following Soviet era significantly slowed Estonia's economic growth, resulting in a wide wealth gap in comparison with neighbouring Finland and Sweden.
Militarization was another aspect of the Soviet state. Large parts of the country, especially the coastal areas, were closed to all but the Soviet military. Most of the sea shore and all sea islands (including Saaremaa and Hiiumaa) were declared "border zones". People not actually residing there were restricted from travelling to them without a permit. A notable closed military installation was the city of Paldiski, which was entirely closed to all public access. The city had a support base for the Soviet Baltic Fleet's submarines and several large military bases, including a nuclear submarine training centre complete with a full-scale model of a nuclear submarine with working nuclear reactors. The Paldiski reactors building passed into Estonian control in 1994 after the last Russian troops left the country. Immigration was another effect of Soviet occupation. Hundreds of thousands of migrants were relocated to Estonia from other parts of the Soviet Union to assist industrialisation and militarisation, contributing an increase of about half a million people within 45 years.
The U.S., UK, France, Italy and the majority of other Western countries considered the annexation of Estonia by the USSR illegal. They retained diplomatic relations with the representatives of the independent Republic of Estonia, never de jure recognised the existence of the Estonian SSR, and never recognised Estonia as a legal constituent part of the Soviet Union. Estonia's return to independence became possible as the Soviet Union faced internal regime challenges, loosening its hold on the outer empire. As the 1980s progressed, a movement for Estonian autonomy started. In the initial period of 1987–1989, this was partially for more economic independence, but as the Soviet Union weakened and it became increasingly obvious that nothing short of full independence would do, Estonia began a course towards self-determination.
In 1989, during the "Singing Revolution", in a landmark demonstration for more independence, more than two million people formed a human chain stretching through Lithuania, Latvia and Estonia, called the Baltic Way. All three nations had similar experiences of occupation and similar aspirations for regaining independence. The Estonian Sovereignty Declaration was issued on 16 November 1988. On 20 August 1991, Estonia declared formal independence during the Soviet military coup attempt in Moscow, reconstituting the pre-1940 state. The Soviet Union recognised the independence of Estonia on 6 September 1991. The first country to diplomatically recognise Estonia's reclaimed independence was Iceland. The last units of the Russian army left on 31 August 1994.
Estonia's land border with Latvia runs 267 kilometers; the Russian border runs 290 kilometers. From 1920 to 1945, Estonia's border with Russia, set by the 1920 Tartu Peace Treaty, extended beyond the Narva River in the northeast and beyond the town of Pechory (Petseri) in the southeast. This territory, amounting to some 2,300 square kilometres (888 sq mi), was incorporated into Russia by Stalin at the end of World War II. For this reason the borders between Estonia and Russia are still not defined.
Estonia lies on the eastern shores of the Baltic Sea immediately across the Gulf of Finland from Finland on the level northwestern part of the rising East European platform between 57.3° and 59.5° N and 21.5° and 28.1° E. Average elevation reaches only 50 metres (164 ft) and the country's highest point is the Suur Munamägi in the southeast at 318 metres (1,043 ft). There is 3,794 kilometres (2,357 mi) of coastline marked by numerous bays, straits, and inlets. The number of islands and islets is estimated at some 2,355 (including those in lakes). Two of them are large enough to constitute separate counties: Saaremaa and Hiiumaa. A small, recent cluster of meteorite craters, the largest of which is called Kaali is found on Saaremaa, Estonia.
Estonia is situated in the northern part of the temperate climate zone and in the transition zone between maritime and continental climate. Estonia has four seasons of near-equal length. Average temperatures range from 16.3 °C (61.3 °F) on the Baltic islands to 18.1 °C (64.6 °F) inland in July, the warmest month, and from −3.5 °C (25.7 °F) on the Baltic islands to −7.6 °C (18.3 °F) inland in February, the coldest month. The average annual temperature in Estonia is 5.2 °C (41.4 °F). The average precipitation in 1961–1990 ranged from 535 to 727 mm (21.1 to 28.6 in) per year.
A maakond (county) is the biggest administrative subdivision. The county government (Maavalitsus) of each county is led by a county governor (Maavanem), who represents the national government at the regional level. Governors are appointed by the Government of Estonia for a term of five years. Several changes were made to the borders of counties after Estonia became independent, most notably the formation of Valga County (from parts of Võru, Tartu and Viljandi counties) and Petseri County (area acquired from Russia with the 1920 Tartu Peace Treaty).
Estonia is a parliamentary representative democratic republic in which the Prime Minister of Estonia is the head of government and which includes a multi-party system. The political culture is stable in Estonia, where power is held between two and three parties that have been in politics for a long time. This situation is similar to other countries in Northern Europe. The former Prime Minister of Estonia, Andrus Ansip, is also Europe's longest-serving Prime Minister (from 2005 until 2014). The current Estonian Prime Minister is Taavi Rõivas, who is the former Minister of Social Affairs and the head of the Estonian Reform Party.
The Parliament of Estonia (Estonian: Riigikogu) or the legislative branch is elected by people for a four-year term by proportional representation. The Estonian political system operates under a framework laid out in the 1992 constitutional document. The Estonian parliament has 101 members and influences the governing of the state primarily by determining the income and the expenses of the state (establishing taxes and adopting the budget). At the same time the parliament has the right to present statements, declarations and appeals to the people of Estonia, ratify and denounce international treaties with other states and international organisations and decide on the Government loans.
The Riigikogu elects and appoints several high officials of the state, including the President of the Republic. In addition to that, the Riigikogu appoints, on the proposal of the President of Estonia, the Chairman of the National Court, the chairman of the board of the Bank of Estonia, the Auditor General, the Legal Chancellor and the Commander-in-Chief of the Defence Forces. A member of the Riigikogu has the right to demand explanations from the Government of the Republic and its members. This enables the members of the parliament to observe the activities of the executive power and the above-mentioned high officials of the state.
The Government of Estonia (Estonian: Vabariigi Valitsus) or the executive branch is formed by the Prime Minister of Estonia, nominated by the president and approved by the parliament. The government exercises executive power pursuant to the Constitution of Estonia and the laws of the Republic of Estonia and consists of twelve ministers, including the Prime Minister. The Prime Minister also has the right to appoint other ministers and assign them a subject to deal with. These are ministers without portfolio — they don't have a ministry to control.
The Prime Minister has the right to appoint a maximum of three such ministers, as the limit of ministers in one government is fifteen. It is also known as the cabinet. The cabinet carries out the country's domestic and foreign policy, shaped by parliament; it directs and co-ordinates the work of government institutions and bears full responsibility for everything occurring within the authority of executive power. The government, headed by the Prime Minister, thus represents the political leadership of the country and makes decisions in the name of the whole executive power.
Estonia has pursued the development of the e-state and e-government. Internet voting is used in elections in Estonia. The first internet voting took place in the 2005 local elections and the first in a parliamentary election was made available for the 2007 elections, in which 30,275 individuals voted over the internet. Voters have a chance to invalidate their electronic vote in traditional elections, if they wish to. In 2009 in its eighth Worldwide Press Freedom Index, Reporters Without Borders ranked Estonia sixth out of 175 countries. In the first ever State of World Liberty Index report, Estonia was ranked first out of 159 countries.
According to the Constitution of Estonia (Estonian: Põhiseadus) the supreme power of the state is vested in the people. The people exercise their supreme power of the state on the elections of the Riigikogu through citizens who have the right to vote. The supreme judicial power is vested in the Supreme Court or Riigikohus, with nineteen justices. The Chief Justice is appointed by the parliament for nine years on nomination by the president. The official Head of State is the President of Estonia, who gives assent to the laws passed by Riigikogu, also having the right of sending them back and proposing new laws.
Estonia was a member of the League of Nations from 22 September 1921, has been a member of the United Nations since 17 September 1991, and of NATO since 29 March 2004, as well as the European Union since 1 May 2004. Estonia is also a member of the Organization for Security and Cooperation in Europe (OSCE), Organisation for Economic Co-operation and Development (OECD), Council of the Baltic Sea States (CBSS) and the Nordic Investment Bank (NIB). As an OSCE participating State, Estonia's international commitments are subject to monitoring under the mandate of the U.S. Helsinki Commission. Estonia has also signed the Kyoto Protocol.
Since regaining independence, Estonia has pursued a foreign policy of close co-operation with its Western European partners. The two most important policy objectives in this regard have been accession into NATO and the European Union, achieved in March and May 2004 respectively. Estonia's international realignment toward the West has been accompanied by a general deterioration in relations with Russia, most recently demonstrated by the protest triggered by the controversial relocation of the Bronze Soldier World War II memorial in Tallinn.
Since the early 1990s, Estonia is involved in active trilateral Baltic states co-operation with Latvia and Lithuania, and Nordic-Baltic co-operation with the Nordic countries. The Baltic Council is the joint forum of the interparliamentary Baltic Assembly (BA) and the intergovernmental Baltic Council of Ministers (BCM). Nordic-Baltic Eight (NB-8) is the joint co-operation of the governments of Denmark, Estonia, Finland, Iceland, Latvia, Lithuania, Norway and Sweden. Nordic-Baltic Six (NB-6), comprising Nordic-Baltic countries that are European Union member states, is a framework for meetings on EU related issues. Parliamentary co-operation between the Baltic Assembly and Nordic Council began in 1989. Annual summits take place, and in addition meetings are organised on all possible levels: speakers, presidiums, commissions, and individual members. The Nordic Council of Ministers has an office in Tallinn with a subsidiary in Tartu and information points in Narva, Valga and Pärnu. Joint Nordic-Baltic projects include the education programme Nordplus and mobility programmes for business and industry and for public administration.
An important element in Estonia's post-independence reorientation has been closer ties with the Nordic countries, especially Finland and Sweden. Indeed, Estonians consider themselves a Nordic people rather than Balts, based on their historical ties with Sweden, Denmark and particularly Finland. In December 1999, then Estonian foreign minister (and since 2006, president of Estonia) Toomas Hendrik Ilves delivered a speech entitled "Estonia as a Nordic Country" to the Swedish Institute for International Affairs. In 2003, the foreign ministry also hosted an exhibit called "Estonia: Nordic with a Twist".
In 2005, Estonia joined the European Union's Nordic Battle Group. It has also shown continued interest in joining the Nordic Council. Whereas in 1992 Russia accounted for 92% of Estonia's international trade, today there is extensive economic interdependence between Estonia and its Nordic neighbours: three quarters of foreign investment in Estonia originates in the Nordic countries (principally Finland and Sweden), to which Estonia sends 42% of its exports (as compared to 6.5% going to Russia, 8.8% to Latvia, and 4.7% to Lithuania). On the other hand, the Estonian political system, its flat rate of income tax, and its non-welfare-state model distinguish it from the Nordic countries and their Nordic model, and indeed from many other European countries.
The military of Estonia is based upon the Estonian Defence Forces (Estonian: Kaitsevägi), which is the name of the unified armed forces of the republic with Maavägi (Army), Merevägi (Navy), Õhuvägi (Air Force) and a paramilitary national guard organisation Kaitseliit (Defence League). The Estonian National Defence Policy aim is to guarantee the preservation of the independence and sovereignty of the state, the integrity of its land, territorial waters, airspace and its constitutional order. Current strategic goals are to defend the country's interests, develop the armed forces for interoperability with other NATO and EU member forces, and participation in NATO missions.
Estonia co-operates with Latvia and Lithuania in several trilateral Baltic defence co-operation initiatives, including Baltic Battalion (BALTBAT), Baltic Naval Squadron (BALTRON), Baltic Air Surveillance Network (BALTNET) and joint military educational institutions such as the Baltic Defence College in Tartu. Future co-operation will include sharing of national infrastructures for training purposes and specialisation of training areas (BALTTRAIN) and collective formation of battalion-sized contingents for use in the NATO rapid-response force. In January 2011 the Baltic states were invited to join NORDEFCO, the defence framework of the Nordic countries.
The Ministry of Defence and the Defence Forces have been working on a cyberwarfare and defence formation for some years now. In 2007, a military doctrine of an e-military of Estonia was officially introduced as the country was under massive cyberattacks in 2007. The proposed aim of the e-military is to secure the vital infrastructure and e-infrastructure of Estonia. The main cyber warfare facility is the Computer Emergency Response Team of Estonia (CERT), founded in 2006. The organisation operates on security issues in local networks.
As a member of the European Union, Estonia is considered a high-income economy by the World Bank. The GDP (PPP) per capita of the country, a good indicator of wealth, was in 2015 $28,781 according to the IMF, between that of Slovak Republic and Lithuania, but below that of other long-time EU members such as Italy or Spain. The country is ranked 8th in the 2015 Index of Economic Freedom, and the 4th freest economy in Europe. Because of its rapid growth, Estonia has often been described as a Baltic Tiger beside Lithuania and Latvia. Beginning 1 January 2011, Estonia adopted the euro and became the 17th eurozone member state.
Estonia produces about 75% of its consumed electricity. In 2011 about 85% of it was generated with locally mined oil shale. Alternative energy sources such as wood, peat, and biomass make up approximately 9% of primary energy production. Renewable wind energy was about 6% of total consumption in 2009. Estonia imports petroleum products from western Europe and Russia. Oil shale energy, telecommunications, textiles, chemical products, banking, services, food and fishing, timber, shipbuilding, electronics, and transportation are key sectors of the economy. The ice-free port of Muuga, near Tallinn, is a modern facility featuring good transshipment capability, a high-capacity grain elevator, chill/frozen storage, and new oil tanker off-loading capabilities.[citation needed] The railroad serves as a conduit between the West, Russia, and other points to the East.[citation needed]
Because of the global economic recession that began in 2007, the GDP of Estonia decreased by 1.4% in the 2nd quarter of 2008, over 3% in the 3rd quarter of 2008, and over 9% in the 4th quarter of 2008. The Estonian government made a supplementary negative budget, which was passed by Riigikogu. The revenue of the budget was decreased for 2008 by EEK 6.1 billion and the expenditure by EEK 3.2 billion. In 2010, the economic situation stabilized and started a growth based on strong exports. In the fourth quarter of 2010, Estonian industrial output increased by 23% compared to the year before. The country has been experiencing economic growth ever since.
Since re-establishing independence, Estonia has styled itself as the gateway between East and West and aggressively pursued economic reform and integration with the West. Estonia's market reforms put it among the economic leaders in the former COMECON area.[citation needed] In 1994, based on the economic theories of Milton Friedman, Estonia became one of the first countries to adopt a flat tax, with a uniform rate of 26% regardless of personal income. In January 2005, the personal income tax rate was reduced to 24%. Another reduction to 23% followed in January 2006. The income tax rate was decreased to 21% by January 2008. The Government of Estonia finalised the design of Estonian euro coins in late 2004, and adopted the euro as the country's currency on 1 January 2011, later than planned due to continued high inflation. A Land Value Tax is levied which is used to fund local municipalities. It is a state level tax, however 100% of the revenue is used to fund Local Councils. The rate is set by the Local Council within the limits of 0.1–2.5%. It is one of the most important sources of funding for municipalities. The Land Value Tax is levied on the value of the land only with improvements and buildings not considered. Very few exemptions are considered on the land value tax and even public institutions are subject to the tax. The tax has contributed to a high rate (~90%) of owner-occupied residences within Estonia, compared to a rate of 67.4% in the United States.
In 1999, Estonia experienced its worst year economically since it regained independence in 1991, largely because of the impact of the 1998 Russian financial crisis.[citation needed] Estonia joined the WTO in November 1999. With assistance from the European Union, the World Bank and the Nordic Investment Bank, Estonia completed most of its preparations for European Union membership by the end of 2002 and now has one of the strongest economies of the new member states of the European Union.[citation needed] Estonia joined the OECD in 2010.
Estonia is a dependent country in the terms of energy and energy production. In recent years many local and foreign companies have been investing in renewable energy sources.[citation needed] The importance of wind power has been increasing steadily in Estonia and currently the total amount of energy production from wind is nearly 60 MW while at the same time roughly 399 MW worth of projects are currently being developed and more than 2800 MW worth of projects are being proposed in the Lake Peipus area and the coastal areas of Hiiumaa.
Estonia has had a market economy since the end of the 1990s and one of the highest per capita income levels in Eastern Europe. Proximity to the Scandinavian markets, its location between the East and West, competitive cost structure and a highly skilled labour force have been the major Estonian comparative advantages in the beginning of the 2000s (decade). As the largest city, Tallinn has emerged as a financial centre and the Tallinn Stock Exchange joined recently with the OMX system. The current government has pursued tight fiscal policies, resulting in balanced budgets and low public debt.
In 2007, however, a large current account deficit and rising inflation put pressure on Estonia's currency, which was pegged to the Euro, highlighting the need for growth in export-generating industries. Estonia exports mainly machinery and equipment, wood and paper, textiles, food products, furniture, and metals and chemical products. Estonia also exports 1.562 billion kilowatt hours of electricity annually. At the same time Estonia imports machinery and equipment, chemical products, textiles, food products and transportation equipment. Estonia imports 200 million kilowatt hours of electricity annually.
Between 2007 and 2013, Estonia receives 53.3 billion kroons (3.4 billion euros) from various European Union Structural Funds as direct supports by creating the largest foreign investments into Estonia ever. Majority of the European Union financial aid will be invested into to the following fields: energy economies, entrepreneurship, administrative capability, education, information society, environment protection, regional and local development, research and development activities, healthcare and welfare, transportation and labour market.
Between 1945 and 1989, the share of ethnic Estonians in the population resident within the currently defined boundaries of Estonia dropped to 61%, caused primarily by the Soviet programme promoting mass immigration of urban industrial workers from Russia, Ukraine, and Belarus, as well as by wartime emigration and Joseph Stalin's mass deportations and executions.[citation needed] By 1989, minorities constituted more than one-third of the population, as the number of non-Estonians had grown almost fivefold.
At the end of the 1980s, Estonians perceived their demographic change as a national catastrophe. This was a result of the migration policies essential to the Soviet Nationalisation Programme aiming to russify Estonia – administrative and military immigration of non-Estonians from the USSR coupled with the deportation of Estonians to the USSR. In the decade following the reconstitution of independence, large-scale emigration by ethnic Russians and the removal of the Russian military bases in 1994 caused the proportion of ethnic Estonians in Estonia to increase from 61% to 69% in 2006.
Modern Estonia is a fairly ethnically heterogeneous country, but this heterogeneity is not a feature of much of the country as the non-Estonian population is concentrated in two of Estonia's counties. Thirteen of Estonia's 15 counties are over 80% ethnic Estonian, the most homogeneous being Hiiumaa, where Estonians account for 98.4% of the population. In the counties of Harju (including the capital city, Tallinn) and Ida-Viru, however, ethnic Estonians make up 60% and 20% of the population, respectively. Russians make up 25.6% of the total population but account for 36% of the population in Harju county and 70% of the population in Ida-Viru county.
The Estonian Cultural Autonomy law that was passed in 1925 was unique in Europe at that time. Cultural autonomies could be granted to minorities numbering more than 3,000 people with longstanding ties to the Republic of Estonia. Before the Soviet occupation, the Germans and Jewish minorities managed to elect a cultural council. The Law on Cultural Autonomy for National Minorities was reinstated in 1993. Historically, large parts of Estonia's northwestern coast and islands have been populated by indigenous ethnically Rannarootslased (Coastal Swedes).
The 2008 United Nations Human Rights Council report called "extremely credible" the description of the citizenship policy of Estonia as "discriminatory". According to surveys, only 5% of the Russian community have considered returning to Russia in the near future. Estonian Russians have developed their own identity – more than half of the respondents recognised that Estonian Russians differ noticeably from the Russians in Russia. When comparing the result with a survey from 2000, then Russians' attitude toward the future is much more positive.
Estonia's constitution guarantees freedom of religion, separation of church and state, and individual rights to privacy of belief and religion. According to the Dentsu Communication Institute Inc, Estonia is one of the least religious countries in the world, with 75.7% of the population claiming to be irreligious. The Eurobarometer Poll 2005 found that only 16% of Estonians profess a belief in a god, the lowest belief of all countries studied (EU study). According to the Lutheran World Federation, the historic Lutheran denomination remains a large presence with 180,000 registered members.
Another major group, inhabitants who follow Eastern Orthodox Christianity, practised chiefly by the Russian minority, and the Russian Orthodox Church is the second largest denomination with 150,000 members. The Estonian Apostolic Orthodox Church, under the Greek-Orthodox Ecumenical Patriarchate, claims another 20,000 members. Thus, the number of adherents of Lutheranism and Orthodoxy, without regard to citizenship or ethnicity, is roughly equal. Refer to the Table below. The Catholics have their Latin Apostolic Administration of Estonia.
Although the Estonian and Germanic languages are of very different origins, one can identify many similar words in Estonian and German, for example. This is primarily because the Estonian language has borrowed nearly one third of its vocabulary from Germanic languages, mainly from Low Saxon (Middle Low German) during the period of German rule, and High German (including standard German). The percentage of Low Saxon and High German loanwords can be estimated at 22–25 percent, with Low Saxon making up about 15 percent.
Academic higher education in Estonia is divided into three levels: bachelor's, master's, and doctoral studies. In some specialties (basic medical studies, veterinary, pharmacy, dentistry, architect-engineer, and a classroom teacher programme) the bachelor's and master's levels are integrated into one unit. Estonian public universities have significantly more autonomy than applied higher education institutions. In addition to organising the academic life of the university, universities can create new curricula, establish admission terms and conditions, approve the budget, approve the development plan, elect the rector, and make restricted decisions in matters concerning assets. Estonia has a moderate number of public and private universities. The largest public universities are the University of Tartu, Tallinn University of Technology, Tallinn University, Estonian University of Life Sciences, Estonian Academy of Arts; the largest private university is Estonian Business School.
The Estonian Academy of Sciences is the national academy of science. The strongest public non-profit research institute that carries out fundamental and applied research is the National Institute of Chemical Physics and Biophysics (NICPB; Estonian KBFI). The first computer centres were established in the late 1950s in Tartu and Tallinn. Estonian specialists contributed in the development of software engineering standards for ministries of the Soviet Union during the 1980s. As of 2011[update], Estonia spends around 2.38% of its GDP on Research and Development, compared to an EU average of around 2.0%.
Today, Estonian society encourages liberty and liberalism, with popular commitment to the ideals of the limited government, discouraging centralised power and corruption. The Protestant work ethic remains a significant cultural staple, and free education is a highly prized institution. Like the mainstream culture in the other Nordic countries, Estonian culture can be seen to build upon the ascetic environmental realities and traditional livelihoods, a heritage of comparatively widespread egalitarianism out of practical reasons (see: Everyman's right and universal suffrage), and the ideals of closeness to nature and self-sufficiency (see: summer cottage).
The Estonian Academy of Arts (Estonian: Eesti Kunstiakadeemia, EKA) is providing higher education in art, design, architecture, media, art history and conservation while Viljandi Culture Academy of University of Tartu has an approach to popularise native culture through such curricula as native construction, native blacksmithing, native textile design, traditional handicraft and traditional music, but also jazz and church music. In 2010, there were 245 museums in Estonia whose combined collections contain more than 10 million objects.
The tradition of Estonian Song Festivals (Laulupidu) started at the height of the Estonian national awakening in 1869. Today, it is one of the largest amateur choral events in the world. In 2004, about 100,000 people participated in the Song Festival. Since 1928, the Tallinn Song Festival Grounds (Lauluväljak) have hosted the event every five years in July. The last festival took place in July 2014. In addition, Youth Song Festivals are also held every four or five years, the last of them in 2011, and the next is scheduled for 2017.
Estonia won the Eurovision Song Contest in 2001 with the song "Everybody" performed by Tanel Padar and Dave Benton. In 2002, Estonia hosted the event. Maarja-Liis Ilus has competed for Estonia on two occasions (1996 and 1997), while Eda-Ines Etti, Koit Toome and Evelin Samuel owe their popularity partly to the Eurovision Song Contest. Lenna Kuurmaa is a very popular singer in Europe[citation needed], with her band Vanilla Ninja. "Rändajad" by Urban Symphony, was the first ever song in Estonian to chart in the UK, Belgium, and Switzerland.
The Estonian literature refers to literature written in the Estonian language (ca. 1 million speakers). The domination of Estonia after the Northern Crusades, from the 13th century to 1918 by Germany, Sweden, and Russia resulted in few early written literary works in the Estonian language. The oldest records of written Estonian date from the 13th century. Originates Livoniae in Chronicle of Henry of Livonia contains Estonian place names, words and fragments of sentences. The Liber Census Daniae (1241) contains Estonian place and family names.
Oskar Luts was the most prominent prose writer of the early Estonian literature, who is still widely read today, especially his lyrical school novel Kevade (Spring). Anton Hansen Tammsaare's social epic and psychological realist pentalogy Truth and Justice captured the evolution of Estonian society from a peasant community to an independent nation. In modern times, Jaan Kross and Jaan Kaplinski are Estonia's best known and most translated writers. Among the most popular writers of the late 20th and early 21st centuries are Tõnu Õnnepalu and Andrus Kivirähk, who uses elements of Estonian folklore and mythology, deforming them into absurd and grotesque.
The architectural history of Estonia mainly reflects its contemporary development in northern Europe. Worth mentioning is especially the architectural ensemble that makes out the medieval old town of Tallinn, which is on the UNESCO World Heritage List. In addition, the country has several unique, more or less preserved hill forts dating from pre-Christian times, a large number of still intact medieval castles and churches, while the countryside is still shaped by the presence of a vast number of manor houses from earlier centuries.
Historically, the cuisine of Estonia has been heavily dependent on seasons and simple peasant food, which today is influenced by many countries. Today, it includes many typical international foods.[citation needed] The most typical foods in Estonia are black bread, pork, potatoes, and dairy products. Traditionally in summer and spring, Estonians like to eat everything fresh – berries, herbs, vegetables, and everything else that comes straight from the garden. Hunting and fishing have also been very common, although currently hunting and fishing are enjoyed mostly as hobbies. Today, it is also very popular to grill outside in summer.
Sport plays an important role in Estonian culture. After declaring independence from Russia in 1918, Estonia first competed as a nation at the 1920 Summer Olympics, although the National Olympic Committee was established in 1923. Estonian athletes took part of the Olympic Games until the country was annexed by the Soviet Union in 1940. The 1980 Summer Olympics Sailing regatta was held in the capital city Tallinn. After regaining independence in 1991, Estonia has participated in all Olympics. Estonia has won most of its medals in athletics, weightlifting, wrestling and cross-country skiing. Estonia has had very good success at the Olympic games given the country's small population. Estonia's best results were being ranked 13th in the medal table at the 1936 Summer Olympics, and 12th at the 2006 Winter Olympics.
Basketball is also a notable sport in Estonia. Estonia national basketball team previously participated in 1936 Summer Olympics, appeared in EuroBasket four times. Estonia national team also qualified to EuroBasket 2015, which will be held in Ukraine. BC Kalev/Cramo, which participates in EuroCup, is the most recent Korvpalli Meistriliiga winner after becoming champion of the league for the 6th time. Tartu Ülikool/Rock, which participates in EuroChallenge, is the second strongest Estonian basketball club, previously winning Korvpalli Meistriliiga 22 times. Six Estonian basketball clubs participates in Baltic Basketball League.
A great power is a sovereign state that is recognized as having the ability and expertise to exert its influence on a global scale. Great powers characteristically possess military and economic strength, as well as diplomatic and soft power influence, which may cause middle or small powers to consider the great powers' opinions before taking actions of their own. International relations theorists have posited that great power status can be characterized into power capabilities, spatial aspects, and status dimensions. Sometimes the status of great powers is formally recognized in conferences such as the Congress of Vienna or an international structure such as the United Nations Security Council (China, France, Russia, the United Kingdom and the United States serve as the body's five permanent members). At the same time the status of great powers can be informally recognized in a forum such as the G7 which consists of Canada, France, Germany, Italy, Japan, the United Kingdom and the United States of America.
The term "great power" was first used to represent the most important powers in Europe during the post-Napoleonic era. The "Great Powers" constituted the "Concert of Europe" and claimed the right to joint enforcement of the postwar treaties. The formalization of the division between small powers and great powers came about with the signing of the Treaty of Chaumont in 1814. Since then, the international balance of power has shifted numerous times, most dramatically during World War I and World War II. While some nations are widely considered to be great powers, there is no definitive list of them. In literature, alternative terms for great power are often world power or major power, but these terms can also be interchangeable with superpower.
Early writings on the subject tended to judge states by the realist criterion, as expressed by the historian A. J. P. Taylor when he noted that "The test of a great power is the test of strength for war." Later writers have expanded this test, attempting to define power in terms of overall military, economic, and political capacity. Kenneth Waltz, the founder of the neorealist theory of international relations, uses a set of five criteria to determine great power: population and territory; resource endowment; economic capability; political stability and competence; and military strength. These expanded criteria can be divided into three heads: power capabilities, spatial aspects, and status.
All states have a geographic scope of interests, actions, or projected power. This is a crucial factor in distinguishing a great power from a regional power; by definition the scope of a regional power is restricted to its region. It has been suggested that a great power should be possessed of actual influence throughout the scope of the prevailing international system. Arnold J. Toynbee, for example, observes that "Great power may be defined as a political force exerting an effect co-extensive with the widest range of the society in which it operates. The Great powers of 1914 were 'world-powers' because Western society had recently become 'world-wide'."
Other important criteria throughout history are that great powers should have enough influence to be included in discussions of political and diplomatic questions of the day, and have influence on the final outcome and resolution. Historically, when major political questions were addressed, several great powers met to discuss them. Before the era of groups like the United Nations, participants of such meetings were not officially named, but were decided based on their great power status. These were conferences which settled important questions based on major historical events. This might mean deciding the political resolution of various geographical and nationalist claims following a major conflict, or other contexts.
Lord Castlereagh, the British Foreign Secretary, first used the term in its diplomatic context, in a letter sent on February 13, 1814: "It affords me great satisfaction to acquaint you that there is every prospect of the Congress terminating with a general accord and Guarantee between the Great powers of Europe, with a determination to support the arrangement agreed upon, and to turn the general influence and if necessary the general arms against the Power that shall first attempt to disturb the Continental peace."
Of the five original great powers recognised at the Congress of Vienna, only France and the United Kingdom have maintained that status continuously to the present day, although France was defeated in the Franco-Prussian War and occupied during World War II. After the Congress of Vienna, the British Empire emerged as the pre-eminent power, due to its navy and the extent of its territories, which signalled the beginning of the Pax Britannica and of the Great Game between the UK and Russia. The balance of power between the Great Powers became a major influence in European politics, prompting Otto von Bismarck to say "All politics reduces itself to this formula: try to be one of three, as long as the world is governed by the unstable equilibrium of five great powers."
Over time, the relative power of these five nations fluctuated, which by the dawn of the 20th century had served to create an entirely different balance of power. Some, such as the United Kingdom and Prussia (as the founder of the newly formed German state), experienced continued economic growth and political power. Others, such as Russia and Austria-Hungary, stagnated. At the same time, other states were emerging and expanding in power, largely through the process of industrialization. These countries seeking to attain great power status were: Italy after the Risorgimento, Japan after the Meiji Restoration, and the United States after its civil war. By the dawn of the 20th century, the balance of world power had changed substantially since the Congress of Vienna. The Eight-Nation Alliance was a belligerent alliance of eight nations against the Boxer Rebellion in China. It formed in 1900 and consisted of the five Congress powers plus Italy, Japan, and the United States, representing the great powers at the beginning of 20th century.
Shifts of international power have most notably occurred through major conflicts. The conclusion of the Great War and the resulting treaties of Versailles, St-Germain, Neuilly, Trianon and Sèvres witnessed the United Kingdom, France, Italy, Japan and the United States as the chief arbiters of the new world order. In the aftermath of World War I the German Empire was defeated, the Austria-Hungarian empire was divided into new, less powerful states and the Russian Empire fell to a revolution. During the Paris Peace Conference, the "Big Four"—France, Italy, United Kingdom and the United States—held noticeably more power and influence on the proceedings and outcome of the treaties than Japan. The Big Four were leading architects of the Treaty of Versailles which was signed by Germany; the Treaty of St. Germain, with Austria; the Treaty of Neuilly, with Bulgaria; the Treaty of Trianon, with Hungary; and the Treaty of Sèvres, with the Ottoman Empire. During the decision-making of the Treaty of Versailles, Italy pulled out of the conference because a part of its demands were not met and temporarily left the other three countries as the sole major architects of that treaty, referred to as the "Big Three".
The victorious great powers also gained an acknowledgement of their status through permanent seats at the League of Nations Council, where they acted as a type of executive body directing the Assembly of the League. However, the Council began with only four permanent members—the United Kingdom, France, Italy, and Japan—because the United States, meant to be the fifth permanent member, left because the US Senate voted on 19 March 1920 against the ratification of the Treaty of Versailles, thus preventing American participation in the League.
When World War II started in 1939, it divided the world into two alliances—the Allies (the United Kingdom and France at first in Europe, China in Asia since 1937, followed in 1941 by the Soviet Union, the United States); and the Axis powers consisting of Germany, Italy and Japan.[nb 1] During World War II, the United States, United Kingdom, and Soviet Union controlled Allied policy and emerged as the "Big Three". The Republic of China and the Big Three were referred as a "trusteeship of the powerful"  and were recognized as the Allied "Big Four" in Declaration by United Nations in 1942. These four countries were referred as the "Four Policemen" of the Allies and considered as the primary victors of World War II. The importance of France was acknowledged by their inclusion, along with the other four, in the group of countries allotted permanent seats in the United Nations Security Council.
Since the end of the World Wars, the term "great power" has been joined by a number of other power classifications. Foremost among these is the concept of the superpower, used to describe those nations with overwhelming power and influence in the rest of the world. It was first coined in 1944 by William T.R. Fox and according to him, there were three superpowers: the British Empire, the United States, and the Soviet Union. But by the mid-1950s the British Empire lost its superpower status, leaving the United States and the Soviet Union as the world's superpowers.[nb 2] The term middle power has emerged for those nations which exercise a degree of global influence, but are insufficient to be decisive on international affairs. Regional powers are those whose influence is generally confined to their region of the world.
During the Cold War, the Asian power of Japan and the European powers of the United Kingdom, France, and West Germany rebuilt their economies. France and the United Kingdom maintained technologically advanced armed forces with power projection capabilities and maintain large defence budgets to this day. Yet, as the Cold War continued, authorities began to question if France and the United Kingdom could retain their long-held statuses as great powers. China, with the world's largest population, has slowly risen to great power status, with large growth in economic and military power in the post-war period. After 1949, the Republic of China began to lose its recognition as the sole legitimate government of China by the other great powers, in favour of the People's Republic of China. Subsequently, in 1971, it lost its permanent seat at the UN Security Council to the People's Republic of China.
According to Joshua Baron – a "researcher, lecturer, and consultant on international conflict" – since the early 1960s direct military conflicts and major confrontations have "receded into the background" with regards to relations among the great powers. Baron argues several reasons why this is the case, citing the unprecedented rise of the United States and its predominant position as the key reason. Baron highlights that since World War Two no other great power has been able to achieve parity or near parity with the United States, with the exception of the Soviet Union for a brief time. This position is unique among the great powers since the start of the modern era (the 16th century), where there has traditionally always been "tremendous parity among the great powers". This unique period of American primacy has been an important factor in maintaining a condition of peace between the great powers.
Another important factor is the apparent consensus among Western great powers that military force is no longer an effective tool of resolving disputes among their peers. This "subset" of great powers – France, Germany, Japan, the United Kingdom and the United States – consider maintaining a "state of peace" as desirable. As evidence, Baron outlines that since the Cuban missile crisis (1962) during the Cold War, these influential Western nations have resolved all disputes among the great powers peacefully at the United Nations and other forums of international discussion.
China, France, Russia, the United Kingdom and the United States are often referred to as great powers by academics due to "their political and economic dominance of the global arena". These five nations are the only states to have permanent seats with veto power on the UN Security Council. They are also the only recognized "Nuclear Weapons States" under the Nuclear Non-Proliferation Treaty, and maintain military expenditures which are among the largest in the world. However, there is no unanimous agreement among authorities as to the current status of these powers or what precisely defines a great power. For example, sources have at times referred to China, France, Russia and the United Kingdom as middle powers.
Japan and Germany are great powers too, though due to their large advanced economies (having the third and fourth largest economies respectively) rather than their strategic and hard power capabilities (i.e., the lack of permanent seats and veto power on the UN Security Council or strategic military reach). Germany has been a member together with the five permanent Security Council members in the P5+1 grouping of world powers. Like China, France, Russia and the United Kingdom; Germany and Japan have also been referred to as middle powers.
In addition to those contemporary great powers mentioned above, Zbigniew Brzezinski and Malik Mohan consider India to be a great power too. Although unlike the contemporary great powers who have long been considered so, India's recognition among authorities as a great power is comparatively recent. However, there is no collective agreement among observers as to the status of India, for example, a number of academics believe that India is emerging as a great power, while some believe that India remains a middle power.
Milena Sterio, American expert of international law, includes the former axis powers (Germany, Italy and Japan) and India among the great powers along with the permanent members of the UNSC. She considers Germany, Japan and Italy to be great powers due to their G7 membership and because of their influence in regional and international organizations.  Various authors describe Italy as an equal major power, while others view Italy as an "intermittent great power" or as "the least of the great powers".
With continuing European integration, the European Union is increasingly being seen as a great power in its own right, with representation at the WTO and at G8 and G-20 summits. This is most notable in areas where the European Union has exclusive competence (i.e. economic affairs). It also reflects a non-traditional conception of Europe's world role as a global "civilian power", exercising collective influence in the functional spheres of trade and diplomacy, as an alternative to military dominance. The European Union is a supranational union and not a sovereign state, and has limited scope in the areas of foreign affairs and defence policy. These remain largely with the member states of the European Union, which include the three great powers of France, Germany and the United Kingdom (referred to as the "EU three").
Referring to great power relations pre-1960, Joshua Baron highlights that starting from around the 16th century and the rise of several European great powers, military conflicts and confrontations was the defining characteristic of diplomacy and relations between such powers. "Between 1500 and 1953, there were 64 wars in which at least one great power was opposed to another, and they averaged little more than five years in length. In approximately a 450-year time frame, on average at least two great powers were fighting one another in each and every year." Even during the period of Pax Britannica (or "the British Peace") between 1815 and 1914, war and military confrontations among the great powers was still a frequent occurrence. In fact, Joshua Baron points out that, in terms of militarized conflicts or confrontations, the UK led the way in this period with nineteen such instances against; Russia (8), France (5), Germany/Prussia (5) and Italy (1).
The Royal Institute of British Architects (RIBA) is a professional body for architects primarily in the United Kingdom, but also internationally, founded for the advancement of architecture under its charter granted in 1837 and Supplemental Charter granted in 1971.
Originally named the Institute of British Architects in London, it was formed in 1834 by several prominent architects, including Philip Hardwick, Thomas Allom, William Donthorne, Thomas Leverton Donaldson, William Adams Nicholson, John Buonarotti Papworth, and Thomas de Grey, 2nd Earl de Grey.
After the grant of the royal charter it had become known as the Royal Institute of British Architects in London, eventually dropping the reference to London in 1892. In 1934, it moved to its current headquarters on Portland Place, with the building being opened by King George V and Queen Mary.
It was granted its Royal Charter in 1837 under King William IV. Supplemental Charters of 1887, 1909 and 1925 were replaced by a single Charter in 1971, and there have been minor amendments since then.
The original Charter of 1837 set out the purpose of the Royal Institute to be: '… the general advancement of Civil Architecture, and for promoting and facilitating the acquirement of the knowledge of the various arts and sciences connected therewith…'
The operational framework is provided by the Byelaws, which are more frequently updated than the Charter. Any revisions to the Charter or Byelaws require the Privy Council's approval. 
The design of the Institute's Mycenean lions medal and the motto ‘Usui civium, decori urbium' has been attributed to Thomas Leverton Donaldson, who had been honorary secretary until 1839. The RIBA Guide to its Archive and History (Angela Mace,1986) records that the first official version of this badge was used as a bookplate for the Institute's library and publications from 1835 to 1891, when it was redesigned by J.H.Metcalfe. It was again redesigned in 1931 by Eric Gill and in 1960 by Joan Hassall. The description in the 1837 by-laws was: "gules, two lions rampant guardant or, supporting a column marked with lines chevron, proper, all standing on a base of the same; a garter surrounding the whole with the inscription Institute of British Architects, anno salutis MDCCCXXXIV; above a mural crown proper, and beneath the motto Usui civium decori urbium ".
In the nineteenth and twentieth centuries the RIBA and its members had a leading part in the promotion of architectural education in the United Kingdom, including the establishment of the Architects' Registration Council of the United Kingdom (ARCUK) and the Board of Architectural Education under the Architects (Registration) Acts, 1931 to 1938. A member of the RIBA, Lionel Bailey Budden, then Associate Professor in the Liverpool University School of Architecture, had contributed the article on Architectural Education published in the fourteenth edition of the Encyclopædia Britannica (1929). His School, Liverpool, was one of the twenty schools named for the purpose of constituting the statutory Board of Architectural Education when the 1931 Act was passed.
Soon after the passing of the 1931 Act, in the book published on the occasion of the Institute's centenary celebration in 1934, Harry Barnes, FRIBA, Chairman of the Registration Committee, mentioned that ARCUK could not be a rival of any architectural association, least of all the RIBA, given the way ARCUK was constituted. Barnes commented that the Act's purpose was not protecting the architectural profession, and that the legitimate interests of the profession were best served by the (then) architectural associations in which some 80 per cent of those practising architecture were to be found.
The RIBA Guide to its Archive and History (1986) has a section on the "Statutory registration of architects" with a bibliography extending from a draft bill of 1887 to one of 1969. The Guide's section on "Education" records the setting up in 1904 of the RIBA Board of Architectural Education, and the system by which any school which applied for recognition, whose syllabus was approved by the Board and whose examinations were conducted by an approved external examiner, and whose standard of attainment was guaranteed by periodical inspections by a "Visiting Board" from the BAE, could be placed on the list of "recognized schools" and its successful students could qualify for exemption from RIBA examinations.
The content of the acts, particularly section 1 (1) of the amending act of 1938, shows the importance which was then attached to giving architects the responsibility of superintending or supervising the building works of local authorities (for housing and other projects), rather than persons professionally qualified only as municipal or other engineers. By the 1970s another issue had emerged affecting education for qualification and registration for practice as an architect, due to the obligation imposed on the United Kingdom and other European governments to comply with European Union Directives concerning mutual recognition of professional qualifications in favour of equal standards across borders, in furtherance of the policy for a single market of the European Union. This led to proposals for reconstituting ARCUK. Eventually, in the 1990s, before proceeding, the government issued a consultation paper "Reform of Architects Registration" (1994). The change of name to "Architects Registration Board" was one of the proposals which was later enacted in the Housing Grants, Construction and Regeneration Act 1996 and reenacted as the Architects Act 1997; another was the abolition of the ARCUK Board of Architectural Education.
RIBA Visiting Boards continue to assess courses for exemption from the RIBA's examinations in architecture. Under arrangements made in 2011 the validation criteria are jointly held by the RIBA and the Architects Registration Board, but unlike the ARB, the RIBA also validates courses outside the UK.
The RIBA is a member organisation, with 44,000 members. Chartered Members are entitled to call themselves chartered architects and to append the post-nominals RIBA after their name; Student Members are not permitted to do so. Formerly, fellowships of the institute were granted, although no longer; those who continue to hold this title instead add FRIBA.
RIBA is based at 66 Portland Place, London—a 1930s Grade II* listed building designed by architect George Grey Wornum with sculptures by Edward Bainbridge Copnall and James Woodford. Parts of the London building are open to the public, including the Library. It has a large architectural bookshop, a café, restaurant and lecture theatres. Rooms are hired out for events.
The Institute also maintains a dozen regional offices around the United Kingdom, it opened its first regional office for the East of England at Cambridge in 1966.
RIBA Enterprises is the commercial arm of RIBA, with a registered office in Newcastle upon Tyne, a base at 15 Bonhill Street in London, and an office in Newark. It employs over 250 staff, approximately 180 of whom are based in Newcastle.
Its services include RIBA Insight, RIBA Appointments, and RIBA Publishing. It publishes the RIBA Product Selector and RIBA Journal. In Newcastle is the NBS, the National Building Specification, which has 130 staff and deals with the building regulations and the Construction Information Service. RIBA Bookshops, which operates online and at 66 Portland Place, is also part of RIBA Enterprises.
The British Architectural Library, sometimes referred to as the RIBA Library, was established in 1834 upon the founding of the institute with donations from members. Now, with over four million items, it is one of the three largest architectural libraries in the world and the largest in Europe. Some items from the collections are on permanent display at the Victoria and Albert Museum (V&A) in the V&A + RIBA Architecture Gallery and included in temporary exhibitions at the RIBA and across Europe and North America. Its collections include:
The overcrowded conditions of the library was one of the reasons why the RIBA moved from 9 Conduit Street to larger premises at 66 Portland Place in 1934. The library remained open throughout World War Two and was able to shelter the archives of Modernist architect Adolf Loos during the war.
The library is based at two public sites: the Reading Room at the RIBA's headquarters, 66 Portland Place, London; and the RIBA Architecture Study Rooms in the Henry Cole Wing of the V&A. The Reading Room, designed by the building's architect George Grey Wornum and his wife Miriam, retains its original 1934 Art Deco interior with open bookshelves, original furniture and double-height central space. The study rooms, opened in 2004, were designed by Wright & Wright. The library is funded entirely by the RIBA but it is open to the public without charge. It operates a free education programme aimed at students, education groups and families, and an information service for RIBA members and the public through the RIBA Information Centre.
Since 2004, through the V&A + RIBA Architecture Partnership, the RIBA and V&A have worked together to promote the understanding and enjoyment of architecture.
In 2004, the two institutions created the Architecture Gallery (Room 128) at the V&A showing artefacts from the collections of both institutions, this was the first permanent gallery devoted to architecture in the UK. The adjacent Architecture Exhibition Space (Room 128a) is used for temporary displays related to architecture. Both spaces were designed by Gareth Hoskins Architects. At the same time the RIBA Library Drawing and Archives Collections moved from 21 Portman Place to new facilities in the Henry Cole Wing at the V&A. Under the Partnership new study rooms were opened where members of the public could view items from the RIBA and V&A architectural collections under the supervision of curatorial staff. These and the nearby education room were designed by Wright & Wright Architects.
RIBA runs many awards including the Stirling Prize for the best new building of the year, the Royal Gold Medal (first awarded in 1848), which honours a distinguished body of work, and the Stephen Lawrence Prize for projects with a construction budget of less than £500,000. The RIBA also awards the President's Medals for student work, which are regarded as the most prestigious awards in architectural education, and the RIBA President's Awards for Research. The RIBA European Award was inaugurated in 2005 for work in the European Union, outside the UK. The RIBA National Award and the RIBA International Award were established in 2007. Since 1966, the RIBA also judges regional awards which are presented locally in the UK regions (East, East Midlands, London, North East, North West, Northern Ireland, Scotland, South/South East, South West/Wessex, Wales, West Midlands and Yorkshire).
Architectural design competitions are used by an organisation that plans to build a new building or refurbish an existing building. They can be used for buildings, engineering work, structures, landscape design projects or public realm artworks. A competition typically asks for architects and/or designers to submit a design proposal in response to a given Brief. The winning design will then be selected by an independent jury panel of design professionals and client representatives. The independence of the jury is vital to the fair conduct of a competition.
In addition to the Architects Registration Board, the RIBA provides accreditation to architecture schools in the UK under a course validation procedure. It also provides validation to international courses without input from the ARB.
The RIBA has three parts to the education process: Part I which is generally a three-year first degree, a year-out of at least one year work experience in an architectural practice precedes the Part II which is generally a two-year post graduate diploma or masters. A further year out must be taken before the RIBA Part III professional exams can be taken. Overall it takes a minimum of seven years before an architecture student can seek chartered status.
In 2007, RIBA called for minimum space standards in newly built British houses after research was published suggesting that British houses were falling behind other European countries. "The average new home sold to people today is significantly smaller than that built in the 1920s... We're way behind the rest of Europe—even densely populated Holland has better proportioned houses than are being built in the country. So let's see minimum space standards for all new homes," said RIBA president Jack Pringle.
Appointments to the Order of the British Empire were at first made on the nomination of the self-governing Dominions of the Empire, the Viceroy of India, and the colonial governors, as well as on nominations from within the United Kingdom. As the Empire evolved into the Commonwealth, nominations continued to come from the Commonwealth realms, in which the monarch remained head of state. These overseas nominations have been discontinued in realms that have established their own Orders—such as the Order of Australia, the Order of Canada, and the New Zealand Order of Merit—but members of the Order are still appointed in the British Overseas Territories.
Any individual made a member of the Order for gallantry could wear an emblem of two crossed silver oak leaves on the same riband, ribbon or bow as the badge. It could not be awarded posthumously and was effectively replaced in 1974 with the Queen's Gallantry Medal. If recipients of the Order of the British Empire for Gallantry received promotion within the Order, whether for gallantry or otherwise, they continued to wear also the insignia of the lower grade with the oak leaves. However, they only used the post-nominal letters of the higher grade.
Honorary knighthoods are appointed to citizens of nations where Queen Elizabeth II is not Head of State, and may permit use of post-nominal letters but not the title of Sir or Dame. Occasionally honorary appointees are, incorrectly, referred to as Sir or Dame - Bill Gates or Bob Geldof, for example. Honorary appointees who later become a citizen of a Commonwealth realm can convert their appointment from honorary to substantive, then enjoy all privileges of membership of the order including use of the title of Sir and Dame for the senior two ranks of the Order. An example is Irish broadcaster Terry Wogan, who was appointed an honorary Knight Commander of the Order in 2005 and on successful application for dual British and Irish citizenship was made a substantive member and subsequently styled as "Sir Terry Wogan KBE".
The Order has six officials: the Prelate; the Dean; the Secretary; the Registrar; the King of Arms; and the Usher. The Bishop of London, a senior bishop in the Church of England, serves as the Order's Prelate. The Dean of St Paul's is ex officio the Dean of the Order. The Order's King of Arms is not a member of the College of Arms, as are many other heraldic officers. The Usher of the Order is known as the Gentleman Usher of the Purple Rod; he does not – unlike his Order of the Garter equivalent, the Gentleman Usher of the Black Rod – perform any duties related to the House of Lords.
Appointments to the Order of the British Empire were discontinued in those Commonwealth realms that established a national system of honours and awards such as the Order of Australia, the Order of Canada, and the New Zealand Order of Merit. In many of these systems, the different levels of award and honour reflect the Imperial system they replaced. Canada, Australia, and New Zealand all have (in increasing level of precedence) Members of, Officers of, and Companions to (rather than Commanders of) their respective orders, with both Australia and New Zealand having Knights and Dames as their highest classes.
The members of The Beatles were made MBEs in 1965. John Lennon justified the comparative merits of his investiture by comparing military membership in the Order: "Lots of people who complained about us receiving the MBE [status] received theirs for heroism in the war – for killing people… We received ours for entertaining other people. I'd say we deserve ours more." Lennon later returned his MBE insignia on 25 November 1969 as part of his ongoing peace protests. Other criticism centres on the claim that many recipients of the Order are being rewarded with honours for simply doing their jobs; critics claim that the civil service and judiciary receive far more orders and honours than leaders of other professions.
The Most Excellent Order of the British Empire is the "order of chivalry of British constitutional monarchy", rewarding contributions to the arts and sciences, work with charitable and welfare organisations and public service outside the Civil Service. It was established on 4 June 1917 by King George V, and comprises five classes, in civil and military divisions, the most senior two of which make the recipient either a knight if male, or dame if female. There is also the related British Empire Medal, whose recipients are affiliated with, but not members of, the order.
At the foundation of the Order, the "Medal of the Order of the British Empire" was instituted, to serve as a lower award granting recipients affiliation but not membership. In 1922, this was renamed the "British Empire Medal". It stopped being awarded by the United Kingdom as part of the 1993 reforms to the honours system, but was again awarded beginning in 2012, starting with 293 BEMs awarded for the Queen's Diamond Jubilee. In addition, the BEM is awarded by the Cook Islands and by some other Commonwealth nations. In 2004, a report entitled "A Matter of Honour: Reforming Our Honours System" by a Commons committee recommended to phase out the Order of the British Empire, as its title was "now considered to be unacceptable, being thought to embody values that are no longer shared by many of the country’s population".
From 1940, the Sovereign could appoint a person as a Commander, Officer or Member of the Order of the British Empire for gallantry for acts of bravery (not in the face of the enemy) below the level required for the George Medal. The grade was determined by the same criteria as usual, and not by the level of gallantry (and with more junior people instead receiving the British Empire Medal). Oddly, this meant that it was awarded for lesser acts of gallantry than the George Medal, but, as an Order, was worn before it and listed before it in post-nominal initials. From 14 January 1958, these awards were designated the Order of the British Empire for Gallantry.
Knights Grand Cross and Knights Commander prefix Sir, and Dames Grand Cross and Dames Commander prefix Dame, to their forenames.[b] Wives of Knights may prefix Lady to their surnames, but no equivalent privilege exists for husbands of Knights or spouses of Dames. Such forms are not used by peers and princes, except when the names of the former are written out in their fullest forms. Clergy of the Church of England or the Church of Scotland do not use the title Sir or Dame as they do not receive the accolade (i.e., they are not dubbed "knight" with a sword), although they do append the post-nominal letters.
India, while remaining an active member of the Commonwealth, chose as a republic to institute its own set of honours awarded by the President of India who holds a republican position some consider similar to that of the monarch in Britain. These are commonly referred to as the Padma Awards and consist of Padma Vibhushan, Padma Bhushan and Padma Shri in descending order. These do not carry any decoration or insignia that can be worn on the person and may not be used as titles along with individuals' names.
The Order is limited to 300 Knights and Dames Grand Cross, 845 Knights and Dames Commander, and 8,960 Commanders. There are no limits applied to the total number of members of the fourth and fifth classes, but no more than 858 Officers and 1,464 Members may be appointed per year. Foreign recipients, as honorary members, do not contribute to the numbers restricted to the Order as full members do. Although the Order of the British Empire has by far the highest number of members of the British Orders of Chivalry, with over 100,000 living members worldwide, there are fewer appointments to knighthoods than in other orders.
Members of all classes of the Order are assigned positions in the order of precedence. Wives of male members of all classes also feature on the order of precedence, as do sons, daughters and daughters-in-law of Knights Grand Cross and Knights Commander; relatives of Ladies of the Order, however, are not assigned any special precedence. As a general rule, individuals can derive precedence from their fathers or husbands, but not from their mothers or wives (see order of precedence in England and Wales for the exact positions).
The territory that now constitutes Tajikistan was previously home to several ancient cultures, including the city of Sarazm of the Neolithic and the Bronze Age, and was later home to kingdoms ruled by people of different faiths and cultures, including the Oxus civilization, Andronovo culture, Buddhism, Nestorian Christianity, Zoroastrianism, and Manichaeism. The area has been ruled by numerous empires and dynasties, including the Achaemenid Empire, Sassanian Empire, Hephthalite Empire, Samanid Empire, Mongol Empire, Timurid dynasty, and the Russian Empire. As a result of the breakup of the Soviet Union, Tajikistan became an independent nation in 1991. A civil war was fought almost immediately after independence, lasting from 1992 to 1997. Since the end of the war, newly established political stability and foreign aid have allowed the country's economy to grow.
Tajiks began to be conscripted into the Soviet Army in 1939 and during World War II around 260,000 Tajik citizens fought against Germany, Finland and Japan. Between 60,000(4%) and 120,000(8%) of Tajikistan's 1,530,000 citizens were killed during World War II. Following the war and Stalin's reign attempts were made to further expand the agriculture and industry of Tajikistan. During 1957–58 Nikita Khrushchev's Virgin Lands Campaign focused attention on Tajikistan, where living conditions, education and industry lagged behind the other Soviet Republics. In the 1980s, Tajikistan had the lowest household saving rate in the USSR, the lowest percentage of households in the two top per capita income groups, and the lowest rate of university graduates per 1000 people. By the late 1980s Tajik nationalists were calling for increased rights. Real disturbances did not occur within the republic until 1990. The following year, the Soviet Union collapsed, and Tajikistan declared its independence.
The nation almost immediately fell into civil war that involved various factions fighting one another; these factions were often distinguished by clan loyalties. More than 500,000 residents fled during this time because of persecution, increased poverty and better economic opportunities in the West or in other former Soviet republics. Emomali Rahmon came to power in 1992, defeating former prime minister Abdumalik Abdullajanov in a November presidential election with 58% of the vote. The elections took place shortly after the end of the war, and Tajikistan was in a state of complete devastation. The estimated dead numbered over 100,000. Around 1.2 million people were refugees inside and outside of the country. In 1997, a ceasefire was reached between Rahmon and opposition parties under the guidance of Gerd D. Merrem, Special Representative to the Secretary General, a result widely praised as a successful United Nations peace keeping initiative. The ceasefire guaranteed 30% of ministerial positions would go to the opposition. Elections were held in 1999, though they were criticized by opposition parties and foreign observers as unfair and Rahmon was re-elected with 98% of the vote. Elections in 2006 were again won by Rahmon (with 79% of the vote) and he began his third term in office. Several opposition parties boycotted the 2006 election and the Organization for Security and Cooperation in Europe (OSCE) criticized it, although observers from the Commonwealth of Independent States claimed the elections were legal and transparent. Rahmon's administration came under further criticism from the OSCE in October 2010 for its censorship and repression of the media. The OSCE claimed that the Tajik Government censored Tajik and foreign websites and instituted tax inspections on independent printing houses that led to the cessation of printing activities for a number of independent newspapers.
Russian border troops were stationed along the Tajik–Afghan border until summer 2005. Since the September 11, 2001 attacks, French troops have been stationed at the Dushanbe Airport in support of air operations of NATO's International Security Assistance Force in Afghanistan. United States Army and Marine Corps personnel periodically visit Tajikistan to conduct joint training missions of up to several weeks duration. The Government of India rebuilt the Ayni Air Base, a military airport located 15 km southwest of Dushanbe, at a cost of $70 million, completing the repairs in September 2010. It is now the main base of the Tajikistan air force. There have been talks with Russia concerning use of the Ayni facility, and Russia continues to maintain a large base on the outskirts of Dushanbe.
In 2010, there were concerns among Tajik officials that Islamic militarism in the east of the country was on the rise following the escape of 25 militants from a Tajik prison in August, an ambush that killed 28 Tajik soldiers in the Rasht Valley in September, and another ambush in the valley in October that killed 30 soldiers, followed by fighting outside Gharm that left 3 militants dead. To date the country's Interior Ministry asserts that the central government maintains full control over the country's east, and the military operation in the Rasht Valley was concluded in November 2010. However, fighting erupted again in July 2012. In 2015 Russia will send more troops to Tajikistan, as confirmed by a report of STRATFOR (magazine online)
Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system. It is, however, a dominant-party system, where the People's Democratic Party of Tajikistan routinely has a vast majority in Parliament. Emomalii Rahmon has held the office of President of Tajikistan continually since November 1994. The Prime Minister is Kokhir Rasulzoda, the First Deputy Prime Minister is Matlubkhon Davlatov and the two Deputy Prime Ministers are Murodali Alimardon and Ruqiya Qurbanova.
Freedom of the press is ostensibly officially guaranteed by the government, but independent press outlets remain restricted, as does a substantial amount of web content. According to the Institute for War & Peace Reporting, access is blocked to local and foreign websites including avesta.tj, Tjknews.com, ferghana.ru, centrasia.ru and journalists are often obstructed from reporting on controversial events. In practice, no public criticism of the regime is tolerated and all direct protest is severely suppressed and does not receive coverage in the local media.
Tajikistan is landlocked, and is the smallest nation in Central Asia by area. It lies mostly between latitudes 36° and 41° N (a small area is north of 41°), and longitudes 67° and 75° E (a small area is east of 75°). It is covered by mountains of the Pamir range, and more than fifty percent of the country is over 3,000 meters (9,800 ft) above sea level. The only major areas of lower land are in the north (part of the Fergana Valley), and in the southern Kofarnihon and Vakhsh river valleys, which form the Amu Darya. Dushanbe is located on the southern slopes above the Kofarnihon valley.
Tajikistan means the "Land of the Tajiks". The suffix "-stan" (Persian: ـستان‎‎ -stān) is Persian for "place of" or "country" and Tajik is, most likely, the name of a pre-Islamic (before the seventh century A.D.) tribe. According to the Library of Congress's 1997 Country Study of Tajikistan, it is difficult to definitively state the origins of the word "Tajik" because the term is "embroiled in twentieth-century political disputes about whether Turkic or Iranian peoples were the original inhabitants of Central Asia."
It was temporarily under the control of the Tibetan empire and Chinese from 650–680 and then under the control of the Umayyads in 710. The Samanid Empire, 819 to 999, restored Persian control of the region and enlarged the cities of Samarkand and Bukhara (both cities are today part of Uzbekistan) which became the cultural centers of Iran and the region was known as Khorasan. The Kara-Khanid Khanate conquered Transoxania (which corresponds approximately with modern-day Uzbekistan, Tajikistan, southern Kyrgyzstan and southwest Kazakhstan) and ruled between 999–1211. Their arrival in Transoxania signaled a definitive shift from Iranian to Turkic predominance in Central Asia, but gradually the Kara-khanids became assimilated into the Perso-Arab Muslim culture of the region.
Russian Imperialism led to the Russian Empire's conquest of Central Asia during the late 19th century's Imperial Era. Between 1864 and 1885 Russia gradually took control of the entire territory of Russian Turkestan, the Tajikistan portion of which had been controlled by the Emirate of Bukhara and Khanate of Kokand. Russia was interested in gaining access to a supply of cotton and in the 1870s attempted to switch cultivation in the region from grain to cotton (a strategy later copied and expanded by the Soviets).[citation needed] By 1885 Tajikistan's territory was either ruled by the Russian Empire or its vassal state, the Emirate of Bukhara, nevertheless Tajiks felt little Russian influence.[citation needed]
During the late 19th Century the Jadidists established themselves as an Islamic social movement throughout the region. Although the Jadidists were pro-modernization and not necessarily anti-Russian the Russians viewed the movement as a threat.[citation needed] Russian troops were required to restore order during uprisings against the Khanate of Kokand between 1910 and 1913. Further violence occurred in July 1916 when demonstrators attacked Russian soldiers in Khujand over the threat of forced conscription during World War I. Despite Russian troops quickly bringing Khujand back under control, clashes continued throughout the year in various locations in Tajikistan.[citation needed]
After the Russian Revolution of 1917 guerrillas throughout Central Asia, known as basmachi, waged a war against Bolshevik armies in a futile attempt to maintain independence. The Bolsheviks prevailed after a four-year war, in which mosques and villages were burned down and the population heavily suppressed. Soviet authorities started a campaign of secularization, practicing Islam, Judaism, and Christianity was discouraged and repressed, and many mosques, churches, and synagogues were closed. As a consequence of the conflict and Soviet agriculture policies, Central Asia, Tajikistan included, suffered a famine that claimed many lives.
In 1924, the Tajik Autonomous Soviet Socialist Republic was created as a part of Uzbekistan, but in 1929 the Tajik Soviet Socialist Republic (Tajik SSR) was made a separate constituent republic, however the predominantly ethnic Tajik cities of Samarkand and Bukhara remained in the Uzbek SSR. Between 1927 and 1934, collectivization of agriculture and a rapid expansion of cotton production took place, especially in the southern region. Soviet collectivization policy brought violence against peasants and forced resettlement occurred throughout Tajikistan. Consequently, some peasants fought collectivization and revived the Basmachi movement. Some small scale industrial development also occurred during this time along with the expansion of irrigation infrastructure.
Two rounds of Soviet purges directed by Moscow (1927–1934 and 1937–1938) resulted in the expulsion of nearly 10,000 people, from all levels of the Communist Party of Tajikistan. Ethnic Russians were sent in to replace those expelled and subsequently Russians dominated party positions at all levels, including the top position of first secretary. Between 1926 and 1959 the proportion of Russians among Tajikistan's population grew from less than 1% to 13%. Bobojon Ghafurov, Tajikistan's First Secretary of the Communist Party of Tajikistan from 1946–1956 was the only Tajikistani politician of significance outside of the country during the Soviet Era. He was followed in office by Tursun Uljabayev (1956–61), Jabbor Rasulov (1961–1982), and Rahmon Nabiyev (1982–1985, 1991–1992).
The parliamentary elections of 2005 aroused many accusations from opposition parties and international observers that President Emomalii Rahmon corruptly manipulates the election process and unemployment. The most recent elections, in February 2010, saw the ruling PDPT lose four seats in Parliament, yet still maintain a comfortable majority. The Organization for Security and Co-operation in Europe election observers said the 2010 polling "failed to meet many key OSCE commitments" and that "these elections failed on many basic democratic standards." The government insisted that only minor violations had occurred, which would not affect the will of the Tajik people.
Tajikistan (i/tɑːˈdʒiːkᵻstɑːn/, /təˈdʒiːkᵻstæn/, or /tæˈdʒiːkiːstæn/; Persian: تاجيكستان‎‎ Тоҷикистон [tɔd͡ʒikɪsˈtɔn]), officially the Republic of Tajikistan (Persian: جمهورى تاجيكستان‎‎ Tajik: Ҷумҳурии Тоҷикистон, Çumhuriji Toçikiston/Jumhuriyi Tojikiston; Russian: Респу́блика Таджикистан, Respublika Tadzhikistan), is a mountainous, landlocked country in Central Asia with an estimated 8 million people in 2013, and an area of 143,100 km2 (55,300 sq mi). It is bordered by Afghanistan to the south, Uzbekistan to the west, Kyrgyzstan to the north, and China to the east. Pakistan lies to the south, separated by the narrow Wakhan Corridor. Traditional homelands of Tajik people included present-day Tajikistan, Afghanistan and Uzbekistan.
The earliest recorded history of the region dates back to about 500 BCE when much, if not all, of modern Tajikistan was part of the Achaemenid Empire. Some authors have also suggested that in the 7th and 6th century BCE parts of modern Tajikistan, including territories in the Zeravshan valley, formed part of Kambojas before it became part of the Achaemenid Empire. After the region's conquest by Alexander the Great it became part of the Greco-Bactrian Kingdom, a successor state of Alexander's empire. Northern Tajikistan (the cities of Khujand and Panjakent) was part of Sogdia, a collection of city-states which was overrun by Scythians and Yuezhi nomadic tribes around 150 BCE. The Silk Road passed through the region and following the expedition of Chinese explorer Zhang Qian during the reign of Wudi (141–87 BCE) commercial relations between Han China and Sogdiana flourished. Sogdians played a major role in facilitating trade and also worked in other capacities, as farmers, carpetweavers, glassmakers, and woodcarvers.
The Kushan Empire, a collection of Yuezhi tribes, took control of the region in the first century CE and ruled until the 4th century CE during which time Buddhism, Nestorian Christianity, Zoroastrianism, and Manichaeism were all practiced in the region. Later the Hephthalite Empire, a collection of nomadic tribes, moved into the region and Arabs brought Islam in the early eighth century. Central Asia continued in its role as a commercial crossroads, linking China, the steppes to the north, and the Islamic heartland.
Tajikistan's economy grew substantially after the war. The GDP of Tajikistan expanded at an average rate of 9.6% over the period of 2000–2007 according to the World Bank data. This improved Tajikistan's position among other Central Asian countries (namely Turkmenistan and Uzbekistan), which seem to have degraded economically ever since. The primary sources of income in Tajikistan are aluminium production, cotton growing and remittances from migrant workers. Cotton accounts for 60% of agricultural output, supporting 75% of the rural population, and using 45% of irrigated arable land. The aluminium industry is represented by the state-owned Tajik Aluminum Company – the biggest aluminium plant in Central Asia and one of the biggest in the world.
Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.
According to some estimates about 20% of the population lives on less than US$1.25 per day. Migration from Tajikistan and the consequent remittances have been unprecedented in their magnitude and economic impact. In 2010, remittances from Tajik labour migrants totaled an estimated $2.1 billion US dollars, an increase from 2009. Tajikistan has achieved transition from a planned to a market economy without substantial and protracted recourse to aid (of which it by now receives only negligible amounts), and by purely market-based means, simply by exporting its main commodity of comparative advantage — cheap labor. The World Bank Tajikistan Policy Note 2006 concludes that remittances have played an important role as one of the drivers of Tajikistan's robust economic growth during the past several years, have increased incomes, and as a result helped significantly reduce poverty.
Drug trafficking is the major illegal source of income in Tajikistan as it is an important transit country for Afghan narcotics bound for Russian and, to a lesser extent, Western European markets; some opium poppy is also raised locally for the domestic market. However, with the increasing assistance from international organizations, such as UNODC, and cooperation with the US, Russian, EU and Afghan authorities a level of progress on the fight against illegal drug-trafficking is being achieved. Tajikistan holds third place in the world for heroin and raw opium confiscations (1216.3 kg of heroin and 267.8 kg of raw opium in the first half of 2006). Drug money corrupts the country's government; according to some experts the well-known personalities that fought on both sides of the civil war and have held the positions in the government after the armistice was signed are now involved in the drug trade. UNODC is working with Tajikistan to strengthen border crossings, provide training, and set up joint interdiction teams. It also helped to establish Tajikistani Drug Control Agency.
As a landlocked country Tajikistan has no ports and the majority of transportation is via roads, air, and rail. In recent years Tajikistan has pursued agreements with Iran and Pakistan to gain port access in those countries via Afghanistan. In 2009, an agreement was made between Tajikistan, Pakistan, and Afghanistan to improve and build a 1,300 km (810 mi) highway and rail system connecting the three countries to Pakistan's ports. The proposed route would go through the Gorno-Badakhshan Autonomous Province in the eastern part of the country. And in 2012, the presidents of Tajikistan, Afghanistan, and Iran signed an agreement to construct roads and railways as well as oil, gas, and water pipelines to connect the three countries.
In 2009 Tajikistan had 26 airports, 18 of which had paved runways, of which two had runways longer than 3,000 meters. The country's main airport is Dushanbe International Airport which as of April 2015, had regularly scheduled flights to major cities in Russia, Central Asia, as well as Delhi, Dubai, Frankfurt, Istanbul, Kabul, Tehran, and Ürümqi amongst others. There are also international flights, mainly to Russia, from Khujand Airport in the northern part of the country as well as limited international services from Kulob Airport, and Qurghonteppa International Airport. Khorog Airport is a domestic airport and also the only airport in the sparsely populated eastern half of the country.
Tajikistan has a population of 7,349,145 (July 2009 est.) of which 70% are under the age of 30 and 35% are between the ages of 14 and 30. Tajiks who speak Tajik (a dialect of Persian) are the main ethnic group, although there are sizable minorities of Uzbeks and Russians, whose numbers are declining due to emigration. The Pamiris of Badakhshan, a small population of Yaghnobi people, and a sizeable minority of Ismailis are all considered to belong to the larger group of Tajiks. All citizens of Tajikistan are called Tajikistanis.
The Pamiri people of Gorno-Badakhshan Autonomous Province in the southeast, bordering Afghanistan and China, though considered part of the Tajik ethnicity, nevertheless are distinct linguistically and culturally from most Tajiks. In contrast to the mostly Sunni Muslim residents of the rest of Tajikistan, the Pamiris overwhelmingly follow the Ismaili sect of Islam, and speak a number of Eastern Iranian languages, including Shughni, Rushani, Khufi and Wakhi. Isolated in the highest parts of the Pamir Mountains, they have preserved many ancient cultural traditions and folk arts that have been largely lost elsewhere in the country.
Sunni Islam of the Hanafi school has been officially recognized by the government since 2009. Tajikistan considers itself a secular state with a Constitution providing for freedom of religion. The Government has declared two Islamic holidays, Id Al-Fitr and Idi Qurbon, as state holidays. According to a U.S. State Department release and Pew research group, the population of Tajikistan is 98% Muslim. Approximately 87%–95% of them are Sunni and roughly 3% are Shia and roughly 7% are non-denominational Muslims. The remaining 2% of the population are followers of Russian Orthodoxy, Protestantism, Zoroastrianism and Buddhism. A great majority of Muslims fast during Ramadan, although only about one third in the countryside and 10% in the cities observe daily prayer and dietary restrictions.
Relationships between religious groups are generally amicable, although there is some concern among mainstream Muslim leaders[who?] that minority religious groups undermine national unity. There is a concern for religious institutions becoming active in the political sphere. The Islamic Renaissance Party (IRP), a major combatant in the 1992–1997 Civil War and then-proponent of the creation of an Islamic state in Tajikistan, constitutes no more than 30% of the government by statute. Membership in Hizb ut-Tahrir, a militant Islamic party which today aims for an overthrow of secular governments and the unification of Tajiks under one Islamic state, is illegal and members are subject to arrest and imprisonment. Numbers of large mosques appropriate for Friday prayers are limited and some[who?] feel this is discriminatory.
By law, religious communities must register by the State Committee on Religious Affairs (SCRA) and with local authorities. Registration with the SCRA requires a charter, a list of 10 or more members, and evidence of local government approval prayer site location. Religious groups who do not have a physical structure are not allowed to gather publicly for prayer. Failure to register can result in large fines and closure of place of worship. There are reports that registration on the local level is sometimes difficult to obtain. People under the age of 18 are also barred from public religious practice.
Despite repeated efforts by the Tajik government to improve and expand health care, the system remains extremely underdeveloped and poor, with severe shortages of medical supplies. The state's Ministry of Labor and Social Welfare reported that 104,272 disabled people are registered in Tajikistan (2000). This group of people suffers most from poverty in Tajikistan. The government of Tajikistan and the World Bank considered activities to support this part of the population described in the World Bank's Poverty Reduction Strategy Paper. Public expenditure on health was at 1% of the GDP in 2004.
Public education in Tajikistan consists of 11 years of primary and secondary education but the government has plans to implement a 12-year system in 2016. There is a relatively large number of tertiary education institutions including Khujand State University which has 76 departments in 15 faculties, Tajikistan State University of Law, Business, & Politics, Khorugh State University, Agricultural University of Tajikistan, Tajik State National University, and several other institutions. Most, but not all, universities were established during the Soviet Era. As of 2008[update] tertiary education enrollment was 17%, significantly below the sub-regional average of 37%. Many Tajiks left the education system due to low demand in the labor market for people with extensive educational training or professional skills.
The dissolution of the Soviet Union was formally enacted on December 26, 1991, as a result of the declaration no. 142-Н of the Soviet of the Republics of the Supreme Soviet of the Soviet Union. The declaration acknowledged the independence of the former Soviet republics and created the Commonwealth of Independent States (CIS), although five of the signatories ratified it much later or not at all. On the previous day, Soviet President Mikhail Gorbachev, the eighth and last leader of the Soviet Union, resigned, declared his office extinct, and handed over its powers – including control of the Soviet nuclear missile launching codes – to Russian President Boris Yeltsin. That evening at 7:32 p.m., the Soviet flag was lowered from the Kremlin for the last time and replaced with the pre-revolutionary Russian flag.
Mikhail Gorbachev was elected General Secretary by the Politburo on March 11, 1985, three hours after predecessor Konstantin Chernenko's death at age 73. Gorbachev, aged 54, was the youngest member of the Politburo. His initial goal as general secretary was to revive the Soviet economy, and he realized that doing so would require reforming underlying political and social structures. The reforms began with personnel changes of senior Brezhnev-era officials who would impede political and economic change. On April 23, 1985, Gorbachev brought two protégés, Yegor Ligachev and Nikolai Ryzhkov, into the Politburo as full members. He kept the "power" ministries happy by promoting KGB Head Viktor Chebrikov from candidate to full member and appointing Minister of Defence Marshal Sergei Sokolov as a Politburo candidate.
This liberalization, however, fostered nationalist movements and ethnic disputes within the Soviet Union. It also led indirectly to the revolutions of 1989, in which Soviet-imposed communist regimes of the Warsaw Pact were peacefully toppled (Romania excepted), which in turn increased pressure on Gorbachev to introduce greater democracy and autonomy for the Soviet Union's constituent republics. Under Gorbachev's leadership, the Communist Party of the Soviet Union in 1989 introduced limited competitive elections to a new central legislature, the Congress of People's Deputies (although the ban on other political parties was not lifted until 1990).
In May 1985, Gorbachev delivered a speech in Leningrad advocating reforms and an anti-alcohol campaign to tackle widespread alcoholism. Prices on vodka, wine, and beer were raised in order to make these drinks more expensive and a disincentive to consumers, and the introduction of rationing. Unlike most forms of rationing intended to conserve scarce goods, this was done to restrict sales with the overt goal of curtailing drunkenness. Gorbachev's plan also included billboards promoting sobriety, increased penalties for public drunkenness, and to censor drinking scenes from old movies. Although this program was not a direct copycat of Tsar Nicholas II's outright prohibition during World War I, Gorbachev faced the same adverse economic reaction as did the last Tsar. The disincentivization of alcohol consumption was a serious blow to the state budget according to Alexander Yakovlev, who noted annual collections of alcohol taxes decreased by 100 billion rubles. Alcohol production migrated to the black market, or through moonshining as some made "bathtub vodka" with homegrown potatoes. Poorer, less educated Russians resorted to drinking unhealthy substitutes such as nail polish, rubbing alcohol or men's cologne, which only served to be an additional burden on Russia's healthcare sector due to the subsequent poisoning cases. The purpose of these reforms, however, was to prop up the existing centrally planned economy, unlike later reforms, which tended toward market socialism.
On July 1, 1985, Gorbachev promoted Eduard Shevardnadze, First Secretary of the Georgian Communist Party, to full member of the Politburo, and the following day appointed him minister of foreign affairs, replacing longtime Foreign Minister Andrei Gromyko. The latter, disparaged as "Mr Nyet" in the West, had served for 28 years as Minister of Foreign Affairs. Gromyko was relegated to the largely ceremonial position of Chairman of the Presidium of the Supreme Soviet (officially Soviet Head of State), as he was considered an "old thinker." Also on July 1, Gorbachev took the opportunity to dispose of his main rival by removing Grigory Romanov from the Politburo, and brought Boris Yeltsin and Lev Zaikov into the CPSU Central Committee Secretariat.
In the fall of 1985, Gorbachev continued to bring younger and more energetic men into government. On September 27, Nikolai Ryzhkov replaced 79-year-old Nikolai Tikhonov as Chairman of the Council of Ministers, effectively the Soviet prime minister, and on October 14, Nikolai Talyzin replaced Nikolai Baibakov as chairman of the State Planning Committee (GOSPLAN). At the next Central Committee meeting on October 15, Tikhonov retired from the Politburo and Talyzin became a candidate. Finally, on December 23, 1985, Gorbachev appointed Yeltsin First Secretary of the Moscow Communist Party replacing Viktor Grishin.
The CTAG (Latvian: Cilvēktiesību aizstāvības grupa, Human Rights Defense Group) Helsinki-86 was founded in July 1986 in the Latvian port town of Liepāja by three workers: Linards Grantiņš, Raimonds Bitenieks, and Mārtiņš Bariss. Its name refers to the human-rights statements of the Helsinki Accords. Helsinki-86 was the first openly anti-Communist organization in the U.S.S.R., and the first openly organized opposition to the Soviet regime, setting an example for other ethnic minorities' pro-independence movements.[citation needed]
The "Jeltoqsan" (Kazakh for "December") of 1986 were riots in Alma-Ata, Kazakhstan, sparked by Gorbachev's dismissal of Dinmukhamed Konayev, the First Secretary of the Communist Party of Kazakhstan and an ethnic Kazakh, who was replaced with Gennady Kolbin, an outsider from the Russian SFSR. Demonstrations started in the morning of December 17, 1986, with 200 to 300 students in front of the Central Committee building on Brezhnev Square protesting Konayev's dismissal and replacement by a Russian. Protesters swelled to 1,000 to 5,000 as other students joined the crowd. The CPK Central Committee ordered troops from the Ministry of Internal Affairs, druzhiniki (volunteers), cadets, policemen, and the KGB to cordon the square and videotape the participants. The situation escalated around 5 p.m., as troops were ordered to disperse the protesters. Clashes between the security forces and the demonstrators continued throughout the night in Almaty.
On the next day, December 18, protests turned into civil unrest as clashes between troops, volunteers, militia units, and Kazakh students turned into a wide-scale confrontation. The clashes could only be controlled on the third day. The Almaty events were followed by smaller protests and demonstrations in Shymkent, Pavlodar, Karaganda, and Taldykorgan. Reports from Kazakh SSR authorities estimated that the riots drew 3,000 people. Other estimates are of at least 30,000 to 40,000 protestors with 5,000 arrested and jailed, and an unknown number of casualties. Jeltoqsan leaders say over 60,000 Kazakhs participated in the protests. According to the Kazakh SSR government, there were two deaths during the riots, including a volunteer police worker and a student. Both of them had died due to blows to the head. About 100 others were detained and several others were sentenced to terms in labor camps. Sources cited by the Library of Congress claimed that at least 200 people died or were summarily executed soon thereafter; some accounts estimate casualties at more than 1,000. The writer Mukhtar Shakhanov claimed that a KGB officer testified that 168 protesters were killed, but that figure remains unconfirmed.
Gorbachev also radically expanded the scope of Glasnost, stating that no subject was off-limits for open discussion in the media. Even so, the cautious Soviet intelligentsia took almost a year to begin pushing the boundaries to see if he meant what he said. For the first time, the Communist Party leader had appealed over the heads of Central Committee members for the people's support in exchange for expansion of liberties. The tactic proved successful: Within two years political reform could no longer be sidetracked by Party "conservatives." An unintended consequence was that having saved reform, Gorbachev's move ultimately killed the very system it was designed to save.
On February 7, 1987, dozens of political prisoners were freed in the first group release since Khrushchev's "thaw" in the mid-1950s. On May 6, 1987, Pamyat, a Russian nationalist group, held an unsanctioned demonstration in Moscow. The authorities did not break up the demonstration and even kept traffic out of the demonstrators' way while they marched to an impromptu meeting with Boris Yeltsin, head of the Moscow Communist Party and at the time one of Gorbachev's closest allies. On July 25, 1987, 300 Crimean Tatars staged a noisy demonstration near the Kremlin Wall for several hours, calling for the right to return to their homeland, from which they were deported in 1944; police and soldiers merely looked on.
On September 10, 1987, after a lecture from hardliner Yegor Ligachev at the Politburo for allowing these two unsanctioned demonstrations in Moscow, Boris Yeltsin wrote a letter of resignation to Gorbachev, who had been holidaying on the Black Sea. Gorbachev was stunned – no one had ever voluntarily resigned from the Politburo. At the October 27, 1987, plenary meeting of the Central Committee, Yeltsin, frustrated that Gorbachev had not addressed any of the issues outlined in his resignation letter, criticized the slow pace of reform, servility to the general secretary, and opposition from Ligachev that had led to his (Yeltsin's) resignation. No one had ever addressed the Party leader so brazenly in front of the Central Committee since Leon Trotsky in the 1920s. In his reply, Gorbachev accused Yeltsin of "political immaturity" and "absolute irresponsibility." No one backed Yeltsin.
On June 14, 1987, about 5,000 people gathered again at Freedom Monument in Riga, and laid flowers to commemorate the anniversary of Stalin's mass deportation of Latvians in 1941. This was the first large demonstration in the Baltic republics to commemorate the anniversary of an event contrary to official Soviet history. The authorities did not crack down on demonstrators, which encouraged more and larger demonstrations throughout the Baltic States. The next major anniversary after the August 23 Molotov Pact demonstration was on November 18, the date of Latvia’s independence in 1918. On November 18, 1987, hundreds of police and civilian militiamen cordoned off the central square to prevent any demonstration at Freedom Monument, but thousands lined the streets of Riga in silent protest regardless.
In spring 1987, a protest movement arose against new phosphate mines in Estonia. Signatures were collected in Tartu, and students assembled in the university's main hall to express lack of confidence in the government. At a demonstration on May 1, 1987, young people showed up with banners and slogans despite an official ban. On August 15, 1987, former political prisoners formed the MRP-AEG group (Estonians for the Public Disclosure of the Molotov-Ribbentrop Pact), which was headed by Tiit Madisson. In September 1987, the Edasi newspaper published a proposal by Edgar Savisaar, Siim Kallas, Tiit Made, and Mikk Titma calling for Estonia's transition to autonomy. Initially geared toward economic independence, then toward a certain amount of political autonomy, the project, Isemajandav Eesti ("A Self-Managing Estonia") became known according to its Estonian acronym, IME, which means "miracle". On October 21, a demonstration dedicated to those who gave their lives in the 1918–1920 Estonian War of Independence took place in Võru, which culminated in a conflict with the militia. For the first time in years, the blue, black, and white national tricolor was publicly displayed.
On October 17, 1987, about 3,000 Armenians demonstrated in Yerevan complaining about the condition of Lake Sevan, the Nairit chemicals plant, and the Metsamor Nuclear Power Plant, and air pollution in Yerevan. Police tried to prevent the protest but took no action to stop it once the march was underway. The demonstration was led by Armenian writers such as Silva Kaputikian, Zori Balayan, and Maro Margarian and leaders from the National Survival organization. The march originated at the Opera Plaza after speakers, mainly intellectuals, addressed the crowd.
On July 1, 1988, the fourth and last day of a bruising 19th Party Conference, Gorbachev won the backing of the tired delegates for his last-minute proposal to create a new supreme legislative body called the Congress of People's Deputies. Frustrated by the old guard's resistance, Gorbachev embarked on a set of constitutional changes to try to separate party and state, and thereby isolate his conservative Party opponents. Detailed proposals for the new Congress of People's Deputies were published on October 2, 1988, and to enable the creation of the new legislature the Supreme Soviet, during its November 29–December 1, 1988, session, implemented amendments to the 1977 Soviet Constitution, enacted a law on electoral reform, and set the date of the election for March 26, 1989.
On October 2, the Popular Front formally launched its political platform at a two-day congress. Väljas attended, gambling that the front could help Estonia become a model of economic and political revival, while moderating separatist and other radical tendencies. On November 16, 1988, the Supreme Soviet of the Estonian SSR adopted a declaration of national sovereignty under which Estonian laws would take precedence over those of the Soviet Union. Estonia's parliament also laid claim to the republic's natural resources including land, inland waters, forests, mineral deposits, and to the means of industrial production, agriculture, construction, state banks, transportation, and municipal services within the territory of Estonia's borders.
In February 20, 1988, after a week of growing demonstrations in Stepanakert, capital of the Nagorno-Karabakh Autonomous Oblast (the Armenian majority area within Azerbaijan Soviet Socialist Republic), the Regional Soviet voted to secede and join with the Soviet Socialist Republic of Armenia. This local vote in a small, remote part of the Soviet Union made headlines around the world; it was an unprecedented defiance of republic and national authorities. On February 22, 1988, in what became known as the "Askeran clash", two Azerbaijanis were killed by Karabakh police. These deaths, announced on state radio, led to the Sumgait Pogrom. Between February 26 and March 1, the city of Sumgait (Azerbaijan) saw violent anti-Armenian rioting during which 32 people were killed. The authorities totally lost control and occupied the city with paratroopers and tanks; nearly all of the 14,000 Armenian residents of Sumgait fled.
Gorbachev refused to make any changes to the status of Nagorno Karabakh, which remained part of Azerbaijan. He instead sacked the Communist Party Leaders in both Republics – on May 21, 1988, Kamran Baghirov was replaced by Abdulrahman Vezirov as First Secretary of the Azerbaijan Communist Party. From July 23 to September 1988, a group of Azerbaijani intellectuals began working for a new organization called the Popular Front of Azerbaijan, loosely based on the Estonian Popular Front. On September 17, when gun battles broke out between the Armenians and Azerbaijanis near Stepanakert, two soldiers were killed and more than two dozen injured. This led to almost tit-for-tat ethnic polarization in Nagorno-Karabakh's two main towns: The Azerbaijani minority was expelled from Stepanakert, and the Armenian minority was expelled from Shusha. On November 17, 1988, in response to the exodus of tens of thousands of Azerbaijanis from Armenia, a series of mass demonstrations began in Baku's Lenin Square, lasting 18 days and attracting half a million demonstrators. On December 5, 1988, the Soviet militia finally moved in, cleared the square by force, and imposed a curfew that lasted ten months.
The rebellion of fellow Armenians in Nagorno-Karabakh had an immediate effect in Armenia itself. Daily demonstrations, which began in the Armenian capital Yerevan on February 18, initially attracted few people, but each day the Nagorno-Karabakh issue became increasingly prominent and numbers swelled. On February 20, a 30,000-strong crowd demonstrated in Theater Square, by February 22, there were 100,000, the next day 300,000, and a transport strike was declared, by February 25, there were close to 1 million demonstrators – about a quarter of Armenia's population. This was the first of the large, peaceful public demonstrations that would become a feature of communism's overthrow in Prague, Berlin, and, ultimately, Moscow. Leading Armenian intellectuals and nationalists, including future first President of independent Armenia Levon Ter-Petrossian, formed the eleven-member Karabakh Committee to lead and organize the new movement.
Gorbachev again refused to make any changes to the status of Nagorno Karabakh, which remained part of Azerbaijan. Instead he sacked both Republics' Communist Party Leaders: On May 21, 1988, Karen Demirchian was replaced by Suren Harutyunyan as First Secretary of the Communist Party of Armenia. However, Harutyunyan quickly decided to run before the nationalist wind and on May 28, allowed Armenians to unfurl the red-blue-gold First Armenian Republic flag for the first time in almost 70 years. On June 15, 1988, the Armenian Supreme Soviet adopted a resolution formally approving the idea of Nagorno Karabakh joining Armenia. Armenia, formerly one of the most loyal Republics, had suddenly turned into the leading rebel republic. On July 5, 1988, when a contingent of troops was sent in to remove demonstrators by force from Yerevan's Zvartnots International Airport, shots were fired and one student protester was killed. In September, further large demonstrations in Yerevan led to the deployment of armored vehicles. In the autumn of 1988 almost all the 200,000 Azerbaijani minority in Armenia was expelled by Armenian Nationalists, with over 100 killed in the process – this, after the Sumgait pogrom earlier that year carried out by Azerbaijanis against ethnic Armenians and subsequent expulsion of all Armenians from Azerbaijan. On November 25, 1988, a military commandant took control of Yerevan as the Soviet government moved to prevent further ethnic violence.
Beginning in February 1988, the Democratic Movement of Moldova (formerly Moldavia) organized public meetings, demonstrations, and song festivals, which gradually grew in size and intensity. In the streets, the center of public manifestations was the Stephen the Great Monument in Chişinău, and the adjacent park harboring Aleea Clasicilor (The "Alee of the Classics [of the Literature]"). On January 15, 1988, in a tribute to Mihai Eminescu at his bust on the Aleea Clasicilor, Anatol Şalaru submitted a proposal to continue the meetings. In the public discourse, the movement called for national awakening, freedom of speech, revival of Moldavian traditions, and for attainment of official status for the Romanian language and return to the Latin alphabet. The transition from "movement" (an informal association) to "front" (a formal association) was seen as a natural "upgrade" once a movement gained momentum with the public, and the Soviet authorities no longer dared to crack down on it.
On April 26, 1988, about 500 people participated in a march organized by the Ukrainian Cultural Club on Kiev's Khreschatyk Street to mark the second anniversary of the Chernobyl nuclear disaster, carrying placards with slogans like "Openness and Democracy to the End." Between May and June 1988, Ukrainian Catholics in western Ukraine celebrated the Millennium of Christianity in Kievan Rus' in secret by holding services in the forests of Buniv, Kalush, Hoshiv, and Zarvanytsia. On June 5, 1988, as the official celebrations of the Millennium were held in Moscow, the Ukrainian Cultural Club hosted its own observances in Kiev at the monument to St. Volodymyr the Great, the grand prince of Kievan Rus'.
On June 16, 1988, 6,000 to 8,000 people gathered in Lviv to hear speakers declare no confidence in the local list of delegates to the 19th Communist Party conference, to begin on June 29. On June 21, a rally in Lviv attracted 50,000 people who had heard about a revised delegate list. Authorities attempted to disperse the rally in front of Druzhba Stadium. On July 7, 10,000 to 20,000 people witnessed the launch of the Democratic Front to Promote Perestroika. On July 17, a group of 10,000 gathered in the village Zarvanytsia for Millennium services celebrated by Ukrainian Greek-Catholic Bishop Pavlo Vasylyk. The militia tried to disperse attendees, but it turned out to be the largest gathering of Ukrainian Catholics since Stalin outlawed the Church in 1946. On August 4, which came to be known as "Bloody Thursday," local authorities violently suppressed a demonstration organized by the Democratic Front to Promote Perestroika. Forty-one people were detained, fined, or sentenced to 15 days of administrative arrest. On September 1, local authorities violently displaced 5,000 students at a public meeting lacking official permission at Ivan Franko State University.
On November 13, 1988, approximately 10,000 people attended an officially sanctioned meeting organized by the cultural heritage organization Spadschyna, the Kyiv University student club Hromada, and the environmental groups Zelenyi Svit ("Green World") and Noosfera, to focus on ecological issues. From November 14–18, 15 Ukrainian activists were among the 100 human-, national- and religious-rights advocates invited to discuss human rights with Soviet officials and a visiting delegation of the U.S. Commission on Security and Cooperation in Europe (also known as the Helsinki Commission). On December 10, hundreds gathered in Kiev to observe International Human Rights Day at a rally organized by the Democratic Union. The unauthorized gathering resulted in the detention of local activists.
The Partyja BPF (Belarusian Popular Front) was established in 1988 as a political party and cultural movement for democracy and independence, à la the Baltic republics’ popular fronts. The discovery of mass graves in Kurapaty outside Minsk by historian Zianon Pazniak, the Belarusian Popular Front’s first leader, gave additional momentum to the pro-democracy and pro-independence movement in Belarus. It claimed that the NKVD performed secret killings in Kurapaty. Initially the Front had significant visibility because its numerous public actions almost always ended in clashes with the police and the KGB.
Spring 1989 saw the people of the Soviet Union exercising a democratic choice, albeit limited, for the first time since 1917, when they elected the new Congress of People's Deputies. Just as important was the uncensored live TV coverage of the legislature's deliberations, where people witnessed the previously feared Communist leadership being questioned and held accountable. This example fueled a limited experiment with democracy in Poland, which quickly led to the toppling of the Communist government in Warsaw that summer – which in turn sparked uprisings that overthrew communism in the other five Warsaw Pact countries before the end of 1989, the year the Berlin Wall fell. These events showed that the people of Eastern Europe and the Soviet Union did not support Gorbachev's drive to modernize Communism; rather, they preferred to abandon it altogether.
In the March 26 general elections, voter participation was an impressive 89.8%, and 1,958 (including 1,225 district seats) of the 2,250 CPD seats were filled. In district races, run-off elections were held in 76 constituencies on April 2 and 9 and fresh elections were organized on April 20 and 14 to May 23, in the 199 remaining constituencies where the required absolute majority was not attained. While most CPSU-endorsed candidates were elected, more than 300 lost to independent candidates such as Yeltsin, physicist Andrei Sakharov and lawyer Anatoly Sobchak.
In the first session of the new Congress of People's Deputies, from May 25 to June 9, hardliners retained control but reformers used the legislature as a platform for debate and criticism – which was broadcast live and uncensored. This transfixed the population; nothing like this freewheeling debate had ever been witnessed in the U.S.S.R. On May 29, Yeltsin managed to secure a seat on the Supreme Soviet, and in the summer he formed the first opposition, the Inter-Regional Deputies Group, composed of Russian nationalists and liberals. Composing the final legislative group in the Soviet Union, those elected in 1989 played a vital part in reforms and the eventual breakup of the Soviet Union during the next two years.
On October 25, 1989, the Supreme Soviet voted to eliminate special seats for the Communist Party and other official organizations in national and local elections, responding to sharp popular criticism that such reserved slots were undemocratic. After vigorous debate, the 542-member Supreme Soviet passed the measure 254-85 (with 36 abstentions). The decision required a constitutional amendment, ratified by the full congress, which met December 12–25. It also passed measures that would allow direct elections for presidents of each of the 15 constituent republics. Gorbachev strongly opposed such a move during debate but was defeated.
The six Warsaw Pact countries of Eastern Europe, while nominally independent, were widely recognized in the international community as the Soviet satellite states. All had been occupied by the Soviet Red Army in 1945, had Soviet-style socialist states imposed upon them, and had very restricted freedom of action in either domestic or international affairs. Any moves towards real independence were suppressed by military force – in the Hungarian Revolution of 1956 and the Prague Spring in 1968. Gorbachev abandoned the oppressive and expensive Brezhnev Doctrine, which mandated intervention in the Warsaw Pact states, in favor of non-intervention in the internal affairs of allies – jokingly termed the Sinatra Doctrine in a reference to the Frank Sinatra song "My Way".
The Baltic Way or Baltic Chain (also Chain of Freedom Estonian: Balti kett, Latvian: Baltijas ceļš, Lithuanian: Baltijos kelias, Russian: Балтийский путь) was a peaceful political demonstration on August 23, 1989. An estimated 2 million people joined hands to form a human chain extending 600 kilometres (370 mi) across Estonia, Latvia and Lithuania, which had been forcibly reincorporated into the Soviet Union in 1944. The colossal demonstration marked the 50th anniversary of the Molotov–Ribbentrop Pact that divided Eastern Europe into spheres of influence and led to the occupation of the Baltic states in 1940.
On December 7, 1989, the Communist Party of Lithuania under the leadership of Algirdas Brazauskas, split from the Communist Party of the Soviet Union and abandoned its claim to have a constitutional "leading role" in politics. A smaller loyalist faction of the Communist Party, headed by hardliner Mykolas Burokevičius, was established and remained affiliated with the CPSU. However, Lithuania’s governing Communist Party was formally independent from Moscow's control – a first for Soviet Republics and a political earthquake that prompted Gorbachev to arrange a visit to Lithuania the following month in a futile attempt to bring the local party back under control.
On July 16, 1989, the Popular Front of Azerbaijan held its first congress and elected Abulfaz Elchibey, who would become President, as its Chairman. On August 19, 600,000 protesters jammed Baku’s Lenin Square (now Azadliq Square) to demand the release of political prisoners. In the second half of 1989, weapons were handed out in Nagorno-Karabakh. When Karabakhis got hold of small arms to replace hunting rifles and crossbows, casualties began to mount; bridges were blown up, roads were blockaded, and hostages were taken.
In a new and effective tactic, the Popular Front launched a rail blockade of Armenia, which caused petrol and food shortages because 85 percent of Armenia's freight came from Azerbaijan. Under pressure from the Popular Front the Communist authorities in Azerbaijan started making concessions. On September 25, they passed a sovereignty law that gave precedence to Azerbaijani law, and on October 4, the Popular Front was permitted to register as a legal organization as long as it lifted the blockade. Transport communications between Azerbaijan and Armenia never fully recovered. Tensions continued to escalate and on December 29, Popular Front activists seized local party offices in Jalilabad, wounding dozens.
On April 7, 1989, Soviet troops and armored personnel carriers were sent to Tbilisi after more than 100,000 people protested in front of Communist Party headquarters with banners calling for Georgia to secede from the Soviet Union and for Abkhazia to be fully integrated into Georgia. On April 9, 1989, troops attacked the demonstrators; some 20 people were killed and more than 200 wounded. This event radicalized Georgian politics, prompting many to conclude that independence was preferable to continued Soviet rule. On April 14, Gorbachev removed Jumber Patiashvili as First Secretary of the Georgian Communist Party and replaced him with former Georgian KGB chief Givi Gumbaridze.
In Ukraine, Lviv and Kiev celebrated Ukrainian Independence Day on January 22, 1989. Thousands gathered in Lviv for an unauthorized moleben (religious service) in front of St. George's Cathedral. In Kiev, 60 activists met in a Kiev apartment to commemorate the proclamation of the Ukrainian People's Republic in 1918. On February 11–12, 1989, the Ukrainian Language Society held its founding congress. On February 15, 1989, the formation of the Initiative Committee for the Renewal of the Ukrainian Autocephalous Orthodox Church was announced. The program and statutes of the movement were proposed by the Writers Association of Ukraine and were published in the journal Literaturna Ukraina on February 16, 1989. The organization heralded Ukrainian dissidents such as Vyacheslav Chornovil.
In late February, large public rallies took place in Kiev to protest the election laws, on the eve of the March 26 elections to the USSR Congress of People's Deputies, and to call for the resignation of the first secretary of the Communist Party of Ukraine, Volodymyr Scherbytsky, lampooned as "the mastodon of stagnation." The demonstrations coincided with a visit to Ukraine by Soviet President Gorbachev. On February 26, 1989, between 20,000 and 30,000 people participated in an unsanctioned ecumenical memorial service in Lviv, marking the anniversary of the death of 19th Century Ukrainian artist and nationalist Taras Shevchenko.
On March 4, 1989, the Memorial Society, committed to honoring the victims of Stalinism and cleansing society of Soviet practices, was founded in Kiev. A public rally was held the next day. On March 12, A pre-election meeting organized in Lviv by the Ukrainian Helsinki Union and the Marian Society Myloserdia (Compassion) was violently dispersed, and nearly 300 people were detained. On March 26, elections were held to the union Congress of People's Deputies; by-elections were held on April 9, May 14, and May 21. Among the 225 Ukrainian deputies, most were conservatives, though a handful of progressives made the cut.
From April 20–23, 1989, pre-election meetings were held in Lviv for four consecutive days, drawing crowds of up to 25,000. The action included an one-hour warning strike at eight local factories and institutions. It was the first labor strike in Lviv since 1944. On May 3, a pre-election rally attracted 30,000 in Lviv. On May 7, The Memorial Society organized a mass meeting at Bykivnia, site of a mass grave of Ukrainian and Polish victims of Stalinist terror. After a march from Kiev to the site, a memorial service was staged.
From mid-May to September 1989, Ukrainian Greek-Catholic hunger strikers staged protests on Moscow's Arbat to call attention to the plight of their Church. They were especially active during the July session of the World Council of Churches held in Moscow. The protest ended with the arrests of the group on September 18. On May 27, 1989, the founding conference of the Lviv regional Memorial Society was held. On June 18, 1989, an estimated 100,000 faithful participated in public religious services in Ivano-Frankivsk in western Ukraine, responding to Cardinal Myroslav Lubachivsky's call for an international day of prayer.
On August 19, 1989, the Russian Orthodox Parish of Saints Peter and Paul announced it would be switching to the Ukrainian Autocephalous Orthodox Church. On September 2, 1989, tens of thousands across Ukraine protested a draft election law that reserved special seats for the Communist Party and for other official organizations: 50,000 in Lviv, 40,000 in Kiev, 10,000 in Zhytomyr, 5,000 each in Dniprodzerzhynsk and Chervonohrad, and 2,000 in Kharkiv. From September 8–10, 1989, writer Ivan Drach was elected to head Rukh, the People's Movement of Ukraine, at its founding congress in Kiev. On September 17, between 150,000 and 200,000 people marched in Lviv, demanding the legalization of the Ukrainian Greek Catholic Church. On September 21, 1989, exhumation of a mass grave begins in Demianiv Laz, a nature preserve south of Ivano-Frankivsk. On September 28, First Secretary of the Communist Party of the Ukraine Volodymyr Shcherbytsky, a holdover from the Brezhnev era, was replaced by Vladimir Ivashko.
On October 1, 1989, a peaceful demonstration of 10,000 to 15,000 people was violently dispersed by the militia in front of Lviv's Druzhba Stadium, where a concert celebrating the Soviet "reunification" of Ukrainian lands was being held. On October 10, Ivano-Frankivsk was the site of a pre-election protest attended by 30,000 people. On October 15, several thousand people gathered in Chervonohrad, Chernivtsi, Rivne, and Zhytomyr; 500 in Dnipropetrovsk; and 30,000 in Lviv to protest the election law. On October 20, faithful and clergy of the Ukrainian Autocephalous Orthodox Church participated in a synod in Lviv, the first since its forced liquidation in the 1930s.
On October 24, the union Supreme Soviet passed a law eliminating special seats for Communist Party and other official organizations' representatives. On October 26, twenty factories in Lviv held strikes and meetings to protest the police brutality of October 1 and the authorities' unwillingness to prosecute those responsible. From October 26–28, the Zelenyi Svit (Friends of the Earth – Ukraine) environmental association held its founding congress, and on October 27 the Ukrainian Supreme Soviet passed a law eliminating the special status of party and other official organizations.
On October 28, 1989, the Ukrainian Supreme Soviet decreed that effective January 1, 1990, Ukrainian would be the official language of Ukraine, while Russian would be used for communication between ethnic groups. On the same day The Congregation of the Church of the Transfiguration in Lviv left the Russian Orthodox Church and proclaimed itself the Ukrainian Greek Catholic Church. The following day, thousands attended a memorial service at Demianiv Laz, and a temporary marker was placed to indicate that a monument to the "victims of the repressions of 1939–1941" soon would be erected.
In mid-November The Shevchenko Ukrainian Language Society was officially registered. On November 19, 1989, a public gathering in Kiev attracted thousands of mourners, friends and family to the reburial in Ukraine of three inmates of the infamous Gulag Camp No. 36 in Perm in the Ural Mountains: human-rights activists Vasyl Stus, Oleksiy Tykhy, and Yuriy Lytvyn. Their remains were reinterred in Baikove Cemetery. On November 26, 1989, a day of prayer and fasting was proclaimed by Cardinal Myroslav Lubachivsky, thousands of faithful in western Ukraine participated in religious services on the eve of a meeting between Pope John Paul II and Soviet President Gorbachev. On November 28, 1989, the Ukrainian SSR's Council for Religious Affairs issued a decree allowing Ukrainian Catholic congregations to register as legal organizations. The decree was proclaimed on December 1, coinciding with a meeting at the Vatican between the pope and the Soviet president.
On September 30, 1989, thousands of Belorussians, denouncing local leaders, marched through Minsk to demand additional cleanup of the 1986 Chernobyl disaster site in Ukraine. Up to 15,000 protesters wearing armbands bearing radioactivity symbols and carrying the banned red-and-white Belorussian national flag filed through torrential rain in defiance of a ban by local authorities. Later, they gathered in the city center near the government's headquarters, where speakers demanded resignation of Yefrem Sokolov, the republic's Communist Party leader, and called for the evacuation of half a million people from the contaminated zones.
Thousands of Soviet troops were sent to the Fergana Valley, southeast of the Uzbek capital Tashkent, to re-establish order after clashes in which local Uzbeks hunted down members of the Meskhetian minority in several days of rioting between June 4–11, 1989; about 100 people were killed. On June 23, 1989, Gorbachev removed Rafiq Nishonov as First Secretary of the Communist Party of the Uzbek SSR and replaced him with Karimov, who went on to lead Uzbekistan as a Soviet Republic and subsequently as an independent state.
In Kazakhstan on June 19, 1989, young men carrying guns, firebombs, iron bars and stones rioted in Zhanaozen, causing a number of deaths. The youths tried to seize a police station and a water-supply station. They brought public transportation to a halt and shut down various shops and industries. By June 25, the rioting had spread to five other towns near the Caspian Sea. A mob of about 150 people armed with sticks, stones and metal rods attacked the police station in Mangishlak, about 90 miles from Zhanaozen, before they were dispersed by government troops flown in by helicopters. Mobs of young people also rampaged through Yeraliev, Shepke, Fort-Shevchenko and Kulsary, where they poured flammable liquid on trains housing temporary workers and set them on fire.
Ethnic tensions had escalated between the Armenians and Azerbaijanis in spring and summer 1988. On January 9, 1990, after the Armenian parliament voted to include Nagorno-Karabakh within its budget, renewed fighting broke out, hostages were taken, and four Soviet soldiers were killed. On January 11, Popular Front radicals stormed party buildings and effectively overthrew the communist powers in the southern town of Lenkoran. Gorbachev resolved to regain control of Azerbaijan; the events that ensued are known as "Black January." Late on January 19, 1990, after blowing up the central television station and cutting the phone and radio lines, 26,000 Soviet troops entered the Azerbaijani capital Baku, smashing barricades, attacking protesters, and firing into crowds. On that night and during subsequent confrontations (which lasted until February), more than 130 people died – the majority of whom were civilians. More than 700 civilians were wounded, hundreds were detained, but only a few were actually tried for alleged criminal offenses.
Following the hardliners' takeover, the September 30, 1990 elections (runoffs on October 14) were characterized by intimidation; several Popular Front candidates were jailed, two were murdered, and unabashed ballot stuffing took place even in the presence of Western observers. The election results reflected the threatening environment; out of the 350 members, 280 were Communists, with only 45 opposition candidates from the Popular Front and other non-communist groups, who together formed a Democratic Bloc ("Dembloc"). In May 1990 Mutalibov was elected Chairman of the Supreme Soviet unopposed.
On January 21, 1990, Rukh organized a 300-mile (480 km) human chain between Kiev, Lviv, and Ivano-Frankivsk. Hundreds of thousands joined hands to commemorate the proclamation of Ukrainian independence in 1918 and the reunification of Ukrainian lands one year later (1919 Unification Act). On January 23, 1990, the Ukrainian Greek-Catholic Church held its first synod since its liquidation by the Soviets in 1946 (an act which the gathering declared invalid). On February 9, 1990, the Ukrainian Ministry of Justice officially registered Rukh. However, the registration came too late for Rukh to stand its own candidates for the parliamentary and local elections on March 4. At the 1990 elections of people's deputies to the Supreme Council (Verkhovna Rada), candidates from the Democratic Bloc won landslide victories in western Ukrainian oblasts. A majority of the seats had to hold run-off elections. On March 18, Democratic candidates scored further victories in the run-offs. The Democratic Bloc gained about 90 out of 450 seats in the new parliament.
On April 6, 1990, the Lviv City Council voted to return St. George Cathedral to the Ukrainian Greek Catholic Church. The Russian Orthodox Church refused to yield. On April 29–30, 1990, the Ukrainian Helsinki Union disbanded to form the Ukrainian Republican Party. On May 15 the new parliament convened. The bloc of conservative communists held 239 seats; the Democratic Bloc, which had evolved into the National Council, had 125 deputies. On June 4, 1990, two candidates remained in the protracted race for parliament chair. The leader of the Communist Party of Ukraine (CPU), Volodymyr Ivashko, was elected with 60 percent of the vote as more than 100 opposition deputies boycotted the election. On June 5–6, 1990, Metropolitan Mstyslav of the U.S.-based Ukrainian Orthodox Church was elected patriarch of the Ukrainian Autocephalous Orthodox Church (UAOC) during that Church's first synod. The UAOC declared its full independence from the Moscow Patriarchate of the Russian Orthodox Church, which in March had granted autonomy to the Ukrainian Orthodox church headed by Metropolitan Filaret.
On June 22, 1990, Volodymyr Ivashko withdrew his candidacy for leader of the Communist Party of Ukraine in view of his new position in parliament. Stanislav Hurenko was elected first secretary of the CPU. On July 11, Ivashko resigned from his post as chairman of the Ukrainian Parliament after he was elected deputy general secretary of the Communist Party of the Soviet Union. The Parliament accepted the resignation a week later, on July 18. On July 16 Parliament overwhelmingly approved the Declaration on State Sovereignty of Ukraine - with a vote of 355 in favour and four against. The people's deputies voted 339 to 5 to proclaim July 16 a Ukrainian national holiday.
On July 23, 1990, Leonid Kravchuk was elected to replace Ivashko as parliament chairman. On July 30, Parliament adopted a resolution on military service ordering Ukrainian soldiers "in regions of national conflict such as Armenia and Azerbaijan" to return to Ukrainian territory. On August 1, Parliament voted overwhelmingly to shut down the Chernobyl Nuclear Power Plant. On August 3, it adopted a law on the economic sovereignty of the Ukrainian republic. On August 19, the first Ukrainian Catholic liturgy in 44 years was celebrated at St. George Cathedral. On September 5–7, the International Symposium on the Great Famine of 1932–1933 was held in Kiev. On September 8, The first "Youth for Christ" rally since 1933 took place held in Lviv, with 40,000 participants. In September 28–30, the Green Party of Ukraine held its founding congress. On September 30, nearly 100,000 people marched in Kiev to protest against the new union treaty proposed by Gorbachev.
On October 25–28, 1990, Rukh held its second congress and declared that its principal goal was the "renewal of independent statehood for Ukraine". On October 28 UAOC faithful, supported by Ukrainian Catholics, demonstrated near St. Sophia’s Cathedral as newly elected Russian Orthodox Church Patriarch Aleksei and Metropolitan Filaret celebrated liturgy at the shrine. On November 1, the leaders of the Ukrainian Greek Catholic Church and of the Ukrainian Autocephalous Orthodox Church, respectively, Metropolitan Volodymyr Sterniuk and Patriarch Mstyslav, met in Lviv during anniversary commemorations of the 1918 proclamation of the Western Ukrainian National Republic.
On January 13, 1991, Soviet troops, along with the KGB Spetsnaz Alpha Group, stormed the Vilnius TV Tower in Lithuania to suppress the independence movement. Fourteen unarmed civilians were killed and hundreds more injured. On the night of July 31, 1991, Russian OMON from Riga, the Soviet military headquarters in the Baltics, assaulted the Lithuanian border post in Medininkai and killed seven Lithuanian servicemen. This event further weakened the Soviet Union's position internationally and domestically, and stiffened Lithuanian resistance.
Faced with growing separatism, Gorbachev sought to restructure the Soviet Union into a less centralized state. On August 20, 1991, the Russian SFSR was scheduled to sign a New Union Treaty that would have converted the Soviet Union into a federation of independent republics with a common president, foreign policy and military. It was strongly supported by the Central Asian republics, which needed the economic advantages of a common market to prosper. However, it would have meant some degree of continued Communist Party control over economic and social life.
More radical reformists were increasingly convinced that a rapid transition to a market economy was required, even if the eventual outcome meant the disintegration of the Soviet Union into several independent states. Independence also accorded with Yeltsin's desires as president of the Russian Federation, as well as those of regional and local authorities to get rid of Moscow’s pervasive control. In contrast to the reformers' lukewarm response to the treaty, the conservatives, "patriots," and Russian nationalists of the USSR – still strong within the CPSU and the military – were opposed to weakening the Soviet state and its centralized power structure.
Thousands of Muscovites came out to defend the White House (the Russian Federation's parliament and Yeltsin's office), the symbolic seat of Russian sovereignty at the time. The organizers tried but ultimately failed to arrest Yeltsin, who rallied opposition to the coup with speech-making atop a tank. The special forces dispatched by the coup leaders took up positions near the White House, but members refused to storm the barricaded building. The coup leaders also neglected to jam foreign news broadcasts, so many Muscovites watched it unfold live on CNN. Even the isolated Gorbachev was able to stay abreast of developments by tuning into BBC World Service on a small transistor radio.
On December 8, the leaders of Russia, Ukraine, and Belarus secretly met in Belavezhskaya Pushcha, in western Belarus, and signed the Belavezha Accords, which proclaimed the Soviet Union had ceased to exist and announced formation of the Commonwealth of Independent States (CIS) as a looser association to take its place. They also invited other republics to join the CIS. Gorbachev called it an unconstitutional coup. However, by this time there was no longer any reasonable doubt that, as the preamble of the Accords put it, "the USSR, as a subject of international law and a geopolitical reality, is ceasing its existence."
On December 12, the Supreme Soviet of the Russian SFSR formally ratified the Belavezha Accords and renounced the 1922 Union Treaty. The Russian deputies were also recalled from the Supreme Soviet of the USSR. The legality of this action was questionable, since Soviet law did not allow a republic to unilaterally recall its deputies. However, no one in either Russia or the Kremlin objected. Any objections from the latter would have likely had no effect, since the Soviet government had effectively been rendered impotent long before December. In effect, the largest and most powerful republic had seceded from the Union. Later that day, Gorbachev hinted for the first time that he was considering stepping down.
Doubts remained over the authority of the Belavezha Accords to disband the Soviet Union, since they were signed by only three republics. However, on December 21, 1991, representatives of 11 of the 12 former republics – all except Georgia – signed the Alma-Ata Protocol, which confirmed the dissolution of the Union and formally established the CIS. They also "accepted" Gorbachev's resignation. While Gorbachev hadn't made any formal plans to leave the scene yet, he did tell CBS News that he would resign as soon as he saw that the CIS was indeed a reality.
In a nationally televised speech early in the morning of December 25, 1991, Gorbachev resigned as president of the USSR – or, as he put it, "I hereby discontinue my activities at the post of President of the Union of Soviet Socialist Republics." He declared the office extinct, and all of its powers (such as control of the nuclear arsenal) were ceded to Yeltsin. A week earlier, Gorbachev had met with Yeltsin and accepted the fait accompli of the Soviet Union's dissolution. On the same day, the Supreme Soviet of the Russian SFSR adopted a statute to change Russia's legal name from "Russian Soviet Federative Socialist Republic" to "Russian Federation," showing that it was now a sovereign state.
On the night of December 25, 1991, at 7:32 p.m. Moscow time, after Gorbachev left the Kremlin, the Soviet flag was lowered for the last time, and the Russian tricolor was raised in its place, symbolically marking the end of the Soviet Union. The next day, December 26, 1991, the Council of Republics, the upper chamber of the Union's Supreme Soviet, issued a formal Declaration recognizing that the Soviet Union had ceased to exist as a state and subject of international law, and voted both itself and the Soviet Union out of existence (the other chamber of the Supreme Soviet, the Council of the Union, had been unable to work since December 12, 1991, when the recall of the Russian deputies left it without a quorum). The following day Yeltsin moved into Gorbachev's former office, though the Russian authorities had taken over the suite two days earlier. By December 31, 1991, the few remaining Soviet institutions that had not been taken over by Russia ceased operation, and individual republics assumed the central government's role.
The Alma-Ata Protocol also addressed other issues, including UN membership. Notably, Russia was authorized to assume the Soviet Union's UN membership, including its permanent seat on the Security Council. The Soviet Ambassador to the UN delivered a letter signed by Russian President Yeltsin to the UN Secretary-General dated December 24, 1991, informing him that by virtue of the Alma-Ata Protocol, Russia was the successor state to the USSR. After being circulated among the other UN member states, with no objection raised, the statement was declared accepted on the last day of the year, December 31, 1991.
On November 18, 1990, the Ukrainian Autocephalous Orthodox Church enthroned Mstyslav as Patriarch of Kiev and all Ukraine during ceremonies at Saint Sophia's Cathedral. Also on November 18, Canada announced that its consul-general to Kiev would be Ukrainian-Canadian Nestor Gayowsky. On November 19, the United States announced that its consul to Kiev would be Ukrainian-American John Stepanchuk. On November 19, the chairmen of the Ukrainian and Russian parliaments, respectively, Kravchuk and Yeltsin, signed a 10-year bilateral pact. In early December 1990 the Party of Democratic Rebirth of Ukraine was founded; on December 15, the Democratic Party of Ukraine was founded.
A gramophone record (phonograph record in American English) or vinyl record, commonly known as a "record", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove. The groove usually starts near the periphery and ends near the center of the disc. Phonograph records are generally described by their diameter in inches (12", 10", 7"), the rotational speed in rpm at which they are played (16 2⁄3, 33 1⁄3, 45, 78), and their time capacity resulting from a combination of those parameters (LP – long playing 33 1⁄3 rpm, SP – 78 rpm single, EP – 12-inch single or extended play, 33 or 45 rpm); their reproductive quality or level of fidelity (high-fidelity, orthophonic, full-range, etc.), and the number of audio channels provided (mono, stereo, quad, etc.).
The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record–with which it had co-existed from the late 1880s through to the 1920s–by the late 1920s. Records retained the largest market share even when new formats such as compact cassette were mass-marketed. By the late 1980s, digital media, in the form of the compact disc, had gained a larger market share, and the vinyl record left the mainstream in 1991. From the 1990s to the 2010s, records continued to be manufactured and sold on a much smaller scale, and were especially used by disc jockeys (DJ)s, released by artists in some genres, and listened to by a niche market of audiophiles. The phonograph record has made a niche resurgence in the early 21st century – 9.2 million records were sold in the U.S. in 2014, a 260% increase since 2009. Likewise, in the UK sales have increased five-fold from 2009 to 2014.
The phonautograph, patented by Léon Scott in 1857, used a vibrating diaphragm and stylus to graphically record sound waves as tracings on sheets of paper, purely for visual analysis and without any intent of playing them back. In the 2000s, these tracings were first scanned by audio engineers and digitally converted into audible sound. Phonautograms of singing and speech made by Scott in 1860 were played back as sound for the first time in 2008. Along with a tuning fork tone and unintelligible snippets recorded as early as 1857, these are the earliest known recordings of sound.
In 1877, Thomas Edison invented the phonograph. Unlike the phonautograph, it was capable of both recording and reproducing sound. Despite the similarity of name, there is no documentary evidence that Edison's phonograph was based on Scott's phonautograph. Edison first tried recording sound on a wax-impregnated paper tape, with the idea of creating a "telephone repeater" analogous to the telegraph repeater he had been working on. Although the visible results made him confident that sound could be physically recorded and reproduced, his notes do not indicate that he actually reproduced sound before his first experiment in which he used tinfoil as a recording medium several months later. The tinfoil was wrapped around a grooved metal cylinder and a sound-vibrated stylus indented the tinfoil while the cylinder was rotated. The recording could be played back immediately. The Scientific American article that introduced the tinfoil phonograph to the public mentioned Marey, Rosapelly and Barlow as well as Scott as creators of devices for recording but, importantly, not reproducing sound. Edison also invented variations of the phonograph that used tape and disc formats. Numerous applications for the phonograph were envisioned, but although it enjoyed a brief vogue as a startling novelty at public demonstrations, the tinfoil phonograph proved too crude to be put to any practical use. A decade later, Edison developed a greatly improved phonograph that used a hollow wax cylinder instead of a foil sheet. This proved to be both a better-sounding and far more useful and durable device. The wax phonograph cylinder created the recorded sound market at the end of the 1880s and dominated it through the early years of the 20th century.
Lateral-cut disc records were developed in the United States by Emile Berliner, who named his system the "gramophone", distinguishing it from Edison's wax cylinder "phonograph" and Columbia's wax cylinder "graphophone". Berliner's earliest discs, first marketed in 1889, but only in Europe, were 5 inches (13 cm) in diameter, and were played with a small hand-propelled machine. Both the records and the machine were adequate only for use as a toy or curiosity, due to the limited sound quality. In the United States in 1894, under the Berliner Gramophone trademark, Berliner started marketing records with somewhat more substantial entertainment value, along with somewhat more substantial gramophones to play them. Berliner's records had poor sound quality compared to wax cylinders, but his manufacturing associate Eldridge R. Johnson eventually improved the sound quality. Abandoning Berliner's "Gramophone" trademark for legal reasons, in 1901 Johnson's and Berliner's separate companies reorganized to form the Victor Talking Machine Company, whose products would come to dominate the market for many years. Emile Berliner moved his company to Montreal in 1900. The factory which became RCA Victor stills exists. There is a dedicated museum in Montreal for Berliner.
In 1901, 10-inch disc records were introduced, followed in 1903 by 12-inch records. These could play for more than three and four minutes respectively, while contemporary cylinders could only play for about two minutes. In an attempt to head off the disc advantage, Edison introduced the Amberol cylinder in 1909, with a maximum playing time of 4½ minutes (at 160 rpm), which in turn were superseded by Blue Amberol Records, which had a playing surface made of celluloid, a plastic, which was far less fragile. Despite these improvements, during the 1910s discs decisively won this early format war, although Edison continued to produce new Blue Amberol cylinders for an ever-dwindling customer base until late in 1929. By 1919 the basic patents for the manufacture of lateral-cut disc records had expired, opening the field for countless companies to produce them. Analog disc records would dominate the home entertainment market until they were outsold by the digital compact disc in the late 1980s (which was in turn supplanted by digital audio recordings distributed via online music stores and Internet file sharing).
Early recordings were made entirely acoustically, the sound being collected by a horn and piped to a diaphragm, which vibrated the cutting stylus. Sensitivity and frequency range were poor, and frequency response was very irregular, giving acoustic recordings an instantly recognizable tonal quality. A singer practically had to put his or her face in the recording horn. Lower-pitched orchestral instruments such as cellos and double basses were often doubled (or replaced) by louder wind instruments, such as tubas. Standard violins in orchestral ensembles were commonly replaced by Stroh violins, which became popular with recording studios.
Contrary to popular belief, if placed properly and prepared-for, drums could be effectively used and heard on even the earliest jazz and military band recordings. The loudest instruments such as the drums and trumpets were positioned the farthest away from the collecting horn. Lillian Hardin Armstrong, a member of King Oliver's Creole Jazz Band, which recorded at Gennett Records in 1923, remembered that at first Oliver and his young second trumpet, Louis Armstrong, stood next to each other and Oliver's horn could not be heard. "They put Louis about fifteen feet over in the corner, looking all sad." For fading instrumental parts in and out while recording, some performers were placed on a moveable platform, which could draw the performer(s) nearer or further away as required.[citation needed]
During the first half of the 1920s, engineers at Western Electric, as well as independent inventors such as Orlando Marsh, developed technology for capturing sound with a microphone, amplifying it with vacuum tubes, then using the amplified signal to drive an electromagnetic recording head. Western Electric's innovations resulted in a greatly expanded and more even frequency response, creating a dramatically fuller, clearer and more natural-sounding recording. Distant or less strong sounds that were impossible to record by the old methods could now be captured. Volume was now limited only by the groove spacing on the record and the limitations of the intended playback device. Victor and Columbia licensed the new electrical system from Western Electric and began issuing electrically recorded discs in 1925. The first classical recording was of Chopin impromptus and Schubert's Litanei by Alfred Cortot for Victor.
Electrical recording preceded electrical home reproduction because of the initial high cost of the new system. In 1925, the Victor company introduced the Victor Orthophonic Victrola, an acoustical record player that was specifically designed to play electrically recorded discs, as part of a line that also included electrically reproducing Electrolas. The acoustical Orthophonics ranged in price from US$95 to US$300, depending on cabinetry; by comparison, the cheapest Electrola cost US$650, the price of a new Ford automobile in an era when clerical jobs paid about $20 a week.
The earliest disc records (1889–1894) were made of various materials including hard rubber. Around 1895, a shellac-based compound was introduced and became standard. Exact formulas for this compound varied by manufacturer and over the course of time, but it was typically composed of about one-third shellac and about two-thirds mineral filler, which meant finely pulverized rock, usually slate and limestone, with an admixture of cotton fibers to add tensile strength, carbon black for color (without this, it tended to be a "dirty" gray or brown color that most record companies considered unattractive), and a very small amount of a lubricant to facilitate mold release during manufacture. Some makers, notably Columbia Records, used a laminated construction with a core disc of coarser material or fiber. The production of shellac records continued until the end of the 78 rpm format (i.e., the late 1950s in most developed countries, but well into the 1960s in some other places), but increasingly less abrasive formulations were used during its declining years and very late examples in truly like-new condition can have as low noise levels as vinyl.
Flexible or so-called "unbreakable" records made of unusual materials were introduced by a number of manufacturers at various times during the 78 rpm era. In the UK, Nicole records, made of celluloid or a similar substance coated onto a cardboard core disc, were produced for a few years beginning in 1904, but they suffered from an exceptionally high level of surface noise. In the United States, Columbia Records introduced flexible, fiber-cored "Marconi Velvet Tone Record" pressings in 1907, but the advantages and longevity of their relatively noiseless surfaces depended on the scrupulous use of special gold-plated Marconi Needles and the product was not a success. Thin, flexible plastic records such as the German Phonycord and the British Filmophone and Goodson records appeared around 1930 but also did not last long. The contemporary French Pathé Cellodiscs, made of a very thin black plastic, which uncannily resembles the vinyl "sound sheet" magazine inserts of the 1965–1985 era, were similarly short-lived. In the US, Hit of the Week records, made of a patented translucent plastic called Durium coated on a heavy brown paper base, were introduced in early 1930. A new issue came out every week and they were sold at newsstands like a weekly magazine. Although inexpensive and commercially successful at first, they soon fell victim to the Great Depression and production in the US ended in 1932. Related Durium records continued to be made somewhat later in the UK and elsewhere, and as remarkably late as 1950 in Italy, where the name "Durium" survived far into the LP era as a trademark on ordinary vinyl records. Despite all these attempts at innovation, shellac compounds continued to be used for the overwhelming majority of commercial 78 rpm records during the lifetime of the format.
In 1931, RCA Victor introduced their vinyl-based Victrolac compound as a material for some unusual-format and special-purpose records. By the end of the 1930s vinyl's advantages of light weight, relative unbreakability and low surface noise had made it the material of choice for prerecorded radio programming and other critical applications. When it came to ordinary 78 rpm records, however, the much higher cost of the raw material, as well as its vulnerability to the heavy pickups and crudely mass-produced steel needles still commonly used in home record players, made its general substitution for shellac impractical at that time. During the Second World War, the United States Armed Forces produced thousands of 12-inch vinyl 78 rpm V-Discs for use by the troops overseas. After the war, the wider use of vinyl became more practical as new record players with relatively lightweight crystal pickups and precision-ground styli made of sapphire or an exotic osmium alloy proliferated. In late 1945, RCA Victor began offering special transparent red vinyl De Luxe pressings of some classical 78s, at a de luxe price. Later, Decca Records introduced vinyl Deccalite 78s, while other record companies came up with vinyl concoctions such as Metrolite, Merco Plastic and Sav-o-flex, but these were mainly used to produce "unbreakable" children's records and special thin vinyl DJ pressings for shipment to radio stations.
In the 1890s, the recording formats of the earliest (toy) discs were mainly 12.5 cm (nominally five inches) in diameter; by the mid-1890s, the discs were usually 7 in (nominally 17.5 cm) in diameter. By 1910 the 10-inch (25.4 cm) record was by far the most popular standard, holding about three minutes of music or other entertainment on a side. From 1903 onwards, 12-inch records (30.5 cm) were also sold commercially, mostly of classical music or operatic selections, with four to five minutes of music per side. Victor, Brunswick and Columbia also issued 12-inch popular medleys, usually spotlighting a Broadway show score. However, other sizes did appear. Eight-inch discs with a 2-inch-diameter (51 mm) label became popular for about a decade in Britain, but they cannot be played in full on most modern record players because the tone arm cannot play far enough in toward the center without modification of the equipment.
The playing time of a phonograph record depended on the turntable speed and the groove spacing. At the beginning of the 20th century, the early discs played for two minutes, the same as early cylinder records. The 12-inch disc, introduced by Victor in 1903, increased the playing time to three and a half minutes. Because a 10-inch 78 rpm record could hold about three minutes of sound per side and the 10-inch size was the standard size for popular music, almost all popular recordings were limited to around three minutes in length. For example, when King Oliver's Creole Jazz Band, including Louis Armstrong on his first recordings, recorded 13 sides at Gennett Records in Richmond, Indiana, in 1923, one side was 2:09 and four sides were 2:52–2:59.
In January 1938, Milt Gabler started recording for his new label, Commodore Records, and to allow for longer continuous performances, he recorded some 12-inch records. Eddie Condon explained: "Gabler realized that a jam session needs room for development." The first two 12-inch recordings did not take advantage of the extra length: "Carnegie Drag" was 3:15; "Carnegie Jump", 2:41. But at the second session, on April 30, the two 12-inch recordings were longer: "Embraceable You" was 4:05; "Serenade to a Shylock", 4:32. Another way around the time limitation was to issue a selection on both sides of a single record. Vaudeville stars Gallagher and Shean recorded "Mr. Gallagher and Mr. Shean", written by Irving and Jack Kaufman, as two sides of a 10-inch 78 in 1922 for Cameo. An obvious workaround for longer recordings was to release a set of records. An early multi-record release was in 1903, when HMV in England made the first complete recording of an opera, Verdi's Ernani, on 40 single-sided discs. In 1940, Commodore released Eddie Condon and his Band's recording of "A Good Man Is Hard to Find" in four parts, issued on both sides of two 12-inch 78s. This limitation on the duration of recordings persisted from 1910 until the invention of the LP record, in 1948. In popular music, this time limitation of about 3:30 on a 10-inch 78 rpm record meant that singers usually did not release long pieces on record. One exception is Frank Sinatra's recording of Rodgers and Hammerstein's "Soliloquy", from Carousel, made on May 28, 1946. Because it ran 7:57, longer than both sides of a standard 78 rpm 10-inch record, it was released on Columbia's Masterwork label (the classical division) as two sides of a 12-inch record. The same was true of John Raitt's performance of the song on the original cast album of Carousel, which had been issued on a 78-rpm album set by American Decca in 1945.
German record company Odeon is often said to have pioneered the album in 1909 when it released the Nutcracker Suite by Tchaikovsky on 4 double-sided discs in a specially designed package. (It is not indicated what size the records are.) However, Deutsche Grammophon had produced an album for its complete recording of the opera Carmen in the previous year. The practice of issuing albums does not seem to have been widely taken up by other record companies for many years; however, HMV provided an album, with a pictorial cover, for the 1917 recording of The Mikado (Gilbert & Sullivan).
By about 1910,[note 1] bound collections of empty sleeves with a paperboard or leather cover, similar to a photograph album, were sold as record albums that customers could use to store their records (the term "record album" was printed on some covers). These albums came in both 10-inch and 12-inch sizes. The covers of these bound books were wider and taller than the records inside, allowing the record album to be placed on a shelf upright, like a book, suspending the fragile records above the shelf and protecting them.
In the 1930s, record companies began issuing collections of 78 rpm records by one performer or of one type of music in specially assembled albums, typically with artwork on the front cover and liner notes on the back or inside cover. Most albums included three or four records, with two sides each, making six or eight tunes per album. When the 12-inch vinyl LP era began in 1949, the single record often had the same or similar number of tunes as a typical album of 78s, and was still often referred to as an "album".
For collectable or nostalgia purposes, or for the benefit of higher-quality audio playback provided by the 78 rpm speed with newer vinyl records and their lightweight stylus pickups, a small number of 78 rpm records have been released since the major labels ceased production. One of the first attempts at this was in the 1950s, when inventor Ewing Dunbar Nunn founded the label Audiophile Records, which released, in addition to standard 33 1/3 rpm LPs, 78 rpm-mastered albums that were microgroove and pressed on vinyl (as opposed to traditional 78s, with their shellac composition and wider 3-mil sized grooves). This was done by the label mainly to take advantage of the wider audio frequency response that faster speeds like 78 rpm can provide for vinyl microgroove records, hence the label's name (obviously catering to the audiophiles of the 1950s "hi-fi" era, when stereo gear could provide a much wider range of audio than before). Also in the late 1950s, Bell Records released a few budget-priced 7" microgrooved records at 78 rpm.
In 1968, Reprise planned to release a series of 78 rpm singles from their artists on their label at the time, called the Reprise Speed Series. Only one disc actually saw release, Randy Newman's I Think It's Going to Rain Today, a track from his self-titled debut album (with The Beehive State on the flipside). Reprise did not proceed further with the series due to a lack of sales for the single, and a lack of general interest in the concept. Guitarist & vocalist Leon Redbone released a promotional 78 rpm record in 1978 featuring two songs (Alabama Jubilee and Please Don't Talk About Me When I'm Gone) from his Champagne Charlie album. In 1980 Stiff Records in the United Kingdom issued a 78 by Joe "King" Carrasco containing the songs Buena (Spanish for "good," with the alternate spelling "Bueno" on the label) and Tuff Enuff. Underground comic cartoonist and 78 rpm record collector Robert Crumb released three discs with his Cheap Suit Serenaders in the 1980s.
In the 1990s Rhino Records issued a series of boxed sets of 78 rpm reissues of early rock and roll hits, intended for owners of vintage jukeboxes. This was a disaster because Rhino did not warn customers that their records were made of vinyl, and that the vintage 78 RPM juke boxes were designed with heavy tone arms and steel needles to play the hard shellac records of their time. This failure to warn customers gave the Rhino 78 records a bad reputation,[citation needed] as they were destroyed by old juke boxes and old record players but played very well on newer 78-capable turntables with modern lightweight tone arms and jewel needles.
In 1931, RCA Victor launched the first commercially available vinyl long-playing record, marketed as program-transcription discs. These revolutionary discs were designed for playback at 33 1⁄3 rpm and pressed on a 30 cm diameter flexible plastic disc, with a duration of about ten minutes playing time per side. RCA Victor's early introduction of a long-play disc was a commercial failure for several reasons including the lack of affordable, reliable consumer playback equipment and consumer wariness during the Great Depression. Because of financial hardships that plagued the recording industry during that period (and RCA's own parched revenues), Victor's long-playing records were discontinued by early 1933.
Vinyl's lower surface noise level than shellac was not forgotten, nor was its durability. In the late 1930s, radio commercials and pre-recorded radio programs being sent to disc jockeys started being stamped in vinyl, so they would not break in the mail. In the mid-1940s, special DJ copies of records started being made of vinyl also, for the same reason. These were all 78 rpm. During and after World War II, when shellac supplies were extremely limited, some 78 rpm records were pressed in vinyl instead of shellac, particularly the six-minute 12-inch (30 cm) 78 rpm records produced by V-Disc for distribution to United States troops in World War II. In the 1940s, radio transcriptions, which were usually on 16-inch records, but sometimes 12-inch, were always made of vinyl, but cut at 33 1⁄3 rpm. Shorter transcriptions were often cut at 78 rpm.
Beginning in 1939, Dr. Peter Goldmark and his staff at Columbia Records and at CBS Laboratories undertook efforts to address problems of recording and playing back narrow grooves and developing an inexpensive, reliable consumer playback system. It took about eight years of study, except when it was suspended because of World War II. Finally, the 12-inch (30 cm) Long Play (LP) 33 1⁄3 rpm microgroove record album was introduced by the Columbia Record Company at a New York press conference on June 18, 1948.
Unwilling to accept and license Columbia's system, in February 1949 RCA Victor, in cooperation of its parent, the Radio Corporation of America, released the first 45 rpm single, 7 inches in diameter with a large center hole. The 45 rpm player included a changing mechanism that allowed multiple disks to be stacked, much as a conventional changer handled 78s. The short playing time of a single 45 rpm side meant that long works, such as symphonies, had to be released on multiple 45s instead of a single LP, but RCA claimed that the new high-speed changer rendered side breaks so brief as to be inaudible or inconsequential. Early 45 rpm records were made from either vinyl or polystyrene. They had a playing time of eight minutes.
One early attempt at lengthening the playing time should be mentioned. At least one manufacturer in the early 1920s, World Records, produced records that played at a constant linear velocity, controlled by Noel Pemberton Billing's patented add-on governor device. As these were played from the outside to the inside, the rotational speed of the records increased as reproduction progressed. This action is similar (although in reverse) to that on the modern compact disc and the CLV version of its predecessor, the Philips Laser Disc.
In 1925, 78.26 rpm was chosen as the standard because of the introduction of the electrically powered synchronous turntable motor. This motor ran at 3600 rpm, such that a 46:1 gear ratio would produce 78.26 rpm. In parts of the world that used 50 Hz current, the standard was 77.92 rpm (3,000 rpm with a 77:2 ratio), which was also the speed at which a strobe disc with 77 lines would "stand still" in 50 Hz light (92 lines for 60 Hz). After World War II these records were retroactively known as 78s, to distinguish them from other newer disc record formats. Earlier they were just called records, or when there was a need to distinguish them from cylinders, disc records.
The older 78 format continued to be mass-produced alongside the newer formats using new materials until about 1960 in the U.S., and in a few countries, such as India (where some Beatles recordings were issued on 78), into the 1960s. For example, Columbia Records' last reissue of Frank Sinatra songs on 78 rpm records was an album called Young at Heart, issued November 1, 1954. As late as the 1970s, some children's records were released at the 78 rpm speed. In the United Kingdom, the 78 rpm single lasted longer than in the United States and the 45 rpm took longer to become popular. The 78 rpm was overtaken in popularity by the 45 rpm in the late 1950s, as teenagers became increasingly affluent.
Some of Elvis Presley's early singles on Sun Records might have sold more copies on 78 than on 45. This is because the majority of those sales in 1954–55 were to the "hillbilly" market in the South and Southwestern United States, where replacing the family 78 rpm player with a new 45 rpm player was a luxury few could afford at the time. By the end of 1957, RCA Victor announced that 78s accounted for less than 10% of Presley's singles sales, essentially announcing the death throes of the 78 rpm format. The last Presley single released on 78 in the United States was RCA Victor 20-7410, I Got Stung/One Night (1958), while the last 78 in the UK was RCA 1194, A Mess Of Blues/Girl Of My Best Friend (1960).
After World War II, two new competing formats came onto the market and gradually replaced the standard "78": the 33 1⁄3 rpm (often just referred to as the 33 rpm), and the 45 rpm (see above). The 33 1⁄3 rpm LP (for "long-play") format was developed by Columbia Records and marketed in June 1948. RCA Victor developed the 45 rpm format and marketed it in March 1949, each pursuing their own r&d in secret. Both types of new disc used narrower grooves, intended to be played with smaller stylus—typically 0.001 inches (25 µm) wide, compared to 0.003 inches (76 µm) for a 78—so the new records were sometimes called Microgroove. In the mid-1950s all record companies agreed to a common recording standard called RIAA equalization. Prior to the establishment of the standard each company used its own preferred standard, requiring discriminating listeners to use pre-amplifiers with multiple selectable equalization curves.
Some recordings, such as books for the blind, were pressed at 16 2⁄3 rpm. Prestige Records released jazz records in this format in the late 1950s; for example, two of their Miles Davis albums were paired together in this format. Peter Goldmark, the man who developed the 33 1⁄3 rpm record, developed the Highway Hi-Fi 16 2⁄3 rpm record to be played in Chrysler automobiles, but poor performance of the system and weak implementation by Chrysler and Columbia led to the demise of the 16 2⁄3 rpm records. Subsequently, the 16 2⁄3 rpm speed was used for narrated publications for the blind and visually impaired, and were never widely commercially available, although it was common to see new turntable models with a 16 rpm speed setting produced as late as the 1970s.
The commercial rivalry between RCA Victor and Columbia Records led to RCA Victor's introduction of what it had intended to be a competing vinyl format, the 7-inch (175 mm) 45 rpm disc. For a two-year period from 1948 to 1950, record companies and consumers faced uncertainty over which of these formats would ultimately prevail in what was known as the "War of the Speeds". (See also format war.) In 1949 Capitol and Decca adopted the new LP format and RCA gave in and issued its first LP in January 1950. The 45 rpm size was gaining in popularity, too, and Columbia issued its first 45s in February 1951. By 1954, 200 million 45s had been sold.
Eventually the 12-inch (300 mm) 33 1⁄3 rpm LP prevailed as the predominant format for musical albums, and 10-inch LPs were no longer issued. The last Columbia Records reissue of any Frank Sinatra songs on a 10-inch LP record was an album called Hall of Fame, CL 2600, issued on October 26, 1956, containing six songs, one each by Tony Bennett, Rosemary Clooney, Johnnie Ray, Frank Sinatra, Doris Day, and Frankie Laine. The 10-inch LP however had a longer life in the United Kingdom, where important early British rock and roll albums such as Lonnie Donegan's Lonnie Donegan Showcase and Billy Fury's The Sound of Fury were released in that form. The 7-inch (175 mm) 45 rpm disc or "single" established a significant niche for shorter duration discs, typically containing one item on each side. The 45 rpm discs typically emulated the playing time of the former 78 rpm discs, while the 12-inch LP discs eventually provided up to one half-hour of recorded material per side.
The 45 rpm discs also came in a variety known as extended play (EP), which achieved up to 10–15 minutes play at the expense of attenuating (and possibly compressing) the sound to reduce the width required by the groove. EP discs were cheaper to produce, and were used in cases where unit sales were likely to be more limited or to reissue LP albums on the smaller format for those people who had only 45 rpm players. LP albums could be purchased 1 EP at a time, with four items per EP, or in a boxed set with 3 EPs or 12 items. The large center hole on 45s allows for easier handling by jukebox mechanisms. EPs were generally discontinued by the late 1950s in the U.S. as three- and four-speed record players replaced the individual 45 players. One indication of the decline of the 45 rpm EP is that the last Columbia Records reissue of Frank Sinatra songs on 45 rpm EP records, called Frank Sinatra (Columbia B-2641) was issued on December 7, 1959. The EP lasted considerably longer in Europe, and was a popular format during the 1960s for recordings by artists such as Serge Gainsbourg and the Beatles.
From the mid-1950s through the 1960s, in the U.S. the common home record player or "stereo" (after the introduction of stereo recording) would typically have had these features: a three- or four-speed player (78, 45, 33 1⁄3, and sometimes 16 2⁄3 rpm); with changer, a tall spindle that would hold several records and automatically drop a new record on top of the previous one when it had finished playing, a combination cartridge with both 78 and microgroove styli and a way to flip between the two; and some kind of adapter for playing the 45s with their larger center hole. The adapter could be a small solid circle that fit onto the bottom of the spindle (meaning only one 45 could be played at a time) or a larger adaptor that fit over the entire spindle, permitting a stack of 45s to be played.
RCA 45s were also adapted to the smaller spindle of an LP player with a plastic snap-in insert known as a "spider". These inserts, commissioned by RCA president David Sarnoff and invented by Thomas Hutchison, were prevalent starting in the 1960s, selling in the tens of millions per year during the 45 rpm heyday. In countries outside the U.S., 45s often had the smaller album-sized holes, e.g., Australia and New Zealand, or as in the United Kingdom, especially before the 1970s, the disc had a small hole within a circular central section held only by three or four lands so that it could be easily punched out if desired (typically for use in jukeboxes).
The term "high fidelity" was coined in the 1920s by some manufacturers of radio receivers and phonographs to differentiate their better-sounding products claimed as providing "perfect" sound reproduction. The term began to be used by some audio engineers and consumers through the 1930s and 1940s. After 1949 a variety of improvements in recording and playback technologies, especially stereo recordings, which became widely available in 1958, gave a boost to the "hi-fi" classification of products, leading to sales of individual components for the home such as amplifiers, loudspeakers, phonographs, and tape players. High Fidelity and Audio were two magazines that hi-fi consumers and engineers could read for reviews of playback equipment and recordings.
Stereophonic sound recording, which attempts to provide a more natural listening experience by reproducing the spatial locations of sound sources in the horizontal plane, was the natural extension to monophonic recording, and attracted various alternative engineering attempts. The ultimately dominant "45/45" stereophonic record system was invented by Alan Blumlein of EMI in 1931 and patented the same year. EMI cut the first stereo test discs using the system in 1933 (see Bell Labs Stereo Experiments of 1933) although the system was not exploited commercially until much later.
The development of quadraphonic records was announced in 1971. These recorded four separate sound signals. This was achieved on the two stereo channels by electronic matrixing, where the additional channels were combined into the main signal. When the records were played, phase-detection circuits in the amplifiers were able to decode the signals into four separate channels. There were two main systems of matrixed quadraphonic records produced, confusingly named SQ (by CBS) and QS (by Sansui). They proved commercially unsuccessful, but were an important precursor to later surround-sound systems, as seen in SACD and home cinema today.
A different format, CD-4 (not to be confused with compact disc), by RCA, encoded the front-rear difference information on an ultrasonic carrier, which required a special wideband cartridge to capture it on carefully calibrated pickup arm/turntable combinations. CD-4 was even less successful than the two matrixed formats. (A further problem was that no cutting heads were available that could handle the HF information. That was remedied by cutting at half the speed. Later, the special half-speed cutting heads and equalization techniques were employed to get a wider frequency response in stereo with reduced distortion and greater headroom.)
Under the direction of recording engineer C. Robert Fine, Mercury Records initiated a minimalist single microphone monaural recording technique in 1951. The first record, a Chicago Symphony Orchestra performance of Pictures at an Exhibition, conducted by Rafael Kubelik, was described as "being in the living presence of the orchestra" by The New York Times music critic. The series of records was then named Mercury Living Presence. In 1955, Mercury began three-channel stereo recordings, still based on the principle of the single microphone. The center (single) microphone was of paramount importance, with the two side mics adding depth and space. Record masters were cut directly from a three-track to two-track mixdown console, with all editing of the master tapes done on the original three-tracks. In 1961, Mercury enhanced this technique with three-microphone stereo recordings using 35 mm magnetic film instead of half-inch tape for recording. The greater thickness and width of 35 mm magnetic film prevented tape layer print-through and pre-echo and gained extended frequency range and transient response. The Mercury Living Presence recordings were remastered to CD in the 1990s by the original producer, Wilma Cozart Fine, using the same method of 3-to-2 mix directly to the master recorder.
Through the 1960s, 1970s, and 1980s, various methods to improve the dynamic range of mass-produced records involved highly advanced disc cutting equipment. These techniques, marketed, to name two, as the CBS DisComputer and Teldec Direct Metal Mastering, were used to reduce inner-groove distortion. RCA Victor introduced another system to reduce dynamic range and achieve a groove with less surface noise under the commercial name of Dynagroove. Two main elements were combined: another disk material with less surface noise in the groove and dynamic compression for masking background noise. Sometimes this was called "diaphragming" the source material and not favoured by some music lovers for its unnatural side effects. Both elements were reflected in the brandname of Dynagroove, described elsewhere in more detail. It also used the earlier advanced method of forward-looking control on groove spacing with respect to volume of sound and position on the disk. Lower recorded volume used closer spacing; higher recorded volume used wider spacing, especially with lower frequencies. Also, the higher track density at lower volumes enabled disk recordings to end farther away from the disk center than usual, helping to reduce endtrack distortion even further.
Also in the late 1970s, "direct-to-disc" records were produced, aimed at an audiophile niche market. These completely bypassed the use of magnetic tape in favor of a "purist" transcription directly to the master lacquer disc. Also during this period, half-speed mastered and "original master" records were released, using expensive state-of-the-art technology. A further late 1970s development was the Disco Eye-Cued system used mainly on Motown 12-inch singles released between 1978 and 1980. The introduction, drum-breaks, or choruses of a track were indicated by widely separated grooves, giving a visual cue to DJs mixing the records. The appearance of these records is similar to an LP, but they only contain one track each side.
The mid-1970s saw the introduction of dbx-encoded records, again for the audiophile niche market. These were completely incompatible with standard record playback preamplifiers, relying on the dbx compandor encoding/decoding scheme to greatly increase dynamic range (dbx encoded disks were recorded with the dynamic range compressed by a factor of two in dB: quiet sounds were meant to be played back at low gain and loud sounds were meant to be played back at high gain, via automatic gain control in the playback equipment; this reduced the effect of surface noise on quiet passages). A similar and very short-lived scheme involved using the CBS-developed "CX" noise reduction encoding/decoding scheme.
ELPJ, a Japanese-based company, sells a laser turntable that uses a laser to read vinyl discs optically, without physical contact. The laser turntable eliminates record wear and the possibility of accidental scratches, which degrade the sound, but its expense limits use primarily to digital archiving of analog records, and the laser does not play back colored vinyl or picture discs. Various other laser-based turntables were tried during the 1990s, but while a laser reads the groove very accurately, since it does not touch the record, the dust that vinyl attracts due to static electric charge is not mechanically pushed out of the groove, worsening sound quality in casual use compared to conventional stylus playback.
In some ways similar to the laser turntable is the IRENE scanning machine for disc records, which images with microphotography in two dimensions, invented by a team of physicists at Lawrence Berkeley Laboratories. IRENE will retrieve the information from a laterally modulated monaural grooved sound source without touching the medium itself, but cannot read vertically modulated information. This excludes grooved recordings such as cylinders and some radio transcriptions that feature a hill-and-dale format of recording, and stereophonic or quadraphonic grooved recordings, which utilize a combination of the two as well as supersonic encoding for quadraphonic.
Terms such as "long-play" (LP) and "extended-play" (EP) describe multi-track records that play much longer than the single-item-per-side records, which typically do not go much past four minutes per side. An LP can play for up to 30 minutes per side, though most played for about 22 minutes per side, bringing the total playing time of a typical LP recording to about forty-five minutes. Many pre-1952 LPs, however, played for about 15 minutes per side. The 7-inch 45 rpm format normally contains one item per side but a 7-inch EP could achieve recording times of 10 to 15 minutes at the expense of attenuating and compressing the sound to reduce the width required by the groove. EP discs were generally used to make available tracks not on singles including tracks on LPs albums in a smaller, less expensive format for those who had only 45 rpm players. The large center hole on 7-inch 45 rpm records allows for easier handling by jukebox mechanisms. The term "album", originally used to mean a "book" with liner notes, holding several 78 rpm records each in its own "page" or sleeve, no longer has any relation to the physical format: a single LP record, or nowadays more typically a compact disc.
In March 1949, as RCA released the 45, Columbia released several hundred 7 inch 33 1/3 rpm small spindle hole singles. This format was soon dropped as it became clear that the RCA 45 was the single of choice and the Columbia 12 inch LP would be the 'album' of choice. The first release of the 45 came in seven colors: black 47-xxxx popular series, yellow 47-xxxx juvenile series, green (teal) 48-xxxx country series, deep red 49-xxxx classical series, bright red (cerise) 50-xxxx blues/spiritual series, light blue 51-xxxx international series, dark blue 52-xxxx light classics. All colors were soon dropped in favor of black because of production problems. However, yellow and deep red were continued until about 1952. The first 45 rpm record created for sale was "PeeWee the Piccolo" RCA 47-0147 pressed in yellow translucent vinyl at the Sherman Avenue plant, Indianapolis Dec. 7, 1948, R.O. Price, plant manager.
The normal commercial disc is engraved with two sound-bearing concentric spiral grooves, one on each side, running from the outside edge towards the center. The last part of the spiral meets an earlier part to form a circle. The sound is encoded by fine variations in the edges of the groove that cause a stylus (needle) placed in it to vibrate at acoustic frequencies when the disc is rotated at the correct speed. Generally, the outer and inner parts of the groove bear no intended sound (an exception is Split Enz's Mental Notes).
Towards the center, at the end of the groove, there is another wide-pitched section known as the lead-out. At the very end of this section the groove joins itself to form a complete circle, called the lock groove; when the stylus reaches this point, it circles repeatedly until lifted from the record. On some recordings (for example Sgt. Pepper's Lonely Hearts Club Band by The Beatles, Super Trouper by Abba and Atom Heart Mother by Pink Floyd), the sound continues on the lock groove, which gives a strange repeating effect. Automatic turntables rely on the position or angular velocity of the arm, as it reaches the wider spacing in the groove, to trigger a mechanism that lifts the arm off the record. Precisely because of this mechanism, most automatic turntables are incapable of playing any audio in the lock groove, since they will lift the arm before it reaches that groove.
When auto-changing turntables were commonplace, records were typically pressed with a raised (or ridged) outer edge and a raised label area, allowing records to be stacked onto each other without the delicate grooves coming into contact, reducing the risk of damage. Auto-changers included a mechanism to support a stack of several records above the turntable itself, dropping them one at a time onto the active turntable to be played in order. Many longer sound recordings, such as complete operas, were interleaved across several 10-inch or 12-inch discs for use with auto-changing mechanisms, so that the first disk of a three-disk recording would carry sides 1 and 6 of the program, while the second disk would carry sides 2 and 5, and the third, sides 3 and 4, allowing sides 1, 2, and 3 to be played automatically; then the whole stack reversed to play sides 4, 5, and 6.
New or "virgin" heavy/heavyweight (180–220 g) vinyl is commonly used for modern audiophile vinyl releases in all genres. Many collectors prefer to have heavyweight vinyl albums, which have been reported to have better sound than normal vinyl because of their higher tolerance against deformation caused by normal play. 180 g vinyl is more expensive to produce only because it uses more vinyl. Manufacturing processes are identical regardless of weight. In fact, pressing lightweight records requires more care. An exception is the propensity of 200 g pressings to be slightly more prone to non-fill, when the vinyl biscuit does not sufficiently fill a deep groove during pressing (percussion or vocal amplitude changes are the usual locations of these artifacts). This flaw causes a grinding or scratching sound at the non-fill point.
The "orange peel" effect on vinyl records is caused by worn molds. Rather than having the proper mirror-like finish, the surface of the record will have a texture that looks like orange peel. This introduces noise into the record, particularly in the lower frequency range. With direct metal mastering (DMM), the master disc is cut on a copper-coated disc, which can also have a minor "orange peel" effect on the disc itself. As this "orange peel" originates in the master rather than being introduced in the pressing stage, there is no ill effect as there is no physical distortion of the groove.
Original master discs are created by lathe-cutting: a lathe is used to cut a modulated groove into a blank record. The blank records for cutting used to be cooked up, as needed, by the cutting engineer, using what Robert K. Morrison describes as a "metallic soap," containing lead litharge, ozokerite, barium sulfate, montan wax, stearin and paraffin, among other ingredients. Cut "wax" sound discs would be placed in a vacuum chamber and gold-sputtered to make them electrically conductive for use as mandrels in an electroforming bath, where pressing stamper parts were made. Later, the French company Pyral invented a ready-made blank disc having a thin nitro-cellulose lacquer coating (approximately 7 mils thickness on both sides) that was applied to an aluminum substrate. Lacquer cuts result in an immediately playable, or processable, master record. If vinyl pressings are wanted, the still-unplayed sound disc is used as a mandrel for electroforming nickel records that are used for manufacturing pressing stampers. The electroformed nickel records are mechanically separated from their respective mandrels. This is done with relative ease because no actual "plating" of the mandrel occurs in the type of electrodeposition known as electroforming, unlike with electroplating, in which the adhesion of the new phase of metal is chemical and relatively permanent. The one-molecule-thick coating of silver (that was sprayed onto the processed lacquer sound disc in order to make its surface electrically conductive) reverse-plates onto the nickel record's face. This negative impression disc (having ridges in place of grooves) is known as a nickel master, "matrix" or "father." The "father" is then used as a mandrel to electroform a positive disc known as a "mother". Many mothers can be grown on a single "father" before ridges deteriorate beyond effective use. The "mothers" are then used as mandrels for electroforming more negative discs known as "sons". Each "mother" can be used to make many "sons" before deteriorating. The "sons" are then converted into "stampers" by center-punching a spindle hole (which was lost from the lacquer sound disc during initial electroforming of the "father"), and by custom-forming the target pressing profile. This allows them to be placed in the dies of the target (make and model) record press and, by center-roughing, to facilitate the adhesion of the label, which gets stuck onto the vinyl pressing without any glue. In this way, several million vinyl discs can be produced from a single lacquer sound disc. When only a few hundred discs are required, instead of electroforming a "son" (for each side), the "father" is removed of its silver and converted into a stamper. Production by this latter method, known as the "two-step-process" (as it does not entail creation of "sons" but does involve creation of "mothers," which are used for test playing and kept as "safeties" for electroforming future "sons") is limited to a few hundred vinyl pressings. The pressing count can increase if the stamper holds out and the quality of the vinyl is high. The "sons" made during a "three-step" electroforming make better stampers since they don't require silver removal (which reduces some high fidelity because of etching erasing part of the smallest groove modulations) and also because they have a stronger metal structure than "fathers".
Breakage was very common in the shellac era. In the 1934 John O'Hara novel, Appointment in Samarra, the protagonist "broke one of his most favorites, Whiteman's Lady of the Evening ... He wanted to cry but could not." A poignant moment in J. D. Salinger's 1951 novel The Catcher in the Rye occurs after the adolescent protagonist buys a record for his younger sister but drops it and "it broke into pieces ... I damn-near cried, it made me feel so terrible." A sequence where a school teacher's collection of 78 rpm jazz records is smashed by a group of rebellious students is a key moment in the film Blackboard Jungle.
Vinyl records do not break easily, but the soft material is easily scratched. Vinyl readily acquires a static charge, attracting dust that is difficult to remove completely. Dust and scratches cause audio clicks and pops. In extreme cases, they can cause the needle to skip over a series of grooves, or worse yet, cause the needle to skip backwards, creating a "locked groove" that repeats over and over. This is the origin of the phrase "like a broken record" or "like a scratched record", which is often used to describe a person or thing that continually repeats itself. Locked grooves are not uncommon and were even heard occasionally in radio broadcasts.
Vinyl records can be warped by heat, improper storage, exposure to sunlight, or manufacturing defects such as excessively tight plastic shrinkwrap on the album cover. A small degree of warp was common, and allowing for it was part of the art of turntable and tonearm design. "wow" (once-per-revolution pitch variation) could result from warp, or from a spindle hole that was not precisely centered. Standard practice for LPs was to place the LP in a paper or plastic inner cover. This, if placed within the outer cardboard cover so that the opening was entirely within the outer cover, was said to reduce ingress of dust onto the record surface. Singles, with rare exceptions, had simple paper covers with no inner cover.
A further limitation of the gramophone record is that fidelity steadily declines as playback progresses; there is more vinyl per second available for fine reproduction of high frequencies at the large-diameter beginning of the groove than exist at the smaller-diameters close to the end of the side. At the start of a groove on an LP there are 510 mm of vinyl per second traveling past the stylus while the ending of the groove gives 200–210 mm of vinyl per second — less than half the linear resolution. Distortion towards the end of the side is likely to become more apparent as record wear increases.*
Tonearm skating forces and other perturbations are also picked up by the stylus. This is a form of frequency multiplexing as the control signal (restoring force) used to keep the stylus in the groove is carried by the same mechanism as the sound itself. Subsonic frequencies below about 20 Hz in the audio signal are dominated by tracking effects, which is one form of unwanted rumble ("tracking noise") and merges with audible frequencies in the deep bass range up to about 100 Hz. High fidelity sound equipment can reproduce tracking noise and rumble. During a quiet passage, woofer speaker cones can sometimes be seen to vibrate with the subsonic tracking of the stylus, at frequencies as low as just above 0.5 Hz (the frequency at which a 33 1⁄3 rpm record turns on the turntable; 5⁄9 Hz exactly on an ideal turntable). Another reason for very low frequency material can be a warped disk: its undulations produce frequencies of only a few hertz and present day amplifiers have large power bandwidths. For this reason, many stereo receivers contained a switchable subsonic filter. Some subsonic content is directly out of phase in each channel. If played back on a mono subwoofer system, the noise will cancel, significantly reducing the amount of rumble that is reproduced.
Due to recording mastering and manufacturing limitations, both high and low frequencies were removed from the first recorded signals by various formulae. With low frequencies, the stylus must swing a long way from side to side, requiring the groove to be wide, taking up more space and limiting the playing time of the record. At high frequencies, hiss, pops, and ticks are significant. These problems can be reduced by using equalization to an agreed standard. During recording the amplitude of low frequencies is reduced, thus reducing the groove width required, and the amplitude at high frequencies is increased. The playback equipment boosts bass and cuts treble so as to restore the tonal balance in the original signal; this also reduces the high frequency noise. Thus more music will fit on the record, and noise is reduced.
In 1926 Joseph P. Maxwell and Henry C. Harrison from Bell Telephone Laboratories disclosed that the recording pattern of the Western Electric "rubber line" magnetic disc cutter had a constant velocity characteristic. This meant that as frequency increased in the treble, recording amplitude decreased. Conversely, in the bass as frequency decreased, recording amplitude increased. Therefore, it was necessary to attenuate the bass frequencies below about 250 Hz, the bass turnover point, in the amplified microphone signal fed to the recording head. Otherwise, bass modulation became excessive and overcutting took place into the next record groove. When played back electrically with a magnetic pickup having a smooth response in the bass region, a complementary boost in amplitude at the bass turnover point was necessary. G. H. Miller in 1934 reported that when complementary boost at the turnover point was used in radio broadcasts of records, the reproduction was more realistic and many of the musical instruments stood out in their true form.
West in 1930 and later P. G. A. H. Voigt (1940) showed that the early Wente-style condenser microphones contributed to a 4 to 6 dB midrange brilliance or pre-emphasis in the recording chain. This meant that the electrical recording characteristics of Western Electric licensees such as Columbia Records and Victor Talking Machine Company in the 1925 era had a higher amplitude in the midrange region. Brilliance such as this compensated for dullness in many early magnetic pickups having drooping midrange and treble response. As a result, this practice was the empirical beginning of using pre-emphasis above 1,000 Hz in 78 rpm and 33 1⁄3 rpm records.
Over the years a variety of record equalization practices emerged and there was no industry standard. For example, in Europe recordings for years required playback with a bass turnover setting of 250–300 Hz and a treble roll-off at 10,000 Hz ranging from 0 to −5 dB or more. In the US there were more varied practices and a tendency to use higher bass turnover frequencies such as 500 Hz as well as a greater treble rolloff like −8.5 dB and even more to record generally higher modulation levels on the record.
Evidence from the early technical literature concerning electrical recording suggests that it wasn't until the 1942–1949 period that there were serious efforts to standardize recording characteristics within an industry. Heretofore, electrical recording technology from company to company was considered a proprietary art all the way back to the 1925 Western Electric licensed method used by Columbia and Victor. For example, what Brunswick-Balke-Collender (Brunswick Corporation) did was different from the practices of Victor.
Broadcasters were faced with having to adapt daily to the varied recording characteristics of many sources: various makers of "home recordings" readily available to the public, European recordings, lateral-cut transcriptions, and vertical-cut transcriptions. Efforts were started in 1942 to standardize within the National Association of Broadcasters (NAB), later known as the National Association of Radio and Television Broadcasters (NARTB). The NAB, among other items, issued recording standards in 1949 for laterally and vertically cut records, principally transcriptions. A number of 78 rpm record producers as well as early LP makers also cut their records to the NAB/NARTB lateral standard.
The lateral cut NAB curve was remarkably similar to the NBC Orthacoustic curve that evolved from practices within the National Broadcasting Company since the mid-1930s. Empirically, and not by any formula, it was learned that the bass end of the audio spectrum below 100 Hz could be boosted somewhat to override system hum and turntable rumble noises. Likewise at the treble end beginning at 1,000 Hz, if audio frequencies were boosted by 16 dB at 10,000 Hz the delicate sibilant sounds of speech and high overtones of musical instruments could survive the noise level of cellulose acetate, lacquer/aluminum, and vinyl disc media. When the record was played back using a complementary inverse curve, signal-to-noise ratio was improved and the programming sounded more lifelike.
Ultimately, the New Orthophonic curve was disclosed in a publication by R.C. Moyer of RCA Victor in 1953. He traced RCA Victor characteristics back to the Western Electric "rubber line" recorder in 1925 up to the early 1950s laying claim to long-held recording practices and reasons for major changes in the intervening years. The RCA Victor New Orthophonic curve was within the tolerances for the NAB/NARTB, Columbia LP, and AES curves. It eventually became the technical predecessor to the RIAA curve.
Delicate sounds and fine overtones were mostly lost, because it took a lot of sound energy to vibrate the recording horn diaphragm and cutting mechanism. There were acoustic limitations due to mechanical resonances in both the recording and playback system. Some pictures of acoustic recording sessions show horns wrapped with tape to help mute these resonances. Even an acoustic recording played back electrically on modern equipment sounds like it was recorded through a horn, notwithstanding a reduction in distortion because of the modern playback. Toward the end of the acoustic era, there were many fine examples of recordings made with horns.
Electric recording which developed during the time that early radio was becoming popular (1925) benefited from the microphones and amplifiers used in radio studios. The early electric recordings were reminiscent tonally of acoustic recordings, except there was more recorded bass and treble as well as delicate sounds and overtones cut on the records. This was in spite of some carbon microphones used, which had resonances that colored the recorded tone. The double button carbon microphone with stretched diaphragm was a marked improvement. Alternatively, the Wente style condenser microphone used with the Western Electric licensed recording method had a brilliant midrange and was prone to overloading from sibilants in speech, but generally it gave more accurate reproduction than carbon microphones.
It was not unusual for electric recordings to be played back on acoustic phonographs. The Victor Orthophonic phonograph was a prime example where such playback was expected. In the Orthophonic, which benefited from telephone research, the mechanical pickup head was redesigned with lower resonance than the traditional mica type. Also, a folded horn with an exponential taper was constructed inside the cabinet to provide better impedance matching to the air. As a result, playback of an Orthophonic record sounded like it was coming from a radio.
Eventually, when it was more common for electric recordings to be played back electrically in the 1930s and 1940s, the overall tone was much like listening to a radio of the era. Magnetic pickups became more common and were better designed as time went on, making it possible to improve the damping of spurious resonances. Crystal pickups were also introduced as lower cost alternatives. The dynamic or moving coil microphone was introduced around 1930 and the velocity or ribbon microphone in 1932. Both of these high quality microphones became widespread in motion picture, radio, recording, and public address applications.
Over time, fidelity, dynamic and noise levels improved to the point that it was harder to tell the difference between a live performance in the studio and the recorded version. This was especially true after the invention of the variable reluctance magnetic pickup cartridge by General Electric in the 1940s when high quality cuts were played on well-designed audio systems. The Capehart radio/phonographs of the era with large diameter electrodynamic loudspeakers, though not ideal, demonstrated this quite well with "home recordings" readily available in the music stores for the public to buy.
There were important quality advances in recordings specifically made for radio broadcast. In the early 1930s Bell Telephone Laboratories and Western Electric announced the total reinvention of disc recording: the Western Electric Wide Range System, "The New Voice of Action". The intent of the new Western Electric system was to improve the overall quality of disc recording and playback. The recording speed was 33 1⁄3 rpm, originally used in the Western Electric/ERPI movie audio disc system implemented in the early Warner Brothers' Vitaphone "talkies" of 1927.
The newly invented Western Electric moving coil or dynamic microphone was part of the Wide Range System. It had a flatter audio response than the old style Wente condenser type and didn't require electronics installed in the microphone housing. Signals fed to the cutting head were pre-emphasized in the treble region to help override noise in playback. Groove cuts in the vertical plane were employed rather than the usual lateral cuts. The chief advantage claimed was more grooves per inch that could be crowded together, resulting in longer playback time. Additionally, the problem of inner groove distortion, which plagued lateral cuts, could be avoided with the vertical cut system. Wax masters were made by flowing heated wax over a hot metal disc thus avoiding the microscopic irregularities of cast blocks of wax and the necessity of planing and polishing.
Vinyl pressings were made with stampers from master cuts that were electroplated in vacuo by means of gold sputtering. Audio response was claimed out to 8,000 Hz, later 13,000 Hz, using light weight pickups employing jeweled styli. Amplifiers and cutters both using negative feedback were employed thereby improving the range of frequencies cut and lowering distortion levels. Radio transcription producers such as World Broadcasting System and Associated Music Publishers (AMP) were the dominant licensees of the Western Electric wide range system and towards the end of the 1930s were responsible for two-thirds of the total radio transcription business. These recordings use a bass turnover of 300 Hz and a 10,000 Hz rolloff of −8.5 dB.
The complete technical disclosure of the Columbia LP by Peter C. Goldmark, Rene' Snepvangers and William S. Bachman in 1949 made it possible for a great variety of record companies to get into the business of making long playing records. The business grew quickly and interest spread in high fidelity sound and the do-it-yourself market for pickups, turntables, amplifier kits, loudspeaker enclosure plans, and AM/FM radio tuners. The LP record for longer works, 45 rpm for pop music, and FM radio became high fidelity program sources in demand. Radio listeners heard recordings broadcast and this in turn generated more record sales. The industry flourished.
There is a theory that vinyl records can audibly represent higher frequencies than compact discs. According to Red Book specifications, the compact disc has a frequency response of 20 Hz up to 22,050 Hz, and most CD players measure flat within a fraction of a decibel from at least 20 Hz to 20 kHz at full output. Turntable rumble obscures the low-end limit of vinyl but the upper end can be, with some cartridges, reasonably flat within a few decibels to 30 kHz, with gentle roll-off. Carrier signals of Quad LPs popular in the 1970s were at 30 kHz to be out of the range of human hearing. The average human auditory system is sensitive to frequencies from 20 Hz to a maximum of around 20,000 Hz. The upper and lower frequency limits of human hearing vary per person.
For the first several decades of disc record manufacturing, sound was recorded directly on to the "master disc" at the recording studio. From about 1950 on (earlier for some large record companies, later for some small ones) it became usual to have the performance first recorded on audio tape, which could then be processed and/or edited, and then dubbed on to the master disc. A record cutter would engrave the grooves into the master disc. Early versions of these master discs were soft wax, and later a harder lacquer was used. The mastering process was originally something of an art as the operator had to manually allow for the changes in sound which affected how wide the space for the groove needed to be on each rotation.
As the playing of gramophone records causes gradual degradation of the recording, they are best preserved by transferring them onto other media and playing the records as rarely as possible. They need to be stored on edge, and do best under environmental conditions that most humans would find comfortable. The medium needs to be kept clean, but alcohol should only be used on PVC or optical media, not on 78s.[citation needed] The equipment for playback of certain formats (e.g., 16 and 78 rpm) is manufactured only in small quantities, leading to increased difficulty in finding equipment to play the recordings.
Where old disc recordings are considered to be of artistic or historic interest, from before the era of tape or where no tape master exists, archivists play back the disc on suitable equipment and record the result, typically onto a digital format, which can be copied and manipulated to remove analog flaws without any further damage to the source recording. For example, Nimbus Records uses a specially built horn record player to transfer 78s. Anyone can do this using a standard record player with a suitable pickup, a phono-preamp (pre-amplifier) and a typical personal computer. However, for accurate transfer, professional archivists carefully choose the correct stylus shape and diameter, tracking weight, equalisation curve and other playback parameters and use high-quality analogue-to-digital converters.
Groove recordings, first designed in the final quarter of the 19th century, held a predominant position for nearly a century—withstanding competition from reel-to-reel tape, the 8-track cartridge, and the compact cassette. In 1988, the compact disc surpassed the gramophone record in unit sales. Vinyl records experienced a sudden decline in popularity between 1988 and 1991, when the major label distributors restricted their return policies, which retailers had been relying on to maintain and swap out stocks of relatively unpopular titles. First the distributors began charging retailers more for new product if they returned unsold vinyl, and then they stopped providing any credit at all for returns. Retailers, fearing they would be stuck with anything they ordered, only ordered proven, popular titles that they knew would sell, and devoted more shelf space to CDs and cassettes. Record companies also deleted many vinyl titles from production and distribution, further undermining the availability of the format and leading to the closure of pressing plants. This rapid decline in the availability of records accelerated the format's decline in popularity, and is seen by some as a deliberate ploy to make consumers switch to CDs, which were more profitable for the record companies.
In spite of their flaws, such as the lack of portability, records still have enthusiastic supporters. Vinyl records continue to be manufactured and sold today, especially by independent rock bands and labels, although record sales are considered to be a niche market composed of audiophiles, collectors, and DJs. Old records and out-of-print recordings in particular are in much demand by collectors the world over. (See Record collecting.) Many popular new albums are given releases on vinyl records and older albums are also given reissues, sometimes on audiophile-grade vinyl.
Many electronic dance music and hip hop releases today are still preferred on vinyl; however, digital copies are still widely available. This is because for disc jockeys ("DJs"), vinyl has an advantage over the CD: direct manipulation of the medium. DJ techniques such as slip-cueing, beatmatching, and scratching originated on turntables. With CDs or compact audio cassettes one normally has only indirect manipulation options, e.g., the play, stop, and pause buttons. With a record one can place the stylus a few grooves farther in or out, accelerate or decelerate the turntable, or even reverse its direction, provided the stylus, record player, and record itself are built to withstand it. However, many CDJ and DJ advances, such as DJ software and time-encoded vinyl, now have these capabilities and more.
In 2014 artist Jack White sold 40,000 copies of his second solo release, Lazaretto, on vinyl. The sales of the record beat the largest sales in one week on vinyl since 1991. The sales record was previously held by Pearl Jam's, Vitalogy, which sold 34,000 copies in one week in 1994. In 2014, the sale of vinyl records was the only physical music medium with increasing sales with relation to the previous year. Sales of other mediums including individual digital tracks, digital albums and compact discs have fallen, the latter having the greatest drop-in-sales rate.
Hyderabad (i/ˈhaɪdərəˌbæd/ HY-dər-ə-bad; often /ˈhaɪdrəˌbæd/) is the capital of the southern Indian state of Telangana and de jure capital of Andhra Pradesh.[A] Occupying 650 square kilometres (250 sq mi) along the banks of the Musi River, it has a population of about 6.7 million and a metropolitan population of about 7.75 million, making it the fourth most populous city and sixth most populous urban agglomeration in India. At an average altitude of 542 metres (1,778 ft), much of Hyderabad is situated on hilly terrain around artificial lakes, including Hussain Sagar—predating the city's founding—north of the city centre.
Established in 1591 by Muhammad Quli Qutb Shah, Hyderabad remained under the rule of the Qutb Shahi dynasty for nearly a century before the Mughals captured the region. In 1724, Mughal viceroy Asif Jah I declared his sovereignty and created his own dynasty, known as the Nizams of Hyderabad. The Nizam's dominions became a princely state during the British Raj, and remained so for 150 years, with the city serving as its capital. The Nizami influence can still be seen in the culture of the Hyderabadi Muslims. The city continued as the capital of Hyderabad State after it was brought into the Indian Union in 1948, and became the capital of Andhra Pradesh after the States Reorganisation Act, 1956. Since 1956, Rashtrapati Nilayam in the city has been the winter office of the President of India. In 2014, the newly formed state of Telangana split from Andhra Pradesh and the city became joint capital of the two states, a transitional arrangement scheduled to end by 2025.
Relics of Qutb Shahi and Nizam rule remain visible today, with the Charminar—commissioned by Muhammad Quli Qutb Shah—coming to symbolise Hyderabad. Golconda fort is another major landmark. The influence of Mughlai culture is also evident in the city's distinctive cuisine, which includes Hyderabadi biryani and Hyderabadi haleem. The Qutb Shahis and Nizams established Hyderabad as a cultural hub, attracting men of letters from different parts of the world. Hyderabad emerged as the foremost centre of culture in India with the decline of the Mughal Empire in the mid-19th century, with artists migrating to the city from the rest of the Indian subcontinent. While Hyderabad is losing its cultural pre-eminence, it is today, due to the Telugu film industry, the country's second-largest producer of motion pictures.
Hyderabad was historically known as a pearl and diamond trading centre, and it continues to be known as the City of Pearls. Many of the city's traditional bazaars, including Laad Bazaar, Begum Bazaar and Sultan Bazaar, have remained open for centuries. However, industrialisation throughout the 20th century attracted major Indian manufacturing, research and financial institutions, including Bharat Heavy Electricals Limited, the National Geophysical Research Institute and the Centre for Cellular and Molecular Biology. Special economic zones dedicated to information technology have encouraged companies from across India and around the world to set up operations and the emergence of pharmaceutical and biotechnology industries in the 1990s led to the area's naming as India's "Genome Valley". With an output of US$74 billion, Hyderabad is the fifth-largest contributor to India's overall gross domestic product.
According to John Everett-Heath, the author of Oxford Concise Dictionary of World Place Names, Hyderabad means "Haydar's city" or "lion city", from haydar (lion) and ābād (city). It was named to honour the Caliph Ali Ibn Abi Talib, who was also known as Haydar because of his lion-like valour in battles. Andrew Petersen, a scholar of Islamic architecture, says the city was originally called Baghnagar (city of gardens). One popular theory suggests that Muhammad Quli Qutb Shah, the founder of the city, named it "Bhagyanagar" or "Bhāgnagar" after Bhagmati, a local nautch (dancing) girl with whom he had fallen in love. She converted to Islam and adopted the title Hyder Mahal. The city was renamed Hyderabad in her honour. According to another source, the city was named after Haidar, the son of Quli Qutb Shah.
Archaeologists excavating near the city have unearthed Iron Age sites that may date from 500 BCE. The region comprising modern Hyderabad and its surroundings was known as Golkonda (Golla Konda-"shepherd's hill"), and was ruled by the Chalukya dynasty from 624 CE to 1075 CE. Following the dissolution of the Chalukya empire into four parts in the 11th century, Golkonda came under the control of the Kakatiya dynasty from 1158, whose seat of power was at Warangal, 148 km (92 mi) northeast of modern Hyderabad.
The Kakatiya dynasty was reduced to a vassal of the Khilji dynasty in 1310 after its defeat by Sultan Alauddin Khilji of the Delhi Sultanate. This lasted until 1321, when the Kakatiya dynasty was annexed by Malik Kafur, Allaudin Khilji's general. During this period, Alauddin Khilji took the Koh-i-Noor diamond, which is said to have been mined from the Kollur Mines of Golkonda, to Delhi. Muhammad bin Tughluq succeeded to the Delhi sultanate in 1325, bringing Warangal under the rule of the Tughlaq dynasty until 1347 when Ala-ud-Din Bahman Shah, a governor under bin Tughluq, rebelled against Delhi and established the Bahmani Sultanate in the Deccan Plateau, with Gulbarga, 200 km (124 mi) west of Hyderabad, as its capital. The Bahmani kings ruled the region until 1518 and were the first independent Muslim rulers of the Deccan.
Sultan Quli, a governor of Golkonda, revolted against the Bahmani Sultanate and established the Qutb Shahi dynasty in 1518; he rebuilt the mud-fort of Golconda and named the city "Muhammad nagar". The fifth sultan, Muhammad Quli Qutb Shah, established Hyderabad on the banks of the Musi River in 1591, to avoid the water shortages experienced at Golkonda. During his rule, he had the Charminar and Mecca Masjid built in the city. On 21 September 1687, the Golkonda Sultanate came under the rule of the Mughal emperor Aurangzeb after a year-long siege of the Golkonda fort. The annexed area was renamed Deccan Suba (Deccan province) and the capital was moved from Golkonda to Aurangabad, about 550 km (342 mi) northwest of Hyderabad.
In 1713 Farrukhsiyar, the Mughal emperor, appointed Asif Jah I to be Viceroy of the Deccan, with the title Nizam-ul-Mulk (Administrator of the Realm). In 1724, Asif Jah I defeated Mubariz Khan to establish autonomy over the Deccan Suba, named the region Hyderabad Deccan, and started what came to be known as the Asif Jahi dynasty. Subsequent rulers retained the title Nizam ul-Mulk and were referred to as Asif Jahi Nizams, or Nizams of Hyderabad. The death of Asif Jah I in 1748 resulted in a period of political unrest as his sons, backed by opportunistic neighbouring states and colonial foreign forces, contended for the throne. The accession of Asif Jah II, who reigned from 1762 to 1803, ended the instability. In 1768 he signed the treaty of Masulipatnam, surrendering the coastal region to the East India Company in return for a fixed annual rent.
In 1769 Hyderabad city became the formal capital of the Nizams. In response to regular threats from Hyder Ali (Dalwai of Mysore), Baji Rao I (Peshwa of the Maratha Empire), and Basalath Jung (Asif Jah II's elder brother, who was supported by the Marquis de Bussy-Castelnau), the Nizam signed a subsidiary alliance with the East India Company in 1798, allowing the British Indian Army to occupy Bolarum (modern Secunderabad) to protect the state's borders, for which the Nizams paid an annual maintenance to the British.
After India gained independence, the Nizam declared his intention to remain independent rather than become part of the Indian Union. The Hyderabad State Congress, with the support of the Indian National Congress and the Communist Party of India, began agitating against Nizam VII in 1948. On 17 September that year, the Indian Army took control of Hyderabad State after an invasion codenamed Operation Polo. With the defeat of his forces, Nizam VII capitulated to the Indian Union by signing an Instrument of Accession, which made him the Rajpramukh (Princely Governor) of the state until 31 October 1956. Between 1946 and 1951, the Communist Party of India fomented the Telangana uprising against the feudal lords of the Telangana region. The Constitution of India, which became effective on 26 January 1950, made Hyderabad State one of the part B states of India, with Hyderabad city continuing to be the capital. In his 1955 report Thoughts on Linguistic States, B. R. Ambedkar, then chairman of the Drafting Committee of the Indian Constitution, proposed designating the city of Hyderabad as the second capital of India because of its amenities and strategic central location. Since 1956, the Rashtrapati Nilayam in Hyderabad has been the second official residence and business office of the President of India; the President stays once a year in winter and conducts official business particularly relating to Southern India.
On 1 November 1956 the states of India were reorganised by language. Hyderabad state was split into three parts, which were merged with neighbouring states to form the modern states of Maharashtra, Karnataka and Andhra Pradesh. The nine Telugu- and Urdu-speaking districts of Hyderabad State in the Telangana region were merged with the Telugu-speaking Andhra State to create Andhra Pradesh, with Hyderabad as its capital. Several protests, known collectively as the Telangana movement, attempted to invalidate the merger and demanded the creation of a new Telangana state. Major actions took place in 1969 and 1972, and a third began in 2010. The city suffered several explosions: one at Dilsukhnagar in 2002 claimed two lives; terrorist bombs in May and August 2007 caused communal tension and riots; and two bombs exploded in February 2013. On 30 July 2013 the government of India declared that part of Andhra Pradesh would be split off to form a new Telangana state, and that Hyderabad city would be the capital city and part of Telangana, while the city would also remain the capital of Andhra Pradesh for no more than ten years. On 3 October 2013 the Union Cabinet approved the proposal, and in February 2014 both houses of Parliament passed the Telangana Bill. With the final assent of the President of India in June 2014, Telangana state was formed.
Situated in the southern part of Telangana in southeastern India, Hyderabad is 1,566 kilometres (973 mi) south of Delhi, 699 kilometres (434 mi) southeast of Mumbai, and 570 kilometres (350 mi) north of Bangalore by road. It lies on the banks of the Musi River, in the northern part of the Deccan Plateau. Greater Hyderabad covers 650 km2 (250 sq mi), making it one of the largest metropolitan areas in India. With an average altitude of 542 metres (1,778 ft), Hyderabad lies on predominantly sloping terrain of grey and pink granite, dotted with small hills, the highest being Banjara Hills at 672 metres (2,205 ft). The city has numerous lakes referred to as sagar, meaning "sea". Examples include artificial lakes created by dams on the Musi, such as Hussain Sagar (built in 1562 near the city centre), Osman Sagar and Himayat Sagar. As of 1996, the city had 140 lakes and 834 water tanks (ponds).
Hyderabad has a tropical wet and dry climate (Köppen Aw) bordering on a hot semi-arid climate (Köppen BSh). The annual mean temperature is 26.6 °C (79.9 °F); monthly mean temperatures are 21–33 °C (70–91 °F). Summers (March–June) are hot and humid, with average highs in the mid-to-high 30s Celsius; maximum temperatures often exceed 40 °C (104 °F) between April and June. The coolest temperatures occur in December and January, when the lowest temperature occasionally dips to 10 °C (50 °F). May is the hottest month, when daily temperatures range from 26 to 39 °C (79–102 °F); December, the coldest, has temperatures varying from 14.5 to 28 °C (57–82 °F).
Hyderabad's lakes and the sloping terrain of its low-lying hills provide habitat for an assortment of flora and fauna. The forest region in and around the city encompasses areas of ecological and biological importance, which are preserved in the form of national parks, zoos, mini-zoos and a wildlife sanctuary. Nehru Zoological Park, the city's one large zoo, is the first in India to have a lion and tiger safari park. Hyderabad has three national parks (Mrugavani National Park, Mahavir Harina Vanasthali National Park and Kasu Brahmananda Reddy National Park), and the Manjira Wildlife Sanctuary is about 50 km (31 mi) from the city. Hyderabad's other environmental reserves are: Kotla Vijayabhaskara Reddy Botanical Gardens, Shamirpet Lake, Hussain Sagar, Fox Sagar Lake, Mir Alam Tank and Patancheru Lake, which is home to regional birds and attracts seasonal migratory birds from different parts of the world. Organisations engaged in environmental and wildlife preservation include the Telangana Forest Department, Indian Council of Forestry Research and Education, the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT), the Animal Welfare Board of India, the Blue Cross of Hyderabad and the University of Hyderabad.
The Greater Hyderabad Municipal Corporation (GHMC) oversees the civic infrastructure of the city's 18 "circles", which together encompass 150 municipal wards. Each ward is represented by a corporator, elected by popular vote. The corporators elect the Mayor, who is the titular head of GHMC; executive powers rest with the Municipal Commissioner, appointed by the state government. The GHMC carries out the city's infrastructural work such as building and maintenance of roads and drains, town planning including construction regulation, maintenance of municipal markets and parks, solid waste management, the issuing of birth and death certificates, the issuing of trade licences, collection of property tax, and community welfare services such as mother and child healthcare, and pre-school and non-formal education. The GHMC was formed in April 2007 by merging the Municipal Corporation of Hyderabad (MCH) with 12 municipalities of the Hyderabad, Ranga Reddy and Medak districts covering a total area of 650 km2 (250 sq mi).:3 In the 2016 municipal election, the Telangana Rashtra Samithi formed the majority and the present Mayor is Bonthu Ram Mohan. The Secunderabad Cantonment Board is a civic administration agency overseeing an area of 40.1 km2 (15.5 sq mi),:93 where there are several military camps.:2 The Osmania University campus is administered independently by the university authority.:93
The jurisdictions of the city's administrative agencies are, in ascending order of size: the Hyderabad Police area, Hyderabad district, the GHMC area ("Hyderabad city") and the area under the Hyderabad Metropolitan Development Authority (HMDA). The HMDA is an apolitical urban planning agency that covers the GHMC and its suburbs, extending to 54 mandals in five districts encircling the city. It coordinates the development activities of GHMC and suburban municipalities and manages the administration of bodies such as the Hyderabad Metropolitan Water Supply and Sewerage Board (HMWSSB).
The HMWSSB regulates rainwater harvesting, sewerage services and water supply, which is sourced from several dams located in the suburbs. In 2005, the HMWSSB started operating a 116-kilometre-long (72 mi) water supply pipeline from Nagarjuna Sagar Dam to meet increasing demand. The Telangana Southern Power Distribution Company Limited manages electricity supply. As of October 2014, there were 15 fire stations in the city, operated by the Telangana State Disaster and Fire Response Department. The government-owned India Post has five head post offices and many sub-post offices in Hyderabad, which are complemented by private courier services.
Hyderabad produces around 4,500 tonnes of solid waste daily, which is transported from collection units in Imlibun, Yousufguda and Lower Tank Bund to the dumpsite in Jawaharnagar. Disposal is managed by the Integrated Solid Waste Management project which was started by the GHMC in 2010. Rapid urbanisation and increased economic activity has also led to increased industrial waste, air, noise and water pollution, which is regulated by the Telangana Pollution Control Board (TPCB). The contribution of different sources to air pollution in 2006 was: 20–50% from vehicles, 40–70% from a combination of vehicle discharge and road dust, 10–30% from industrial discharges and 3–10% from the burning of household rubbish. Deaths resulting from atmospheric particulate matter are estimated at 1,700–3,000 each year. Ground water around Hyderabad, which has a hardness of up to 1000 ppm, around three times higher than is desirable, is the main source of drinking water but the increasing population and consequent increase in demand has led to a decline in not only ground water but also river and lake levels. This shortage is further exacerbated by inadequately treated effluent discharged from industrial treatment plants polluting the water sources of the city.
The Commissionerate of Health and Family Welfare is responsible for planning, implementation and monitoring of all facilities related to health and preventive services. As of 2010[update]–11, the city had 50 government hospitals, 300 private and charity hospitals and 194 nursing homes providing around 12,000 hospital beds, fewer than half the required 25,000. For every 10,000 people in the city, there are 17.6 hospital beds, 9 specialist doctors, 14 nurses and 6 physicians. The city also has about 4,000 individual clinics and 500 medical diagnostic centres. Private clinics are preferred by many residents because of the distance to, poor quality of care at and long waiting times in government facilities,:60–61 despite the high proportion of the city's residents being covered by government health insurance: 24% according to a National Family Health Survey in 2005.:41 As of 2012[update], many new private hospitals of various sizes were opened or being built. Hyderabad also has outpatient and inpatient facilities that use Unani, homeopathic and Ayurvedic treatments.
In the 2005 National Family Health Survey, it was reported that the city's total fertility rate is 1.8,:47 which is below the replacement rate. Only 61% of children had been provided with all basic vaccines (BCG, measles and full courses of polio and DPT), fewer than in all other surveyed cities except Meerut.:98 The infant mortality rate was 35 per 1,000 live births, and the mortality rate for children under five was 41 per 1,000 live births.:97 The survey also reported that a third of women and a quarter of men are overweight or obese, 49% of children below 5 years are anaemic, and up to 20% of children are underweight,:44, 55–56 while more than 2% of women and 3% of men suffer from diabetes.:57
When the GHMC was created in 2007, the area occupied by the municipality increased from 175 km2 (68 sq mi) to 650 km2 (250 sq mi). Consequently, the population increased by 87%, from 3,637,483 in the 2001 census to 6,809,970 in the 2011 census, 24% of which are migrants from elsewhere in India,:2 making Hyderabad the nation's fourth most populous city. As of 2011[update], the population density is 18,480/km2 (47,900/sq mi). At the same 2011 census, the Hyderabad Urban Agglomeration had a population of 7,749,334, making it the sixth most populous urban agglomeration in the country. The population of the Hyderabad urban agglomeration has since been estimated by electoral officials to be 9.1 million as of early 2013 but is expected to exceed 10 million by the end of the year. There are 3,500,802 male and 3,309,168 female citizens—a sex ratio of 945 females per 1000 males, higher than the national average of 926 per 1000. Among children aged 0–6 years, 373,794 are boys and 352,022 are girls—a ratio of 942 per 1000. Literacy stands at 82.96% (male 85.96%; female 79.79%), higher than the national average of 74.04%. The socio-economic strata consist of 20% upper class, 50% middle class and 30% working class.
Referred to as "Hyderabadi", the residents of Hyderabad are predominantly Telugu and Urdu speaking people, with minority Bengali, Gujarati (including Memon), Kannada (including Nawayathi), Malayalam, Marathi, Marwari, Odia, Punjabi, Tamil and Uttar Pradeshi communities. Hyderabad is home to a unique dialect of Urdu called Hyderabadi Urdu, which is a type of Dakhini, and is the mother tongue of most Hyderabadi Muslims, a unique community who owe much of their history, language, cuisine, and culture to Hyderabad, and the various dynasties who previously ruled. Hadhrami Arabs, African Arabs, Armenians, Abyssinians, Iranians, Pathans and Turkish people are also present; these communities, of which the Hadhrami are the largest, declined after Hyderabad State became part of the Indian Union, as they lost the patronage of the Nizams.
In the greater metropolitan area, 13% of the population live below the poverty line. According to a 2012 report submitted by GHMC to the World Bank, Hyderabad has 1,476 slums with a total population of 1.7 million, of whom 66% live in 985 slums in the "core" of the city (the part that formed Hyderabad before the April 2007 expansion) and the remaining 34% live in 491 suburban tenements. About 22% of the slum-dwelling households had migrated from different parts of India in the last decade of the 20th century, and 63% claimed to have lived in the slums for more than 10 years.:55 Overall literacy in the slums is 60–80% and female literacy is 52–73%. A third of the slums have basic service connections, and the remainder depend on general public services provided by the government. There are 405 government schools, 267 government aided schools, 175 private schools and 528 community halls in the slum areas.:70 According to a 2008 survey by the Centre for Good Governance, 87.6% of the slum-dwelling households are nuclear families, 18% are very poor, with an income up to ₹20000 (US$300) per annum, 73% live below the poverty line (a standard poverty line recognised by the Andhra Pradesh Government is ₹24000 (US$360) per annum), 27% of the chief wage earners (CWE) are casual labour and 38% of the CWE are illiterate. About 3.72% of the slum children aged 5–14 do not go to school and 3.17% work as child labour, of whom 64% are boys and 36% are girls. The largest employers of child labour are street shops and construction sites. Among the working children, 35% are engaged in hazardous jobs.:59
Many historic and tourist sites lie in south central Hyderabad, such as the Charminar, the Mecca Masjid, the Salar Jung Museum, the Nizam's Museum, the Falaknuma Palace, and the traditional retail corridor comprising the Pearl Market, Laad Bazaar and Madina Circle. North of the river are hospitals, colleges, major railway stations and business areas such as Begum Bazaar, Koti, Abids, Sultan Bazaar and Moazzam Jahi Market, along with administrative and recreational establishments such as the Reserve Bank of India, the Telangana Secretariat, the Hyderabad Mint, the Telangana Legislature, the Public Gardens, the Nizam Club, the Ravindra Bharathi, the State Museum, the Birla Temple and the Birla Planetarium.
North of central Hyderabad lie Hussain Sagar, Tank Bund Road, Rani Gunj and the Secunderabad Railway Station. Most of the city's parks and recreational centres, such as Sanjeevaiah Park, Indira Park, Lumbini Park, NTR Gardens, the Buddha statue and Tankbund Park are located here. In the northwest part of the city there are upscale residential and commercial areas such as Banjara Hills, Jubilee Hills, Begumpet, Khairatabad and Miyapur. The northern end contains industrial areas such as Sanathnagar, Moosapet, Balanagar, Patancheru and Chanda Nagar. The northeast end is dotted with residential areas. In the eastern part of the city lie many defence research centres and Ramoji Film City. The "Cyberabad" area in the southwest and west of the city has grown rapidly since the 1990s. It is home to information technology and bio-pharmaceutical companies and to landmarks such as Hyderabad Airport, Osman Sagar, Himayath Sagar and Kasu Brahmananda Reddy National Park.
Heritage buildings constructed during the Qutb Shahi and Nizam eras showcase Indo-Islamic architecture influenced by Medieval, Mughal and European styles. After the 1908 flooding of the Musi River, the city was expanded and civic monuments constructed, particularly during the rule of Mir Osman Ali Khan (the VIIth Nizam), whose patronage of architecture led to him being referred to as the maker of modern Hyderabad. In 2012, the government of India declared Hyderabad the first "Best heritage city of India".
Qutb Shahi architecture of the 16th and early 17th centuries followed classical Persian architecture featuring domes and colossal arches. The oldest surviving Qutb Shahi structure in Hyderabad is the ruins of Golconda fort built in the 16th century. The Charminar, Mecca Masjid, Charkaman and Qutb Shahi tombs are other existing structures of this period. Among these the Charminar has become an icon of the city; located in the centre of old Hyderabad, it is a square structure with sides 20 m (66 ft) long and four grand arches each facing a road. At each corner stands a 56 m (184 ft)-high minaret. Most of the historical bazaars that still exist were constructed on the street north of Charminar towards Golconda fort. The Charminar, Qutb Shahi tombs and Golconda fort are considered to be monuments of national importance in India; in 2010 the Indian government proposed that the sites be listed for UNESCO World Heritage status.:11–18
Among the oldest surviving examples of Nizam architecture in Hyderabad is the Chowmahalla Palace, which was the seat of royal power. It showcases a diverse array of architectural styles, from the Baroque Harem to its Neoclassical royal court. The other palaces include Falaknuma Palace (inspired by the style of Andrea Palladio), Purani Haveli, King Kothi and Bella Vista Palace all of which were built at the peak of Nizam rule in the 19th century. During Mir Osman Ali Khan's rule, European styles, along with Indo-Islamic, became prominent. These styles are reflected in the Falaknuma Palace and many civic monuments such as the Hyderabad High Court, Osmania Hospital, Osmania University, the State Central Library, City College, the Telangana Legislature, the State Archaeology Museum, Jubilee Hall, and Hyderabad and Kachiguda railway stations. Other landmarks of note are Paigah Palace, Asman Garh Palace, Basheer Bagh Palace, Errum Manzil and the Spanish Mosque, all constructed by the Paigah family.:16–17
Hyderabad is the largest contributor to the gross domestic product (GDP), tax and other revenues, of Telangana, and the sixth largest deposit centre and fourth largest credit centre nationwide, as ranked by the Reserve Bank of India (RBI) in June 2012. Its US$74 billion GDP made it the fifth-largest contributor city to India's overall GDP in 2011–12. Its per capita annual income in 2011 was ₹44300 (US$660). As of 2006[update], the largest employers in the city were the governments of Andhra Pradesh (113,098 employees) and India (85,155). According to a 2005 survey, 77% of males and 19% of females in the city were employed. The service industry remains dominant in the city, and 90% of the employed workforce is engaged in this sector.
Hyderabad's role in the pearl trade has given it the name "City of Pearls" and up until the 18th century, the city was also the only global trading centre for large diamonds. Industrialisation began under the Nizams in the late 19th century, helped by railway expansion that connected the city with major ports. From the 1950s to the 1970s, Indian enterprises, such as Bharat Heavy Electricals Limited (BHEL), Nuclear Fuel Complex (NFC), National Mineral Development Corporation (NMDC), Bharat Electronics (BEL), Electronics Corporation of India Limited (ECIL), Defence Research and Development Organisation (DRDO), Hindustan Aeronautics Limited (HAL), Centre for Cellular and Molecular Biology (CCMB), Centre for DNA Fingerprinting and Diagnostics (CDFD), State Bank of Hyderabad (SBH) and Andhra Bank (AB) were established in the city. The city is home to Hyderabad Securities formerly known as Hyderabad Stock Exchange (HSE), and houses the regional office of the Securities and Exchange Board of India (SEBI). In 2013, the Bombay Stock Exchange (BSE) facility in Hyderabad was forecast to provide operations and transactions services to BSE-Mumbai by the end of 2014. The growth of the financial services sector has helped Hyderabad evolve from a traditional manufacturing city to a cosmopolitan industrial service centre. Since the 1990s, the growth of information technology (IT), IT-enabled services (ITES), insurance and financial institutions has expanded the service sector, and these primary economic activities have boosted the ancillary sectors of trade and commerce, transport, storage, communication, real estate and retail.
The establishment of Indian Drugs and Pharmaceuticals Limited (IDPL), a public sector undertaking, in 1961 was followed over the decades by many national and global companies opening manufacturing and research facilities in the city. As of 2010[update], the city manufactured one third of India's bulk drugs and 16% of biotechnology products, contributing to its reputation as "India's pharmaceutical capital" and the "Genome Valley of India". Hyderabad is a global centre of information technology, for which it is known as Cyberabad (Cyber City). As of 2013[update], it contributed 15% of India's and 98% of Andhra Pradesh's exports in IT and ITES sectors and 22% of NASSCOM's total membership is from the city. The development of HITEC City, a township with extensive technological infrastructure, prompted multinational companies to establish facilities in Hyderabad. The city is home to more than 1300 IT and ITES firms, including global conglomerates such as Microsoft (operating its largest R&D campus outside the US), Google, IBM, Yahoo!, Dell, Facebook,:3 and major Indian firms including Tech Mahindra, Infosys, Tata Consultancy Services (TCS), Polaris and Wipro.:3 In 2009 the World Bank Group ranked the city as the second best Indian city for doing business. The city and its suburbs contain the highest number of special economic zones of any Indian city.
Like the rest of India, Hyderabad has a large informal economy that employs 30% of the labour force.:71 According to a survey published in 2007, it had 40–50,000 street vendors, and their numbers were increasing.:9 Among the street vendors, 84% are male and 16% female,:12 and four fifths are "stationary vendors" operating from a fixed pitch, often with their own stall.:15–16 Most are financed through personal savings; only 8% borrow from moneylenders.:19 Vendor earnings vary from ₹50 (74¢ US) to ₹800 (US$12) per day.:25 Other unorganised economic sectors include dairy, poultry farming, brick manufacturing, casual labour and domestic help. Those involved in the informal economy constitute a major portion of urban poor.:71
Hyderabad emerged as the foremost centre of culture in India with the decline of the Mughal Empire. After the fall of Delhi in 1857, the migration of performing artists to the city particularly from the north and west of the Indian sub continent, under the patronage of the Nizam, enriched the cultural milieu. This migration resulted in a mingling of North and South Indian languages, cultures and religions, which has since led to a co-existence of Hindu and Muslim traditions, for which the city has become noted.:viii A further consequence of this north–south mix is that both Telugu and Urdu are official languages of Telangana. The mixing of religions has also resulted in many festivals being celebrated in Hyderabad such as Ganesh Chaturthi, Diwali and Bonalu of Hindu tradition and Eid ul-Fitr and Eid al-Adha by Muslims.
In the past, Qutb Shahi rulers and Nizams attracted artists, architects and men of letters from different parts of the world through patronage. The resulting ethnic mix popularised cultural events such as mushairas (poetic symposia). The Qutb Shahi dynasty particularly encouraged the growth of Deccani Urdu literature leading to works such as the Deccani Masnavi and Diwan poetry, which are among the earliest available manuscripts in Urdu. Lazzat Un Nisa, a book compiled in the 15th century at Qutb Shahi courts, contains erotic paintings with diagrams for secret medicines and stimulants in the eastern form of ancient sexual arts. The reign of the Nizams saw many literary reforms and the introduction of Urdu as a language of court, administration and education. In 1824, a collection of Urdu Ghazal poetry, named Gulzar-e-Mahlaqa, authored by Mah Laqa Bai—the first female Urdu poet to produce a Diwan—was published in Hyderabad.
Hyderabad has continued with these traditions in its annual Hyderabad Literary Festival, held since 2010, showcasing the city's literary and cultural creativity. Organisations engaged in the advancement of literature include the Sahitya Akademi, the Urdu Academy, the Telugu Academy, the National Council for Promotion of Urdu Language, the Comparative Literature Association of India, and the Andhra Saraswata Parishad. Literary development is further aided by state institutions such as the State Central Library, the largest public library in the state which was established in 1891, and other major libraries including the Sri Krishna Devaraya Andhra Bhasha Nilayam, the British Library and the Sundarayya Vignana Kendram.
South Indian music and dances such as the Kuchipudi and Bharatanatyam styles are popular in the Deccan region. As a result of their culture policies, North Indian music and dance gained popularity during the rule of the Mughals and Nizams, and it was also during their reign that it became a tradition among the nobility to associate themselves with tawaif (courtesans). These courtesans were revered as the epitome of etiquette and culture, and were appointed to teach singing, poetry and classical dance to many children of the aristocracy. This gave rise to certain styles of court music, dance and poetry. Besides western and Indian popular music genres such as filmi music, the residents of Hyderabad play city-based marfa music, dholak ke geet (household songs based on local Folklore), and qawwali, especially at weddings, festivals and other celebratory events. The state government organises the Golconda Music and Dance Festival, the Taramati Music Festival and the Premavathi Dance Festival to further encourage the development of music.
Although the city is not particularly noted for theatre and drama, the state government promotes theatre with multiple programmes and festivals in such venues as the Ravindra Bharati, Shilpakala Vedika and Lalithakala Thoranam. Although not a purely music oriented event, Numaish, a popular annual exhibition of local and national consumer products, does feature some musical performances. The city is home to the Telugu film industry, popularly known as Tollywood and as of 2012[update], produces the second largest number of films in India behind Bollywood. Films in the local Hyderabadi dialect are also produced and have been gaining popularity since 2005. The city has also hosted international film festivals such as the International Children's Film Festival and the Hyderabad International Film Festival. In 2005, Guinness World Records declared Ramoji Film City to be the world's largest film studio.
The region is well known for its Golconda and Hyderabad painting styles which are branches of Deccani painting. Developed during the 16th century, the Golconda style is a native style blending foreign techniques and bears some similarity to the Vijayanagara paintings of neighbouring Mysore. A significant use of luminous gold and white colours is generally found in the Golconda style. The Hyderabad style originated in the 17th century under the Nizams. Highly influenced by Mughal painting, this style makes use of bright colours and mostly depicts regional landscape, culture, costumes and jewellery.
Although not a centre for handicrafts itself, the patronage of the arts by the Mughals and Nizams attracted artisans from the region to Hyderabad. Such crafts include: Bidriware, a metalwork handicraft from neighbouring Karnataka, which was popularised during the 18th century and has since been granted a Geographical Indication (GI) tag under the auspices of the WTO act; and Zari and Zardozi, embroidery works on textile that involve making elaborate designs using gold, silver and other metal threads. Another example of a handicraft drawn to Hyderabad is Kalamkari, a hand-painted or block-printed cotton textile that comes from cities in Andhra Pradesh. This craft is distinguished in having both a Hindu style, known as Srikalahasti and entirely done by hand, and an Islamic style, known as Machilipatnam that uses both hand and block techniques. Examples of Hyderabad's arts and crafts are housed in various museums including the Salar Jung Museum (housing "one of the largest one-man-collections in the world"), the AP State Archaeology Museum, the Nizam Museum, the City Museum and the Birla Science Museum.
Hyderabadi cuisine comprises a broad repertoire of rice, wheat and meat dishes and the skilled use of various spices. Hyderabadi biryani and Hyderabadi haleem, with their blend of Mughlai and Arab cuisines, have become iconic dishes of India. Hyderabadi cuisine is highly influenced by Mughlai and to some extent by French, Arabic, Turkish, Iranian and native Telugu and Marathwada cuisines. Other popular native dishes include nihari, chakna, baghara baingan and the desserts qubani ka meetha, double ka meetha and kaddu ki kheer (a sweet porridge made with sweet gourd).
One of Hyderabad's earliest newspapers, The Deccan Times, was established in the 1780s. In modern times, the major Telugu dailies published in Hyderabad are Eenadu, Andhra Jyothy, Sakshi and Namaste Telangana, while the major English papers are The Times of India, The Hindu and The Deccan Chronicle. The major Urdu papers include The Siasat Daily, The Munsif Daily and Etemaad. Many coffee table magazines, professional magazines and research journals are also regularly published. The Secunderabad Cantonment Board established the first radio station in Hyderabad State around 1919. Deccan Radio was the first radio public broadcast station in the city starting on 3 February 1935, with FM broadcasting beginning in 2000. The available channels in Hyderabad include All India Radio, Radio Mirchi, Radio City, Red FM and Big FM.
Television broadcasting in Hyderabad began in 1974 with the launch of Doordarshan, the Government of India's public service broadcaster, which transmits two free-to-air terrestrial television channels and one satellite channel. Private satellite channels started in July 1992 with the launch of Star TV. Satellite TV channels are accessible via cable subscription, direct-broadcast satellite services or internet-based television. Hyderabad's first dial-up internet access became available in the early 1990s and was limited to software development companies. The first public internet access service began in 1995, with the first private sector internet service provider (ISP) starting operations in 1998. In 2015, high-speed public WiFi was introduced in parts of the city.
Public and private schools in Hyderabad are governed by the Central Board of Secondary Education and follow a "10+2+3" plan. About two-thirds of pupils attend privately run institutions. Languages of instruction include English, Hindi, Telugu and Urdu. Depending on the institution, students are required to sit the Secondary School Certificate or the Indian Certificate of Secondary Education. After completing secondary education, students enroll in schools or junior colleges with a higher secondary facility. Admission to professional graduation colleges in Hyderabad, many of which are affiliated with either Jawaharlal Nehru Technological University Hyderabad (JNTUH) or Osmania University (OU), is through the Engineering Agricultural and Medical Common Entrance Test (EAM-CET).
There are 13 universities in Hyderabad: two private universities, two deemed universities, six state universities and three central universities. The central universities are the University of Hyderabad, Maulana Azad National Urdu University and the English and Foreign Languages University. Osmania University, established in 1918, was the first university in Hyderabad and as of 2012[update] is India's second most popular institution for international students. The Dr. B. R. Ambedkar Open University, established in 1982, is the first distance learning open university in India.
Hyderabad is also home to a number of centres specialising in particular fields such as biomedical sciences, biotechnology and pharmaceuticals, such as the National Institute of Pharmaceutical Education and Research (NIPER) and National Institute of Nutrition (NIN). Hyderabad has five major medical schools—Osmania Medical College, Gandhi Medical College, Nizam's Institute of Medical Sciences, Deccan College of Medical Sciences and Shadan Institute of Medical Sciences—and many affiliated teaching hospitals. The Government Nizamia Tibbi College is a college of Unani medicine. Hyderabad is also the headquarters of the Indian Heart Association, a non-profit foundation for cardiovascular education.
Institutes in Hyderabad include the National Institute of Rural Development, the Indian School of Business, the Institute of Public Enterprise, the Administrative Staff College of India and the Sardar Vallabhbhai Patel National Police Academy. Technical and engineering schools include the International Institute of Information Technology, Hyderabad (IIITH), Birla Institute of Technology and Science, Pilani – Hyderabad (BITS Hyderabad) and Indian Institute of Technology, Hyderabad (IIT-H) as well as agricultural engineering institutes such as the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and the Acharya N. G. Ranga Agricultural University. Hyderabad also has schools of fashion design including Raffles Millennium International, NIFT Hyderabad and Wigan and Leigh College. The National Institute of Design, Hyderabad (NID-H), will offer undergraduate and postgraduate courses from 2015.
The most popular sports played in Hyderabad are cricket and association football. At the professional level, the city has hosted national and international sports events such as the 2002 National Games of India, the 2003 Afro-Asian Games, the 2004 AP Tourism Hyderabad Open women's tennis tournament, the 2007 Military World Games, the 2009 World Badminton Championships and the 2009 IBSF World Snooker Championship. The city hosts a number of venues suitable for professional competition such as the Swarnandhra Pradesh Sports Complex for field hockey, the G. M. C. Balayogi Stadium in Gachibowli for athletics and football, and for cricket, the Lal Bahadur Shastri Stadium and Rajiv Gandhi International Cricket Stadium, home ground of the Hyderabad Cricket Association. Hyderabad has hosted many international cricket matches, including matches in the 1987 and the 1996 ICC Cricket World Cups. The Hyderabad cricket team represents the city in the Ranji Trophy—a first-class cricket tournament among India's states and cities. Hyderabad is also home to the Indian Premier League franchise Sunrisers Hyderabad. A previous franchise was the Deccan Chargers, which won the 2009 Indian Premier League held in South Africa.
During British rule, Secunderabad became a well-known sporting centre and many race courses, parade grounds and polo fields were built.:18 Many elite clubs formed by the Nizams and the British such as the Secunderabad Club, the Nizam Club and the Hyderabad Race Club, which is known for its horse racing especially the annual Deccan derby, still exist. In more recent times, motorsports has become popular with the Andhra Pradesh Motor Sports Club organising popular events such as the Deccan ¼ Mile Drag, TSD Rallies and 4x4 off-road rallying.
International-level sportspeople from Hyderabad include: cricketers Ghulam Ahmed, M. L. Jaisimha, Mohammed Azharuddin, V. V. S. Laxman, Venkatapathy Raju, Shivlal Yadav, Arshad Ayub, Syed Abid Ali and Noel David; football players Syed Abdul Rahim, Syed Nayeemuddin and Shabbir Ali; tennis player Sania Mirza; badminton players S. M. Arif, Pullela Gopichand, Saina Nehwal, P. V. Sindhu, Jwala Gutta and Chetan Anand; hockey players Syed Mohammad Hadi and Mukesh Kumar; rifle shooters Gagan Narang and Asher Noria and bodybuilder Mir Mohtesham Ali Khan.
The most commonly used forms of medium distance transport in Hyderabad include government owned services such as light railways and buses, as well as privately operated taxis and auto rickshaws. Bus services operate from the Mahatma Gandhi Bus Station in the city centre and carry over 130 million passengers daily across the entire network.:76 Hyderabad's light rail transportation system, the Multi-Modal Transport System (MMTS), is a three line suburban rail service used by over 160,000 passengers daily. Complementing these government services are minibus routes operated by Setwin (Society for Employment Promotion & Training in Twin Cities). Intercity rail services also operate from Hyderabad; the main, and largest, station is Secunderabad Railway Station, which serves as Indian Railways' South Central Railway zone headquarters and a hub for both buses and MMTS light rail services connecting Secunderabad and Hyderabad. Other major railway stations in Hyderabad are Hyderabad Deccan Station, Kachiguda Railway Station, Begumpet Railway Station, Malkajgiri Railway Station and Lingampally Railway Station. The Hyderabad Metro, a new rapid transit system, is to be added to the existing public transport infrastructure and is scheduled to operate three lines by 2015.
As of 2012[update], there are over 3.5 million vehicles operating in the city, of which 74% are two-wheelers, 15% cars and 3% three-wheelers. The remaining 8% include buses, goods vehicles and taxis. The large number of vehicles coupled with relatively low road coverage—roads occupy only 9.5% of the total city area:79—has led to widespread traffic congestion especially since 80% of passengers and 60% of freight are transported by road.:3 The Inner Ring Road, the Outer Ring Road, the Hyderabad Elevated Expressway, the longest flyover in India, and various interchanges, overpasses and underpasses were built to ease the congestion. Maximum speed limits within the city are 50 km/h (31 mph) for two-wheelers and cars, 35 km/h (22 mph) for auto rickshaws and 40 km/h (25 mph) for light commercial vehicles and buses.
Hyderabad sits at the junction of three National Highways linking it to six other states: NH-7 runs 2,369 km (1,472 mi) from Varanasi, Uttar Pradesh, in the north to Kanyakumari, Tamil Nadu, in the south; NH-9, runs 841 km (523 mi) east-west between Machilipatnam, Andhra Pradesh, and Pune, Maharashtra; and the 280 km (174 mi) NH-163 links Hyderabad to Bhopalpatnam, Chhattisgarh NH-765 links Hyderabad to Srisailam. Five state highways, SH-1, SH-2, SH-4, SH-5 and SH-6, either start from, or pass through, Hyderabad.:58
The original Latin word "universitas" refers in general to "a number of persons associated into one body, a society, company, community, guild, corporation, etc." At the time of the emergence of urban town life and medieval guilds, specialised "associations of students and teachers with collective legal rights usually guaranteed by charters issued by princes, prelates, or the towns in which they were located" came to be denominated by this general term. Like other guilds, they were self-regulating and determined the qualifications of their members.
An important idea in the definition of a university is the notion of academic freedom. The first documentary evidence of this comes from early in the life of the first university. The University of Bologna adopted an academic charter, the Constitutio Habita, in 1158 or 1155, which guaranteed the right of a traveling scholar to unhindered passage in the interests of education. Today this is claimed as the origin of "academic freedom". This is now widely recognised internationally - on 18 September 1988, 430 university rectors signed the Magna Charta Universitatum, marking the 900th anniversary of Bologna's foundation. The number of universities signing the Magna Charta Universitatum continues to grow, drawing from all parts of the world.
European higher education took place for hundreds of years in Christian cathedral schools or monastic schools (scholae monasticae), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century. The earliest universities were developed under the aegis of the Latin Church by papal bull as studia generalia and perhaps from cathedral schools. It is possible, however, that the development of cathedral schools into universities was quite rare, with the University of Paris being an exception. Later they were also founded by Kings (University of Naples Federico II, Charles University in Prague, Jagiellonian University in Kraków) or municipal administrations (University of Cologne, University of Erfurt). In the early medieval period, most new universities were founded from pre-existing schools, usually when these schools were deemed to have become primarily sites of higher education. Many historians state that universities and cathedral schools were a continuation of the interest in learning promoted by monasteries.
All over Europe rulers and city governments began to create universities to satisfy a European thirst for knowledge, and the belief that society would benefit from the scholarly expertise generated from these institutions. Princes and leaders of city governments perceived the potential benefits of having a scholarly expertise develop with the ability to address difficult problems and achieve desired ends. The emergence of humanism was essential to this understanding of the possible utility of universities as well as the revival of interest in knowledge gained from ancient Greek texts.
The rediscovery of Aristotle's works–more than 3000 pages of it would eventually be translated –fuelled a spirit of inquiry into natural processes that had already begun to emerge in the 12th century. Some scholars believe that these works represented one of the most important document discoveries in Western intellectual history. Richard Dales, for instance, calls the discovery of Aristotle's works "a turning point in the history of Western thought." After Aristotle re-emerged, a community of scholars, primarily communicating in Latin, accelerated the process and practice of attempting to reconcile the thoughts of Greek antiquity, and especially ideas related to understanding the natural world, with those of the church. The efforts of this "scholasticism" were focused on applying Aristotelian logic and thoughts about natural processes to biblical passages and attempting to prove the viability of those passages through reason. This became the primary mission of lecturers, and the expectation of students.
The university culture developed differently in northern Europe than it did in the south, although the northern (primarily Germany, France and Great Britain) and southern universities (primarily Italy) did have many elements in common. Latin was the language of the university, used for all texts, lectures, disputations and examinations. Professors lectured on the books of Aristotle for logic, natural philosophy, and metaphysics; while Hippocrates, Galen, and Avicenna were used for medicine. Outside of these commonalities, great differences separated north and south, primarily in subject matter. Italian universities focused on law and medicine, while the northern universities focused on the arts and theology. There were distinct differences in the quality of instruction in these areas which were congruent with their focus, so scholars would travel north or south based on their interests and means. There was also a difference in the types of degrees awarded at these universities. English, French and German universities usually awarded bachelor's degrees, with the exception of degrees in theology, for which the doctorate was more common. Italian universities awarded primarily doctorates. The distinction can be attributed to the intent of the degree holder after graduation – in the north the focus tended to be on acquiring teaching positions, while in the south students often went on to professional positions. The structure of northern universities tended to be modeled after the system of faculty governance developed at the University of Paris. Southern universities tended to be patterned after the student-controlled model begun at the University of Bologna. Among the southern universities, a further distinction has been noted between those of northern Italy, which followed the pattern of Bologna as a "self-regulating, independent corporation of scholars" and those of southern Italy and Iberia, which were "founded by royal and imperial charter to serve the needs of government."
Their endowment by a prince or monarch and their role in training government officials made these Mediterranean universities similar to Islamic madrasas, although madrasas were generally smaller and individual teachers, rather than the madrasa itself, granted the license or degree. Scholars like Arnold H. Green and Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madrasahs became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Other scholars regard the university as uniquely European in origin and characteristics.
Many scholars (including Makdisi) have argued that early medieval universities were influenced by the religious madrasahs in Al-Andalus, the Emirate of Sicily, and the Middle East (during the Crusades). Other scholars see this argument as overstated. Lowe and Yasuhara have recently drawn on the well-documented influences of scholarship from the Islamic world on the universities of Western Europe to call for a reconsideration of the development of higher education, turning away from a concern with local institutional structures to a broader consideration within a global context.
During the Early Modern period (approximately late 15th century to 1800), the universities of Europe would see a tremendous amount of growth, productivity and innovative research. At the end of the Middle Ages, about 400 years after the first university was founded, there were twenty-nine universities spread throughout Europe. In the 15th century, twenty-eight new ones were created, with another eighteen added between 1500 and 1625. This pace continued until by the end of the 18th century there were approximately 143 universities in Europe and Eastern Europe, with the highest concentrations in the German Empire (34), Italian countries (26), France (25), and Spain (23) – this was close to a 500% increase over the number of universities toward the end of the Middle Ages. This number does not include the numerous universities that disappeared, or institutions that merged with other universities during this time. It should be noted that the identification of a university was not necessarily obvious during the Early Modern period, as the term is applied to a burgeoning number of institutions. In fact, the term "university" was not always used to designate a higher education institution. In Mediterranean countries, the term studium generale was still often used, while "Academy" was common in Northern European countries.
The propagation of universities was not necessarily a steady progression, as the 17th century was rife with events that adversely affected university expansion. Many wars, and especially the Thirty Years' War, disrupted the university landscape throughout Europe at different times. War, plague, famine, regicide, and changes in religious power and structure often adversely affected the societies that provided support for universities. Internal strife within the universities themselves, such as student brawling and absentee professors, acted to destabilize these institutions as well. Universities were also reluctant to give up older curricula, and the continued reliance on the works of Aristotle defied contemporary advancements in science and the arts. This era was also affected by the rise of the nation-state. As universities increasingly came under state control, or formed under the auspices of the state, the faculty governance model (begun by the University of Paris) became more and more prominent. Although the older student-controlled universities still existed, they slowly started to move toward this structural organization. Control of universities still tended to be independent, although university leadership was increasingly appointed by the state.
Although the structural model provided by the University of Paris, where student members are controlled by faculty "masters," provided a standard for universities, the application of this model took at least three different forms. There were universities that had a system of faculties whose teaching addressed a very specific curriculum; this model tended to train specialists. There was a collegiate or tutorial model based on the system at University of Oxford where teaching and organization was decentralized and knowledge was more of a generalist nature. There were also universities that combined these models, using the collegiate model but having a centralized organization.
Early Modern universities initially continued the curriculum and research of the Middle Ages: natural philosophy, logic, medicine, theology, mathematics, astronomy (and astrology), law, grammar and rhetoric. Aristotle was prevalent throughout the curriculum, while medicine also depended on Galen and Arabic scholarship. The importance of humanism for changing this state-of-affairs cannot be underestimated. Once humanist professors joined the university faculty, they began to transform the study of grammar and rhetoric through the studia humanitatis. Humanist professors focused on the ability of students to write and speak with distinction, to translate and interpret classical texts, and to live honorable lives. Other scholars within the university were affected by the humanist approaches to learning and their linguistic expertise in relation to ancient texts, as well as the ideology that advocated the ultimate importance of those texts. Professors of medicine such as Niccolò Leoniceno, Thomas Linacre and William Cop were often trained in and taught from a humanist perspective as well as translated important ancient medical texts. The critical mindset imparted by humanism was imperative for changes in universities and scholarship. For instance, Andreas Vesalius was educated in a humanist fashion before producing a translation of Galen, whose ideas he verified through his own dissections. In law, Andreas Alciatus infused the Corpus Juris with a humanist perspective, while Jacques Cujas humanist writings were paramount to his reputation as a jurist. Philipp Melanchthon cited the works of Erasmus as a highly influential guide for connecting theology back to original texts, which was important for the reform at Protestant universities. Galileo Galilei, who taught at the Universities of Pisa and Padua, and Martin Luther, who taught at the University of Wittenberg (as did Melanchthon), also had humanist training. The task of the humanists was to slowly permeate the university; to increase the humanist presence in professorships and chairs, syllabi and textbooks so that published works would demonstrate the humanistic ideal of science and scholarship.
Although the initial focus of the humanist scholars in the university was the discovery, exposition and insertion of ancient texts and languages into the university, and the ideas of those texts into society generally, their influence was ultimately quite progressive. The emergence of classical texts brought new ideas and led to a more creative university climate (as the notable list of scholars above attests to). A focus on knowledge coming from self, from the human, has a direct implication for new forms of scholarship and instruction, and was the foundation for what is commonly known as the humanities. This disposition toward knowledge manifested in not simply the translation and propagation of ancient texts, but also their adaptation and expansion. For instance, Vesalius was imperative for advocating the use of Galen, but he also invigorated this text with experimentation, disagreements and further research. The propagation of these texts, especially within the universities, was greatly aided by the emergence of the printing press and the beginning of the use of the vernacular, which allowed for the printing of relatively large texts at reasonable prices.
There are several major exceptions on tuition fees. In many European countries, it is possible to study without tuition fees. Public universities in Nordic countries were entirely without tuition fees until around 2005. Denmark, Sweden and Finland then moved to put in place tuition fees for foreign students. Citizens of EU and EEA member states and citizens from Switzerland remain exempted from tuition fees, and the amounts of public grants granted to promising foreign students were increased to offset some of the impact.
Colloquially, the term university may be used to describe a phase in one's life: "When I was at university..." (in the United States and Ireland, college is often used instead: "When I was in college..."). In Australia, Canada, New Zealand, the United Kingdom, Nigeria, the Netherlands, Spain and the German-speaking countries university is often contracted to uni. In Ghana, New Zealand and in South Africa it is sometimes called "varsity" (although this has become uncommon in New Zealand in recent years). "Varsity" was also common usage in the UK in the 19th century.[citation needed] "Varsity" is still in common usage in Scotland.
In Canada, "college" generally refers to a two-year, non-degree-granting institution, while "university" connotes a four-year, degree-granting institution. Universities may be sub-classified (as in the Macleans rankings) into large research universities with many PhD granting programs and medical schools (for example, McGill University); "comprehensive" universities that have some PhDs but aren't geared toward research (such as Waterloo); and smaller, primarily undergraduate universities (such as St. Francis Xavier).
Although each institution is organized differently, nearly all universities have a board of trustees; a president, chancellor, or rector; at least one vice president, vice-chancellor, or vice-rector; and deans of various divisions. Universities are generally divided into a number of academic departments, schools or faculties. Public university systems are ruled over by government-run higher education boards. They review financial requests and budget proposals and then allocate funds for each university in the system. They also approve new programs of instruction and cancel or make changes in existing programs. In addition, they plan for the further coordinated growth and development of the various institutions of higher education in the state or country. However, many public universities in the world have a considerable degree of financial, research and pedagogical autonomy. Private universities are privately funded and generally have broader independence from state policies. However, they may have less independence from business corporations depending on the source of their finances.
The funding and organization of universities varies widely between different countries around the world. In some countries universities are predominantly funded by the state, while in others funding may come from donors or from fees which students attending the university must pay. In some countries the vast majority of students attend university in their local town, while in other countries universities attract students from all over the world, and may provide university accommodation for their students.
Universities created by bilateral or multilateral treaties between states are intergovernmental. An example is the Academy of European Law, which offers training in European law to lawyers, judges, barristers, solicitors, in-house counsel and academics. EUCLID (Pôle Universitaire Euclide, Euclid University) is chartered as a university and umbrella organisation dedicated to sustainable development in signatory countries, and the United Nations University engages in efforts to resolve the pressing global problems that are of concern to the United Nations, its peoples and member states. The European University Institute, a post-graduate university specialised in the social sciences, is officially an intergovernmental organisation, set up by the member states of the European Union.
A national university is generally a university created or run by a national state but at the same time represents a state autonomic institution which functions as a completely independent body inside of the same state. Some national universities are closely associated with national cultural or political aspirations, for instance the National University of Ireland in the early days of Irish independence collected a large amount of information on the Irish language and Irish culture. Reforms in Argentina were the result of the University Revolution of 1918 and its posterior reforms by incorporating values that sought for a more equal and laic higher education system.
In 1963, the Robbins Report on universities in the United Kingdom concluded that such institutions should have four main "objectives essential to any properly balanced system: instruction in skills; the promotion of the general powers of the mind so as to produce not mere specialists but rather cultivated men and women; to maintain research in balance with teaching, since teaching should not be separated from the advancement of learning and the search for truth; and to transmit a common culture and common standards of citizenship."
Until the 19th century, religion played a significant role in university curriculum; however, the role of religion in research universities decreased in the 19th century, and by the end of the 19th century, the German university model had spread around the world. Universities concentrated on science in the 19th and 20th centuries and became increasingly accessible to the masses. In Britain, the move from Industrial Revolution to modernity saw the arrival of new civic universities with an emphasis on science and engineering, a movement initiated in 1960 by Sir Keith Murray (chairman of the University Grants Committee) and Sir Samuel Curran, with the formation of the University of Strathclyde. The British also established universities worldwide, and higher education became available to the masses not only in Europe.
By the end of the early modern period, the structure and orientation of higher education had changed in ways that are eminently recognizable for the modern context. Aristotle was no longer a force providing the epistemological and methodological focus for universities and a more mechanistic orientation was emerging. The hierarchical place of theological knowledge had for the most part been displaced and the humanities had become a fixture, and a new openness was beginning to take hold in the construction and dissemination of knowledge that were to become imperative for the formation of the modern state.
The epistemological tensions between scientists and universities were also heightened by the economic realities of research during this time, as individual scientists, associations and universities were vying for limited resources. There was also competition from the formation of new colleges funded by private benefactors and designed to provide free education to the public, or established by local governments to provide a knowledge hungry populace with an alternative to traditional universities. Even when universities supported new scientific endeavors, and the university provided foundational training and authority for the research and conclusions, they could not compete with the resources available through private benefactors.
Other historians find incongruity in the proposition that the very place where the vast number of the scholars that influenced the scientific revolution received their education should also be the place that inhibits their research and the advancement of science. In fact, more than 80% of the European scientists between 1450–1650 included in the Dictionary of Scientific Biography were university trained, of which approximately 45% held university posts. It was the case that the academic foundations remaining from the Middle Ages were stable, and they did provide for an environment that fostered considerable growth and development. There was considerable reluctance on the part of universities to relinquish the symmetry and comprehensiveness provided by the Aristotelian system, which was effective as a coherent system for understanding and interpreting the world. However, university professors still utilized some autonomy, at least in the sciences, to choose epistemological foundations and methods. For instance, Melanchthon and his disciples at University of Wittenberg were instrumental for integrating Copernican mathematical constructs into astronomical debate and instruction. Another example was the short-lived but fairly rapid adoption of Cartesian epistemology and methodology in European universities, and the debates surrounding that adoption, which led to more mechanistic approaches to scientific problems as well as demonstrated an openness to change. There are many examples which belie the commonly perceived intransigence of universities. Although universities may have been slow to accept new sciences and methodologies as they emerged, when they did accept new ideas it helped to convey legitimacy and respectability, and supported the scientific changes through providing a stable environment for instruction and material resources.
Regardless of the way the tension between universities, individual scientists, and the scientific revolution itself is perceived, there was a discernible impact on the way that university education was constructed. Aristotelian epistemology provided a coherent framework not simply for knowledge and knowledge construction, but also for the training of scholars within the higher education setting. The creation of new scientific constructs during the scientific revolution, and the epistemological challenges that were inherent within this creation, initiated the idea of both the autonomy of science and the hierarchy of the disciplines. Instead of entering higher education to become a "general scholar" immersed in becoming proficient in the entire curriculum, there emerged a type of scholar that put science first and viewed it as a vocation in itself. The divergence between those focused on science and those still entrenched in the idea of a general scholar exacerbated the epistemological tensions that were already beginning to emerge.
Examining the influence of humanism on scholars in medicine, mathematics, astronomy and physics may suggest that humanism and universities were a strong impetus for the scientific revolution. Although the connection between humanism and the scientific discovery may very well have begun within the confines of the university, the connection has been commonly perceived as having been severed by the changing nature of science during the scientific revolution. Historians such as Richard S. Westfall have argued that the overt traditionalism of universities inhibited attempts to re-conceptualize nature and knowledge and caused an indelible tension between universities and scientists. This resistance to changes in science may have been a significant factor in driving many scientists away from the university and toward private benefactors, usually in princely courts, and associations with newly forming scientific societies.
Grapes are a type of fruit that grow in clusters of 15 to 300, and can be crimson, black, dark blue, yellow, green, orange, and pink. "White" grapes are actually green in color, and are evolutionarily derived from the purple grape. Mutations in two regulatory genes of white grapes turn off production of anthocyanins, which are responsible for the color of purple grapes. Anthocyanins and other pigment chemicals of the larger family of polyphenols in purple grapes are responsible for the varying shades of purple in red wines. Grapes are typically an ellipsoid shape resembling a prolate spheroid.
The cultivation of the domesticated grape began 6,000–8,000 years ago in the Near East. Yeast, one of the earliest domesticated microorganisms, occurs naturally on the skins of grapes, leading to the innovation of alcoholic drinks such as wine. The earliest archeological evidence for a dominant position of wine-making in human culture dates from 8,000 years ago in Georgia. The oldest winery was found in Armenia, dating to around 4000 BC.[citation needed] By the 9th century AD the city of Shiraz was known to produce some of the finest wines in the Middle East. Thus it has been proposed that Syrah red wine is named after Shiraz, a city in Persia where the grape was used to make Shirazi wine.[citation needed] Ancient Egyptian hieroglyphics record the cultivation of purple grapes,[citation needed] and history attests to the ancient Greeks, Phoenicians, and Romans growing purple grapes for both eating and wine production[citation needed]. The growing of grapes would later spread to other regions in Europe, as well as North Africa, and eventually in North America.
Comparing diets among Western countries, researchers have discovered that although the French tend to eat higher levels of animal fat, the incidence of heart disease remains low in France. This phenomenon has been termed the French paradox, and is thought to occur from protective benefits of regularly consuming red wine. Apart from potential benefits of alcohol itself, including reduced platelet aggregation and vasodilation, polyphenols (e.g., resveratrol) mainly in the grape skin provide other suspected health benefits, such as:
Grape juice is obtained from crushing and blending grapes into a liquid. The juice is often sold in stores or fermented and made into wine, brandy, or vinegar. Grape juice that has been pasteurized, removing any naturally occurring yeast, will not ferment if kept sterile, and thus contains no alcohol. In the wine industry, grape juice that contains 7–23% of pulp, skins, stems and seeds is often referred to as "must". In North America, the most common grape juice is purple and made from Concord grapes, while white grape juice is commonly made from Niagara grapes, both of which are varieties of native American grapes, a different species from European wine grapes. In California, Sultana (known there as Thompson Seedless) grapes are sometimes diverted from the raisin or table market to produce white juice.
Red wine may offer health benefits more so than white because potentially beneficial compounds are present in grape skin, and only red wine is fermented with skins. The amount of fermentation time a wine spends in contact with grape skins is an important determinant of its resveratrol content. Ordinary non-muscadine red wine contains between 0.2 and 5.8 mg/L, depending on the grape variety, because it is fermented with the skins, allowing the wine to absorb the resveratrol. By contrast, a white wine contains lower phenolic contents because it is fermented after removal of skins.
Commercially cultivated grapes can usually be classified as either table or wine grapes, based on their intended method of consumption: eaten raw (table grapes) or used to make wine (wine grapes). While almost all of them belong to the same species, Vitis vinifera, table and wine grapes have significant differences, brought about through selective breeding. Table grape cultivars tend to have large, seedless fruit (see below) with relatively thin skin. Wine grapes are smaller, usually seeded, and have relatively thick skins (a desirable characteristic in winemaking, since much of the aroma in wine comes from the skin). Wine grapes also tend to be very sweet: they are harvested at the time when their juice is approximately 24% sugar by weight. By comparison, commercially produced "100% grape juice", made from table grapes, is usually around 15% sugar by weight.
In the Bible, grapes are first mentioned when Noah grows them on his farm (Genesis 9:20–21). Instructions concerning wine are given in the book of Proverbs and in the book of Isaiah, such as in Proverbs 20:1 and Isaiah 5:20–25. Deuteronomy 18:3–5,14:22–27,16:13–15 tell of the use of wine during Jewish feasts. Grapes were also significant to both the Greeks and Romans, and their god of agriculture, Dionysus, was linked to grapes and wine, being frequently portrayed with grape leaves on his head. Grapes are especially significant for Christians, who since the Early Church have used wine in their celebration of the Eucharist. Views on the significance of the wine vary between denominations. In Christian art, grapes often represent the blood of Christ, such as the grape leaves in Caravaggio’s John the Baptist.
There are several sources of the seedlessness trait, and essentially all commercial cultivators get it from one of three sources: Thompson Seedless, Russian Seedless, and Black Monukka, all being cultivars of Vitis vinifera. There are currently more than a dozen varieties of seedless grapes. Several, such as Einset Seedless, Benjamin Gunnels's Prime seedless grapes, Reliance, and Venus, have been specifically cultivated for hardiness and quality in the relatively cold climates of northeastern United States and southern Ontario.
Anthocyanins tend to be the main polyphenolics in purple grapes whereas flavan-3-ols (i.e. catechins) are the more abundant phenolic in white varieties. Total phenolic content, a laboratory index of antioxidant strength, is higher in purple varieties due almost entirely to anthocyanin density in purple grape skin compared to absence of anthocyanins in white grape skin. It is these anthocyanins that are attracting the efforts of scientists to define their properties for human health. Phenolic content of grape skin varies with cultivar, soil composition, climate, geographic origin, and cultivation practices or exposure to diseases, such as fungal infections.
The Catholic Church uses wine in the celebration of the Eucharist because it is part of the tradition passed down through the ages starting with Jesus Christ at the Last Supper, where Catholics believe the consecrated bread and wine literally become the body and blood of Jesus Christ, a dogma known as transubstantiation. Wine is used (not grape juice) both due to its strong Scriptural roots, and also to follow the tradition set by the early Christian Church. The Code of Canon Law of the Catholic Church (1983), Canon 924 says that the wine used must be natural, made from grapes of the vine, and not corrupt. In some circumstances, a priest may obtain special permission to use grape juice for the consecration, however this is extremely rare and typically requires sufficient impetus to warrant such a dispensation, such as personal health of the priest.
The Iranian languages or Iranic languages form a branch of the Indo-Iranian languages, which in turn are a branch of the Indo-European language family. The speakers of Iranian languages are known as Iranian peoples. Historical Iranian languages are grouped in three stages: Old Iranian (until 400 BCE), Middle Iranian (400 BCE – 900 CE), and New Iranian (since 900 CE). Of the Old Iranian languages, the better understood and recorded ones are Old Persian (a language of Achaemenid Iran) and Avestan (the language of the Avesta). Middle Iranian languages included Middle Persian (a language of Sassanid Iran), Parthian, and Bactrian.
As of 2008, there were an estimated 150–200 million native speakers of Iranian languages. Ethnologue estimates there are 86 Iranian languages, the largest amongst them being Persian, Pashto, Kurdish, and Balochi.
The term Iranian is applied to any language which descends from the ancestral Proto-Iranian language. Iranian derives from the Persian and Sanskrit origin word Arya.
The use of the term for the Iranian language family was introduced in 1836 by Christian Lassen. Robert Needham Cust used the term Irano-Aryan in 1878, and Orientalists such as George Abraham Grierson and Max Müller contrasted Irano-Aryan (Iranian) and Indo-Aryan (Indic). Some recent scholarship, primarily in German, has revived this convention.
All Iranian languages are descended from a common ancestor, Proto-Iranian. In turn, and together with Proto-Indo-Aryan and the Nuristani languages, Proto-Iranian descends from a common ancestor Proto-Indo-Iranian. The Indo-Iranian languages are thought to have originated in Central Asia. The Andronovo culture is the suggested candidate for the common Indo-Iranian culture ca. 2000 BC.
It was situated precisely in the western part of Central Asia that borders present-day Russia (and present-day Kazakhstan). It was in relative proximity to the other satem ethno-linguistic groups of the Indo-European family, like Thracian, Balto-Slavic and others, and to common Indo-European's original homeland (more precisely, the steppes of southern Russia to the north of the Caucasus), according to the reconstructed linguistic relationships of common Indo-European.
Proto-Iranian thus dates to some time after Proto-Indo-Iranian break-up, or the early second millennium BCE, as the Old Iranian languages began to break off and evolve separately as the various Iranian tribes migrated and settled in vast areas of southeastern Europe, the Iranian plateau, and Central Asia.
The multitude of Middle Iranian languages and peoples indicate that great linguistic diversity must have existed among the ancient speakers of Iranian languages. Of that variety of languages/dialects, direct evidence of only two have survived. These are:
Old Persian is the Old Iranian dialect as it was spoken in south-western Iran by the inhabitants of Parsa, who also gave their name to their region and language. Genuine Old Persian is best attested in one of the three languages of the Behistun inscription, composed circa 520 BC, and which is the last inscription (and only inscription of significant length) in which Old Persian is still grammatically correct. Later inscriptions are comparatively brief, and typically simply copies of words and phrases from earlier ones, often with grammatical errors, which suggests that by the 4th century BC the transition from Old Persian to Middle Persian was already far advanced, but efforts were still being made to retain an "old" quality for official proclamations.
The other directly attested Old Iranian dialects are the two forms of Avestan, which take their name from their use in the Avesta, the liturgical texts of indigenous Iranian religion that now goes by the name of Zoroastrianism but in the Avesta itself is simply known as vohu daena (later: behdin). The language of the Avesta is subdivided into two dialects, conventionally known as "Old (or 'Gathic') Avestan", and "Younger Avestan". These terms, which date to the 19th century, are slightly misleading since 'Younger Avestan' is not only much younger than 'Old Avestan', but also from a different geographic region. The Old Avestan dialect is very archaic, and at roughly the same stage of development as Rigvedic Sanskrit. On the other hand, Younger Avestan is at about the same linguistic stage as Old Persian, but by virtue of its use as a sacred language retained its "old" characteristics long after the Old Iranian languages had yielded to their Middle Iranian stage. Unlike Old Persian, which has Middle Persian as its known successor, Avestan has no clearly identifiable Middle Iranian stage (the effect of Middle Iranian is indistinguishable from effects due to other causes).
In addition to Old Persian and Avestan, which are the only directly attested Old Iranian languages, all Middle Iranian languages must have had a predecessor "Old Iranian" form of that language, and thus can all be said to have had an (at least hypothetical) "Old" form. Such hypothetical Old Iranian languages include Carduchi (the hypothetical predecessor to Kurdish) and Old Parthian. Additionally, the existence of unattested languages can sometimes be inferred from the impact they had on neighbouring languages. Such transfer is known to have occurred for Old Persian, which has (what is called) a "Median" substrate in some of its vocabulary. Also, foreign references to languages can also provide a hint to the existence of otherwise unattested languages, for example through toponyms/ethnonyms or in the recording of vocabulary, as Herodotus did for what he called "Scythian".
Conventionally, Iranian languages are grouped in "western" and "eastern" branches. These terms have little meaning with respect to Old Avestan as that stage of the language may predate the settling of the Iranian peoples into western and eastern groups. The geographic terms also have little meaning when applied to Younger Avestan since it isn't known where that dialect (or dialects) was spoken either. Certain is only that Avestan (all forms) and Old Persian are distinct, and since Old Persian is "western", and Avestan was not Old Persian, Avestan acquired a default assignment to "eastern". Confusing the issue is the introduction of a western Iranian substrate in later Avestan compositions and redactions undertaken at the centers of imperial power in western Iran (either in the south-west in Persia, or in the north-west in Nisa/Parthia and Ecbatana/Media).
Two of the earliest dialectal divisions among Iranian indeed happen to not follow the later division into Western and Eastern blocks. These concern the fate of the Proto-Indo-Iranian first-series palatal consonants, *ć and *dź:
As a common intermediate stage, it is possible to reconstruct depalatalized affricates: *c, *dz. (This coincides with the state of affairs in the neighboring Nuristani languages.) A further complication however concerns the consonant clusters *ćw and *dźw:
It is possible that other distinct dialect groups were already in existence during this period. Good candidates are the hypothethical ancestor languages of Alanian/Scytho-Sarmatian subgroup of Scythian in the far northwest; and the hypothetical "Old Parthian" (the Old Iranian ancestor of Parthian) in the near northwest, where original *dw > *b (paralleling the development of *ćw).
What is known in Iranian linguistic history as the "Middle Iranian" era is thought to begin around the 4th century BCE lasting through the 9th century. Linguistically the Middle Iranian languages are conventionally classified into two main groups, Western and Eastern.
The Western family includes Parthian (Arsacid Pahlavi) and Middle Persian, while Bactrian, Sogdian, Khwarezmian, Saka, and Old Ossetic (Scytho-Sarmatian) fall under the Eastern category. The two languages of the Western group were linguistically very close to each other, but quite distinct from their eastern counterparts. On the other hand, the Eastern group was an areal entity whose languages retained some similarity to Avestan. They were inscribed in various Aramaic-derived alphabets which had ultimately evolved from the Achaemenid Imperial Aramaic script, though Bactrian was written using an adapted Greek script.
Middle Persian (Pahlavi) was the official language under the Sasanian dynasty in Iran. It was in use from the 3rd century CE until the beginning of the 10th century. The script used for Middle Persian in this era underwent significant maturity. Middle Persian, Parthian and Sogdian were also used as literary languages by the Manichaeans, whose texts also survive in various non-Iranian languages, from Latin to Chinese. Manichaean texts were written in a script closely akin to the Syriac script.
Following the Islamic Conquest of Persia (Iran), there were important changes in the role of the different dialects within the Persian Empire. The old prestige form of Middle Iranian, also known as Pahlavi, was replaced by a new standard dialect called Dari as the official language of the court. The name Dari comes from the word darbâr (دربار), which refers to the royal court, where many of the poets, protagonists, and patrons of the literature flourished. The Saffarid dynasty in particular was the first in a line of many dynasties to officially adopt the new language in 875 CE. Dari may have been heavily influenced by regional dialects of eastern Iran, whereas the earlier Pahlavi standard was based more on western dialects. This new prestige dialect became the basis of Standard New Persian. Medieval Iranian scholars such as Abdullah Ibn al-Muqaffa (8th century) and Ibn al-Nadim (10th century) associated the term "Dari" with the eastern province of Khorasan, while they used the term "Pahlavi" to describe the dialects of the northwestern areas between Isfahan and Azerbaijan, and "Pârsi" ("Persian" proper) to describe the Dialects of Fars. They also noted that the unofficial language of the royalty itself was yet another dialect, "Khuzi", associated with the western province of Khuzestan.
The Islamic conquest also brought with it the adoption of Arabic script for writing Persian and much later, Kurdish, Pashto and Balochi. All three were adapted to the writing by the addition of a few letters. This development probably occurred some time during the second half of the 8th century, when the old middle Persian script began dwindling in usage. The Arabic script remains in use in contemporary modern Persian. Tajik script was first Latinised in the 1920s under the then Soviet nationality policy. The script was however subsequently Cyrillicized in the 1930s by the Soviet government.
The geographical regions in which Iranian languages were spoken were pushed back in several areas by newly neighbouring languages. Arabic spread into some parts of Western Iran (Khuzestan), and Turkic languages spread through much of Central Asia, displacing various Iranian languages such as Sogdian and Bactrian in parts of what is today Turkmenistan, Uzbekistan and Tajikistan. In Eastern Europe, mostly comprising the territory of modern-day Ukraine, southern European Russia, and parts of the Balkans, the core region of the native Scythians, Sarmatians, and Alans had been decisively been taken over as a result of absorption and assimilation (e.g. Slavicisation) by the various Proto-Slavic population of the region, by the 6th century AD. This resulted in the displacement and extinction of the once predominant Scythian languages of the region. Sogdian's close relative Yaghnobi barely survives in a small area of the Zarafshan valley east of Samarkand, and Saka as Ossetic in the Caucasus, which is the sole remnant of the once predominant Scythian languages in Eastern Europe proper and large parts of the North Caucasus. Various small Iranian languages in the Pamirs survive that are derived from Eastern Iranian.