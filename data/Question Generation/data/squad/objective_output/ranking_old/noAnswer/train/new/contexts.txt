An Internet service provider (ISP) is an organization that provides services for accessing, using, the Internet. Internet service providers may be organized in various forms, such as commercial, community-owned, non-profit, or otherwise privately owned.
Internet services typically provided by ISPs include Internet access, Internet transit, domain name registration, web hosting, Usenet service, and colocation.
The Internet was developed as a network between government research laboratories and participating departments of universities. By the late 1980s, a process was set in place towards public, commercial use of the Internet. The remaining restrictions were removed by 1995, 4 years after the introduction of the World Wide Web.
In 1989, the first ISPs were established in Australia and the United States. In Brookline, Massachusetts, The World became the first commercial ISP in the US. Its first customer was served in November 1989.
On 23 April 2014, the U.S. Federal Communications Commission (FCC) was reported to be considering a new rule that will permit ISPs to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On 15 May 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On 10 November 2014, President Barack Obama recommended that the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On 16 January 2015, Republicans presented legislation, in the form of a U.S. Congress H.R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers. On 31 January 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on 26 February 2015. Adoption of this notion would reclassify internet service from one of information to one of the telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to the New York Times.
On 26 February 2015, the FCC ruled in favor of net neutrality by adopting Title II (common carrier) of the Communications Act of 1934 and Section 706 in the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."
On 12 March 2015, the FCC released the specific details of the net neutrality rules. On 13 April 2015, the FCC published the final rule on its new "Net Neutrality" regulations.
 ISPs provide Internet access, employing a range of technologies to connect users to their network. Available technologies have ranged from computer modems with acoustic couplers to telephone lines, to television cable (CATV), wireless Ethernet (wi-fi), and fiber optics.
For users and small businesses, traditional options include copper wires to provide dial-up, DSL, typically asymmetric digital subscriber line (ADSL), cable modem or Integrated Services Digital Network (ISDN) (typically basic rate interface). Using fiber-optics to end users is called Fiber To The Home or similar names.
For customers with more demanding requirements (such as medium-to-large businesses, or other ISPs) can use higher-speed DSL (such as single-pair high-speed digital subscriber line), Ethernet, metropolitan Ethernet, gigabit Ethernet, Frame Relay, ISDN Primary Rate Interface, ATM (Asynchronous Transfer Mode) and synchronous optical networking (SONET).
A mailbox provider is an organization that provides services for hosting electronic mail domains with access to storage for mail boxes. It provides email servers to send, receive, accept, and store email for end users or other organizations.
Many mailbox providers are also access providers, while others are not (e.g., Yahoo! Mail, Outlook.com, Gmail, AOL Mail, Po box). The definition given in RFC 6650 covers email hosting services, as well as the relevant department of companies, universities, organizations, groups, and individuals that manage their mail servers themselves. The task is typically accomplished by implementing Simple Mail Transfer Protocol (SMTP) and possibly providing access to messages through Internet Message Access Protocol (IMAP), the Post Office Protocol, Webmail, or a proprietary protocol.
Internet hosting services provide email, web-hosting, or online storage services. Other services include virtual server, cloud services, or physical server operation.
Just as their customers pay them for Internet access, ISPs themselves pay upstream ISPs for Internet access. An upstream ISP usually has a larger network than the contracting ISP or is able to provide the contracting ISP with access to parts of the Internet the contracting ISP by itself has no access to.
In the simplest case, a single connection is established to an upstream ISP and is used to transmit data to or from areas of the Internet beyond the home network; this mode of interconnection is often cascaded multiple times until reaching a tier 1 carrier. In reality, the situation is often more complex. ISPs with more than one point of presence (PoP) may have separate connections to an upstream ISP at multiple PoPs, or they may be customers of multiple upstream ISPs and may have connections to each one of them at one or more point of presence. Transit ISPs provide large amounts of bandwidth for connecting hosting ISPs and access ISPs.
A virtual ISP (VISP) is an operation that purchases services from another ISP, sometimes called a wholesale ISP in this context, which allow the VISP's customers to access the Internet using services and infrastructure owned and operated by the wholesale ISP. VISPs resemble mobile virtual network operators and competitive local exchange carriers for voice communications.
Free ISPs are Internet service providers that provide service free of charge. Many free ISPs display advertisements while the user is connected; like commercial television, in a sense they are selling the user's attention to the advertiser. Other free ISPs, sometimes called freenets, are run on a nonprofit basis, usually with volunteer staff.[citation needed]
A wireless Internet service provider (WISP) is an Internet service provider with a network based on wireless networking. Technology may include commonplace Wi-Fi wireless mesh networking, or proprietary equipment designed to operate over open 900 MHz, 2.4 GHz, 4.9, 5.2, 5.4, 5.7, and 5.8 GHz bands or licensed frequencies such as 2.5 GHz (EBS/BRS), 3.65 GHz (NN) and in the UHF band (including the MMDS frequency band) and LMDS.[citation needed]
ISPs may engage in peering, where multiple ISPs interconnect at peering points or Internet exchange points (IXs), allowing routing of data between each network, without charging one another for the data transmitted—data that would otherwise have passed through a third upstream ISP, incurring charges from the upstream ISP.
Network hardware, software and specifications, as well as the expertise of network management personnel are important in ensuring that data follows the most efficient route, and upstream connections work reliably. A tradeoff between cost and efficiency is possible.[citation needed]
Internet service providers in many countries are legally required (e.g., via Communications Assistance for Law Enforcement Act (CALEA) in the U.S.) to allow law enforcement agencies to monitor some or all of the information transmitted by the ISP. Furthermore, in some countries ISPs are subject to monitoring by intelligence agencies. In the U.S., a controversial National Security Agency program known as PRISM provides for broad monitoring of Internet users traffic and has raised concerns about potential violation of the privacy protections in the Fourth Amendment to the United States Constitution. Modern ISPs integrate a wide array of surveillance and packet sniffing equipment into their networks, which then feeds the data to law-enforcement/intelligence networks (such as DCSNet in the United States, or SORM in Russia) allowing monitoring of Internet traffic in real time.
The fiber is most often spun into yarn or thread and used to make a soft, breathable textile. The use of cotton for fabric is known to date to prehistoric times; fragments of cotton fabric dated from 5000 BC have been excavated in Mexico and the Indus Valley Civilization in Ancient India (modern-day Pakistan and some parts of India). Although cultivated since antiquity, it was the invention of the cotton gin that lowered the cost of production that led to its widespread use, and it is the most widely used natural fiber cloth in clothing today.
The earliest evidence of cotton use in South Asia has been found at the site of Mehrgarh, Pakistan, where cotton threads have been found preserved in copper beads; these finds have been dated to Neolithic (between 6000 and 5000 BCE). Cotton cultivation in the region is dated to the Indus Valley Civilization, which covered parts of modern eastern Pakistan and northwestern India between 3300 and 1300 BCE The Indus cotton industry was well-developed and some methods used in cotton spinning and fabrication continued to be used until the industrialization of India. Between 2000 and 1000 BC cotton became widespread across much of India. For example, it has been found at the site of Hallus in Karnataka dating from around 1000 BC.
In Iran (Persia), the history of cotton dates back to the Achaemenid era (5th century BC); however, there are few sources about the planting of cotton in pre-Islamic Iran. The planting of cotton was common in Merv, Ray and Pars of Iran. In Persian poets' poems, especially Ferdowsi's Shahname, there are references to cotton ("panbe" in Persian). Marco Polo (13th century) refers to the major products of Persia, including cotton. John Chardin, a French traveler of the 17th century who visited the Safavid Persia, spoke approvingly of the vast cotton farms of Persia.
Though known since antiquity the commercial growing of cotton in Egypt only started in 1820's, following a Frenchman, by the name of M. Jumel, propositioning the then ruler, Mohamed Ali Pasha, that he could earn a substantial income by growing an extra-long staple Maho (Barbadence) cotton, in Lower Egypt, for the French market. Mohamed Ali Pasha accepted the proposition and granted himself the monopoly on the sale and export of cotton in Egypt; and later dictated cotton should be grown in preference to other crops. By the time of the American Civil war annual exports had reached $16 million (120,000 bales), which rose to $56 million by 1864, primarily due to the loss of the Confederate supply on the world market. Exports continued to grow even after the reintroduction of US cotton, produced now by a paid workforce, and Egyptian exports reached 1.2 million bales a year by 1903.
During the late medieval period, cotton became known as an imported fiber in northern Europe, without any knowledge of how it was derived, other than that it was a plant. Because Herodotus had written in his Histories, Book III, 106, that in India trees grew in the wild producing wool, it was assumed that the plant was a tree, rather than a shrub. This aspect is retained in the name for cotton in several Germanic languages, such as German Baumwolle, which translates as "tree wool" (Baum means "tree"; Wolle means "wool"). Noting its similarities to wool, people in the region could only imagine that cotton must be produced by plant-borne sheep. John Mandeville, writing in 1350, stated as fact the now-preposterous belief: "There grew there [India] a wonderful tree which bore tiny lambs on the endes of its branches. These branches were so pliable that they bent down to allow the lambs to feed when they are hungrie  [sic]." (See Vegetable Lamb of Tartary.) By the end of the 16th century, cotton was cultivated throughout the warmer regions in Asia and the Americas.
India's cotton-processing sector gradually declined during British expansion in India and the establishment of colonial rule during the late 18th and early 19th centuries. This was largely due to aggressive colonialist mercantile policies of the British East India Company, which made cotton processing and manufacturing workshops in India uncompetitive. Indian markets were increasingly forced to supply only raw cotton and, by British-imposed law, to purchase manufactured textiles from Britain.[citation needed]
The advent of the Industrial Revolution in Britain provided a great boost to cotton manufacture, as textiles emerged as Britain's leading export. In 1738, Lewis Paul and John Wyatt, of Birmingham, England, patented the roller spinning machine, as well as the flyer-and-bobbin system for drawing cotton to a more even thickness using two sets of rollers that traveled at different speeds. Later, the invention of the James Hargreaves' spinning jenny in 1764, Richard Arkwright's spinning frame in 1769 and Samuel Crompton's spinning mule in 1775 enabled British spinners to produce cotton yarn at much higher rates. From the late 18th century on, the British city of Manchester acquired the nickname "Cottonopolis" due to the cotton industry's omnipresence within the city, and Manchester's role as the heart of the global cotton trade.
Production capacity in Britain and the United States was improved by the invention of the cotton gin by the American Eli Whitney in 1793. Before the development of cotton gins, the cotton fibers had to be pulled from the seeds tediously by hand. By the late 1700s a number of crude ginning machines had been developed. However, to produce a bale of cotton required over 600 hours of human labor, making large-scale production uneconomical in the United States, even with the use of humans as slave labor. The gin that Whitney manufactured (the Holmes design) reduced the hours down to just a dozen or so per bale. Although Whitney patented his own design for a cotton gin, he manufactured a prior design from Henry Odgen Holmes, for which Holmes filed a patent in 1796. Improving technology and increasing control of world markets allowed British traders to develop a commercial chain in which raw cotton fibers were (at first) purchased from colonial plantations, processed into cotton cloth in the mills of Lancashire, and then exported on British ships to captive colonial markets in West Africa, India, and China (via Shanghai and Hong Kong).
By the 1840s, India was no longer capable of supplying the vast quantities of cotton fibers needed by mechanized British factories, while shipping bulky, low-price cotton from India to Britain was time-consuming and expensive. This, coupled with the emergence of American cotton as a superior type (due to the longer, stronger fibers of the two domesticated native American species, Gossypium hirsutum and Gossypium barbadense), encouraged British traders to purchase cotton from plantations in the United States and plantations in the Caribbean. By the mid-19th century, "King Cotton" had become the backbone of the southern American economy. In the United States, cultivating and harvesting cotton became the leading occupation of slaves.
During the American Civil War, American cotton exports slumped due to a Union blockade on Southern ports, and also because of a strategic decision by the Confederate government to cut exports, hoping to force Britain to recognize the Confederacy or enter the war. This prompted the main purchasers of cotton, Britain and France, to turn to Egyptian cotton. British and French traders invested heavily in cotton plantations. The Egyptian government of Viceroy Isma'il took out substantial loans from European bankers and stock exchanges. After the American Civil War ended in 1865, British and French traders abandoned Egyptian cotton and returned to cheap American exports,[citation needed] sending Egypt into a deficit spiral that led to the country declaring bankruptcy in 1876, a key factor behind Egypt's occupation by the British Empire in 1882.
Cotton remained a key crop in the Southern economy after emancipation and the end of the Civil War in 1865. Across the South, sharecropping evolved, in which landless black and white farmers worked land owned by others in return for a share of the profits. Some farmers rented the land and bore the production costs themselves. Until mechanical cotton pickers were developed, cotton farmers needed additional labor to hand-pick cotton. Picking cotton was a source of income for families across the South. Rural and small town school systems had split vacations so children could work in the fields during "cotton-picking."
Successful cultivation of cotton requires a long frost-free period, plenty of sunshine, and a moderate rainfall, usually from 600 to 1,200 mm (24 to 47 in). Soils usually need to be fairly heavy, although the level of nutrients does not need to be exceptional. In general, these conditions are met within the seasonally dry tropics and subtropics in the Northern and Southern hemispheres, but a large proportion of the cotton grown today is cultivated in areas with less rainfall that obtain the water from irrigation. Production of the crop for a given year usually starts soon after harvesting the preceding autumn. Cotton is naturally a perennial but is grown as an annual to help control pests. Planting time in spring in the Northern hemisphere varies from the beginning of February to the beginning of June. The area of the United States known as the South Plains is the largest contiguous cotton-growing region in the world. While dryland (non-irrigated) cotton is successfully grown in this region, consistent yields are only produced with heavy reliance on irrigation water drawn from the Ogallala Aquifer. Since cotton is somewhat salt and drought tolerant, this makes it an attractive crop for arid and semiarid regions. As water resources get tighter around the world, economies that rely on it face difficulties and conflict, as well as potential environmental problems. For example, improper cropping and irrigation practices have led to desertification in areas of Uzbekistan, where cotton is a major export. In the days of the Soviet Union, the Aral Sea was tapped for agricultural irrigation, largely of cotton, and now salination is widespread.
Genetically modified (GM) cotton was developed to reduce the heavy reliance on pesticides. The bacterium Bacillus thuringiensis (Bt) naturally produces a chemical harmful only to a small fraction of insects, most notably the larvae of moths and butterflies, beetles, and flies, and harmless to other forms of life. The gene coding for Bt toxin has been inserted into cotton, causing cotton, called Bt cotton, to produce this natural insecticide in its tissues. In many regions, the main pests in commercial cotton are lepidopteran larvae, which are killed by the Bt protein in the transgenic cotton they eat. This eliminates the need to use large amounts of broad-spectrum insecticides to kill lepidopteran pests (some of which have developed pyrethroid resistance). This spares natural insect predators in the farm ecology and further contributes to noninsecticide pest management.
But Bt cotton is ineffective against many cotton pests, however, such as plant bugs, stink bugs, and aphids; depending on circumstances it may still be desirable to use insecticides against these. A 2006 study done by Cornell researchers, the Center for Chinese Agricultural Policy and the Chinese Academy of Science on Bt cotton farming in China found that after seven years these secondary pests that were normally controlled by pesticide had increased, necessitating the use of pesticides at similar levels to non-Bt cotton and causing less profit for farmers because of the extra expense of GM seeds. However, a 2009 study by the Chinese Academy of Sciences, Stanford University and Rutgers University refuted this. They concluded that the GM cotton effectively controlled bollworm. The secondary pests were mostly miridae (plant bugs) whose increase was related to local temperature and rainfall and only continued to increase in half the villages studied. Moreover, the increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. A 2012 Chinese study concluded that Bt cotton halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders. The International Service for the Acquisition of Agri-biotech Applications (ISAAA) said that, worldwide, GM cotton was planted on an area of 25 million hectares in 2011. This was 69% of the worldwide total area planted in cotton.
GM cotton acreage in India grew at a rapid rate, increasing from 50,000 hectares in 2002 to 10.6 million hectares in 2011. The total cotton area in India was 12.1 million hectares in 2011, so GM cotton was grown on 88% of the cotton area. This made India the country with the largest area of GM cotton in the world. A long-term study on the economic impacts of Bt cotton in India, published in the Journal PNAS in 2012, showed that Bt cotton has increased yields, profits, and living standards of smallholder farmers. The U.S. GM cotton crop was 4.0 million hectares in 2011 the second largest area in the world, the Chinese GM cotton crop was third largest by area with 3.9 million hectares and Pakistan had the fourth largest GM cotton crop area of 2.6 million hectares in 2011. The initial introduction of GM cotton proved to be a success in Australia – the yields were equivalent to the non-transgenic varieties and the crop used much less pesticide to produce (85% reduction). The subsequent introduction of a second variety of GM cotton led to increases in GM cotton production until 95% of the Australian cotton crop was GM in 2009 making Australia the country with the fifth largest GM cotton crop in the world. Other GM cotton growing countries in 2011 were Argentina, Myanmar, Burkina Faso, Brazil, Mexico, Colombia, South Africa and Costa Rica.
Organic cotton is generally understood as cotton from plants not genetically modified and that is certified to be grown without the use of any synthetic agricultural chemicals, such as fertilizers or pesticides. Its production also promotes and enhances biodiversity and biological cycles. In the United States, organic cotton plantations are required to enforce the National Organic Program (NOP). This institution determines the allowed practices for pest control, growing, fertilizing, and handling of organic crops. As of 2007, 265,517 bales of organic cotton were produced in 24 countries, and worldwide production was growing at a rate of more than 50% per year.
Historically, in North America, one of the most economically destructive pests in cotton production has been the boll weevil. Due to the US Department of Agriculture's highly successful Boll Weevil Eradication Program (BWEP), this pest has been eliminated from cotton in most of the United States. This program, along with the introduction of genetically engineered Bt cotton (which contains a bacterial gene that codes for a plant-produced protein that is toxic to a number of pests such as cotton bollworm and pink bollworm), has allowed a reduction in the use of synthetic insecticides.
Most cotton in the United States, Europe and Australia is harvested mechanically, either by a cotton picker, a machine that removes the cotton from the boll without damaging the cotton plant, or by a cotton stripper, which strips the entire boll off the plant. Cotton strippers are used in regions where it is too windy to grow picker varieties of cotton, and usually after application of a chemical defoliant or the natural defoliation that occurs after a freeze. Cotton is a perennial crop in the tropics, and without defoliation or freezing, the plant will continue to grow.
The era of manufactured fibers began with the development of rayon in France in the 1890s. Rayon is derived from a natural cellulose and cannot be considered synthetic, but requires extensive processing in a manufacturing process, and led the less expensive replacement of more naturally derived materials. A succession of new synthetic fibers were introduced by the chemicals industry in the following decades. Acetate in fiber form was developed in 1924. Nylon, the first fiber synthesized entirely from petrochemicals, was introduced as a sewing thread by DuPont in 1936, followed by DuPont's acrylic in 1944. Some garments were created from fabrics based on these fibers, such as women's hosiery from nylon, but it was not until the introduction of polyester into the fiber marketplace in the early 1950s that the market for cotton came under threat. The rapid uptake of polyester garments in the 1960s caused economic hardship in cotton-exporting economies, especially in Central American countries, such as Nicaragua, where cotton production had boomed tenfold between 1950 and 1965 with the advent of cheap chemical pesticides. Cotton production recovered in the 1970s, but crashed to pre-1960 levels in the early 1990s.
Beginning as a self-help program in the mid-1960s, the Cotton Research and Promotion Program (CRPP) was organized by U.S. cotton producers in response to cotton's steady decline in market share. At that time, producers voted to set up a per-bale assessment system to fund the program, with built-in safeguards to protect their investments. With the passage of the Cotton Research and Promotion Act of 1966, the program joined forces and began battling synthetic competitors and re-establishing markets for cotton. Today, the success of this program has made cotton the best-selling fiber in the U.S. and one of the best-selling fibers in the world.[citation needed]
Cotton is used to make a number of textile products. These include terrycloth for highly absorbent bath towels and robes; denim for blue jeans; cambric, popularly used in the manufacture of blue work shirts (from which we get the term "blue-collar"); and corduroy, seersucker, and cotton twill. Socks, underwear, and most T-shirts are made from cotton. Bed sheets often are made from cotton. Cotton also is used to make yarn used in crochet and knitting. Fabric also can be made from recycled or recovered cotton that otherwise would be thrown away during the spinning, weaving, or cutting process. While many fabrics are made completely of cotton, some materials blend cotton with other fibers, including rayon and synthetic fibers such as polyester. It can either be used in knitted or woven fabrics, as it can be blended with elastine to make a stretchier thread for knitted fabrics, and apparel such as stretch jeans.
The cottonseed which remains after the cotton is ginned is used to produce cottonseed oil, which, after refining, can be consumed by humans like any other vegetable oil. The cottonseed meal that is left generally is fed to ruminant livestock; the gossypol remaining in the meal is toxic to monogastric animals. Cottonseed hulls can be added to dairy cattle rations for roughage. During the American slavery period, cotton root bark was used in folk remedies as an abortifacient, that is, to induce a miscarriage. Gossypol was one of the many substances found in all parts of the cotton plant and it was described by the scientists as 'poisonous pigment'. It also appears to inhibit the development of sperm or even restrict the mobility of the sperm. Also, it is thought to interfere with the menstrual cycle by restricting the release of certain hormones.
Cotton linters are fine, silky fibers which adhere to the seeds of the cotton plant after ginning. These curly fibers typically are less than 1⁄8 inch (3.2 mm) long. The term also may apply to the longer textile fiber staple lint as well as the shorter fuzzy fibers from some upland species. Linters are traditionally used in the manufacture of paper and as a raw material in the manufacture of cellulose. In the UK, linters are referred to as "cotton wool". This can also be a refined product (absorbent cotton in U.S. usage) which has medical, cosmetic and many other practical uses. The first medical use of cotton wool was by Sampson Gamgee at the Queen's Hospital (later the General Hospital) in Birmingham, England.
Cotton lisle is a finely-spun, tightly twisted type of cotton that is noted for being strong and durable. Lisle is composed of two strands that have each been twisted an extra twist per inch than ordinary yarns and combined to create a single thread. The yarn is spun so that it is compact and solid. This cotton is used mainly for underwear, stockings, and gloves. Colors applied to this yarn are noted for being more brilliant than colors applied to softer yarn. This type of thread was first made in the city of Lisle, France (now Lille), hence its name.
The largest producers of cotton, currently (2009), are China and India, with annual production of about 34 million bales and 33.4 million bales, respectively; most of this production is consumed by their respective textile industries. The largest exporters of raw cotton are the United States, with sales of $4.9 billion, and Africa, with sales of $2.1 billion. The total international trade is estimated to be $12 billion. Africa's share of the cotton trade has doubled since 1980. Neither area has a significant domestic textile industry, textile manufacturing having moved to developing nations in Eastern and South Asia such as India and China. In Africa, cotton is grown by numerous small holders. Dunavant Enterprises, based in Memphis, Tennessee, is the leading cotton broker in Africa, with hundreds of purchasing agents. It operates cotton gins in Uganda, Mozambique, and Zambia. In Zambia, it often offers loans for seed and expenses to the 180,000 small farmers who grow cotton for it, as well as advice on farming methods. Cargill also purchases cotton in Africa for export.
The 25,000 cotton growers in the United States of America are heavily subsidized at the rate of $2 billion per year although China now provides the highest overall level of cotton sector support. The future of these subsidies is uncertain and has led to anticipatory expansion of cotton brokers' operations in Africa. Dunavant expanded in Africa by buying out local operations. This is only possible in former British colonies and Mozambique; former French colonies continue to maintain tight monopolies, inherited from their former colonialist masters, on cotton purchases at low fixed prices.
While Brazil was fighting the US through the WTO's Dispute Settlement Mechanism against a heavily subsidized cotton industry, a group of four least-developed African countries – Benin, Burkina Faso, Chad, and Mali – also known as "Cotton-4" have been the leading protagonist for the reduction of US cotton subsidies through negotiations. The four introduced a "Sectoral Initiative in Favour of Cotton", presented by Burkina Faso's President Blaise Compaoré during the Trade Negotiations Committee on 10 June 2003.
In addition to concerns over subsidies, the cotton industries of some countries are criticized for employing child labor and damaging workers' health by exposure to pesticides used in production. The Environmental Justice Foundation has campaigned against the prevalent use of forced child and adult labor in cotton production in Uzbekistan, the world's third largest cotton exporter. The international production and trade situation has led to "fair trade" cotton clothing and footwear, joining a rapidly growing market for organic clothing, fair fashion or "ethical fashion". The fair trade system was initiated in 2005 with producers from Cameroon, Mali and Senegal.
A public genome sequencing effort of cotton was initiated in 2007 by a consortium of public researchers. They agreed on a strategy to sequence the genome of cultivated, tetraploid cotton. "Tetraploid" means that cultivated cotton actually has two separate genomes within its nucleus, referred to as the A and D genomes. The sequencing consortium first agreed to sequence the D-genome relative of cultivated cotton (G. raimondii, a wild Central American cotton species) because of its small size and limited number of repetitive elements. It is nearly one-third the number of bases of tetraploid cotton (AD), and each chromosome is only present once.[clarification needed] The A genome of G. arboreum would be sequenced next. Its genome is roughly twice the size of G. raimondii's. Part of the difference in size between the two genomes is the amplification of retrotransposons (GORGE). Once both diploid genomes are assembled, then research could begin sequencing the actual genomes of cultivated cotton varieties. This strategy is out of necessity; if one were to sequence the tetraploid genome without model diploid genomes, the euchromatic DNA sequences of the AD genomes would co-assemble and the repetitive elements of AD genomes would assembly independently into A and D sequences respectively. Then there would be no way to untangle the mess of AD sequences without comparing them to their diploid counterparts.
The public sector effort continues with the goal to create a high-quality, draft genome sequence from reads generated by all sources. The public-sector effort has generated Sanger reads of BACs, fosmids, and plasmids as well as 454 reads. These later types of reads will be instrumental in assembling an initial draft of the D genome. In 2010, two companies (Monsanto and Illumina), completed enough Illumina sequencing to cover the D genome of G. raimondii about 50x. They announced that they would donate their raw reads to the public. This public relations effort gave them some recognition for sequencing the cotton genome. Once the D genome is assembled from all of this raw material, it will undoubtedly assist in the assembly of the AD genomes of cultivated varieties of cotton, but a lot of hard work remains.
Egypt (i/ˈiːdʒɪpt/; Arabic: مِصر‎ Miṣr, Egyptian Arabic: مَصر Maṣr, Coptic: Ⲭⲏⲙⲓ Khemi), officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia, via a land bridge formed by the Sinai Peninsula. It is the world's only contiguous Eurafrasian nation. Most of Egypt's territory of 1,010,408 square kilometres (390,000 sq mi) lies within the Nile Valley. Egypt is a Mediterranean country. It is bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south and Libya to the west.
Egypt has one of the longest histories of any modern country, arising in the tenth millennium BC as one of the world's first nation states. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypt's rich cultural heritage is an integral part of its national identity, having endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. Although Christianised in the first century of the Common Era, it was subsequently Islamised due to the Islamic conquests of the seventh century.
With over 90 million inhabitants, Egypt is the most populous country in North Africa and the Arab World, the third-most populous in Africa (after Nigeria and Ethiopia), and the fifteenth-most populous in the world. The great majority of its people live near the banks of the Nile River, an area of about 40,000 square kilometres (15,000 sq mi), where the only arable land is found. The large regions of the Sahara desert, which constitute most of Egypt's territory, are sparsely inhabited. About half of Egypt's residents live in urban areas, with most spread across the densely populated centres of greater Cairo, Alexandria and other major cities in the Nile Delta.
Modern Egypt is considered to be a regional and middle power, with significant cultural, political, and military influence in North Africa, the Middle East and the Muslim world. Its economy is one of the largest and most diversified in the Middle East, with sectors such as tourism, agriculture, industry and services at almost equal production levels. In 2011, longtime President Hosni Mubarak stepped down amid mass protests. Later elections saw the rise of the Muslim Brotherhood, which was ousted by the army a year later amid mass protests.
Miṣr (IPA: [mi̠sˤr] or Egyptian Arabic pronunciation: [mesˤɾ]; Arabic: مِصر‎) is the Classical Quranic Arabic and modern official name of Egypt, while Maṣr (IPA: [mɑsˤɾ]; Egyptian Arabic: مَصر) is the local pronunciation in Egyptian Arabic. The name is of Semitic origin, directly cognate with other Semitic words for Egypt such as the Hebrew מִצְרַיִם (Mitzráyim). The oldest attestation of this name for Egypt is the Akkadian 𒆳 𒈪 𒄑 𒊒 KURmi-iṣ-ru miṣru, related to miṣru/miṣirru/miṣaru, meaning "border" or "frontier".
By about 6000 BC, a Neolithic culture rooted in the Nile Valley. During the Neolithic era, several predynastic cultures developed independently in Upper and Lower Egypt. The Badarian culture and the successor Naqada series are generally regarded as precursors to dynastic Egypt. The earliest known Lower Egyptian site, Merimda, predates the Badarian by about seven hundred years. Contemporaneous Lower Egyptian communities coexisted with their southern counterparts for more than two thousand years, remaining culturally distinct, but maintaining frequent contact through trade. The earliest known evidence of Egyptian hieroglyphic inscriptions appeared during the predynastic period on Naqada III pottery vessels, dated to about 3200 BC.
The First Intermediate Period ushered in a time of political upheaval for about 150 years. Stronger Nile floods and stabilisation of government, however, brought back renewed prosperity for the country in the Middle Kingdom c. 2040 BC, reaching a peak during the reign of Pharaoh Amenemhat III. A second period of disunity heralded the arrival of the first foreign ruling dynasty in Egypt, that of the Semitic Hyksos. The Hyksos invaders took over much of Lower Egypt around 1650 BC and founded a new capital at Avaris. They were driven out by an Upper Egyptian force led by Ahmose I, who founded the Eighteenth Dynasty and relocated the capital from Memphis to Thebes.
The New Kingdom c. 1550–1070 BC began with the Eighteenth Dynasty, marking the rise of Egypt as an international power that expanded during its greatest extension to an empire as far south as Tombos in Nubia, and included parts of the Levant in the east. This period is noted for some of the most well known Pharaohs, including Hatshepsut, Thutmose III, Akhenaten and his wife Nefertiti, Tutankhamun and Ramesses II. The first historically attested expression of monotheism came during this period as Atenism. Frequent contacts with other nations brought new ideas to the New Kingdom. The country was later invaded and conquered by Libyans, Nubians and Assyrians, but native Egyptians eventually drove them out and regained control of their country.
In 525 BC, the powerful Achaemenid Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. The entire Twenty-seventh Dynasty of Egypt, from 525 BC to 402 BC, save for Petubastis III, was an entirely Persian ruled period, with the Achaemenid kings all being granted the title of pharaoh. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians.
The Ptolemaic Kingdom was a powerful Hellenistic state, extending from southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia. Alexandria became the capital city and a centre of Greek culture and trade. To gain recognition by the native Egyptian populace, they named themselves as the successors to the Pharaohs. The later Ptolemies took on Egyptian traditions, had themselves portrayed on public monuments in Egyptian style and dress, and participated in Egyptian religious life.
The last ruler from the Ptolemaic line was Cleopatra VII, who committed suicide following the burial of her lover Mark Antony who had died in her arms (from a self-inflicted stab wound), after Octavian had captured Alexandria and her mercenary forces had fled. The Ptolemies faced rebellions of native Egyptians often caused by an unwanted regime and were involved in foreign and civil wars that led to the decline of the kingdom and its annexation by Rome. Nevertheless, Hellenistic culture continued to thrive in Egypt well after the Muslim conquest.
The Byzantines were able to regain control of the country after a brief Sasanian Persian invasion early in the 7th century amidst the Byzantine–Sasanian War of 602–628 during which they established a new short-lived province for ten years known as Sasanian Egypt, until 639–42, when Egypt was invaded and conquered by the Islamic Empire by the Muslim Arabs. When they defeated the Byzantine Armies in Egypt, the Arabs brought Sunni Islam to the country. Early in this period, Egyptians began to blend their new faith with indigenous beliefs and practices, leading to various Sufi orders that have flourished to this day. These earlier rites had survived the period of Coptic Christianity.
Muhammad Ali Pasha evolved the military from one that convened under the tradition of the corvée to a great modernised army. He introduced conscription of the male peasantry in 19th century Egypt, and took a novel approach to create his great army, strengthening it with numbers and in skill. Education and training of the new soldiers was not an option; the new concepts were furthermore enforced by isolation. The men were held in barracks to avoid distraction of their growth as a military unit to be reckoned with. The resentment for the military way of life eventually faded from the men and a new ideology took hold, one of nationalism and pride. It was with the help of this newly reborn martial unit that Muhammad Ali imposed his rule over Egypt.
The Suez Canal, built in partnership with the French, was completed in 1869. Its construction led to enormous debt to European banks, and caused popular discontent because of the onerous taxation it required. In 1875 Ismail was forced to sell Egypt's share in the canal to the British Government. Within three years this led to the imposition of British and French controllers who sat in the Egyptian cabinet, and, "with the financial power of the bondholders behind them, were the real power in the Government."
The new government drafted and implemented a constitution in 1923 based on a parliamentary system. Saad Zaghlul was popularly elected as Prime Minister of Egypt in 1924. In 1936, the Anglo-Egyptian Treaty was concluded. Continued instability due to remaining British influence and increasing political involvement by the king led to the dissolution of the parliament in a military coup d'état known as the 1952 Revolution. The Free Officers Movement forced King Farouk to abdicate in support of his son Fuad. British military presence in Egypt lasted until 1954.
In 1958, Egypt and Syria formed a sovereign union known as the United Arab Republic. The union was short-lived, ending in 1961 when Syria seceded, thus ending the union. During most of its existence, the United Arab Republic was also in a loose confederation with North Yemen (or the Mutawakkilite Kingdom of Yemen), known as the United Arab States. In 1959, the All-Palestine Government of the Gaza Strip, an Egyptian client state, was absorbed into the United Arab Republic under the pretext of Arab union, and was never restored.
In mid May 1967, the Soviet Union issued warnings to Nasser of an impending Israeli attack on Syria. Although the chief of staff Mohamed Fawzi verified them as "baseless", Nasser took three successive steps that made the war virtually inevitable: On 14 May he deployed his troops in Sinai near the border with Israel, on 19 May he expelled the UN peacekeepers stationed in the Sinai Peninsula border with Israel, and on 23 May he closed the Straits of Tiran to Israeli shipping. On 26 May Nasser declared, "The battle will be a general one and our basic objective will be to destroy Israel".
At the time of the fall of the Egyptian monarchy in the early 1950s, less than half a million Egyptians were considered upper class and rich, four million middle class and 17 million lower class and poor. Fewer than half of all primary-school-age children attended school, most of them being boys. Nasser's policies changed this. Land reform and distribution, the dramatic growth in university education, and government support to national industries greatly improved social mobility and flattened the social curve. From academic year 1953-54 through 1965-66, overall public school enrolments more than doubled. Millions of previously poor Egyptians, through education and jobs in the public sector, joined the middle class. Doctors, engineers, teachers, lawyers, journalists, constituted the bulk of the swelling middle class in Egypt under Nasser. During the 1960s, the Egyptian economy went from sluggish to the verge of collapse, the society became less free, and Nasser's appeal waned considerably.
In 1970, President Nasser died and was succeeded by Anwar Sadat. Sadat switched Egypt's Cold War allegiance from the Soviet Union to the United States, expelling Soviet advisors in 1972. He launched the Infitah economic reform policy, while clamping down on religious and secular opposition. In 1973, Egypt, along with Syria, launched the October War, a surprise attack to regain part of the Sinai territory Israel had captured 6 years earlier. it presented Sadat with a victory that allowed him to regain the Sinai later in return for peace with Israel.
In 1975, Sadat shifted Nasser's economic policies and sought to use his popularity to reduce government regulations and encourage foreign investment through his program of Infitah. Through this policy, incentives such as reduced taxes and import tariffs attracted some investors, but investments were mainly directed at low risk and profitable ventures like tourism and construction, abandoning Egypt's infant industries. Even though Sadat's policy was intended to modernise Egypt and assist the middle class, it mainly benefited the higher class, and, because of the elimination of subsidies on basic foodstuffs, led to the 1977 Egyptian Bread Riots.
During Mubarak's reign, the political scene was dominated by the National Democratic Party, which was created by Sadat in 1978. It passed the 1993 Syndicates Law, 1995 Press Law, and 1999 Nongovernmental Associations Law which hampered freedoms of association and expression by imposing new regulations and draconian penalties on violations.[citation needed] As a result, by the late 1990s parliamentary politics had become virtually irrelevant and alternative avenues for political expression were curtailed as well.
Constitutional changes voted on 19 March 2007 prohibited parties from using religion as a basis for political activity, allowed the drafting of a new anti-terrorism law, authorised broad police powers of arrest and surveillance, and gave the president power to dissolve parliament and end judicial election monitoring. In 2009, Dr. Ali El Deen Hilal Dessouki, Media Secretary of the National Democratic Party (NDP), described Egypt as a "pharaonic" political system, and democracy as a "long-term goal". Dessouki also stated that "the real center of power in Egypt is the military".
On 18 January 2014, the interim government instituted a new constitution following a referendum in which 98.1% of voters were supportive. Participation was low with only 38.6% of registered voters participating although this was higher than the 33% who voted in a referendum during Morsi's tenure. On 26 March 2014 Abdel Fattah el-Sisi the head of the Egyptian Armed Forces, who at this time was in control of the country, resigned from the military, announcing he would stand as a candidate in the 2014 presidential election. The poll, held between 26 and 28 May 2014, resulted in a landslide victory for el-Sisi. Sisi was sworn into office as President of Egypt on 8 June 2014. The Muslim Brotherhood and some liberal and secular activist groups boycotted the vote. Even though the military-backed authorities extended voting to a third day, the 46% turnout was lower than the 52% turnout in the 2012 election.
Most of Egypt's rain falls in the winter months. South of Cairo, rainfall averages only around 2 to 5 mm (0.1 to 0.2 in) per year and at intervals of many years. On a very thin strip of the northern coast the rainfall can be as high as 410 mm (16.1 in), mostly between October and March. Snow falls on Sinai's mountains and some of the north coastal cities such as Damietta, Baltim, Sidi Barrany, etc. and rarely in Alexandria. A very small amount of snow fell on Cairo on 13 December 2013, the first time Cairo received snowfall in many decades. Frost is also known in mid-Sinai and mid-Egypt. Egypt is the driest and the sunniest country in the world, and most of its land surface is desert.
The plan stated that the following numbers of species of different groups had been recorded from Egypt: algae (1483 species), animals (about 15,000 species of which more than 10,000 were insects), fungi (more than 627 species), monera (319 species), plants (2426 species), protozoans (371 species). For some major groups, for example lichen-forming fungi and nematode worms, the number was not known. Apart from small and well-studied groups like amphibians, birds, fish, mammals and reptiles, the many of those numbers are likely to increase as further species are recorded from Egypt. For the fungi, including lichen-forming species, for example, subsequent work has shown that over 2200 species have been recorded from Egypt, and the final figure of all fungi actually occurring in the country is expected to be much higher.
The House of Representatives, whose members are elected to serve five-year terms, specialises in legislation. Elections were last held between November 2011 and January 2012 which was later dissolved. The next parliamentary election will be held within 6 months of the constitution's ratification on 18 January 2014. Originally, the parliament was to be formed before the president was elected, but interim president Adly Mansour pushed the date. The Egyptian presidential election, 2014, took place on 26–28 May 2014. Official figures showed a turnout of 25,578,233 or 47.5%, with Abdel Fattah el-Sisi winning with 23.78 million votes, or 96.91% compared to 757,511 (3.09%) for Hamdeen Sabahi.
In the 1980s, 1990s, and 2000s, terrorist attacks in Egypt became numerous and severe, and began to target Christian Copts, foreign tourists and government officials. In the 1990s an Islamist group, Al-Gama'a al-Islamiyya, engaged in an extended campaign of violence, from the murders and attempted murders of prominent writers and intellectuals, to the repeated targeting of tourists and foreigners. Serious damage was done to the largest sector of Egypt's economy—tourism—and in turn to the government, but it also devastated the livelihoods of many of the people on whom the group depended for support.
On 18 January 2014, the interim government successfully institutionalised a more secular constitution. The president is elected to a four-year term and may serve 2 terms. The parliament may impeach the president. Under the constitution, there is a guarantee of gender equality and absolute freedom of thought. The military retains the ability to appoint the national Minister of Defence for the next 8 years. Under the constitution, political parties may not be based on "religion, race, gender or geography".
The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.
As a result of modernisation efforts over the years, Egypt's healthcare system has made great strides forward. Access to healthcare in both urban and rural areas greatly improved and immunisation programs are now able to cover 98% of the population. Life expectancy increased from 44.8 years during the 1960s to 72.12 years in 2009. There was a noticeable decline of the infant mortality rate (during the 1970s to the 1980s the infant mortality rate was 101-132/1000 live births, in 2000 the rate was 50-60/1000, and in 2008 it was 28-30/1000).
Cairo University is ranked as 401-500 according to the Academic Ranking of World Universities (Shanghai Ranking) and 551-600 according to QS World University Rankings. American University in Cairo is ranked as 360 according to QS World University Rankings and Al-Azhar University, Alexandria University and Ain Shams University fall in the 701+ range. Egypt is currently opening new research institutes for the aim of modernising research in the nation, the most recent example of which is Zewail City of Science and Technology.
Coptic Christians face discrimination at multiple levels of the government, ranging from disproportionate representation in government ministries to laws that limit their ability to build or repair churches. Intolerance of Bahá'ís and non-orthodox Muslim sects, such as Sufis, Shi'a and Ahmadis, also remains a problem. When the government moved to computerise identification cards, members of religious minorities, such as Bahá'ís, could not obtain identification documents. An Egyptian court ruled in early 2008 that members of other faiths may obtain identity cards without listing their faiths, and without becoming officially recognised.
Egypt actively practices capital punishment. Egypt's authorities do not release figures on death sentences and executions, despite repeated requests over the years by human rights organisations. The United Nations human rights office and various NGOs expressed "deep alarm" after an Egyptian Minya Criminal Court sentenced 529 people to death in a single hearing on 25 March 2014. Sentenced supporters of former President Mohamed Morsi will be executed for their alleged role in violence following his ousting in July 2013. The judgment was condemned as a violation of international law. By May 2014, approximately 16,000 people (and as high as more than 40,000 by one independent count), mostly Brotherhood members or supporters, have been imprisoned after the coup  after the Muslim Brotherhood was labelled as terrorist organisation by the post-coup interim Egyptian government.
After Morsi was ousted by the military, the judiciary system aligned itself with the new government, actively suopporting the repression of Muslim Brotherhood members. This resulted in a sharp increase in mass death sentences that arose criticism from the US president Barack Obama and the General Secretary of the UN, Ban Ki Moon. In April 2013, one judge of the Minya governatorate of Upper Egypt, sentenced 1,212 people to death. In December 2014 the judge Mohammed Nagi Shahata, notorious for his fierceness in passing on death sentences, condemened to the capital penalty 188 members of the Muslim Brotherhood, for assaulting a police station. Various Egyptian and international human rights organisations have already pointed out the lack of fair trials, that often last only a few minutes and do not take into consideration the procedural standards of fair trials.
The United States provides Egypt with annual military assistance, which in 2015 amounted to US$1.3 billion. In 1989, Egypt was designated as a major non-NATO ally of the United States. Nevertheless, ties between the two countries have partially soured since the July 2013 military coup that deposed Islamist president Mohamed Morsi, with the Obama administration condemning Egypt's violent crackdown on the Muslim Brotherhood and its supporters, and cancelling future military exercises involving the two countries. There have been recent attempts, however, to normalise relations between the two, with both governments frequently calling for mutual support in the fight against regional and international terrorism.
The Egyptian military has dozens of factories manufacturing weapons as well as consumer goods. The Armed Forces' inventory includes equipment from different countries around the world. Equipment from the former Soviet Union is being progressively replaced by more modern US, French, and British equipment, a significant portion of which is built under license in Egypt, such as the M1 Abrams tank.[citation needed] Relations with Russia have improved significantly following Mohamed Morsi's removal and both countries have worked since then to strengthen military and trade ties among other aspects of bilateral co-operation. Relations with China have also improved considerably. In 2014, Egypt and China have established a bilateral "comprehensive strategic partnership".
The permanent headquarters of the Arab League are located in Cairo and the body's secretary general has traditionally been Egyptian. This position is currently held by former foreign minister Nabil el-Araby. The Arab League briefly moved from Egypt to Tunis in 1978 to protest the Egypt-Israel Peace Treaty, but it later returned to Cairo in 1989. Gulf monarchies, including the United Arab Emirates and Saudi Arabia, have pledged billions of dollars to help Egypt overcome its economic difficulties since the July 2013 coup.
Following the 1973 war and the subsequent peace treaty, Egypt became the first Arab nation to establish diplomatic relations with Israel. Despite that, Israel is still widely considered as a hostile state by the majority of Egyptians. Egypt has played a historical role as a mediator in resolving various disputes in the Middle East, most notably its handling of the Israeli-Palestinian conflict and the peace process. Egypt's ceasefire and truce brokering efforts in Gaza have hardly been challenged following Israel's evacuation of its settlements from the strip in 2005, despite increasing animosity towards the Hamas government in Gaza following the ouster of Mohamed Morsi, and despite recent attempts by countries like Turkey and Qatar to take over this role.
Egypt's economy depends mainly on agriculture, media, petroleum imports, natural gas, and tourism; there are also more than three million Egyptians working abroad, mainly in Saudi Arabia, the Persian Gulf and Europe. The completion of the Aswan High Dam in 1970 and the resultant Lake Nasser have altered the time-honored place of the Nile River in the agriculture and ecology of Egypt. A rapidly growing population, limited arable land, and dependence on the Nile all continue to overtax resources and stress the economy.
Egypt has a developed energy market based on coal, oil, natural gas, and hydro power. Substantial coal deposits in the northeast Sinai are mined at the rate of about 600,000 tonnes (590,000 long tons; 660,000 short tons) per year. Oil and gas are produced in the western desert regions, the Gulf of Suez, and the Nile Delta. Egypt has huge reserves of gas, estimated at 2,180 cubic kilometres (520 cu mi), and LNG up to 2012 exported to many countries. In 2013, the Egyptian General Petroleum Co (EGPC) said the country will cut exports of natural gas and tell major industries to slow output this summer to avoid an energy crisis and stave off political unrest, Reuters has reported. Egypt is counting on top liquid natural gas (LNG) exporter Qatar to obtain additional gas volumes in summer, while encouraging factories to plan their annual maintenance for those months of peak demand, said EGPC chairman, Tarek El Barkatawy. Egypt produces its own energy, but has been a net oil importer since 2008 and is rapidly becoming a net importer of natural gas.
Economic conditions have started to improve considerably, after a period of stagnation, due to the adoption of more liberal economic policies by the government as well as increased revenues from tourism and a booming stock market. In its annual report, the International Monetary Fund (IMF) has rated Egypt as one of the top countries in the world undertaking economic reforms. Some major economic reforms undertaken by the government since 2003 include a dramatic slashing of customs and tariffs. A new taxation law implemented in 2005 decreased corporate taxes from 40% to the current 20%, resulting in a stated 100% increase in tax revenue by the year 2006.
Foreign direct investment (FDI) in Egypt increased considerably before the removal of Hosni Mubarak, exceeding $6 billion in 2006, due to economic liberalisation and privatisation measures taken by minister of investment Mahmoud Mohieddin.[citation needed] Since the fall of Hosni Mubarak in 2011, Egypt has experienced a drastic fall in both foreign investment and tourism revenues, followed by a 60% drop in foreign exchange reserves, a 3% drop in growth, and a rapid devaluation of the Egyptian pound.
Although one of the main obstacles still facing the Egyptian economy is the limited trickle down of wealth to the average population, many Egyptians criticise their government for higher prices of basic goods while their standards of living or purchasing power remains relatively stagnant. Corruption is often cited by Egyptians as the main impediment to further economic growth. The government promised major reconstruction of the country's infrastructure, using money paid for the newly acquired third mobile license ($3 billion) by Etisalat in 2006. In the Corruption Perceptions Index 2013, Egypt was ranked 114 out of 177.
Egypt's most prominent multinational companies are the Orascom Group and Raya Contact Center. The information technology (IT) sector has expanded rapidly in the past few years, with many start-ups selling outsourcing services to North America and Europe, operating with companies such as Microsoft, Oracle and other major corporations, as well as many small and medium size enterprises. Some of these companies are the Xceed Contact Center, Raya, E Group Connections and C3. The IT sector has been stimulated by new Egyptian entrepreneurs with government encouragement.[citation needed]
Egypt has a wide range of beaches situated on the Mediterranean and the Red Sea that extend to over 3,000 km. The Red Sea has serene waters, coloured coral reefs, rare fish and beautiful mountains. The Akba Gulf beaches also provide facilities for practising sea sports. Safaga tops the Red Sea zone with its beautiful location on the Suez Gulf. Last but not least, Sharm el-Sheikh (or City of Peace), Hurghada, Luxor (known as world's greatest open-air museum/ or City of the ⅓ of world monuments), Dahab, Ras Sidr, Marsa Alam, Safaga and the northern coast of the Mediterranean are major tourist's destinations of the recreational tourism.
Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt.
The Suez Canal is an artificial sea-level waterway in Egypt considered the most important centre of the maritime transport in the Middle East, connecting the Mediterranean Sea and the Red Sea. Opened in November 1869 after 10 years of construction work, it allows ship transport between Europe and Asia without navigation around Africa. The northern terminus is Port Said and the southern terminus is Port Tawfiq at the city of Suez. Ismailia lies on its west bank, 3 km (1.9 mi) from the half-way point.
The canal is 193.30 km (120.11 mi) long, 24 m (79 ft) deep and 205 metres (673 ft) wide as of 2010. It consists of the northern access channel of 22 km (14 mi), the canal itself of 162.25 km (100.82 mi) and the southern access channel of 9 km (5.6 mi). The canal is a single lane with passing places in the "Ballah By-Pass" and the Great Bitter Lake. It contains no locks; seawater flows freely through the canal. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. The current south of the lakes changes with the tide at Suez.
Drinking water supply and sanitation in Egypt is characterised by both achievements and challenges. Among the achievements are an increase of piped water supply between 1990 and 2010 from 89% to 100% in urban areas and from 39% to 93% in rural areas despite rapid population growth, the elimination of open defecation in rural areas during the same period, and in general a relatively high level of investment in infrastructure. Access to an improved water source in Egypt is now practically universal with a rate of 99%. About one half of the population is connected to sanitary sewers.
Partly because of low sanitation coverage about 17,000 children die each year because of diarrhoea. Another challenge is low cost recovery due to water tariffs that are among the lowest in the world. This in turn requires government subsidies even for operating costs, a situation that has been aggravated by salary increases without tariff increases after the Arab Spring. Poor operation of facilities, such as water and wastewater treatment plants, as well as limited government accountability and transparency, are also issues.
Ethnic Egyptians are by far the largest ethnic group in the country, constituting 91% of the total population. Ethnic minorities include the Abazas, Turks, Greeks, Bedouin Arab tribes living in the eastern deserts and the Sinai Peninsula, the Berber-speaking Siwis (Amazigh) of the Siwa Oasis, and the Nubian communities clustered along the Nile. There are also tribal Beja communities concentrated in the south-eastern-most corner of the country, and a number of Dom clans mostly in the Nile Delta and Faiyum who are progressively becoming assimilated as urbanisation increases.
Egypt also hosts an unknown number of refugees and asylum seekers, estimated to be between 500,000 and 3 million. There are some 70,000 Palestinian refugees, and about 150,000 recently arrived Iraqi refugees, but the number of the largest group, the Sudanese, is contested.[nb 1] The once-vibrant and ancient Greek and Jewish communities in Egypt have almost disappeared, with only a small number remaining in the country, but many Egyptian Jews visit on religious or other occasions and tourism. Several important Jewish archaeological and historical sites are found in Cairo, Alexandria and other cities.
The official language of the Republic is Modern Standard Arabic. Arabic was adopted by the Egyptians after the Arab invasion of Egypt. The spoken languages are: Egyptian Arabic (68%), Sa'idi Arabic (29%), Eastern Egyptian Bedawi Arabic (1.6%), Sudanese Arabic (0.6%), Domari (0.3%), Nobiin (0.3%), Beja (0.1%), Siwi and others. Additionally, Greek, Armenian and Italian are the main languages of immigrants. In Alexandria in the 19th century there was a large community of Italian Egyptians and Italian was the "lingua franca" of the city.
Although Egypt was a majority Christian country before the 7th Century, after Islam arrived, the country was slowly Islamified to become a majority Muslim country. Egypt emerged as a centre of politics and culture in the Muslim world. Under Anwar Sadat, Islam became the official state religion and Sharia the main source of law. It is estimated that 15 million Egyptians follow Native Sufi orders, with the Sufi leadership asserting that the numbers are much greater as many Egyptian Sufis are not officially registered with a Sufi order.
Of the Christian minority in Egypt over 90% belong to the native Coptic Orthodox Church of Alexandria, an Oriental Orthodox Christian Church. Other native Egyptian Christians are adherents of the Coptic Catholic Church, the Evangelical Church of Egypt and various other Protestant denominations. Non-native Christian communities are largely found in the urban regions of Cairo and Alexandria, such as the Syro-Lebanese, who belong to Greek Catholic, Greek Orthodox, and Maronite Catholic denominations.
Egypt recognises only three religions: Islam, Christianity, and Judaism. Other faiths and minority Muslim sects practised by Egyptians, such as the small Bahá'í and Ahmadi community, are not recognised by the state and face persecution since they are labelled as far right groups that threaten Egypt's national security. Individuals, particularly Baha'is and atheists, wishing to include their religion (or lack thereof) on their mandatory state issued identification cards are denied this ability (see Egyptian identification card controversy), and are put in the position of either not obtaining required identification or lying about their faith. A 2008 court ruling allowed members of unrecognised faiths to obtain identification and leave the religion field blank.
The Egyptians were one of the first major civilisations to codify design elements in art and architecture. Egyptian blue, also known as calcium copper silicate is a pigment used by Egyptians for thousands of years. It is considered to be the first synthetic pigment. The wall paintings done in the service of the Pharaohs followed a rigid code of visual rules and meanings. Egyptian civilisation is renowned for its colossal pyramids, temples and monumental tombs. Well-known examples are the Pyramid of Djoser designed by ancient architect and engineer Imhotep, the Sphinx, and the temple of Abu Simbel. Modern and contemporary Egyptian art can be as diverse as any works in the world art scene, from the vernacular architecture of Hassan Fathy and Ramses Wissa Wassef, to Mahmoud Mokhtar's sculptures, to the distinctive Coptic iconography of Isaac Fanous. The Cairo Opera House serves as the main performing arts venue in the Egyptian capital.
Egyptian literature traces its beginnings to ancient Egypt and is some of the earliest known literature. Indeed, the Egyptians were the first culture to develop literature as we know it today, that is, the book. It is an important cultural element in the life of Egypt. Egyptian novelists and poets were among the first to experiment with modern styles of Arabic literature, and the forms they developed have been widely imitated throughout the Middle East. The first modern Egyptian novel Zaynab by Muhammad Husayn Haykal was published in 1913 in the Egyptian vernacular. Egyptian novelist Naguib Mahfouz was the first Arabic-language writer to win the Nobel Prize in Literature. Egyptian women writers include Nawal El Saadawi, well known for her feminist activism, and Alifa Rifaat who also writes about women and tradition.
Egyptian cinema became a regional force with the coming of sound. In 1936, Studio Misr, financed by industrialist Talaat Harb, emerged as the leading Egyptian studio, a role the company retained for three decades. For over 100 years, more than 4000 films have been produced in Egypt, three quarters of the total Arab production.[citation needed] Egypt is considered the leading country in the field of cinema in the Middle East. Actors from all over the Arab World seek to appear in the Egyptian cinema for the sake of fame. The Cairo International Film Festival has been rated as one of 11 festivals with a top class rating worldwide by the International Federation of Film Producers' Associations.
Egyptian music is a rich mixture of indigenous, Mediterranean, African and Western elements. It has been an integral part of Egyptian culture since antiquity. The ancient Egyptians credited one of their gods Hathor with the invention of music, which Osiris in turn used as part of his effort to civilise the world. Egyptians used music instruments since then. Contemporary Egyptian music traces its beginnings to the creative work of people such as Abdu El Hamouli, Almaz and Mahmoud Osman, who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez whose age is considered the golden age of music in Egypt and the whole Middle East and North-Africa. Prominent contemporary Egyptian pop singers include Amr Diab and Mohamed Mounir.
Egypt has one of the oldest civilisations in the world. It has been in contact with many other civilisations and nations and has been through so many eras, starting from prehistoric age to the modern age, passing through so many ages such as; Pharonic, Roman, Greek, Islamic and many other ages. Because of this wide variation of ages, the continuous contact with other nations and the big number of conflicts Egypt had been through, at least 60 museums may be found in Egypt, mainly covering a wide area of these ages and conflicts.
Some consider koshari (a mixture of rice, lentils, and macaroni) to be the national dish. Fried onions can be also added to koshari. In addition, ful medames (mashed fava beans) is one of the most popular dishes. Fava bean is also used in making falafel (also known as "ta'meyya"), which may have originated in Egypt and spread to other parts of the Middle East. Garlic fried with coriander is added to mulukhiyya, a popular green soup made from finely chopped jute leaves, sometimes with chicken or rabbit.
Football is the most popular national sport of Egypt. The Cairo Derby is one of the fiercest derbies in Africa, and the BBC picked it as one of the 7 toughest derbies in the world. Al Ahly is the most successful club of the 20th century in the African continent according to CAF, closely followed by their rivals Zamalek SC. Al Ahly was named in 2000 by the Confederation of African Football as the "African Club of the Century". With twenty titles, Al Ahly is currently the world's most successful club in terms of international trophies, surpassing Italy's A.C. Milan and Argentina's Boca Juniors, both having eighteen.
Egypt has hosted several international competitions. the last one was 2009 FIFA U-20 World Cup which took place between 24 September - 16 October 2009. On Friday 19 September of the year 2014, Guinness World Records has announced that Egyptian scuba diver Ahmed Gabr is the new title holder for deepest salt water scuba dive, at 332.35 metres. Ahmed set a new world record Friday when he reached a depth of more than 1,000 feet. The 14-hour feat took Gabr 1,066 feet down into the abyss near the Egyptian town of Dahab in ther Red Sea, where he works as a diving instructor.
The annelids are bilaterally symmetrical, triploblastic, coelomate, invertebrate organisms. They also have parapodia for locomotion. Most textbooks still use the traditional division into polychaetes (almost all marine), oligochaetes (which include earthworms) and leech-like species. Cladistic research since 1997 has radically changed this scheme, viewing leeches as a sub-group of oligochaetes and oligochaetes as a sub-group of polychaetes. In addition, the Pogonophora, Echiura and Sipuncula, previously regarded as separate phyla, are now regarded as sub-groups of polychaetes. Annelids are considered members of the Lophotrochozoa, a "super-phylum" of protostomes that also includes molluscs, brachiopods, flatworms and nemerteans.
The basic annelid form consists of multiple segments. Each segment has the same sets of organs and, in most polychaetes, has a pair of parapodia that many species use for locomotion. Septa separate the segments of many species, but are poorly defined or absent in others, and Echiura and Sipuncula show no obvious signs of segmentation. In species with well-developed septa, the blood circulates entirely within blood vessels, and the vessels in segments near the front ends of these species are often built up with muscles that act as hearts. The septa of such species also enable them to change the shapes of individual segments, which facilitates movement by peristalsis ("ripples" that pass along the body) or by undulations that improve the effectiveness of the parapodia. In species with incomplete septa or none, the blood circulates through the main body cavity without any kind of pump, and there is a wide range of locomotory techniques – some burrowing species turn their pharynges inside out to drag themselves through the sediment.
Although many species can reproduce asexually and use similar mechanisms to regenerate after severe injuries, sexual reproduction is the normal method in species whose reproduction has been studied. The minority of living polychaetes whose reproduction and lifecycles are known produce trochophore larvae, that live as plankton and then sink and metamorphose into miniature adults. Oligochaetes are full hermaphrodites and produce a ring-like cocoon around their bodies, in which the eggs and hatchlings are nourished until they are ready to emerge.
Earthworms are Oligochaetes that support terrestrial food chains both as prey and in some regions are important in aeration and enriching of soil. The burrowing of marine polychaetes, which may constitute up to a third of all species in near-shore environments, encourages the development of ecosystems by enabling water and oxygen to penetrate the sea floor. In addition to improving soil fertility, annelids serve humans as food and as bait. Scientists observe annelids to monitor the quality of marine and fresh water. Although blood-letting is no longer in favor with doctors, some leech species are regarded as endangered species because they have been over-harvested for this purpose in the last few centuries. Ragworms' jaws are now being studied by engineers as they offer an exceptional combination of lightness and strength.
Since annelids are soft-bodied, their fossils are rare – mostly jaws and the mineralized tubes that some of the species secreted. Although some late Ediacaran fossils may represent annelids, the oldest known fossil that is identified with confidence comes from about 518 million years ago in the early Cambrian period. Fossils of most modern mobile polychaete groups appeared by the end of the Carboniferous, about 299 million years ago. Palaeontologists disagree about whether some body fossils from the mid Ordovician, about 472 to 461 million years ago, are the remains of oligochaetes, and the earliest indisputable fossils of the group appear in the Tertiary period, which began 65 million years ago.
No single feature distinguishes Annelids from other invertebrate phyla, but they have a distinctive combination of features. Their bodies are long, with segments that are divided externally by shallow ring-like constrictions called annuli and internally by septa ("partitions") at the same points, although in some species the septa are incomplete and in a few cases missing. Most of the segments contain the same sets of organs, although sharing a common gut, circulatory system and nervous system makes them inter-dependent. Their bodies are covered by a cuticle (outer covering) that does not contain cells but is secreted by cells in the skin underneath, is made of tough but flexible collagen and does not molt – on the other hand arthropods' cuticles are made of the more rigid α-chitin, and molt until the arthropods reach their full size. Most annelids have closed circulatory systems, where the blood makes its entire circuit via blood vessels.
Most of an annelid's body consists of segments that are practically identical, having the same sets of internal organs and external chaetae (Greek χαιτη, meaning "hair") and, in some species, appendages. However, the frontmost and rearmost sections are not regarded as true segments as they do not contain the standard sets of organs and do not develop in the same way as the true segments. The frontmost section, called the prostomium (Greek προ- meaning "in front of" and στομα meaning "mouth") contains the brain and sense organs, while the rearmost, called the pygidium (Greek πυγιδιον, meaning "little tail") or periproct contains the anus, generally on the underside. The first section behind the prostomium, called the peristomium (Greek περι- meaning "around" and στομα meaning "mouth"), is regarded by some zoologists as not a true segment, but in some polychaetes the peristomium has chetae and appendages like those of other segments.
Annelids' cuticles are made of collagen fibers, usually in layers that spiral in alternating directions so that the fibers cross each other. These are secreted by the one-cell deep epidermis (outermost skin layer). A few marine annelids that live in tubes lack cuticles, but their tubes have a similar structure, and mucus-secreting glands in the epidermis protect their skins. Under the epidermis is the dermis, which is made of connective tissue, in other words a combination of cells and non-cellular materials such as collagen. Below this are two layers of muscles, which develop from the lining of the coelom (body cavity): circular muscles make a segment longer and slimmer when they contract, while under them are longitudinal muscles, usually four distinct strips, whose contractions make the segment shorter and fatter. Some annelids also have oblique internal muscles that connect the underside of the body to each side.
The setae ("hairs") of annelids project out from the epidermis to provide traction and other capabilities. The simplest are unjointed and form paired bundles near the top and bottom of each side of each segment. The parapodia ("limbs") of annelids that have them often bear more complex chetae at their tips – for example jointed, comb-like or hooked. Chetae are made of moderately flexible β-chitin and are formed by follicles, each of which has a chetoblast ("hair-forming") cell at the bottom and muscles that can extend or retract the cheta. The chetoblasts produce chetae by forming microvilli, fine hair-like extensions that increase the area available for secreting the cheta. When the cheta is complete, the microvilli withdraw into the chetoblast, leaving parallel tunnels that run almost the full length of the cheta. Hence annelids' chetae are structurally different from the setae ("bristles") of arthropods, which are made of the more rigid α-chitin, have a single internal cavity, and are mounted on flexible joints in shallow pits in the cuticle.
Nearly all polychaetes have parapodia that function as limbs, while other major annelid groups lack them. Parapodia are unjointed paired extensions of the body wall, and their muscles are derived from the circular muscles of the body. They are often supported internally by one or more large, thick chetae. The parapodia of burrowing and tube-dwelling polychaetes are often just ridges whose tips bear hooked chetae. In active crawlers and swimmers the parapodia are often divided into large upper and lower paddles on a very short trunk, and the paddles are generally fringed with chetae and sometimes with cirri (fused bundles of cilia) and gills.
The brain generally forms a ring round the pharynx (throat), consisting of a pair of ganglia (local control centers) above and in front of the pharynx, linked by nerve cords either side of the pharynx to another pair of ganglia just below and behind it. The brains of polychaetes are generally in the prostomium, while those of clitellates are in the peristomium or sometimes the first segment behind the peristomium. In some very mobile and active polychaetes the brain is enlarged and more complex, with visible hindbrain, midbrain and forebrain sections. The rest of the central nervous system is generally "ladder-like", consisting of a pair of nerve cords that run through the bottom part of the body and have in each segment paired ganglia linked by a transverse connection. From each segmental ganglion a branching system of local nerves runs into the body wall and then encircles the body. However, in most polychaetes the two main nerve cords are fused, and in the tube-dwelling genus Owenia the single nerve chord has no ganglia and is located in the epidermis.
As in arthropods, each muscle fiber (cell) is controlled by more than one neuron, and the speed and power of the fiber's contractions depends on the combined effects of all its neurons. Vertebrates have a different system, in which one neuron controls a group of muscle fibers. Most annelids' longitudinal nerve trunks include giant axons (the output signal lines of nerve cells). Their large diameter decreases their resistance, which allows them to transmit signals exceptionally fast. This enables these worms to withdraw rapidly from danger by shortening their bodies. Experiments have shown that cutting the giant axons prevents this escape response but does not affect normal movement.
The sensors are primarily single cells that detect light, chemicals, pressure waves and contact, and are present on the head, appendages (if any) and other parts of the body. Nuchal ("on the neck") organs are paired, ciliated structures found only in polychaetes, and are thought to be chemosensors. Some polychaetes also have various combinations of ocelli ("little eyes") that detect the direction from which light is coming and camera eyes or compound eyes that can probably form images. The compound eyes probably evolved independently of arthropods' eyes. Some tube-worms use ocelli widely spread over their bodies to detect the shadows of fish, so that they can quickly withdraw into their tubes. Some burrowing and tube-dwelling polychaetes have statocysts (tilt and balance sensors) that tell them which way is down. A few polychaete genera have on the undersides of their heads palps that are used both in feeding and as "feelers", and some of these also have antennae that are structurally similar but probably are used mainly as "feelers".
Most annelids have a pair of coelomata (body cavities) in each segment, separated from other segments by septa and from each other by vertical mesenteries. Each septum forms a sandwich with connective tissue in the middle and mesothelium (membrane that serves as a lining) from the preceding and following segments on either side. Each mesentery is similar except that the mesothelium is the lining of each of the pair of coelomata, and the blood vessels and, in polychaetes, the main nerve cords are embedded in it. The mesothelium is made of modified epitheliomuscular cells; in other words, their bodies form part of the epithelium but their bases extend to form muscle fibers in the body wall. The mesothelium may also form radial and circular muscles on the septa, and circular muscles around the blood vessels and gut. Parts of the mesothelium, especially on the outside of the gut, may also form chloragogen cells that perform similar functions to the livers of vertebrates: producing and storing glycogen and fat; producing the oxygen-carrier hemoglobin; breaking down proteins; and turning nitrogenous waste products into ammonia and urea to be excreted.
Many annelids move by peristalsis (waves of contraction and expansion that sweep along the body), or flex the body while using parapodia to crawl or swim. In these animals the septa enable the circular and longitudinal muscles to change the shape of individual segments, by making each segment a separate fluid-filled "balloon". However, the septa are often incomplete in annelids that are semi-sessile or that do not move by peristalsis or by movements of parapodia – for example some move by whipping movements of the body, some small marine species move by means of cilia (fine muscle-powered hairs) and some burrowers turn their pharynges (throats) inside out to penetrate the sea-floor and drag themselves into it.
The fluid in the coelomata contains coelomocyte cells that defend the animals against parasites and infections. In some species coelomocytes may also contain a respiratory pigment – red hemoglobin in some species, green chlorocruorin in others (dissolved in the plasma) – and provide oxygen transport within their segments. Respiratory pigment is also dissolved in the blood plasma. Species with well-developed septa generally also have blood vessels running all long their bodies above and below the gut, the upper one carrying blood forwards while the lower one carries it backwards. Networks of capillaries in the body wall and around the gut transfer blood between the main blood vessels and to parts of the segment that need oxygen and nutrients. Both of the major vessels, especially the upper one, can pump blood by contracting. In some annelids the forward end of the upper blood vessel is enlarged with muscles to form a heart, while in the forward ends of many earthworms some of the vessels that connect the upper and lower main vessels function as hearts. Species with poorly developed or no septa generally have no blood vessels and rely on the circulation within the coelom for delivering nutrients and oxygen.
However, leeches and their closest relatives have a body structure that is very uniform within the group but significantly different from that of other annelids, including other members of the Clitellata. In leeches there are no septa, the connective tissue layer of the body wall is so thick that it occupies much of the body, and the two coelomata are widely separated and run the length of the body. They function as the main blood vessels, although they are side-by-side rather than upper and lower. However, they are lined with mesothelium, like the coelomata and unlike the blood vessels of other annelids. Leeches generally use suckers at their front and rear ends to move like inchworms. The anus is on the upper surface of the pygidium.
Feeding structures in the mouth region vary widely, and have little correlation with the animals' diets. Many polychaetes have a muscular pharynx that can be everted (turned inside out to extend it). In these animals the foremost few segments often lack septa so that, when the muscles in these segments contract, the sharp increase in fluid pressure from all these segments everts the pharynx very quickly. Two families, the Eunicidae and Phyllodocidae, have evolved jaws, which can be used for seizing prey, biting off pieces of vegetation, or grasping dead and decaying matter. On the other hand, some predatory polychaetes have neither jaws nor eversible pharynges. Selective deposit feeders generally live in tubes on the sea-floor and use palps to find food particles in the sediment and then wipe them into their mouths. Filter feeders use "crowns" of palps covered in cilia that wash food particles towards their mouths. Non-selective deposit feeders ingest soil or marine sediments via mouths that are generally unspecialized. Some clitellates have sticky pads in the roofs of their mouths, and some of these can evert the pads to capture prey. Leeches often have an eversible proboscis, or a muscular pharynx with two or three teeth.
The gut is generally an almost straight tube supported by the mesenteries (vertical partitions within segments), and ends with the anus on the underside of the pygidium. However, in members of the tube-dwelling family Siboglinidae the gut is blocked by a swollen lining that houses symbiotic bacteria, which can make up 15% of the worms' total weight. The bacteria convert inorganic matter – such as hydrogen sulfide and carbon dioxide from hydrothermal vents, or methane from seeps – to organic matter that feeds themselves and their hosts, while the worms extend their palps into the gas flows to absorb the gases needed by the bacteria.
Annelids with blood vessels use metanephridia to remove soluble waste products, while those without use protonephridia. Both of these systems use a two-stage filtration process, in which fluid and waste products are first extracted and these are filtered again to re-absorb any re-usable materials while dumping toxic and spent materials as urine. The difference is that protonephridia combine both filtration stages in the same organ, while metanephridia perform only the second filtration and rely on other mechanisms for the first – in annelids special filter cells in the walls of the blood vessels let fluids and other small molecules pass into the coelomic fluid, where it circulates to the metanephridia. In annelids the points at which fluid enters the protonephridia or metanephridia are on the forward side of a septum while the second-stage filter and the nephridiopore (exit opening in the body wall) are in the following segment. As a result, the hindmost segment (before the growth zone and pygidium) has no structure that extracts its wastes, as there is no following segment to filter and discharge them, while the first segment contains an extraction structure that passes wastes to the second, but does not contain the structures that re-filter and discharge urine.
It is thought that annelids were originally animals with two separate sexes, which released ova and sperm into the water via their nephridia. The fertilized eggs develop into trochophore larvae, which live as plankton. Later they sink to the sea-floor and metamorphose into miniature adults: the part of the trochophore between the apical tuft and the prototroch becomes the prostomium (head); a small area round the trochophore's anus becomes the pygidium (tail-piece); a narrow band immediately in front of that becomes the growth zone that produces new segments; and the rest of the trochophore becomes the peristomium (the segment that contains the mouth).
However, the lifecycles of most living polychaetes, which are almost all marine animals, are unknown, and only about 25% of the 300+ species whose lifecycles are known follow this pattern. About 14% use a similar external fertilization but produce yolk-rich eggs, which reduce the time the larva needs to spend among the plankton, or eggs from which miniature adults emerge rather than larvae. The rest care for the fertilized eggs until they hatch – some by producing jelly-covered masses of eggs which they tend, some by attaching the eggs to their bodies and a few species by keeping the eggs within their bodies until they hatch. These species use a variety of methods for sperm transfer; for example, in some the females collect sperm released into the water, while in others the males have a penis that inject sperm into the female. There is no guarantee that this is a representative sample of polychaetes' reproductive patterns, and it simply reflects scientists' current knowledge.
Some polychaetes breed only once in their lives, while others breed almost continuously or through several breeding seasons. While most polychaetes remain of one sex all their lives, a significant percentage of species are full hermaphrodites or change sex during their lives. Most polychaetes whose reproduction has been studied lack permanent gonads, and it is uncertain how they produce ova and sperm. In a few species the rear of the body splits off and becomes a separate individual that lives just long enough to swim to a suitable environment, usually near the surface, and spawn.
Most mature clitellates (the group that includes earthworms and leeches) are full hermaphrodites, although in a few leech species younger adults function as males and become female at maturity. All have well-developed gonads, and all copulate. Earthworms store their partners' sperm in spermathecae ("sperm stores") and then the clitellum produces a cocoon that collects ova from the ovaries and then sperm from the spermathecae. Fertilization and development of earthworm eggs takes place in the cocoon. Leeches' eggs are fertilized in the ovaries, and then transferred to the cocoon. In all clitellates the cocoon also either produces yolk when the eggs are fertilized or nutrients while they are developing. All clitellates hatch as miniature adults rather than larvae.
Charles Darwin's book The Formation of Vegetable Mould through the Action of Worms (1881) presented the first scientific analysis of earthworms' contributions to soil fertility. Some burrow while others live entirely on the surface, generally in moist leaf litter. The burrowers loosen the soil so that oxygen and water can penetrate it, and both surface and burrowing worms help to produce soil by mixing organic and mineral matter, by accelerating the decomposition of organic matter and thus making it more quickly available to other organisms, and by concentrating minerals and converting them to forms that plants can use more easily. Earthworms are also important prey for birds ranging in size from robins to storks, and for mammals ranging from shrews to badgers, and in some cases conserving earthworms may be essential for conserving endangered birds.
Terrestrial annelids can be invasive in some situations. In the glaciated areas of North America, for example, almost all native earthworms are thought to have been killed by the glaciers and the worms currently found in those areas are all introduced from other areas, primarily from Europe, and, more recently, from Asia. Northern hardwood forests are especially negatively impacted by invasive worms through the loss of leaf duff, soil fertility, changes in soil chemistry and the loss of ecological diversity. Especially of concern is Amynthas agrestis and at least one state (Wisconsin) has listed it as a prohibited species.
Earthworms make a significant contribution to soil fertility. The rear end of the Palolo worm, a marine polychaete that tunnels through coral, detaches in order to spawn at the surface, and the people of Samoa regard these spawning modules as a delicacy. Anglers sometimes find that worms are more effective bait than artificial flies, and worms can be kept for several days in a tin lined with damp moss. Ragworms are commercially important as bait and as food sources for aquaculture, and there have been proposals to farm them in order to reduce over-fishing of their natural populations. Some marine polychaetes' predation on molluscs causes serious losses to fishery and aquaculture operations.
Accounts of the use of leeches for the medically dubious practise of blood-letting have come from China around 30 AD, India around 200 AD, ancient Rome around 50 AD and later throughout Europe. In the 19th century medical demand for leeches was so high that some areas' stocks were exhausted and other regions imposed restrictions or bans on exports, and Hirudo medicinalis is treated as an endangered species by both IUCN and CITES. More recently leeches have been used to assist in microsurgery, and their saliva has provided anti-inflammatory compounds and several important anticoagulants, one of which also prevents tumors from spreading.
Since annelids are soft-bodied, their fossils are rare. Polychaetes' fossil record consists mainly of the jaws that some species had and the mineralized tubes that some secreted. Some Ediacaran fossils such as Dickinsonia in some ways resemble polychaetes, but the similarities are too vague for these fossils to be classified with confidence. The small shelly fossil Cloudina, from 549 to 542 million years ago, has been classified by some authors as an annelid, but by others as a cnidarian (i.e. in the phylum to which jellyfish and sea anemones belong). Until 2008 the earliest fossils widely accepted as annelids were the polychaetes Canadia and Burgessochaeta, both from Canada's Burgess Shale, formed about 505 million years ago in the early Cambrian. Myoscolex, found in Australia and a little older than the Burgess Shale, was possibly an annelid. However, it lacks some typical annelid features and has features which are not usually found in annelids and some of which are associated with other phyla. Then Simon Conway Morris and John Peel reported Phragmochaeta from Sirius Passet, about 518 million years old, and concluded that it was the oldest annelid known to date. There has been vigorous debate about whether the Burgess Shale fossil Wiwaxia was a mollusc or an annelid. Polychaetes diversified in the early Ordovician, about 488 to 474 million years ago. It is not until the early Ordovician that the first annelid jaws are found, thus the crown-group cannot have appeared before this date and probably appeared somewhat later. By the end of the Carboniferous, about 299 million years ago, fossils of most of the modern mobile polychaete groups had appeared. Many fossil tubes look like those made by modern sessile polychaetes  , but the first tubes clearly produced by polychaetes date from the Jurassic, less than 199 million years ago.
The earliest good evidence for oligochaetes occurs in the Tertiary period, which began 65 million years ago, and it has been suggested that these animals evolved around the same time as flowering plants in the early Cretaceous, from 130 to 90 million years ago. A trace fossil consisting of a convoluted burrow partly filled with small fecal pellets may be evidence that earthworms were present in the early Triassic period from 251 to 245 million years ago. Body fossils going back to the mid Ordovician, from 472 to 461 million years ago, have been tentatively classified as oligochaetes, but these identifications are uncertain and some have been disputed.
Traditionally the annelids have been divided into two major groups, the polychaetes and clitellates. In turn the clitellates were divided into oligochaetes, which include earthworms, and hirudinomorphs, whose best-known members are leeches. For many years there was no clear arrangement of the approximately 80 polychaete families into higher-level groups. In 1997 Greg Rouse and Kristian Fauchald attempted a "first heuristic step in terms of bringing polychaete systematics to an acceptable level of rigour", based on anatomical structures, and divided polychaetes into:
In 2007 Torsten Struck and colleagues compared 3 genes in 81 taxa, of which 9 were outgroups, in other words not considered closely related to annelids but included to give an indication of where the organisms under study are placed on the larger tree of life. For a cross-check the study used an analysis of 11 genes (including the original 3) in 10 taxa. This analysis agreed that clitellates, pogonophorans and echiurans were on various branches of the polychaete family tree. It also concluded that the classification of polychaetes into Scolecida, Canalipalpata and Aciculata was useless, as the members of these alleged groups were scattered all over the family tree derived from comparing the 81 taxa. In addition, it also placed sipunculans, generally regarded at the time as a separate phylum, on another branch of the polychaete tree, and concluded that leeches were a sub-group of oligochaetes rather than their sister-group among the clitellates. Rouse accepted the analyses based on molecular phylogenetics, and their main conclusions are now the scientific consensus, although the details of the annelid family tree remain uncertain.
In addition to re-writing the classification of annelids and 3 previously independent phyla, the molecular phylogenetics analyses undermine the emphasis that decades of previous writings placed on the importance of segmentation in the classification of invertebrates. Polychaetes, which these analyses found to be the parent group, have completely segmented bodies, while polychaetes' echiurans and sipunculan offshoots are not segmented and pogonophores are segmented only in the rear parts of their bodies. It now seems that segmentation can appear and disappear much more easily in the course of evolution than was previously thought. The 2007 study also noted that the ladder-like nervous system, which is associated with segmentation, is less universal previously thought in both annelids and arthropods.[n 2]
Annelids are members of the protostomes, one of the two major superphyla of bilaterian animals – the other is the deuterostomes, which includes vertebrates. Within the protostomes, annelids used to be grouped with arthropods under the super-group Articulata ("jointed animals"), as segmentation is obvious in most members of both phyla. However, the genes that drive segmentation in arthropods do not appear to do the same in annelids. Arthropods and annelids both have close relatives that are unsegmented. It is at least as easy to assume that they evolved segmented bodies independently as it is to assume that the ancestral protostome or bilaterian was segmented and that segmentation disappeared in many descendant phyla. The current view is that annelids are grouped with molluscs, brachiopods and several other phyla that have lophophores (fan-like feeding structures) and/or trochophore larvae as members of Lophotrochozoa. Bryzoa may be the most basal phylum (the one that first became distinctive) within the Lophotrochozoa, and the relationships between the other members are not yet known. Arthropods are now regarded as members of the Ecdysozoa ("animals that molt"), along with some phyla that are unsegmented.
The "Lophotrochozoa" hypothesis is also supported by the fact that many phyla within this group, including annelids, molluscs, nemerteans and flatworms, follow a similar pattern in the fertilized egg's development. When their cells divide after the 4-cell stage, descendants of these 4 cells form a spiral pattern. In these phyla the "fates" of the embryo's cells, in other words the roles their descendants will play in the adult animal, are the same and can be predicted from a very early stage. Hence this development pattern is often described as "spiral determinate cleavage".
Chinese political philosophy dates back to the Spring and Autumn Period, specifically with Confucius in the 6th century BC. Chinese political philosophy was developed as a response to the social and political breakdown of the country characteristic of the Spring and Autumn Period and the Warring States period. The major philosophies during the period, Confucianism, Legalism, Mohism, Agrarianism and Taoism, each had a political aspect to their philosophical schools. Philosophers such as Confucius, Mencius, and Mozi, focused on political unity and political stability as the basis of their political philosophies. Confucianism advocated a hierarchical, meritocratic government based on empathy, loyalty, and interpersonal relationships. Legalism advocated a highly authoritarian government based on draconian punishments and laws. Mohism advocated a communal, decentralized government centered on frugality and ascetism. The Agrarians advocated a peasant utopian communalism and egalitarianism. Taoism advocated a proto-anarchism. Legalism was the dominant political philosophy of the Qin Dynasty, but was replaced by State Confucianism in the Han Dynasty. Prior to China's adoption of communism, State Confucianism remained the dominant political philosophy of China up to the 20th century.
Western political philosophy originates in the philosophy of ancient Greece, where political philosophy dates back to at least Plato. Ancient Greece was dominated by city-states, which experimented with various forms of political organization, grouped by Plato into four categories: timocracy, tyranny, democracy and oligarchy. One of the first, extremely important classical works of political philosophy is Plato's Republic, which was followed by Aristotle's Nichomachean Ethics and Politics. Roman political philosophy was influenced by the Stoics, including the Roman statesman Cicero.
Indian political philosophy evolved in ancient times and demarcated a clear distinction between (1) nation and state (2) religion and state. The constitutions of Hindu states evolved over time and were based on political and legal treatises and prevalent social institutions. The institutions of state were broadly divided into governance, administration, defense, law and order. Mantranga, the principal governing body of these states, consisted of the King, Prime Minister, Commander in chief of army, Chief Priest of the King. The Prime Minister headed the committee of ministers along with head of executive (Maha Amatya).
Chanakya, 4th Century BC Indian political philosopher. The Arthashastra provides an account of the science of politics for a wise ruler, policies for foreign affairs and wars, the system of a spy state and surveillance and economic stability of the state. Chanakya quotes several authorities including Bruhaspati, Ushanas, Prachetasa Manu, Parasara, and Ambi, and described himself as a descendant of a lineage of political philosophers, with his father Chanaka being his immediate predecessor. Another influential extant Indian treatise on political philosophy is the Sukra Neeti. An example of a code of law in ancient India is the Manusmṛti or Laws of Manu.
The early Christian philosophy of Augustine of Hippo was heavily influenced by Plato. A key change brought about by Christian thought was the moderatation of the Stoicism and theory of justice of the Roman world, as well emphasis on the role of the state in applying mercy as a moral example. Augustine also preached that one was not a member of his or her city, but was either a citizen of the City of God (Civitas Dei) or the City of Man (Civitas Terrena). Augustine's City of God is an influential work of this period that attacked the thesis, held by many Christian Romans, that the Christian view could be realized on Earth.
The rise of Islam, based on both the Qur'an and Muhammad strongly altered the power balances and perceptions of origin of power in the Mediterranean region. Early Islamic philosophy emphasized an inexorable link between science and religion, and the process of ijtihad to find truth—in effect all philosophy was "political" as it had real implications for governance. This view was challenged by the "rationalist" Mutazilite philosophers, who held a more Hellenic view, reason above revelation, and as such are known to modern scholars as the first speculative theologians of Islam; they were supported by a secular aristocracy who sought freedom of action independent of the Caliphate. By the late ancient period, however, the "traditionalist" Asharite view of Islam had in general triumphed. According to the Asharites, reason must be subordinate to the Quran and the Sunna.
Islamic political philosophy, was, indeed, rooted in the very sources of Islam—i.e., the Qur'an and the Sunnah, the words and practices of Muhammad—thus making it essentially theocratic. However, in the Western thought, it is generally supposed that it was a specific area peculiar merely to the great philosophers of Islam: al-Kindi (Alkindus), al-Farabi (Abunaser), İbn Sina (Avicenna), Ibn Bajjah (Avempace), Ibn Rushd (Averroes), and Ibn Khaldun. The political conceptions of Islam such as kudrah (power), sultan, ummah, cemaa (obligation)-and even the "core" terms of the Qur'an—i.e., ibadah (worship), din (religion), rab (master) and ilah (deity)—is taken as the basis of an analysis. Hence, not only the ideas of the Muslim political philosophers but also many other jurists and ulama posed political ideas and theories. For example, the ideas of the Khawarij in the very early years of Islamic history on Khilafa and Ummah, or that of Shia Islam on the concept of Imamah are considered proofs of political thought. The clashes between the Ehl-i Sunna and Shia in the 7th and 8th centuries had a genuine political character.
Medieval political philosophy in Europe was heavily influenced by Christian thinking. It had much in common with the Mutazalite Islamic thinking in that the Roman Catholics though subordinating philosophy to theology did not subject reason to revelation but in the case of contradictions, subordinated reason to faith as the Asharite of Islam. The Scholastics by combining the philosophy of Aristotle with the Christianity of St. Augustine emphasized the potential harmony inherent in reason and revelation. Perhaps the most influential political philosopher of medieval Europe was St. Thomas Aquinas who helped reintroduce Aristotle's works, which had only been transmitted to Catholic Europe through Muslim Spain, along with the commentaries of Averroes. Aquinas's use of them set the agenda, for scholastic political philosophy dominated European thought for centuries even unto the Renaissance.
One of the most influential works during this burgeoning period was Niccolò Machiavelli's The Prince, written between 1511–12 and published in 1532, after Machiavelli's death. That work, as well as The Discourses, a rigorous analysis of the classical period, did much to influence modern political thought in the West. A minority (including Jean-Jacques Rousseau) interpreted The Prince as a satire meant to be given to the Medici after their recapture of Florence and their subsequent expulsion of Machiavelli from Florence. Though the work was written for the di Medici family in order to perhaps influence them to free him from exile, Machiavelli supported the Republic of Florence rather than the oligarchy of the di Medici family. At any rate, Machiavelli presents a pragmatic and somewhat consequentialist view of politics, whereby good and evil are mere means used to bring about an end—i.e., the secure and powerful state. Thomas Hobbes, well known for his theory of the social contract, goes on to expand this view at the start of the 17th century during the English Renaissance. Although neither Machiavelli nor Hobbes believed in the divine right of kings, they both believed in the inherent selfishness of the individual. It was necessarily this belief that led them to adopt a strong central power as the only means of preventing the disintegration of the social order.
These theorists were driven by two basic questions: one, by what right or need do people form states; and two, what the best form for a state could be. These fundamental questions involved a conceptual distinction between the concepts of "state" and "government." It was decided that "state" would refer to a set of enduring institutions through which power would be distributed and its use justified. The term "government" would refer to a specific group of people who occupied the institutions of the state, and create the laws and ordinances by which the people, themselves included, would be bound. This conceptual distinction continues to operate in political science, although some political scientists, philosophers, historians and cultural anthropologists have argued that most political action in any given society occurs outside of its state, and that there are societies that are not organized into states that nevertheless must be considered in political terms. As long as the concept of natural order was not introduced, the social sciences could not evolve independently of theistic thinking. Since the cultural revolution of the 17th century in England, which spread to France and the rest of Europe, society has been considered subject to natural laws akin to the physical world.
Political and economic relations were drastically influenced by these theories as the concept of the guild was subordinated to the theory of free trade, and Roman Catholic dominance of theology was increasingly challenged by Protestant churches subordinate to each nation-state, which also (in a fashion the Roman Catholic Church often decried angrily) preached in the vulgar or native language of each region. However, the enlightenment was an outright attack on religion, particularly Christianity. The most outspoken critic of the church in France was François Marie Arouet de Voltaire, a representative figure of the enlightenment. After Voltaire, religion would never be the same again in France.
In the Ottoman Empire, these ideological reforms did not take place and these views did not integrate into common thought until much later. As well, there was no spread of this doctrine within the New World and the advanced civilizations of the Aztec, Maya, Inca, Mohican, Delaware, Huron and especially the Iroquois. The Iroquois philosophy in particular gave much to Christian thought of the time and in many cases actually inspired some of the institutions adopted in the United States: for example, Benjamin Franklin was a great admirer of some of the methods of the Iroquois Confederacy, and much of early American literature emphasized the political philosophy of the natives.
John Locke in particular exemplified this new age of political theory with his work Two Treatises of Government. In it Locke proposes a state of nature theory that directly complements his conception of how political development occurs and how it can be founded through contractual obligation. Locke stood to refute Sir Robert Filmer's paternally founded political theory in favor of a natural system based on nature in a particular given system. The theory of the divine right of kings became a passing fancy, exposed to the type of ridicule with which John Locke treated it. Unlike Machiavelli and Hobbes but like Aquinas, Locke would accept Aristotle's dictum that man seeks to be happy in a state of social harmony as a social animal. Unlike Aquinas's preponderant view on the salvation of the soul from original sin, Locke believes man's mind comes into this world as tabula rasa. For Locke, knowledge is neither innate, revealed nor based on authority but subject to uncertainty tempered by reason, tolerance and moderation. According to Locke, an absolute ruler as proposed by Hobbes is unnecessary, for natural law is based on reason and seeking peace and survival for man.
The Marxist critique of capitalism — developed with Friedrich Engels — was, alongside liberalism and fascism, one of the defining ideological movements of the Twentieth Century. The industrial revolution produced a parallel revolution in political thought. Urbanization and capitalism greatly reshaped society. During this same period, the socialist movement began to form. In the mid-19th century, Marxism was developed, and socialism in general gained increasing popular support, mostly from the urban working class. Without breaking entirely from the past, Marx established principles that would be used by future revolutionaries of the 20th century namely Vladimir Lenin, Mao Zedong, Ho Chi Minh, and Fidel Castro. Though Hegel's philosophy of history is similar to Immanuel Kant's, and Karl Marx's theory of revolution towards the common good is partly based on Kant's view of history—Marx declared that he was turning Hegel's dialectic, which was "standing on its head", "the right side up again". Unlike Marx who believed in historical materialism, Hegel believed in the Phenomenology of Spirit. By the late 19th century, socialism and trade unions were established members of the political landscape. In addition, the various branches of anarchism, with thinkers such as Mikhail Bakunin, Pierre-Joseph Proudhon or Peter Kropotkin, and syndicalism also gained some prominence. In the Anglo-American world, anti-imperialism and pluralism began gaining currency at the turn of the 20th century.
World War I was a watershed event in human history, changing views of governments and politics. The Russian Revolution of 1917 (and similar, albeit less successful, revolutions in many other European countries) brought communism - and in particular the political theory of Leninism, but also on a smaller level Luxemburgism (gradually) - on the world stage. At the same time, social democratic parties won elections and formed governments for the first time, often as a result of the introduction of universal suffrage. However, a group of central European economists led by Austrian School economists Ludwig von Mises and Friedrich Hayek identified the collectivist underpinnings to the various new socialist and fascist doctrines of government power as being different brands of political totalitarianism.
From the end of World War II until 1971, when John Rawls published A Theory of Justice, political philosophy declined in the Anglo-American academic world, as analytic philosophers expressed skepticism about the possibility that normative judgments had cognitive content, and political science turned toward statistical methods and behavioralism. In continental Europe, on the other hand, the postwar decades saw a huge blossoming of political philosophy, with Marxism dominating the field. This was the time of Jean-Paul Sartre and Louis Althusser, and the victories of Mao Zedong in China and Fidel Castro in Cuba, as well as the events of May 1968 led to increased interest in revolutionary ideology, especially by the New Left. A number of continental European émigrés to Britain and the United States—including Karl Popper, Friedrich Hayek, Leo Strauss, Isaiah Berlin, Eric Voegelin and Judith Shklar—encouraged continued study in political philosophy in the Anglo-American world, but in the 1950s and 1960s they and their students remained at odds with the analytic establishment.
Communism remained an important focus especially during the 1950s and 1960s. Colonialism and racism were important issues that arose. In general, there was a marked trend towards a pragmatic approach to political issues, rather than a philosophical one. Much academic debate regarded one or both of two pragmatic topics: how (or whether) to apply utilitarianism to problems of political policy, or how (or whether) to apply economic models (such as rational choice theory) to political issues. The rise of feminism, LGBT social movements and the end of colonial rule and of the political exclusion of such minorities as African Americans and sexual minorities in the developed world has led to feminist, postcolonial, and multicultural thought becoming significant. This led to a challenge to the social contract by philosophers Charles W. Mills in his book The Racial Contract and Carole Patemen in her book The Sexual Contract that the social contract excluded persons of colour and women respectively.
In Anglo-American academic political philosophy, the publication of John Rawls's A Theory of Justice in 1971 is considered a milestone. Rawls used a thought experiment, the original position, in which representative parties choose principles of justice for the basic structure of society from behind a veil of ignorance. Rawls also offered a criticism of utilitarian approaches to questions of political justice. Robert Nozick's 1974 book Anarchy, State, and Utopia, which won a National Book Award, responded to Rawls from a libertarian perspective and gained academic respectability for libertarian viewpoints.
Contemporaneously with the rise of analytic ethics in Anglo-American thought, in Europe several new lines of philosophy directed at critique of existing societies arose between the 1950s and 1980s. Most of these took elements of Marxist economic analysis, but combined them with a more cultural or ideological emphasis. Out of the Frankfurt School, thinkers like Herbert Marcuse, Theodor W. Adorno, Max Horkheimer, and Jürgen Habermas combined Marxian and Freudian perspectives. Along somewhat different lines, a number of other continental thinkers—still largely influenced by Marxism—put new emphases on structuralism and on a "return to Hegel". Within the (post-) structuralist line (though mostly not taking that label) are thinkers such as Gilles Deleuze, Michel Foucault, Claude Lefort, and Jean Baudrillard. The Situationists were more influenced by Hegel; Guy Debord, in particular, moved a Marxist analysis of commodity fetishism to the realm of consumption, and looked at the relation between consumerism and dominant ideology formation.
Another debate developed around the (distinct) criticisms of liberal political theory made by Michael Walzer, Michael Sandel and Charles Taylor. The liberal-communitarian debate is often considered valuable for generating a new set of philosophical problems, rather than a profound and illuminating clash of perspective.These and other communitarians (such as Alasdair MacIntyre and Daniel A. Bell) argue that, contra liberalism, communities are prior to individuals and therefore should be the center of political focus. Communitarians tend to support greater local control as well as economic and social policies which encourage the growth of social capital.
A pair of overlapping political perspectives arising toward the end of the 20th century are republicanism (or neo- or civic-republicanism) and the capability approach. The resurgent republican movement aims to provide an alternate definition of liberty from Isaiah Berlin's positive and negative forms of liberty, namely "liberty as non-domination." Unlike liberals who understand liberty as "non-interference," "non-domination" entails individuals not being subject to the arbitrary will of anyother person. To a liberal, a slave who is not interfered with may be free, yet to a republican the mere status as a slave, regardless of how that slave is treated, is objectionable. Prominent republicans include historian Quentin Skinner, jurist Cass Sunstein, and political philosopher Philip Pettit. The capability approach, pioneered by economists Mahbub ul Haq and Amartya Sen and further developed by legal scholar Martha Nussbaum, understands freedom under allied lines: the real-world ability to act. Both the capability approach and republicanism treat choice as something which must be resourced. In other words, it is not enough to be legally able to do something, but to have the real option of doing it.
Relations between Grand Lodges are determined by the concept of Recognition. Each Grand Lodge maintains a list of other Grand Lodges that it recognises. When two Grand Lodges recognise and are in Masonic communication with each other, they are said to be in amity, and the brethren of each may visit each other's Lodges and interact Masonically. When two Grand Lodges are not in amity, inter-visitation is not allowed. There are many reasons why one Grand Lodge will withhold or withdraw recognition from another, but the two most common are Exclusive Jurisdiction and Regularity.
Since the middle of the 19th century, Masonic historians have sought the origins of the movement in a series of similar documents known as the Old Charges, dating from the Regius Poem in about 1425 to the beginning of the 18th century. Alluding to the membership of a lodge of operative masons, they relate a mythologised history of the craft, the duties of its grades, and the manner in which oaths of fidelity are to be taken on joining. The fifteenth century also sees the first evidence of ceremonial regalia.
A dispute during the Lausanne Congress of Supreme Councils of 1875 prompted the Grand Orient de France to commission a report by a Protestant pastor which concluded that, as Freemasonry was not a religion, it should not require a religious belief. The new constitutions read, "Its principles are absolute liberty of conscience and human solidarity", the existence of God and the immortality of the soul being struck out. It is possible that the immediate objections of the United Grand Lodge of England were at least partly motivated by the political tension between France and Britain at the time. The result was the withdrawal of recognition of the Grand Orient of France by the United Grand Lodge of England, a situation that continues today.
At the dawn of the Grand Lodge era, during the 1720s, James Anderson composed the first printed constitutions for Freemasons, the basis for most subsequent constitutions, which specifically excluded women from Freemasonry. As Freemasonry spread, continental masons began to include their ladies in Lodges of Adoption, which worked three degrees with the same names as the men's but different content. The French officially abandoned the experiment in the early 19th century. Later organisations with a similar aim emerged in the United States, but distinguished the names of the degrees from those of male masonry.
In contrast to Catholic allegations of rationalism and naturalism, Protestant objections are more likely to be based on allegations of mysticism, occultism, and even Satanism. Masonic scholar Albert Pike is often quoted (in some cases misquoted) by Protestant anti-Masons as an authority for the position of Masonry on these issues. However, Pike, although undoubtedly learned, was not a spokesman for Freemasonry and was also controversial among Freemasons in general. His writings represented his personal opinion only, and furthermore an opinion grounded in the attitudes and understandings of late 19th century Southern Freemasonry of the USA. Notably, his book carries in the preface a form of disclaimer from his own Grand Lodge. No one voice has ever spoken for the whole of Freemasonry.
In 1799, English Freemasonry almost came to a halt due to Parliamentary proclamation. In the wake of the French Revolution, the Unlawful Societies Act 1799 banned any meetings of groups that required their members to take an oath or obligation. The Grand Masters of both the Moderns and the Antients Grand Lodges called on Prime Minister William Pitt (who was not a Freemason) and explained to him that Freemasonry was a supporter of the law and lawfully constituted authority and was much involved in charitable work. As a result, Freemasonry was specifically exempted from the terms of the Act, provided that each private lodge's Secretary placed with the local "Clerk of the Peace" a list of the members of his lodge once a year. This continued until 1967 when the obligation of the provision was rescinded by Parliament.
In some countries anti-Masonry is often related to antisemitism and anti-Zionism. For example, In 1980, the Iraqi legal and penal code was changed by Saddam Hussein's ruling Ba'ath Party, making it a felony to "promote or acclaim Zionist principles, including Freemasonry, or who associate [themselves] with Zionist organisations". Professor Andrew Prescott of the University of Sheffield writes: "Since at least the time of the Protocols of the Elders of Zion, antisemitism has gone hand in hand with anti-masonry, so it is not surprising that allegations that 11 September was a Zionist plot have been accompanied by suggestions that the attacks were inspired by a masonic world order".
The bulk of Masonic ritual consists of degree ceremonies. Candidates for Freemasonry are progressively initiated into Freemasonry, first in the degree of Entered Apprentice. Some time later, in a separate ceremony, they will be passed to the degree of Fellowcraft, and finally they will be raised to the degree of Master Mason. In all of these ceremonies, the candidate is entrusted with passwords, signs and grips peculiar to his new rank. Another ceremony is the annual installation of the Master and officers of the Lodge. In some jurisdictions Installed Master is valued as a separate rank, with its own secrets to distinguish its members. In other jurisdictions, the grade is not recognised, and no inner ceremony conveys new secrets during the installation of a new Master of the Lodge.
English Freemasonry spread to France in the 1720s, first as lodges of expatriates and exiled Jacobites, and then as distinctively French lodges which still follow the ritual of the Moderns. From France and England, Freemasonry spread to most of Continental Europe during the course of the 18th century. The Grande Loge de France formed under the Grand Mastership of the Duke of Clermont, who exercised only nominal authority. His successor, the Duke of Orléans, reconstituted the central body as the Grand Orient de France in 1773. Briefly eclipsed during the French Revolution, French Freemasonry continued to grow in the next century.
The majority of Freemasonry considers the Liberal (Continental) strand to be Irregular, and thus withhold recognition. For the Continental lodges, however, having a different approach to Freemasonry was not a reason for severing masonic ties. In 1961, an umbrella organisation, Centre de Liaison et d'Information des Puissances maçonniques Signataires de l'Appel de Strasbourg (CLIPSAS) was set up, which today provides a forum for most of these Grand Lodges and Grand Orients worldwide. Included in the list of over 70 Grand Lodges and Grand Orients are representatives of all three of the above categories, including mixed and women's organisations. The United Grand Lodge of England does not communicate with any of these jurisdictions, and expects its allies to follow suit. This creates the distinction between Anglo-American and Continental Freemasonry.
The denomination with the longest history of objection to Freemasonry is the Roman Catholic Church. The objections raised by the Roman Catholic Church are based on the allegation that Masonry teaches a naturalistic deistic religion which is in conflict with Church doctrine. A number of Papal pronouncements have been issued against Freemasonry. The first was Pope Clement XII's In eminenti apostolatus, 28 April 1738; the most recent was Pope Leo XIII's Ab apostolici, 15 October 1890. The 1917 Code of Canon Law explicitly declared that joining Freemasonry entailed automatic excommunication, and banned books favouring Freemasonry.
In 1933, the Orthodox Church of Greece officially declared that being a Freemason constitutes an act of apostasy and thus, until he repents, the person involved with Freemasonry cannot partake of the Eucharist. This has been generally affirmed throughout the whole Eastern Orthodox Church. The Orthodox critique of Freemasonry agrees with both the Roman Catholic and Protestant versions: "Freemasonry cannot be at all compatible with Christianity as far as it is a secret organisation, acting and teaching in mystery and secret and deifying rationalism."
In addition, most Grand Lodges require the candidate to declare a belief in a Supreme Being. In a few cases, the candidate may be required to be of a specific religion. The form of Freemasonry most common in Scandinavia (known as the Swedish Rite), for example, accepts only Christians. At the other end of the spectrum, "Liberal" or Continental Freemasonry, exemplified by the Grand Orient de France, does not require a declaration of belief in any deity, and accepts atheists (a cause of discord with the rest of Freemasonry).
Exclusive Jurisdiction is a concept whereby only one Grand Lodge will be recognised in any geographical area. If two Grand Lodges claim jurisdiction over the same area, the other Grand Lodges will have to choose between them, and they may not all decide to recognise the same one. (In 1849, for example, the Grand Lodge of New York split into two rival factions, each claiming to be the legitimate Grand Lodge. Other Grand Lodges had to choose between them until the schism was healed.) Exclusive Jurisdiction can be waived when the two over-lapping Grand Lodges are themselves in Amity and agree to share jurisdiction (for example, since the Grand Lodge of Connecticut is in Amity with the Prince Hall Grand Lodge of Connecticut, the principle of Exclusive Jurisdiction does not apply, and other Grand Lodges may recognise both).
There is no clear mechanism by which these local trade organisations became today's Masonic Lodges, but the earliest rituals and passwords known, from operative lodges around the turn of the 17th–18th centuries, show continuity with the rituals developed in the later 18th century by accepted or speculative Masons, as those members who did not practice the physical craft came to be known. The minutes of the Lodge of Edinburgh (Mary's Chapel) No. 1 in Scotland show a continuity from an operative lodge in 1598 to a modern speculative Lodge. It is reputed to be the oldest Masonic Lodge in the world.
Prince Hall Freemasonry exists because of the refusal of early American lodges to admit African-Americans. In 1775, an African-American named Prince Hall, along with fourteen other African-Americans, was initiated into a British military lodge with a warrant from the Grand Lodge of Ireland, having failed to obtain admission from the other lodges in Boston. When the military Lodge left North America, those fifteen men were given the authority to meet as a Lodge, but not to initiate Masons. In 1784, these individuals obtained a Warrant from the Premier Grand Lodge of England (GLE) and formed African Lodge, Number 459. When the UGLE was formed in 1813, all U.S.-based Lodges were stricken from their rolls – due largely to the War of 1812. Thus, separated from both UGLE and any concordantly recognised U.S. Grand Lodge, African Lodge re-titled itself as the African Lodge, Number 1 – and became a de facto "Grand Lodge" (this Lodge is not to be confused with the various Grand Lodges on the Continent of Africa). As with the rest of U.S. Freemasonry, Prince Hall Freemasonry soon grew and organised on a Grand Lodge system for each state.
Maria Deraismes was initiated into Freemasonry in 1882, then resigned to allow her lodge to rejoin their Grand Lodge. Having failed to achieve acceptance from any masonic governing body, she and Georges Martin started a mixed masonic lodge that actually worked masonic ritual. Annie Besant spread the phenomenon to the English speaking world. Disagreements over ritual led to the formation of exclusively female bodies of Freemasons in England, which spread to other countries. Meanwhile, the French had re-invented Adoption as an all-female lodge in 1901, only to cast it aside again in 1935. The lodges, however, continued to meet, which gave rise, in 1959, to a body of women practising continental Freemasonry.
Many Islamic anti-Masonic arguments are closely tied to both antisemitism and Anti-Zionism, though other criticisms are made such as linking Freemasonry to al-Masih ad-Dajjal (the false Messiah). Some Muslim anti-Masons argue that Freemasonry promotes the interests of the Jews around the world and that one of its aims is to destroy the Al-Aqsa Mosque in order to rebuild the Temple of Solomon in Jerusalem. In article 28 of its Covenant, Hamas states that Freemasonry, Rotary, and other similar groups "work in the interest of Zionism and according to its instructions ..."
The preserved records of the Reichssicherheitshauptamt (the Reich Security Main Office) show the persecution of Freemasons during the Holocaust. RSHA Amt VII (Written Records) was overseen by Professor Franz Six and was responsible for "ideological" tasks, by which was meant the creation of antisemitic and anti-Masonic propaganda. While the number is not accurately known, it is estimated that between 80,000 and 200,000 Freemasons were killed under the Nazi regime. Masonic concentration camp inmates were graded as political prisoners and wore an inverted red triangle.
Freemasonry consists of fraternal organisations that trace their origins to the local fraternities of stonemasons, which from the end of the fourteenth century regulated the qualifications of stonemasons and their interaction with authorities and clients. The degrees of freemasonry retain the three grades of medieval craft guilds, those of Apprentice, Journeyman or fellow (now called Fellowcraft), and Master Mason. These are the degrees offered by Craft (or Blue Lodge) Freemasonry. Members of these organisations are known as Freemasons or Masons. There are additional degrees, which vary with locality and jurisdiction, and are usually administered by different bodies than the craft degrees.
Candidates for Freemasonry will have met most active members of the Lodge they are joining before they are initiated. The process varies between jurisdictions, but the candidate will typically have been introduced by a friend at a Lodge social function, or at some form of open evening in the Lodge. In modern times, interested people often track down a local Lodge through the Internet. The onus is on candidates to ask to join; while candidates may be encouraged to ask, they are never invited. Once the initial inquiry is made, an interview usually follows to determine the candidate's suitability. If the candidate decides to proceed from here, the Lodge ballots on the application before he (or she, depending on the Masonic Jurisdiction) can be accepted.
Freemasonry, as it exists in various forms all over the world, has a membership estimated by the United Grand Lodge of England at around six million worldwide. The fraternity is administratively organised into independent Grand Lodges (or sometimes Grand Orients), each of which governs its own Masonic jurisdiction, which consists of subordinate (or constituent) Lodges. The largest single jurisdiction, in terms of membership, is the United Grand Lodge of England (with a membership estimated at around a quarter million). The Grand Lodge of Scotland and Grand Lodge of Ireland (taken together) have approximately 150,000 members. In the United States total membership is just under two million.
The idea of Masonic brotherhood probably descends from a 16th-century legal definition of a brother as one who has taken an oath of mutual support to another. Accordingly, Masons swear at each degree to keep the contents of that degree secret, and to support and protect their brethren unless they have broken the law. In most Lodges the oath or obligation is taken on a Volume of Sacred Law, whichever book of divine revelation is appropriate to the religious beliefs of the individual brother (usually the Bible in the Anglo-American tradition). In Progressive continental Freemasonry, books other than scripture are permissible, a cause of rupture between Grand Lodges.
The earliest known American lodges were in Pennsylvania. The Collector for the port of Pennsylvania, John Moore, wrote of attending lodges there in 1715, two years before the formation of the first Grand Lodge in London. The Premier Grand Lodge of England appointed a Provincial Grand Master for North America in 1731, based in Pennsylvania. Other lodges in the colony obtained authorisations from the later Antient Grand Lodge of England, the Grand Lodge of Scotland, and the Grand Lodge of Ireland, which was particularly well represented in the travelling lodges of the British Army. Many lodges came into existence with no warrant from any Grand Lodge, applying and paying for their authorisation only after they were confident of their own survival.
Masonic lodges existed in Iraq as early as 1917, when the first lodge under the United Grand Lodge of England (UGLE) was opened. Nine lodges under UGLE existed by the 1950s, and a Scottish lodge was formed in 1923. However, the position changed following the revolution, and all lodges were forced to close in 1965. This position was later reinforced under Saddam Hussein; the death penalty was "prescribed" for those who "promote or acclaim Zionist principles, including freemasonry, or who associate [themselves] with Zionist organisations."
The ritual form on which the Grand Orient of France was based was abolished in England in the events leading to the formation of the United Grand Lodge of England in 1813. However the two jurisdictions continued in amity (mutual recognition) until events of the 1860s and 1870s drove a seemingly permanent wedge between them. In 1868 the Supreme Council of the Ancient and Accepted Scottish Rite of the State of Louisiana appeared in the jurisdiction of the Grand Lodge of Louisiana, recognised by the Grand Orient de France, but regarded by the older body as an invasion of their jurisdiction. The new Scottish rite body admitted blacks, and the resolution of the Grand Orient the following year that neither colour, race, nor religion could disqualify a man from Masonry prompted the Grand Lodge to withdraw recognition, and it persuaded other American Grand Lodges to do the same.
In 1983, the Church issued a new code of canon law. Unlike its predecessor, the 1983 Code of Canon Law did not explicitly name Masonic orders among the secret societies it condemns. It states: "A person who joins an association which plots against the Church is to be punished with a just penalty; one who promotes or takes office in such an association is to be punished with an interdict." This named omission of Masonic orders caused both Catholics and Freemasons to believe that the ban on Catholics becoming Freemasons may have been lifted, especially after the perceived liberalisation of Vatican II. However, the matter was clarified when Cardinal Joseph Ratzinger (later Pope Benedict XVI), as the Prefect of the Congregation for the Doctrine of the Faith, issued a Declaration on Masonic Associations, which states: "... the Church's negative judgment in regard to Masonic association remains unchanged since their principles have always been considered irreconcilable with the doctrine of the Church and therefore membership in them remains forbidden. The faithful who enroll in Masonic associations are in a state of grave sin and may not receive Holy Communion." For its part, Freemasonry has never objected to Catholics joining their fraternity. Those Grand Lodges in amity with UGLE deny the Church's claims. The UGLE now states that "Freemasonry does not seek to replace a Mason's religion or provide a substitute for it."
Even in modern democracies, Freemasonry is sometimes viewed with distrust. In the UK, Masons working in the justice system, such as judges and police officers, were from 1999 to 2009 required to disclose their membership. While a parliamentary inquiry found that there has been no evidence of wrongdoing, it was felt that any potential loyalties Masons might have, based on their vows to support fellow Masons, should be transparent to the public. The policy of requiring a declaration of masonic membership of applicants for judicial office (judges and magistrates) was ended in 2009 by Justice Secretary Jack Straw (who had initiated the requirement in the 1990s). Straw stated that the rule was considered disproportionate, since no impropriety or malpractice had been shown as a result of judges being Freemasons.
The Masonic Lodge is the basic organisational unit of Freemasonry. The Lodge meets regularly to conduct the usual formal business of any small organisation (pay bills, organise social and charitable events, elect new members, etc.). In addition to business, the meeting may perform a ceremony to confer a Masonic degree or receive a lecture, which is usually on some aspect of Masonic history or ritual. At the conclusion of the meeting, the Lodge might adjourn for a formal dinner, or festive board, sometimes involving toasting and song.
During the ceremony of initiation, the candidate is expected to swear (usually on a volume of sacred text appropriate to his personal religious faith) to fulfil certain obligations as a Mason. In the course of three degrees, new masons will promise to keep the secrets of their degree from lower degrees and outsiders, and to support a fellow Mason in distress (as far as practicality and the law permit). There is instruction as to the duties of a Freemason, but on the whole, Freemasons are left to explore the craft in the manner they find most satisfying. Some will further explore the ritual and symbolism of the craft, others will focus their involvement on the social side of the Lodge, while still others will concentrate on the charitable functions of the lodge.
Regularity is a concept based on adherence to Masonic Landmarks, the basic membership requirements, tenets and rituals of the craft. Each Grand Lodge sets its own definition of what these landmarks are, and thus what is Regular and what is Irregular (and the definitions do not necessarily agree between Grand Lodges). Essentially, every Grand Lodge will hold that its landmarks (its requirements, tenets and rituals) are Regular, and judge other Grand Lodges based on those. If the differences are significant, one Grand Lodge may declare the other "Irregular" and withdraw or withhold recognition.
All Freemasons begin their journey in the "craft" by being progressively initiated, passed and raised into the three degrees of Craft, or Blue Lodge Masonry. During these three rituals, the candidate is progressively taught the meanings of the Lodge symbols, and entrusted with grips, signs and words to signify to other Masons that he has been so initiated. The initiations are part allegory and part lecture, and revolve around the construction of the Temple of Solomon, and the artistry and death of his chief architect, Hiram Abiff. The degrees are those of Entered apprentice, Fellowcraft and Master Mason. While many different versions of these rituals exist, with at least two different lodge layouts and versions of the Hiram myth, each version is recognisable to any Freemason from any jurisdiction.
The first Grand Lodge, the Grand Lodge of London and Westminster (later called the Grand Lodge of England (GLE)), was founded on 24 June 1717, when four existing London Lodges met for a joint dinner. Many English Lodges joined the new regulatory body, which itself entered a period of self-publicity and expansion. However, many Lodges could not endorse changes which some Lodges of the GLE made to the ritual (they came to be known as the Moderns), and a few of these formed a rival Grand Lodge on 17 July 1751, which they called the "Antient Grand Lodge of England." These two Grand Lodges vied for supremacy until the Moderns promised to return to the ancient ritual. They united on 27 December 1813 to form the United Grand Lodge of England (UGLE).
Widespread segregation in 19th- and early 20th-century North America made it difficult for African-Americans to join Lodges outside of Prince Hall jurisdictions – and impossible for inter-jurisdiction recognition between the parallel U.S. Masonic authorities. By the 1980s, such discrimination was a thing of the past, and today most U.S. Grand Lodges recognise their Prince Hall counterparts, and the authorities of both traditions are working towards full recognition. The United Grand Lodge of England has no problem with recognising Prince Hall Grand Lodges. While celebrating their heritage as lodges of black Americans, Prince Hall is open to all men regardless of race or religion.
In general, Continental Freemasonry is sympathetic to Freemasonry amongst women, dating from the 1890s when French lodges assisted the emergent co-masonic movement by promoting enough of their members to the 33rd degree of the Ancient and Accepted Scottish Rite to allow them, in 1899, to form their own grand council, recognised by the other Continental Grand Councils of that Rite. The United Grand Lodge of England issued a statement in 1999 recognising the two women's grand lodges there to be regular in all but the participants. While they were not, therefore, recognised as regular, they were part of Freemasonry "in general". The attitude of most regular Anglo-American grand lodges remains that women Freemasons are not legitimate Masons.
Since the founding of Freemasonry, many Bishops of the Church of England have been Freemasons, such as Archbishop Geoffrey Fisher. In the past, few members of the Church of England would have seen any incongruity in concurrently adhering to Anglican Christianity and practicing Freemasonry. In recent decades, however, reservations about Freemasonry have increased within Anglicanism, perhaps due to the increasing prominence of the evangelical wing of the church. The former Archbishop of Canterbury, Dr Rowan Williams, appeared to harbour some reservations about Masonic ritual, whilst being anxious to avoid causing offence to Freemasons inside and outside the Church of England. In 2003 he felt it necessary to apologise to British Freemasons after he said that their beliefs were incompatible with Christianity and that he had barred the appointment of Freemasons to senior posts in his diocese when he was Bishop of Monmouth.
In Italy, Freemasonry has become linked to a scandal concerning the Propaganda Due lodge (a.k.a. P2). This lodge was chartered by the Grande Oriente d'Italia in 1877, as a lodge for visiting Masons unable to attend their own lodges. Under Licio Gelli's leadership, in the late 1970s, P2 became involved in the financial scandals that nearly bankrupted the Vatican Bank. However, by this time the lodge was operating independently and irregularly, as the Grand Orient had revoked its charter and expelled Gelli in 1976.
It was only in the 1980s that digital telephony transmission networks became possible, such as with ISDN networks, assuring a minimum bit rate (usually 128 kilobits/s) for compressed video and audio transmission. During this time, there was also research into other forms of digital video and audio communication. Many of these technologies, such as the Media space, are not as widely used today as videoconferencing but were still an important area of research. The first dedicated systems started to appear in the market as ISDN networks were expanding throughout the world. One of the first commercial videoconferencing systems sold to companies came from PictureTel Corp., which had an Initial Public Offering in November, 1984.
The MC controls the conferencing while it is active on the signaling plane, which is simply where the system manages conferencing creation, endpoint signaling and in-conferencing controls. This component negotiates parameters with every endpoint in the network and controls conferencing resources. While the MC controls resources and signaling negotiations, the MP operates on the media plane and receives media from each endpoint. The MP generates output streams from each endpoint and redirects the information to other endpoints in the conference.
High speed Internet connectivity has become more widely available at a reasonable cost and the cost of video capture and display technology has decreased. Consequently, personal videoconferencing systems based on a webcam, personal computer system, software compression and broadband Internet connectivity have become affordable to the general public. Also, the hardware used for this technology has continued to improve in quality, and prices have dropped dramatically. The availability of freeware (often as part of chat programs) has made software based videoconferencing accessible to many.
Videophone calls (also: videocalls, video chat as well as Skype and Skyping in verb form), differ from videoconferencing in that they expect to serve individuals, not groups. However that distinction has become increasingly blurred with technology improvements such as increased bandwidth and sophisticated software clients that can allow for multiple parties on a call. In general everyday usage the term videoconferencing is now frequently used instead of videocall for point-to-point calls between two units. Both videophone calls and videoconferencing are also now commonly referred to as a video link.
Technological developments by videoconferencing developers in the 2010s have extended the capabilities of video conferencing systems beyond the boardroom for use with hand-held mobile devices that combine the use of video, audio and on-screen drawing capabilities broadcasting in real-time over secure networks, independent of location. Mobile collaboration systems now allow multiple people in previously unreachable locations, such as workers on an off-shore oil rig, the ability to view and discuss issues with colleagues thousands of miles away. Traditional videoconferencing system manufacturers have begun providing mobile applications as well, such as those that allow for live and still image streaming.
Videoconferencing provides students with the opportunity to learn by participating in two-way communication forums. Furthermore, teachers and lecturers worldwide can be brought to remote or otherwise isolated educational facilities. Students from diverse communities and backgrounds can come together to learn about one another, although language barriers will continue to persist. Such students are able to explore, communicate, analyze and share information and ideas with one another. Through videoconferencing, students can visit other parts of the world to speak with their peers, and visit museums and educational facilities. Such virtual field trips can provide enriched learning opportunities to students, especially those in geographically isolated locations, and to the economically disadvantaged. Small schools can use these technologies to pool resources and provide courses, such as in foreign languages, which could not otherwise be offered.
One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the "Picturephone") was introduced to the public at the 1964 New York World's Fair –two deaf users were able to communicate freely with each other between the fair and another city. Various universities and other organizations, including British Telecom's Martlesham facility, have also conducted extensive research on signing via videotelephony. The use of sign language via videotelephony was hampered for many years due to the difficulty of its use over slow analogue copper phone lines, coupled with the high cost of better quality ISDN (data) phone lines. Those factors largely disappeared with the introduction of more efficient video codecs and the advent of lower cost high-speed ISDN data and IP (Internet) services in the 1990s.
In the increasingly globalized film industry, videoconferencing has become useful as a method by which creative talent in many different locations can collaborate closely on the complex details of film production. For example, for the 2013 award-winning animated film Frozen, Burbank-based Walt Disney Animation Studios hired the New York City-based husband-and-wife songwriting team of Robert Lopez and Kristen Anderson-Lopez to write the songs, which required two-hour-long transcontinental videoconferences nearly every weekday for about 14 months.
A videoconference system is generally higher cost than a videophone and deploys greater capabilities. A videoconference (also known as a videoteleconference) allows two or more locations to communicate via live, simultaneous two-way video and audio transmissions. This is often accomplished by the use of a multipoint control unit (a centralized distribution and call management system) or by a similar non-centralized multipoint capability embedded in each videoconferencing unit. Again, technology improvements have circumvented traditional definitions by allowing multiple party videoconferencing via web-based applications.
This technique was very expensive, though, and could not be used for applications such as telemedicine, distance education, and business meetings. Attempts at using normal telephony networks to transmit slow-scan video, such as the first systems developed by AT&T Corporation, first researched in the 1950s, failed mostly due to the poor picture quality and the lack of efficient video compression techniques. The greater 1 MHz bandwidth and 6 Mbit/s bit rate of the Picturephone in the 1970s also did not achieve commercial success, mostly due to its high cost, but also due to a lack of network effect —with only a few hundred Picturephones in the world, users had extremely few contacts they could actually call to, and interoperability with other videophone systems would not exist for decades.
While videoconferencing technology was initially used primarily within internal corporate communication networks, one of the first community service usages of the technology started in 1992 through a unique partnership with PictureTel and IBM Corporations which at the time were promoting a jointly developed desktop based videoconferencing product known as the PCS/1. Over the next 15 years, Project DIANE (Diversified Information and Assistance Network) grew to utilize a variety of videoconferencing platforms to create a multi-state cooperative public service and distance education network consisting of several hundred schools, neighborhood centers, libraries, science museums, zoos and parks, public assistance centers, and other community oriented organizations.
Simultaneous videoconferencing among three or more remote points is possible by means of a Multipoint Control Unit (MCU). This is a bridge that interconnects calls from several sources (in a similar way to the audio conference call). All parties call the MCU, or the MCU can also call the parties which are going to participate, in sequence. There are MCU bridges for IP and ISDN-based videoconferencing. There are MCUs which are pure software, and others which are a combination of hardware and software. An MCU is characterised according to the number of simultaneous calls it can handle, its ability to conduct transposing of data rates and protocols, and features such as Continuous Presence, in which multiple parties can be seen on-screen at once. MCUs can be stand-alone hardware devices, or they can be embedded into dedicated videoconferencing units.
Videoconferencing can enable individuals in distant locations to participate in meetings on short notice, with time and money savings. Technology such as VoIP can be used in conjunction with desktop videoconferencing to enable low-cost face-to-face business meetings without leaving the desk, especially for businesses with widespread offices. The technology is also used for telecommuting, in which employees work from home. One research report based on a sampling of 1,800 corporate employees showed that, as of June 2010, 54% of the respondents with access to video conferencing used it “all of the time” or “frequently”.
Finally, in the 1990s, Internet Protocol-based videoconferencing became possible, and more efficient video compression technologies were developed, permitting desktop, or personal computer (PC)-based videoconferencing. In 1992 CU-SeeMe was developed at Cornell by Tim Dorcey et al. In 1995 the first public videoconference between North America and Africa took place, linking a technofair in San Francisco with a techno-rave and cyberdeli in Cape Town. At the Winter Olympics opening ceremony in Nagano, Japan, Seiji Ozawa conducted the Ode to Joy from Beethoven's Ninth Symphony simultaneously across five continents in near-real time.
The core technology used in a videoconferencing system is digital compression of audio and video streams in real time. The hardware or software that performs compression is called a codec (coder/decoder). Compression rates of up to 1:500 can be achieved. The resulting digital stream of 1s and 0s is subdivided into labeled packets, which are then transmitted through a digital network of some kind (usually ISDN or IP). The use of audio modems in the transmission line allow for the use of POTS, or the Plain Old Telephone System, in some low-speed applications, such as videotelephony, because they convert the digital pulses to/from analog waves in the audio spectrum range.
Typical use of the various technologies described above include calling or conferencing on a one-on-one, one-to-many or many-to-many basis for personal, business, educational, deaf Video Relay Service and tele-medical, diagnostic and rehabilitative use or services. New services utilizing videocalling and videoconferencing, such as teachers and psychologists conducting online sessions, personal videocalls to inmates incarcerated in penitentiaries, and videoconferencing to resolve airline engineering issues at maintenance facilities, are being created or evolving on an ongoing basis.
Videoconferencing is a highly useful technology for real-time telemedicine and telenursing applications, such as diagnosis, consulting, transmission of medical images, etc... With videoconferencing, patients may contact nurses and physicians in emergency or routine situations; physicians and other paramedical professionals can discuss cases across large distances. Rural areas can use this technology for diagnostic purposes, thus saving lives and making more efficient use of health care money. For example, a rural medical center in Ohio, United States, used videoconferencing to successfully cut the number of transfers of sick infants to a hospital 70 miles (110 km) away. This had previously cost nearly $10,000 per transfer.
VRS services have become well developed nationally in Sweden since 1997 and also in the United States since the first decade of the 2000s. With the exception of Sweden, VRS has been provided in Europe for only a few years since the mid-2000s, and as of 2010 has not been made available in many European Union countries, with most European countries still lacking the legislation or the financing for large-scale VRS services, and to provide the necessary telecommunication equipment to deaf users. Germany and the Nordic countries are among the other leaders in Europe, while the United States is another world leader in the provisioning of VRS services.
In May 2005, the first high definition video conferencing systems, produced by LifeSize Communications, were displayed at the Interop trade show in Las Vegas, Nevada, able to provide video at 30 frames per second with a 1280 by 720 display resolution. Polycom introduced its first high definition video conferencing system to the market in 2006. As of the 2010s, high definition resolution for videoconferencing became a popular feature, with most major suppliers in the videoconferencing market offering it.
Some systems are capable of multipoint conferencing with no MCU, stand-alone, embedded or otherwise. These use a standards-based H.323 technique known as "decentralized multipoint", where each station in a multipoint call exchanges video and audio directly with the other stations with no central "manager" or other bottleneck. The advantages of this technique are that the video and audio will generally be of higher quality because they don't have to be relayed through a central point. Also, users can make ad-hoc multipoint calls without any concern for the availability or control of an MCU. This added convenience and quality comes at the expense of some increased network bandwidth, because every station must transmit to every other station directly.
The U.S. Social Security Administration (SSA), which oversees the world's largest administrative judicial system under its Office of Disability Adjudication and Review (ODAR), has made extensive use of videoconferencing to conduct hearings at remote locations. In Fiscal Year (FY) 2009, the U.S. Social Security Administration (SSA) conducted 86,320 videoconferenced hearings, a 55% increase over FY 2008. In August 2010, the SSA opened its fifth and largest videoconferencing-only National Hearing Center (NHC), in St. Louis, Missouri. This continues the SSA's effort to use video hearings as a means to clear its substantial hearing backlog. Since 2007, the SSA has also established NHCs in Albuquerque, New Mexico, Baltimore, Maryland, Falls Church, Virginia, and Chicago, Illinois.
Windows 8 introduced major changes to the operating system's platform and user interface to improve its user experience on tablets, where Windows was now competing with mobile operating systems, including Android and iOS. In particular, these changes included a touch-optimized Windows shell based on Microsoft's "Metro" design language, the Start screen (which displays programs and dynamically updated content on a grid of tiles), a new platform for developing apps with an emphasis on touchscreen input, integration with online services (including the ability to sync apps and settings between devices), and Windows Store, an online store for downloading and purchasing new software. Windows 8 added support for USB 3.0, Advanced Format hard drives, near field communications, and cloud computing. Additional security features were introduced, such as built-in antivirus software, integration with Microsoft SmartScreen phishing filtering service and support for UEFI Secure Boot on supported devices with UEFI firmware, to prevent malware from infecting the boot process.
Windows 8 was released to a mixed critical reception. Although reaction towards its performance improvements, security enhancements, and improved support for touchscreen devices was positive, the new user interface of the operating system was widely criticized for being potentially confusing and difficult to learn (especially when used with a keyboard and mouse instead of a touchscreen). Despite these shortcomings, 60 million Windows 8 licenses have been sold through January 2013, a number which included both upgrades and sales to OEMs for new PCs.
Windows 8 development started before Windows 7 had shipped in 2009. At the Consumer Electronics Show in January 2011, it was announced that the next version of Windows would add support for ARM system-on-chips alongside the existing x86 processors produced by vendors, especially AMD and Intel. Windows division president Steven Sinofsky demonstrated an early build of the port on prototype devices, while Microsoft CEO Steve Ballmer announced the company's goal for Windows to be "everywhere on every kind of device without compromise." Details also began to surface about a new application framework for Windows 8 codenamed "Jupiter", which would be used to make "immersive" applications using XAML (similarly to Windows Phone and Silverlight) that could be distributed via a new packaging system and a rumored application store.
Three milestone releases of Windows 8 leaked to the general public. Milestone 1, Build 7850, was leaked on April 12, 2011. It was the first build where the text of a window was written centered instead of aligned to the left. It was also probably the first appearance of the Metro-style font, and its wallpaper had the text shhh... let's not leak our hard work. However, its detailed build number reveals that the build was created on September 22, 2010. The leaked copy edition was Enterprise edition. The OS still reads as "Windows 7". Milestone 2, Build 7955, was leaked on April 25, 2011. The traditional Blue Screen of Death (BSoD) was replaced by a new Black screen, although this was later scrapped. This build introduced a new ribbon in Windows Explorer. Build 7959, with minor changes but the first 64-bit version, was leaked on May 1, 2011. The "Windows 7" logo was temporarily replaced with text displaying "Microsoft Confidential". On June 17, 2011, build 7989 64-bit edition was leaked. It introduced a new boot screen featuring the same fish as the default Windows 7 Beta wallpaper, which was later scrapped, and the circling dots as featured in the final (although the final version comes with smaller circling dots throbber). It also had the text Welcome below them, although this was also scrapped.
The build was released for download later in the day in standard 32-bit and 64-bit versions, plus a special 64-bit version which included SDKs and developer tools (Visual Studio Express and Expression Blend) for developing Metro-style apps. The Windows Store was announced during the presentation, but was not available in this build. According to Microsoft, there were about 535,000 downloads of the developer preview within the first 12 hours of its release. Originally set to expire on March 11, 2012, in February 2012 the Developer Preview's expiry date was changed to January 15, 2013.
On February 29, 2012, Microsoft released Windows 8 Consumer Preview, the beta version of Windows 8, build 8250. Alongside other changes, the build removed the Start button from the taskbar for the first time since its debut on Windows 95; according to Windows manager Chaitanya Sareen, the Start button was removed to reflect their view that on Windows 8, the desktop was an "app" itself, and not the primary interface of the operating system. Windows president Steven Sinofsky said more than 100,000 changes had been made since the developer version went public. The day after its release, Windows 8 Consumer Preview had been downloaded over one million times. Like the Developer Preview, the Consumer Preview expired on January 15, 2013.
Many other builds were released until the Japan's Developers Day conference, when Steven Sinofsky announced that Windows 8 Release Preview (build 8400) would be released during the first week of June. On May 28, 2012, Windows 8 Release Preview (Standard Simplified Chinese x64 edition, not China-specific version, build 8400) was leaked online on various Chinese and BitTorrent websites. On May 31, 2012, Windows 8 Release Preview was released to the public by Microsoft. Major items in the Release Preview included the addition of Sports, Travel, and News apps, along with an integrated version of Adobe Flash Player in Internet Explorer. Like the Developer Preview and the Consumer Preview, the release preview expired on January 15, 2013.
On August 1, 2012, Windows 8 (build 9200) was released to manufacturing with the build number 6.2.9200.16384 . Microsoft planned to hold a launch event on October 25, 2012 and release Windows 8 for general availability on the next day. However, only a day after its release to manufacturing, a copy of the final version of Windows 8 Enterprise N (a version for European markets lacking bundled media players to comply with a court ruling) leaked online, followed by leaks of the final versions of Windows 8 Pro and Enterprise a few days later. On August 15, 2012, Windows 8 was made available to download for MSDN and TechNet subscribers. Windows 8 was made available to Software Assurance customers on August 16, 2012. Windows 8 was made available for students with a DreamSpark Premium subscription on August 22, 2012, earlier than advertised.
Relatively few changes were made from the Release Preview to the final version; these included updated versions of its pre-loaded apps, the renaming of Windows Explorer to File Explorer, the replacement of the Aero Glass theme from Windows Vista and 7 with a new flat and solid-colored theme, and the addition of new background options for the Start screen, lock screen, and desktop. Prior to its general availability on October 26, 2012, updates were released for some of Windows 8's bundled apps, and a "General Availability Cumulative Update" (which included fixes to improve performance, compatibility, and battery life) was released on Tuesday, October 9, 2012. Microsoft indicated that due to improvements to its testing infrastructure, general improvements of this nature are to be released more frequently through Windows Update instead of being relegated to OEMs and service packs only.
Microsoft began an advertising campaign centered around Windows 8 and its Surface tablet in October 2012, starting with its first television advertisement premiering on October 14, 2012. Microsoft's advertising budget of US$1.5–1.8 billion was significantly larger than the US$200 million campaign used to promote Windows 95. As part of its campaign, Microsoft set up 34 pop-up stores inside malls (primarily focusing on Surface), provided training for retail employees in partnership with Intel, and collaborated with the electronics store chain Best Buy to design expanded spaces to showcase devices. In an effort to make retail displays of Windows 8 devices more "personal", Microsoft also developed a character known in English-speaking markets as "Allison Brown", whose fictional profile (including personal photos, contacts, and emails) is also featured on demonstration units of Windows 8 devices.
In May 2013, Microsoft launched a new television campaign for Windows 8 illustrating the capabilities and pricing of Windows 8 tablets in comparison to the iPad, which featured the voice of Siri remarking on the iPad's limitations in a parody of Apple's "Get a Mac" advertisements. On June 12, 2013 during game 1 of the 2013 Stanley Cup Finals, Microsoft premiered the first ad in its "Windows Everywhere" campaign, which promoted Windows 8, Windows Phone 8, and the company's suite of online services as an interconnected platform.
New features and functionality in Windows 8 include a faster startup through UEFI integration and the new "Hybrid Boot" mode (which hibernates the Windows kernel on shutdown to speed up the subsequent boot), a new lock screen with a clock and notifications, and the ability for enterprise users to create live USB versions of Windows (known as Windows To Go). Windows 8 also adds native support for USB 3.0 devices, which allow for faster data transfers and improved power management with compatible devices, and hard disk 4KB Advanced Format support, as well as support for near field communication to facilitate sharing and communication between devices.
Windows Explorer, which has been renamed File Explorer, now includes a ribbon in place of the command bar. File operation dialog boxes have been updated to provide more detailed statistics, the ability to pause file transfers, and improvements in the ability to manage conflicts when copying files. A new "File History" function allows incremental revisions of files to be backed up to and restored from a secondary storage device, while Storage Spaces allows users to combine different sized hard disks into virtual drives and specify mirroring, parity, or no redundancy on a folder-by-folder basis.
Task Manager has been redesigned, including a new processes tab with the option to display fewer or more details of running applications and background processes, a heat map using different colors indicating the level of resource usage, network and disk counters, grouping by process type (e.g. applications, background processes and Windows processes), friendly names for processes and a new option which allows users to search the web to find information about obscure processes. Additionally, the Blue Screen of Death has been updated with a simpler and modern design with less technical information displayed.
New security features in Windows 8 include two new authentication methods tailored towards touchscreens (PINs and picture passwords), the addition of antivirus capabilities to Windows Defender (bringing it in parity with Microsoft Security Essentials). SmartScreen filtering integrated into Windows, Family Safety offers Parental controls, which allows parents to monitor and manage their children's activities on a device with activity reports and safety controls. Windows 8 also provides integrated system recovery through the new "Refresh" and "Reset" functions, including system recovery from USB drive. Windows 8's first security patches would be released on November 13, 2012; it would contain three fixes deemed "critical" by the company.
Windows 8 supports a feature of the UEFI specification known as "Secure boot", which uses a public-key infrastructure to verify the integrity of the operating system and prevent unauthorized programs such as bootkits from infecting the device's boot process. Some pre-built devices may be described as "certified" by Microsoft; these must have secure boot enabled by default, and provide ways for users to disable or re-configure the feature. ARM-based Windows RT devices must have secure boot permanently enabled.
Windows 8 provides heavier integration with online services from Microsoft and others. A user can now log in to Windows with a Microsoft account, which can be used to access services and synchronize applications and settings between devices. Windows 8 also ships with a client app for Microsoft's SkyDrive cloud storage service, which also allows apps to save files directly to SkyDrive. A SkyDrive client for the desktop and File Explorer is not included in Windows 8, and must be downloaded separately. Bundled multimedia apps are provided under the Xbox brand, including Xbox Music, Xbox Video, and the Xbox SmartGlass companion for use with an Xbox 360 console. Games can integrate into an Xbox Live hub app, which also allows users to view their profile and gamerscore. Other bundled apps provide the ability to link Flickr and Facebook. Due to Facebook Connect service changes, Facebook support is disabled in all bundled apps effective June 8, 2015.
Internet Explorer 10 is included as both a desktop program and a touch-optimized app, and includes increased support for HTML5, CSS3, and hardware acceleration. The Internet Explorer app does not support plugins or ActiveX components, but includes a version of Adobe Flash Player that is optimized for touch and low power usage. Initially, Adobe Flash would only work on sites included on a "Compatibility View" whitelist; however, after feedback from users and additional compatibility tests, an update in March 2013 changed this behavior to use a smaller blacklist of sites with known compatibility issues instead, allowing Flash to be used on most sites by default. The desktop version does not contain these limitations.
Windows 8 also incorporates improved support for mobile broadband; the operating system can now detect the insertion of a SIM card and automatically configure connection settings (including APNs and carrier branding), and reduce its internet usage in order to conserve bandwidth on metered networks. Windows 8 also adds an integrated airplane mode setting to globally disable all wireless connectivity as well. Carriers can also offer account management systems through Windows Store apps, which can be automatically installed as a part of the connection process and offer usage statistics on their respective tile.
Windows 8 introduces a new style of application, Windows Store apps. According to Microsoft developer Jensen Harris, these apps are to be optimized for touchscreen environments and are more specialized than current desktop applications. Apps can run either in a full-screen mode, or be snapped to the side of a screen. Apps can provide toast notifications on screen or animate their tiles on the Start screen with dynamic content. Apps can use "contracts"; a collection of hooks to provide common functionality that can integrate with other apps, including search and sharing. Apps can also provide integration with other services; for example, the People app can connect to a variety of different social networks and services (such as Facebook, Skype, and People service), while the Photos app can aggregate photos from services such as Facebook and Flickr.
Windows Store apps run within a new set of APIs known as Windows Runtime, which supports programming languages such as C, C++, Visual Basic .NET, C#, along with HTML5 and JavaScript. If written in some "high-level" languages, apps written for Windows Runtime can be compatible with both Intel and ARM versions of Windows, otherwise they are not binary code compatible. Components may be compiled as Windows Runtime Components, permitting consumption by all compatible languages. To ensure stability and security, apps run within a sandboxed environment, and require permissions to access certain functionality, such as accessing the Internet or a camera.
Retail versions of Windows 8 are only able to install these apps through Windows Store—a namesake distribution platform which offers both apps, and listings for desktop programs certified for comparability with Windows 8. A method to sideload apps from outside Windows Store is available to devices running Windows 8 Enterprise and joined to a domain; Windows 8 Pro and Windows RT devices that are not part of a domain can also sideload apps, but only after special product keys are obtained through volume licensing.
The term "Immersive app" had been used internally by Microsoft developers to refer to the apps prior to the first official presentation of Windows 8, after which they were referred to as "Metro-style apps" in reference to the Metro design language. The term was phased out in August 2012; a Microsoft spokesperson denied rumors that the change was related to a potential trademark issue, and stated that "Metro" was only a codename that would be replaced prior to Windows 8's release. Following these reports, the terms "Modern UI-style apps", "Windows 8-style apps" and "Windows Store apps" began to be used by various Microsoft documents and material to refer to the new apps. In an interview on September 12, 2012, Soma Somasegar (vice president of Microsoft's development software division) confirmed that "Windows Store apps" would be the official term for the apps. An MSDN page explaining the Metro design language uses the term "Modern design" to refer to the language as a whole.
Exceptions to the restrictions faced by Windows Store apps are given to web browsers. The user's default browser can distribute a Metro-style web browser in same package as the desktop version, which has access to functionality unavailable to other apps, such as being able to permanently run in the background, use multiple background processes, and use Windows API code instead of WinRT (allowing for code to be re-used with the desktop version, while still taking advantage of features available to Windows Store apps, such as charms). Microsoft advertises this exception privilege "New experience enabled" (formerly "Metro-style enabled").
The developers of both Chrome and Firefox committed to developing Metro-style versions of their browsers; while Chrome's "Windows 8 mode" uses a full-screen version of the existing desktop interface, Firefox's version (which was first made available on the "Aurora" release channel in September 2013) uses a touch-optimized interface inspired by the Android version of Firefox. In October 2013, Chrome's app was changed to mimic the desktop environment used by Chrome OS. Development of the Firefox app for Windows 8 has since been cancelled, citing a lack of user adoption for the beta versions.
Windows 8 introduces significant changes to the operating system's user interface, many of which are aimed at improving its experience on tablet computers and other touchscreen devices. The new user interface is based on Microsoft's Metro design language, and uses a Start screen similar to that of Windows Phone 7 as the primary means of launching applications. The Start screen displays a customizable array of tiles linking to various apps and desktop programs, some of which can display constantly updated information and content through "live tiles". As a form of multi-tasking, apps can be snapped to the side of a screen. Alongside the traditional Control Panel, a new simplified and touch-optimized settings app known as "PC Settings" is used for basic configuration and user settings. It does not include many of the advanced options still accessible from the normal Control Panel.
A vertical toolbar known as the charms (accessed by swiping from the right edge of a touchscreen, or pointing the cursor at hotspots in the right corners of a screen) provides access to system and app-related functions, such as search, sharing, device management, settings, and a Start button. The traditional desktop environment for running desktop applications is accessed via a tile on the Start screen. The Start button on the taskbar from previous versions of Windows has been converted into a hotspot in the lower-left corner of the screen, which displays a large tooltip displaying a thumbnail of the Start screen. Swiping from the left edge of a touchscreen or clicking in the top-left corner of the screen allows one to switch between apps and Desktop. Pointing the cursor in the top-left corner of the screen and moving down reveals a thumbnail list of active apps. Aside from the removal of the Start button and the replacement of the Aero Glass theme with a flatter and solid-colored design, the desktop interface on Windows 8 is similar to that of Windows 7.
Several notable features have been removed in Windows 8, beginning with the traditional Start menu. Support for playing DVD-Video was removed from Windows Media Player due to the cost of licensing the necessary decoders (especially for devices which do not include optical disc drives at all) and the prevalence of online streaming services. For the same reasons, Windows Media Center is not included by default on Windows 8, but Windows Media Center and DVD playback support can be purchased in the "Pro Pack" (which upgrades the system to Windows 8 Pro) or "Media Center Pack" add-on for Windows 8 Pro. As with prior versions, third-party DVD player software can still be used to enable DVD playback.
Backup and Restore, the backup component of Windows, is deprecated. It still ships with Windows 8 and continues to work on preset schedules, but is pushed to the background and can only be accessed through a Control Panel applet called "Windows 7 File Recovery".:76 Shadow Copy, a component of Windows Explorer that once saved previous versions of changed files, no longer protects local files and folders. It can only access previous versions of shared files stored on a Windows Server computer.:74 The subsystem on which these components worked, however, is still available for other software to use.:74
Microsoft released minimum hardware requirements for tablet and laplet devices to be "certified" for Windows 8, and defined a convertible form factor as a standalone device that combines the PC, display and rechargeable power source with a mechanically attached keyboard and pointing device in a single chassis. A convertible can be transformed into a tablet where the attached input devices are hidden or removed leaving the display as the only input mechanism. On March 12, 2013, Microsoft amended its certification requirements to only require that screens on tablets have a minimum resolution of 1024×768 (down from the previous 1366×768). The amended requirement is intended to allow "greater design flexibility" for future products.
Windows 8 is available in three different editions, of which the lowest version, branded simply as Windows 8, and Windows 8 Pro, were sold at retail in most countries, and as pre-loaded software on new computers. Each edition of Windows 8 includes all of the capabilities and features of the edition below it, and add additional features oriented towards their market segments. For example, Pro added BitLocker, Hyper-V, the ability to join a domain, and the ability to install Windows Media Center as a paid add-on. Users of Windows 8 can purchase a "Pro Pack" license that upgrades their system to Windows 8 Pro through Add features to Windows. This license also includes Windows Media Center. Windows 8 Enterprise contains additional features aimed towards business environments, and is only available through volume licensing. A port of Windows 8 for ARM architecture, Windows RT, is marketed as an edition of Windows 8, but was only included as pre-loaded software on devices specifically developed for it.
Windows 8 was distributed as a retail box product on DVD, and through a digital download that could be converted into DVD or USB install media. As part of a launch promotion, Microsoft offered Windows 8 Pro upgrades at a discounted price of US$39.99 online, or $69.99 for retail box from its launch until January 31, 2013; afterward the Windows 8 price has been $119.99 and the Pro price $199.99. Those who purchased new PCs pre-loaded with Windows 7 Home Basic, Home Premium, Professional, or Ultimate between June 2, 2012 and January 31, 2013 could digitally purchase a Windows 8 Pro upgrade for US$14.99. Several PC manufacturers offered rebates and refunds on Windows 8 upgrades obtained through the promotion on select models, such as Hewlett-Packard (in the U.S. and Canada on select models), and Acer (in Europe on selected Ultrabook models). During these promotions, the Windows Media Center add-on for Windows 8 Pro was also offered for free.
Unlike previous versions of Windows, Windows 8 was distributed at retail in "Upgrade" licenses only, which require an existing version of Windows to install. The "full version software" SKU, which was more expensive but could be installed on computers without an eligible OS or none at all, was discontinued. In lieu of full version, a specialized "System Builder" SKU was introduced. The "System Builder" SKU replaced the original equipment manufacturer (OEM) SKU, which was only allowed to be used on PCs meant for resale, but added a "Personal Use License" exemption that officially allowed its purchase and personal use by users on homebuilt computers.
Retail distribution of Windows 8 has since been discontinued in favor of Windows 8.1. Unlike 8, 8.1 is available as "full version software" at both retail and online for download that does not require a previous version of Windows in order to be installed. Pricing for these new copies remain identical. With the retail release returning to full version software for Windows 8.1, the "Personal Use License" exemption was removed from the OEM SKU, meaning that end users building their own PCs for personal use must use the full retail version in order to satisfy the Windows 8.1 licensing requirements. Windows 8.1 with Bing is a special OEM-specific SKU of Windows 8.1 subsidized by Microsoft's Bing search engine.
The three desktop editions of Windows 8 support 32-bit and 64-bit architectures; retail copies of Windows 8 include install DVDs for both architectures, while the online installer automatically installs the version corresponding with the architecture of the system's existing Windows installation. The 32-bit version runs on CPUs compatible with x86 architecture 3rd generation (known as IA-32) or newer, and can run 32-bit and 16-bit applications, although 16-bit support must be enabled first. (16-bit applications are developed for CPUs compatible with x86 2nd generation, first conceived in 1978. Microsoft started moving away from this architecture after Windows 95.)
Windows RT, the only edition of Windows 8 for systems with ARM processors, only supports applications included with the system (such as a special version of Office 2013), supplied through Windows Update, or Windows Store apps, to ensure that the system only runs applications that are optimized for the architecture. Windows RT does not support running IA-32 or x64 applications. Windows Store apps can either support both the x86 and ARM architectures, or compiled to support a specific architecture.
Following the unveiling of Windows 8, Microsoft faced criticism (particularly from free software supporters) for mandating that devices receiving its optional certification for Windows 8 have secure boot enabled by default using a key provided by Microsoft. Concerns were raised that secure boot could prevent or hinder the use of alternate operating systems such as Linux. In a post discussing secure boot on the Building Windows 8 blog, Microsoft developer Tony Mangefeste indicated that vendors would provide means to customize secure boot, stating that "At the end of the day, the customer is in control of their PC. Microsoft's philosophy is to provide customers with the best experience first, and allow them to make decisions themselves." Microsoft's certification guidelines for Windows 8 ultimately revealed that vendors would be required to provide means for users to re-configure or disable secure boot in their device's UEFI firmware. It also revealed that ARM devices (Windows RT) would be required to have secure boot permanently enabled, with no way for users to disable it. However, Tom Warren of The Verge noted that other vendors have implemented similar hardware restrictions on their own ARM-based tablet and smartphone products (including those running Microsoft's own Windows Phone platform), but still argued that Microsoft should "keep a consistent approach across ARM and x86, though, not least because of the number of users who'd love to run Android alongside Windows 8 on their future tablets." No mandate is made regarding the installation of third-party certificates that would enable running alternative programs.
Several notable video game developers criticized Microsoft for making its Windows Store a closed platform subject to its own regulations, as it conflicted with their view of the PC as an open platform. Markus "Notch" Persson (creator of the indie game Minecraft), Gabe Newell (co-founder of Valve Corporation and developer of software distribution platform Steam), and Rob Pardo from Activision Blizzard voiced concern about the closed nature of the Windows Store. However, Tom Warren of The Verge stated that Microsoft's addition of the Store was simply responding to the success of both Apple and Google in pursuing the "curated application store approach."
Reviews of the various editions of Windows 8 have been mixed. Tom Warren of The Verge said that although Windows 8's emphasis on touch computing was significant and risked alienating desktop users, a "tablet PC with Windows 8 makes an iPad feel immediately out of date" due to the capabilities of the operating system's hybrid model and increased focus on cloud services. David Pierce of The Verge described Windows 8 as "the first desktop operating system that understands what a computer is supposed to do in 2012" and praised Microsoft's "no compromise" approach and the operating system's emphasis on Internet connectivity and cloud services. Pierce also considered the Start Screen to be a "brilliant innovation for desktop computers" when compared with "folder-littered desktops on every other OS" because it allows users to interact with dynamic information. In contrast, an ExtremeTech article said it was Microsoft "flailing" and a review in PC Magazine condemned the Metro-style user interface. Some of the included apps in Windows 8 were considered to be basic and lacking in functionality, but the Xbox apps were praised for their promotion of a multi-platform entertainment experience. Other improvements and features (such as File History, Storage Spaces, and the updated Task Manager) were also regarded as positive changes. Peter Bright of Ars Technica wrote that while its user interface changes may overshadow them, Windows 8's improved performance, updated file manager, new storage functionality, expanded security features, and updated Task Manager were still positive improvements for the operating system. Bright also said that Windows 8's duality towards tablets and traditional PCs was an "extremely ambitious" aspect of the platform as well, but criticized Microsoft for emulating Apple's model of a closed distribution platform when implementing the Windows Store.
The interface of Windows 8 has been the subject of mixed reaction. Bright wrote that its system of hot corners and edge swiping "wasn't very obvious" due to the lack of instructions provided by the operating system on the functions accessed through the user interface, even by the video tutorial added on the RTM release (which only instructed users to point at corners of the screen or swipe from its sides). Despite this "stumbling block", Bright said that Windows 8's interface worked well in some places, but began to feel incoherent when switching between the "Metro" and desktop environments, sometimes through inconsistent means. Tom Warren of The Verge wrote that the new interface was "as stunning as it is surprising", contributing to an "incredibly personal" experience once it is customized by the user, but had a steep learning curve, and was awkward to use with a keyboard and mouse. He noted that while forcing all users to use the new touch-oriented interface was a risky move for Microsoft as a whole, it was necessary in order to push development of apps for the Windows Store. Others, such as Adrian Kingsley-Hughes from ZDNet, considered the interface to be "clumsy and impractical" due to its inconsistent design (going as far as considering it "two operating systems unceremoniously bolted together"), and concluded that "Windows 8 wasn't born out of a need or demand; it was born out of a desire on Microsoft's part to exert its will on the PC industry and decide to shape it in a direction—touch and tablets -- that allows it to compete against, and remain relevant in the face of Apple's iPad."
However, according to research firm NPD, sales of devices running Windows in the United States have declined 21 percent compared to the same time period in 2011. As the holiday shopping season wrapped up, Windows 8 sales continued to lag, even as Apple reported brisk sales. The market research firm IDC reported an overall drop in PC sales for the quarter, and said the drop may have been partly due to consumer reluctance to embrace the new features of the OS and poor support from OEM for these features. This capped the first year of declining PC sales to the Asia Pacific region, as consumers bought more mobile devices than Windows PCs.
Windows 8 surpassed Windows Vista in market share with a 5.1% usage rate according to numbers posted in July 2013 by Net Applications, with usage on a steady upward trajectory. However, intake of Windows 8 still lags behind that of Windows Vista and Windows 7 at the same point in their release cycles. Windows 8's tablet market share has also been growing steadily, with 7.4% of tablets running Windows in Q1 2013 according to Strategy Analytics, up from nothing just a year before. However, this is still well below Android and iOS, which posted 43.4% and 48.2% market share respectively, although both operating systems have been on the market much longer than Windows 8. Strategy Analytics also noted "a shortage of top tier apps" for Windows tablets despite Microsoft strategy of paying developers to create apps for the operating system (in addition to for Windows Phone).
In March 2013, Microsoft also amended its certification requirements to allow tablets to use the 1024×768 resolution as a minimum; this change is expected to allow the production of certified Windows 8 tablets in smaller form factors—a market which is currently dominated by Android-based tablets. Despite the reaction of industry experts, Microsoft reported that they had sold 100 million licenses in the first six months. This matched sales of Windows 7 over a similar period. This statistic includes shipments to channel warehouses which now need to be sold in order to make way for new shipments.
In February 2014, Bloomberg reported that Microsoft would be lowering the price of Windows 8 licenses by 70% for devices that retail under US$250; alongside the announcement that an update to the operating system would allow OEMs to produce devices with as little as 1 GB of RAM and 16 GB of storage, critics felt that these changes would help Windows compete against Linux-based devices in the low-end market, particularly those running Chrome OS. Microsoft had similarly cut the price of Windows XP licenses to compete against the early waves of Linux-based netbooks. Reports also indicated that Microsoft was planning to offer cheaper Windows 8 licenses to OEMs in exchange for setting Internet Explorer's default search engine to Bing. Some media outlets falsely reported that the SKU associated with this plan, "Windows 8.1 with Bing", was a variant which would be a free or low-cost version of Windows 8 for consumers using older versions of Windows. On April 2, 2014, Microsoft ultimately announced that it would be removing license fees entirely for devices with screens smaller than 9 inches, and officially confirmed the rumored "Windows 8.1 with Bing" OEM SKU on May 23, 2014.
In May 2014, the Government of China banned the internal purchase of Windows 8-based products under government contracts requiring "energy-efficient" devices. The Xinhua News Agency claimed that Windows 8 was being banned in protest of Microsoft's support lifecycle policy and the end of support for Windows XP (which, as of January 2014, had a market share of 49% in China), as the government "obviously cannot ignore the risks of running OS [sic] without guaranteed technical support." However, Ni Guangnan of the Chinese Academy of Sciences had also previously warned that Windows 8 could allegedly expose users to surveillance by the United States government due to its heavy use of internet-based services.
In June 2014, state broadcaster China Central Television (CCTV) broadcast a news story further characterizing Windows 8 as a threat to national security. The story featured an interview with Ni Guangnan, who stated that operating systems could aggregate "sensitive user information" that could be used to "understand the conditions and activities of our national economy and society", and alleged that per documents leaked by Edward Snowden, the U.S. government had worked with Microsoft to retrieve encrypted information. Yang Min, a computer scientist at Fudan University, also stated that "the security features of Windows 8 are basically to the benefit of Microsoft, allowing them control of the users' data, and that poses a big challenge to the national strategy for information security." Microsoft denied the claims in a number of posts on the Chinese social network Sina Weibo, which stated that the company had never "assisted any government in an attack of another government or clients" or provided client data to the U.S. government, never "provided any government the authority to directly visit" or placed any backdoors in its products and services, and that it had never concealed government requests for client data.
An upgrade to Windows 8 known as Windows 8.1 was officially announced by Microsoft on May 14, 2013. Following a presentation devoted to the upgrade at Build 2013, a public beta version of the upgrade was released on June 26, 2013. Windows 8.1 was released to OEM hardware partners on August 27, 2013, and released publicly as a free download through Windows Store on October 17, 2013. Volume license customers and subscribers to MSDN Plus and TechNet Plus were initially unable to obtain the RTM version upon its release; a spokesperson said the policy was changed to allow Microsoft to work with OEMs "to ensure a quality experience at general availability." However, after criticism, Microsoft reversed its decision and released the RTM build on MSDN and TechNet on September 9, 2013.
The upgrade addressed a number of criticisms faced by Windows 8 upon its release, with additional customization options for the Start screen, the restoration of a visible Start button on the desktop, the ability to snap up to four apps on a single display, and the ability to boot to the desktop instead of the Start screen. Windows 8's stock apps were also updated, a new Bing-based unified search system was added, SkyDrive was given deeper integration with the operating system, and a number of new stock apps, along with a tutorial, were added. Windows 8.1 also added support for 3D printing, Miracast media streaming, NFC printing, and Wi-Fi Direct.
John was born to Henry II of England and Eleanor of Aquitaine on 24 December 1166. Henry had inherited significant territories along the Atlantic seaboard—Anjou, Normandy and England—and expanded his empire by conquering Brittany. Henry married the powerful Eleanor of Aquitaine, who reigned over the Duchy of Aquitaine and had a tenuous claim to Toulouse and Auvergne in southern France, in addition to being the former wife of Louis VII of France. The result was the Angevin Empire, named after Henry's paternal title as Count of Anjou and, more specifically, its seat in Angers.[nb 2] The Empire, however, was inherently fragile: although all the lands owed allegiance to Henry, the disparate parts each had their own histories, traditions and governance structures. As one moved south through Anjou and Aquitaine, the extent of Henry's power in the provinces diminished considerably, scarcely resembling the modern concept of an empire at all. Some of the traditional ties between parts of the empire such as Normandy and England were slowly dissolving over time. It was unclear what would happen to the empire on Henry's death. Although the custom of primogeniture, under which an eldest son would inherit all his father's lands, was slowly becoming more widespread across Europe, it was less popular amongst the Norman kings of England. Most believed that Henry would divide the empire, giving each son a substantial portion, and hoping that his children would continue to work together as allies after his death. To complicate matters, much of the Angevin empire was held by Henry only as a vassal of the King of France of the rival line of the House of Capet. Henry had often allied himself with the Holy Roman Emperor against France, making the feudal relationship even more challenging.
In 1173 John's elder brothers, backed by Eleanor, rose in revolt against Henry in the short-lived rebellion of 1173 to 1174. Growing irritated with his subordinate position to Henry II and increasingly worried that John might be given additional lands and castles at his expense, Henry the Young King travelled to Paris and allied himself with Louis VII. Eleanor, irritated by her husband's persistent interference in Aquitaine, encouraged Richard and Geoffrey to join their brother Henry in Paris. Henry II triumphed over the coalition of his sons, but was generous to them in the peace settlement agreed at Montlouis. Henry the Young King was allowed to travel widely in Europe with his own household of knights, Richard was given Aquitaine back, and Geoffrey was allowed to return to Brittany; only Eleanor was imprisoned for her role in the revolt.
After Richard's death on 6 April 1199 there were two potential claimants to the Angevin throne: John, whose claim rested on being the sole surviving son of Henry II, and young Arthur I of Brittany, who held a claim as the son of John's elder brother Geoffrey. Richard appears to have started to recognise John as his heir presumptive in the final years before his death, but the matter was not clear-cut and medieval law gave little guidance as to how the competing claims should be decided. With Norman law favouring John as the only surviving son of Henry II and Angevin law favouring Arthur as the only son of Henry's elder son, the matter rapidly became an open conflict. John was supported by the bulk of the English and Norman nobility and was crowned at Westminster, backed by his mother, Eleanor. Arthur was supported by the majority of the Breton, Maine and Anjou nobles and received the support of Philip II, who remained committed to breaking up the Angevin territories on the continent. With Arthur's army pressing up the Loire valley towards Angers and Philip's forces moving down the valley towards Tours, John's continental empire was in danger of being cut in two.
Although John was the Count of Poitou and therefore the rightful feudal lord over the Lusignans, they could legitimately appeal John's actions in France to his own feudal lord, Philip. Hugh did exactly this in 1201 and Philip summoned John to attend court in Paris in 1202, citing the Le Goulet treaty to strengthen his case. John was unwilling to weaken his authority in western France in this way. He argued that he need not attend Philip's court because of his special status as the Duke of Normandy, who was exempt by feudal tradition from being called to the French court. Philip argued that he was summoning John not as the Duke of Normandy, but as the Count of Poitou, which carried no such special status. When John still refused to come, Philip declared John in breach of his feudal responsibilities, reassigned all of John's lands that fell under the French crown to Arthur – with the exception of Normandy, which he took back for himself – and began a fresh war against John.
The nature of government under the Angevin monarchs was ill-defined and uncertain. John's predecessors had ruled using the principle of vis et voluntas, or "force and will", taking executive and sometimes arbitrary decisions, often justified on the basis that a king was above the law. Both Henry II and Richard had argued that kings possessed a quality of "divine majesty"; John continued this trend and claimed an "almost imperial status" for himself as ruler. During the 12th century, there were contrary opinions expressed about the nature of kingship, and many contemporary writers believed that monarchs should rule in accordance with the custom and the law, and take counsel of the leading members of the realm. There was as yet no model for what should happen if a king refused to do so. Despite his claim to unique authority within England, John would sometimes justify his actions on the basis that he had taken council with the barons. Modern historians remain divided as to whether John suffered from a case of "royal schizophrenia" in his approach to government, or if his actions merely reflected the complex model of Angevin kingship in the early 13th century.
At the start of John's reign there was a sudden change in prices, as bad harvests and high demand for food resulted in much higher prices for grain and animals. This inflationary pressure was to continue for the rest of the 13th century and had long-term economic consequences for England. The resulting social pressures were complicated by bursts of deflation that resulted from John's military campaigns. It was usual at the time for the king to collect taxes in silver, which was then re-minted into new coins; these coins would then be put in barrels and sent to royal castles around the country, to be used to hire mercenaries or to meet other costs. At those times when John was preparing for campaigns in Normandy, for example, huge quantities of silver had to be withdrawn from the economy and stored for months, which unintentionally resulted in periods during which silver coins were simply hard to come by, commercial credit difficult to acquire and deflationary pressure placed on the economy. The result was political unrest across the country. John attempted to address some of the problems with the English currency in 1204 and 1205 by carrying out a radical overhaul of the coinage, improving its quality and consistency.
John, the youngest of five sons of King Henry II of England and Eleanor of Aquitaine, was at first not expected to inherit significant lands. Following the failed rebellion of his elder brothers between 1173 and 1174, however, John became Henry's favourite child. He was appointed the Lord of Ireland in 1177 and given lands in England and on the continent. John's elder brothers William, Henry and Geoffrey died young; by the time Richard I became king in 1189, John was a potential heir to the throne. John unsuccessfully attempted a rebellion against Richard's royal administrators whilst his brother was participating in the Third Crusade. Despite this, after Richard died in 1199, John was proclaimed King of England, and came to an agreement with Philip II of France to recognise John's possession of the continental Angevin lands at the peace treaty of Le Goulet in 1200.
The character of John's relationship with his second wife, Isabella of Angoulême, is unclear. John married Isabella whilst she was relatively young – her exact date of birth is uncertain, and estimates place her between at most 15 and more probably towards nine years old at the time of her marriage.[nb 15] Even by the standards of the time, Isabella was married whilst very young. John did not provide a great deal of money for his wife's household and did not pass on much of the revenue from her lands, to the extent that historian Nicholas Vincent has described him as being "downright mean" towards Isabella. Vincent concluded that the marriage was not a particularly "amicable" one. Other aspects of their marriage suggest a closer, more positive relationship. Chroniclers recorded that John had a "mad infatuation" with Isabella, and certainly John had conjugal relationships with Isabella between at least 1207 and 1215; they had five children. In contrast to Vincent, historian William Chester Jordan concludes that the pair were a "companionable couple" who had a successful marriage by the standards of the day.
When war with France broke out again in 1202, John achieved early victories, but shortages of military resources and his treatment of Norman, Breton and Anjou nobles resulted in the collapse of his empire in northern France in 1204. John spent much of the next decade attempting to regain these lands, raising huge revenues, reforming his armed forces and rebuilding continental alliances. John's judicial reforms had a lasting impact on the English common law system, as well as providing an additional source of revenue. An argument with Pope Innocent III led to John's excommunication in 1209, a dispute finally settled by the king in 1213. John's attempt to defeat Philip in 1214 failed due to the French victory over John's allies at the battle of Bouvines. When he returned to England, John faced a rebellion by many of his barons, who were unhappy with his fiscal policies and his treatment of many of England's most powerful nobles. Although both John and the barons agreed to the Magna Carta peace treaty in 1215, neither side complied with its conditions. Civil war broke out shortly afterwards, with the barons aided by Louis of France. It soon descended into a stalemate. John died of dysentery contracted whilst on campaign in eastern England during late 1216; supporters of his son Henry III went on to achieve victory over Louis and the rebel barons the following year.
Baronial unrest in England prevented the departure of the planned 1205 expedition, and only a smaller force under William Longespée deployed to Poitou. In 1206 John departed for Poitou himself, but was forced to divert south to counter a threat to Gascony from Alfonso VIII of Castile. After a successful campaign against Alfonso, John headed north again, taking the city of Angers. Philip moved south to meet John; the year's campaigning ended in stalemate and a two-year truce was made between the two rulers.
During John's early years, Henry attempted to resolve the question of his succession. Henry the Young King had been crowned King of England in 1170, but was not given any formal powers by his father; he was also promised Normandy and Anjou as part of his future inheritance. Richard was to be appointed the Count of Poitou with control of Aquitaine, whilst Geoffrey was to become the Duke of Brittany. At this time it seemed unlikely that John would ever inherit substantial lands, and he was jokingly nicknamed "Lackland" by his father.
John grew up to be around 5 ft 5 in (1.68 m) tall, relatively short, with a "powerful, barrel-chested body" and dark red hair; he looked to contemporaries like an inhabitant of Poitou. John enjoyed reading and, unusually for the period, built up a travelling library of books. He enjoyed gambling, in particular at backgammon, and was an enthusiastic hunter, even by medieval standards. He liked music, although not songs. John would become a "connoisseur of jewels", building up a large collection, and became famous for his opulent clothes and also, according to French chroniclers, for his fondness for bad wine. As John grew up, he became known for sometimes being "genial, witty, generous and hospitable"; at other moments, he could be jealous, over-sensitive and prone to fits of rage, "biting and gnawing his fingers" in anger.[nb 3]
When the Archbishop of Canterbury, Hubert Walter, died on 13 July 1205, John became involved in a dispute with Pope Innocent III that would lead to the king's excommunication. The Norman and Angevin kings had traditionally exercised a great deal of power over the church within their territories. From the 1040s onwards, however, successive popes had put forward a reforming message that emphasised the importance of the church being "governed more coherently and more hierarchically from the centre" and established "its own sphere of authority and jurisdiction, separate from and independent of that of the lay ruler", in the words of historian Richard Huscroft. After the 1140s, these principles had been largely accepted within the English church, albeit with an element of concern about centralising authority in Rome. These changes brought the customary rights of lay rulers such as John over ecclesiastical appointments into question. Pope Innocent was, according to historian Ralph Turner, an "ambitious and aggressive" religious leader, insistent on his rights and responsibilities within the church.
In 1185 John made his first visit to Ireland, accompanied by 300 knights and a team of administrators. Henry had tried to have John officially proclaimed King of Ireland, but Pope Lucius III would not agree. John's first period of rule in Ireland was not a success. Ireland had only recently been conquered by Anglo-Norman forces, and tensions were still rife between Henry II, the new settlers and the existing inhabitants. John infamously offended the local Irish rulers by making fun of their unfashionable long beards, failed to make allies amongst the Anglo-Norman settlers, began to lose ground militarily against the Irish and finally returned to England later in the year, blaming the viceroy, Hugh de Lacy, for the fiasco.
Henry the Young King fought a short war with his brother Richard in 1183 over the status of England, Normandy and Aquitaine. Henry II moved in support of Richard, and Henry the Young King died from dysentery at the end of the campaign. With his primary heir dead, Henry rearranged the plans for the succession: Richard was to be made King of England, albeit without any actual power until the death of his father; Geoffrey would retain Brittany; and John would now become the Duke of Aquitaine in place of Richard. Richard refused to give up Aquitaine; Henry II was furious and ordered John, with help from Geoffrey, to march south and retake the duchy by force. The two attacked the capital of Poitiers, and Richard responded by attacking Brittany. The war ended in stalemate and a tense family reconciliation in England at the end of 1184.
Under mounting political pressure, John finally negotiated terms for a reconciliation, and the papal terms for submission were accepted in the presence of the papal legate Pandulf Verraccio in May 1213 at the Templar Church at Dover. As part of the deal, John offered to surrender the Kingdom of England to the papacy for a feudal service of 1,000 marks (equivalent to £666 at the time) annually: 700 marks (£466) for England and 300 marks (£200) for Ireland, as well as recompensing the church for revenue lost during the crisis. The agreement was formalised in the Bulla Aurea, or Golden Bull. This resolution produced mixed responses. Although some chroniclers felt that John had been humiliated by the sequence of events, there was little public reaction. Innocent benefited from the resolution of his long-standing English problem, but John probably gained more, as Innocent became a firm supporter of John for the rest of his reign, backing him in both domestic and continental policy issues. Innocent immediately turned against Philip, calling upon him to reject plans to invade England and to sue for peace. John paid some of the compensation money he had promised the church, but he ceased making payments in late 1214, leaving two-thirds of the sum unpaid; Innocent appears to have conveniently forgotten this debt for the good of the wider relationship.
The political turmoil continued. John began to explore an alliance with the French king Philip II, freshly returned from the crusade. John hoped to acquire Normandy, Anjou and the other lands in France held by Richard in exchange for allying himself with Philip. John was persuaded not to pursue an alliance by his mother. Longchamp, who had left England after Walter's intervention, now returned, and argued that he had been wrongly removed as justiciar. John intervened, suppressing Longchamp's claims in return for promises of support from the royal administration, including a reaffirmation of his position as heir to the throne. When Richard still did not return from the crusade, John began to assert that his brother was dead or otherwise permanently lost. Richard had in fact been captured en route to England by the Duke of Austria and was handed over to Emperor Henry VI, who held him for ransom. John seized the opportunity and went to Paris, where he formed an alliance with Philip. He agreed to set aside his wife, Isabella of Gloucester, and marry Philip's sister, Alys, in exchange for Philip's support. Fighting broke out in England between forces loyal to Richard and those being gathered by John. John's military position was weak and he agreed to a truce; in early 1194 the king finally returned to England, and John's remaining forces surrendered. John retreated to Normandy, where Richard finally found him later that year. Richard declared that his younger brother – despite being 27 years old – was merely "a child who has had evil counsellors" and forgave him, but removed his lands with the exception of Ireland.
The political situation in England rapidly began to deteriorate. Longchamp refused to work with Puiset and became unpopular with the English nobility and clergy. John exploited this unpopularity to set himself up as an alternative ruler with his own royal court, complete with his own justiciar, chancellor and other royal posts, and was happy to be portrayed as an alternative regent, and possibly the next king. Armed conflict broke out between John and Longchamp, and by October 1191 Longchamp was isolated in the Tower of London with John in control of the city of London, thanks to promises John had made to the citizens in return for recognition as Richard's heir presumptive. At this point Walter of Coutances, the Archbishop of Rouen, returned to England, having been sent by Richard to restore order. John's position was undermined by Walter's relative popularity and by the news that Richard had married whilst in Cyprus, which presented the possibility that Richard would have legitimate children and heirs.
Letters of support from the pope arrived in April but by then the rebel barons had organised. They congregated at Northampton in May and renounced their feudal ties to John, appointing Robert fitz Walter as their military leader. This self-proclaimed "Army of God" marched on London, taking the capital as well as Lincoln and Exeter. John's efforts to appear moderate and conciliatory had been largely successful, but once the rebels held London they attracted a fresh wave of defectors from John's royalist faction. John instructed Langton to organise peace talks with the rebel barons.
The new peace would only last for two years; war recommenced in the aftermath of John's decision in August 1200 to marry Isabella of Angoulême. In order to remarry, John first needed to abandon Isabel, Countess of Gloucester, his first wife; John accomplished this by arguing that he had failed to get the necessary papal permission to marry Isabel in the first place – as a cousin, John could not have legally wed her without this. It remains unclear why John chose to marry Isabella of Angoulême. Contemporary chroniclers argued that John had fallen deeply in love with Isabella, and John may have been motivated by desire for an apparently beautiful, if rather young, girl. On the other hand, the Angoumois lands that came with Isabella were strategically vital to John: by marrying Isabella, John was acquiring a key land route between Poitou and Gascony, which significantly strengthened his grip on Aquitaine.[nb 5]
After his coronation, John moved south into France with military forces and adopted a defensive posture along the eastern and southern Normandy borders. Both sides paused for desultory negotiations before the war recommenced; John's position was now stronger, thanks to confirmation that the counts Baldwin IX of Flanders and Renaud of Boulogne had renewed the anti-French alliances they had previously agreed to with Richard. The powerful Anjou nobleman William des Roches was persuaded to switch sides from Arthur to John; suddenly the balance seemed to be tipping away from Philip and Arthur in favour of John. Neither side was keen to continue the conflict, and following a papal truce the two leaders met in January 1200 to negotiate possible terms for peace. From John's perspective, what then followed represented an opportunity to stabilise control over his continental possessions and produce a lasting peace with Philip in Paris. John and Philip negotiated the May 1200 Treaty of Le Goulet; by this treaty, Philip recognised John as the rightful heir to Richard in respect to his French possessions, temporarily abandoning the wider claims of his client, Arthur.[nb 4] John, in turn, abandoned Richard's former policy of containing Philip through alliances with Flanders and Boulogne, and accepted Philip's right as the legitimate feudal overlord of John's lands in France. John's policy earned him the disrespectful title of "John Softsword" from some English chroniclers, who contrasted his behaviour with his more aggressive brother, Richard.
The rebel barons responded by inviting the French prince Louis to lead them: Louis had a claim to the English throne by virtue of his marriage to Blanche of Castile, a granddaughter of Henry II. Philip may have provided him with private support but refused to openly support Louis, who was excommunicated by Innocent for taking part in the war against John. Louis' planned arrival in England presented a significant problem for John, as the prince would bring with him naval vessels and siege engines essential to the rebel cause. Once John contained Alexander in Scotland, he marched south to deal with the challenge of the coming invasion.
Further desertions of John's local allies at the beginning of 1203 steadily reduced John's freedom to manoeuvre in the region. He attempted to convince Pope Innocent III to intervene in the conflict, but Innocent's efforts were unsuccessful. As the situation became worse for John, he appears to have decided to have Arthur killed, with the aim of removing his potential rival and of undermining the rebel movement in Brittany. Arthur had initially been imprisoned at Falaise and was then moved to Rouen. After this, Arthur's fate remains uncertain, but modern historians believe he was murdered by John. The annals of Margam Abbey suggest that "John had captured Arthur and kept him alive in prison for some time in the castle of Rouen ... when John was drunk he slew Arthur with his own hand and tying a heavy stone to the body cast it into the Seine."[nb 7] Rumours of the manner of Arthur's death further reduced support for John across the region. Arthur's sister, Eleanor, who had also been captured at Mirebeau, was kept imprisoned by John for many years, albeit in relatively good conditions.
John's position in France was considerably strengthened by the victory at Mirebeau, but John's treatment of his new prisoners and of his ally, William de Roches, quickly undermined these gains. De Roches was a powerful Anjou noble, but John largely ignored him, causing considerable offence, whilst the king kept the rebel leaders in such bad conditions that twenty-two of them died. At this time most of the regional nobility were closely linked through kinship, and this behaviour towards their relatives was regarded as unacceptable. William de Roches and other of John's regional allies in Anjou and Brittany deserted him in favour of Philip, and Brittany rose in fresh revolt. John's financial situation was tenuous: once factors such as the comparative military costs of materiel and soldiers were taken into account, Philip enjoyed a considerable, although not overwhelming, advantage of resources over John.[nb 6]
In the aftermath of John's death William Marshal was declared the protector of the nine-year-old Henry III. The civil war continued until royalist victories at the battles of Lincoln and Dover in 1217. Louis gave up his claim to the English throne and signed the Treaty of Lambeth. The failed Magna Carta agreement was resuscitated by Marshal's administration and reissued in an edited form in 1217 as a basis for future government. Henry III continued his attempts to reclaim Normandy and Anjou until 1259, but John's continental losses and the consequent growth of Capetian power in the 13th century proved to mark a "turning point in European history".
One of John's principal challenges was acquiring the large sums of money needed for his proposed campaigns to reclaim Normandy. The Angevin kings had three main sources of income available to them, namely revenue from their personal lands, or demesne; money raised through their rights as a feudal lord; and revenue from taxation. Revenue from the royal demesne was inflexible and had been diminishing slowly since the Norman conquest. Matters were not helped by Richard's sale of many royal properties in 1189, and taxation played a much smaller role in royal income than in later centuries. English kings had widespread feudal rights which could be used to generate income, including the scutage system, in which feudal military service was avoided by a cash payment to the king. He derived income from fines, court fees and the sale of charters and other privileges. John intensified his efforts to maximise all possible sources of income, to the extent that he has been described as "avaricious, miserly, extortionate and moneyminded". John also used revenue generation as a way of exerting political control over the barons: debts owed to the crown by the king's favoured supporters might be forgiven; collection of those owed by enemies was more stringently enforced.
The administration of justice was of particular importance to John. Several new processes had been introduced to English law under Henry II, including novel disseisin and mort d'ancestor. These processes meant the royal courts had a more significant role in local law cases, which had previously been dealt with only by regional or local lords. John increased the professionalism of local sergeants and bailiffs, and extended the system of coroners first introduced by Hubert Walter in 1194, creating a new class of borough coroners. John worked extremely hard to ensure that this system operated well, through judges he had appointed, by fostering legal specialists and expertise, and by intervening in cases himself. John continued to try relatively minor cases, even during military crises. Viewed positively, Lewis Warren considers that John discharged "his royal duty of providing justice ... with a zeal and a tirelessness to which the English common law is greatly endebted". Seen more critically, John may have been motivated by the potential of the royal legal process to raise fees, rather than a desire to deliver simple justice; John's legal system also only applied to free men, rather than to all of the population. Nonetheless, these changes were popular with many free tenants, who acquired a more reliable legal system that could bypass the barons, against whom such cases were often brought. John's reforms were less popular with the barons themselves, especially as they remained subject to arbitrary and frequently vindictive royal justice.
In the 1940s, new interpretations of John's reign began to emerge, based on research into the record evidence of his reign, such as pipe rolls, charters, court documents and similar primary records. Notably, an essay by Vivian Galbraith in 1945 proposed a "new approach" to understanding the ruler. The use of recorded evidence was combined with an increased scepticism about two of the most colourful chroniclers of John's reign, Roger of Wendover and Matthew Paris. In many cases the detail provided by these chroniclers, both writing after John's death, was challenged by modern historians. Interpretations of Magna Carta and the role of the rebel barons in 1215 have been significantly revised: although the charter's symbolic, constitutional value for later generations is unquestionable, in the context of John's reign most historians now consider it a failed peace agreement between "partisan" factions. There has been increasing debate about the nature of John's Irish policies. Specialists in Irish medieval history, such as Sean Duffy, have challenged the conventional narrative established by Lewis Warren, suggesting that Ireland was less stable by 1216 than was previously supposed.
John was deeply suspicious of the barons, particularly those with sufficient power and wealth to potentially challenge the king. Numerous barons were subjected to John's malevolentia, even including William Marshal, a famous knight and baron normally held up as a model of utter loyalty. The most infamous case, which went beyond anything considered acceptable at the time, proved to be that of William de Braose, a powerful marcher lord with lands in Ireland. De Braose was subjected to punitive demands for money, and when he refused to pay a huge sum of 40,000 marks (equivalent to £26,666 at the time),[nb 13] his wife and one of his sons were imprisoned by John, which resulted in their deaths. De Braose died in exile in 1211, and his grandsons remained in prison until 1218. John's suspicions and jealousies meant that he rarely enjoyed good relationships with even the leading loyalist barons.
This trend for the king to rely on his own men at the expense of the barons was exacerbated by the tradition of Angevin royal ira et malevolentia – "anger and ill-will" – and John's own personality. From Henry II onwards, ira et malevolentia had come to describe the right of the king to express his anger and displeasure at particular barons or clergy, building on the Norman concept of malevoncia – royal ill-will. In the Norman period, suffering the king's ill-will meant difficulties in obtaining grants, honours or petitions; Henry II had infamously expressed his fury and ill-will towards Thomas Becket; this ultimately resulted in Becket's death. John now had the additional ability to "cripple his vassals" on a significant scale using his new economic and judicial measures, which made the threat of royal anger all the more serious.
John spent much of 1205 securing England against a potential French invasion. As an emergency measure, John recreated a version of Henry II's Assize of Arms of 1181, with each shire creating a structure to mobilise local levies. When the threat of invasion faded, John formed a large military force in England intended for Poitou, and a large fleet with soldiers under his own command intended for Normandy. To achieve this, John reformed the English feudal contribution to his campaigns, creating a more flexible system under which only one knight in ten would actually be mobilised, but would be financially supported by the other nine; knights would serve for an indefinite period. John built up a strong team of engineers for siege warfare and a substantial force of professional crossbowmen. The king was supported by a team of leading barons with military expertise, including William Longespée, William the Marshal, Roger de Lacy and, until he fell from favour, the marcher lord William de Braose.
During the remainder of his reign, John focused on trying to retake Normandy. The available evidence suggests that John did not regard the loss of the Duchy as a permanent shift in Capetian power. Strategically, John faced several challenges: England itself had to be secured against possible French invasion, the sea-routes to Bordeaux needed to be secured following the loss of the land route to Aquitaine, and his remaining possessions in Aquitaine needed to be secured following the death of his mother, Eleanor, in April 1204. John's preferred plan was to use Poitou as a base of operations, advance up the Loire valley to threaten Paris, pin down the French forces and break Philip's internal lines of communication before landing a maritime force in the Duchy itself. Ideally, this plan would benefit from the opening of a second front on Philip's eastern frontiers with Flanders and Boulogne – effectively a re-creation of Richard's old strategy of applying pressure from Germany. All of this would require a great deal of money and soldiers.
John remained Lord of Ireland throughout his reign. He drew on the country for resources to fight his war with Philip on the continent. Conflict continued in Ireland between the Anglo-Norman settlers and the indigenous Irish chieftains, with John manipulating both groups to expand his wealth and power in the country. During Richard's rule, John had successfully increased the size of his lands in Ireland, and he continued this policy as king. In 1210 the king crossed into Ireland with a large army to crush a rebellion by the Anglo-Norman lords; he reasserted his control of the country and used a new charter to order compliance with English laws and customs in Ireland. John stopped short of trying to actively enforce this charter on the native Irish kingdoms, but historian David Carpenter suspects that he might have done so, had the baronial conflict in England not intervened. Simmering tensions remained with the native Irish leaders even after John left for England.
In the late 12th and early 13th centuries the border and political relationship between England and Scotland was disputed, with the kings of Scotland claiming parts of what is now northern England. John's father, Henry II, had forced William the Lion to swear fealty to him at the Treaty of Falaise in 1174. This had been rescinded by Richard I in exchange for financial compensation in 1189, but the relationship remained uneasy. John began his reign by reasserting his sovereignty over the disputed northern counties. He refused William's request for the earldom of Northumbria, but did not intervene in Scotland itself and focused on his continental problems. The two kings maintained a friendly relationship, meeting in 1206 and 1207, until it was rumoured in 1209 that William was intending to ally himself with Philip II of France. John invaded Scotland and forced William to sign the Treaty of Norham, which gave John control of William's daughters and required a payment of £10,000. This effectively crippled William's power north of the border, and by 1212 John had to intervene militarily to support the Scottish king against his internal rivals.[nb 16] John made no efforts to reinvigorate the Treaty of Falaise, though, and both William and Alexander remained independent kings, supported by, but not owing fealty to, John.
John treated the interdict as "the equivalent of a papal declaration of war". He responded by attempting to punish Innocent personally and to drive a wedge between those English clergy that might support him and those allying themselves firmly with the authorities in Rome. John seized the lands of those clergy unwilling to conduct services, as well as those estates linked to Innocent himself; he arrested the illicit concubines that many clerics kept during the period, only releasing them after the payment of fines; he seized the lands of members of the church who had fled England, and he promised protection for those clergy willing to remain loyal to him. In many cases, individual institutions were able to negotiate terms for managing their own properties and keeping the produce of their estates. By 1209 the situation showed no signs of resolution, and Innocent threatened to excommunicate John if he did not acquiesce to Langton's appointment. When this threat failed, Innocent excommunicated the king in November 1209. Although theoretically a significant blow to John's legitimacy, this did not appear to greatly worry the king. Two of John's close allies, Emperor Otto IV and Count Raymond VI of Toulouse, had already suffered the same punishment themselves, and the significance of excommunication had been somewhat devalued. John simply tightened his existing measures and accrued significant sums from the income of vacant sees and abbeys: one 1213 estimate, for example, suggested the church had lost an estimated 100,000 marks (equivalent to £66,666 at the time) to John. Official figures suggest that around 14% of annual income from the English church was being appropriated by John each year.
John was incensed about what he perceived as an abrogation of his customary right as monarch to influence the election. He complained both about the choice of Langton as an individual, as John felt he was overly influenced by the Capetian court in Paris, and about the process as a whole. He barred Langton from entering England and seized the lands of the archbishopric and other papal possessions. Innocent set a commission in place to try to convince John to change his mind, but to no avail. Innocent then placed an interdict on England in March 1208, prohibiting clergy from conducting religious services, with the exception of baptisms for the young, and confessions and absolutions for the dying.
The first part of the campaign went well, with John outmanoeuvring the forces under the command of Prince Louis and retaking the county of Anjou by the end of June. John besieged the castle of Roche-au-Moine, a key stronghold, forcing Louis to give battle against John's larger army. The local Angevin nobles refused to advance with the king; left at something of a disadvantage, John retreated back to La Rochelle. Shortly afterwards, Philip won the hard-fought battle of Bouvines in the north against Otto and John's other allies, bringing an end to John's hopes of retaking Normandy. A peace agreement was signed in which John returned Anjou to Philip and paid the French king compensation; the truce was intended to last for six years. John arrived back in England in October.
In 1214 John began his final campaign to reclaim Normandy from Philip. John was optimistic, as he had successfully built up alliances with the Emperor Otto, Renaud of Boulogne and Count Ferdinand of Flanders; he was enjoying papal favour; and he had successfully built up substantial funds to pay for the deployment of his experienced army. Nonetheless, when John left for Poitou in February 1214, many barons refused to provide military service; mercenary knights had to fill the gaps. John's plan was to split Philip's forces by pushing north-east from Poitou towards Paris, whilst Otto, Renaud and Ferdinand, supported by William Longespée, marched south-west from Flanders.
The rebels made the first move in the war, seizing the strategic Rochester Castle, owned by Langton but left almost unguarded by the archbishop. John was well prepared for a conflict. He had stockpiled money to pay for mercenaries and ensured the support of the powerful marcher lords with their own feudal forces, such as William Marshal and Ranulf of Chester. The rebels lacked the engineering expertise or heavy equipment necessary to assault the network of royal castles that cut off the northern rebel barons from those in the south. John's strategy was to isolate the rebel barons in London, protect his own supply lines to his key source of mercenaries in Flanders, prevent the French from landing in the south-east, and then win the war through slow attrition. John put off dealing with the badly deteriorating situation in North Wales, where Llywelyn the Great was leading a rebellion against the 1211 settlement.
Neither John nor the rebel barons seriously attempted to implement the peace accord. The rebel barons suspected that the proposed baronial council would be unacceptable to John and that he would challenge the legality of the charter; they packed the baronial council with their own hardliners and refused to demobilise their forces or surrender London as agreed. Despite his promises to the contrary, John appealed to Innocent for help, observing that the charter compromised the pope's rights under the 1213 agreement that had appointed him John's feudal lord. Innocent obliged; he declared the charter "not only shameful and demeaning, but illegal and unjust" and excommunicated the rebel barons. The failure of the agreement led rapidly to the First Barons' War.
The king returned west but is said to have lost a significant part of his baggage train along the way. Roger of Wendover provides the most graphic account of this, suggesting that the king's belongings, including the Crown Jewels, were lost as he crossed one of the tidal estuaries which empties into the Wash, being sucked in by quicksand and whirlpools. Accounts of the incident vary considerably between the various chroniclers and the exact location of the incident has never been confirmed; the losses may have involved only a few of his pack-horses. Modern historians assert that by October 1216 John faced a "stalemate", "a military situation uncompromised by defeat".
In September 1216 John began a fresh, vigorous attack. He marched from the Cotswolds, feigned an offensive to relieve the besieged Windsor Castle, and attacked eastwards around London to Cambridge to separate the rebel-held areas of Lincolnshire and East Anglia. From there he travelled north to relieve the rebel siege at Lincoln and back east to King's Lynn, probably to order further supplies from the continent.[nb 17] In King's Lynn, John contracted dysentery, which would ultimately prove fatal. Meanwhile, Alexander II invaded northern England again, taking Carlisle in August and then marching south to give homage to Prince Louis for his English possessions; John narrowly missed intercepting Alexander along the way. Tensions between Louis and the English barons began to increase, prompting a wave of desertions, including William Marshal's son William and William Longespée, who both returned to John's faction.
In the 16th century political and religious changes altered the attitude of historians towards John. Tudor historians were generally favourably inclined towards the king, focusing on John's opposition to the Papacy and his promotion of the special rights and prerogatives of a king. Revisionist histories written by John Foxe, William Tyndale and Robert Barnes portrayed John as an early Protestant hero, and John Foxe included the king in his Book of Martyrs. John Speed's Historie of Great Britaine in 1632 praised John's "great renown" as a king; he blamed the bias of medieval chroniclers for the king's poor reputation.
Nineteenth-century fictional depictions of John were heavily influenced by Sir Walter Scott's historical romance, Ivanhoe, which presented "an almost totally unfavourable picture" of the king; the work drew on Victorian histories of the period and on Shakespeare's play. Scott's work influenced the late 19th-century children's writer Howard Pyle's book The Merry Adventures of Robin Hood, which in turn established John as the principal villain within the traditional Robin Hood narrative. During the 20th century, John was normally depicted in fictional books and films alongside Robin Hood. Sam De Grasse's role as John in the black-and-white 1922 film version shows John committing numerous atrocities and acts of torture. Claude Rains played John in the 1938 colour version alongside Errol Flynn, starting a trend for films to depict John as an "effeminate ... arrogant and cowardly stay-at-home". The character of John acts either to highlight the virtues of King Richard, or contrasts with the Sheriff of Nottingham, who is usually the "swashbuckling villain" opposing Robin. An extreme version of this trend can be seen in the Disney cartoon version, for example, which depicts John, voiced by Peter Ustinov, as a "cowardly, thumbsucking lion". Popular works that depict John beyond the Robin Hood legends, such as James Goldman's play and later film, The Lion in Winter, set in 1183, commonly present him as an "effete weakling", in this instance contrasted with the more masculine Henry II, or as a tyrant, as in A. A. Milne's poem for children, "King John's Christmas".
Historical interpretations of John have been subject to considerable change over the years. Medieval chroniclers provided the first contemporary, or near contemporary, histories of John's reign. One group of chroniclers wrote early in John's life, or around the time of his accession, including Richard of Devizes, William of Newburgh, Roger of Hoveden and Ralph de Diceto. These historians were generally unsympathetic to John's behaviour under Richard's rule, but slightly more positive towards the very earliest years of John's reign. Reliable accounts of the middle and later parts of John's reign are more limited, with Gervase of Canterbury and Ralph of Coggeshall writing the main accounts; neither of them were positive about John's performance as king. Much of John's later, negative reputation was established by two chroniclers writing after the king's death, Roger of Wendover and Matthew Paris, the latter claiming that John attempted conversion to Islam in exchange for military aid from the Almohad ruler Muhammad al-Nasir - a story which is considered to be untrue by modern historians.
Popular representations of John first began to emerge during the Tudor period, mirroring the revisionist histories of the time. The anonymous play The Troublesome Reign of King John portrayed the king as a "proto-Protestant martyr", similar to that shown in John Bale's morality play Kynge Johan, in which John attempts to save England from the "evil agents of the Roman Church". By contrast, Shakespeare's King John, a relatively anti-Catholic play that draws on The Troublesome Reign for its source material, offers a more "balanced, dual view of a complex monarch as both a proto-Protestant victim of Rome's machinations and as a weak, selfishly motivated ruler". Anthony Munday's play The Downfall and The Death of Robert Earl of Huntington portrays many of John's negative traits, but adopts a positive interpretation of the king's stand against the Roman Catholic Church, in line with the contemporary views of the Tudor monarchs. By the middle of the 17th century, plays such as Robert Davenport's King John and Matilda, although based largely on the earlier Elizabethan works, were transferring the role of Protestant champion to the barons and focusing more on the tyrannical aspects of John's behaviour.
Contemporary chroniclers were mostly critical of John's performance as king, and his reign has since been the subject of significant debate and periodic revision by historians from the 16th century onwards. Historian Jim Bradbury has summarised the contemporary historical opinion of John's positive qualities, observing that John is today usually considered a "hard-working administrator, an able man, an able general". Nonetheless, modern historians agree that he also had many faults as king, including what historian Ralph Turner describes as "distasteful, even dangerous personality traits", such as pettiness, spitefulness and cruelty. These negative qualities provided extensive material for fiction writers in the Victorian era, and John remains a recurring character within Western popular culture, primarily as a villain in films and stories depicting the Robin Hood legends.
Henry II wanted to secure the southern borders of Aquitaine and decided to betroth his youngest son to Alais, the daughter and heiress of Humbert III of Savoy. As part of this agreement John was promised the future inheritance of Savoy, Piedmont, Maurienne, and the other possessions of Count Humbert. For his part in the potential marriage alliance, Henry II transferred the castles of Chinon, Loudun and Mirebeau into John's name; as John was only five years old his father would continue to control them for practical purposes. Henry the Young King was unimpressed by this; although he had yet to be granted control of any castles in his new kingdom, these were effectively his future property and had been given away without consultation. Alais made the trip over the Alps and joined Henry II's court, but she died before marrying John, which left the prince once again without an inheritance.
For the remaining years of Richard's reign, John supported his brother on the continent, apparently loyally. Richard's policy on the continent was to attempt to regain through steady, limited campaigns the castles he had lost to Philip II whilst on crusade. He allied himself with the leaders of Flanders, Boulogne and the Holy Roman Empire to apply pressure on Philip from Germany. In 1195 John successfully conducted a sudden attack and siege of Évreux castle, and subsequently managed the defences of Normandy against Philip. The following year, John seized the town of Gamaches and led a raiding party within 50 miles (80 km) of Paris, capturing the Bishop of Beauvais. In return for this service, Richard withdrew his malevolentia (ill-will) towards John, restored him to the county of Gloucestershire and made him again the Count of Mortain.
Unfortunately, Isabella was already engaged to Hugh of Lusignan, an important member of a key Poitou noble family and brother of Count Raoul of Eu, who possessed lands along the sensitive eastern Normandy border. Just as John stood to benefit strategically from marrying Isabella, so the marriage threatened the interests of the Lusignans, whose own lands currently provided the key route for royal goods and troops across Aquitaine. Rather than negotiating some form of compensation, John treated Hugh "with contempt"; this resulted in a Lusignan uprising that was promptly crushed by John, who also intervened to suppress Raoul in Normandy.
In late 1203, John attempted to relieve Château Gaillard, which although besieged by Philip was guarding the eastern flank of Normandy. John attempted a synchronised operation involving land-based and water-borne forces, considered by most historians today to have been imaginative in conception, but overly complex for forces of the period to have carried out successfully. John's relief operation was blocked by Philip's forces, and John turned back to Brittany in an attempt to draw Philip away from eastern Normandy. John successfully devastated much of Brittany, but did not deflect Philip's main thrust into the east of Normandy. Opinions vary amongst historians as to the military skill shown by John during this campaign, with most recent historians arguing that his performance was passable, although not impressive.[nb 8] John's situation began to deteriorate rapidly. The eastern border region of Normandy had been extensively cultivated by Philip and his predecessors for several years, whilst Angevin authority in the south had been undermined by Richard's giving away of various key castles some years before. His use of routier mercenaries in the central regions had rapidly eaten away his remaining support in this area too, which set the stage for a sudden collapse of Angevin power.[nb 9] John retreated back across the Channel in December, sending orders for the establishment of a fresh defensive line to the west of Chateau Gaillard. In March 1204, Gaillard fell. John's mother Eleanor died the following month. This was not just a personal blow for John, but threatened to unravel the widespread Angevin alliances across the far south of France. Philip moved south around the new defensive line and struck upwards at the heart of the Duchy, now facing little resistance. By August, Philip had taken Normandy and advanced south to occupy Anjou and Poitou as well. John's only remaining possession on the Continent was now the Duchy of Aquitaine.
The result was a sequence of innovative but unpopular financial measures.[nb 10] John levied scutage payments eleven times in his seventeen years as king, as compared to eleven times in total during the reign of the preceding three monarchs. In many cases these were levied in the absence of any actual military campaign, which ran counter to the original idea that scutage was an alternative to actual military service. John maximised his right to demand relief payments when estates and castles were inherited, sometimes charging enormous sums, beyond barons' abilities to pay. Building on the successful sale of sheriff appointments in 1194, John initiated a new round of appointments, with the new incumbents making back their investment through increased fines and penalties, particularly in the forests. Another innovation of Richard's, increased charges levied on widows who wished to remain single, was expanded under John. John continued to sell charters for new towns, including the planned town of Liverpool, and charters were sold for markets across the kingdom and in Gascony.[nb 11] The king introduced new taxes and extended existing ones. The Jews, who held a vulnerable position in medieval England, protected only by the king, were subject to huge taxes; £44,000 was extracted from the community by the tallage of 1210; much of it was passed on to the Christian debtors of Jewish moneylenders.[nb 12] John created a new tax on income and movable goods in 1207 – effectively a version of a modern income tax – that produced £60,000; he created a new set of import and export duties payable directly to the crown. John found that these measures enabled him to raise further resources through the confiscation of the lands of barons who could not pay or refused to pay.
John's personal life greatly affected his reign. Contemporary chroniclers state that John was sinfully lustful and lacking in piety. It was common for kings and nobles of the period to keep mistresses, but chroniclers complained that John's mistresses were married noblewomen, which was considered unacceptable. John had at least five children with mistresses during his first marriage to Isabelle of Gloucester, and two of those mistresses are known to have been noblewomen. John's behaviour after his second marriage to Isabella of Angoulême is less clear, however. None of John's known illegitimate children were born after he remarried, and there is no actual documentary proof of adultery after that point, although John certainly had female friends amongst the court throughout the period. The specific accusations made against John during the baronial revolts are now generally considered to have been invented for the purposes of justifying the revolt; nonetheless, most of John's contemporaries seem to have held a poor opinion of his sexual behaviour.[nb 14]
John had already begun to improve his Channel forces before the loss of Normandy and he rapidly built up further maritime capabilities after its collapse. Most of these ships were placed along the Cinque Ports, but Portsmouth was also enlarged. By the end of 1204 he had around 50 large galleys available; another 54 vessels were built between 1209 and 1212. William of Wrotham was appointed "keeper of the galleys", effectively John's chief admiral. Wrotham was responsible for fusing John's galleys, the ships of the Cinque Ports and pressed merchant vessels into a single operational fleet. John adopted recent improvements in ship design, including new large transport ships called buisses and removable forecastles for use in combat.
Royal power in Wales was unevenly applied, with the country divided between the marcher lords along the borders, royal territories in Pembrokeshire and the more independent native Welsh lords of North Wales. John took a close interest in Wales and knew the country well, visiting every year between 1204 and 1211 and marrying his illegitimate daughter, Joan, to the Welsh prince Llywelyn the Great. The king used the marcher lords and the native Welsh to increase his own territory and power, striking a sequence of increasingly precise deals backed by royal military power with the Welsh rulers. A major royal expedition to enforce these agreements occurred in 1211, after Llywelyn attempted to exploit the instability caused by the removal of William de Braose, through the Welsh uprising of 1211. John's invasion, striking into the Welsh heartlands, was a military success. Llywelyn came to terms that included an expansion of John's power across much of Wales, albeit only temporarily.
Innocent gave some dispensations as the crisis progressed. Monastic communities were allowed to celebrate Mass in private from 1209 onwards, and late in 1212 the Holy Viaticum for the dying was authorised. The rules on burials and lay access to churches appear to have been steadily circumvented, at least unofficially. Although the interdict was a burden to much of the population, it did not result in rebellion against John. By 1213, though, John was increasingly worried about the threat of French invasion. Some contemporary chroniclers suggested that in January Philip II of France had been charged with deposing John on behalf of the papacy, although it appears that Innocent merely prepared secret letters in case Innocent needed to claim the credit if Philip did successfully invade England.
Within a few months of John's return, rebel barons in the north and east of England were organising resistance to his rule. John held a council in London in January 1215 to discuss potential reforms and sponsored discussions in Oxford between his agents and the rebels during the spring. John appears to have been playing for time until Pope Innocent III could send letters giving him explicit papal support. This was particularly important for John, as a way of pressuring the barons but also as a way of controlling Stephen Langton, the Archbishop of Canterbury. In the meantime, John began to recruit fresh mercenary forces from Poitou, although some were later sent back to avoid giving the impression that the king was escalating the conflict. John announced his intent to become a crusader, a move which gave him additional political protection under church law.
John's campaign started well. In November John retook Rochester Castle from rebel baron William d'Aubigny in a sophisticated assault. One chronicler had not seen "a siege so hard pressed or so strongly resisted", whilst historian Reginald Brown describes it as "one of the greatest [siege] operations in England up to that time". Having regained the south-east John split his forces, sending William Longespée to retake the north side of London and East Anglia, whilst John himself headed north via Nottingham to attack the estates of the northern barons. Both operations were successful and the majority of the remaining rebels were pinned down in London. In January 1216 John marched against Alexander II of Scotland, who had allied himself with the rebel cause. John took back Alexander's possessions in northern England in a rapid campaign and pushed up towards Edinburgh over a ten-day period.
John's illness grew worse and by the time he reached Newark Castle he was unable to travel any farther; John died on the night of 18 October. Numerous – probably fictitious – accounts circulated soon after his death that he had been killed by poisoned ale, poisoned plums or a "surfeit of peaches". His body was escorted south by a company of mercenaries and he was buried in Worcester Cathedral in front of the altar of St Wulfstan. A new sarcophagus with an effigy was made for him in 1232, in which his remains now rest.
By the Victorian period in the 19th century historians were more inclined to draw on the judgements of the chroniclers and to focus on John's moral personality. Kate Norgate, for example, argued that John's downfall had been due not to his failure in war or strategy, but due to his "almost superhuman wickedness", whilst James Ramsay blamed John's family background and his cruel personality for his downfall. Historians in the "Whiggish" tradition, focusing on documents such as the Domesday Book and Magna Carta, trace a progressive and universalist course of political and economic development in England over the medieval period. These historians were often inclined to see John's reign, and his signing of Magna Carta in particular, as a positive step in the constitutional development of England, despite the flaws of the king himself. Winston Churchill, for example, argued that "[w]hen the long tally is added, it will be seen that the British nation and the English-speaking world owe far more to the vices of John than to the labours of virtuous sovereigns".
John (24 December 1166 – 19 October 1216), also known as John Lackland (Norman French: Johan sanz Terre), was King of England from 6 April 1199 until his death in 1216. John lost the duchy of Normandy to King Philip II of France, which resulted in the collapse of most of the Angevin Empire and contributed to the subsequent growth in power of the Capetian dynasty during the 13th century. The baronial revolt at the end of John's reign led to the sealing of the Magna Carta, a document sometimes considered to be an early step in the evolution of the constitution of the United Kingdom.
Shortly after his birth, John was passed from Eleanor into the care of a wet nurse, a traditional practice for medieval noble families. Eleanor then left for Poitiers, the capital of Aquitaine, and sent John and his sister Joan north to Fontevrault Abbey. This may have been done with the aim of steering her youngest son, with no obvious inheritance, towards a future ecclesiastical career. Eleanor spent the next few years conspiring against her husband Henry and neither parent played a part in John's very early life. John was probably, like his brothers, assigned a magister whilst he was at Fontevrault, a teacher charged with his early education and with managing the servants of his immediate household; John was later taught by Ranulph Glanville, a leading English administrator. John spent some time as a member of the household of his eldest living brother Henry the Young King, where he probably received instruction in hunting and military skills.
John had spent the conflict travelling alongside his father, and was given widespread possessions across the Angevin empire as part of the Montlouis settlement; from then onwards, most observers regarded John as Henry II's favourite child, although he was the furthest removed in terms of the royal succession. Henry II began to find more lands for John, mostly at various nobles' expense. In 1175 he appropriated the estates of the late Earl of Cornwall and gave them to John. The following year, Henry disinherited the sisters of Isabelle of Gloucester, contrary to legal custom, and betrothed John to the now extremely wealthy Isabelle. In 1177, at the Council of Oxford, Henry dismissed William FitzAldelm as the Lord of Ireland and replaced him with the ten-year-old John.
When John's elder brother Richard became king in September 1189, he had already declared his intention of joining the Third Crusade. Richard set about raising the huge sums of money required for this expedition through the sale of lands, titles and appointments, and attempted to ensure that he would not face a revolt while away from his empire. John was made Count of Mortain, was married to the wealthy Isabel of Gloucester, and was given valuable lands in Lancaster and the counties of Cornwall, Derby, Devon, Dorset, Nottingham and Somerset, all with the aim of buying his loyalty to Richard whilst the king was on crusade. Richard retained royal control of key castles in these counties, thereby preventing John from accumulating too much military and political power, and, for the time being, the king named the four-year-old Arthur of Brittany as the heir to the throne. In return, John promised not to visit England for the next three years, thereby in theory giving Richard adequate time to conduct a successful crusade and return from the Levant without fear of John seizing power. Richard left political authority in England – the post of justiciar – jointly in the hands of Bishop Hugh de Puiset and William Mandeville, and made William Longchamp, the Bishop of Ely, his chancellor. Mandeville immediately died, and Longchamp took over as joint justiciar with Puiset, which would prove to be a less than satisfactory partnership. Eleanor, the queen mother, convinced Richard to allow John into England in his absence.
Warfare in Normandy at the time was shaped by the defensive potential of castles and the increasing costs of conducting campaigns. The Norman frontiers had limited natural defences but were heavily reinforced with castles, such as Château Gaillard, at strategic points, built and maintained at considerable expense. It was difficult for a commander to advance far into fresh territory without having secured his lines of communication by capturing these fortifications, which slowed the progress of any attack. Armies of the period could be formed from either feudal or mercenary forces. Feudal levies could only be raised for a fixed length of time before they returned home, forcing an end to a campaign; mercenary forces, often called Brabançons after the Duchy of Brabant but actually recruited from across northern Europe, could operate all year long and provide a commander with more strategic options to pursue a campaign, but cost much more than equivalent feudal forces. As a result, commanders of the period were increasingly drawing on larger numbers of mercenaries.
John initially adopted a defensive posture similar to that of 1199: avoiding open battle and carefully defending his key castles. John's operations became more chaotic as the campaign progressed, and Philip began to make steady progress in the east. John became aware in July that Arthur's forces were threatening his mother, Eleanor, at Mirebeau Castle. Accompanied by William de Roches, his seneschal in Anjou, he swung his mercenary army rapidly south to protect her. His forces caught Arthur by surprise and captured the entire rebel leadership at the battle of Mirebeau. With his southern flank weakening, Philip was forced to withdraw in the east and turn south himself to contain John's army.
John inherited a sophisticated system of administration in England, with a range of royal agents answering to the Royal Household: the Chancery kept written records and communications; the Treasury and the Exchequer dealt with income and expenditure respectively; and various judges were deployed to deliver justice around the kingdom. Thanks to the efforts of men like Hubert Walter, this trend towards improved record keeping continued into his reign. Like previous kings, John managed a peripatetic court that travelled around the kingdom, dealing with both local and national matters as he went. John was very active in the administration of England and was involved in every aspect of government. In part he was following in the tradition of Henry I and Henry II, but by the 13th century the volume of administrative work had greatly increased, which put much more pressure on a king who wished to rule in this style. John was in England for much longer periods than his predecessors, which made his rule more personal than that of previous kings, particularly in previously ignored areas such as the north.
John's royal household was based around several groups of followers. One group was the familiares regis, John's immediate friends and knights who travelled around the country with him. They also played an important role in organising and leading military campaigns. Another section of royal followers were the curia regis; these curiales were the senior officials and agents of the king and were essential to his day-to-day rule. Being a member of these inner circles brought huge advantages, as it was easier to gain favours from the king, file lawsuits, marry a wealthy heiress or have one's debts remitted. By the time of Henry II, these posts were increasingly being filled by "new men" from outside the normal ranks of the barons. This intensified under John's rule, with many lesser nobles arriving from the continent to take up positions at court; many were mercenary leaders from Poitou. These men included soldiers who would become infamous in England for their uncivilised behaviour, including Falkes de Breauté, Geard d'Athies, Engelard de Cigongé and Philip Marc. Many barons perceived the king's household as what Ralph Turner has characterised as a "narrow clique enjoying royal favour at barons' expense" staffed by men of lesser status.
John's lack of religious conviction has been noted by contemporary chroniclers and later historians, with some suspecting that John was at best impious, or even atheistic, a very serious issue at the time. Contemporary chroniclers catalogued his various anti-religious habits at length, including his failure to take communion, his blasphemous remarks, and his witty but scandalous jokes about church doctrine, including jokes about the implausibility of the Resurrection. They commented on the paucity of John's charitable donations to the church. Historian Frank McLynn argues that John's early years at Fontevrault, combined with his relatively advanced education, may have turned him against the church. Other historians have been more cautious in interpreting this material, noting that chroniclers also reported John's personal interest in the life of St Wulfstan of Worcester and his friendships with several senior clerics, most especially with Hugh of Lincoln, who was later declared a saint. Financial records show a normal royal household engaged in the usual feasts and pious observances – albeit with many records showing John's offerings to the poor to atone for routinely breaking church rules and guidance. The historian Lewis Warren has argued that the chronicler accounts were subject to considerable bias and the King was "at least conventionally devout," citing his pilgrimages and interest in religious scripture and commentaries.
During the truce of 1206–1208, John focused on building up his financial and military resources in preparation for another attempt to recapture Normandy. John used some of this money to pay for new alliances on Philip's eastern frontiers, where the growth in Capetian power was beginning to concern France's neighbours. By 1212 John had successfully concluded alliances with his nephew Otto IV, a contender for the crown of Holy Roman Emperor in Germany, as well as with the counts Renaud of Boulogne and Ferdinand of Flanders. The invasion plans for 1212 were postponed because of fresh English baronial unrest about service in Poitou. Philip seized the initiative in 1213, sending his elder son, Louis, to invade Flanders with the intention of next launching an invasion of England. John was forced to postpone his own invasion plans to counter this threat. He launched his new fleet to attack the French at the harbour of Damme. The attack was a success, destroying Philip's vessels and any chances of an invasion of England that year. John hoped to exploit this advantage by invading himself late in 1213, but baronial discontent again delayed his invasion plans until early 1214, in what would prove to be his final Continental campaign.
John wanted John de Gray, the Bishop of Norwich and one of his own supporters, to be appointed Archbishop of Canterbury after the death of Walter, but the cathedral chapter for Canterbury Cathedral claimed the exclusive right to elect Walter's successor. They favoured Reginald, the chapter's sub-prior. To complicate matters, the bishops of the province of Canterbury also claimed the right to appoint the next archbishop. The chapter secretly elected Reginald and he travelled to Rome to be confirmed; the bishops challenged the appointment and the matter was taken before Innocent. John forced the Canterbury chapter to change their support to John de Gray, and a messenger was sent to Rome to inform the papacy of the new decision. Innocent disavowed both Reginald and John de Gray, and instead appointed his own candidate, Stephen Langton. John refused Innocent's request that he consent to Langton's appointment, but the pope consecrated Langton anyway in June 1207.
Tensions between John and the barons had been growing for several years, as demonstrated by the 1212 plot against the king. Many of the disaffected barons came from the north of England; that faction was often labelled by contemporaries and historians as "the Northerners". The northern barons rarely had any personal stake in the conflict in France, and many of them owed large sums of money to John; the revolt has been characterised as "a rebellion of the king's debtors". Many of John's military household joined the rebels, particularly amongst those that John had appointed to administrative roles across England; their local links and loyalties outweighed their personal loyalty to John. Tension also grew across North Wales, where opposition to the 1211 treaty between John and Llywelyn was turning into open conflict. For some the appointment of Peter des Roches as justiciar was an important factor, as he was considered an "abrasive foreigner" by many of the barons. The failure of John's French military campaign in 1214 was probably the final straw that precipitated the baronial uprising during John's final years as king; James Holt describes the path to civil war as "direct, short and unavoidable" following the defeat at Bouvines.
John met the rebel leaders at Runnymede, near Windsor Castle, on 15 June 1215. Langton's efforts at mediation created a charter capturing the proposed peace agreement; it was later renamed Magna Carta, or "Great Charter". The charter went beyond simply addressing specific baronial complaints, and formed a wider proposal for political reform, albeit one focusing on the rights of free men, not serfs and unfree labour. It promised the protection of church rights, protection from illegal imprisonment, access to swift justice, new taxation only with baronial consent and limitations on scutage and other feudal payments. A council of twenty-five barons would be created to monitor and ensure John's future adherence to the charter, whilst the rebel army would stand down and London would be surrendered to the king.
Prince Louis intended to land in the south of England in May 1216, and John assembled a naval force to intercept him. Unfortunately for John, his fleet was dispersed by bad storms and Louis landed unopposed in Kent. John hesitated and decided not to attack Louis immediately, either due to the risks of open battle or over concerns about the loyalty of his own men. Louis and the rebel barons advanced west and John retreated, spending the summer reorganising his defences across the rest of the kingdom. John saw several of his military household desert to the rebels, including his half-brother, William Longespée. By the end of the summer the rebels had regained the south-east of England and parts of the north.
John's first wife, Isabel, Countess of Gloucester, was released from imprisonment in 1214; she remarried twice, and died in 1217. John's second wife, Isabella of Angoulême, left England for Angoulême soon after the king's death; she became a powerful regional leader, but largely abandoned the children she had had by John. John had five legitimate children, all by Isabella. His eldest son, Henry III, ruled as king for the majority of the 13th century. Richard became a noted European leader and ultimately the King of the Romans in the Holy Roman Empire. Joan married Alexander II of Scotland to become his queen consort. Isabella married the Holy Roman Emperor Frederick II. His youngest daughter, Eleanor, married William Marshal's son, also called William, and later the famous English rebel Simon de Montfort. John had a number of illegitimate children by various mistresses, including nine sons – Richard, Oliver, John, Geoffrey, Henry, Osbert Gifford, Eudes, Bartholomew and probably Philip – and three daughters – Joan, Maud and probably Isabel. Of these, Joan became the most famous, marrying Prince Llywelyn the Great of Wales.
Most historians today, including John's recent biographers Ralph Turner and Lewis Warren, argue that John was an unsuccessful monarch, but note that his failings were exaggerated by 12th- and 13th-century chroniclers. Jim Bradbury notes the current consensus that John was a "hard-working administrator, an able man, an able general", albeit, as Turner suggests, with "distasteful, even dangerous personality traits", including pettiness, spitefulness and cruelty. John Gillingham, author of a major biography of Richard I, follows this line too, although he considers John a less effective general than do Turner or Warren, and describes him "one of the worst kings ever to rule England". Bradbury takes a moderate line, but suggests that in recent years modern historians have been overly lenient towards John's numerous faults. Popular historian Frank McLynn maintains a counter-revisionist perspective on John, arguing that the king's modern reputation amongst historians is "bizarre", and that as a monarch John "fails almost all those [tests] that can be legitimately set".
In the human digestive system, food enters the mouth and mechanical digestion of the food starts by the action of mastication (chewing), a form of mechanical digestion, and the wetting contact of saliva. Saliva, a liquid secreted by the salivary glands, contains salivary amylase, an enzyme which starts the digestion of starch in the food; the saliva also contains mucus, which lubricates the food, and hydrogen carbonate, which provides the ideal conditions of pH (alkaline) for amylase to work. After undergoing mastication and starch digestion, the food will be in the form of a small, round slurry mass called a bolus. It will then travel down the esophagus and into the stomach by the action of peristalsis. Gastric juice in the stomach starts protein digestion. Gastric juice mainly contains hydrochloric acid and pepsin. As these two chemicals may damage the stomach wall, mucus is secreted by the stomach, providing a slimy layer that acts as a shield against the damaging effects of the chemicals. At the same time protein digestion is occurring, mechanical mixing occurs by peristalsis, which is waves of muscular contractions that move along the stomach wall. This allows the mass of food to further mix with the digestive enzymes.
Other animals, such as rabbits and rodents, practise coprophagia behaviours - eating specialised faeces in order to re-digest food, especially in the case of roughage. Capybara, rabbits, hamsters and other related species do not have a complex digestive system as do, for example, ruminants. Instead they extract more nutrition from grass by giving their food a second pass through the gut. Soft faecal pellets of partially digested food are excreted and generally consumed immediately. They also produce normal droppings, which are not eaten.
Digestive systems take many forms. There is a fundamental distinction between internal and external digestion. External digestion developed earlier in evolutionary history, and most fungi still rely on it. In this process, enzymes are secreted into the environment surrounding the organism, where they break down an organic material, and some of the products diffuse back to the organism. Animals have a tube (gastrointestinal tract) in which internal digestion occurs, which is more efficient because more of the broken down products can be captured, and the internal chemical environment can be more efficiently controlled.
The nitrogen fixing Rhizobia are an interesting case, wherein conjugative elements naturally engage in inter-kingdom conjugation. Such elements as the Agrobacterium Ti or Ri plasmids contain elements that can transfer to plant cells. Transferred genes enter the plant cell nucleus and effectively transform the plant cells into factories for the production of opines, which the bacteria use as carbon and energy sources. Infected plant cells form crown gall or root tumors. The Ti and Ri plasmids are thus endosymbionts of the bacteria, which are in turn endosymbionts (or parasites) of the infected plant.
Teeth (singular tooth) are small whitish structures found in the jaws (or mouths) of many vertebrates that are used to tear, scrape, milk and chew food. Teeth are not made of bone, but rather of tissues of varying density and hardness, such as enamel, dentine and cementum. Human teeth have a blood and nerve supply which enables proprioception. This is the ability of sensation when chewing, for example if we were to bite into something too hard for our teeth, such as a chipped plate mixed in food, our teeth send a message to our brain and we realise that it cannot be chewed, so we stop trying.
The abomasum is the fourth and final stomach compartment in ruminants. It is a close equivalent of a monogastric stomach (e.g., those in humans or pigs), and digesta is processed here in much the same way. It serves primarily as a site for acid hydrolysis of microbial and dietary protein, preparing these protein sources for further digestion and absorption in the small intestine. Digesta is finally moved into the small intestine, where the digestion and absorption of nutrients occurs. Microbes produced in the reticulo-rumen are also digested in the small intestine.
An earthworm's digestive system consists of a mouth, pharynx, esophagus, crop, gizzard, and intestine. The mouth is surrounded by strong lips, which act like a hand to grab pieces of dead grass, leaves, and weeds, with bits of soil to help chew. The lips break the food down into smaller pieces. In the pharynx, the food is lubricated by mucus secretions for easier passage. The esophagus adds calcium carbonate to neutralize the acids formed by food matter decay. Temporary storage occurs in the crop where food and calcium carbonate are mixed. The powerful muscles of the gizzard churn and mix the mass of food and dirt. When the churning is complete, the glands in the walls of the gizzard add enzymes to the thick paste, which helps chemically breakdown the organic matter. By peristalsis, the mixture is sent to the intestine where friendly bacteria continue chemical breakdown. This releases carbohydrates, protein, fat, and various vitamins and minerals for absorption into the body.
Digestion of some fats can begin in the mouth where lingual lipase breaks down some short chain lipids into diglycerides. However fats are mainly digested in the small intestine. The presence of fat in the small intestine produces hormones that stimulate the release of pancreatic lipase from the pancreas and bile from the liver which helps in the emulsification of fats for absorption of fatty acids. Complete digestion of one molecule of fat (a triglyceride) results a mixture of fatty acids, mono- and di-glycerides, as well as some undigested triglycerides, but no free glycerol molecules.
Digestion is the breakdown of large insoluble food molecules into small water-soluble food molecules so that they can be absorbed into the watery blood plasma. In certain organisms, these smaller substances are absorbed through the small intestine into the blood stream. Digestion is a form of catabolism that is often divided into two processes based on how food is broken down: mechanical and chemical digestion. The term mechanical digestion refers to the physical breakdown of large pieces of food into smaller pieces which can subsequently be accessed by digestive enzymes. In chemical digestion, enzymes break down food into the small molecules the body can use.
Different phases of digestion take place including: the cephalic phase , gastric phase, and intestinal phase. The cephalic phase occurs at the sight, thought and smell of food, which stimulate the cerebral cortex. Taste and smell stimuli are sent to the hypothalamus and medulla oblongata. After this it is routed through the vagus nerve and release of acetylcholine. Gastric secretion at this phase rises to 40% of maximum rate. Acidity in the stomach is not buffered by food at this point and thus acts to inhibit parietal (secretes acid) and G cell (secretes gastrin) activity via D cell secretion of somatostatin. The gastric phase takes 3 to 4 hours. It is stimulated by distension of the stomach, presence of food in stomach and decrease in pH. Distention activates long and myenteric reflexes. This activates the release of acetylcholine, which stimulates the release of more gastric juices. As protein enters the stomach, it binds to hydrogen ions, which raises the pH of the stomach. Inhibition of gastrin and gastric acid secretion is lifted. This triggers G cells to release gastrin, which in turn stimulates parietal cells to secrete gastric acid. Gastric acid is about 0.5% hydrochloric acid (HCl), which lowers the pH to the desired pH of 1-3. Acid release is also triggered by acetylcholine and histamine. The intestinal phase has two parts, the excitatory and the inhibitory. Partially digested food fills the duodenum. This triggers intestinal gastrin to be released. Enterogastric reflex inhibits vagal nuclei, activating sympathetic fibers causing the pyloric sphincter to tighten to prevent more food from entering, and inhibits local reflexes.
In a channel transupport system, several proteins form a contiguous channel traversing the inner and outer membranes of the bacteria. It is a simple system, which consists of only three protein subunits: the ABC protein, membrane fusion protein (MFP), and outer membrane protein (OMP)[specify]. This secretion system transports various molecules, from ions, drugs, to proteins of various sizes (20 - 900 kDa). The molecules secreted vary in size from the small Escherichia coli peptide colicin V, (10 kDa) to the Pseudomonas fluorescens cell adhesion protein LapA of 900 kDa.
In addition to the use of the multiprotein complexes listed above, Gram-negative bacteria possess another method for release of material: the formation of outer membrane vesicles. Portions of the outer membrane pinch off, forming spherical structures made of a lipid bilayer enclosing periplasmic materials. Vesicles from a number of bacterial species have been found to contain virulence factors, some have immunomodulatory effects, and some can directly adhere to and intoxicate host cells. While release of vesicles has been demonstrated as a general response to stress conditions, the process of loading cargo proteins seems to be selective.
Underlying the process is muscle movement throughout the system through swallowing and peristalsis. Each step in digestion requires energy, and thus imposes an "overhead charge" on the energy made available from absorbed substances. Differences in that overhead cost are important influences on lifestyle, behavior, and even physical structures. Examples may be seen in humans, who differ considerably from other hominids (lack of hair, smaller jaws and musculature, different dentition, length of intestines, cooking, etc.).
Digestion begins in the mouth with the secretion of saliva and its digestive enzymes. Food is formed into a bolus by the mechanical mastication and swallowed into the esophagus from where it enters the stomach through the action of peristalsis. Gastric juice contains hydrochloric acid and pepsin which would damage the walls of the stomach and mucus is secreted for protection. In the stomach further release of enzymes break down the food further and this is combined with the churning action of the stomach. The partially digested food enters the duodenum as a thick semi-liquid chyme. In the small intestine, the larger part of digestion takes place and this is helped by the secretions of bile, pancreatic juice and intestinal juice. The intestinal walls are lined with villi, and their epithelial cells is covered with numerous microvilli to improve the absorption of nutrients by increasing the surface area of the intestine.
Lactase is an enzyme that breaks down the disaccharide lactose to its component parts, glucose and galactose. Glucose and galactose can be absorbed by the small intestine. Approximately 65 percent of the adult population produce only small amounts of lactase and are unable to eat unfermented milk-based foods. This is commonly known as lactose intolerance. Lactose intolerance varies widely by ethnic heritage; more than 90 percent of peoples of east Asian descent are lactose intolerant, in contrast to about 5 percent of people of northern European descent.
After some time (typically 1–2 hours in humans, 4–6 hours in dogs, 3–4 hours in house cats),[citation needed] the resulting thick liquid is called chyme. When the pyloric sphincter valve opens, chyme enters the duodenum where it mixes with digestive enzymes from the pancreas and bile juice from the liver and then passes through the small intestine, in which digestion continues. When the chyme is fully digested, it is absorbed into the blood. 95% of absorption of nutrients occurs in the small intestine. Water and minerals are reabsorbed back into the blood in the colon (large intestine) where the pH is slightly acidic about 5.6 ~ 6.9. Some vitamins, such as biotin and vitamin K (K2MK7) produced by bacteria in the colon are also absorbed into the blood in the colon. Waste material is eliminated from the rectum during defecation.
In mammals, preparation for digestion begins with the cephalic phase in which saliva is produced in the mouth and digestive enzymes are produced in the stomach. Mechanical and chemical digestion begin in the mouth where food is chewed, and mixed with saliva to begin enzymatic processing of starches. The stomach continues to break food down mechanically and chemically through churning and mixing with both acids and enzymes. Absorption occurs in the stomach and gastrointestinal tract, and the process finishes with defecation.
Protein digestion occurs in the stomach and duodenum in which 3 main enzymes, pepsin secreted by the stomach and trypsin and chymotrypsin secreted by the pancreas, break down food proteins into polypeptides that are then broken down by various exopeptidases and dipeptidases into amino acids. The digestive enzymes however are mostly secreted as their inactive precursors, the zymogens. For example, trypsin is secreted by pancreas in the form of trypsinogen, which is activated in the duodenum by enterokinase to form trypsin. Trypsin then cleaves proteins to smaller polypeptides.
Miami (/maɪˈæmi/; Spanish pronunciation: [maiˈami]) is a city located on the Atlantic coast in southeastern Florida and the seat of Miami-Dade County. The 44th-most populated city proper in the United States, with a population of 430,332, it is the principal, central, and most populous city of the Miami metropolitan area, and the second most populous metropolis in the Southeastern United States after Washington, D.C. According to the U.S. Census Bureau, Miami's metro area is the eighth-most populous and fourth-largest urban area in the United States, with a population of around 5.5 million.
Miami is a major center, and a leader in finance, commerce, culture, media, entertainment, the arts, and international trade. In 2012, Miami was classified as an Alpha−World City in the World Cities Study Group's inventory. In 2010, Miami ranked seventh in the United States in terms of finance, commerce, culture, entertainment, fashion, education, and other sectors. It ranked 33rd among global cities. In 2008, Forbes magazine ranked Miami "America's Cleanest City", for its year-round good air quality, vast green spaces, clean drinking water, clean streets, and city-wide recycling programs. According to a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States, and the world's fifth-richest city in terms of purchasing power. Miami is nicknamed the "Capital of Latin America", is the second largest U.S. city with a Spanish-speaking majority, and the largest city with a Cuban-American plurality.
Downtown Miami is home to the largest concentration of international banks in the United States, and many large national and international companies. The Civic Center is a major center for hospitals, research institutes, medical centers, and biotechnology industries. For more than two decades, the Port of Miami, known as the "Cruise Capital of the World", has been the number one cruise passenger port in the world. It accommodates some of the world's largest cruise ships and operations, and is the busiest port in both passenger traffic and cruise lines.
Miami is noted as "the only major city in the United States conceived by a woman, Julia Tuttle", a local citrus grower and a wealthy Cleveland native. The Miami area was better known as "Biscayne Bay Country" in the early years of its growth. In the late 19th century, reports described the area as a promising wilderness. The area was also characterized as "one of the finest building sites in Florida." The Great Freeze of 1894–95 hastened Miami's growth, as the crops of the Miami area were the only ones in Florida that survived. Julia Tuttle subsequently convinced Henry Flagler, a railroad tycoon, to expand his Florida East Coast Railway to the region, for which she became known as "the mother of Miami." Miami was officially incorporated as a city on July 28, 1896 with a population of just over 300. It was named for the nearby Miami River, derived from Mayaimi, the historic name of Lake Okeechobee.
Black labor played a crucial role in Miami's early development. During the beginning of the 20th century, migrants from the Bahamas and African-Americans constituted 40 percent of the city's population. Whatever their role in the city's growth, their community's growth was limited to a small space. When landlords began to rent homes to African-Americans in neighborhoods close to Avenue J (what would later become NW Fifth Avenue), a gang of white man with torches visited the renting families and warned them to move or be bombed.
During the early 20th century, northerners were attracted to the city, and Miami prospered during the 1920s with an increase in population and infrastructure. The legacy of Jim Crow was embedded in these developments. Miami's chief of police, H. Leslie Quigg, did not hide the fact that he, like many other white Miami police officers, was a member of the Ku Klux Klan. Unsurprisingly, these officers enforced social codes far beyond the written law. Quigg, for example, "personally and publicly beat a colored bellboy to death for speaking directly to a white woman."
After Fidel Castro rose to power in Cuba in 1959, many wealthy Cubans sought refuge in Miami, further increasing the population. The city developed businesses and cultural amenities as part of the New South. In the 1980s and 1990s, South Florida weathered social problems related to drug wars, immigration from Haiti and Latin America, and the widespread destruction of Hurricane Andrew. Racial and cultural tensions were sometimes sparked, but the city developed in the latter half of the 20th century as a major international, financial, and cultural center. It is the second-largest U.S. city (after El Paso, Texas) with a Spanish-speaking majority, and the largest city with a Cuban-American plurality.
Miami and its suburbs are located on a broad plain between the Florida Everglades to the west and Biscayne Bay to the east, which also extends from Florida Bay north to Lake Okeechobee. The elevation of the area never rises above 40 ft (12 m) and averages at around 6 ft (1.8 m) above mean sea level in most neighborhoods, especially near the coast. The highest undulations are found along the coastal Miami Rock Ridge, whose substrate underlies most of the eastern Miami metropolitan region. The main portion of the city lies on the shores of Biscayne Bay which contains several hundred natural and artificially created barrier islands, the largest of which contains Miami Beach and South Beach. The Gulf Stream, a warm ocean current, runs northward just 15 miles (24 km) off the coast, allowing the city's climate to stay warm and mild all year.
The surface bedrock under the Miami area is called Miami oolite or Miami limestone. This bedrock is covered by a thin layer of soil, and is no more than 50 feet (15 m) thick. Miami limestone formed as the result of the drastic changes in sea level associated with recent glaciations or ice ages. Beginning some 130,000 years ago the Sangamonian Stage raised sea levels to approximately 25 feet (8 m) above the current level. All of southern Florida was covered by a shallow sea. Several parallel lines of reef formed along the edge of the submerged Florida plateau, stretching from the present Miami area to what is now the Dry Tortugas. The area behind this reef line was in effect a large lagoon, and the Miami limestone formed throughout the area from the deposition of oolites and the shells of bryozoans. Starting about 100,000 years ago the Wisconsin glaciation began lowering sea levels, exposing the floor of the lagoon. By 15,000 years ago, the sea level had dropped to 300 to 350 feet (90 to 110 m) below the contemporary level. The sea level rose quickly after that, stabilizing at the current level about 4000 years ago, leaving the mainland of South Florida just above sea level.
Beneath the plain lies the Biscayne Aquifer, a natural underground source of fresh water that extends from southern Palm Beach County to Florida Bay, with its highest point peaking around the cities of Miami Springs and Hialeah. Most of the Miami metropolitan area obtains its drinking water from this aquifer. As a result of the aquifer, it is not possible to dig more than 15 to 20 ft (5 to 6 m) beneath the city without hitting water, which impedes underground construction, though some underground parking garages exist. For this reason, the mass transit systems in and around Miami are elevated or at-grade.[citation needed]
Miami is partitioned into many different sections, roughly into North, South, West and Downtown. The heart of the city is Downtown Miami and is technically on the eastern side of the city. This area includes Brickell, Virginia Key, Watson Island, and PortMiami. Downtown is South Florida's central business district, and Florida's largest and most influential central business district. Downtown has the largest concentration of international banks in the U.S. along Brickell Avenue. Downtown is home to many major banks, courthouses, financial headquarters, cultural and tourist attractions, schools, parks and a large residential population. East of Downtown, across Biscayne Bay is South Beach. Just northwest of Downtown, is the Civic Center, which is Miami's center for hospitals, research institutes and biotechnology with hospitals such as Jackson Memorial Hospital, Miami VA Hospital, and the University of Miami's Leonard M. Miller School of Medicine.
The southern side of Miami includes Coral Way, The Roads and Coconut Grove. Coral Way is a historic residential neighborhood built in 1922 connecting Downtown with Coral Gables, and is home to many old homes and tree-lined streets. Coconut Grove was established in 1825 and is the location of Miami's City Hall in Dinner Key, the Coconut Grove Playhouse, CocoWalk, many nightclubs, bars, restaurants and bohemian shops, and as such, is very popular with local college students. It is a historic neighborhood with narrow, winding roads, and a heavy tree canopy. Coconut Grove has many parks and gardens such as Villa Vizcaya, The Kampong, The Barnacle Historic State Park, and is the home of the Coconut Grove Convention Center and numerous historic homes and estates.
The northern side of Miami includes Midtown, a district with a great mix of diversity with many West Indians, Hispanics, European Americans, bohemians, and artists. Edgewater, and Wynwood, are neighborhoods of Midtown and are made up mostly of high-rise residential towers and are home to the Adrienne Arsht Center for the Performing Arts. The wealthier residents usually live in the northeastern part, in Midtown, the Design District, and the Upper East Side, with many sought after 1920s homes and home of the MiMo Historic District, a style of architecture originated in Miami in the 1950s. The northern side of Miami also has notable African American and Caribbean immigrant communities such as Little Haiti, Overtown (home of the Lyric Theater), and Liberty City.
Miami has a tropical monsoon climate (Köppen climate classification Am) with hot and humid summers and short, warm winters, with a marked drier season in the winter. Its sea-level elevation, coastal location, position just above the Tropic of Cancer, and proximity to the Gulf Stream shapes its climate. With January averaging 67.2 °F (19.6 °C), winter features mild to warm temperatures; cool air usually settles after the passage of a cold front, which produces much of the little amount of rainfall during the season. Lows occasionally fall below 50 °F (10 °C), but very rarely below 35 °F (2 °C). Highs generally range between 70–77 °F (21–25 °C).
The wet season begins some time in May, ending in mid-October. During this period, temperatures are in the mid 80s to low 90s (29–35 °C), accompanied by high humidity, though the heat is often relieved by afternoon thunderstorms or a sea breeze that develops off the Atlantic Ocean, which then allow lower temperatures, but conditions still remain very muggy. Much of the year's 55.9 inches (1,420 mm) of rainfall occurs during this period. Dewpoints in the warm months range from 71.9 °F (22.2 °C) in June to 73.7 °F (23.2 °C) in August.
The city proper is home to less than one-thirteenth of the population of South Florida. Miami is the 42nd-most populous city in the United States. The Miami metropolitan area, which includes Miami-Dade, Broward and Palm Beach counties, had a combined population of more than 5.5 million people, ranked seventh largest in the United States, and is the largest metropolitan area in the Southeastern United States. As of 2008[update], the United Nations estimates that the Miami Urban Agglomeration is the 44th-largest in the world.
In 1960, non-Hispanic whites represented 80% of Miami-Dade county's population. In 1970, the Census Bureau reported Miami's population as 45.3% Hispanic, 32.9% non-Hispanic White, and 22.7% Black. Miami's explosive population growth has been driven by internal migration from other parts of the country, primarily up until the 1980s, as well as by immigration, primarily from the 1960s to the 1990s. Today, immigration to Miami has slowed significantly and Miami's growth today is attributed greatly to its fast urbanization and high-rise construction, which has increased its inner city neighborhood population densities, such as in Downtown, Brickell, and Edgewater, where one area in Downtown alone saw a 2,069% increase in population in the 2010 Census. Miami is regarded as more of a multicultural mosaic, than it is a melting pot, with residents still maintaining much of, or some of their cultural traits. The overall culture of Miami is heavily influenced by its large population of Hispanics and blacks mainly from the Caribbean islands.
Several large companies are headquartered in or around Miami, including but not limited to: Akerman Senterfitt, Alienware, Arquitectonica, Arrow Air, Bacardi, Benihana, Brightstar Corporation, Burger King, Celebrity Cruises, Carnival Corporation, Carnival Cruise Lines, Crispin Porter + Bogusky, Duany Plater-Zyberk & Company, Espírito Santo Financial Group, Fizber.com, Greenberg Traurig, Holland & Knight, Inktel Direct, Interval International, Lennar, Navarro Discount Pharmacies, Norwegian Cruise Lines, Oceania Cruises, Perry Ellis International, RCTV International, Royal Caribbean Cruise Lines, Ryder Systems, Seabourn Cruise Line, Sedano's, Telefónica USA, UniMÁS, Telemundo, Univision, U.S. Century Bank, Vector Group and World Fuel Services. Because of its proximity to Latin America, Miami serves as the headquarters of Latin American operations for more than 1400 multinational corporations, including AIG, American Airlines, Cisco, Disney, Exxon, FedEx, Kraft Foods, LEO Pharma Americas, Microsoft, Yahoo, Oracle, SBC Communications, Sony, Symantec, Visa International, and Wal-Mart.
Miami is a major television production center, and the most important city in the U.S. for Spanish language media. Univisión, Telemundo and UniMÁS have their headquarters in Miami, along with their production studios. The Telemundo Television Studios produces much of the original programming for Telemundo, such as their telenovelas and talk shows. In 2011, 85% of Telemundo's original programming was filmed in Miami. Miami is also a major music recording center, with the Sony Music Latin and Universal Music Latin Entertainment headquarters in the city, along with many other smaller record labels. The city also attracts many artists for music video and film shootings.
Since 2001, Miami has been undergoing a large building boom with more than 50 skyscrapers rising over 400 feet (122 m) built or currently under construction in the city. Miami's skyline is ranked third-most impressive in the U.S., behind New York City and Chicago, and 19th in the world according to the Almanac of Architecture and Design. The city currently has the eight tallest (as well as thirteen of the fourteen tallest) skyscrapers in the state of Florida, with the tallest being the 789-foot (240 m) Four Seasons Hotel & Tower.
During the mid-2000s, the city witnessed its largest real estate boom since the Florida land boom of the 1920s. During this period, the city had well over a hundred approved high-rise construction projects in which 50 were actually built. In 2007, however, the housing market crashed causing lots of foreclosures on houses. This rapid high-rise construction, has led to fast population growth in the city's inner neighborhoods, primarily in Downtown, Brickell and Edgewater, with these neighborhoods becoming the fastest-growing areas in the city. The Miami area ranks 8th in the nation in foreclosures. In 2011, Forbes magazine named Miami the second-most miserable city in the United States due to its high foreclosure rate and past decade of corruption among public officials. In 2012, Forbes magazine named Miami the most miserable city in the United States because of a crippling housing crisis that has cost multitudes of residents their homes and jobs. The metro area has one of the highest violent crime rates in the country and workers face lengthy daily commutes.
Miami International Airport and PortMiami are among the nation's busiest ports of entry, especially for cargo from South America and the Caribbean. The Port of Miami is the world's busiest cruise port, and MIA is the busiest airport in Florida, and the largest gateway between the United States and Latin America. Additionally, the city has the largest concentration of international banks in the country, primarily along Brickell Avenue in Brickell, Miami's financial district. Due to its strength in international business, finance and trade, many international banks have offices in Downtown such as Espírito Santo Financial Group, which has its U.S. headquarters in Miami. Miami was also the host city of the 2003 Free Trade Area of the Americas negotiations, and is one of the leading candidates to become the trading bloc's headquarters.
Tourism is also an important industry in Miami. Along with finance and business, the beaches, conventions, festivals and events draw over 38 million visitors annually into the city, from across the country and around the world, spending $17.1 billion. The Art Deco District in South Beach, is reputed as one of the most glamorous in the world for its nightclubs, beaches, historical buildings, and shopping. Annual events such as the Sony Ericsson Open, Art Basel, Winter Music Conference, South Beach Wine & Food Festival, and Mercedes-Benz Fashion Week Miami attract millions to the metropolis every year.
According to the U.S. Census Bureau, in 2004, Miami had the third highest incidence of family incomes below the federal poverty line in the United States, making it the third poorest city in the USA, behind only Detroit, Michigan (ranked #1) and El Paso, Texas (ranked #2). Miami is also one of the very few cities where its local government went bankrupt, in 2001. However, since that time, Miami has experienced a revival: in 2008, Miami was ranked as "America's Cleanest City" according to Forbes for its year-round good air quality, vast green spaces, clean drinking water, clean streets and city-wide recycling programs. In a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States (of four U.S. cities included in the survey) and the world's fifth-richest city, in terms of purchasing power.
In addition to such annual festivals like Calle Ocho Festival and Carnaval Miami, Miami is home to many entertainment venues, theaters, museums, parks and performing arts centers. The newest addition to the Miami arts scene is the Adrienne Arsht Center for the Performing Arts, the second-largest performing arts center in the United States after the Lincoln Center in New York City, and is the home of the Florida Grand Opera. Within it are the Ziff Ballet Opera House, the center's largest venue, the Knight Concert Hall, the Carnival Studio Theater and the Peacock Rehearsal Studio. The center attracts many large-scale operas, ballets, concerts, and musicals from around the world and is Florida's grandest performing arts center. Other performing arts venues in Miami include the Gusman Center for the Performing Arts, Coconut Grove Playhouse, Colony Theatre, Lincoln Theatre, New World Center, Actor's Playhouse at the Miracle Theatre, Jackie Gleason Theatre, Manuel Artime Theater, Ring Theatre, Playground Theatre, Wertheim Performing Arts Center, the Fair Expo Center and the Bayfront Park Amphitheater for outdoor music events.
In the early 1970s, the Miami disco sound came to life with TK Records, featuring the music of KC and the Sunshine Band, with such hits as "Get Down Tonight", "(Shake, Shake, Shake) Shake Your Booty" and "That's the Way (I Like It)"; and the Latin-American disco group, Foxy (band), with their hit singles "Get Off" and "Hot Number". Miami-area natives George McCrae and Teri DeSario were also popular music artists during the 1970s disco era. The Bee Gees moved to Miami in 1975 and have lived here ever since then. Miami-influenced, Gloria Estefan and the Miami Sound Machine, hit the popular music scene with their Cuban-oriented sound and had hits in the 1980s with "Conga" and "Bad Boys".
Miami is also considered a "hot spot" for dance music, Freestyle, a style of dance music popular in the 80's and 90's heavily influenced by Electro, hip-hop, and disco. Many popular Freestyle acts such as Pretty Tony, Debbie Deb, Stevie B, and Exposé, originated in Miami. Indie/folk acts Cat Power and Iron & Wine are based in the city, while alternative hip hop artist Sage Francis, electro artist Uffie, and the electroclash duo Avenue D were born in Miami, but musically based elsewhere. Also, ska punk band Against All Authority is from Miami, and rock/metal bands Nonpoint and Marilyn Manson each formed in neighboring Fort Lauderdale. Cuban American female recording artist, Ana Cristina, was born in Miami in 1985.
This was also a period of alternatives to nightclubs, the warehouse party, acid house, rave and outdoor festival scenes of the late 1980s and early 1990s were havens for the latest trends in electronic dance music, especially house and its ever-more hypnotic, synthetic offspring techno and trance, in clubs like the infamous Warsaw Ballroom better known as Warsaw and The Mix where DJs like david padilla (who was the resident DJ for both) and radio. The new sound fed back into mainstream clubs across the country. The scene in SoBe, along with a bustling secondhand market for electronic instruments and turntables, had a strong democratizing effect, offering amateur, "bedroom" DJs the opportunity to become proficient and popular as both music players and producers, regardless of the whims of the professional music and club industries. Some of these notable DJs are John Benetiz (better known as JellyBean Benetiz), Danny Tenaglia, and David Padilla.
Cuban immigrants in the 1960s brought the Cuban sandwich, medianoche, Cuban espresso, and croquetas, all of which have grown in popularity to all Miamians, and have become symbols of the city's varied cuisine. Today, these are part of the local culture, and can be found throughout the city in window cafés, particularly outside of supermarkets and restaurants. Restaurants such as Versailles restaurant in Little Havana is a landmark eatery of Miami. Located on the Atlantic Ocean, and with a long history as a seaport, Miami is also known for its seafood, with many seafood restaurants located along the Miami River, and in and around Biscayne Bay. Miami is also the home of restaurant chains such as Burger King, Tony Roma's and Benihana.
The Miami area has a unique dialect, (commonly called the "Miami accent") which is widely spoken. The dialect developed among second- or third-generation Hispanics, including Cuban-Americans, whose first language was English (though some non-Hispanic white, black, and other races who were born and raised the Miami area tend to adopt it as well.) It is based on a fairly standard American accent but with some changes very similar to dialects in the Mid-Atlantic (especially the New York area dialect, Northern New Jersey English, and New York Latino English.) Unlike Virginia Piedmont, Coastal Southern American, and Northeast American dialects and Florida Cracker dialect (see section below), "Miami accent" is rhotic; it also incorporates a rhythm and pronunciation heavily influenced by Spanish (wherein rhythm is syllable-timed). However, this is a native dialect of English, not learner English or interlanguage; it is possible to differentiate this variety from an interlanguage spoken by second-language speakers in that "Miami accent" does not generally display the following features: there is no addition of /ɛ/ before initial consonant clusters with /s/, speakers do not confuse of /dʒ/ with /j/, (e.g., Yale with jail), and /r/ and /rr/ are pronounced as alveolar approximant [ɹ] instead of alveolar tap [ɾ] or alveolar trill [r] in Spanish.
Miami's main four sports teams are the Miami Dolphins of the National Football League, the Miami Heat of the National Basketball Association, the Miami Marlins of Major League Baseball, and the Florida Panthers of the National Hockey League. As well as having all four major professional teams, Miami is also home to the Major League Soccer expansion team led by David Beckham, Sony Ericsson Open for professional tennis, numerous greyhound racing tracks, marinas, jai alai venues, and golf courses. The city streets has hosted professional auto races, the Miami Indy Challenge and later the Grand Prix Americas. The Homestead-Miami Speedway oval hosts NASCAR national races.
Miami's tropical weather allows for year-round outdoors activities. The city has numerous marinas, rivers, bays, canals, and the Atlantic Ocean, which make boating, sailing, and fishing popular outdoors activities. Biscayne Bay has numerous coral reefs which make snorkeling and scuba diving popular. There are over 80 parks and gardens in the city. The largest and most popular parks are Bayfront Park and Bicentennial Park (located in the heart of Downtown and the location of the American Airlines Arena and Bayside Marketplace), Tropical Park, Peacock Park, Morningside Park, Virginia Key, and Watson Island.
The government of the City of Miami (proper) uses the mayor-commissioner type of system. The city commission consists of five commissioners which are elected from single member districts. The city commission constitutes the governing body with powers to pass ordinances, adopt regulations, and exercise all powers conferred upon the city in the city charter. The mayor is elected at large and appoints a city manager. The City of Miami is governed by Mayor Tomás Regalado and 5 City commissioners which oversee the five districts in the City. The commission's regular meetings are held at Miami City Hall, which is located at 3500 Pan American Drive on Dinner Key in the neighborhood of Coconut Grove .
Miami has one of the largest television markets in the nation and the second largest in the state of Florida. Miami has several major newspapers, the main and largest newspaper being The Miami Herald. El Nuevo Herald is the major and largest Spanish-language newspaper. The Miami Herald and El Nuevo Herald are Miami's and South Florida's main, major and largest newspapers. The papers left their longtime home in downtown Miami in 2013. The newspapers are now headquartered at the former home of U.S. Southern Command in Doral.
Other major newspapers include Miami Today, headquartered in Brickell, Miami New Times, headquartered in Midtown, Miami Sun Post, South Florida Business Journal, Miami Times, and Biscayne Boulevard Times. An additional Spanish-language newspapers, Diario Las Americas also serve Miami. The Miami Herald is Miami's primary newspaper with over a million readers and is headquartered in Downtown in Herald Plaza. Several other student newspapers from the local universities, such as the oldest, the University of Miami's The Miami Hurricane, Florida International University's The Beacon, Miami-Dade College's The Metropolis, Barry University's The Buccaneer, amongst others. Many neighborhoods and neighboring areas also have their own local newspapers such as the Aventura News, Coral Gables Tribune, Biscayne Bay Tribune, and the Palmetto Bay News.
Miami is also the headquarters and main production city of many of the world's largest television networks, record label companies, broadcasting companies and production facilities, such as Telemundo, TeleFutura, Galavisión, Mega TV, Univisión, Univision Communications, Inc., Universal Music Latin Entertainment, RCTV International and Sunbeam Television. In 2009, Univisión announced plans to build a new production studio in Miami, dubbed 'Univisión Studios'. Univisión Studios is currently headquartered in Miami, and will produce programming for all of Univisión Communications' television networks.
Miami International Airport serves as the primary international airport of the Greater Miami Area. One of the busiest international airports in the world, Miami International Airport caters to over 35 million passengers a year. The airport is a major hub and the single largest international gateway for American Airlines. Miami International is the busiest airport in Florida, and is the United States' second-largest international port of entry for foreign air passengers behind New York's John F. Kennedy International Airport, and is the seventh-largest such gateway in the world. The airport's extensive international route network includes non-stop flights to over seventy international cities in North and South America, Europe, Asia, and the Middle East.
Miami is home to one of the largest ports in the United States, the PortMiami. It is the largest cruise ship port in the world. The port is often called the "Cruise Capital of the World" and the "Cargo Gateway of the Americas". It has retained its status as the number one cruise/passenger port in the world for well over a decade accommodating the largest cruise ships and the major cruise lines. In 2007, the port served 3,787,410 passengers. Additionally, the port is one of the nation's busiest cargo ports, importing 7.8 million tons of cargo in 2007. Among North American ports, it ranks second only to the Port of South Louisiana in New Orleans in terms of cargo tonnage imported/exported from Latin America. The port is on 518 acres (2 km2) and has 7 passenger terminals. China is the port's number one import country, and Honduras is the number one export country. Miami has the world's largest amount of cruise line headquarters, home to: Carnival Cruise Lines, Celebrity Cruises, Norwegian Cruise Line, Oceania Cruises, and Royal Caribbean International. In 2014, the Port of Miami Tunnel was completed and will serve the PortMiami.
Miami's heavy-rail rapid transit system, Metrorail, is an elevated system comprising two lines and 23 stations on a 24.4-mile (39.3 km)-long line. Metrorail connects the urban western suburbs of Hialeah, Medley, and inner-city Miami with suburban The Roads, Coconut Grove, Coral Gables, South Miami and urban Kendall via the central business districts of Miami International Airport, the Civic Center, and Downtown. A free, elevated people mover, Metromover, operates 21 stations on three different lines in greater Downtown Miami, with a station at roughly every two blocks of Downtown and Brickell. Several expansion projects are being funded by a transit development sales tax surcharge throughout Miami-Dade County.
Construction is currently underway on the Miami Intermodal Center and Miami Central Station, a massive transportation hub servicing Metrorail, Amtrak, Tri-Rail, Metrobus, Greyhound Lines, taxis, rental cars, MIA Mover, private automobiles, bicycles and pedestrians adjacent to Miami International Airport. Completion of the Miami Intermodal Center is expected to be completed by winter 2011, and will serve over 150,000 commuters and travelers in the Miami area. Phase I of Miami Central Station is scheduled to begin service in the spring of 2012, and Phase II in 2013.
Miami is the southern terminus of Amtrak's Atlantic Coast services, running two lines, the Silver Meteor and the Silver Star, both terminating in New York City. The Miami Amtrak Station is located in the suburb of Hialeah near the Tri-Rail/Metrorail Station on NW 79 St and NW 38 Ave. Current construction of the Miami Central Station will move all Amtrak operations from its current out-of-the-way location to a centralized location with Metrorail, MIA Mover, Tri-Rail, Miami International Airport, and the Miami Intermodal Center all within the same station closer to Downtown. The station was expected to be completed by 2012, but experienced several delays and was later expected to be completed in late 2014, again pushed back to early 2015.
Florida High Speed Rail was a proposed government backed high-speed rail system that would have connected Miami, Orlando, and Tampa. The first phase was planned to connect Orlando and Tampa and was offered federal funding, but it was turned down by Governor Rick Scott in 2011. The second phase of the line was envisioned to connect Miami. By 2014, a private project known as All Aboard Florida by a company of the historic Florida East Coast Railway began construction of a higher-speed rail line in South Florida that is planned to eventually terminate at Orlando International Airport.
Miami's road system is based along the numerical "Miami Grid" where Flagler Street forms the east-west baseline and Miami Avenue forms the north-south meridian. The corner of Flagler Street and Miami Avenue is in the middle of Downtown in front of the Downtown Macy's (formerly the Burdine's headquarters). The Miami grid is primarily numerical so that, for example, all street addresses north of Flagler Street and west of Miami Avenue have "NW" in their address. Because its point of origin is in Downtown, which is close to the coast, therefore, the "NW" and "SW" quadrants are much larger than the "SE" and "NE" quadrants. Many roads, especially major ones, are also named (e.g., Tamiami Trail/SW 8th St), although, with exceptions, the number is in more common usage among locals.
Miami has six major causeways that span over Biscayne Bay connecting the western mainland, with the eastern barrier islands along the Atlantic Ocean. The Rickenbacker Causeway is the southernmost causeway and connects Brickell to Virginia Key and Key Biscayne. The Venetian Causeway and MacArthur Causeway connect Downtown with South Beach. The Julia Tuttle Causeway connects Midtown and Miami Beach. The 79th Street Causeway connects the Upper East Side with North Beach. The northernmost causeway, the Broad Causeway, is the smallest of Miami's six causeways, and connects North Miami with Bal Harbour.
In recent years the city government, under Mayor Manny Diaz, has taken an ambitious stance in support of bicycling in Miami for both recreation and commuting. Every month, the city hosts "Bike Miami", where major streets in Downtown and Brickell are closed to automobiles, but left open for pedestrians and bicyclists. The event began in November 2008, and has doubled in popularity from 1,500 participants to about 3,000 in the October 2009 Bike Miami. This is the longest-running such event in the US. In October 2009, the city also approved an extensive 20-year plan for bike routes and paths around the city. The city has begun construction of bike routes as of late 2009, and ordinances requiring bike parking in all future construction in the city became mandatory as of October 2009.
Spectre (2015) is the twenty-fourth James Bond film produced by Eon Productions. It features Daniel Craig in his fourth performance as James Bond, and Christoph Waltz as Ernst Stavro Blofeld, with the film marking the character's re-introduction into the series. It was directed by Sam Mendes as his second James Bond film following Skyfall, and was written by John Logan, Neal Purvis, Robert Wade and Jez Butterworth. It is distributed by Metro-Goldwyn-Mayer and Columbia Pictures. With a budget around $245 million, it is the most expensive Bond film and one of the most expensive films ever made.
The story sees Bond pitted against the global criminal organisation Spectre, marking the group's first appearance in an Eon Productions film since 1971's Diamonds Are Forever,[N 2] and tying Craig's series of films together with an overarching storyline. Several recurring James Bond characters, including M, Q and Eve Moneypenny return, with the new additions of Léa Seydoux as Dr. Madeleine Swann, Dave Bautista as Mr. Hinx, Andrew Scott as Max Denbigh and Monica Bellucci as Lucia Sciarra.
Spectre was released on 26 October 2015 in the United Kingdom on the same night as the world premiere at the Royal Albert Hall in London, followed by a worldwide release. It was released in the United States on 6 November 2015. It became the second James Bond film to be screened in IMAX venues after Skyfall, although it was not filmed with IMAX cameras. Spectre received mixed reviews upon its release; although criticised for its length, lack of screen time for new characters, and writing, it received praise for its action sequences and cinematography. The theme song, "Writing's on the Wall", received mixed reviews, particularly compared to the previous theme; nevertheless, it won the Golden Globe for Best Original Song and was nominated for the Academy Award in the same category. As of 20 February 2016[update], Spectre has grossed over $879 million worldwide.
Following Garreth Mallory's promotion to M, on a mission in Mexico City unofficially ordered by a posthumous message from the previous M, 007 James Bond kills three men plotting a terrorist bombing during the Day of the Dead and gives chase to Marco Sciarra, an assassin who survived the attack. In the ensuing struggle, Bond steals his ring, which is emblazoned with a stylised octopus, and then kills Sciarra by kicking him out of a helicopter. Upon returning to London, Bond is indefinitely suspended from field duty by M, who is in the midst of a power struggle with C, the head of the privately-backed Joint Intelligence Service, consisting of the recently merged MI5 and MI6. C campaigns for Britain to form alongside 8 other countries "Nine Eyes ", a global surveillance and intelligence co-operation initiative between nine member states, and uses his influence to close down the '00' section, believing it to be outdated.
Bond disobeys M's order and travels to Rome to attend Sciarra's funeral. That evening he visits Sciarra's widow Lucia, who tells him about Spectre, a criminal organisation to which her husband belonged. Bond infiltrates a Spectre meeting, where he identifies the leader, Franz Oberhauser. When Oberhauser addresses Bond by name, he escapes and is pursued by Mr. Hinx, a Spectre assassin. Moneypenny informs Bond that the information he collected leads to Mr. White, former member of Quantum, a subsidiary of Spectre. Bond asks her to investigate Oberhauser, who was presumed dead years earlier.
Bond travels to Austria to find White, who is dying of thallium poisoning. He admits to growing disenchanted with Quantum and tells Bond to find and protect his daughter, Dr. Madeline Swann, who will take him to L'Américain; this will in turn lead him to Spectre. White then commits suicide. Bond locates Swann at the Hoffler Klinik, but she is abducted by Hinx. Bond rescues her and the two meet Q, who discovers that Sciarra's ring links Oberhauser to Bond's previous missions, identifying Le Chiffre, Dominic Greene and Raoul Silva as Spectre agents. Swann reveals that L'Américain is a hotel in Tangier.
The two travel to the hotel and discover White's secret room where they find co-ordinates pointing to Oberhauser's operations base in the desert. They travel by train to the nearest station, but are once again confronted by Hinx; they engage in a fight throughout the train in which Mr Hinx is eventually thrown off the train by Bond with Swann's assistance. After arriving at the station, Bond and Swann are escorted to Oberhauser's base. There, he reveals that Spectre has been staging terrorist attacks around the world, creating a need for the Nine Eyes programme. In return Spectre will be given unlimited access to intelligence gathered by Nine Eyes. Bond is tortured as Oberhauser discusses their shared history: after the younger Bond was orphaned, Oberhauser's father, Hannes, became his temporary guardian. Believing that Bond supplanted his role as son, Oberhauser killed his father and staged his own death, subsequently adopting the name Ernst Stavro Blofeld and going on to form Spectre. Bond and Swann escape, destroying the base in the process, leaving Blofeld to apparently die during the explosion.
Bond and Swann return to London where they meet M, Bill Tanner, Q, and Moneypenny; they intend to arrest C and stop Nine Eyes from going online. Swann leaves Bond, telling him she cannot be part of a life involving espionage, and is subsequently kidnapped. On the way, the group is ambushed and Bond is kidnapped, but the rest still proceed with the plan. After Q succeeds in preventing the Nine Eyes from going online, a brief struggle between M and C ends with the latter falling to his death. Meanwhile, Bond is taken to the old MI6 building, which is scheduled for demolition, and frees himself. Moving throughout the ruined labyrinth, he encounters a disfigured Blofeld, who tells him that he has three minutes to escape the building before explosives are detonated or die trying to save Swann. Bond finds Swann and the two escape by boat as the building collapses. Bond shoots down Blofeld's helicopter, which crashes onto Westminster Bridge. As Blofeld crawls away from the wreckage, Bond confronts him but ultimately leaves him to be arrested by M. Bond leaves the bridge with Swann.
The ownership of the Spectre organisation—originally stylised "SPECTRE" as an acronym of SPecial Executive for Counter-intelligence, Terrorism, Revenge and Extortion—and its characters, had been at the centre of long-standing litigation starting in 1961 between Ian Fleming and Kevin McClory over the film rights to the novel Thunderball. The dispute began after Fleming incorporated elements of an undeveloped film script written by McClory and screenwriter Jack Whittingham—including characters and plot points—into Thunderball, which McClory contested in court, claiming ownership over elements of the novel. In 1963, Fleming settled out of court with McClory, in an agreement which awarded McClory the film rights. This enabled him to become a producer for the 1965 film Thunderball—with Albert R. Broccoli and Harry Saltzman as executive producers—and the non-Eon film Never Say Never Again, an updated remake of Thunderball, in 1983.[N 3] A second remake, entitled Warhead 2000 A.D., was planned for production and release in the 1990s before being abandoned. Under the terms of the 1963 settlement, the literary rights stayed with Fleming, allowing the Spectre organisation and associated characters to continue appearing in print.
In November 2013 MGM and the McClory estate formally settled the issue with Danjaq, LLC—sister company of Eon Productions—with MGM acquiring the full copyright film rights to the concept of Spectre and all of the characters associated with it. With the acquisition of the film rights and the organisation's re-introduction to the series' continuity, the SPECTRE acronym was discarded and the organisation reimagined as "Spectre".
In November 2014, Sony Pictures Entertainment was targeted by hackers who released details of confidential e-mails between Sony executives regarding several high-profile film projects. Included within these were several memos relating to the production of Spectre, claiming that the film was over budget, detailing early drafts of the script written by John Logan, and expressing Sony's frustration with the project. Eon Productions later issued a statement confirming the leak of what they called "an early version of the screenplay".
Despite being an original story, Spectre draws on Ian Fleming's source material, most notably in the character of Franz Oberhauser, played by Christoph Waltz. Oberhauser shares his name with Hannes Oberhauser, a background character in the short story "Octopussy" from the Octopussy and The Living Daylights collection, and who is named in the film as having been a temporary legal guardian of a young Bond in 1983. Similarly, Charmian Bond is shown to have been his full-time guardian, observing the back story established by Fleming. With the acquisition of the rights to Spectre and its associated characters, screenwriters Neal Purvis and Robert Wade revealed that the film would provide a minor retcon to the continuity of the previous films, with the Quantum organisation alluded to in Casino Royale and introduced in Quantum of Solace reimagined as a division within Spectre rather than an independent organisation.
Further references to Fleming's material can be found throughout the film; an MI6 safehouse is called "Hildebrand Rarities and Antiques", a reference to the short story "The Hildebrand Rarity" from the For Your Eyes Only short story collection.[citation needed] Bond's torture by Blofeld mirrors his torture by the title character of Kingsley Amis' continuation novel Colonel Sun.[citation needed]
The main cast was revealed in December 2014 at the 007 Stage at Pinewood Studios. Daniel Craig returned for his fourth appearance as James Bond, while Ralph Fiennes, Naomie Harris and Ben Whishaw reprised their roles as M, Eve Moneypenny and Q respectively, having been established in Skyfall. Rory Kinnear also reprised his role as Bill Tanner in his third appearance in the series.
Christoph Waltz was cast in the role of Franz Oberhauser, though he refused to comment on the nature of the part. It was later revealed with the film's release that he is Ernst Stavro Blofeld. Dave Bautista was cast as Mr. Hinx after producers sought an actor with a background in contact sports. After casting Bérénice Lim Marlohe, a relative newcomer, as Sévérine in Skyfall, Mendes consciously sought out a more experienced actor for the role of Madeleine Swann, ultimately casting Léa Seydoux in the role. Monica Bellucci joined the cast as Lucia Sciarra, becoming, at the age of fifty, the oldest actress to be cast as a Bond girl. In a separate interview with Danish website Euroman, Jesper Christensen revealed he would be reprising his role as Mr. White from Casino Royale and Quantum of Solace. Christensen's character was reportedly killed off in a scene intended to be used as an epilogue to Quantum of Solace, before it was removed from the final cut of the film, enabling his return in Spectre.
In addition to the principal cast, Alessandro Cremona was cast as Marco Sciarra, Stephanie Sigman was cast as Estrella, and Detlef Bothe was cast as a villain for scenes shot in Austria. In February 2015 over fifteen hundred extras were hired for the pre-title sequence set in Mexico, though they were duplicated in the film, giving the effect of around ten thousand extras.
In March 2013 Mendes said he would not return to direct the next film in the series, then known as Bond 24; he later recanted and announced that he would return, as he found the script and the plans for the long-term future of the franchise appealing. In directing Skyfall and Spectre, Mendes became the first director to oversee two consecutive Bond films since John Glen directed The Living Daylights and Licence to Kill in 1987 and 1989. Skyfall writer John Logan resumed his role of scriptwriter, collaborating with Neal Purvis and Robert Wade, who returned for their sixth Bond film.[N 4] The writer Jez Butterworth also worked on the script, alongside Mendes and Craig. Dennis Gassner returned as the film's production designer, while cinematographer Hoyte van Hoytema took over from Roger Deakins. In July 2015 Mendes noted that the combined crew of Spectre numbered over one thousand, making it a larger production than Skyfall. Craig is listed as co-producer.
Mendes revealed that production would begin on 8 December 2014 at Pinewood Studios, with filming taking seven months. Mendes also confirmed several filming locations, including London, Mexico City and Rome. Van Hoytema shot the film on Kodak 35 mm film stock. Early filming took place at Pinewood Studios, and around London, with scenes variously featuring Craig and Harris at Bond's flat, and Craig and Kinnear travelling down the River Thames.
Filming started in Austria in December 2014, with production taking in the area around Sölden—including the Ötztal Glacier Road, Rettenbach glacier and the adjacent ski resort and cable car station—and Obertilliach and Lake Altaussee, before concluding in February 2015. Scenes filmed in Austria centred on the Ice Q Restaurant, standing in for the fictional Hoffler Klinik, a private medical clinic in the Austrian Alps. Filming included an action scene featuring a Land Rover Defender Bigfoot and a Range Rover Sport. Production was temporarily halted first by an injury to Craig, who sprained his knee whilst shooting a fight scene, and later by an accident involving a filming vehicle that saw three crew members injured, at least one of them seriously.
Filming temporarily returned to England to shoot scenes at Blenheim Palace in Oxfordshire, which stood in for a location in Rome, before moving on to the city itself for a five-week shoot across the city, with locations including the Ponte Sisto bridge and the Roman Forum. The production faced opposition from a variety of special interest groups and city authorities, who were concerned about the potential for damage to historical sites around the city, and problems with graffiti and rubbish appearing in the film. A car chase scene set along the banks of the Tiber River and through the streets of Rome featured an Aston Martin DB10 and a Jaguar C-X75. The C-X75 was originally developed as a hybrid electric vehicle with four independent electric engines powered by two jet turbines, before the project was cancelled. The version used for filming was converted to use a conventional internal combustion engine, to minimise the potential for disruption from mechanical problems with the complex hybrid system. The C-X75s used for filming were developed by the engineering division of Formula One racing team Williams, who built the original C-X75 prototype for Jaguar.
With filming completed in Rome, production moved to Mexico City in late March to shoot the film's opening sequence, with scenes to include the Day of the Dead festival filmed in and around the Zócalo and the Centro Histórico district. The planned scenes required the city square to be closed for filming a sequence involving a fight aboard a Messerschmitt-Bölkow-Blohm Bo 105 helicopter flown by stunt pilot Chuck Aaron, which called for modifications to be made to several buildings to prevent damage. This particular scene in Mexico required 1,500 extras, 10 giant skeletons and 250,000 paper flowers. Reports in the Mexican media added that the film's second unit would move to Palenque in the state of Chiapas, to film aerial manoeuvres considered too dangerous to shoot in an urban area.
Following filming in Mexico, and during a scheduled break, Craig was flown to New York to undergo minor surgery to fix his knee injury. It was reported that filming was not affected and he had returned to filming at Pinewood Studios as planned on 22 April.
A brief shoot at London's City Hall was filmed on 18 April 2015, while Mendes was on location. On 17 May 2015 filming took place on the Thames in London. Stunt scenes involving Craig and Seydoux on a speedboat as well as a low flying helicopter near Westminster Bridge were shot at night, with filming temporarily closing both Westminster and Lambeth Bridges. Scenes were also shot on the river near MI6's headquarters at Vauxhall Cross. The crew returned to the river less than a week later to film scenes solely set on Westminster Bridge. The London Fire Brigade was on set to simulate rain as well as monitor smoke used for filming. Craig, Seydoux, and Waltz, as well as Harris and Fiennes, were seen being filmed. Prior to this, scenes involving Fiennes were shot at a restaurant in Covent Garden. Filming then took place in Trafalgar Square. In early June, the crew, as well as Craig, Seydoux, and Waltz, returned to the Thames for a final time to continue filming scenes previously shot on the river.
After wrapping up in England, production travelled to Morocco in June, with filming taking place in Oujda, Tangier and Erfoud, after preliminary work was completed by the production's second unit. An explosion filmed in Morocco holds a Guinness World Record for the "Largest film stunt explosion" in cinematic history, with the record credited to production designer Chris Corbould. Principal photography concluded on 5 July 2015. A wrap-up party for Spectre was held in commemoration before entering post-production. Filming took 128 days.
Whilst filming in Mexico City, speculation in the media claimed that the script had been altered to accommodate the demands of Mexican authorities—reportedly influencing details of the scene and characters, casting choices, and modifying the script in order to portray the country in a "positive light"—in order to secure tax concessions and financial support worth up to $20 million for the film. This was denied by producer Michael G. Wilson, who stated that the scene had always been intended to be shot in Mexico as production had been attracted to the imagery of the Day of the Dead, and that the script had been developed from there. Production of Skyfall had previously faced similar problems while attempting to secure permits to shoot the film's pre-title sequence in India before moving to Istanbul.
Thomas Newman returned as Spectre's composer. Rather than composing the score once the film had moved into post-production, Newman worked during filming. The theatrical trailer released in July 2015 contained a rendition of John Barry's On Her Majesty's Secret Service theme. Mendes revealed that the final film would have more than one hundred minutes of music. The soundtrack album was released on 23 October 2015 in the UK and 6 November 2015 in the USA on the Decca Records label.
In September 2015 it was announced that Sam Smith and regular collaborator Jimmy Napes had written the film's title theme, "Writing's on the Wall", with Smith performing it for the film. Smith said the song came together in one session and that he and Napes wrote it in under half an hour before recording a demo. Satisfied with the quality, the demo was used in the final release.
The song was released as a digital download on 25 September 2015. It received mixed reviews from critics and fans, particularly in comparison to Adele's "Skyfall". The mixed reception to the song led to Shirley Bassey trending on Twitter on the day it was released. It became the first Bond theme to reach number one in the UK Singles Chart. The English band Radiohead also composed a song for the film, which went unused.
During the December 2014 press conference announcing the start of filming, Aston Martin and Eon unveiled the new DB10 as the official car for the film. The DB10 was designed in collaboration between Aston Martin and the filmmakers, with only 10 being produced especially for Spectre as a celebration of the 50th anniversary of the company's association with the franchise. Only eight of those 10 were used for the film, however; the remaining two were used for promotional work. After modifying the Jaguar C-X75 for the film, Williams F1 carried the 007 logo on their cars at the 2015 Mexican Grand Prix, with the team playing host to the cast and crew ahead of the Mexican premiere of the film.
To promote the film, production continued the trend established during Skyfall's production of releasing still images of clapperboards and video blogs on Eon's official social media accounts.
On 13 March 2015, several members of the cast and crew, including Craig, Whishaw, Wilson and Mendes, as well as previous James Bond actor, Sir Roger Moore, appeared in a sketch written by David Walliams and the Dawson Brothers for Comic Relief's Red Nose Day on BBC One. In the sketch, they film a behind-the-scenes mockumentary on the filming of Spectre. The first teaser trailer for Spectre was released worldwide in March 2015, followed by the theatrical trailer in July and the final trailer in October.
Spectre had its world premiere in London on 26 October 2015 at the Royal Albert Hall, the same day as its general release in the United Kingdom and Republic of Ireland. Following the announcement of the start of filming, Paramount Pictures brought forward the release of Mission: Impossible – Rogue Nation to avoid competing with Spectre. In March 2015 IMAX corporation announced that Spectre would be screened in its cinemas, following Skyfall's success with the company. In the UK it received a wider release than Skyfall, with a minimum of 647 cinemas including 40 IMAX screens, compared to Skyfall's 587 locations and 21 IMAX screens.
As of 21 February 2016[update] Spectre has grossed $879.3 million worldwide; $138.1 million of the takings have been generated from the UK market and $199.8 million from North America.
In the United Kingdom, the film grossed £4.1 million ($6.4 million) from its Monday preview screenings. It grossed £6.3 million ($9.2 million) on its opening day and then £5.7 million ($8.8 million) on Wednesday, setting UK records for both days. In the film's first seven days it grossed £41.7 million ($63.8 million), breaking the UK record for highest first-week opening, set by Harry Potter and the Prisoner of Azkaban's £23.88 million ($36.9 million) in 2004. Its Friday–Saturday gross was £20.4 million ($31.2 million) compared to Skyfall's £20.1 million ($31 million). The film also broke the record for the best per-screen opening average with $110,000, a record previously held by The Dark Knight with $100,200. It has grossed a total of $136.3 million there. In the U.K., it surpassed Avatar to become the country's highest-grossing IMAX release ever with $10.09 million.
Spectre opened in Germany with $22.45 million (including previews), which included a new record for the biggest Saturday of all time, Australia with $8.7 million (including previews) and South Korea opened to $8.2 million (including previews). Despite the 13 November Paris attacks, which led to numerous theaters being closed down, the film opened with $14.6 million (including $2 million in previews) in France. In Mexico, where part of the film was shot, it debuted with more than double that of Skyfall with $4.5 million. It also bested its predecessor's opening in various Nordic regions where MGM is distributing, such as in Finland ($2.66 million) and Norway ($2.91 million), and in other markets like Denmark ($4.2 million), the Netherlands ($3.38 million), and Sweden ($3.1 million). In India, it opened at No. 1 with $4.8 million which is 4% above the opening of Skyfall. It topped the German-speaking Switzerland box office for four weeks and in the Netherlands, it has held the No. 1 spot for seven weeks straight where it has topped Minions to become the top movie of the year. The top earning markets are Germany ($70.3 million) and France ($38.8 million). In Paris, it has the second highest ticket sales of all time with $4.1 million tickets sold only behind Spider-Man 3 which sold over $6.32 million tickets in 2007.
In the United States and Canada, the film opened on 6 November 2015, and in its opening weekend, was originally projected to gross $70–75 million from 3,927 screens, the widest release for a Bond film. However, after grossing $5.25 million from its early Thursday night showings and $28 million on its opening day, weekend projections were increased to $75–80 million. The film ended up grossing $70.4 million in its opening weekend (about $20 million less than Skyfall's $90.6 million debut, including IMAX previews), but nevertheless finished first at the box office. IMAX generated $9.1 million for Spectre at 374 screens, premium large format made $8 million from 429 cinemas, reaping 11% of the film's opening, which means that Spectre earned $17.1 million (23%) of its opening weekend total in large-format venues. Cinemark XD generated $1.85 million in 112 XD locations.
In China, it opened on 12 November and earned $15 million on its opening day, which is the second biggest 2D single day gross for a Hollywood film behind the $18.5 million opening day of Mission: Impossible – Rogue Nation and occupying 43% of all available screens which included $790,000 in advance night screenings. Through its opening weekend, it earned $48.1 million from 14,700 screens which is 198% ahead of Skyfall, a new record for a Hollywood 2D opening. IMAX contributed $4.6 million on 246 screens, also a new record for a three-day opening for a November release (breaking Interstellar's record). In its second weekend, it added $12.1 million falling precipitously by 75% which is the second worst second weekend drop for any major Hollywood release in China of 2015. It grossed a total of $84.7 million there after four weekends. Albeit a strong opening it failed to attain the $100 million mark as projected.
Spectre has received mixed reviews, with many reviewers either giving the film highly positive or highly negative feedback. Many critics praised the film's opening scene, action sequences, stuntwork, cinematography and performances from the cast. In some early reviews, the film received favourable comparisons with its predecessor, Skyfall. Rotten Tomatoes sampled 274 reviews and judged 64% of the critiques to be positive, saying that the film "nudges Daniel Craig's rebooted Bond closer to the glorious, action-driven spectacle of earlier entries, although it's admittedly reliant on established 007 formula." On Metacritic, the film has a rating of 60 out of 100, based on 48 critics, indicating "mixed or average reviews". Audiences polled by CinemaScore gave the film an average grade of "A−" on an A+ to F scale.
Prior to its UK release, Spectre mostly received positive reviews. Mark Kermode, writing in The Guardian, gave the film four out of five stars, observing that the film did not live up to the standard set by Skyfall, but was able to tap into audience expectations. Writing in the same publication, Peter Bradshaw gave the film a full five stars, calling it "inventive, intelligent and complex", and singling out Craig's performance as the film's highlight. In another five star review, The Daily Telegraph's Robbie Collin described Spectre as "a swaggering show of confidence'", lauding it as "a feat of pure cinematic necromancy." In an otherwise positive, but overall less enthusiastic review, IGN's Chris Tilly considered Spectre "solid if unspectacular", and gave the film a 7.2 score (out of a possible 10), saying that "the film falls frustratingly short of greatness."
Critical appraisal of the film was mixed in the United States. In a lukewarm review for RogerEbert.com, Matt Zoller Seitz gave the film 2.5 stars out of 4, describing Spectre as inconsistent and unable to capitalise on its potential. Kenneth Turan, reviewing the film for Los Angeles Times, concluded that Spectre "comes off as exhausted and uninspired". Manohla Dargis of The New York Times panned the film as having "nothing surprising" and sacrificing its originality for the sake of box office returns. Forbes' Scott Mendelson also heavily criticised the film, denouncing Spectre as "the worst 007 movie in 30 years". Darren Franich of Entertainment Weekly viewed Spectre as "an overreaction to our current blockbuster moment", aspiring "to be a serialized sequel" and proving "itself as a Saga". While noting that "[n]othing that happens in Spectre holds up to even minor logical scrutiny", he had "come not to bury Spectre, but to weirdly praise it. Because the final act of the movie is so strange, so willfully obtuse, that it deserves extra attention." In a positive review Rolling Stone, Peter Travers gave the film 3.5 stars out of 4, describing "The 24th movie about the British MI6 agent with a license to kill is party time for Bond fans, a fierce, funny, gorgeously produced valentine to the longest-running franchise in movies". Other positive reviews from Mick LaSalle from the San Francisco Chronicle, gave it a perfect 100 score, stating: “One of the great satisfactions of Spectre is that, in addition to all the stirring action, and all the timely references to a secret organization out to steal everyone’s personal information, we get to believe in Bond as a person.” Stephen Whitty from the New York Daily News, gave it an 80 grade, saying: “Craig is cruelly efficient. Dave Bautista makes a good, Oddjob-like assassin. And while Lea Seydoux doesn’t leave a huge impression as this film’s “Bond girl,” perhaps it’s because we’ve already met — far too briefly — the hypnotic Monica Bellucci, as the first real “Bond woman” since Diana Rigg.” Richard Roeper from the Chicago Sun-Times, gave it a 75 grade. He stated: “This is the 24th Bond film and it ranks solidly in the middle of the all-time rankings, which means it’s still a slick, beautifully photographed, action-packed, international thriller with a number of wonderfully, ludicrously entertaining set pieces, a sprinkling of dry wit, myriad gorgeous women and a classic psycho-villain who is clearly out of his mind but seems to like it that way.” Michael Phillips over at the Chicago Tribune, gave it a 75 grade. He stated: “For all its workmanlike devotion to out-of-control helicopters, “Spectre” works best when everyone’s on the ground, doing his or her job, driving expensive fast cars heedlessly, detonating the occasional wisecrack, enjoying themselves and their beautiful clothes.” Guy Lodge from Variety, gave it a 70 score, stating: “What’s missing is the unexpected emotional urgency of “Skyfall,” as the film sustains its predecessor’s nostalgia kick with a less sentimental bent.”
Christopher Orr, writing in The Atlantic, also criticised the film, saying that Spectre "backslides on virtually every [aspect]". Lawrence Toppman of The Charlotte Observer called Craig's performance "Bored, James Bored." Alyssa Rosenberg, writing for The Washington Post, stated that the film turned into "a disappointingly conventional Bond film."
In India, it was reported that the Indian Central Board of Film Certification (CBFC) censored kissing scenes featuring Monica Bellucci, Daniel Craig, and Léa Seydoux. They also muted all profanity. This prompted criticism of the board online, especially on Twitter.
A sequel to Spectre will begin development in spring 2016. Sam Mendes has stated he will not return to direct the next 007 film. Christoph Waltz has signed on for two more films in the series, but his return depends on whether or not Craig will again portray Bond.
Bush's margin of victory in the popular vote was the smallest ever for a reelected incumbent president, but marked the first time since his father's victory 16 years prior that a candidate won a majority of the popular vote. The electoral map closely resembled that of 2000, with only three states changing sides: New Mexico and Iowa voted Republican in 2004 after having voted Democratic in 2000, while New Hampshire voted Democratic in 2004 after previously voting Republican. In the Electoral College, Bush received 286 votes to Kerry's 252.
Just eight months into his presidency, the terrorist attacks of September 11, 2001 suddenly transformed Bush into a wartime president. Bush's approval ratings surged to near 90%. Within a month, the forces of a coalition led by the United States entered Afghanistan, which had been sheltering Osama bin Laden, suspected mastermind of the September 11 attacks. By December, the Taliban had been removed as rulers of Kabul, although a long and ongoing reconstruction would follow, severely hampered by ongoing turmoil and violence within the country.
The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the country. The United States invaded Iraq on March 20, 2003, along with a "coalition of the willing" that consisted of additional troops from the United Kingdom, and to a lesser extent, from Australia and Poland. Within about three weeks, the invasion caused the collapse of both the Iraqi government and its armed forces, however, the U.S. and allied forces failed to find any weapon of mass destruction in Iraq. Traces of former materials and weapons labs were reported to have been located, but no "smoking guns". Nevertheless, on May 1, George W. Bush landed on the aircraft carrier USS Abraham Lincoln, in a Lockheed S-3 Viking, where he gave a speech announcing the end of "major combat operations" in the Iraq War. Bush's approval rating in May was at 66%, according to a CNN–USA Today–Gallup poll. However, Bush's high approval ratings did not last. First, while the war itself was popular in the U.S., the reconstruction and attempted "democratization" of Iraq lost some support as months passed and casualty figures increased, with no decrease in violence nor progress toward stability or reconstruction. Second, as investigators combed through the country, they failed to find the predicted WMD stockpiles, which led to debate over the rationale for the war.
On March 10, 2004, Bush officially clinched the number of delegates needed to be nominated at the 2004 Republican National Convention in New York City. Bush accepted the nomination on September 2, 2004, and selected Vice President Dick Cheney as his running mate. (In New York, the ticket was also on the ballot as candidates of the Conservative Party of New York State.) During the convention and throughout the campaign, Bush focused on two themes: defending America against terrorism and building an ownership society. The ownership society included allowing people to invest some of their Social Security in the stock market, increasing home and stock ownership, and encouraging more people to buy their own health insurance.
By summer of 2003, Howard Dean had become the apparent front runner for the Democratic nomination, performing strongly in most polls and leading the pack with the largest campaign war chest. Dean's strength as a fund raiser was attributed mainly to his embrace of the Internet for campaigning. The majority of his donations came from individual supporters, who became known as Deanites, or, more commonly, Deaniacs. Generally regarded as a pragmatic centrist during his time as governor, Dean emerged during his presidential campaign as a left-wing populist, denouncing the policies of the Bush administration (especially the 2003 invasion of Iraq) as well as fellow Democrats, who, in his view, failed to strongly oppose them. Senator Lieberman, a liberal on domestic issues but a hawk on the War on Terror, failed to gain traction with liberal Democratic primary voters.
In September 2003, retired four-star general Wesley Clark announced his intention to run in the presidential primary election for the Democratic Party nomination. His campaign focused on themes of leadership and patriotism; early campaign ads relied heavily on biography. His late start left him with relatively few detailed policy proposals. This weakness was apparent in his first few debates, although he soon presented a range of position papers, including a major tax-relief plan. Nevertheless, the Democrats did not flock to support his campaign.
In sheer numbers, Kerry had fewer endorsements than Howard Dean, who was far ahead in the superdelegate race going into the Iowa caucuses in January 2004, although Kerry led the endorsement race in Iowa, New Hampshire, Arizona, South Carolina, New Mexico and Nevada. Kerry's main perceived weakness was in his neighboring state of New Hampshire and nearly all national polls. Most other states did not have updated polling numbers to give an accurate placing for the Kerry campaign before Iowa. Heading into the primaries, Kerry's campaign was largely seen as in trouble, particularly after he fired campaign manager Jim Jordan. The key factors enabling it to survive were when fellow Massachusetts Senator Ted Kennedy assigned Mary Beth Cahill to be the campaign manager, as well as Kerry's mortgaging his own home to lend the money to his campaign (while his wife was a billionaire, campaign finance rules prohibited using one's personal fortune). He also brought on the "magical" Michael Whouley who would be credited with helping bring home the Iowa victory the same as he did in New Hampshire for Al Gore in 2000 against Bill Bradley.
In the race for individual contributions, economist Lyndon LaRouche dominated the pack leading up to the primaries. According to the Federal Election Commission statistics, LaRouche had more individual contributors to his 2004 presidential campaign than any other candidate, until the final quarter of the primary season, when John Kerry surpassed him. As of the April 15 filing, LaRouche had 7834 individual contributions, of those who have given cumulatively, $200 or more, as compared to 6257 for John Kerry, 5582 for John Edwards, 4090 for Howard Dean, and 2744 for Gephardt.
By the January 2004 Iowa caucuses, the field had dwindled down to nine candidates, as Bob Graham had dropped out of the race. Howard Dean was a strong front-runner. However, the Iowa caucuses yielded unexpectedly strong results for Democratic candidates John Kerry, who earned 38% of the state's delegates and John Edwards, who took 32%. Former front-runner Howard Dean slipped to 18% and third place, and Richard Gephardt finished fourth (11%). In the days leading up to the Iowa vote, there was much negative campaigning between the Dean and Gephardt camps.
The dismal results caused Gephardt to drop out and later endorse Kerry. Carol Moseley Braun also dropped out, endorsing Howard Dean. Besides the impact of coming in third, Dean was further hurt by a speech he gave at a post-caucus rally. Dean was shouting over the cheers of his enthusiastic audience, but the crowd noise was being filtered out by his unidirectional microphone, leaving only his full-throated exhortations audible to the television viewers. To those at home, he seemed to raise his voice out of sheer emotion. The incessant replaying of the "Dean Scream" by the press became a debate on the topic of whether Dean was the victim of media bias. The scream scene was shown approximately 633 times by cable and broadcast news networks in just four days following the incident, a number that does not include talk shows and local news broadcasts. However, those who were in the actual audience that day insist that they were not aware of the infamous "scream" until they returned to their hotel rooms and saw it on TV.
The following week, John Edwards won the South Carolina primary and finished a strong second in Oklahoma to Clark. Lieberman dropped out of the campaign the following day. Kerry dominated throughout February and his support quickly snowballed as he won caucuses and primaries, taking in a string of wins in Michigan, Washington, Maine, Tennessee, Washington, D.C., Nevada, Wisconsin, Utah, Hawaii, and Idaho. Clark and Dean dropped out during this time, leaving Edwards as the only real threat to Kerry. Kucinich and Sharpton continued to run despite poor results at the polls.
In March's Super Tuesday, Kerry won decisive victories in the California, Connecticut, Georgia, Maryland, Massachusetts, New York, Ohio, and Rhode Island primaries and the Minnesota caucuses. Dean, despite having withdrawn from the race two weeks earlier, won his home state of Vermont. Edwards finished only slightly behind Kerry in Georgia, but, failing to win a single state other than South Carolina, chose to withdraw from the presidential race. Sharpton followed suit a couple weeks later. Kuninch did not leave the race officially until July.
On July 6, John Kerry selected John Edwards as his running mate, shortly before the 2004 Democratic National Convention in Boston, held later that month. Days before Kerry announced Edwards as his running mate, Kerry gave a short list of three candidates: Sen John Edwards, Rep Dick Gephardt, and Gov Tom Vilsack. Heading into the convention, the Kerry/Edwards ticket unveiled their new slogan—a promise to make America "stronger at home and more respected in the world." Kerry made his Vietnam War experience the prominent theme of the convention. In accepting the nomination, he began his speech with, "I'm John Kerry and I'm reporting for duty." He later delivered what may have been the speech's most memorable line when he said, "the future doesn't belong to fear, it belongs to freedom", a quote that later appeared in a Kerry/Edwards television advertisement.
Bush focused his campaign on national security, presenting himself as a decisive leader and contrasted Kerry as a "flip-flopper." This strategy was designed to convey to American voters the idea that Bush could be trusted to be tough on terrorism while Kerry would be "uncertain in the face of danger." Bush (just as his father did with Dukakis in the 1988 election) also sought to portray Kerry as a "Massachusetts liberal" who was out of touch with mainstream Americans. One of Kerry's slogans was "Stronger at home, respected in the world." This advanced the suggestion that Kerry would pay more attention to domestic concerns; it also encapsulated Kerry's contention that Bush had alienated American allies by his foreign policy.
During August and September 2004, there was an intense focus on events that occurred in the late 1960s and early 1970s. Bush was accused of failing to fulfill his required service in the Texas Air National Guard. However, the focus quickly shifted to the conduct of CBS News after they aired a segment on 60 Minutes Wednesday introducing what became known as the Killian documents. Serious doubts about the documents' authenticity quickly emerged, leading CBS to appoint a review panel that eventually resulted in the firing of the news producer and other significant staffing changes.
The first debate was held on September 30 at the University of Miami, moderated by Jim Lehrer of PBS. During the debate, slated to focus on foreign policy, Kerry accused Bush of having failed to gain international support for the 2003 Invasion of Iraq, saying the only countries assisting the U.S. during the invasion were the United Kingdom and Australia. Bush replied to this by saying, "Well, actually, he forgot Poland." Later, a consensus formed among mainstream pollsters and pundits that Kerry won the debate decisively, strengthening what had come to be seen as a weak and troubled campaign. In the days after, coverage focused on Bush's apparent annoyance with Kerry and numerous scowls and negative facial expressions.
The second presidential debate was held at Washington University in St. Louis, Missouri, on October 8, moderated by Charles Gibson of ABC. Conducted in a town meeting format, less formal than the first presidential debate, this debate saw Bush and Kerry taking questions on a variety of subjects from a local audience. Bush attempted to deflect criticism of what was described as his scowling demeanor during the first debate, joking at one point about one of Kerry's remarks, "That answer made me want to scowl."
Bush and Kerry met for the third and final debate at Arizona State University on October 13. 51 million viewers watched the debate which was moderated by Bob Schieffer of CBS News. However, at the time of the ASU debate, there were 15.2 million viewers tuned in to watch the Major League Baseball playoffs broadcast simultaneously. After Kerry, responding to a question about gay rights, reminded the audience that Vice President Cheney's daughter was a lesbian, Cheney responded with a statement calling himself "a pretty angry father" due to Kerry using Cheney's daughter's sexual orientation for his political purposes.
One elector in Minnesota cast a ballot for president with the name of "John Ewards"  [sic] written on it. The Electoral College officials certified this ballot as a vote for John Edwards for president. The remaining nine electors cast ballots for John Kerry. All ten electors in the state cast ballots for John Edwards for vice president (John Edwards's name was spelled correctly on all ballots for vice president). This was the first time in U.S. history that an elector had cast a vote for the same person to be both president and vice president; another faithless elector in the 1800 election had voted twice for Aaron Burr, but under that electoral system only votes for the president's position were cast, with the runner-up in the Electoral College becoming vice president (and the second vote for Burr was discounted and re-assigned to Thomas Jefferson in any event, as it violated Electoral College rules).
The morning after the election, the major candidates were neck and neck. It was clear that the result in Ohio, along with two other states who had still not declared (New Mexico and Iowa), would decide the winner. Bush had established a lead of around 130,000 votes but the Democrats pointed to provisional ballots that had yet to be counted, initially reported to number as high as 200,000. Bush had preliminary leads of less than 5% of the vote in only four states, but if Iowa, Nevada and New Mexico had all eventually gone to Kerry, a win for Bush in Ohio would have created a 269–269 tie in the Electoral College. The result of an electoral tie would cause the election to be decided in the House of Representatives with each state casting one vote, regardless of population. Such a scenario would almost certainly have resulted in a victory for Bush, as Republicans controlled more House delegations. Therefore, the outcome of the election hinged solely on the result in Ohio, regardless of the final totals elsewhere. In the afternoon Ohio's Secretary of State, Ken Blackwell, announced that it was statistically impossible for the Democrats to make up enough valid votes in the provisional ballots to win. At the time provisional ballots were reported as numbering 140,000 (and later estimated to be only 135,000). Faced with this announcement, John Kerry conceded defeat. Had Kerry won Ohio, he would have won the election despite losing the national popular vote by over 3 million votes, a complete reversal of the 2000 election when Bush won the presidency despite losing the popular vote to Al Gore by over 500,000 votes.
At the official counting of the electoral votes on January 6, a motion was made contesting Ohio's electoral votes. Because the motion was supported by at least one member of both the House of Representatives and the Senate, election law mandated that each house retire to debate and vote on the motion. In the House of Representatives, the motion was supported by 31 Democrats. It was opposed by 178 Republicans, 88 Democrats and one independent. Not voting were 52 Republicans and 80 Democrats. Four people elected to the House had not yet taken office, and one seat was vacant. In the Senate, it was supported only by its maker, Senator Boxer, with 74 Senators opposed and 25 not voting. During the debate, no Senator argued that the outcome of the election should be changed by either court challenge or revote. Senator Boxer claimed that she had made the motion not to challenge the outcome, but to "shed the light of truth on these irregularities."
Kerry would later state that "the widespread irregularities make it impossible to know for certain that the [Ohio] outcome reflected the will of the voters." In the same article, Democratic National Committee Chairman Howard Dean said "I'm not confident that the election in Ohio was fairly decided... We know that there was substantial voter suppression, and the machines were not reliable. It should not be a surprise that the Republicans are willing to do things that are unethical to manipulate elections. That's what we suspect has happened."
At the invitation of the United States government, the Organization for Security and Cooperation in Europe (OSCE) sent a team of observers to monitor the presidential elections in 2004. It was the first time the OSCE had sent observers to a U.S. presidential election, although they had been invited in the past. In September 2004 the OSCE issued a report on U.S. electoral processes and the election final report. The report reads: "The November 2, 2004 elections in the United States mostly met the OSCE commitments included in the 1990 Copenhagen Document. They were conducted in an environment that reflects a long-standing democratic tradition, including institutions governed by the rule of law, free and generally professional media, and a civil society intensively engaged in the election process. There was exceptional public interest in the two leading presidential candidates and the issues raised by their respective campaigns, as well as in the election process itself."
The 2004 election was the first to be affected by the campaign finance reforms mandated by the Bipartisan Campaign Reform Act of 2002 (also known as the McCain–Feingold Bill for its sponsors in the United States Senate). Because of the Act's restrictions on candidates' and parties' fundraising, a large number of so-called 527 groups emerged. Named for a section of the Internal Revenue Code, these groups were able to raise large amounts of money for various political causes as long as they do not coordinate their activities with political campaigns. Examples of 527s include Swift Boat Veterans for Truth, MoveOn.org, the Media Fund, and America Coming Together. Many such groups were active throughout the campaign season. (There was some similar activity, although on a much lesser scale, during the 2000 campaign.)
To distinguish official campaigning from independent campaigning, political advertisements on television were required to include a verbal disclaimer identifying the organization responsible for the advertisement. Advertisements produced by political campaigns usually included the statement, "I'm [candidate's name], and I approve this message." Advertisements produced by independent organizations usually included the statement, "[Organization name] is responsible for the content of this advertisement", and from September 3 (60 days before the general election), such organizations' ads were prohibited from mentioning any candidate by name. Previously, television advertisements only required a written "paid for by" disclaimer on the screen.
A ballot initiative in Colorado, known as Amendment 36, would have changed the way in which the state apportions its electoral votes. Rather than assigning all 9 of the state's electors to the candidate with a plurality of popular votes, under the amendment Colorado would have assigned presidential electors proportionally to the statewide vote count, which would be a unique system (Nebraska and Maine assign electoral votes based on vote totals within each congressional district). Detractors claimed that this splitting would diminish Colorado's influence in the Electoral College, and the amendment ultimately failed, receiving only 34% of the vote.
Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expression, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through paralanguage. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion.
Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out "noise", i.e. similar molecules without biotic content.
Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
Effective verbal or spoken communication is dependent on a number of factors and cannot be fully isolated from other important interpersonal skills such as non-verbal communication, listening skills and clarification. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word "language" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages. Communication is two-way process not merely one-way.
Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.
The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication.
The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:
In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of "communication noise" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized "nervous system" of plants. The original meaning of the word "neuron" in Greek is "vegetable fiber" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed]
Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997).
Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.
Neptune is the eighth and farthest known planet from the Sun in the Solar System. It is the fourth-largest planet by diameter and the third-largest by mass. Among the giant planets in the Solar System, Neptune is the most dense. Neptune is 17 times the mass of Earth and is slightly more massive than its near-twin Uranus, which is 15 times the mass of Earth and slightly larger than Neptune.[c] Neptune orbits the Sun once every 164.8 years at an average distance of 30.1 astronomical units (4.50×109 km). Named after the Roman god of the sea, its astronomical symbol is ♆, a stylised version of the god Neptune's trident.
Neptune is not visible to the unaided eye and is the only planet in the Solar System found by mathematical prediction rather than by empirical observation. Unexpected changes in the orbit of Uranus led Alexis Bouvard to deduce that its orbit was subject to gravitational perturbation by an unknown planet. Neptune was subsequently observed with a telescope on 23 September 1846 by Johann Galle within a degree of the position predicted by Urbain Le Verrier. Its largest moon, Triton, was discovered shortly thereafter, though none of the planet's remaining known 14 moons were located telescopically until the 20th century. The planet's distance from Earth gives it a very small apparent size, making it challenging to study with Earth-based telescopes. Neptune was visited by Voyager 2, when it flew by the planet on 25 August 1989. The advent of Hubble Space Telescope and large ground-based telescopes with adaptive optics has recently allowed for additional detailed observations from afar.
Neptune is similar in composition to Uranus, and both have compositions that differ from those of the larger gas giants, Jupiter and Saturn. Like Jupiter and Saturn, Neptune's atmosphere is composed primarily of hydrogen and helium, along with traces of hydrocarbons and possibly nitrogen, but contains a higher proportion of "ices" such as water, ammonia, and methane. However, its interior, like that of Uranus, is primarily composed of ices and rock, and hence Uranus and Neptune are normally considered "ice giants" to emphasise this distinction. Traces of methane in the outermost regions in part account for the planet's blue appearance.
In contrast to the hazy, relatively featureless atmosphere of Uranus, Neptune's atmosphere has active and visible weather patterns. For example, at the time of the Voyager 2 flyby in 1989, the planet's southern hemisphere had a Great Dark Spot comparable to the Great Red Spot on Jupiter. These weather patterns are driven by the strongest sustained winds of any planet in the Solar System, with recorded wind speeds as high as 2,100 kilometres per hour (580 m/s; 1,300 mph). Because of its great distance from the Sun, Neptune's outer atmosphere is one of the coldest places in the Solar System, with temperatures at its cloud tops approaching 55 K (−218 °C). Temperatures at the planet's centre are approximately 5,400 K (5,100 °C). Neptune has a faint and fragmented ring system (labelled "arcs"), which was first detected during the 1960s and confirmed by Voyager 2.
Some of the earliest recorded observations ever made through a telescope, Galileo's drawings on 28 December 1612 and 27 January 1613, contain plotted points that match up with what is now known to be the position of Neptune. On both occasions, Galileo seems to have mistaken Neptune for a fixed star when it appeared close—in conjunction—to Jupiter in the night sky; hence, he is not credited with Neptune's discovery. At his first observation in December 1612, Neptune was almost stationary in the sky because it had just turned retrograde that day. This apparent backward motion is created when Earth's orbit takes it past an outer planet. Because Neptune was only beginning its yearly retrograde cycle, the motion of the planet was far too slight to be detected with Galileo's small telescope. In July 2009, University of Melbourne physicist David Jamieson announced new evidence suggesting that Galileo was at least aware that the 'star' he had observed had moved relative to the fixed stars.
In 1821, Alexis Bouvard published astronomical tables of the orbit of Neptune's neighbour Uranus. Subsequent observations revealed substantial deviations from the tables, leading Bouvard to hypothesise that an unknown body was perturbing the orbit through gravitational interaction. In 1843, John Couch Adams began work on the orbit of Uranus using the data he had. Via Cambridge Observatory director James Challis, he requested extra data from Sir George Airy, the Astronomer Royal, who supplied it in February 1844. Adams continued to work in 1845–46 and produced several different estimates of a new planet.
Meanwhile, Le Verrier by letter urged Berlin Observatory astronomer Johann Gottfried Galle to search with the observatory's refractor. Heinrich d'Arrest, a student at the observatory, suggested to Galle that they could compare a recently drawn chart of the sky in the region of Le Verrier's predicted location with the current sky to seek the displacement characteristic of a planet, as opposed to a fixed star. On the evening of 23 September 1846, the day Galle received the letter, he discovered Neptune within 1° of where Le Verrier had predicted it to be, about 12° from Adams' prediction. Challis later realised that he had observed the planet twice, on 4 and 12 August, but did not recognise it as a planet because he lacked an up-to-date star map and was distracted by his concurrent work on comet observations.
In the wake of the discovery, there was much nationalistic rivalry between the French and the British over who deserved credit for the discovery. Eventually, an international consensus emerged that both Le Verrier and Adams jointly deserved credit. Since 1966, Dennis Rawlins has questioned the credibility of Adams's claim to co-discovery, and the issue was re-evaluated by historians with the return in 1998 of the "Neptune papers" (historical documents) to the Royal Observatory, Greenwich. After reviewing the documents, they suggest that "Adams does not deserve equal credit with Le Verrier for the discovery of Neptune. That credit belongs only to the person who succeeded both in predicting the planet's place and in convincing astronomers to search for it."
Claiming the right to name his discovery, Le Verrier quickly proposed the name Neptune for this new planet, though falsely stating that this had been officially approved by the French Bureau des Longitudes. In October, he sought to name the planet Le Verrier, after himself, and he had loyal support in this from the observatory director, François Arago. This suggestion met with stiff resistance outside France. French almanacs quickly reintroduced the name Herschel for Uranus, after that planet's discoverer Sir William Herschel, and Leverrier for the new planet.
Most languages today, even in countries that have no direct link to Greco-Roman culture, use some variant of the name "Neptune" for the planet. However, in Chinese, Japanese, and Korean, the planet's name was translated as "sea king star" (海王星), because Neptune was the god of the sea. In Mongolian, Neptune is called Dalain Van (Далайн ван), reflecting its namesake god's role as the ruler of the sea. In modern Greek the planet is called Poseidon (Ποσειδώνας, Poseidonas), the Greek counterpart of Neptune. In Hebrew, "Rahab" (רהב), from a Biblical sea monster mentioned in the Book of Psalms, was selected in a vote managed by the Academy of the Hebrew Language in 2009 as the official name for the planet, even though the existing Latin term "Neptun" (נפטון) is commonly used. In Māori, the planet is called Tangaroa, named after the Māori god of the sea. In Nahuatl, the planet is called Tlāloccītlalli, named after the rain god Tlāloc.
From its discovery in 1846 until the subsequent discovery of Pluto in 1930, Neptune was the farthest known planet. When Pluto was discovered it was considered a planet, and Neptune thus became the penultimate known planet, except for a 20-year period between 1979 and 1999 when Pluto's elliptical orbit brought it closer to the Sun than Neptune. The discovery of the Kuiper belt in 1992 led many astronomers to debate whether Pluto should be considered a planet or as part of the Kuiper belt. In 2006, the International Astronomical Union defined the word "planet" for the first time, reclassifying Pluto as a "dwarf planet" and making Neptune once again the outermost known planet in the Solar System.
Neptune's mass of 1.0243×1026 kg, is intermediate between Earth and the larger gas giants: it is 17 times that of Earth but just 1/19th that of Jupiter.[d] Its gravity at 1 bar is 11.15 m/s2, 1.14 times the surface gravity of Earth, and surpassed only by Jupiter. Neptune's equatorial radius of 24,764 km is nearly four times that of Earth. Neptune, like Uranus, is an ice giant, a subclass of giant planet, due to their smaller size and higher concentrations of volatiles relative to Jupiter and Saturn. In the search for extrasolar planets, Neptune has been used as a metonym: discovered bodies of similar mass are often referred to as "Neptunes", just as scientists refer to various extrasolar bodies as "Jupiters".
The mantle is equivalent to 10 to 15 Earth masses and is rich in water, ammonia and methane. As is customary in planetary science, this mixture is referred to as icy even though it is a hot, dense fluid. This fluid, which has a high electrical conductivity, is sometimes called a water–ammonia ocean. The mantle may consist of a layer of ionic water in which the water molecules break down into a soup of hydrogen and oxygen ions, and deeper down superionic water in which the oxygen crystallises but the hydrogen ions float around freely within the oxygen lattice. At a depth of 7000 km, the conditions may be such that methane decomposes into diamond crystals that rain downwards like hailstones. Very-high-pressure experiments at the Lawrence Livermore National Laboratory suggest that the base of the mantle may comprise an ocean of liquid carbon with floating solid 'diamonds'.
At high altitudes, Neptune's atmosphere is 80% hydrogen and 19% helium. A trace amount of methane is also present. Prominent absorption bands of methane exist at wavelengths above 600 nm, in the red and infrared portion of the spectrum. As with Uranus, this absorption of red light by the atmospheric methane is part of what gives Neptune its blue hue, although Neptune's vivid azure differs from Uranus's milder cyan. Because Neptune's atmospheric methane content is similar to that of Uranus, some unknown atmospheric constituent is thought to contribute to Neptune's colour.
Models suggest that Neptune's troposphere is banded by clouds of varying compositions depending on altitude. The upper-level clouds lie at pressures below one bar, where the temperature is suitable for methane to condense. For pressures between one and five bars (100 and 500 kPa), clouds of ammonia and hydrogen sulfide are thought to form. Above a pressure of five bars, the clouds may consist of ammonia, ammonium sulfide, hydrogen sulfide and water. Deeper clouds of water ice should be found at pressures of about 50 bars (5.0 MPa), where the temperature reaches 273 K (0 °C). Underneath, clouds of ammonia and hydrogen sulfide may be found.
High-altitude clouds on Neptune have been observed casting shadows on the opaque cloud deck below. There are also high-altitude cloud bands that wrap around the planet at constant latitude. These circumferential bands have widths of 50–150 km and lie about 50–110 km above the cloud deck. These altitudes are in the layer where weather occurs, the troposphere. Weather does not occur in the higher stratosphere or thermosphere. Unlike Uranus, Neptune's composition has a higher volume of ocean, whereas Uranus has a smaller mantle.
For reasons that remain obscure, the planet's thermosphere is at an anomalously high temperature of about 750 K. The planet is too far from the Sun for this heat to be generated by ultraviolet radiation. One candidate for a heating mechanism is atmospheric interaction with ions in the planet's magnetic field. Other candidates are gravity waves from the interior that dissipate in the atmosphere. The thermosphere contains traces of carbon dioxide and water, which may have been deposited from external sources such as meteorites and dust.
Neptune also resembles Uranus in its magnetosphere, with a magnetic field strongly tilted relative to its rotational axis at 47° and offset at least 0.55 radii, or about 13500 km from the planet's physical centre. Before Voyager 2's arrival at Neptune, it was hypothesised that Uranus's tilted magnetosphere was the result of its sideways rotation. In comparing the magnetic fields of the two planets, scientists now think the extreme orientation may be characteristic of flows in the planets' interiors. This field may be generated by convective fluid motions in a thin spherical shell of electrically conducting liquids (probably a combination of ammonia, methane and water) resulting in a dynamo action.
The dipole component of the magnetic field at the magnetic equator of Neptune is about 14 microteslas (0.14 G). The dipole magnetic moment of Neptune is about 2.2 × 1017 T·m3 (14 μT·RN3, where RN is the radius of Neptune). Neptune's magnetic field has a complex geometry that includes relatively large contributions from non-dipolar components, including a strong quadrupole moment that may exceed the dipole moment in strength. By contrast, Earth, Jupiter and Saturn have only relatively small quadrupole moments, and their fields are less tilted from the polar axis. The large quadrupole moment of Neptune may be the result of offset from the planet's centre and geometrical constraints of the field's dynamo generator.
Neptune has a planetary ring system, though one much less substantial than that of Saturn. The rings may consist of ice particles coated with silicates or carbon-based material, which most likely gives them a reddish hue. The three main rings are the narrow Adams Ring, 63,000 km from the centre of Neptune, the Le Verrier Ring, at 53,000 km, and the broader, fainter Galle Ring, at 42,000 km. A faint outward extension to the Le Verrier Ring has been named Lassell; it is bounded at its outer edge by the Arago Ring at 57,000 km.
Neptune's weather is characterised by extremely dynamic storm systems, with winds reaching speeds of almost 600 m/s (2,200 km/h; 1,300 mph)—nearly reaching supersonic flow. More typically, by tracking the motion of persistent clouds, wind speeds have been shown to vary from 20 m/s in the easterly direction to 325 m/s westward. At the cloud tops, the prevailing winds range in speed from 400 m/s along the equator to 250 m/s at the poles. Most of the winds on Neptune move in a direction opposite the planet's rotation. The general pattern of winds showed prograde rotation at high latitudes vs. retrograde rotation at lower latitudes. The difference in flow direction is thought to be a "skin effect" and not due to any deeper atmospheric processes. At 70° S latitude, a high-speed jet travels at a speed of 300 m/s.
In 2007, it was discovered that the upper troposphere of Neptune's south pole was about 10 K warmer than the rest of its atmosphere, which averages approximately 73 K (−200 °C). The temperature differential is enough to let methane, which elsewhere is frozen in the troposphere, escape into the stratosphere near the pole. The relative "hot spot" is due to Neptune's axial tilt, which has exposed the south pole to the Sun for the last quarter of Neptune's year, or roughly 40 Earth years. As Neptune slowly moves towards the opposite side of the Sun, the south pole will be darkened and the north pole illuminated, causing the methane release to shift to the north pole.
The Scooter is another storm, a white cloud group farther south than the Great Dark Spot. This nickname first arose during the months leading up to the Voyager 2 encounter in 1989, when they were observed moving at speeds faster than the Great Dark Spot (and images acquired later would subsequently reveal the presence of clouds moving even faster than those that had initially been detected by Voyager 2). The Small Dark Spot is a southern cyclonic storm, the second-most-intense storm observed during the 1989 encounter. It was initially completely dark, but as Voyager 2 approached the planet, a bright core developed and can be seen in most of the highest-resolution images.
Neptune's dark spots are thought to occur in the troposphere at lower altitudes than the brighter cloud features, so they appear as holes in the upper cloud decks. As they are stable features that can persist for several months, they are thought to be vortex structures. Often associated with dark spots are brighter, persistent methane clouds that form around the tropopause layer. The persistence of companion clouds shows that some former dark spots may continue to exist as cyclones even though they are no longer visible as a dark feature. Dark spots may dissipate when they migrate too close to the equator or possibly through some other unknown mechanism.
Neptune's more varied weather when compared to Uranus is due in part to its higher internal heating. Although Neptune lies over 50% further from the Sun than Uranus, and receives only 40% its amount of sunlight, the two planets' surface temperatures are roughly equal. The upper regions of Neptune's troposphere reach a low temperature of 51.8 K (−221.3 °C). At a depth where the atmospheric pressure equals 1 bar (100 kPa), the temperature is 72.00 K (−201.15 °C). Deeper inside the layers of gas, the temperature rises steadily. As with Uranus, the source of this heating is unknown, but the discrepancy is larger: Uranus only radiates 1.1 times as much energy as it receives from the Sun; whereas Neptune radiates about 2.61 times as much energy as it receives from the Sun. Neptune is the farthest planet from the Sun, yet its internal energy is sufficient to drive the fastest planetary winds seen in the Solar System. Depending on the thermal properties of its interior, the heat left over from Neptune's formation may be sufficient to explain its current heat flow, though it is more difficult to simultaneously explain Uranus's lack of internal heat while preserving the apparent similarity between the two planets.
On 11 July 2011, Neptune completed its first full barycentric orbit since its discovery in 1846, although it did not appear at its exact discovery position in the sky, because Earth was in a different location in its 365.26-day orbit. Because of the motion of the Sun in relation to the barycentre of the Solar System, on 11 July Neptune was also not at its exact discovery position in relation to the Sun; if the more common heliocentric coordinate system is used, the discovery longitude was reached on 12 July 2011.
Neptune's orbit has a profound impact on the region directly beyond it, known as the Kuiper belt. The Kuiper belt is a ring of small icy worlds, similar to the asteroid belt but far larger, extending from Neptune's orbit at 30 AU out to about 55 AU from the Sun. Much in the same way that Jupiter's gravity dominates the asteroid belt, shaping its structure, so Neptune's gravity dominates the Kuiper belt. Over the age of the Solar System, certain regions of the Kuiper belt became destabilised by Neptune's gravity, creating gaps in the Kuiper belt's structure. The region between 40 and 42 AU is an example.
There do exist orbits within these empty regions where objects can survive for the age of the Solar System. These resonances occur when Neptune's orbital period is a precise fraction of that of the object, such as 1:2, or 3:4. If, say, an object orbits the Sun once for every two Neptune orbits, it will only complete half an orbit by the time Neptune returns to its original position. The most heavily populated resonance in the Kuiper belt, with over 200 known objects, is the 2:3 resonance. Objects in this resonance complete 2 orbits for every 3 of Neptune, and are known as plutinos because the largest of the known Kuiper belt objects, Pluto, is among them. Although Pluto crosses Neptune's orbit regularly, the 2:3 resonance ensures they can never collide. The 3:4, 3:5, 4:7 and 2:5 resonances are less populated.
Neptune has a number of known trojan objects occupying both the Sun–Neptune L4 and L5 Lagrangian points—gravitationally stable regions leading and trailing Neptune in its orbit, respectively. Neptune trojans can be viewed as being in a 1:1 resonance with Neptune. Some Neptune trojans are remarkably stable in their orbits, and are likely to have formed alongside Neptune rather than being captured. The first and so far only object identified as associated with Neptune's trailing L5 Lagrangian point is 2008 LC18. Neptune also has a temporary quasi-satellite, (309239) 2007 RW10. The object has been a quasi-satellite of Neptune for about 12,500 years and it will remain in that dynamical state for another 12,500 years.
The formation of the ice giants, Neptune and Uranus, has proven difficult to model precisely. Current models suggest that the matter density in the outer regions of the Solar System was too low to account for the formation of such large bodies from the traditionally accepted method of core accretion, and various hypotheses have been advanced to explain their formation. One is that the ice giants were not formed by core accretion but from instabilities within the original protoplanetary disc and later had their atmospheres blasted away by radiation from a nearby massive OB star.
An alternative concept is that they formed closer to the Sun, where the matter density was higher, and then subsequently migrated to their current orbits after the removal of the gaseous protoplanetary disc. This hypothesis of migration after formation is favoured, due to its ability to better explain the occupancy of the populations of small objects observed in the trans-Neptunian region. The current most widely accepted explanation of the details of this hypothesis is known as the Nice model, which explores the effect of a migrating Neptune and the other giant planets on the structure of the Kuiper belt.
Neptune has 14 known moons. Triton is the largest Neptunian moon, comprising more than 99.5% of the mass in orbit around Neptune,[e] and it is the only one massive enough to be spheroidal. Triton was discovered by William Lassell just 17 days after the discovery of Neptune itself. Unlike all other large planetary moons in the Solar System, Triton has a retrograde orbit, indicating that it was captured rather than forming in place; it was probably once a dwarf planet in the Kuiper belt. It is close enough to Neptune to be locked into a synchronous rotation, and it is slowly spiralling inward because of tidal acceleration. It will eventually be torn apart, in about 3.6 billion years, when it reaches the Roche limit. In 1989, Triton was the coldest object that had yet been measured in the Solar System, with estimated temperatures of 38 K (−235 °C).
From July to September 1989, Voyager 2 discovered six moons of Neptune. Of these, the irregularly shaped Proteus is notable for being as large as a body of its density can be without being pulled into a spherical shape by its own gravity. Although the second-most-massive Neptunian moon, it is only 0.25% the mass of Triton. Neptune's innermost four moons—Naiad, Thalassa, Despina and Galatea—orbit close enough to be within Neptune's rings. The next-farthest out, Larissa, was originally discovered in 1981 when it had occulted a star. This occultation had been attributed to ring arcs, but when Voyager 2 observed Neptune in 1989, Larissa was found to have caused it. Five new irregular moons discovered between 2002 and 2003 were announced in 2004. A new moon and the smallest yet, S/2004 N 1, was found in 2013. Because Neptune was the Roman god of the sea, Neptune's moons have been named after lesser sea gods.
Because of the distance of Neptune from Earth, its angular diameter only ranges from 2.2 to 2.4 arcseconds, the smallest of the Solar System planets. Its small apparent size makes it challenging to study it visually. Most telescopic data was fairly limited until the advent of Hubble Space Telescope (HST) and large ground-based telescopes with adaptive optics (AO). The first scientifically useful observation of Neptune from ground-based telescopes using adaptive optics, was commenced in 1997 from Hawaii. Neptune is currently entering its spring and summer season and has been shown to be heating up, with increased atmospheric activity and brightness as a consequence. Combined with technological advancements, ground-based telescopes with adaptive optics are recording increasingly more detailed images of this Outer Planet. Both the HST and AO telescopes on Earth has made many new discoveries within the Solar System since the mid-1990s, with a large increase in the number of known satellites and moons around the Outer Planets for example. In 2004 and 2005, five new small satellites of Neptune with diameters between 38 and 61 kilometres were discovered.
Voyager 2 is the only spacecraft that has visited Neptune. The spacecraft's closest approach to the planet occurred on 25 August 1989. Because this was the last major planet the spacecraft could visit, it was decided to make a close flyby of the moon Triton, regardless of the consequences to the trajectory, similarly to what was done for Voyager 1's encounter with Saturn and its moon Titan. The images relayed back to Earth from Voyager 2 became the basis of a 1989 PBS all-night program, Neptune All Night.
After the Voyager 2 flyby mission, the next step in scientific exploration of the Neptunian system, is considered to be a Flagship orbital mission. Such a hypothetical mission is envisioned to be possible at in the late 2020s or early 2030s. However, there have been a couple of discussions to launch Neptune missions sooner. In 2003, there was a proposal in NASA's "Vision Missions Studies" for a "Neptune Orbiter with Probes" mission that does Cassini-level science. Another, more recent proposal was for Argo, a flyby spacecraft to be launched in 2019, that would visit Jupiter, Saturn, Neptune, and a Kuiper belt object. The focus would be on Neptune and its largest moon Triton to be investigated around 2029. The proposed New Horizons 2 mission (which was later scrapped) might also have done a close flyby of the Neptunian system.
The Qing dynasty (Chinese: 清朝; pinyin: Qīng Cháo; Wade–Giles: Ch'ing Ch'ao; IPA: [tɕʰíŋ tʂʰɑ̌ʊ̯]), officially the Great Qing (Chinese: 大清; pinyin: Dà Qīng), also called the Empire of the Great Qing, or the Manchu dynasty, was the last imperial dynasty of China, ruling from 1644 to 1912 with a brief, abortive restoration in 1917. It was preceded by the Ming dynasty and succeeded by the Republic of China. The Qing multi-cultural empire lasted almost three centuries and formed the territorial base for the modern Chinese state.
The dynasty was founded by the Jurchen Aisin Gioro clan in Manchuria. In the late sixteenth century, Nurhaci, originally a Ming vassal, began organizing Jurchen clans into "Banners", military-social units. Nurhaci formed these clans into a unified entity, the subjects of which became known collectively as the Manchu people. By 1636, his son Hong Taiji began driving Ming forces out of Liaodong and declared a new dynasty, the Qing. In 1644, peasant rebels led by Li Zicheng conquered the Ming capital Beijing. Rather than serve them, Ming general Wu Sangui made an alliance with the Manchus and opened the Shanhai Pass to the Banner Armies led by Prince Dorgon, who defeated the rebels and seized Beijing. The conquest of China proper was not completed until 1683 under the Kangxi Emperor (r. 1661–1722). The Ten Great Campaigns of the Qianlong Emperor from the 1750s to the 1790s extended Qing control into Central Asia. While the early rulers maintained their Manchu ways, and while their official title was Emperor they were known as khans to the Mongols and patronized Tibetan Buddhism, they governed using Confucian styles and institutions of bureaucratic government. They retained the imperial examinations to recruit Han Chinese to work under or in parallel with Manchus. They also adapted the ideals of the tributary system in international relations, and in places such as Taiwan, the Qing so-called internal foreign policy closely resembled colonial policy and control.
The reign of the Qianlong Emperor (1735–1796) saw the apogee and initial decline in prosperity and imperial control. The population rose to some 400 million, but taxes and government revenues were fixed at a low rate, virtually guaranteeing eventual fiscal crisis. Corruption set in, rebels tested government legitimacy, and ruling elites did not change their mindsets in the face of changes in the world system. Following the Opium War, European powers imposed unequal treaties, free trade, extraterritoriality and treaty ports under foreign control. The Taiping Rebellion (1850–64) and Dungan Revolt (1862–77) in Central Asia led to the deaths of some 20 million people. In spite of these disasters, in the Tongzhi Restoration of the 1860s, Han Chinese elites rallied to the defense of the Confucian order and the Qing rulers. The initial gains in the Self-Strengthening Movement were destroyed in the First Sino-Japanese War of 1895, in which the Qing lost its influence over Korea and the possession of Taiwan. New Armies were organized, but the ambitious Hundred Days' Reform of 1898 was turned back by Empress Dowager Cixi, a ruthless but capable leader. When, in response to the violently anti-foreign Yihetuan ("Boxers"), foreign powers invaded China, the Empress Dowager declared war on them, leading to defeat and the flight of the Imperial Court to Xi'an.
After agreeing to sign the Boxer Protocol the government then initiated unprecedented fiscal and administrative reforms, including elections, a new legal code, and abolition of the examination system. Sun Yat-sen and other revolutionaries competed with reformers such as Liang Qichao and monarchists such as Kang Youwei to transform the Qing empire into a modern nation. After the death of Empress Dowager Cixi and the Guangxu Emperor in 1908, the hardline Manchu court alienated reformers and local elites alike. Local uprisings starting on October 11, 1911 led to the Xinhai Revolution. Puyi, the last emperor, abdicated on February 12, 1912.
Nurhaci declared himself the "Bright Khan" of the Later Jin (lit. "gold") state in honor both of the 12–13th century Jurchen Jin dynasty and of his Aisin Gioro clan (Aisin being Manchu for the Chinese 金 (jīn, "gold")).  His son Hong Taiji renamed the dynasty Great Qing in 1636. There are competing explanations on the meaning of Qīng (lit. "clear" or "pure"). The name may have been selected in reaction to the name of the Ming dynasty (明), which consists of the Chinese characters for "sun" (日) and "moon" (月), both associated with the fire element of the Chinese zodiacal system. The character Qīng (清) is composed of "water" (氵) and "azure" (青), both associated with the water element. This association would justify the Qing conquest as defeat of fire by water. The water imagery of the new name may also have had Buddhist overtones of perspicacity and enlightenment and connections with the Bodhisattva Manjusri. The Manchu name daicing, which sounds like a phonetic rendering of Dà Qīng or Dai Ching, may in fact have been derived from a Mongolian word that means "warrior". Daicing gurun may therefore have meant "warrior state", a pun that was only intelligible to Manchu and Mongol people. In the later part of the dynasty, however, even the Manchus themselves had forgotten this possible meaning.
After conquering "China proper", the Manchus identified their state as "China" (中國, Zhōngguó; "Middle Kingdom"), and referred to it as Dulimbai Gurun in Manchu (Dulimbai means "central" or "middle," gurun means "nation" or "state"). The emperors equated the lands of the Qing state (including present day Northeast China, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, and rejecting the idea that "China" only meant Han areas. The Qing emperors proclaimed that both Han and non-Han peoples were part of "China." They used both "China" and "Qing" to refer to their state in official documents, international treaties (as the Qing was known internationally as "China" or the "Chinese Empire") and foreign affairs, and "Chinese language" (Dulimbai gurun i bithe) included Chinese, Manchu, and Mongol languages, and "Chinese people" (中國之人 Zhōngguó zhī rén; Manchu: Dulimbai gurun i niyalma) referred to all subjects of the empire. In the Chinese-language versions of its treaties and its maps of the world, the Qing government used "Qing" and "China" interchangeably.
The Qing dynasty was founded not by Han Chinese, who constitute the majority of the Chinese population, but by a sedentary farming people known as the Jurchen, a Tungusic people who lived around the region now comprising the Chinese provinces of Jilin and Heilongjiang. The Manchus are sometimes mistaken for a nomadic people, which they were not. What was to become the Manchu state was founded by Nurhaci, the chieftain of a minor Jurchen tribe – the Aisin Gioro – in Jianzhou in the early 17th century. Originally a vassal of the Ming emperors, Nurhachi embarked on an intertribal feud in 1582 that escalated into a campaign to unify the nearby tribes. By 1616, he had sufficiently consolidated Jianzhou so as to be able to proclaim himself Khan of the Great Jin in reference to the previous Jurchen dynasty.
Relocating his court from Jianzhou to Liaodong provided Nurhachi access to more resources; it also brought him in close contact with the Khorchin Mongol domains on the plains of Mongolia. Although by this time the once-united Mongol nation had long since fragmented into individual and hostile tribes, these tribes still presented a serious security threat to the Ming borders. Nurhachi's policy towards the Khorchins was to seek their friendship and cooperation against the Ming, securing his western border from a powerful potential enemy.
There were too few ethnic Manchus to conquer China, so they gained strength by defeating and absorbing Mongols, but more importantly, adding Han Chinese to the Eight Banners. The Manchus had to create an entire "Jiu Han jun" (Old Han Army) due to the massive amount of Han Chinese soldiers which were absorbed into the Eight Banners by both capture and defection, Ming artillery was responsible for many victories against the Manchus, so the Manchus established an artillery corps made out of Han Chinese soldiers in 1641 and the swelling of Han Chinese numbers in the Eight Banners led in 1642 of all Eight Han Banners being created. It was defected Ming Han Chinese armies which conquered southern China for the Qing.
This was followed by the creation of the first two Han Banners in 1637 (increasing to eight in 1642). Together these military reforms enabled Hong Taiji to resoundingly defeat Ming forces in a series of battles from 1640 to 1642 for the territories of Songshan and Jinzhou. This final victory resulted in the surrender of many of the Ming dynasty's most battle-hardened troops, the death of Yuan Chonghuan at the hands of the Chongzhen Emperor (who thought Yuan had betrayed him), and the complete and permanent withdrawal of the remaining Ming forces north of the Great Wall.
Hong Taiji's bureaucracy was staffed with many Han Chinese, including many newly surrendered Ming officials. The Manchus' continued dominance was ensured by an ethnic quota for top bureaucratic appointments. Hong Taiji's reign also saw a fundamental change of policy towards his Han Chinese subjects. Nurhaci had treated Han in Liaodong differently according to how much grain they had, those with less than 5 to 7 sin were treated like chattel while those with more than that amount were rewarded with property. Due to a revolt by Han in Liaodong in 1623, Nurhachi, who previously gave concessions to conquered Han subjects in Liaodong, turned against them and ordered that they no longer be trusted; He enacted discriminatory policies and killings against them, while ordering that Han who assimilated to the Jurchen (in Jilin) before 1619 be treated equally as Jurchens were and not like the conquered Han in Liaodong. Hong Taiji instead incorporated them into the Jurchen "nation" as full (if not first-class) citizens, obligated to provide military service. By 1648, less than one-sixth of the bannermen were of Manchu ancestry. This change of policy not only increased Hong Taiji's manpower and reduced his military dependence on banners not under his personal control, it also greatly encouraged other Han Chinese subjects of the Ming dynasty to surrender and accept Jurchen rule when they were defeated militarily. Through these and other measures Hong Taiji was able to centralize power unto the office of the Khan, which in the long run prevented the Jurchen federation from fragmenting after his death.
Hong Taiji died suddenly in September 1643 without a designated heir. As the Jurchens had traditionally "elected" their leader through a council of nobles, the Qing state did not have in place a clear succession system until the reign of the Kangxi Emperor. The leading contenders for power at this time were Hong Taiji's oldest son Hooge and Hong Taiji' half brother Dorgon. A compromise candidate in the person of Hong Taiji's five-year-old son, Fulin, was installed as the Shunzhi Emperor, with Dorgon as regent and de facto leader of the Manchu nation.
Ming government officials fought against each other, against fiscal collapse, and against a series of peasant rebellions. They were unable to capitalise on the Manchu succession dispute and installation of a minor as emperor. In April 1644, the capital at Beijing was sacked by a coalition of rebel forces led by Li Zicheng, a former minor Ming official, who established a short-lived Shun dynasty. The last Ming ruler, the Chongzhen Emperor, committed suicide when the city fell, marking the official end of the dynasty.
Li Zicheng then led a coalition of rebel forces numbering 200,000[a] to confront Wu Sangui, the general commanding the Ming garrison at Shanhai Pass. Shanhai Pass is a pivotal pass of the Great Wall, located fifty miles northeast of Beijing, and for years its defenses kept the Manchus from directly raiding the Ming capital. Wu Sangui, caught between a rebel army twice his size and a foreign enemy he had fought for years, decided to cast his lot with the Manchus, with whom he was familiar. Wu Sangui may have been influenced by Li Zicheng's mistreatment of his family and other wealthy and cultured officials; it was said that Li also took Wu's concubine Chen Yuanyuan for himself. Wu and Dorgon allied in the name of avenging the death of the Chongzhen Emperor. Together, the two former enemies met and defeated Li Zicheng's rebel forces in battle on May 27, 1644.
The newly allied armies captured Beijing on June 6. The Shunzhi Emperor was invested as the "Son of Heaven" on October 30. The Manchus, who had positioned themselves as political heir to the Ming emperor by defeating the rebel Li Zicheng, completed the symbolic transition by holding a formal funeral for the Chongzhen Emperor. However the process of conquering the rest of China took another seventeen years of battling Ming loyalists, pretenders and rebels. The last Ming pretender, Prince Gui, sought refuge with the King of Burma, but was turned over to a Qing expeditionary army commanded by Wu Sangui, who had him brought back to Yunnan province and executed in early 1662.
Han Chinese Banners were made up of Han Chinese who defected to the Qing up to 1644 and joined the Eight Banners, giving them social and legal privileges in addition to being acculturated to Manchu culture. So many Han defected to the Qing and swelled the ranks of the Eight Banners that ethnic Manchus became a minority, making up only 16% in 1648, with Han Bannermen dominating at 75% and Mongol Bannermen making up the rest. This multi-ethnic force in which Manchus were only a minority conquered China for the Qing.
The Qing showed that the Manchus valued military skills in propaganda targeted towards the Ming military to get them to defect to the Qing, since the Ming civilian political system discriminated against the military. The three Liaodong Han Bannermen officers who played a massive role in the conquest of southern China from the Ming were Shang Kexi, Geng Zhongming, and Kong Youde and they governed southern China autonomously as viceroys for the Qing after their conquests. Normally the Manchu Bannermen acted only as reserve forces or in the rear and were used predominantly for quick strikes with maximum impact, so as to minimize ethnic Manchu losses; instead, the Qing used defected Han Chinese troops to fight as the vanguard during the entire conquest of China.
First, the Manchus had entered "China proper" because Dorgon responded decisively to Wu Sangui's appeal. Then, after capturing Beijing, instead of sacking the city as the rebels had done, Dorgon insisted, over the protests of other Manchu princes, on making it the dynastic capital and reappointing most Ming officials. Choosing Beijing as the capital had not been a straightforward decision, since no major Chinese dynasty had directly taken over its immediate predecessor's capital. Keeping the Ming capital and bureaucracy intact helped quickly stabilize the regime and sped up the conquest of the rest of the country. However, not all of Dorgon's policies were equally popular nor easily implemented.
Dorgon's controversial July 1645 edict (the "haircutting order") forced adult Han Chinese men to shave the front of their heads and comb the remaining hair into the queue hairstyle which was worn by Manchu men, on pain of death. The popular description of the order was: "To keep the hair, you lose the head; To keep your head, you cut the hair." To the Manchus, this policy was a test of loyalty and an aid in distinguishing friend from foe. For the Han Chinese, however, it was a humiliating reminder of Qing authority that challenged traditional Confucian values. The Classic of Filial Piety (Xiaojing) held that "a person's body and hair, being gifts from one's parents, are not to be damaged." Under the Ming dynasty, adult men did not cut their hair but instead wore it in the form of a top-knot. The order triggered strong resistance to Qing rule in Jiangnan and massive killing of ethnic Han Chinese. It was Han Chinese defectors who carried out massacres against people refusing to wear the queue.. Li Chengdong, a Han Chinese general who had served the Ming but surrendered to the Qing, ordered his Han troops to carry out three separate massacres in the city of Jiading within a month, resulting in tens of thousands of deaths. At the end of the third massacre, there was hardly any living person left in this city. Jiangyin also held out against about 10,000 Han Chinese Qing troops for 83 days. When the city wall was finally breached on 9 October 1645, the Han Chinese Qing army led by the Han Chinese Ming defector Liu Liangzuo (劉良佐), who had been ordered to "fill the city with corpses before you sheathe your swords," massacred the entire population, killing between 74,000 and 100,000 people. The queue was the only aspect of Manchu culture which the Qing forced on the common Han population. The Qing required people serving as officials to wear Manchu clothing, but allowed non-official Han civilians to continue wearing Hanfu (Han clothing).
Although his support had been essential to Shunzhi's ascent, Dorgon had through the years centralised so much power in his hands as to become a direct threat to the throne. So much so that upon his death he was extraordinarily bestowed the posthumous title of Emperor Yi (Chinese: 義皇帝), the only instance in Qing history in which a Manchu "prince of the blood" (Chinese: 親王) was so honored. Two months into Shunzhi's personal rule, Dorgon was not only stripped of his titles, but his corpse was disinterred and mutilated.[b] to atone for multiple "crimes", one of which was persecuting to death Shunzhi’s agnate eldest brother, Hooge. More importantly, Dorgon's symbolic fall from grace also signaled a political purge of his family and associates at court, thus reverting power back to the person of the emperor. After a promising start, Shunzhi's reign was cut short by his early death in 1661 at the age of twenty-four from smallpox. He was succeeded by his third son Xuanye, who reigned as the Kangxi Emperor.
The Manchus sent Han Bannermen to fight against Koxinga's Ming loyalists in Fujian. The Qing carried out a massive depopulation policy and seaban forcing people to evacuated the coast in order to deprive Koxinga's Ming loyalists of resources, this has led to a myth that it was because Manchus were "afraid of water". In Fujian, it was Han Bannermen who were the ones carrying out the fighting and killing for the Qing and this disproved the entirely irrelevant claim that alleged fear of the water on part of the Manchus had to do with the coastal evacuation and seaban. Even though a poem refers to the soldiers carrying out massacres in Fujian as "barbarian", both Han Green Standard Army and Han Bannermen were involved in the fighting for the Qing side and carried out the worst slaughter. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.
The sixty-one year reign of the Kangxi Emperor was the longest of any Chinese emperor. Kangxi's reign is also celebrated as the beginning of an era known as the "High Qing", during which the dynasty reached the zenith of its social, economic and military power. Kangxi's long reign started when he was eight years old upon the untimely demise of his father. To prevent a repeat of Dorgon's dictatorial monopolizing of power during the regency, the Shunzhi Emperor, on his deathbed, hastily appointed four senior cabinet ministers to govern on behalf of his young son. The four ministers — Sonin, Ebilun, Suksaha, and Oboi — were chosen for their long service, but also to counteract each other's influences. Most important, the four were not closely related to the imperial family and laid no claim to the throne. However, as time passed, through chance and machination, Oboi, the most junior of the four, achieved such political dominance as to be a potential threat. Even though Oboi's loyalty was never an issue, his personal arrogance and political conservatism led him into an escalating conflict with the young emperor. In 1669 Kangxi, through trickery, disarmed and imprisoned Oboi — a significant victory for a fifteen-year-old emperor over a wily politician and experienced commander.
The early Manchu rulers also established two foundations of legitimacy which help to explain the stability of their dynasty. The first was the bureaucratic institutions and the neo-Confucian culture which they adopted from earlier dynasties. Manchu rulers and Han Chinese scholar-official elites gradually came to terms with each other. The examination system offered a path for ethnic Han to become officials. Imperial patronage of Kangxi Dictionary demonstrated respect for Confucian learning, while the Sacred Edict of 1670 effectively extolled Confucian family values. The second major source of stability was the Central Asian aspect of their Manchu identity which allowed them to appeal to Mongol, Tibetan and Uighur constituents. The Qing used the title of Emperor (Huangdi) in Chinese while among Mongols the Qing monarch was referred to as Bogda khan (wise Khan), and referred to as Gong Ma in Tibet. Qianlong propagated the image of himself as Buddhist sage rulers, patrons of Tibetan Buddhism. In the Manchu language, the Qing monarch was alternately referred to as either Huwangdi (Emperor) or Khan with no special distinction between the two usages. The Kangxi Emperor also welcomed to his court Jesuit missionaries, who had first come to China under the Ming. Missionaries including Tomás Pereira, Martino Martini, Johann Adam Schall von Bell, Ferdinand Verbiest and Antoine Thomas held significant positions as military weapons experts, mathematicians, cartographers, astronomers and advisers to the emperor. The relationship of trust was however lost in the later Chinese Rites controversy.
Yet controlling the "Mandate of Heaven" was a daunting task. The vastness of China's territory meant that there were only enough banner troops to garrison key cities forming the backbone of a defense network that relied heavily on surrendered Ming soldiers. In addition, three surrendered Ming generals were singled out for their contributions to the establishment of the Qing dynasty, ennobled as feudal princes (藩王), and given governorships over vast territories in Southern China. The chief of these was Wu Sangui, who was given the provinces of Yunnan and Guizhou, while generals Shang Kexi and Geng Jingzhong were given Guangdong and Fujian provinces respectively.
As the years went by, the three feudal lords and their extensive territories became increasingly autonomous. Finally, in 1673, Shang Kexi petitioned Kangxi for permission to retire to his hometown in Liaodong province and nominated his son as his successor. The young emperor granted his retirement, but denied the heredity of his fief. In reaction, the two other generals decided to petition for their own retirements to test Kangxi's resolve, thinking that he would not risk offending them. The move backfired as the young emperor called their bluff by accepting their requests and ordering that all three fiefdoms to be reverted to the crown.
Faced with the stripping of their powers, Wu Sangui, later joined by Geng Zhongming and by Shang Kexi's son Shang Zhixin, felt they had no choice but to revolt. The ensuing Revolt of the Three Feudatories lasted for eight years. Wu attempted, ultimately in vain, to fire the embers of south China Ming loyalty by restoring Ming customs, ordering that the resented queues be cut, and declaring himself emperor of a new dynasty. At the peak of the rebels' fortunes, they extended their control as far north as the Yangtze River, nearly establishing a divided China. Wu then hesitated to go further north, not being able to coordinate strategy with his allies, and Kangxi was able to unify his forces for a counterattack led by a new generation of Manchu generals. By 1681, the Qing government had established control over a ravaged southern China which took several decades to recover. Manchu Generals and Bannermen were initially put to shame by the better performance of the Han Chinese Green Standard Army, who fought better than them against the rebels and this was noted by Kangxi, leading him to task Generals Sun Sike, Wang Jinbao, and Zhao Liangdong to lead Green Standard Soldiers to crush the rebels. The Qing thought that Han Chinese were superior at battling other Han people and so used the Green Standard Army as the dominant and majority army in crushing the rebels instead of Bannermen. Similarly, in northwestern China against Wang Fuchen, the Qing used Han Chinese Green Standard Army soldiers and Han Chinese Generals such as Zhang Liangdong, Wang Jinbao, and Zhang Yong as the primary military forces. This choice was due to the rocky terrain, which favoured infantry troops over cavalry, to the desire to keep Bannermen in the reserves, and, again, to the belief that Han troops were better at fighting other Han people. These Han generals achieved victory over the rebels. Also due to the mountainous terrain, Sichuan and southern Shaanxi were also retaken by the Han Chinese Green Standard Army under Wang Jinbao and Zhao Liangdong in 1680, with Manchus only participating in dealing with logistics and provisions. 400,000 Green Standard Army soldiers and 150,000 Bannermen served on the Qing side during the war. 213 Han Chinese Banner companies, and 527 companies of Mongol and Manchu Banners were mobilized by the Qing during the revolt. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.
The Qing forces were crushed by Wu from 1673-1674. The Qing had the support of the majority of Han Chinese soldiers and Han elite against the Three Feudatories, since they refused to join Wu Sangui in the revolt, while the Eight Banners and Manchu officers fared poorly against Wu Sangui, so the Qing responded with using a massive army of more than 900,000 Han Chinese (non-Banner) instead of the Eight Banners, to fight and crush the Three Feudatories. Wu Sangui's forces were crushed by the Green Standard Army, made out of defected Ming soldiers.
To extend and consolidate the dynasty's control in Central Asia, the Kangxi Emperor personally led a series of military campaigns against the Dzungars in Outer Mongolia. The Kangxi Emperor was able to successfully expel Galdan's invading forces from these regions, which were then incorporated into the empire. Galdan was eventually killed in the Dzungar–Qing War. In 1683, Qing forces received the surrender of Taiwan from Zheng Keshuang, grandson of Koxinga, who had conquered Taiwan from the Dutch colonists as a base against the Qing. Zheng Keshuang was awarded the title "Duke Haicheng" (海澄公) and was inducted into the Han Chinese Plain Red Banner of the Eight Banners when he moved to Beijing. Several Ming princes had accompanied Koxinga to Taiwan in 1661-1662, including the Prince of Ningjing Zhu Shugui and Prince Zhu Honghuan (朱弘桓), son of Zhu Yihai, where they lived in the Kingdom of Tungning. The Qing sent the 17 Ming princes still living on Taiwan in 1683 back to mainland China where they spent the rest of their lives in exile since their lives were spared from execution. Winning Taiwan freed Kangxi's forces for series of battles over Albazin, the far eastern outpost of the Tsardom of Russia. Zheng's former soldiers on Taiwan like the rattan shield troops were also inducted into the Eight Banners and used by the Qing against Russian Cossacks at Albazin. The 1689 Treaty of Nerchinsk was China's first formal treaty with a European power and kept the border peaceful for the better part of two centuries. After Galdan's death, his followers, as adherents to Tibetan Buddhism, attempted to control the choice of the next Dalai Lama. Kangxi dispatched two armies to Lhasa, the capital of Tibet, and installed a Dalai Lama sympathetic to the Qing.
After the Kangxi Emperor's death in the winter of 1722, his fourth son, Prince Yong (雍親王), became the Yongzheng Emperor. In the later years of Kangxi's reign, Yongzheng and his brothers had fought, and there were rumours that he had usurped the throne(most of the rumours believe that Yongzheng's brother Yingzhen (Kangxi's 14th son) is the real successor of Kangxi Emperor, the reason why Yingzhen failed to sit on the throne is because Yongzheng and his confidant Keduo Long tampered the content of Kangxi's testament at the night when Kangxi passed away), a charge for which there is little evidence. In fact, his father had trusted him with delicate political issues and discussed state policy with him. When Yongzheng came to power at the age of 45, he felt a sense of urgency about the problems which had accumulated in his father's later years and did not need instruction in how to exercise power. In the words of one recent historian, he was "severe, suspicious, and jealous, but extremely capable and resourceful," and in the words of another, turned out to be an "early modern state-maker of the first order."
He moved rapidly. First, he promoted Confucian orthodoxy and reversed what he saw as his father's laxness by cracking down on unorthodox sects and by decapitating an anti-Manchu writer his father had pardoned. In 1723 he outlawed Christianity and expelled Christian missionaries, though some were allowed to remain in the capital. Next, he moved to control the government. He expanded his father's system of Palace Memorials which brought frank and detailed reports on local conditions directly to the throne without being intercepted by the bureaucracy, and created a small Grand Council of personal advisors which eventually grew into the emperor's de facto cabinet for the rest of the dynasty. He shrewdly filled key positions with Manchu and Han Chinese officials who depended on his patronage. When he began to realize that the financial crisis was even greater than he had thought, Yongzheng rejected his father's lenient approach to local landowning elites and mounted a campaign to enforce collection of the land tax. The increased revenues were to be used for "money to nourish honesty" among local officials and for local irrigation, schools, roads, and charity. Although these reforms were effective in the north, in the south and lower Yangzi valley, where Kangxi had wooed the elites, there were long established networks of officials and landowners. Yongzheng dispatched experienced Manchu commissioners to penetrate the thickets of falsified land registers and coded account books, but they were met with tricks, passivity, and even violence. The fiscal crisis persisted.
In 1725 Yongzheng bestowed the hereditary title of Marquis on a descendant of the Ming dynasty Imperial family, Zhu Zhiliang, who received a salary from the Qing government and whose duty was to perform rituals at the Ming tombs, and was also inducted the Chinese Plain White Banner in the Eight Banners. Later the Qianlong Emperor bestowed the title Marquis of Extended Grace posthumously on Zhu Zhuliang in 1750, and the title passed on through twelve generations of Ming descendants until the end of the Qing dynasty.
Yongzheng also inherited diplomatic and strategic problems. A team made up entirely of Manchus drew up the Treaty of Kyakhta (1727) to solidify the diplomatic understanding with Russia. In exchange for territory and trading rights, the Qing would have a free hand dealing with the situation in Mongolia. Yongzheng then turned to that situation, where the Zunghars threatened to re-emerge, and to the southwest, where local Miao chieftains resisted Qing expansion. These campaigns drained the treasury but established the emperor's control of the military and military finance.
Qianlong's reign saw the launch of several ambitious cultural projects, including the compilation of the Siku Quanshu, or Complete Repository of the Four Branches of Literature. With a total of over 3,400 books, 79,000 chapters, and 36,304 volumes, the Siku Quanshu is the largest collection of books in Chinese history. Nevertheless, Qianlong used Literary Inquisition to silence opposition. The accusation of individuals began with the emperor's own interpretation of the true meaning of the corresponding words. If the emperor decided these were derogatory or cynical towards the dynasty, persecution would begin. Literary inquisition began with isolated cases at the time of Shunzhi and Kangxi, but became a pattern under Qianlong's rule, during which there were 53 cases of literary persecution.
China also began suffering from mounting overpopulation during this period. Population growth was stagnant for the first half of the 17th century due to civil wars and epidemics, but prosperity and internal stability gradually reversed this trend. The introduction of new crops from the Americas such as the potato and peanut allowed an improved food supply as well, so that the total population of China during the 18th century ballooned from 100 million to 300 million people. Soon all available farmland was used up, forcing peasants to work ever-smaller and more intensely worked plots. The Qianlong Emperor once bemoaned the country's situation by remarking "The population continues to grow, but the land does not." The only remaining part of the empire that had arable farmland was Manchuria, where the provinces of Jilin and Heilongjiang had been walled off as a Manchu homeland. The emperor decreed for the first time that Han Chinese civilians were forbidden to settle. Mongols were forbidden by the Qing from crossing the borders of their banners, even into other Mongol Banners and from crossing into neidi (the Han Chinese 18 provinces) and were given serious punishments if they did in order to keep the Mongols divided against each other to benefit the Qing.
However Qing rule saw an massively increasing amount of Han Chinese both illegally and legally streaming into Manchuria and settling down to cultivate land as Manchu landlords desired Han Chinese peasants to rent on their land and grow grain, most Han Chinese migrants were not evicted as they went over the Great Wall and Willow Palisade, during the eighteenth century Han Chinese farmed 500,000 hectares of privately owned land in Manchuria and 203,583 hectares of lands which were part of coutrier stations, noble estates, and Banner lands, in garrisons and towns in Manchuria Han Chinese made up 80% of the population.
Han Chinese farmers were resettled from north China by the Qing to the area along the Liao River in order to restore the land to cultivation. Wasteland was reclaimed by Han Chinese squatters in addition to other Han who rented land from Manchu landlords. Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia so that Han Chinese farmed 500,000 hectares in Manchuria and tens of thousands of hectares in Inner Mongolia by the 1780s. Qianlong allowed Han Chinese peasants suffering from drought to move into Manchuria despite him issuing edicts in favor of banning them from 1740–1776. Chinese tenant farmers rented or even claimed title to land from the "imperial estates" and Manchu Bannerlands in the area. Besides moving into the Liao area in southern Manchuria, the path linking Jinzhou, Fengtian, Tieling, Changchun, Hulun, and Ningguta was settled by Han Chinese during the Qianlong Emperor's rule, and Han Chinese were the majority in urban areas of Manchuria by 1800. To increase the Imperial Treasury's revenue, the Qing sold formerly Manchu only lands along the Sungari to Han Chinese at the beginning of the Daoguang Emperor's reign, and Han Chinese filled up most of Manchuria's towns by the 1840s according to Abbe Huc.
However, the 18th century saw the European empires gradually expand across the world, as European states developed economies built on maritime trade. The dynasty was confronted with newly developing concepts of the international system and state to state relations. European trading posts expanded into territorial control in nearby India and on the islands that are now Indonesia. The Qing response, successful for a time, was in 1756 to establish the Canton System, which restricted maritime trade to that city and gave monopoly trading rights to private Chinese merchants. The British East India Company and the Dutch East India Company had long before been granted similar monopoly rights by their governments.
Demand in Europe for Chinese goods such as silk, tea, and ceramics could only be met if European companies funneled their limited supplies of silver into China. In the late 1700s, the governments of Britain and France were deeply concerned about the imbalance of trade and the drain of silver. To meet the growing Chinese demand for opium, the British East India Company greatly expanded its production in Bengal. Since China's economy was essentially self-sufficient, the country had little need to import goods or raw materials from the Europeans, so the usual way of payment was through silver. The Daoguang Emperor, concerned both over the outflow of silver and the damage that opium smoking was causing to his subjects, ordered Lin Zexu to end the opium trade. Lin confiscated the stocks of opium without compensation in 1839, leading Britain to send a military expedition the following year.
The First Opium War revealed the outdated state of the Chinese military. The Qing navy, composed entirely of wooden sailing junks, was severely outclassed by the modern tactics and firepower of the British Royal Navy. British soldiers, using advanced muskets and artillery, easily outmaneuvered and outgunned Qing forces in ground battles. The Qing surrender in 1842 marked a decisive, humiliating blow to China. The Treaty of Nanjing, the first of the unequal treaties, demanded war reparations, forced China to open up the five ports of Canton, Amoy, Fuchow, Ningpo and Shanghai to western trade and missionaries, and to cede Hong Kong Island to Britain. It revealed many inadequacies in the Qing government and provoked widespread rebellions against the already hugely unpopular regime.
The Taiping Rebellion in the mid-19th century was the first major instance of anti-Manchu sentiment threatening the stability of the dynasty. Hong Xiuquan, a failed civil service candidate, led the Taiping Rebellion, amid widespread social unrest and worsening famine. In 1851 Hong Xiuquan and others launched an uprising in Guizhou province, established the Taiping Heavenly Kingdom with Hong himself as king, claiming he often had visions of God and that he was the brother of Jesus Christ. Slavery, concubinage, arranged marriage, opium smoking, footbinding, judicial torture, and the worship of idols were all banned. However, success and subsequent authority and power led to internal feuds, defections and corruption. In addition, British and French troops, equipped with modern weapons, had come to the assistance of the Qing imperial army. It was not until 1864 that Qing armies under Zeng Guofan succeeded in crushing the revolt. The rebellion not only posed the most serious threat towards Qing rulers; it was also "bloodiest civil war of all time." Between 20 and 30 million people died during its fourteen-year course from 1850 to 1864. After the outbreak of this rebellion, there were also revolts by the Muslims and Miao people of China against the Qing dynasty, most notably in the Dungan Revolt (1862–77) in the northwest and the Panthay Rebellion (1856–1873) in Yunnan.
The Western powers, largely unsatisfied with the Treaty of Nanjing, gave grudging support to the Qing government during the Taiping and Nian Rebellions. China's income fell sharply during the wars as vast areas of farmland were destroyed, millions of lives were lost, and countless armies were raised and equipped to fight the rebels. In 1854, Britain tried to re-negotiate the Treaty of Nanjing, inserting clauses allowing British commercial access to Chinese rivers and the creation of a permanent British embassy at Beijing.
Ratification of the treaty the following year led to resumption of hostilities and in 1860, with Anglo-French forces marching on Beijing, the emperor and his court fled the capital for the imperial hunting lodge at Rehe. Once in Beijing, the Anglo-French forces looted the Old Summer Palace, and in an act of revenge for the arrest of several Englishmen, burnt it to the ground. Prince Gong, a younger half-brother of the emperor, who had been left as his brother's proxy in the capital, was forced to sign the Convention of Beijing. Meanwhile, the humiliated emperor died the following year at Rehe.
Chinese generals and officials such as Zuo Zongtang led the suppression of rebellions and stood behind the Manchus. When the Tongzhi Emperor came to the throne at the age of five in 1861, these officials rallied around him in what was called the Tongzhi Restoration. Their aim was to adopt western military technology in order to preserve Confucian values. Zeng Guofan, in alliance with Prince Gong, sponsored the rise of younger officials such as Li Hongzhang, who put the dynasty back on its feet financially and instituted the Self-Strengthening Movement. The reformers then proceeded with institutional reforms, including China's first unified ministry of foreign affairs, the Zongli Yamen; allowing foreign diplomats to reside in the capital; establishment of the Imperial Maritime Customs Service; the formation of modernized armies, such as the Beiyang Army, as well as a navy; and the purchase from Europeans of armament factories. 
The dynasty lost control of peripheral territories bit by bit. In return for promises of support against the British and the French, the Russian Empire took large chunks of territory in the Northeast in 1860. The period of cooperation between the reformers and the European powers ended with the Tientsin Massacre of 1870, which was incited by the murder of French nuns set off by the belligerence of local French diplomats. Starting with the Cochinchina Campaign in 1858, France expanded control of Indochina. By 1883, France was in full control of the region and had reached the Chinese border. The Sino-French War began with a surprise attack by the French on the Chinese southern fleet at Fuzhou. After that the Chinese declared war on the French. A French invasion of Taiwan was halted and the French were defeated on land in Tonkin at the Battle of Bang Bo. However Japan threatened to enter the war against China due to the Gapsin Coup and China chose to end the war with negotiations. The war ended in 1885 with the Treaty of Tientsin (1885) and the Chinese recognition of the French protectorate in Vietnam.
Historians have judged the Qing dynasty's vulnerability and weakness to foreign imperialism in the 19th century to be based mainly on its maritime naval weakness while it achieved military success against westerners on land, the historian Edward L. Dreyer said that "China’s nineteenth-century humiliations were strongly related to her weakness and failure at sea. At the start of the Opium War, China had no unified navy and no sense of how vulnerable she was to attack from the sea; British forces sailed and steamed wherever they wanted to go......In the Arrow War (1856–60), the Chinese had no way to prevent the Anglo-French expedition of 1860 from sailing into the Gulf of Zhili and landing as near as possible to Beijing. Meanwhile, new but not exactly modern Chinese armies suppressed the midcentury rebellions, bluffed Russia into a peaceful settlement of disputed frontiers in Central Asia, and defeated the French forces on land in the Sino-French War (1884–85). But the defeat of the fleet, and the resulting threat to steamship traffic to Taiwan, forced China to conclude peace on unfavorable terms."
In 1884, pro-Japanese Koreans in Seoul led the Gapsin Coup. Tensions between China and Japan rose after China intervened to suppress the uprising. Japanese Prime Minister Itō Hirobumi and Li Hongzhang signed the Convention of Tientsin, an agreement to withdraw troops simultaneously, but the First Sino-Japanese War of 1895 was a military humiliation. The Treaty of Shimonoseki recognized Korean independence and ceded Taiwan and the Pescadores to Japan. The terms might have been harsher, but when Japanese citizen attacked and wounded Li Hongzhang, an international outcry shamed the Japanese into revising them. The original agreement stipulated the cession of Liaodong Peninsula to Japan, but Russia, with its own designs on the territory, along with Germany and France, in what was known as the Triple Intervention, successfully put pressure on the Japanese to abandon the peninsula.
These years saw an evolution in the participation of Empress Dowager Cixi (Wade–Giles: Tz'u-Hsi) in state affairs. She entered the imperial palace in the 1850s as a concubine to the Xianfeng Emperor (r. 1850–1861) and came to power in 1861 after her five-year-old son, the Tongzhi Emperor ascended the throne. She, the Empress Dowager Ci'an (who had been Xianfeng's empress), and Prince Gong (a son of the Daoguang Emperor), staged a coup that ousted several regents for the boy emperor. Between 1861 and 1873, she and Ci'an served as regents, choosing the reign title "Tongzhi" (ruling together). Following the emperor's death in 1875, Cixi's nephew, the Guangxu Emperor, took the throne, in violation of the dynastic custom that the new emperor be of the next generation, and another regency began. In the spring of 1881, Ci'an suddenly died, aged only forty-three, leaving Cixi as sole regent. 
From 1889, when Guangxu began to rule in his own right, to 1898, the Empress Dowager lived in semi-retirement, spending the majority of the year at the Summer Palace. On November 1, 1897, two German Roman Catholic missionaries were murdered in the southern part of Shandong Province (the Juye Incident). In response, Germany used the murders as a pretext for a naval occupation of Jiaozhou Bay. The occupation prompted a "scramble for concessions" in 1898, which included the German lease of Jiazhou Bay, the Russian acquisition of Liaodong, and the British lease of the New Territories of Hong Kong.
In the wake of these external defeats, the Guangxu Emperor initiated the Hundred Days' Reform of 1898. Newer, more radical advisers such as Kang Youwei were given positions of influence. The emperor issued a series of edicts and plans were made to reorganize the bureaucracy, restructure the school system, and appoint new officials. Opposition from the bureaucracy was immediate and intense. Although she had been involved in the initial reforms, the empress dowager stepped in to call them off, arrested and executed several reformers, and took over day-to-day control of policy. Yet many of the plans stayed in place, and the goals of reform were implanted.
Widespread drought in North China, combined with the imperialist designs of European powers and the instability of the Qing government, created conditions that led to the emergence of the Righteous and Harmonious Fists, or "Boxers." In 1900, local groups of Boxers proclaiming support for the Qing dynasty murdered foreign missionaries and large numbers of Chinese Christians, then converged on Beijing to besiege the Foreign Legation Quarter. A coalition of European, Japanese, and Russian armies (the Eight-Nation Alliance) then entered China without diplomatic notice, much less permission. Cixi declared war on all of these nations, only to lose control of Beijing after a short, but hard-fought campaign. She fled to Xi'an. The victorious allies drew up scores of demands on the Qing government, including compensation for their expenses in invading China and execution of complicit officials.
By the early 20th century, mass civil disorder had begun in China, and it was growing continuously. To overcome such problems, Empress Dowager Cixi issued an imperial edict in 1901 calling for reform proposals from the governors-general and governors and initiated the era of the dynasty's "New Policies", also known as the "Late Qing Reform". The edict paved the way for the most far-reaching reforms in terms of their social consequences, including the creation of a national education system and the abolition of the imperial examinations in 1905.
The Guangxu Emperor died on November 14, 1908, and on November 15, 1908, Cixi also died. Rumors held that she or Yuan Shikai ordered trusted eunuchs to poison the Guangxu Emperor, and an autopsy conducted nearly a century later confirmed lethal levels of arsenic in his corpse. Puyi, the oldest son of Zaifeng, Prince Chun, and nephew to the childless Guangxu Emperor, was appointed successor at the age of two, leaving Zaifeng with the regency. This was followed by the dismissal of General Yuan Shikai from his former positions of power. In April 1911 Zaifeng created a cabinet in which there were two vice-premiers. Nonetheless, this cabinet was also known by contemporaries as "The Royal Cabinet" because among the thirteen cabinet members, five were members of the imperial family or Aisin Gioro relatives. This brought a wide range of negative opinions from senior officials like Zhang Zhidong. The Wuchang Uprising of October 10, 1911, led to the creation of a new central government, the Republic of China, in Nanjing with Sun Yat-sen as its provisional head. Many provinces soon began "separating" from Qing control. Seeing a desperate situation unfold, the Qing government brought Yuan Shikai back to military power. He took control of his Beiyang Army to crush the revolution in Wuhan at the Battle of Yangxia. After taking the position of Prime Minister and creating his own cabinet, Yuan Shikai went as far as to ask for the removal of Zaifeng from the regency. This removal later proceeded with directions from Empress Dowager Longyu.
With Zaifeng gone, Yuan Shikai and his Beiyang commanders effectively dominated Qing politics. He reasoned that going to war would be unreasonable and costly, especially when noting that the Qing government had a goal for constitutional monarchy. Similarly, Sun Yat-sen's government wanted a republican constitutional reform, both aiming for the benefit of China's economy and populace. With permission from Empress Dowager Longyu, Yuan Shikai began negotiating with Sun Yat-sen, who decided that his goal had been achieved in forming a republic, and that therefore he could allow Yuan to step into the position of President of the Republic of China.
On 12 February 1912, after rounds of negotiations, Longyu issued an imperial edict bringing about the abdication of the child emperor Puyi. This brought an end to over 2,000 years of Imperial China and began an extended period of instability of warlord factionalism. The unorganized political and economic systems combined with a widespread criticism of Chinese culture led to questioning and doubt about the future. In the 1930s, the Empire of Japan invaded Northeast China and founded Manchukuo in 1932, with Puyi, as the emperor. After the invasion by the Soviet Union, Manchukuo collapsed in 1945.
The early Qing emperors adopted the bureaucratic structures and institutions from the preceding Ming dynasty but split rule between Han Chinese and Manchus, with some positions also given to Mongols. Like previous dynasties, the Qing recruited officials via the imperial examination system, until the system was abolished in 1905. The Qing divided the positions into civil and military positions, each having nine grades or ranks, each subdivided into a and b categories. Civil appointments ranged from attendant to the emperor or a Grand Secretary in the Forbidden City (highest) to being a prefectural tax collector, deputy jail warden, deputy police commissioner or tax examiner. Military appointments ranged from being a field marshal or chamberlain of the imperial bodyguard to a third class sergeant, corporal or a first or second class private.
The formal structure of the Qing government centered on the Emperor as the absolute ruler, who presided over six Boards (Ministries[c]), each headed by two presidents[d] and assisted by four vice presidents.[e] In contrast to the Ming system, however, Qing ethnic policy dictated that appointments were split between Manchu noblemen and Han officials who had passed the highest levels of the state examinations. The Grand Secretariat,[f] which had been an important policy-making body under the Ming, lost its importance during the Qing and evolved into an imperial chancery. The institutions which had been inherited from the Ming formed the core of the Qing "Outer Court," which handled routine matters and was located in the southern part of the Forbidden City.
In order not to let the routine administration take over the running of the empire, the Qing emperors made sure that all important matters were decided in the "Inner Court," which was dominated by the imperial family and Manchu nobility and which was located in the northern part of the Forbidden City. The core institution of the inner court was the Grand Council.[g] It emerged in the 1720s under the reign of the Yongzheng Emperor as a body charged with handling Qing military campaigns against the Mongols, but it soon took over other military and administrative duties and served to centralize authority under the crown. The Grand Councillors[h] served as a sort of privy council to the emperor.
From the early Qing, the central government was characterized by a system of dual appointments by which each position in the central government had a Manchu and a Han Chinese assigned to it. The Han Chinese appointee was required to do the substantive work and the Manchu to ensure Han loyalty to Qing rule. The distinction between Han Chinese and Manchus extended to their court costumes. During the Qianlong Emperor's reign, for example, members of his family were distinguished by garments with a small circular emblem on the back, whereas Han officials wore clothing with a square emblem.
In addition to the six boards, there was a Lifan Yuan unique to the Qing government. This institution was established to supervise the administration of Tibet and the Mongol lands. As the empire expanded, it took over administrative responsibility of all minority ethnic groups living in and around the empire, including early contacts with Russia — then seen as a tribute nation. The office had the status of a full ministry and was headed by officials of equal rank. However, appointees were at first restricted only to candidates of Manchu and Mongol ethnicity, until later open to Han Chinese as well.
Even though the Board of Rites and Lifan Yuan performed some duties of a foreign office, they fell short of developing into a professional foreign service. It was not until 1861 — a year after losing the Second Opium War to the Anglo-French coalition — that the Qing government bowed to foreign pressure and created a proper foreign affairs office known as the Zongli Yamen. The office was originally intended to be temporary and was staffed by officials seconded from the Grand Council. However, as dealings with foreigners became increasingly complicated and frequent, the office grew in size and importance, aided by revenue from customs duties which came under its direct jurisdiction.
There was also another government institution called Imperial Household Department which was unique to the Qing dynasty. It was established before the fall of the Ming, but it became mature only after 1661, following the death of the Shunzhi Emperor and the accession of his son, the Kangxi Emperor. The department's original purpose was to manage the internal affairs of the imperial family and the activities of the inner palace (in which tasks it largely replaced eunuchs), but it also played an important role in Qing relations with Tibet and Mongolia, engaged in trading activities (jade, ginseng, salt, furs, etc.), managed textile factories in the Jiangnan region, and even published books. Relations with the Salt Superintendents and salt merchants, such as those at Yangzhou, were particularly lucrative, especially since they were direct, and did not go through absorptive layers of bureaucracy. The department was manned by booi,[o] or "bondservants," from the Upper Three Banners. By the 19th century, it managed the activities of at least 56 subagencies.
Qing China reached its largest extent during the 18th century, when it ruled China proper (eighteen provinces) as well as the areas of present-day Northeast China, Inner Mongolia, Outer Mongolia, Xinjiang and Tibet, at approximately 13 million km2 in size. There were originally 18 provinces, all of which in China proper, but later this number was increased to 22, with Manchuria and Xinjiang being divided or turned into provinces. Taiwan, originally part of Fujian province, became a province of its own in the late 19th century, but was ceded to the Empire of Japan in 1895 following the First Sino-Japanese War. In addition, many surrounding countries, such as Korea (Joseon dynasty), Vietnam frequently paid tribute to China during much of this period. Khanate of Kokand were forced to submit as protectorate and pay tribute to the Qing dynasty in China between 1774 and 1798.
The Qing organization of provinces was based on the fifteen administrative units set up by the Ming dynasty, later made into eighteen provinces by splitting for example, Huguang into Hubei and Hunan provinces. The provincial bureaucracy continued the Yuan and Ming practice of three parallel lines, civil, military, and censorate, or surveillance. Each province was administered by a governor (巡撫, xunfu) and a provincial military commander (提督, tidu). Below the province were prefectures (府, fu) operating under a prefect (知府, zhīfǔ), followed by subprefectures under a subprefect. The lowest unit was the county, overseen by a county magistrate. The eighteen provinces are also known as "China proper". The position of viceroy or governor-general (總督, zongdu) was the highest rank in the provincial administration. There were eight regional viceroys in China proper, each usually took charge of two or three provinces. The Viceroy of Zhili, who was responsible for the area surrounding the capital Beijing, is usually considered as the most honorable and powerful viceroy among the eight.
By the mid-18th century, the Qing had successfully put outer regions such as Inner and Outer Mongolia, Tibet and Xinjiang under its control. Imperial commissioners and garrisons were sent to Mongolia and Tibet to oversee their affairs. These territories were also under supervision of a central government institution called Lifan Yuan. Qinghai was also put under direct control of the Qing court. Xinjiang, also known as Chinese Turkestan, was subdivided into the regions north and south of the Tian Shan mountains, also known today as Dzungaria and Tarim Basin respectively, but the post of Ili General was established in 1762 to exercise unified military and administrative jurisdiction over both regions. Dzungaria was fully opened to Han migration by the Qianlong Emperor from the beginning. Han migrants were at first forbidden from permanently settling in the Tarim Basin but were the ban was lifted after the invasion by Jahangir Khoja in the 1820s. Likewise, Manchuria was also governed by military generals until its division into provinces, though some areas of Xinjiang and Northeast China were lost to the Russian Empire in the mid-19th century. Manchuria was originally separated from China proper by the Inner Willow Palisade, a ditch and embankment planted with willows intended to restrict the movement of the Han Chinese, as the area was off-limits to civilian Han Chinese until the government started colonizing the area, especially since the 1860s.
With respect to these outer regions, the Qing maintained imperial control, with the emperor acting as Mongol khan, patron of Tibetan Buddhism and protector of Muslims. However, Qing policy changed with the establishment of Xinjiang province in 1884. During The Great Game era, taking advantage of the Dungan revolt in northwest China, Yaqub Beg invaded Xinjiang from Central Asia with support from the British Empire, and made himself the ruler of the kingdom of Kashgaria. The Qing court sent forces to defeat Yaqub Beg and Xinjiang was reconquered, and then the political system of China proper was formally applied onto Xinjiang. The Kumul Khanate, which was incorporated into the Qing empire as a vassal after helping Qing defeat the Zunghars in 1757, maintained its status after Xinjiang turned into a province through the end of the dynasty in the Xinhai Revolution up until 1930. In early 20th century, Britain sent an expedition force to Tibet and forced Tibetans to sign a treaty. The Qing court responded by asserting Chinese sovereignty over Tibet, resulting in the 1906 Anglo-Chinese Convention signed between Britain and China. The British agreed not to annex Tibetan territory or to interfere in the administration of Tibet, while China engaged not to permit any other foreign state to interfere with the territory or internal administration of Tibet. Furthermore, similar to Xinjiang which was converted into a province earlier, the Qing government also turned Manchuria into three provinces in the early 20th century, officially known as the "Three Northeast Provinces", and established the post of Viceroy of the Three Northeast Provinces to oversee these provinces, making the total number of regional viceroys to nine.
The early Qing military was rooted in the Eight Banners first developed by Nurhaci to organize Jurchen society beyond petty clan affiliations. There were eight banners in all, differentiated by color. The yellow, bordered yellow, and white banners were known as the "Upper Three Banners" and were under the direct command of the emperor. Only Manchus belonging to the Upper Three Banners, and selected Han Chinese who had passed the highest level of martial exams could serve as the emperor's personal bodyguards. The remaining Banners were known as the "Lower Five Banners." They were commanded by hereditary Manchu princes descended from Nurhachi's immediate family, known informally as the "Iron cap princes". Together they formed the ruling council of the Manchu nation as well as high command of the army. Nurhachi's son Hong Taiji expanded the system to include mirrored Mongol and Han Banners. After capturing Beijing in 1644, the relatively small Banner armies were further augmented by the Green Standard Army, made up of those Ming troops who had surrendered to the Qing, which eventually outnumbered Banner troops three to one. They maintained their Ming era organization and were led by a mix of Banner and Green Standard officers.[citation needed]
Banner Armies were organized along ethnic lines, namely Manchu and Mongol, but included non-Manchu bondservants registered under the household of their Manchu masters. The years leading up to the conquest increased the number of Han Chinese under Manchu rule, leading Hong Taiji to create the Eight Han Banners (zh), and around the time of the Qing takeover of Beijing, their numbers rapidly swelled. Han Bannermen held high status and power in the early Qing period, especially immediately after the conquest during Shunzhi and Kangxi's reign where they dominated Governor-Generalships and Governorships across China at the expense of both Manchu Bannermen and Han civilians. Han also numerically dominated the Banners up until the mid 18th century. European visitors in Beijing called them "Tartarized Chinese" or "Tartarified Chinese". It was in Qianlong's reign that the Qianlong Emperor, concerned about maintaining Manchu identity, re-emphasized Manchu ethnicity, ancestry, language, and culture in the Eight Banners and started a mass discharge of Han Bannermen from the Eight Banners, either asking them to voluntarily resign from the Banner rolls or striking their names off. This led to a change from Han majority to a Manchu majority within the Banner system, and previous Han Bannermen garrisons in southern China such as at Fuzhou, Zhenjiang, Guangzhou, were replaced by Manchu Bannermen in the purge, which started in 1754. The turnover by Qianlong most heavily impacted Han banner garrisons stationed in the provinces while it less impacted Han Bannermen in Beijing, leaving a larger proportion of remaining Han Bannermen in Beijing than the provinces. Han Bannermen's status was decreased from that point on with Manchu Banners gaining higher status. Han Bannermen numbered 75% in 1648 Shunzhi's reign, 72% in 1723 Yongzheng's reign, but decreased to 43% in 1796 during the first year of Jiaqing's reign, which was after Qianlong's purge. The mass discharge was known as the Disbandment of the Han Banners (zh). Qianlong directed most of his ire at those Han Bannermen descended from defectors who joined the Qing after the Qing passed through the Great Wall at Shanhai Pass in 1644, deeming their ancestors as traitors to the Ming and therefore untrustworthy, while retaining Han Bannermen who were descended from defectors who joined the Qing before 1644 in Liaodong and marched through Shanhai pass, also known as those who "followed the Dragon through the pass" (從龍入關; cong long ru guan).
Early during the Taiping Rebellion, Qing forces suffered a series of disastrous defeats culminating in the loss of the regional capital city of Nanjing in 1853. Shortly thereafter, a Taiping expeditionary force penetrated as far north as the suburbs of Tianjin, the imperial heartlands. In desperation the Qing court ordered a Chinese official, Zeng Guofan, to organize regional and village militias into an emergency army called tuanlian. Zeng Guofan's strategy was to rely on local gentry to raise a new type of military organization from those provinces that the Taiping rebels directly threatened. This new force became known as the Xiang Army, named after the Hunan region where it was raised. The Xiang Army was a hybrid of local militia and a standing army. It was given professional training, but was paid for out of regional coffers and funds its commanders — mostly members of the Chinese gentry — could muster. The Xiang Army and its successor, the Huai Army, created by Zeng Guofan's colleague and mentee Li Hongzhang, were collectively called the "Yong Ying" (Brave Camp).
Zeng Guofan had no prior military experience. Being a classically educated official, he took his blueprint for the Xiang Army from the Ming general Qi Jiguang, who, because of the weakness of regular Ming troops, had decided to form his own "private" army to repel raiding Japanese pirates in the mid-16th century. Qi Jiguang's doctrine was based on Neo-Confucian ideas of binding troops' loyalty to their immediate superiors and also to the regions in which they were raised. Zeng Guofan's original intention for the Xiang Army was simply to eradicate the Taiping rebels. However, the success of the Yongying system led to its becoming a permanent regional force within the Qing military, which in the long run created problems for the beleaguered central government.
First, the Yongying system signaled the end of Manchu dominance in Qing military establishment. Although the Banners and Green Standard armies lingered on as a drain on resources, henceforth the Yongying corps became the Qing government's de facto first-line troops. Second, the Yongying corps were financed through provincial coffers and were led by regional commanders, weakening central government's grip on the whole country. Finally, the nature of Yongying command structure fostered nepotism and cronyism amongst its commanders, who laid the seeds of regional warlordism in the first half of the 20th century.
By the late 19th century, the most conservative elements within the Qing court could no longer ignore China's military weakness. In 1860, during the Second Opium War, the capital Beijing was captured and the Summer Palace sacked by a relatively small Anglo-French coalition force numbering 25,000. The advent of modern weaponry resulting from the European Industrial Revolution had rendered China's traditionally trained and equipped army and navy obsolete. The government attempts to modernize during the Self-Strengthening Movement were initially successful, but yielded few lasting results because of the central government's lack of funds, lack of political will, and unwillingness to depart from tradition.
Losing the First Sino-Japanese War of 1894–1895 was a watershed. Japan, a country long regarded by the Chinese as little more than an upstart nation of pirates, annihilated the Qing government's modernized Beiyang Fleet, then deemed to be the strongest naval force in Asia. The Japanese victory occurred a mere three decades after the Meiji Restoration set a feudal Japan on course to emulate the Western nations in their economic and technological achievements. Finally, in December 1894, the Qing government took concrete steps to reform military institutions and to re-train selected units in westernized drills, tactics and weaponry. These units were collectively called the New Army. The most successful of these was the Beiyang Army under the overall supervision and control of a former Huai Army commander, General Yuan Shikai, who used his position to build networks of loyal officers and eventually become President of the Republic of China.
The most significant fact of early and mid-Qing social history was population growth. The population doubled during the 18th century. People in this period were also remarkably on the move. There is evidence suggesting that the empire's rapidly expanding population was geographically mobile on a scale, which, in term of its volume and its protracted and routinized nature, was unprecedented in Chinese history. Indeed, the Qing government did far more to encourage mobility than to discourage it. Migration took several different forms, though might be divided in two varieties: permanent migration for resettlement, and relocation conceived by the party (in theory at least) as a temporary sojourn. Parties to the latter would include the empire's increasingly large and mobile manual workforce, as well as its densely overlapping internal diaspora of local-origin-based merchant groups. It would also included the patterned movement of Qing subjects overseas, largely to Southeastern Asia, in search of trade and other economic opportunities.
According to statute, Qing society was divided into relatively closed estates, of which in most general terms there were five. Apart from the estates of the officials, the comparatively minuscule aristocracy, and the degree-holding literati, there also existed a major division among ordinary Chinese between commoners and people with inferior status. They were divided into two categories: one of them, the good "commoner" people, the other "mean" people. The majority of the population belonged to the first category and were described as liangmin, a legal term meaning good people, as opposed to jianmin meaning the mean (or ignoble) people. Qing law explicitly stated that the traditional four occupational groups of scholars, farmers, artisans and merchants were "good", or having a status of commoners. On the other hand, slaves or bondservants, entertainers (including prostitutes and actors), and those low-level employees of government officials were the "mean people". Mean people were considered legally inferior to commoners and suffered unequal treatments, forbidden to take the imperial examination.
By the end of the 17th century, the Chinese economy had recovered from the devastation caused by the wars in which the Ming dynasty were overthrown, and the resulting breakdown of order. In the following century, markets continued to expand as in the late Ming period, but with more trade between regions, a greater dependence on overseas markets and a greatly increased population. After the re-opening of the southeast coast, which had been closed in the late 17th century, foreign trade was quickly re-established, and was expanding at 4% per annum throughout the latter part of the 18th century. China continued to export tea, silk and manufactures, creating a large, favorable trade balance with the West. The resulting inflow of silver expanded the money supply, facilitating the growth of competitive and stable markets.
The government broadened land ownership by returning land that had been sold to large landowners in the late Ming period by families unable to pay the land tax. To give people more incentives to participate in the market, they reduced the tax burden in comparison with the late Ming, and replaced the corvée system with a head tax used to hire laborers. The administration of the Grand Canal was made more efficient, and transport opened to private merchants. A system of monitoring grain prices eliminated severe shortages, and enabled the price of rice to rise slowly and smoothly through the 18th century. Wary of the power of wealthy merchants, Qing rulers limited their trading licenses and usually refused them permission to open new mines, except in poor areas. These restrictions on domestic resource exploration, as well as on foreign trade, are held by some scholars as a cause of the Great Divergence, by which the Western world overtook China economically.
By the end of the 18th century the population had risen to 300 million from approximately 150 million during the late Ming dynasty. The dramatic rise in population was due to several reasons, including the long period of peace and stability in the 18th century and the import of new crops China received from the Americas, including peanuts, sweet potatoes and maize. New species of rice from Southeast Asia led to a huge increase in production. Merchant guilds proliferated in all of the growing Chinese cities and often acquired great social and even political influence. Rich merchants with official connections built up huge fortunes and patronized literature, theater and the arts. Textile and handicraft production boomed.
The Qing emperors were generally adept at poetry and often skilled in painting, and offered their patronage to Confucian culture. The Kangxi and Qianlong Emperors, for instance, embraced Chinese traditions both to control them and to proclaim their own legitimacy. The Kangxi Emperor sponsored the Peiwen Yunfu, a rhyme dictionary published in 1711, and the Kangxi Dictionary published in 1716, which remains to this day an authoritative reference. The Qianlong Emperor sponsored the largest collection of writings in Chinese history, the Siku Quanshu, completed in 1782. Court painters made new versions of the Song masterpiece, Zhang Zeduan's Along the River During the Qingming Festival whose depiction of a prosperous and happy realm demonstrated the beneficence of the emperor. The emperors undertook tours of the south and commissioned monumental scrolls to depict the grandeur of the occasion. Imperial patronage also encouraged the industrial production of ceramics and Chinese export porcelain.
Yet the most impressive aesthetic works were done among the scholars and urban elite. Calligraphy and painting remained a central interest to both court painters and scholar-gentry who considered the Four Arts part of their cultural identity and social standing. The painting of the early years of the dynasty included such painters as the orthodox Four Wangs and the individualists Bada Shanren (1626–1705) and Shitao (1641–1707). The nineteenth century saw such innovations as the Shanghai School and the Lingnan School which used the technical skills of tradition to set the stage for modern painting.
Literature grew to new heights in the Qing period. Poetry continued as a mark of the cultivated gentleman, but women wrote in larger and larger numbers and poets came from all walks of life. The poetry of the Qing dynasty is a lively field of research, being studied (along with the poetry of the Ming dynasty) for its association with Chinese opera, developmental trends of Classical Chinese poetry, the transition to a greater role for vernacular language, and for poetry by women in Chinese culture. The Qing dynasty was a period of much literary collection and criticism, and many of the modern popular versions of Classical Chinese poems were transmitted through Qing dynasty anthologies, such as the Quantangshi and the Three Hundred Tang Poems. Pu Songling brought the short story form to a new level in his Strange Stories from a Chinese Studio, published in the mid-18th century, and Shen Fu demonstrated the charm of the informal memoir in Six Chapters of a Floating Life, written in the early 19th century but published only in 1877. The art of the novel reached a pinnacle in Cao Xueqin's Dream of the Red Chamber, but its combination of social commentary and psychological insight were echoed in highly skilled novels such as Wu Jingzi's The Scholars (1750) and Li Ruzhen's Flowers in the Mirror (1827).
Cuisine aroused a cultural pride in the accumulated richness of a long and varied past. The gentleman gourmet, such as Yuan Mei, applied aesthetic standards to the art of cooking, eating, and appreciation of tea at a time when New World crops and products entered everyday life. The Suiyuan Shidan written by him, detailed the culinary esthetics and theory, along with a wide range of recipes from the ruling period of Qianlong during Qing Dynasty. The Manchu Han Imperial Feast originated at the court. Although this banquet was probably never common, it reflected an appreciation by Han Chinese for Manchu culinary customs. Nevertheless, culinary traditionalists such as Yuan Mei lambasted the opulent culinary rituals of the Manchu Han Imperial Feast, saying that it is cause in part by "...the vulgar habits of bad chefs" and that "Display this trite are useful only for welcoming new relations through one’s gates or when the boss comes to visit." (皆惡廚陋習。只可用之於新親上門，上司入境)
The German states proclaimed their union as the German Empire under the Prussian king, Wilhelm I, uniting Germany as a nation-state. The Treaty of Frankfurt of 10 May 1871 gave Germany most of Alsace and some parts of Lorraine, which became the Imperial territory of Alsace-Lorraine (Reichsland Elsaß-Lothringen).The German conquest of France and the unification of Germany upset the European balance of power, that had existed since the Congress of Vienna in 1815 and Otto von Bismarck maintained great authority in international affairs for two decades. French determination to regain Alsace-Lorraine and fear of another Franco-German war, along with British apprehension about the balance of power, became factors in the causes of World War I.
The Ems telegram had exactly the effect on French public opinion that Bismarck had intended. "This text produced the effect of a red flag on the Gallic bull", Bismarck later wrote. Gramont, the French foreign minister, declared that he felt "he had just received a slap". The leader of the monarchists in Parliament, Adolphe Thiers, spoke for moderation, arguing that France had won the diplomatic battle and there was no reason for war, but he was drowned out by cries that he was a traitor and a Prussian. Napoleon's new prime minister, Emile Ollivier, declared that France had done all that it could humanly and honorably do to prevent the war, and that he accepted the responsibility "with a light heart." A crowd of 15–20,000 people, carrying flags and patriotic banners, marched through the streets of Paris, demanding war. On 19 July 1870 a declaration of war was sent to the Prussian government. The southern German states immediately sided with Prussia.
The army was still equipped with the Dreyse needle gun of Battle of Königgrätz fame, which was by this time showing the age of its 25-year-old design. The rifle had a range of only 600 m (2,000 ft) and lacked the rubber breech seal that permitted aimed shots. The deficiencies of the needle gun were more than compensated for by the famous Krupp 6-pounder (3 kg) steel breech-loading cannons being issued to Prussian artillery batteries. Firing a contact-detonated shell, the Krupp gun had a longer range and a higher rate of fire than the French bronze muzzle loading cannon, which relied on faulty time fuses.
The first action of the Franco-Prussian War took place on 4 August 1870. This battle saw the unsupported division of General Douay of I Corps, with some attached cavalry, which was posted to watch the border, attacked in overwhelming but uncoordinated fashion by the German 3rd Army. During the day, elements of a Bavarian and two Prussian corps became engaged and were aided by Prussian artillery, which blasted holes in the defenses of the town. Douay held a very strong position initially, thanks to the accurate long-range fire of the Chassepots but his force was too thinly stretched to hold it. Douay was killed in the late morning when a caisson of the divisional mitrailleuse battery exploded near him; the encirclement of the town by the Prussians threatened the French avenue of retreat.
The French were unaware of German numerical superiority at the beginning of the battle as the German 2nd Army did not attack all at once. Treating the oncoming attacks as merely skirmishes, Frossard did not request additional support from other units. By the time he realized what kind of a force he was opposing, it was too late. Seriously flawed communications between Frossard and those in reserve under Bazaine slowed down so much that by the time the reserves received orders to move out to Spicheren, German soldiers from the 1st and 2nd armies had charged up the heights. Because the reserves had not arrived, Frossard erroneously believed that he was in grave danger of being outflanked as German soldiers under General von Glume were spotted in Forbach. Instead of continuing to defend the heights, by the close of battle after dusk he retreated to the south. The German casualties were relatively high due to the advance and the effectiveness of the chassepot rifle. They were quite startled in the morning when they had found out that their efforts were not in vain—Frossard had abandoned his position on the heights.
The Battle of Gravelotte, or Gravelotte–St. Privat (18 August), was the largest battle during the Franco-Prussian War. It was fought about 6 miles (9.7 km) west of Metz, where on the previous day, having intercepted the French army's retreat to the west at the Battle of Mars-La-Tour, the Prussians were now closing in to complete the destruction of the French forces. The combined German forces, under Field Marshal Count Helmuth von Moltke, were the Prussian First and Second Armies of the North German Confederation numbering about 210 infantry battalions, 133 cavalry squadrons, and 732 heavy cannons totaling 188,332 officers and men. The French Army of the Rhine, commanded by Marshal François-Achille Bazaine, numbering about 183 infantry battalions, 104 cavalry squadrons, backed by 520 heavy cannons, totaling 112,800 officers and men, dug in along high ground with their southern left flank at the town of Rozerieulles, and their northern right flank at St. Privat.
With the defeat of Marshal Bazaine's Army of the Rhine at Gravelotte, the French were forced to retire to Metz, where they were besieged by over 150,000 Prussian troops of the First and Second Armies. Napoleon III and MacMahon formed the new French Army of Châlons, to march on to Metz to rescue Bazaine. Napoleon III personally led the army with Marshal MacMahon in attendance. The Army of Châlons marched northeast towards the Belgian border to avoid the Prussians before striking south to link up with Bazaine. The Prussians, under the command of Field Marshal Count Helmuth von Moltke, took advantage of this maneuver to catch the French in a pincer grip. He left the Prussian First and Second Armies besieging Metz, except three corps detached to form the Army of the Meuse under the Crown Prince of Saxony. With this army and the Prussian Third Army, Moltke marched northward and caught up with the French at Beaumont on 30 August. After a sharp fight in which they lost 5,000 men and 40 cannons, the French withdrew toward Sedan. Having reformed in the town, the Army of Châlons was immediately isolated by the converging Prussian armies. Napoleon III ordered the army to break out of the encirclement immediately. With MacMahon wounded on the previous day, General Auguste Ducrot took command of the French troops in the field.
When the war had begun, European public opinion heavily favored the Germans; many Italians attempted to sign up as volunteers at the Prussian embassy in Florence and a Prussian diplomat visited Giuseppe Garibaldi in Caprera. Bismarck's demand for the return of Alsace caused a dramatic shift in that sentiment in Italy, which was best exemplified by the reaction of Garibaldi soon after the revolution in Paris, who told the Movimento of Genoa on 7 September 1870 that "Yesterday I said to you: war to the death to Bonaparte. Today I say to you: rescue the French Republic by every means." Garibaldi went to France and assumed command of the Army of the Vosges, with which he operated around Dijon till the end of the war.
On 10 October, hostilities began between German and French republican forces near Orléans. At first, the Germans were victorious but the French drew reinforcements and defeated the Germans at the Battle of Coulmiers on 9 November. After the surrender of Metz, more than 100,000 well-trained and experienced German troops joined the German 'Southern Army'. The French were forced to abandon Orléans on 4 December, and were finally defeated at the Battle of Le Mans (10–12 January). A second French army which operated north of Paris was turned back at the Battle of Amiens (27 November), the Battle of Bapaume (3 January 1871) and the Battle of St. Quentin (13 January).
Although public opinion in Paris was strongly against any form of surrender or concession to the Prussians, the Government realised that it could not hold the city for much longer, and that Gambetta's provincial armies would probably never break through to relieve Paris. President Trochu resigned on 25 January and was replaced by Favre, who signed the surrender two days later at Versailles, with the armistice coming into effect at midnight. Several sources claim that in his carriage on the way back to Paris, Favre broke into tears, and collapsed into his daughter's arms as the guns around Paris fell silent at midnight. At Tours, Gambetta received word from Paris on 30 January that the Government had surrendered. Furious, he refused to surrender and launched an immediate attack on German forces at Orleans which, predictably, failed. A delegation of Parisian diplomats arrived in Tours by train on 5 February to negotiate with Gambetta, and the following day Gambetta stepped down and surrendered control of the provincial armies to the Government of National Defence, which promptly ordered a cease-fire across France.
The quick German victory over the French stunned neutral observers, many of whom had expected a French victory and most of whom had expected a long war. The strategic advantages possessed by the Germans were not appreciated outside Germany until after hostilities had ceased. Other countries quickly discerned the advantages given to the Germans by their military system, and adopted many of their innovations, particularly the General Staff, universal conscription and highly detailed mobilization systems.
The effect of these differences was accentuated by the pre-war preparations. The Prussian General Staff had drawn up minutely detailed mobilization plans using the railway system, which in turn had been partly laid out in response to recommendations of a Railway Section within the General Staff. The French railway system, with multiple competing companies, had developed purely from commercial pressures and many journeys to the front in Alsace and Lorraine involved long diversions and frequent changes between trains. Furthermore, no system had been put in place for military control of the railways, and officers simply commandeered trains as they saw fit. Rail sidings and marshalling yards became choked with loaded wagons, with nobody responsible for unloading them or directing them to the destination.
At the Battle of Mars-la-Tours, the Prussian 12th Cavalry Brigade, commanded by General Adalbert von Bredow, conducted a charge against a French artillery battery. The attack was a costly success and came to be known as "von Bredow's Death Ride", which was held to prove that cavalry charges could still prevail on the battlefield. Use of traditional cavalry on the battlefields of 1914 proved to be disastrous, due to accurate, long-range rifle fire, machine-guns and artillery. Von Bredow's attack had succeeded only because of an unusually effective artillery bombardment just before the charge, along with favorable terrain that masked his approach.
In the Prussian province of Posen, with a large Polish population, there was strong support for the French and angry demonstrations at news of Prussian-German victories—a clear manifestation of Polish nationalist feeling. Calls were also made for Polish recruits to desert from the Prussian Army—though these went mainly unheeded. An alarming report on the Posen situation, sent to Bismarck on 16 August 1870, led to the quartering of reserve troop contingents in the restive province. The Franco-Prussian War thus turned out to be a significant event also in German–Polish relations, marking the beginning of a prolonged period of repressive measures by the authorities and efforts at Germanisation.
The causes of the Franco-Prussian War are deeply rooted in the events surrounding the unification of Germany. In the aftermath of the Austro–Prussian War of 1866, Prussia had annexed numerous territories and formed the North German Confederation. This new power destabilized the European balance of power established by the Congress of Vienna in 1815 after the Napoleonic Wars. Napoleon III, then the emperor of France, demanded compensations in Belgium and on the left bank of the Rhine to secure France's strategic position, which the Prussian chancellor, Otto von Bismarck, flatly refused. Prussia then turned its attention towards the south of Germany, where it sought to incorporate the southern German kingdoms, Bavaria, Württemberg, Baden and Hesse-Darmstadt, into a unified Prussia-dominated Germany. France was strongly opposed to any further alliance of German states, which would have significantly strengthened the Prussian military.
The French Army consisted in peacetime of approximately 400,000 soldiers, some of them regulars, others conscripts who until 1869 served the comparatively long period of seven years with the colours. Some of them were veterans of previous French campaigns in the Crimean War, Algeria, the Franco-Austrian War in Italy, and in the Franco-Mexican War. However, following the "Seven Weeks War" between Prussia and Austria four years earlier, it had been calculated that the French Army could field only 288,000 men to face the Prussian Army when perhaps 1,000,000 would be required. Under Marshal Adolphe Niel, urgent reforms were made. Universal conscription (rather than by ballot, as previously) and a shorter period of service gave increased numbers of reservists, who would swell the army to a planned strength of 800,000 on mobilisation. Those who for any reason were not conscripted were to be enrolled in the Garde Mobile, a militia with a nominal strength of 400,000. However, the Franco-Prussian War broke out before these reforms could be completely implemented. The mobilisation of reservists was chaotic and resulted in large numbers of stragglers, while the Garde Mobile were generally untrained and often mutinous.
The Prussian army was controlled by the General Staff, under Field Marshal Helmuth von Moltke. The Prussian army was unique in Europe for having the only such organisation in existence, whose purpose in peacetime was to prepare the overall war strategy, and in wartime to direct operational movement and organise logistics and communications. The officers of the General Staff were hand-picked from the Prussian Kriegsakademie (War Academy). Moltke embraced new technology, particularly the railroad and telegraph, to coordinate and accelerate mobilisation of large forces.
General Frossard's II Corps and Marshal Bazaine's III Corps crossed the German border on 2 August, and began to force the Prussian 40th Regiment of the 16th Infantry Division from the town of Saarbrücken with a series of direct attacks. The Chassepot rifle proved its worth against the Dreyse rifle, with French riflemen regularly outdistancing their Prussian counterparts in the skirmishing around Saarbrücken. However the Prussians resisted strongly, and the French suffered 86 casualties to the Prussian 83 casualties. Saarbrücken also proved to be a major obstacle in terms of logistics. Only one railway there led to the German hinterland but could be easily defended by a single force, and the only river systems in the region ran along the border instead of inland. While the French hailed the invasion as the first step towards the Rhineland and later Berlin, General Le Bœuf and Napoleon III were receiving alarming reports from foreign news sources of Prussian and Bavarian armies massing to the southeast in addition to the forces to the north and northeast.
According to some historians, Bismarck adroitly created a diplomatic crisis over the succession to the Spanish throne, then edited a dispatch about a meeting between King William of Prussia and the French ambassador, to make it appear that the French had been insulted. The French press and parliament demanded a war, which the generals of Napoleon III assured him that France would win. Napoleon and his Prime Minister, Émile Ollivier, for their parts sought war to solve their problems with political disunity in France. On 16 July 1870, the French parliament voted to declare war on the German Kingdom of Prussia and hostilities began three days later. The German coalition mobilised its troops much more quickly than the French and rapidly invaded northeastern France. The German forces were superior in numbers, had better training and leadership and made more effective use of modern technology, particularly railroads and artillery.
The fighting within the town had become extremely intense, becoming a door to door battle of survival. Despite a never-ending attack of Prussian infantry, the soldiers of the 2nd Division kept to their positions. The people of the town of Wissembourg finally surrendered to the Germans. The French troops who did not surrender retreated westward, leaving behind 1,000 dead and wounded and another 1,000 prisoners and all of their remaining ammunition. The final attack by the Prussian troops also cost c. 1,000 casualties. The German cavalry then failed to pursue the French and lost touch with them. The attackers had an initial superiority of numbers, a broad deployment which made envelopment highly likely but the effectiveness of French Chassepot rifle-fire inflicted costly repulses on infantry attacks, until the French infantry had been extensively bombarded by the Prussian artillery.
The Franco-Prussian War or Franco-German War (German: Deutsch-Französischer Krieg, lit. German-French War, French: Guerre franco-allemande, lit. Franco-German War), often referred to in France as the War of 1870 (19 July 1870 – 10 May 1871), was a conflict between the Second French Empire and the German states of the North German Confederation led by the Kingdom of Prussia. The conflict was caused by Prussian ambitions to extend German unification. Some historians argue that the Prussian chancellor Otto von Bismarck planned to provoke a French attack in order to draw the southern German states—Baden, Württemberg, Bavaria and Hesse-Darmstadt—into an alliance with the North German Confederation dominated by Prussia, while others contend that Bismarck did not plan anything and merely exploited the circumstances as they unfolded.
The immediate cause of the war resided in the candidacy of a Leopold of Hohenzollern-Sigmaringen, a Prussian prince, to the throne of Spain. France feared encirclement by an alliance between Prussia and Spain. The Hohenzollern prince's candidacy was withdrawn under French diplomatic pressure, but Otto von Bismarck goaded the French into declaring war by altering a telegram sent by William I. Releasing the Ems Dispatch to the public, Bismarck made it sound as if the king had treated the French envoy in a demeaning fashion, which inflamed public opinion in France.
The Battle of Wörth (also known as Fröschwiller or Reichshoffen) began when the two armies clashed again on 6 August near Wörth in the town of Fröschwiller, about 10 miles (16 km) from Wissembourg. The Crown Prince of Prussia's 3rd army had, on the quick reaction of his Chief of Staff General von Blumenthal, drawn reinforcements which brought its strength up to 140,000 troops. The French had been slowly reinforced and their force numbered only 35,000. Although badly outnumbered, the French defended their position just outside Fröschwiller. By afternoon, the Germans had suffered c. 10,500 killed or wounded and the French had lost a similar number of casualties and another c. 9,200 men taken prisoner, a loss of about 50%. The Germans captured Fröschwiller which sat on a hilltop in the centre of the French line. Having lost any hope for victory and facing a massacre, the French army disengaged and retreated in a westerly direction towards Bitche and Saverne, hoping to join French forces on the other side of the Vosges mountains. The German 3rd army did not pursue the French but remained in Alsace and moved slowly south, attacking and destroying the French garrisons in the vicinity.
In Prussia, some officials considered a war against France both inevitable and necessary to arouse German nationalism in those states that would allow the unification of a great German empire. This aim was epitomized by Prussian Chancellor Otto von Bismarck's later statement: "I did not doubt that a Franco-German war must take place before the construction of a United Germany could be realised." Bismarck also knew that France should be the aggressor in the conflict to bring the southern German states to side with Prussia, hence giving Germans numerical superiority. Many Germans also viewed the French as the traditional destabilizer of Europe, and sought to weaken France to prevent further breaches of the peace.
On 18 August, the battle began when at 08:00 Moltke ordered the First and Second Armies to advance against the French positions. By 12:00, General Manstein opened up the battle before the village of Amanvillers with artillery from the 25th Infantry Division. But the French had spent the night and early morning digging trenches and rifle pits while placing their artillery and their mitrailleuses in concealed positions. Finally aware of the Prussian advance, the French opened up a massive return fire against the mass of advancing Germans. The battle at first appeared to favor the French with their superior Chassepot rifle. However, the Prussian artillery was superior with the all-steel Krupp breech-loading gun. By 14:30, General Steinmetz, the commander of the First Army, unilaterally launched his VIII Corps across the Mance Ravine in which the Prussian infantry were soon pinned down by murderous rifle and mitrailleuse fire from the French positions. At 15:00, the massed guns of the VII and VIII Corps opened fire to support the attack. But by 16:00, with the attack in danger of stalling, Steinmetz ordered the VII Corps forward, followed by the 1st Cavalry Division.
French infantry were equipped with the breech-loading Chassepot rifle, one of the most modern mass-produced firearms in the world at the time. With a rubber ring seal and a smaller bullet, the Chassepot had a maximum effective range of some 1,500 metres (4,900 ft) with a short reloading time. French tactics emphasised the defensive use of the Chassepot rifle in trench-warfare style fighting—the so-called feu de bataillon. The artillery was equipped with rifled, muzzle-loaded La Hitte guns. The army also possessed a precursor to the machine-gun: the mitrailleuse, which could unleash significant, concentrated firepower but nevertheless lacked range and was comparatively immobile, and thus prone to being easily overrun. The mitrailleuse was mounted on an artillery gun carriage and grouped in batteries in a similar fashion to cannon.
On 1 September 1870, the battle opened with the Army of Châlons, with 202 infantry battalions, 80 cavalry squadrons and 564 guns, attacking the surrounding Prussian Third and Meuse Armies totaling 222 infantry battalions, 186 cavalry squadrons and 774 guns. General De Wimpffen, the commander of the French V Corps in reserve, hoped to launch a combined infantry and cavalry attack against the Prussian XI Corps. But by 11:00, Prussian artillery took a toll on the French while more Prussian troops arrived on the battlefield. The French cavalry, commanded by General Marguerite, launched three desperate attacks on the nearby village of Floing where the Prussian XI Corps was concentrated. Marguerite was killed leading the very first charge and the two additional charges led to nothing but heavy losses. By the end of the day, with no hope of breaking out, Napoleon III called off the attacks. The French lost over 17,000 men, killed or wounded, with 21,000 captured. The Prussians reported their losses at 2,320 killed, 5,980 wounded and 700 captured or missing. By the next day, on 2 September, Napoleon III surrendered and was taken prisoner with 104,000 of his soldiers. It was an overwhelming victory for the Prussians, for they not only captured an entire French army, but the leader of France as well. The defeat of the French at Sedan had decided the war in Prussia's favour. One French army was now immobilised and besieged in the city of Metz, and no other forces stood on French ground to prevent a German invasion. Nevertheless, the war would continue.
A pre-war plan laid out by the late Marshal Niel called for a strong French offensive from Thionville towards Trier and into the Prussian Rhineland. This plan was discarded in favour of a defensive plan by Generals Charles Frossard and Bartélemy Lebrun, which called for the Army of the Rhine to remain in a defensive posture near the German border and repel any Prussian offensive. As Austria along with Bavaria, Württemberg and Baden were expected to join in a revenge war against Prussia, I Corps would invade the Bavarian Palatinate and proceed to "free" the South German states in concert with Austro-Hungarian forces. VI Corps would reinforce either army as needed.
On the French side, planning after the disaster at Wissembourg had become essential. General Le Bœuf, flushed with anger, was intent upon going on the offensive over the Saar and countering their loss. However, planning for the next encounter was more based upon the reality of unfolding events rather than emotion or pride, as Intendant General Wolff told him and his staff that supply beyond the Saar would be impossible. Therefore, the armies of France would take up a defensive position that would protect against every possible attack point, but also left the armies unable to support each other.
Following the Army of the Loire's defeats, Gambetta turned to General Faidherbe's Army of the North. The army had achieved several small victories at towns such as Ham, La Hallue, and Amiens and was protected by the belt of fortresses in northern France, allowing Faidherbe's men to launch quick attacks against isolated Prussian units, then retreat behind the fortresses. Despite access to the armaments factories of Lille, the Army of the North suffered from severe supply difficulties, which depressed morale. In January 1871, Gambetta forced Faidherbe to march his army beyond the fortresses and engage the Prussians in open battle. The army was severely weakened by low morale, supply problems, the terrible winter weather and low troop quality, whilst general Faidherbe was unable to command due to his poor health, the result of decades of campaigning in West Africa. At the Battle of St. Quentin, the Army of the North suffered a crushing defeat and was scattered, releasing thousands of Prussian soldiers to be relocated to the East.
Despite odds of four to one, the III Corps launched a risky attack. The French were routed and the III Corps captured Vionville, blocking any further escape attempts to the west. Once blocked from retreat, the French in the fortress of Metz had no choice but to engage in a fight that would see the last major cavalry engagement in Western Europe. The battle soon erupted, and III Corps was shattered by incessant cavalry charges, losing over half its soldiers. The German Official History recorded 15,780 casualties and French casualties of 13,761 men.
With the defeat of the First Army, Prince Frederick Charles ordered a massed artillery attack against Canrobert's position at St. Privat to prevent the Guards attack from failing too. At 19:00 the 3rd Division of Fransecky's II Corps of the Second Army advanced across Ravine while the XII Corps cleared out the nearby town of Roncourt and with the survivors of the 1st Guards Infantry Division launched a fresh attack against the ruins of St. Privat. At 20:00, the arrival of the Prussian 4th Infantry Division of the II Corps and with the Prussian right flank on Mance Ravine, the line stabilised. By then, the Prussians of the 1st Guards Infantry Division and the XII and II Corps captured St. Privat forcing the decimated French forces to withdraw. With the Prussians exhausted from the fighting, the French were now able to mount a counter-attack. General Bourbaki, however, refused to commit the reserves of the French Old Guard to the battle because, by that time, he considered the overall situation a 'defeat'. By 22:00, firing largely died down across the battlefield for the night. The next morning, the French Army of the Rhine, rather than resume the battle with an attack of its own against the battle-weary German armies, retreated to Metz where they were besieged and forced to surrender two months later.
When the war began, the French government ordered a blockade of the North German coasts, which the small North German navy (Norddeutsche Bundesmarine) with only five ironclads could do little to oppose. For most of the war, the three largest German ironclads were out of service with engine troubles; only the turret ship SMS Arminius was available to conduct operations. By the time engine repairs had been completed, the French fleet had already departed. The blockade proved only partially successful due to crucial oversights by the planners in Paris. Reservists that were supposed to be at the ready in case of war, were working in the Newfoundland fisheries or in Scotland. Only part of the 470-ship French Navy put to sea on 24 July. Before long, the French navy ran short of coal, needing 200 short tons (180 t) per day and having a bunker capacity in the fleet of only 250 short tons (230 t). A blockade of Wilhelmshaven failed and conflicting orders about operations in the Baltic Sea or a return to France, made the French naval efforts futile. Spotting a blockade-runner became unwelcome because of the question du charbon; pursuit of Prussian ships quickly depleted the coal reserves of the French ships.
The Battle of Spicheren, on 5 August, was the second of three critical French defeats. Moltke had originally planned to keep Bazaine's army on the Saar River until he could attack it with the 2nd Army in front and the 1st Army on its left flank, while the 3rd Army closed towards the rear. The aging General von Steinmetz made an overzealous, unplanned move, leading the 1st Army south from his position on the Moselle. He moved straight toward the town of Spicheren, cutting off Prince Frederick Charles from his forward cavalry units in the process.
The Prussian General Staff developed by Moltke proved to be extremely effective, in contrast to the traditional French school. This was in large part due to the fact that the Prussian General Staff was created to study previous Prussian operations and learn to avoid mistakes. The structure also greatly strengthened Moltke's ability to control large formations spread out over significant distances. The Chief of the General Staff, effectively the commander in chief of the Prussian army, was independent of the minister of war and answered only to the monarch. The French General Staff—along with those of every other European military—was little better than a collection of assistants for the line commanders. This disorganization hampered the French commanders' ability to exercise control of their forces.
The Germans expected to negotiate an end to the war but immediately ordered an advance on Paris; by 15 September Moltke issued the orders for an investment of Paris and on 20 September the encirclement was complete. Bismarck met Favre on 18 September at the Château de Ferrières and demanded a frontier immune to a French war of revenge, which included Strasbourg, Alsace and most the Moselle department in Lorraine of which Metz was the capital. In return for an armistice for the French to elect a National Assembly, Bismarck demanded the surrender of Strasbourg and the fortress city of Toul. To allow supplies into Paris, one of the perimeter forts had to be handed over. Favre was unaware that the real aim of Bismarck in making such extortionate demands was to establish a durable peace on the new western frontier of Germany, preferably by a peace with a friendly government, on terms acceptable to French public opinion. An impregnable military frontier was an inferior alternative to him, favoured only by the militant nationalists on the German side.
By 16:50, with the Prussian southern attacks in danger of breaking up, the Prussian 3rd Guards Infantry Brigade of the Second Army opened an attack against the French positions at St. Privat which were commanded by General Canrobert. At 17:15, the Prussian 4th Guards Infantry Brigade joined the advance followed at 17:45 by the Prussian 1st Guards Infantry Brigade. All of the Prussian Guard attacks were pinned down by lethal French gunfire from the rifle pits and trenches. At 18:15 the Prussian 2nd Guards Infantry Brigade, the last of the 1st Guards Infantry Division, was committed to the attack on St. Privat while Steinmetz committed the last of the reserves of the First Army across the Mance Ravine. By 18:30, a considerable portion of the VII and VIII Corps disengaged from the fighting and withdrew towards the Prussian positions at Rezonville.
The Prussian Army, under the terms of the armistice, held a brief victory parade in Paris on 17 February; the city was silent and draped with black and the Germans quickly withdrew. Bismarck honoured the armistice, by allowing train loads of food into Paris and withdrawing Prussian forces to the east of the city, prior to a full withdrawal once France agreed to pay a five billion franc war indemnity. At the same time, Prussian forces were concentrated in the provinces of Alsace and Lorraine. An exodus occurred from Paris as some 200,000 people, predominantly middle-class, went to the countryside.
When the news arrived at Paris of the surrender at Sedan of Napoleon III and 80,000 men, the Second Empire was overthrown by a popular uprising in Paris, which forced the proclamation of a Provisional Government and a Third Republic by general Trochu, Favre and Gambetta at Paris on 4 September, the new government calling itself the Government of National Defence. After the German victory at Sedan, most of the French standing army was either besieged in Metz or prisoner of the Germans, who hoped for an armistice and an end to the war. Bismarck wanted an early peace but had difficulty in finding a legitimate French authority with which to negotiate. The Government of National Defence had no electoral mandate, the Emperor was a captive and the Empress in exile but there had been no abdication de jure and the army was still bound by an oath of allegiance to the defunct imperial régime.
A series of swift Prussian and German victories in eastern France, culminating in the Siege of Metz and the Battle of Sedan, saw the army of the Second Empire decisively defeated (Napoleon III had been captured at Sedan on 2 September). A Government of National Defence declared the Third Republic in Paris on 4 September and continued the war and for another five months, the German forces fought and defeated new French armies in northern France. Following the Siege of Paris, the capital fell on 28 January 1871 and then a revolutionary uprising called the Paris Commune seized power in the capital and held it for two months, until it was bloodily suppressed by the regular French army at the end of May 1871.
Some historians argue that Napoleon III also sought war, particularly for the diplomatic defeat in 1866 in leveraging any benefits from the Austro-Prussian War, and he believed he would win a conflict with Prussia. They also argue that he wanted a war to resolve growing domestic political problems. Other historians, notably French historian Pierre Milza, dispute this. On 8 May 1870, shortly before the war, French voters had overwhelmingly supported Napoleon III's program in a national plebiscite, with 7,358,000 votes yes against 1,582,000 votes no, an increase of support of two million votes since the legislative elections in 1869. According to Milza, the Emperor had no need for a war to increase his popularity.
To relieve pressure from the expected German attack into Alsace-Lorraine, Napoleon III and the French high command planned a seaborne invasion of northern Germany as soon as war began. The French expected the invasion to divert German troops and to encourage Denmark to join in the war, with its 50,000-strong army and the Royal Danish Navy. It was discovered that Prussia had recently built defences around the big North German ports, including coastal artillery batteries with Krupp heavy artillery, which with a range of 4,000 yards (3,700 m), had double the range of French naval guns. The French Navy lacked the heavy guns to engage the coastal defences and the topography of the Prussian coast made a seaborne invasion of northern Germany impossible.
The Prussian Army was composed not of regulars but conscripts. Service was compulsory for all of men of military age, and thus Prussia and its North and South German allies could mobilise and field some 1,000,000 soldiers in time of war. German tactics emphasised encirclement battles like Cannae and using artillery offensively whenever possible. Rather than advancing in a column or line formation, Prussian infantry moved in small groups that were harder to target by artillery or French defensive fire. The sheer number of soldiers available made encirclement en masse and destruction of French formations relatively easy.
In addition, the Prussian military education system was superior to the French model; Prussian staff officers were trained to exhibit initiative and independent thinking. Indeed, this was Moltke's expectation. The French, meanwhile, suffered from an education and promotion system that stifled intellectual development. According to the military historian Dallas Irvine, the system "was almost completely effective in excluding the army's brain power from the staff and high command. To the resulting lack of intelligence at the top can be ascribed all the inexcusable defects of French military policy."
The French breech-loading rifle, the Chassepot, had a far longer range than the German needle gun; 1,500 yards (1,400 m) compared to 600 yd (550 m). The French also had an early machine-gun type weapon, the mitrailleuse, which could fire its thirty-seven barrels at a range of around 1,200 yd (1,100 m). It was developed in such secrecy, that little training with the weapon had occurred, leaving French gunners with no experience; the gun was treated like artillery and in this role it was ineffective. Worse still, once the small number of soldiers who had been trained how to use the new weapon became casualties, there were no replacements who knew how to operate the mitrailleuse.
The French Marines and naval infantry intended for the invasion of northern Germany were dispatched to reinforce the French Army of Châlons and fell into captivity at Sedan along with Napoleon III. A shortage of officers, following the capture of most of the professional French army at the Siege of Metz and at the Battle of Sedan, led naval officers to be sent from their ships to command hastily assembled reservists of the Garde Mobile. As the autumn storms of the North Sea forced the return of more of the French ships, the blockade of the north German ports diminished and in September 1870 the French navy abandoned the blockade for the winter. The rest of the navy retired to ports along the English Channel and remained in port for the rest of the war.
Marshal MacMahon, now closest to Wissembourg, spread his four divisions over 20 miles (32 km) to react to any Prussian invasion. This organization of forces was due to a lack of supplies, forcing each division to seek out basic provisions along with the representatives of the army supply arm that was supposed to aid them. What made a bad situation much worse was the conduct of General Auguste-Alexandre Ducrot, commander of the 1st Division. He told General Abel Douay, commander of the 2nd Division, on 1 August that "The information I have received makes me suppose that the enemy has no considerable forces very near his advance posts, and has no desire to take the offensive". Two days later, he told MacMahon that he had not found "a single enemy post ... it looks to me as if the menace of the Bavarians is simply bluff". Even though Ducrot shrugged off the possibility of an attack by the Germans, MacMahon tried to warn the other divisions of his army, without success.
During the war, the Paris National Guard, particularly in the working-class neighbourhoods of Paris, had become highly politicised and units elected officers; many refused to wear uniforms or obey commands from the national government. National guard units tried to seize power in Paris on 31 October 1870 and 22 January 1871. On 18 March 1871, when the regular army tried to remove cannons from an artillery park on Montmartre, National Guard units resisted and killed two army generals. The national government and regular army forces retreated to Versailles and a revolutionary government was proclaimed in Paris. A Commune was elected, which was dominated by socialists, anarchists and revolutionaries. The red flag replaced the French tricolour and a civil war began between the Commune and the regular army, which attacked and recaptured Paris from 21–28 May in La Semaine Sanglante (Bloody week).
While the French army under General MacMahon engaged the German 3rd Army at the Battle of Wörth, the German 1st Army under Steinmetz finished their advance west from Saarbrücken. A patrol from the German 2nd Army under Prince Friedrich Karl of Prussia spotted decoy fires close and Frossard's army farther off on a distant plateau south of the town of Spicheren, and took this as a sign of Frossard's retreat. Ignoring Moltke's plan again, both German armies attacked Frossard's French 2nd Corps, fortified between Spicheren and Forbach.
The casualties were horrible, especially for the attacking Prussian forces. A grand total of 20,163 German troops were killed, wounded or missing in action during the August 18 battle. The French losses were 7,855 killed and wounded along with 4,420 prisoners of war (half of them were wounded) for a total of 12,275. While most of the Prussians fell under the French Chassepot rifles, most French fell under the Prussian Krupp shells. In a breakdown of the casualties, Frossard's II Corps of the Army of the Rhine suffered 621 casualties while inflicting 4,300 casualties on the Prussian First Army under Steinmetz before the Pointe du Jour. The Prussian Guards Infantry Divisions losses were even more staggering with 8,000 casualties out of 18,000 men. The Special Guards Jäger lost 19 officers, a surgeon and 431 men out of a total of 700. The 2nd Guards Infantry Brigade lost 39 officers and 1,076 men. The 3rd Guards Infantry Brigade lost 36 officers and 1,060 men. On the French side, the units holding St. Privat lost more than half their number in the village.
While the republican government was amenable to war reparations or ceding colonial territories in Africa or in South East Asia to Prussia, Favre on behalf of the Government of National Defense, declared on 6 September that France would not "yield an inch of its territory nor a stone of its fortresses." The republic then renewed the declaration of war, called for recruits in all parts of the country and pledged to drive the German troops out of France by a guerre à outrance. Under these circumstances, the Germans had to continue the war, yet could not pin down any proper military opposition in their vicinity. As the bulk of the remaining French armies were digging-in near Paris, the German leaders decided to put pressure upon the enemy by attacking Paris. By September 15, German troops reached the outskirts of the fortified city. On September 19, the Germans surrounded it and erected a blockade, as already established at Metz.
Albrecht von Roon, the Prussian Minister of War from 1859 to 1873, put into effect a series of reforms of the Prussian military system in the 1860s. Among these were two major reforms that substantially increased the military power of Germany. The first was a reorganization of the army that integrated the regular army and the Landwehr reserves. The second was the provision for the conscription of every male Prussian of military age in the event of mobilization. Thus, despite the population of France being greater than the population of all of the German states that participated in the war, the Germans mobilized more soldiers for battle.
The French were equipped with bronze, rifled muzzle-loading artillery, while the Prussians used new steel breech-loading guns, which had a far longer range and a faster rate of fire. Prussian gunners strove for a high rate of fire, which was discouraged in the French army in the belief that it wasted ammunition. In addition, the Prussian artillery batteries had 30% more guns than their French counterparts. The Prussian guns typically opened fire at a range of 2–3 kilometres (1.2–1.9 mi), beyond the range of French artillery or the Chassepot rifle. The Prussian batteries could thus destroy French artillery with impunity, before being moved forward to directly support infantry attacks.
On 28 January 1871 the Government of National Defence based in Paris negotiated an armistice with the Prussians. With Paris starving, and Gambetta's provincial armies reeling from one disaster after another, French foreign minister Favre went to Versailles on 24 January to discuss peace terms with Bismarck. Bismarck agreed to end the siege and allow food convoys to immediately enter Paris (including trains carrying millions of German army rations), on condition that the Government of National Defence surrender several key fortresses outside Paris to the Prussians. Without the forts, the French Army would no longer be able to defend Paris.
During the fighting, the Communards killed c. 500 people, including the Archbishop of Paris, and burned down many government buildings, including the Tuileries Palace and the Hotel de Ville. Communards captured with weapons were routinely shot by the army and Government troops killed from 7,000–30,000 Communards in the fighting and in massacres of men, women, and children during and after the Commune. More recent histories, based on studies of the number buried in Paris cemeteries and in mass graves after the fall of the Commune, put the number killed at between 6,000 and 10,000. Twenty-six courts were established to try more than 40,000 people who had been arrested, which took until 1875 and imposed 95 death sentences, of which 23 were inflicted. Forced labour for life was imposed on 251 people, 1,160 people were transported to "a fortified place" and 3,417 people were transported. About 20,000 Communards were held in prison hulks until released in 1872 and a great many Communards fled abroad to England, Switzerland, Belgium or the United States. The survivors were amnestied by a bill introduced by Gambetta in 1880 and allowed to return.
At the outset of the Franco-Prussian War, 462,000 German soldiers concentrated on the French frontier while only 270,000 French soldiers could be moved to face them, the French army having lost 100,000 stragglers before a shot was fired through poor planning and administration. This was partly due to the peacetime organisations of the armies. Each Prussian Corps was based within a Kreis (literally "circle") around the chief city in an area. Reservists rarely lived more than a day's travel from their regiment's depot. By contrast, French regiments generally served far from their depots, which in turn were not in the areas of France from which their soldiers were drawn. Reservists often faced several days' journey to report to their depots, and then another long journey to join their regiments. Large numbers of reservists choked railway stations, vainly seeking rations and orders.
The events of the Franco-Prussian War had great influence on military thinking over the next forty years. Lessons drawn from the war included the need for a general staff system, the scale and duration of future wars and the tactical use of artillery and cavalry. The bold use of artillery by the Prussians, to silence French guns at long range and then to directly support infantry attacks at close range, proved to be superior to the defensive doctrine employed by French gunners. The Prussian tactics were adopted by European armies by 1914, exemplified in the French 75, an artillery piece optimised to provide direct fire support to advancing infantry. Most European armies ignored the evidence of the Russo-Japanese War of 1904–05 which suggested that infantry armed with new smokeless-powder rifles could engage gun crews effectively. This forced gunners to fire at longer range using indirect fire, usually from a position of cover.
The creation of a unified German Empire ended the balance of power that had been created with the Congress of Vienna after the end of the Napoleonic Wars. Germany had established itself as the main power in continental Europe with the most powerful and professional army in the world.[citation needed] Although Great Britain remained the dominant world power, British involvement in European affairs during the late 19th century was very limited, allowing Germany to exercise great influence over the European mainland.[citation needed] Besides, the Crown Prince's marriage with the daughter of Queen Victoria was only the most prominent of several German–British relationships.
Frédéric François Chopin (/ˈʃoʊpæn/; French pronunciation: ​[fʁe.de.ʁik fʁɑ̃.swa ʃɔ.pɛ̃]; 22 February or 1 March 1810 – 17 October 1849), born Fryderyk Franciszek Chopin,[n 1] was a Polish and French (by citizenship and birth of father) composer and a virtuoso pianist of the Romantic era, who wrote primarily for the solo piano. He gained and has maintained renown worldwide as one of the leading musicians of his era, whose "poetic genius was based on a professional technique that was without equal in his generation." Chopin was born in what was then the Duchy of Warsaw, and grew up in Warsaw, which after 1815 became part of Congress Poland. A child prodigy, he completed his musical education and composed his earlier works in Warsaw before leaving Poland at the age of 20, less than a month before the outbreak of the November 1830 Uprising.
At the age of 21 he settled in Paris. Thereafter, during the last 18 years of his life, he gave only some 30 public performances, preferring the more intimate atmosphere of the salon. He supported himself by selling his compositions and teaching piano, for which he was in high demand. Chopin formed a friendship with Franz Liszt and was admired by many of his musical contemporaries, including Robert Schumann. In 1835 he obtained French citizenship. After a failed engagement to Maria Wodzińska, from 1837 to 1847 he maintained an often troubled relationship with the French writer George Sand. A brief and unhappy visit to Majorca with Sand in 1838–39 was one of his most productive periods of composition. In his last years, he was financially supported by his admirer Jane Stirling, who also arranged for him to visit Scotland in 1848. Through most of his life, Chopin suffered from poor health. He died in Paris in 1849, probably of tuberculosis.
All of Chopin's compositions include the piano. Most are for solo piano, though he also wrote two piano concertos, a few chamber pieces, and some songs to Polish lyrics. His keyboard style is highly individual and often technically demanding; his own performances were noted for their nuance and sensitivity. Chopin invented the concept of instrumental ballade. His major piano works also include mazurkas, waltzes, nocturnes, polonaises, études, impromptus, scherzos, preludes and sonatas, some published only after his death. Influences on his compositional style include Polish folk music, the classical tradition of J. S. Bach, Mozart and Schubert, the music of all of whom he admired, as well as the Paris salons where he was a frequent guest. His innovations in style, musical form, and harmony, and his association of music with nationalism, were influential throughout and after the late Romantic period.
In his native Poland, in France, where he composed most of his works, and beyond, Chopin's music, his status as one of music's earliest superstars, his association (if only indirect) with political insurrection, his love life and his early death have made him, in the public consciousness, a leading symbol of the Romantic era. His works remain popular, and he has been the subject of numerous films and biographies of varying degrees of historical accuracy.
Fryderyk Chopin was born in Żelazowa Wola, 46 kilometres (29 miles) west of Warsaw, in what was then the Duchy of Warsaw, a Polish state established by Napoleon. The parish baptismal record gives his birthday as 22 February 1810, and cites his given names in the Latin form Fridericus Franciscus (in Polish, he was Fryderyk Franciszek). However, the composer and his family used the birthdate 1 March,[n 2] which is now generally accepted as the correct date.
Fryderyk's father, Nicolas Chopin, was a Frenchman from Lorraine who had emigrated to Poland in 1787 at the age of sixteen. Nicolas tutored children of the Polish aristocracy, and in 1806 married Justyna Krzyżanowska, a poor relative of the Skarbeks, one of the families for whom he worked. Fryderyk was baptized on Easter Sunday, 23 April 1810, in the same church where his parents had married, in Brochów. His eighteen-year-old godfather, for whom he was named, was Fryderyk Skarbek, a pupil of Nicolas Chopin. Fryderyk was the couple's second child and only son; he had an elder sister, Ludwika (1807–55), and two younger sisters, Izabela (1811–81) and Emilia (1812–27). Nicolas was devoted to his adopted homeland, and insisted on the use of the Polish language in the household.
In October 1810, six months after Fryderyk's birth, the family moved to Warsaw, where his father acquired a post teaching French at the Warsaw Lyceum, then housed in the Saxon Palace. Fryderyk lived with his family in the Palace grounds. The father played the flute and violin; the mother played the piano and gave lessons to boys in the boarding house that the Chopins kept. Chopin was of slight build, and even in early childhood was prone to illnesses.
Fryderyk may have had some piano instruction from his mother, but his first professional music tutor, from 1816 to 1821, was the Czech pianist Wojciech Żywny. His elder sister Ludwika also took lessons from Żywny, and occasionally played duets with her brother. It quickly became apparent that he was a child prodigy. By the age of seven Fryderyk had begun giving public concerts, and in 1817 he composed two polonaises, in G minor and B-flat major. His next work, a polonaise in A-flat major of 1821, dedicated to Żywny, is his earliest surviving musical manuscript.
In 1817 the Saxon Palace was requisitioned by Warsaw's Russian governor for military use, and the Warsaw Lyceum was reestablished in the Kazimierz Palace (today the rectorate of Warsaw University). Fryderyk and his family moved to a building, which still survives, adjacent to the Kazimierz Palace. During this period, Fryderyk was sometimes invited to the Belweder Palace as playmate to the son of the ruler of Russian Poland, Grand Duke Constantine; he played the piano for the Duke and composed a march for him. Julian Ursyn Niemcewicz, in his dramatic eclogue, "Nasze Przebiegi" ("Our Discourses", 1818), attested to "little Chopin's" popularity.
From September 1823 to 1826 Chopin attended the Warsaw Lyceum, where he received organ lessons from the Czech musician Wilhelm Würfel during his first year. In the autumn of 1826 he began a three-year course under the Silesian composer Józef Elsner at the Warsaw Conservatory, studying music theory, figured bass and composition.[n 3] Throughout this period he continued to compose and to give recitals in concerts and salons in Warsaw. He was engaged by the inventors of a mechanical organ, the "eolomelodicon", and on this instrument in May 1825 he performed his own improvisation and part of a concerto by Moscheles. The success of this concert led to an invitation to give a similar recital on the instrument before Tsar Alexander I, who was visiting Warsaw; the Tsar presented him with a diamond ring. At a subsequent eolomelodicon concert on 10 June 1825, Chopin performed his Rondo Op. 1. This was the first of his works to be commercially published and earned him his first mention in the foreign press, when the Leipzig Allgemeine Musikalische Zeitung praised his "wealth of musical ideas".
During 1824–28 Chopin spent his vacations away from Warsaw, at a number of locales.[n 4] In 1824 and 1825, at Szafarnia, he was a guest of Dominik Dziewanowski, the father of a schoolmate. Here for the first time he encountered Polish rural folk music. His letters home from Szafarnia (to which he gave the title "The Szafarnia Courier"), written in a very modern and lively Polish, amused his family with their spoofing of the Warsaw newspapers and demonstrated the youngster's literary gift.
In 1827, soon after the death of Chopin's youngest sister Emilia, the family moved from the Warsaw University building, adjacent to the Kazimierz Palace, to lodgings just across the street from the university, in the south annex of the Krasiński Palace on Krakowskie Przedmieście,[n 5] where Chopin lived until he left Warsaw in 1830.[n 6] Here his parents continued running their boarding house for male students; the Chopin Family Parlour (Salonik Chopinów) became a museum in the 20th century. In 1829 the artist Ambroży Mieroszewski executed a set of portraits of Chopin family members, including the first known portrait of the composer.[n 7]
Four boarders at his parents' apartments became Chopin's intimates: Tytus Woyciechowski, Jan Nepomucen Białobłocki, Jan Matuszyński and Julian Fontana; the latter two would become part of his Paris milieu. He was friendly with members of Warsaw's young artistic and intellectual world, including Fontana, Józef Bohdan Zaleski and Stefan Witwicki. He was also attracted to the singing student Konstancja Gładkowska. In letters to Woyciechowski, he indicated which of his works, and even which of their passages, were influenced by his fascination with her; his letter of 15 May 1830 revealed that the slow movement (Larghetto) of his Piano Concerto No. 1 (in E minor) was secretly dedicated to her – "It should be like dreaming in beautiful springtime – by moonlight." His final Conservatory report (July 1829) read: "Chopin F., third-year student, exceptional talent, musical genius."
In September 1828 Chopin, while still a student, visited Berlin with a family friend, zoologist Feliks Jarocki, enjoying operas directed by Gaspare Spontini and attending concerts by Carl Friedrich Zelter, Felix Mendelssohn and other celebrities. On an 1829 return trip to Berlin, he was a guest of Prince Antoni Radziwiłł, governor of the Grand Duchy of Posen—himself an accomplished composer and aspiring cellist. For the prince and his pianist daughter Wanda, he composed his Introduction and Polonaise brillante in C major for cello and piano, Op. 3.
Back in Warsaw that year, Chopin heard Niccolò Paganini play the violin, and composed a set of variations, Souvenir de Paganini. It may have been this experience which encouraged him to commence writing his first Études, (1829–32), exploring the capacities of his own instrument. On 11 August, three weeks after completing his studies at the Warsaw Conservatory, he made his debut in Vienna. He gave two piano concerts and received many favourable reviews—in addition to some commenting (in Chopin's own words) that he was "too delicate for those accustomed to the piano-bashing of local artists". In one of these concerts, he premiered his Variations on Là ci darem la mano, Op. 2 (variations on an aria from Mozart's opera Don Giovanni) for piano and orchestra. He returned to Warsaw in September 1829, where he premiered his Piano Concerto No. 2 in F minor, Op. 21 on 17 March 1830.
Chopin's successes as a composer and performer opened the door to western Europe for him, and on 2 November 1830, he set out, in the words of Zdzisław Jachimecki, "into the wide world, with no very clearly defined aim, forever." With Woyciechowski, he headed for Austria, intending to go on to Italy. Later that month, in Warsaw, the November 1830 Uprising broke out, and Woyciechowski returned to Poland to enlist. Chopin, now alone in Vienna, was nostalgic for his homeland, and wrote to a friend, "I curse the moment of my departure." When in September 1831 he learned, while travelling from Vienna to Paris, that the uprising had been crushed, he expressed his anguish in the pages of his private journal: "Oh God! ... You are there, and yet you do not take vengeance!" Jachimecki ascribes to these events the composer's maturing "into an inspired national bard who intuited the past, present and future of his native Poland."
Chopin arrived in Paris in late September 1831; he would never return to Poland, thus becoming one of many expatriates of the Polish Great Emigration. In France he used the French versions of his given names, and after receiving French citizenship in 1835, he travelled on a French passport. However, Chopin remained close to his fellow Poles in exile as friends and confidants and he never felt fully comfortable speaking French. Chopin's biographer Adam Zamoyski writes that he never considered himself to be French, despite his father's French origins, and always saw himself as a Pole.
In Paris, Chopin encountered artists and other distinguished figures, and found many opportunities to exercise his talents and achieve celebrity. During his years in Paris he was to become acquainted with, among many others, Hector Berlioz, Franz Liszt, Ferdinand Hiller, Heinrich Heine, Eugène Delacroix, and Alfred de Vigny. Chopin was also acquainted with the poet Adam Mickiewicz, principal of the Polish Literary Society, some of whose verses he set as songs.
Two Polish friends in Paris were also to play important roles in Chopin's life there. His fellow student at the Warsaw Conservatory, Julian Fontana, had originally tried unsuccessfully to establish himself in England; Albert Grzymała, who in Paris became a wealthy financier and society figure, often acted as Chopin's adviser and "gradually began to fill the role of elder brother in [his] life." Fontana was to become, in the words of Michałowski and Samson, Chopin's "general factotum and copyist".
At the end of 1831, Chopin received the first major endorsement from an outstanding contemporary when Robert Schumann, reviewing the Op. 2 Variations in the Allgemeine musikalische Zeitung (his first published article on music), declared: "Hats off, gentlemen! A genius." On 26 February 1832 Chopin gave a debut Paris concert at the Salle Pleyel which drew universal admiration. The critic François-Joseph Fétis wrote in the Revue et gazette musicale: "Here is a young man who ... taking no model, has found, if not a complete renewal of piano music, ... an abundance of original ideas of a kind to be found nowhere else ..." After this concert, Chopin realized that his essentially intimate keyboard technique was not optimal for large concert spaces. Later that year he was introduced to the wealthy Rothschild banking family, whose patronage also opened doors for him to other private salons (social gatherings of the aristocracy and artistic and literary elite). By the end of 1832 Chopin had established himself among the Parisian musical elite, and had earned the respect of his peers such as Hiller, Liszt, and Berlioz. He no longer depended financially upon his father, and in the winter of 1832 he began earning a handsome income from publishing his works and teaching piano to affluent students from all over Europe. This freed him from the strains of public concert-giving, which he disliked.
Chopin seldom performed publicly in Paris. In later years he generally gave a single annual concert at the Salle Pleyel, a venue that seated three hundred. He played more frequently at salons, but preferred playing at his own Paris apartment for small groups of friends. The musicologist Arthur Hedley has observed that "As a pianist Chopin was unique in acquiring a reputation of the highest order on the basis of a minimum of public appearances—few more than thirty in the course of his lifetime." The list of musicians who took part in some of his concerts provides an indication of the richness of Parisian artistic life during this period. Examples include a concert on 23 March 1833, in which Chopin, Liszt and Hiller performed (on pianos) a concerto by J.S. Bach for three keyboards; and, on 3 March 1838, a concert in which Chopin, his pupil Adolphe Gutmann, Charles-Valentin Alkan, and Alkan's teacher Joseph Zimmermann performed Alkan's arrangement, for eight hands, of two movements from Beethoven's 7th symphony. Chopin was also involved in the composition of Liszt's Hexameron; he wrote the sixth (and final) variation on Bellini's theme. Chopin's music soon found success with publishers, and in 1833 he contracted with Maurice Schlesinger, who arranged for it to be published not only in France but, through his family connections, also in Germany and England.
In the spring of 1834, Chopin attended the Lower Rhenish Music Festival in Aix-la-Chapelle with Hiller, and it was there that Chopin met Felix Mendelssohn. After the festival, the three visited Düsseldorf, where Mendelssohn had been appointed musical director. They spent what Mendelssohn described as "a very agreeable day", playing and discussing music at his piano, and met Friedrich Wilhelm Schadow, director of the Academy of Art, and some of his eminent pupils such as Lessing, Bendemann, Hildebrandt and Sohn. In 1835 Chopin went to Carlsbad, where he spent time with his parents; it was the last time he would see them. On his way back to Paris, he met old friends from Warsaw, the Wodzińskis. He had made the acquaintance of their daughter Maria in Poland five years earlier, when she was eleven. This meeting prompted him to stay for two weeks in Dresden, when he had previously intended to return to Paris via Leipzig. The sixteen-year-old girl's portrait of the composer is considered, along with Delacroix's, as among Chopin's best likenesses. In October he finally reached Leipzig, where he met Schumann, Clara Wieck and Felix Mendelssohn, who organised for him a performance of his own oratorio St. Paul, and who considered him "a perfect musician". In July 1836 Chopin travelled to Marienbad and Dresden to be with the Wodziński family, and in September he proposed to Maria, whose mother Countess Wodzińska approved in principle. Chopin went on to Leipzig, where he presented Schumann with his G minor Ballade. At the end of 1836 he sent Maria an album in which his sister Ludwika had inscribed seven of his songs, and his 1835 Nocturne in C-sharp minor, Op. 27, No. 1. The anodyne thanks he received from Maria proved to be the last letter he was to have from her.
Although it is not known exactly when Chopin first met Liszt after arriving in Paris, on 12 December 1831 he mentioned in a letter to his friend Woyciechowski that "I have met Rossini, Cherubini, Baillot, etc.—also Kalkbrenner. You would not believe how curious I was about Herz, Liszt, Hiller, etc." Liszt was in attendance at Chopin's Parisian debut on 26 February 1832 at the Salle Pleyel, which led him to remark: "The most vigorous applause seemed not to suffice to our enthusiasm in the presence of this talented musician, who revealed a new phase of poetic sentiment combined with such happy innovation in the form of his art."
The two became friends, and for many years lived in close proximity in Paris, Chopin at 38 Rue de la Chaussée-d'Antin, and Liszt at the Hôtel de France on the Rue Lafitte, a few blocks away. They performed together on seven occasions between 1833 and 1841. The first, on 2 April 1833, was at a benefit concert organized by Hector Berlioz for his bankrupt Shakespearean actress wife Harriet Smithson, during which they played George Onslow's Sonata in F minor for piano duet. Later joint appearances included a benefit concert for the Benevolent Association of Polish Ladies in Paris. Their last appearance together in public was for a charity concert conducted for the Beethoven Memorial in Bonn, held at the Salle Pleyel and the Paris Conservatory on 25 and 26 April 1841.
Although the two displayed great respect and admiration for each other, their friendship was uneasy and had some qualities of a love-hate relationship. Harold C. Schonberg believes that Chopin displayed a "tinge of jealousy and spite" towards Liszt's virtuosity on the piano, and others have also argued that he had become enchanted with Liszt's theatricality, showmanship and success. Liszt was the dedicatee of Chopin's Op. 10 Études, and his performance of them prompted the composer to write to Hiller, "I should like to rob him of the way he plays my studies." However, Chopin expressed annoyance in 1843 when Liszt performed one of his nocturnes with the addition of numerous intricate embellishments, at which Chopin remarked that he should play the music as written or not play it at all, forcing an apology. Most biographers of Chopin state that after this the two had little to do with each other, although in his letters dated as late as 1848 he still referred to him as "my friend Liszt". Some commentators point to events in the two men's romantic lives which led to a rift between them; there are claims that Liszt had displayed jealousy of his mistress Marie d'Agoult's obsession with Chopin, while others believe that Chopin had become concerned about Liszt's growing relationship with George Sand.
In 1836, at a party hosted by Marie d'Agoult, Chopin met the French author George Sand (born [Amantine] Aurore [Lucile] Dupin). Short (under five feet, or 152 cm), dark, big-eyed and a cigar smoker, she initially repelled Chopin, who remarked, "What an unattractive person la Sand is. Is she really a woman?" However, by early 1837 Maria Wodzińska's mother had made it clear to Chopin in correspondence that a marriage with her daughter was unlikely to proceed. It is thought that she was influenced by his poor health and possibly also by rumours about his associations with women such as d'Agoult and Sand. Chopin finally placed the letters from Maria and her mother in a package on which he wrote, in Polish, "My tragedy". Sand, in a letter to Grzymała of June 1838, admitted strong feelings for the composer and debated whether to abandon a current affair in order to begin a relationship with Chopin; she asked Grzymała to assess Chopin's relationship with Maria Wodzińska, without realising that the affair, at least from Maria's side, was over.
In June 1837 Chopin visited London incognito in the company of the piano manufacturer Camille Pleyel where he played at a musical soirée at the house of English piano maker James Broadwood. On his return to Paris, his association with Sand began in earnest, and by the end of June 1838 they had become lovers. Sand, who was six years older than the composer, and who had had a series of lovers, wrote at this time: "I must say I was confused and amazed at the effect this little creature had on me ... I have still not recovered from my astonishment, and if I were a proud person I should be feeling humiliated at having been carried away ..." The two spent a miserable winter on Majorca (8 November 1838 to 13 February 1839), where, together with Sand's two children, they had journeyed in the hope of improving the health of Chopin and that of Sand's 15-year-old son Maurice, and also to escape the threats of Sand's former lover Félicien Mallefille. After discovering that the couple were not married, the deeply traditional Catholic people of Majorca became inhospitable, making accommodation difficult to find. This compelled the group to take lodgings in a former Carthusian monastery in Valldemossa, which gave little shelter from the cold winter weather.
On 3 December, Chopin complained about his bad health and the incompetence of the doctors in Majorca: "Three doctors have visited me ... The first said I was dead; the second said I was dying; and the third said I was about to die." He also had problems having his Pleyel piano sent to him. It finally arrived from Paris in December. Chopin wrote to Pleyel in January 1839: "I am sending you my Preludes [(Op. 28)]. I finished them on your little piano, which arrived in the best possible condition in spite of the sea, the bad weather and the Palma customs." Chopin was also able to undertake work on his Ballade No. 2, Op. 38; two Polonaises, Op. 40; and the Scherzo No. 3, Op. 39.
Although this period had been productive, the bad weather had such a detrimental effect on Chopin's health that Sand determined to leave the island. To avoid further customs duties, Sand sold the piano to a local French couple, the Canuts.[n 8] The group traveled first to Barcelona, then to Marseilles, where they stayed for a few months while Chopin convalesced. In May 1839 they headed for the summer to Sand's estate at Nohant, where they spent most summers until 1846. In autumn they returned to Paris, where Chopin's apartment at 5 rue Tronchet was close to Sand's rented accommodation at the rue Pigalle. He frequently visited Sand in the evenings, but both retained some independence. In 1842 he and Sand moved to the Square d'Orléans, living in adjacent buildings.
At the funeral of the tenor Adolphe Nourrit in Paris in 1839, Chopin made a rare appearance at the organ, playing a transcription of Franz Schubert's lied Die Gestirne. On 26 July 1840 Chopin and Sand were present at the dress rehearsal of Berlioz's Grande symphonie funèbre et triomphale, composed to commemorate the tenth anniversary of the July Revolution. Chopin was reportedly unimpressed with the composition.
During the summers at Nohant, particularly in the years 1839–43, Chopin found quiet, productive days during which he composed many works, including his Polonaise in A-flat major, Op. 53. Among the visitors to Nohant were Delacroix and the mezzo-soprano Pauline Viardot, whom Chopin had advised on piano technique and composition. Delacroix gives an account of staying at Nohant in a letter of 7 June 1842:
From 1842 onwards, Chopin showed signs of serious illness. After a solo recital in Paris on 21 February 1842, he wrote to Grzymała: "I have to lie in bed all day long, my mouth and tonsils are aching so much." He was forced by illness to decline a written invitation from Alkan to participate in a repeat performance of the Beethoven Seventh Symphony arrangement at Erard's on 1 March 1843. Late in 1844, Charles Hallé visited Chopin and found him "hardly able to move, bent like a half-opened penknife and evidently in great pain", although his spirits returned when he started to play the piano for his visitor. Chopin's health continued to deteriorate, particularly from this time onwards. Modern research suggests that apart from any other illnesses, he may also have suffered from temporal lobe epilepsy.
Chopin's relations with Sand were soured in 1846 by problems involving her daughter Solange and Solange's fiancé, the young fortune-hunting sculptor Auguste Clésinger. The composer frequently took Solange's side in quarrels with her mother; he also faced jealousy from Sand's son Maurice. Chopin was utterly indifferent to Sand's radical political pursuits, while Sand looked on his society friends with disdain. As the composer's illness progressed, Sand had become less of a lover and more of a nurse to Chopin, whom she called her "third child". In letters to third parties, she vented her impatience, referring to him as a "child," a "little angel", a "sufferer" and a "beloved little corpse." In 1847 Sand published her novel Lucrezia Floriani, whose main characters—a rich actress and a prince in weak health—could be interpreted as Sand and Chopin; the story was uncomplimentary to Chopin, who could not have missed the allusions as he helped Sand correct the printer's galleys. In 1847 he did not visit Nohant, and he quietly ended their ten-year relationship following an angry correspondence which, in Sand's words, made "a strange conclusion to nine years of exclusive friendship." The two would never meet again.
Chopin's output as a composer throughout this period declined in quantity year by year. Whereas in 1841 he had written a dozen works, only six were written in 1842 and six shorter pieces in 1843. In 1844 he wrote only the Op. 58 sonata. 1845 saw the completion of three mazurkas (Op. 59). Although these works were more refined than many of his earlier compositions, Zamoyski opines that "his powers of concentration were failing and his inspiration was beset by anguish, both emotional and intellectual."
Chopin's public popularity as a virtuoso began to wane, as did the number of his pupils, and this, together with the political strife and instability of the time, caused him to struggle financially. In February 1848, with the cellist Auguste Franchomme, he gave his last Paris concert, which included three movements of the Cello Sonata Op. 65.
Chopin's life was covered in a BBC TV documentary Chopin – The Women Behind The Music (2010), and in a 2010 documentary realised by Angelo Bozzolini and Roberto Prosseda for Italian television.
Chopin's life and his relations with George Sand have been fictionalized in numerous films. The 1945 biographical film A Song to Remember earned Cornel Wilde an Academy Award nomination as Best Actor for his portrayal of the composer. Other film treatments have included: La valse de l'adieu (France, 1928) by Henry Roussel, with Pierre Blanchar as Chopin; Impromptu (1991), starring Hugh Grant as Chopin; La note bleue (1991); and Chopin: Desire for Love (2002).
Possibly the first venture into fictional treatments of Chopin's life was a fanciful operatic version of some of its events. Chopin was written by Giacomo Orefice and produced in Milan in 1901. All the music is derived from that of Chopin.
Chopin has figured extensively in Polish literature, both in serious critical studies of his life and music and in fictional treatments. The earliest manifestation was probably an 1830 sonnet on Chopin by Leon Ulrich. French writers on Chopin (apart from Sand) have included Marcel Proust and André Gide; and he has also featured in works of Gottfried Benn and Boris Pasternak. There are numerous biographies of Chopin in English (see bibliography for some of these).
Numerous recordings of Chopin's works are available. On the occasion of the composer's bicentenary, the critics of The New York Times recommended performances by the following contemporary pianists (among many others): Martha Argerich, Vladimir Ashkenazy, Emanuel Ax, Evgeny Kissin, Murray Perahia, Maurizio Pollini and Krystian Zimerman. The Warsaw Chopin Society organizes the Grand prix du disque de F. Chopin for notable Chopin recordings, held every five years.
The British Library notes that "Chopin's works have been recorded by all the great pianists of the recording era." The earliest recording was an 1895 performance by Paul Pabst of the Nocturne in E major Op. 62 No. 2. The British Library site makes available a number of historic recordings, including some by Alfred Cortot, Ignaz Friedman, Vladimir Horowitz, Benno Moiseiwitsch, Paderewski, Arthur Rubinstein, Xaver Scharwenka and many others. A select discography of recordings of Chopin works by pianists representing the various pedagogic traditions stemming from Chopin is given by Methuen-Campbell in his work tracing the lineage and character of those traditions.
Chopin's music remains very popular and is regularly performed, recorded and broadcast worldwide. The world's oldest monographic music competition, the International Chopin Piano Competition, founded in 1927, is held every five years in Warsaw. The Fryderyk Chopin Institute of Poland lists on its website over eighty societies world-wide devoted to the composer and his music. The Institute site also lists nearly 1,500 performances of Chopin works on YouTube as of January 2014.
Chopin's music was used in the 1909 ballet Chopiniana, choreographed by Michel Fokine and orchestrated by Alexander Glazunov. Sergei Diaghilev commissioned additional orchestrations—from Stravinsky, Anatoly Lyadov, Sergei Taneyev and Nikolai Tcherepnin—for later productions, which used the title Les Sylphides.
In April, during the Revolution of 1848 in Paris, he left for London, where he performed at several concerts and at numerous receptions in great houses. This tour was suggested to him by his Scottish pupil Jane Stirling and her elder sister. Stirling also made all the logistical arrangements and provided much of the necessary funding.
In London Chopin took lodgings at Dover Street, where the firm of Broadwood provided him with a grand piano. At his first engagement, on 15 May at Stafford House, the audience included Queen Victoria and Prince Albert. The Prince, who was himself a talented musician, moved close to the keyboard to view Chopin's technique. Broadwood also arranged concerts for him; among those attending were Thackeray and the singer Jenny Lind. Chopin was also sought after for piano lessons, for which he charged the high fee of one guinea (£1.05 in present British currency) per hour, and for private recitals for which the fee was 20 guineas. At a concert on 7 July he shared the platform with Viardot, who sang arrangements of some of his mazurkas to Spanish texts.
In late summer he was invited by Jane Stirling to visit Scotland, where he stayed at Calder House near Edinburgh and at Johnstone Castle in Renfrewshire, both owned by members of Stirling's family. She clearly had a notion of going beyond mere friendship, and Chopin was obliged to make it clear to her that this could not be so. He wrote at this time to Grzymała "My Scottish ladies are kind, but such bores", and responding to a rumour about his involvement, answered that he was "closer to the grave than the nuptial bed." He gave a public concert in Glasgow on 27 September, and another in Edinburgh, at the Hopetoun Rooms on Queen Street (now Erskine House) on 4 October. In late October 1848, while staying at 10 Warriston Crescent in Edinburgh with the Polish physician Adam Łyszczyński, he wrote out his last will and testament—"a kind of disposition to be made of my stuff in the future, if I should drop dead somewhere", he wrote to Grzymała.
Chopin made his last public appearance on a concert platform at London's Guildhall on 16 November 1848, when, in a final patriotic gesture, he played for the benefit of Polish refugees. By this time he was very seriously ill, weighing under 99 pounds (i.e. less than 45 kg), and his doctors were aware that his sickness was at a terminal stage.
At the end of November, Chopin returned to Paris. He passed the winter in unremitting illness, but gave occasional lessons and was visited by friends, including Delacroix and Franchomme. Occasionally he played, or accompanied the singing of Delfina Potocka, for his friends. During the summer of 1849, his friends found him an apartment in Chaillot, out of the centre of the city, for which the rent was secretly subsidised by an admirer, Princess Obreskoff. Here in June 1849 he was visited by Jenny Lind.
With his health further deteriorating, Chopin desired to have a family member with him. In June 1849 his sister Ludwika came to Paris with her husband and daughter, and in September, supported by a loan from Jane Stirling, he took an apartment at Place Vendôme 12. After 15 October, when his condition took a marked turn for the worse, only a handful of his closest friends remained with him, although Viardot remarked sardonically that "all the grand Parisian ladies considered it de rigueur to faint in his room."
Some of his friends provided music at his request; among them, Potocka sang and Franchomme played the cello. Chopin requested that his body be opened after death (for fear of being buried alive) and his heart returned to Warsaw where it rests at the Church of the Holy Cross. He also bequeathed his unfinished notes on a piano tuition method, Projet de méthode, to Alkan for completion. On 17 October, after midnight, the physician leaned over him and asked whether he was suffering greatly. "No longer", he replied. He died a few minutes before two o'clock in the morning. Those present at the deathbed appear to have included his sister Ludwika, Princess Marcelina Czartoryska, Sand's daughter Solange, and his close friend Thomas Albrecht. Later that morning, Solange's husband Clésinger made Chopin's death mask and a cast of his left hand.
Chopin's disease and the cause of his death have since been a matter of discussion. His death certificate gave the cause as tuberculosis, and his physician, Jean Cruveilhier, was then the leading French authority on this disease. Other possibilities have been advanced including cystic fibrosis, cirrhosis and alpha 1-antitrypsin deficiency. However, the attribution of tuberculosis as principal cause of death has not been disproved. Permission for DNA testing, which could put the matter to rest, has been denied by the Polish government.
The funeral, held at the Church of the Madeleine in Paris, was delayed almost two weeks, until 30 October. Entrance was restricted to ticket holders as many people were expected to attend. Over 3,000 people arrived without invitations, from as far as London, Berlin and Vienna, and were excluded.
Mozart's Requiem was sung at the funeral; the soloists were the soprano Jeanne-Anais Castellan, the mezzo-soprano Pauline Viardot, the tenor Alexis Dupont, and the bass Luigi Lablache; Chopin's Preludes No. 4 in E minor and No. 6 in B minor were also played. The organist at the funeral was Louis Lefébure-Wély. The funeral procession to Père Lachaise Cemetery, which included Chopin's sister Ludwika, was led by the aged Prince Adam Czartoryski. The pallbearers included Delacroix, Franchomme, and Camille Pleyel. At the graveside, the Funeral March from Chopin's Piano Sonata No. 2 was played, in Reber's instrumentation.
Chopin's tombstone, featuring the muse of music, Euterpe, weeping over a broken lyre, was designed and sculpted by Clésinger. The expenses of the funeral and monument, amounting to 5,000 francs, were covered by Jane Stirling, who also paid for the return of the composer's sister Ludwika to Warsaw. Ludwika took Chopin's heart in an urn, preserved in alcohol, back to Poland in 1850.[n 9] She also took a collection of two hundred letters from Sand to Chopin; after 1851 these were returned to Sand, who seems to have destroyed them.
Over 230 works of Chopin survive; some compositions from early childhood have been lost. All his known works involve the piano, and only a few range beyond solo piano music, as either piano concertos, songs or chamber music.
Chopin was educated in the tradition of Beethoven, Haydn, Mozart and Clementi; he used Clementi's piano method with his own students. He was also influenced by Hummel's development of virtuoso, yet Mozartian, piano technique. He cited Bach and Mozart as the two most important composers in shaping his musical outlook. Chopin's early works are in the style of the "brilliant" keyboard pieces of his era as exemplified by the works of Ignaz Moscheles, Friedrich Kalkbrenner, and others. Less direct in the earlier period are the influences of Polish folk music and of Italian opera. Much of what became his typical style of ornamentation (for example, his fioriture) is taken from singing. His melodic lines were increasingly reminiscent of the modes and features of the music of his native country, such as drones.
Chopin took the new salon genre of the nocturne, invented by the Irish composer John Field, to a deeper level of sophistication. He was the first to write ballades and scherzi as individual concert pieces. He essentially established a new genre with his own set of free-standing preludes (Op. 28, published 1839). He exploited the poetic potential of the concept of the concert étude, already being developed in the 1820s and 1830s by Liszt, Clementi and Moscheles, in his two sets of studies (Op. 10 published in 1833, Op. 25 in 1837).
Chopin also endowed popular dance forms with a greater range of melody and expression. Chopin's mazurkas, while originating in the traditional Polish dance (the mazurek), differed from the traditional variety in that they were written for the concert hall rather than the dance hall; "it was Chopin who put the mazurka on the European musical map." The series of seven polonaises published in his lifetime (another nine were published posthumously), beginning with the Op. 26 pair (published 1836), set a new standard for music in the form. His waltzes were also written specifically for the salon recital rather than the ballroom and are frequently at rather faster tempos than their dance-floor equivalents.
Some of Chopin's well-known pieces have acquired descriptive titles, such as the Revolutionary Étude (Op. 10, No. 12), and the Minute Waltz (Op. 64, No. 1). However, with the exception of his Funeral March, the composer never named an instrumental work beyond genre and number, leaving all potential extramusical associations to the listener; the names by which many of his pieces are known were invented by others. There is no evidence to suggest that the Revolutionary Étude was written with the failed Polish uprising against Russia in mind; it merely appeared at that time. The Funeral March, the third movement of his Sonata No. 2 (Op. 35), the one case where he did give a title, was written before the rest of the sonata, but no specific event or death is known to have inspired it.
The last opus number that Chopin himself used was 65, allocated to the Cello Sonata in G minor. He expressed a deathbed wish that all his unpublished manuscripts be destroyed. At the request of the composer's mother and sisters, however, his musical executor Julian Fontana selected 23 unpublished piano pieces and grouped them into eight further opus numbers (Opp. 66–73), published in 1855. In 1857, 17 Polish songs that Chopin wrote at various stages of his life were collected and published as Op. 74, though their order within the opus did not reflect the order of composition.
Works published since 1857 have received alternative catalogue designations instead of opus numbers. The present standard musicological reference for Chopin's works is the Kobylańska Catalogue (usually represented by the initials 'KK'), named for its compiler, the Polish musicologist Krystyna Kobylańska.
Chopin's original publishers included Maurice Schlesinger and Camille Pleyel. His works soon began to appear in popular 19th-century piano anthologies. The first collected edition was by Breitkopf & Härtel (1878–1902). Among modern scholarly editions of Chopin's works are the version under the name of Paderewski published between 1937 and 1966 and the more recent Polish "National Edition", edited by Jan Ekier, both of which contain detailed explanations and discussions regarding choices and sources.
Improvisation stands at the centre of Chopin's creative processes. However, this does not imply impulsive rambling: Nicholas Temperley writes that "improvisation is designed for an audience, and its starting-point is that audience's expectations, which include the current conventions of musical form." The works for piano and orchestra, including the two concertos, are held by Temperley to be "merely vehicles for brilliant piano playing ... formally longwinded and extremely conservative". After the piano concertos (which are both early, dating from 1830), Chopin made no attempts at large-scale multi-movement forms, save for his late sonatas for piano and for cello; "instead he achieved near-perfection in pieces of simple general design but subtle and complex cell-structure." Rosen suggests that an important aspect of Chopin's individuality is his flexible handling of the four-bar phrase as a structural unit.
J. Barrie Jones suggests that "amongst the works that Chopin intended for concert use, the four ballades and four scherzos stand supreme", and adds that "the Barcarolle Op. 60 stands apart as an example of Chopin's rich harmonic palette coupled with an Italianate warmth of melody." Temperley opines that these works, which contain "immense variety of mood, thematic material and structural detail", are based on an extended "departure and return" form; "the more the middle section is extended, and the further it departs in key, mood and theme, from the opening idea, the more important and dramatic is the reprise when it at last comes."
Chopin's mazurkas and waltzes are all in straightforward ternary or episodic form, sometimes with a coda. The mazurkas often show more folk features than many of his other works, sometimes including modal scales and harmonies and the use of drone basses. However, some also show unusual sophistication, for example Op. 63 No. 3, which includes a canon at one beat's distance, a great rarity in music.
Chopin's polonaises show a marked advance on those of his Polish predecessors in the form (who included his teachers Zywny and Elsner). As with the traditional polonaise, Chopin's works are in triple time and typically display a martial rhythm in their melodies, accompaniments and cadences. Unlike most of their precursors, they also require a formidable playing technique.
The 21 nocturnes are more structured, and of greater emotional depth, than those of Field (whom Chopin met in 1833). Many of the Chopin nocturnes have middle sections marked by agitated expression (and often making very difficult demands on the performer) which heightens their dramatic character.
Chopin's études are largely in straightforward ternary form. He used them to teach his own technique of piano playing—for instance playing double thirds (Op. 25, No. 6), playing in octaves (Op. 25, No. 10), and playing repeated notes (Op. 10, No.  7).
The preludes, many of which are very brief (some consisting of simple statements and developments of a single theme or figure), were described by Schumann as "the beginnings of studies". Inspired by J.S. Bach's The Well-Tempered Clavier, Chopin's preludes move up the circle of fifths (rather than Bach's chromatic scale sequence) to create a prelude in each major and minor tonality. The preludes were perhaps not intended to be played as a group, and may even have been used by him and later pianists as generic preludes to others of his pieces, or even to music by other composers, as Kenneth Hamilton suggests: he has noted a recording by Ferruccio Busoni of 1922, in which the Prelude Op. 28 No. 7 is followed by the Étude Op. 10 No. 5.
The two mature piano sonatas (No. 2, Op. 35, written in 1839 and No. 3, Op. 58, written in 1844) are in four movements. In Op. 35, Chopin was able to combine within a formal large musical structure many elements of his virtuosic piano technique—"a kind of dialogue between the public pianism of the brilliant style and the German sonata principle". The last movement, a brief (75-bar) perpetuum mobile in which the hands play in unmodified octave unison throughout, was found shocking and unmusical by contemporaries, including Schumann. The Op. 58 sonata is closer to the German tradition, including many passages of complex counterpoint, "worthy of Brahms" according to the music historians Kornel Michałowski and Jim Samson.
Chopin's harmonic innovations may have arisen partly from his keyboard improvisation technique. Temperley says that in his works "novel harmonic effects frequently result from the combination of ordinary appoggiaturas or passing notes with melodic figures of accompaniment", and cadences are delayed by the use of chords outside the home key (neapolitan sixths and diminished sevenths), or by sudden shifts to remote keys. Chord progressions sometimes anticipate the shifting tonality of later composers such as Claude Debussy, as does Chopin's use of modal harmony.
In 1841, Léon Escudier wrote of a recital given by Chopin that year, "One may say that Chopin is the creator of a school of piano and a school of composition. In truth, nothing equals the lightness, the sweetness with which the composer preludes on the piano; moreover nothing may be compared to his works full of originality, distinction and grace." Chopin refused to conform to a standard method of playing and believed that there was no set technique for playing well. His style was based extensively on his use of very independent finger technique. In his Projet de méthode he wrote: "Everything is a matter of knowing good fingering ... we need no less to use the rest of the hand, the wrist, the forearm and the upper arm." He further stated: "One needs only to study a certain position of the hand in relation to the keys to obtain with ease the most beautiful quality of sound, to know how to play short notes and long notes, and [to attain] unlimited dexterity." The consequences of this approach to technique in Chopin's music include the frequent use of the entire range of the keyboard, passages in double octaves and other chord groupings, swiftly repeated notes, the use of grace notes, and the use of contrasting rhythms (four against three, for example) between the hands.
Polish composers of the following generation included virtuosi such as Moritz Moszkowski, but, in the opinion of J. Barrie Jones, his "one worthy successor" among his compatriots was Karol Szymanowski (1882–1937). Edvard Grieg, Antonín Dvořák, Isaac Albéniz, Pyotr Ilyich Tchaikovsky and Sergei Rachmaninoff, among others, are regarded by critics as having been influenced by Chopin's use of national modes and idioms. Alexander Scriabin was devoted to the music of Chopin, and his early published works include nineteen mazurkas, as well as numerous études and preludes; his teacher Nikolai Zverev drilled him in Chopin's works to improve his virtuosity as a performer. In the 20th century, composers who paid homage to (or in some cases parodied) the music of Chopin included George Crumb, Bohuslav Martinů, Darius Milhaud, Igor Stravinsky and Heitor Villa-Lobos.
Jonathan Bellman writes that modern concert performance style—set in the "conservatory" tradition of late 19th- and 20th-century music schools, and suitable for large auditoria or recordings—militates against what is known of Chopin's more intimate performance technique. The composer himself said to a pupil that "concerts are never real music, you have to give up the idea of hearing in them all the most beautiful things of art." Contemporary accounts indicate that in performance, Chopin avoided rigid procedures sometimes incorrectly attributed to him, such as "always crescendo to a high note", but that he was concerned with expressive phrasing, rhythmic consistency and sensitive colouring. Berlioz wrote in 1853 that Chopin "has created a kind of chromatic embroidery ... whose effect is so strange and piquant as to be impossible to describe ... virtually nobody but Chopin himself can play this music and give it this unusual turn". Hiller wrote that "What in the hands of others was elegant embellishment, in his hands became a colourful wreath of flowers."
Chopin's music is frequently played with rubato, "the practice in performance of disregarding strict time, 'robbing' some note-values for expressive effect". There are differing opinions as to how much, and what type, of rubato is appropriate for his works. Charles Rosen comments that "most of the written-out indications of rubato in Chopin are to be found in his mazurkas ... It is probable that Chopin used the older form of rubato so important to Mozart ... [where] the melody note in the right hand is delayed until after the note in the bass ... An allied form of this rubato is the arpeggiation of the chords thereby delaying the melody note; according to Chopin's pupil, Karol Mikuli, Chopin was firmly opposed to this practice."
Friederike Müller, a pupil of Chopin, wrote: "[His] playing was always noble and beautiful; his tones sang, whether in full forte or softest piano. He took infinite pains to teach his pupils this legato, cantabile style of playing. His most severe criticism was 'He—or she—does not know how to join two notes together.' He also demanded the strictest adherence to rhythm. He hated all lingering and dragging, misplaced rubatos, as well as exaggerated ritardandos ... and it is precisely in this respect that people make such terrible errors in playing his works."
With his mazurkas and polonaises, Chopin has been credited with introducing to music a new sense of nationalism. Schumann, in his 1836 review of the piano concertos, highlighted the composer's strong feelings for his native Poland, writing that "Now that the Poles are in deep mourning [after the failure of the November 1830 rising], their appeal to us artists is even stronger ... If the mighty autocrat in the north [i.e. Nicholas I of Russia] could know that in Chopin's works, in the simple strains of his mazurkas, there lurks a dangerous enemy, he would place a ban on his music. Chopin's works are cannon buried in flowers!" The biography of Chopin published in 1863 under the name of Franz Liszt (but probably written by Carolyne zu Sayn-Wittgenstein) claims that Chopin "must be ranked first among the first musicians ... individualizing in themselves the poetic sense of an entire nation."
Some modern commentators have argued against exaggerating Chopin's primacy as a "nationalist" or "patriotic" composer. George Golos refers to earlier "nationalist" composers in Central Europe, including Poland's Michał Kleofas Ogiński and Franciszek Lessel, who utilised polonaise and mazurka forms. Barbara Milewski suggests that Chopin's experience of Polish music came more from "urbanised" Warsaw versions than from folk music, and that attempts (by Jachimecki and others) to demonstrate genuine folk music in his works are without basis. Richard Taruskin impugns Schumann's attitude toward Chopin's works as patronizing and comments that Chopin "felt his Polish patriotism deeply and sincerely" but consciously modelled his works on the tradition of Bach, Beethoven, Schubert and Field.
A reconciliation of these views is suggested by William Atwood: "Undoubtedly [Chopin's] use of traditional musical forms like the polonaise and mazurka roused nationalistic sentiments and a sense of cohesiveness amongst those Poles scattered across Europe and the New World ... While some sought solace in [them], others found them a source of strength in their continuing struggle for freedom. Although Chopin's music undoubtedly came to him intuitively rather than through any conscious patriotic design, it served all the same to symbolize the will of the Polish people ..."
Jones comments that "Chopin's unique position as a composer, despite the fact that virtually everything he wrote was for the piano, has rarely been questioned." He also notes that Chopin was fortunate to arrive in Paris in 1831—"the artistic environment, the publishers who were willing to print his music, the wealthy and aristocratic who paid what Chopin asked for their lessons"—and these factors, as well as his musical genius, also fuelled his contemporary and later reputation. While his illness and his love-affairs conform to some of the stereotypes of romanticism, the rarity of his public recitals (as opposed to performances at fashionable Paris soirées) led Arthur Hutchings to suggest that "his lack of Byronic flamboyance [and] his aristocratic reclusiveness make him exceptional" among his romantic contemporaries, such as Liszt and Henri Herz.
Chopin's qualities as a pianist and composer were recognized by many of his fellow musicians. Schumann named a piece for him in his suite Carnaval, and Chopin later dedicated his Ballade No. 2 in F major to Schumann. Elements of Chopin's music can be traced in many of Liszt's later works. Liszt later transcribed for piano six of Chopin's Polish songs. A less fraught friendship was with Alkan, with whom he discussed elements of folk music, and who was deeply affected by Chopin's death.
Two of Chopin's long-standing pupils, Karol Mikuli (1821–1897) and Georges Mathias, were themselves piano teachers and passed on details of his playing to their own students, some of whom (such as Raoul Koczalski) were to make recordings of his music. Other pianists and composers influenced by Chopin's style include Louis Moreau Gottschalk, Édouard Wolff (1816–1880) and Pierre Zimmermann. Debussy dedicated his own 1915 piano Études to the memory of Chopin; he frequently played Chopin's music during his studies at the Paris Conservatoire, and undertook the editing of Chopin's piano music for the publisher Jacques Durand.
The Early Triassic was between 250 million to 247 million years ago and was dominated by deserts as Pangaea had not yet broken up, thus the interior was nothing but arid. The Earth had just witnessed a massive die-off in which 95% of all life went extinct. The most common life on earth were Lystrosaurus, Labyrinthodont, and Euparkeria along with many other creatures that managed to survive the Great Dying. Temnospondyli evolved during this time and would be the dominant predator for much of the Triassic.
The climatic changes of the late Jurassic and Cretaceous provided for further adaptive radiation. The Jurassic was the height of archosaur diversity, and the first birds and eutherian mammals also appeared. Angiosperms radiated sometime in the early Cretaceous, first in the tropics, but the even temperature gradient allowed them to spread toward the poles throughout the period. By the end of the Cretaceous, angiosperms dominated tree floras in many areas, although some evidence suggests that biomass was still dominated by cycad and ferns until after the Cretaceous–Paleogene extinction.
The lower (Triassic) boundary is set by the Permian–Triassic extinction event, during which approximately 90% to 96% of marine species and 70% of terrestrial vertebrates became extinct. It is also known as the "Great Dying" because it is considered the largest mass extinction in the Earth's history. The upper (Cretaceous) boundary is set at the Cretaceous–Tertiary (KT) extinction event (now more accurately called the Cretaceous–Paleogene (or K–Pg) extinction event), which may have been caused by the impactor that created Chicxulub Crater on the Yucatán Peninsula. Towards the Late Cretaceous large volcanic eruptions are also believed to have contributed to the Cretaceous–Paleogene extinction event. Approximately 50% of all genera became extinct, including all of the non-avian dinosaurs.
The Late Cretaceous spans from 100 million to 65 million years ago. The Late Cretaceous featured a cooling trend that would continue on in the Cenozoic period. Eventually, tropics were restricted to the equator and areas beyond the tropic lines featured extreme seasonal changes in weather. Dinosaurs still thrived as new species such as Tyrannosaurus, Ankylosaurus, Triceratops and Hadrosaurs dominated the food web. In the oceans, Mosasaurs ruled the seas to fill the role of the Ichthyosaurs, and huge plesiosaurs, such as Elasmosaurus, evolved. Also, the first flowering plants evolved. At the end of the Cretaceous, the Deccan traps and other volcanic eruptions were poisoning the atmosphere. As this was continuing, it is thought that a large meteor smashed into earth, creating the Chicxulub Crater in an event known as the K-T Extinction, the fifth and most recent mass extinction event, in which 75% of life on earth went extinct, including all non-avian dinosaurs. Everything over 10 kilograms went extinct. The age of the dinosaurs was over.
The climate of the Cretaceous is less certain and more widely disputed. Higher levels of carbon dioxide in the atmosphere are thought to have caused the world temperature gradient from north to south to become almost flat: temperatures were about the same across the planet. Average temperatures were also higher than today by about 10°C. In fact, by the middle Cretaceous, equatorial ocean waters (perhaps as warm as 20 °C in the deep ocean) may have been too warm for sea life,[dubious – discuss][citation needed] and land areas near the equator may have been deserts despite their proximity to water. The circulation of oxygen to the deep ocean may also have been disrupted.[dubious – discuss] For this reason, large volumes of organic matter that was unable to decompose accumulated, eventually being deposited as "black shale".
The Late Triassic spans from 237 million to 200 million years ago. Following the bloom of the Middle Triassic, the Late Triassic featured frequent heat spells, as well as moderate precipitation (10-20 inches per year). The recent warming led to a boom of reptilian evolution on land as the first true dinosaurs evolve, as well as pterosaurs. All this climatic change, however, resulted in a large die-out known as the Triassic-Jurassic extinction event, in which all archosaurs (excluding ancient crocodiles), most synapsids, and almost all large amphibians went extinct, as well as 34% of marine life in the fourth mass extinction event of the world. The cause is debatable.
The Early Cretaceous spans from 145 million to 100 million years ago. The Early Cretaceous saw the expansion of seaways, and as a result, the decline and extinction of sauropods (except in South America). Many coastal shallows were created, and that caused Ichthyosaurs to die out. Mosasaurs evolved to replace them as head of the seas. Some island-hopping dinosaurs, like Eustreptospondylus, evolved to cope with the coastal shallows and small islands of ancient Europe. Other dinosaurs rose up to fill the empty space that the Jurassic-Cretaceous extinction left behind, such as Carcharodontosaurus and Spinosaurus. Of the most successful would be the Iguanodon which spread to every continent. Seasons came back into effect and the poles got seasonally colder, but dinosaurs still inhabited this area like the Leaellynasaura which inhabited the polar forests year-round, and many dinosaurs migrated there during summer like Muttaburrasaurus. Since it was too cold for crocodiles, it was the last stronghold for large amphibians, like Koolasuchus. Pterosaurs got larger as species like Tapejara and Ornithocheirus evolved.
Sea levels began to rise during the Jurassic, which was probably caused by an increase in seafloor spreading. The formation of new crust beneath the surface displaced ocean waters by as much as 200 m (656 ft) more than today, which flooded coastal areas. Furthermore, Pangaea began to rift into smaller divisions, bringing more land area in contact with the ocean by forming the Tethys Sea. Temperatures continued to increase and began to stabilize. Humidity also increased with the proximity of water, and deserts retreated.
The Early Jurassic spans from 200 million years to 175 million years ago. The climate was much more humid than the Triassic, and as a result, the world was very tropical. In the oceans, Plesiosaurs, Ichthyosaurs and Ammonites fill waters as the dominant races of the seas. On land, dinosaurs and other reptiles stake their claim as the dominant race of the land, with species such as Dilophosaurus at the top. The first true crocodiles evolved, pushing out the large amphibians to near extinction. All-in-all, reptiles rise to rule the world. Meanwhile, the first true mammals evolve, but remained relatively small sized.
Compared to the vigorous convergent plate mountain-building of the late Paleozoic, Mesozoic tectonic deformation was comparatively mild. The sole major Mesozoic orogeny occurred in what is now the Arctic, creating the Innuitian orogeny, the Brooks Range, the Verkhoyansk and Cherskiy Ranges in Siberia, and the Khingan Mountains in Manchuria. This orogeny was related to the opening of the Arctic Ocean and subduction of the North China and Siberian cratons under the Pacific Ocean. Nevertheless, the era featured the dramatic rifting of the supercontinent Pangaea. Pangaea gradually split into a northern continent, Laurasia, and a southern continent, Gondwana. This created the passive continental margin that characterizes most of the Atlantic coastline (such as along the U.S. East Coast) today.
Recent research indicates that the specialized animals that formed complex ecosystems, with high biodiversity, complex food webs and a variety of niches, took much longer to reestablish, recovery did not begin until the start of the mid-Triassic, 4M to 6M years after the extinction and was not complete until 30M years after the Permian–Triassic extinction event. Animal life was then dominated by various archosaurian reptiles: dinosaurs, pterosaurs, and aquatic reptiles such as ichthyosaurs, plesiosaurs, and mosasaurs.
The era began in the wake of the Permian–Triassic extinction event, the largest well-documented mass extinction in Earth's history, and ended with the Cretaceous–Paleogene extinction event, another mass extinction which is known for having killed off non-avian dinosaurs, as well as other plant and animal species. The Mesozoic was a time of significant tectonic, climate and evolutionary activity. The era witnessed the gradual rifting of the supercontinent Pangaea into separate landmasses that would eventually move into their current positions. The climate of the Mesozoic was varied, alternating between warming and cooling periods. Overall, however, the Earth was hotter than it is today. Non-avian dinosaurs appeared in the Late Triassic and became the dominant terrestrial vertebrates early in the Jurassic, occupying this position for about 135 million years until their demise at the end of the Cretaceous. Birds first appeared in the Jurassic, having evolved from a branch of theropod dinosaurs. The first mammals also appeared during the Mesozoic, but would remain small—less than 15 kg (33 lb)—until the Cenozoic.
The Middle Triassic spans from 247 million to 237 million years ago. The Middle Triassic featured the beginnings of the breakup of Pangaea, and the beginning of the Tethys Sea. The ecosystem had recovered from the devastation that was the Great Dying. Phytoplankton, coral, and crustaceans all had recovered, and the reptiles began to get bigger and bigger. New aquatic reptiles evolved such as Ichthyosaurs and Nothosaurs. Meanwhile, on land, Pine forests flourished, bringing along mosquitoes and fruit flies. The first ancient crocodilians evolved, which sparked competition with the large amphibians that had since ruled the freshwater world.
The Late Jurassic spans from 163 million to 145 million years ago. The Late Jurassic featured a massive extinction of sauropods and Ichthyosaurs due to the separation of Pangaea into Laurasia and Gondwana in an extinction known as the Jurassic-Cretaceous extinction. Sea levels rose, destroying fern prairies and creating shallows in its wake. Ichthyosaurs went extinct whereas sauropods, as a whole, did not die out in the Jurassic; in fact, some species, like the Titanosaurus, lived up to the K-T extinction. The increase in sea-levels opened up the Atlantic sea way which would continue to get larger over time. The divided world would give opportunity for the diversification of new dinosaurs.
The Triassic was generally dry, a trend that began in the late Carboniferous, and highly seasonal, especially in the interior of Pangaea. Low sea levels may have also exacerbated temperature extremes. With its high specific heat capacity, water acts as a temperature-stabilizing heat reservoir, and land areas near large bodies of water—especially the oceans—experience less variation in temperature. Because much of the land that constituted Pangaea was distant from the oceans, temperatures fluctuated greatly, and the interior of Pangaea probably included expansive areas of desert. Abundant red beds and evaporites such as halite support these conclusions, but evidence exists that the generally dry climate of the Triassic was punctuated by episodes of increased rainfall. Most important humid episodes were the Carnian Pluvial Event and one in the Rhaetian, few million years before the Triassic–Jurassic extinction event.
The dominant land plant species of the time were gymnosperms, which are vascular, cone-bearing, non-flowering plants such as conifers that produce seeds without a coating. This is opposed to the earth's current flora, in which the dominant land plants in terms of number of species are angiosperms. One particular plant genus, Ginkgo, is thought to have evolved at this time and is represented today by a single species, Ginkgo biloba. As well, the extant genus Sequoia is believed to have evolved in the Mesozoic.
Saint Helena (/ˌseɪnt həˈliːnə/ SAYNT-hə-LEE-nə) is a volcanic tropical island in the South Atlantic Ocean, 4,000 kilometres (2,500 mi) east of Rio de Janeiro and 1,950 kilometres (1,210 mi) west of the Cunene River, which marks the border between Namibia and Angola in southwestern Africa. It is part of the British Overseas Territory of Saint Helena, Ascension and Tristan da Cunha. Saint Helena measures about 16 by 8 kilometres (10 by 5 mi) and has a population of 4,255 (2008 census). It was named after Saint Helena of Constantinople.
The island was uninhabited when discovered by the Portuguese in 1502. One of the most remote islands in the world, it was for centuries an important stopover for ships sailing to Europe from Asia and South Africa. Napoleon was imprisoned there in exile by the British, as were Dinuzulu kaCetshwayo (for leading a Zulu army against British rule) and more than 5,000 Boers taken prisoner during the Second Boer War.
Between 1791 and 1833, Saint Helena became the site of a series of experiments in conservation, reforestation and attempts to boost rainfall artificially. This environmental intervention was closely linked to the conceptualisation of the processes of environmental change and helped establish the roots of environmentalism.
Most historical accounts state that the island was discovered on 21 May 1502 by the Galician navigator João da Nova sailing at the service of Portugal, and that he named it "Santa Helena" after Helena of Constantinople. Another theory holds that the island found by da Nova was actually Tristan da Cunha, 2,430 kilometres (1,510 mi) to the south, and that Saint Helena was discovered by some of the ships attached to the squadron of Estêvão da Gama expedition on 30 July 1503 (as reported in the account of clerk Thomé Lopes). However, a paper published in 2015 reviewed the discovery date and dismissed the 18 August as too late for da Nova to make a discovery and then return to Lisbon by 11 September 1502, whether he sailed from St Helena or Tristan da Cunha. It demonstrates the 21 May is probably a Protestant rather than Catholic or Orthodox feast-day, first quoted in 1596 by Jan Huyghen van Linschoten, who was probably mistaken because the island was discovered several decades before the Reformation and start of Protestantism. The alternative discovery date of 3 May, the Catholic feast-day for the finding of the True Cross by Saint Helena in Jerusalem, quoted by Odoardo Duarte Lopes and Sir Thomas Herbert is suggested as being historically more credible.
The Portuguese found the island uninhabited, with an abundance of trees and fresh water. They imported livestock, fruit trees and vegetables, and built a chapel and one or two houses. Though they formed no permanent settlement, the island was an important rendezvous point and source of food for ships travelling from Asia to Europe, and frequently sick mariners were left on the island to recover, before taking passage on the next ship to call on the island.
Englishman Sir Francis Drake probably located the island on the final leg of his circumnavigation of the world (1577–1580). Further visits by other English explorers followed, and, once Saint Helena’s location was more widely known, English ships of war began to lie in wait in the area to attack Portuguese India carracks on their way home. In developing their Far East trade, the Dutch also began to frequent the island. The Portuguese and Spanish soon gave up regularly calling at the island, partly because they used ports along the West African coast, but also because of attacks on their shipping, the desecration of their chapel and religious icons, destruction of their livestock and destruction of plantations by Dutch and English sailors.
The Dutch Republic formally made claim to Saint Helena in 1633, although there is no evidence that they ever occupied, colonised or fortified it. By 1651, the Dutch had mainly abandoned the island in favour of their colony at the Cape of Good Hope.
In 1657, Oliver Cromwell granted the English East India Company a charter to govern Saint Helena and the following year the company decided to fortify the island and colonise it with planters. The first governor, Captain John Dutton, arrived in 1659, making Saint Helena one of Britain's oldest colonies outside North America and the Caribbean. A fort and houses were built. After the Restoration of the English monarchy in 1660, the East India Company received a royal charter giving it the sole right to fortify and colonise the island. The fort was renamed James Fort and the town Jamestown, in honour of the Duke of York, later James II of England.
Between January and May 1673, the Dutch East India Company forcibly took the island, before English reinforcements restored English East India Company control. The company experienced difficulty attracting new immigrants, and sentiments of unrest and rebellion fomented among the inhabitants. Ecological problems, including deforestation, soil erosion, vermin and drought, led Governor Isaac Pyke to suggest in 1715 that the population be moved to Mauritius, but this was not acted upon and the company continued to subsidise the community because of the island's strategic location. A census in 1723 recorded 1,110 people, including 610 slaves.
18th century governors tried to tackle the island's problems by implementing tree plantation, improving fortifications, eliminating corruption, building a hospital, tackling the neglect of crops and livestock, controlling the consumption of alcohol and introducing legal reforms. From about 1770, the island enjoyed a lengthy period of prosperity. Captain James Cook visited the island in 1775 on the final leg of his second circumnavigation of the world. St. James' Church was erected in Jamestown in 1774 and in 1791–92 Plantation House was built, and has since been the official residence of the Governor.
On leaving the University of Oxford, in 1676, Edmond Halley visited Saint Helena and set up an astronomical observatory with a 7.3-metre-long (24 ft) aerial telescope with the intention of studying stars from the Southern Hemisphere. The site of this telescope is near Saint Mathew's Church in Hutt's Gate, in the Longwood district. The 680-metre (2,230 ft) high hill there is named for him and is called Halley's Mount.
Throughout this period, Saint Helena was an important port of call of the East India Company. East Indiamen would stop there on the return leg of their voyages to British India and China. At Saint Helena ships could replenish supplies of water and provisions, and during war time, form convoys that would sail under the protection of vessels of the Royal Navy. Captain James Cook's vessel HMS Endeavour anchored and resupplied off the coast of St Helena in May 1771, on her return from the European discovery of the east coast of Australia and rediscovery of New Zealand.
The importation of slaves was made illegal in 1792. Governor Robert Patton (1802–1807) recommended that the company import Chinese labour to supplement the rural workforce. The coolie labourers arrived in 1810, and their numbers reached 600 by 1818. Many were allowed to stay, and their descendents became integrated into the population. An 1814 census recorded 3,507 people on the island.
In 1815, the British government selected Saint Helena as the place of detention of Napoleon Bonaparte. He was taken to the island in October 1815. Napoleon stayed at the Briars pavilion on the grounds of the Balcombe family's home until his permanent residence, Longwood House, was completed in December 1815. Napoleon died there on 5 May 1821.
After Napoleon's death, the thousands of temporary visitors were soon withdrawn and the East India Company resumed full control of Saint Helena. Between 1815 and 1830, the EIC made available to the government of the island the packet schooner St Helena, which made multiple trips per year between the island and the Cape carrying passengers both ways, and supplies of wine and provisions back to the island.
Owing to Napoleon's praise of Saint Helena’s coffee during his exile on the island, the product enjoyed a brief popularity in Paris in the years after his death.
Although the importation of slaves to St Helena had been banned in 1792, the phased emancipation of over 800 resident slaves did not take place until 1827, which was still some six years before the British Parliament passed legislation to ban slavery in the colonies.
Under the provisions of the 1833 India Act, control of Saint Helena was passed from the East India Company to the British Crown, becoming a crown colony. Subsequent administrative cost-cutting triggered the start of a long-term population decline whereby those who could afford to do so tended to leave the island for better opportunities elsewhere. The latter half of the 19th century saw the advent of steam ships not reliant on trade winds, as well as the diversion of Far East trade away from the traditional South Atlantic shipping lanes to a route via the Red Sea (which, prior to the building of the Suez Canal, involved a short overland section). These factors contributed to a decline in the number of ships calling at the island from 1,100 in 1855 to only 288 in 1889.
In 1840, a British naval station established to suppress the African slave trade was based on the island, and between 1840 and 1849 over 15,000 freed slaves, known as "Liberated Africans", were landed there.
In 1858, the French emperor Napoleon III successfully gained the possession, in the name of the French government, of Longwood House and the lands around it, last residence of Napoleon I (who died there in 1821). It is still French property, administered by a French representative and under the authority of the French Ministry of Foreign Affairs.
On 11 April 1898 American Joshua Slocum, on his famous and epic solo round the world voyage arrived at Jamestown. He departed on 20 April 1898 for the final leg of his circumnavigation having been extended hospitality from the governor, his Excellency Sir R A Standale, presented two lectures on his voyage and been invited to Longwood by the French Consular agent.
A local industry manufacturing fibre from New Zealand flax was successfully reestablished in 1907 and generated considerable income during the First World War. Ascension Island was made a dependency of Saint Helena in 1922, and Tristan da Cunha followed in 1938. During the Second World War, the United States built Wideawake airport on Ascension in 1942, but no military use was made of Saint Helena.
During this period, the island enjoyed increased revenues through the sale of flax, with prices peaking in 1951. However, the industry declined because of transportation costs and competition from synthetic fibres. The decision by the British Post Office to use synthetic fibres for its mailbags was a further blow, contributing to the closure of the island's flax mills in 1965.
From 1958, the Union Castle shipping line gradually reduced its service calls to the island. Curnow Shipping, based in Avonmouth, replaced the Union-Castle Line mailship service in 1977, using the RMS (Royal Mail Ship) St Helena.
The British Nationality Act 1981 reclassified Saint Helena and the other Crown colonies as British Dependent Territories. The islanders lost their right of abode in Britain. For the next 20 years, many could find only low-paid work with the island government, and the only available employment outside Saint Helena was on the Falkland Islands and Ascension Island. The Development and Economic Planning Department, which still operates, was formed in 1988 to contribute to raising the living standards of the people of Saint Helena.
In 1989, Prince Andrew launched the replacement RMS St Helena to serve the island; the vessel was specially built for the Cardiff–Cape Town route and features a mixed cargo/passenger layout.
The Saint Helena Constitution took effect in 1989 and provided that the island would be governed by a Governor and Commander-in-Chief, and an elected Executive and Legislative Council. In 2002, the British Overseas Territories Act 2002 granted full British citizenship to the islanders, and renamed the Dependent Territories (including Saint Helena) the British Overseas Territories. In 2009, Saint Helena and its two territories received equal status under a new constitution, and the British Overseas Territory was renamed Saint Helena, Ascension and Tristan da Cunha.
The UK government has spent £250 million in the construction of the island's airport. Expected to be fully operational early 2016, it is expected to help the island towards self-sufficiency and encourage economic development, reducing dependence on British government aid. The airport is also expected to kick start the tourism industry, with up to 30,000 visitors expected annually. As of August, 2015 ticketing was postponed until an airline could be firmly designated.
Located in the South Atlantic Ocean on the Mid-Atlantic Ridge, more than 2,000 kilometres (1,200 mi) from the nearest major landmass, Saint Helena is one of the most remote places in the world. The nearest port on the continent is Namibe in southern Angola, and the nearest international airport the Quatro de Fevereiro Airport of Angola's capital Luanda; connections to Cape Town in South Africa are used for most shipping needs, such as the mail boat that serves the island, the RMS St Helena. The island is associated with two other isolated islands in the southern Atlantic, also British territories: Ascension Island about 1,300 kilometres (810 mi) due northwest in more equatorial waters and Tristan da Cunha, which is well outside the tropics 2,430 kilometres (1,510 mi) to the south. The island is situated in the Western Hemisphere and has the same longitude as Cornwall in the United Kingdom. Despite its remote location, it is classified as being in West Africa by the United Nations.
The island of Saint Helena has a total area of 122 km2 (47 sq mi), and is composed largely of rugged terrain of volcanic origin (the last volcanic eruptions occurred about 7 million years ago). Coastal areas are covered in volcanic rock and warmer and drier than the centre. The highest point of the island is Diana's Peak at 818 m (2,684 ft). In 1996 it became the island's first national park. Much of the island is covered by New Zealand flax, a legacy of former industry, but there are some original trees augmented by plantations, including those of the Millennium Forest project which was established in 2002 to replant part of the lost Great Wood and is now managed by the Saint Helena National Trust. When the island was discovered, it was covered with unique indigenous vegetation, including a remarkable cabbage tree species. The island's hinterland must have been a dense tropical forest but the coastal areas were probably also quite green. The modern landscape is very different, with widespread bare rock in the lower areas, although inland it is green, mainly due to introduced vegetation. There are no native land mammals, but cattle, cats, dogs, donkeys, goats, mice, rabbits, rats and sheep have been introduced, and native species have been adversely affected as a result. The dramatic change in landscape must be attributed to these introductions. As a result, the string tree (Acalypha rubrinervis) and the St Helena olive (Nesiota elliptica) are now extinct, and many of the other endemic plants are threatened with extinction.
There are several rocks and islets off the coast, including: Castle Rock, Speery Island, the Needle, Lower Black Rock, Upper Black Rock (South), Bird Island (Southwest), Black Rock, Thompson's Valley Island, Peaked Island, Egg Island, Lady's Chair, Lighter Rock (West), Long Ledge (Northwest), Shore Island, George Island, Rough Rock Island, Flat Rock (East), the Buoys, Sandy Bay Island, the Chimney, White Bird Island and Frightus Rock (Southeast), all of which are within one kilometre (0.62 miles) of the shore.
The national bird of Saint Helena is the Saint Helena plover, known locally as the wirebird. It appears on the coat of arms of Saint Helena and on the flag.
The climate of Saint Helena is tropical, marine and mild, tempered by the Benguela Current and trade winds that blow almost continuously. The climate varies noticeably across the island. Temperatures in Jamestown, on the north leeward shore, range between 21–28 °C (70–82 °F) in the summer (January to April) and 17–24 °C (63–75 °F) during the remainder of the year. The temperatures in the central areas are, on average, 5–6 °C (9.0–10.8 °F) lower. Jamestown also has a very low annual rainfall, while 750–1,000 mm (30–39 in) falls per year on the higher ground and the south coast, where it is also noticeably cloudier. There are weather recording stations in the Longwood and Blue Hill districts.
Saint Helena is divided into eight districts, each with a community centre. The districts also serve as statistical subdivisions. The island is a single electoral area and elects twelve representatives to the Legislative Council of fifteen.
Saint Helena was first settled by the English in 1659, and the island has a population of about 4,250 inhabitants, mainly descended from people from Britain – settlers ("planters") and soldiers – and slaves who were brought there from the beginning of settlement – initially from Africa (the Cape Verde Islands, Gold Coast and west coast of Africa are mentioned in early records), then India and Madagascar. Eventually the planters felt there were too many slaves and no more were imported after 1792.
In 1840, St Helena became a provisioning station for the British West Africa Squadron, preventing slavery to Brazil (mainly), and many thousands of slaves were freed on the island. These were all African, and about 500 stayed while the rest were sent on to the West Indies and Cape Town, and eventually to Sierra Leone.
Imported Chinese labourers arrived in 1810, reaching a peak of 618 in 1818, after which numbers were reduced. Only a few older men remained after the British Crown took over the government of the island from the East India Company in 1834. The majority were sent back to China, although records in the Cape suggest that they never got any farther than Cape Town. There were also a very few Indian lascars who worked under the harbour master.
The citizens of Saint Helena hold British Overseas Territories citizenship. On 21 May 2002, full British citizenship was restored by the British Overseas Territories Act 2002. See also British nationality law.
During periods of unemployment, there has been a long pattern of emigration from the island since the post-Napoleonic period. The majority of "Saints" emigrated to the UK, South Africa and in the early years, Australia. The population has steadily declined since the late 1980s and has dropped from 5,157 at the 1998 census to 4,255 in 2008. In the past emigration was characterised by young unaccompanied persons leaving to work on long-term contracts on Ascension and the Falkland Islands, but since "Saints" were re-awarded UK citizenship in 2002, emigration to the UK by a wider range of wage-earners has accelerated due to the prospect of higher wages and better progression prospects.
Most residents belong to the Anglican Communion and are members of the Diocese of St Helena, which has its own bishop and includes Ascension Island. The 150th anniversary of the diocese was celebrated in June 2009.
Other Christian denominations on the island include: Roman Catholic (since 1852), Salvation Army (since 1884), Baptist (since 1845) and, in more recent times, Seventh-day Adventist (since 1949), New Apostolic and Jehovah's Witnesses (of which one in 35 residents is a member, the highest ratio of any country). The Catholics are pastorally served by the Mission sui iuris of Saint Helena, Ascension and Tristan da Cunha, whose office of ecclesiastical superior is vested in the Apostolic Prefecture of the Falkland Islands.
Executive authority in Saint Helena is vested in Queen Elizabeth II and is exercised on her behalf by the Governor of Saint Helena. The Governor is appointed by the Queen on the advice of the British government. Defence and Foreign Affairs remain the responsibility of the United Kingdom.
There are fifteen seats in the Legislative Council of Saint Helena, a unicameral legislature, in addition to a Speaker and a Deputy Speaker. Twelve of the fifteen members are elected in elections held every four years. The three ex officio members are the Chief Secretary, Financial Secretary and Attorney General. The Executive Council is presided over by the Governor, and consists of three ex officio officers and five elected members of the Legislative Council appointed by the Governor. There is no elected Chief Minister, and the Governor acts as the head of government. In January 2013 it was proposed that the Executive Council would be led by a "Chief Councillor" who would be elected by the members of the Legislative Council and would nominate the other members of the Executive Council. These proposals were put to a referendum on 23 March 2013 where they were defeated by 158 votes to 42 on a 10% turnout.
One commentator has observed that, notwithstanding the high unemployment resulting from the loss of full passports during 1981–2002, the level of loyalty to the British monarchy by the St Helena population is probably not exceeded in any other part of the world. King George VI is the only reigning monarch to have visited the island. This was in 1947 when the King, accompanied by Queen Elizabeth (later the Queen Mother), Princess Elizabeth (later Queen Elizabeth II) and Princess Margaret were travelling to South Africa. Prince Philip arrived at St Helena in 1957 and then his son Prince Andrew visited as a member of the armed forces in 1984 and his sister the Princess Royal arrived in 2002.
In 2012, the government of St. Helena funded the creation of the St. Helena Human Rights Action Plan 2012-2015. Work is being done under this action plan, including publishing awareness-raising articles in local newspapers, providing support for members of the public with human rights queries, and extending several UN Conventions on human rights to St. Helena.
In recent years[when?], there have been reports of child abuse in St Helena. Britain’s Foreign and Commonwealth Office (FCO) has been accused of lying to the United Nations about child abuse in St Helena to cover up allegations, including cases of a police officer having raped a four-year-old girl and of a police officer having mutilated a two-year-old.
St Helena has long been known for its high proportion of endemic birds and vascular plants. The highland areas contain most of the 400 endemic species recognised to date. Much of the island has been identified by BirdLife International as being important for bird conservation, especially the endemic Saint Helena plover or wirebird, and for seabirds breeding on the offshore islets and stacks, in the north-east and the south-west Important Bird Areas. On the basis of these endemics and an exceptional range of habitats, Saint Helena is on the United Kingdom's tentative list for future UNESCO World Heritage Sites.
St Helena's biodiversity, however, also includes marine vertebrates, invertebrates (freshwater, terrestrial and marine), fungi (including lichen-forming species), non-vascular plants, seaweeds and other biological groups. To date, very little is known about these, although more than 200 lichen-forming fungi have been recorded, including 9 endemics, suggesting that many significant discoveries remain to be made.
The island had a monocrop economy until 1966, based on the cultivation and processing of New Zealand flax for rope and string. St Helena's economy is now weak, and is almost entirely sustained by aid from the British government. The public sector dominates the economy, accounting for about 50% of gross domestic product. Inflation was running at 4% in 2005. There have been increases in the cost of fuel, power and all imported goods.
The tourist industry is heavily based on the promotion of Napoleon's imprisonment. A golf course also exists and the possibility for sportfishing tourism is great. Three hotels operate on the island but the arrival of tourists is directly linked to the arrival and departure schedule of the RMS St Helena. Some 3,200 short-term visitors arrived on the island in 2013.
Saint Helena produces what is said to be the most expensive coffee in the world. It also produces and exports Tungi Spirit, made from the fruit of the prickly or cactus pears, Opuntia ficus-indica ("Tungi" is the local St Helenian name for the plant). Ascension Island, Tristan da Cunha and Saint Helena all issue their own postage stamps which provide a significant income.
Quoted at constant 2002 prices, GDP fell from £12 million in 1999-2000 to £11 million in 2005-06. Imports are mainly from the UK and South Africa and amounted to £6.4 million in 2004-05 (quoted on an FOB basis). Exports are much smaller, amounting to £0.2 million in 2004-05. Exports are mainly fish and coffee; Philatelic sales were £0.06 million in 2004-05. The limited number of visiting tourists spent about £0.4 million in 2004-05, representing a contribution to GDP of 3%.
Public expenditure rose from £10 million in 2001-02 to £12 million in 2005-06 to £28m in 2012-13. The contribution of UK budgetary aid to total SHG government expenditure rose from £4.6 million in to £6.4 million to £12.1 million over the same period. Wages and salaries represent about 38% of recurrent expenditure.
Unemployment levels are low (31 individuals in 2013, compared to 50 in 2004 and 342 in 1998). Employment is dominated by the public sector, the number of government positions has fallen from 1,142 in 2006 to just over 800 in 2013. St Helena’s private sector employs approximately 45% of the employed labour force and is largely dominated by small and micro businesses with 218 private businesses employing 886 in 2004.
Household survey results suggest the percentage of households spending less than £20 per week on a per capita basis fell from 27% to 8% between 2000 and 2004, implying a decline in income poverty. Nevertheless, 22% of the population claimed social security benefit in 2006/7, most of them aged over 60, a sector that represents 20% of the population.
In 1821, Saul Solomon issued a 70,560 copper tokens worth a halfpenny each Payable at St Helena by Solomon, Dickson and Taylor – presumably London partners – that circulated alongside the East India Company's local coinage until the Crown took over the island in 1836. The coin remains readily available to collectors.
Today Saint Helena has its own currency, the Saint Helena pound, which is at parity with the pound sterling. The government of Saint Helena produces its own coinage and banknotes. The Bank of Saint Helena was established on Saint Helena and Ascension Island in 2004. It has branches in Jamestown on Saint Helena, and Georgetown, Ascension Island and it took over the business of the St. Helena government savings bank and Ascension Island Savings Bank.
Saint Helena is one of the most remote islands in the world, has one commercial airport under construction, and travel to the island is by ship only. A large military airfield is located on Ascension Island, with two Friday flights to RAF Brize Norton, England (as from September 2010). These RAF flights offer a limited number of seats to civilians.
The ship RMS Saint Helena runs between St Helena and Cape Town on a 5-day voyage, also visiting Ascension Island and Walvis Bay, and occasionally voyaging north to Tenerife and Portland, UK. It berths in James Bay, St Helena approximately thirty times per year. The RMS Saint Helena was due for decommissioning in 2010. However, its service life has been extended indefinitely until the airport is completed.
After a long period of rumour and consultation, the British government announced plans to construct an airport in Saint Helena in March 2005. The airport was expected to be completed by 2010. However an approved bidder, the Italian firm Impregilo, was not chosen until 2008, and then the project was put on hold in November 2008, allegedly due to new financial pressures brought on by the Financial crisis of 2007–2010. By January 2009, construction had not commenced and no final contracts had been signed. Governor Andrew Gurr departed for London in an attempt to speed up the process and solve the problems.
On 22 July 2010, the British government agreed to help pay for the new airport using taxpayer money. In November 2011 a new deal between the British government and South African civil engineering company Basil Read was signed and the airport was scheduled to open in February 2016, with flights to and from South Africa and the UK. In March 2015 South African airline Comair became the preferred bidder to provide weekly air service between the island and Johannesburg, starting from 2016.
The first aircraft, a South African Beechcraft King Air 200, landed at the new airport on 15 September 2015, prior to conducting a series of flights to calibrate the airport's radio navigation equipment.
The first helicopter landing at the new airfield was conducted by the Wildcat HMA.2 ZZ377 from 825 Squadron 201 Flight, embarked on visiting HMS Lancaster on 23 October 2015.
A minibus offers a basic service to carry people around Saint Helena, with most services designed to take people into Jamestown for a few hours on weekdays to conduct their business. Car hire is available for visitors.
Radio St Helena, which started operations on Christmas Day 1967, provided a local radio service that had a range of about 100 km (62 mi) from the island, and also broadcast internationally on shortwave radio (11092.5 kHz) on one day a year. The station presented news, features and music in collaboration with its sister newspaper, the St Helena Herald. It closed on 25 December 2012 to make way for a new three-channel FM service, also funded by St. Helena Government and run by the South Atlantic Media Services (formerly St. Helena Broadcasting (Guarantee) Corporation).
Saint FM provided a local radio service for the island which was also available on internet radio and relayed in Ascension Island. The station was not government funded. It was launched in January 2005 and closed on 21 December 2012. It broadcast news, features and music in collaboration with its sister newspaper, the St Helena Independent (which continues).
Saint FM Community Radio took over the radio channels vacated by Saint FM and launched on 10 March 2013. The station operates as a limited-by-guarantee company owned by its members and is registered as a fund-raising Association. Membership is open to everyone, and grants access to a live audio stream.
St Helena Online is a not-for-profit internet news service run from the UK by a former print and BBC journalist, working in partnership with Saint FM and the St Helena Independent.
Sure South Atlantic Ltd ("Sure") offers television for the island via 17 analogue terrestrial UHF channels, offering a mix of British, US, and South African programming. The channels are from DSTV and include Mnet, SuperSport and BBC channels. The feed signal, from MultiChoice DStv in South Africa, is received by a satellite dish at Bryant's Beacon from Intelsat 7 in the Ku band.
SURE provide the telecommunications service in the territory through a digital copper-based telephone network including ADSL-broadband service. In August 2011 the first fibre-optic link has been installed on the island, which connects the television receive antennas at Bryant's Beacon to the Cable & Wireless Technical Centre in the Briars.
A satellite ground station with a 7.6-metre (25 ft) satellite dish installed in 1989 at The Briars is the only international connection providing satellite links through Intelsat 707 to Ascension island and the United Kingdom. Since all international telephone and internet communications are relying on this single satellite link both internet and telephone service are subject to sun outages.
Saint Helena has the international calling code +290 which, since 2006, Tristan da Cunha shares. Saint Helena telephone numbers changed from 4 to 5 digits on 1 October 2013 by being prefixed with the digit "2", i.e. 2xxxx, with the range 5xxxx being reserved for mobile numbering, and 8xxx being used for Tristan da Cunha numbers (these still shown as 4 digits).
Saint Helena has a 10/3.6 Mbit/s internet link via Intelsat 707 provided by SURE. Serving a population of more than 4,000, this single satellite link is considered inadequate in terms of bandwidth.
ADSL-broadband service is provided with maximum speeds of up to 1536 KBit/s downstream and 512 KBit/s upstream offered on contract levels from lite £16 per month to gold+ at £190 per month. There are a few public WiFi hotspots in Jamestown, which are also being operated by SURE (formerly Cable & Wireless).
The South Atlantic Express, a 10,000 km (6,214 mi) submarine communications cable connecting Africa to South America, run by the undersea fibre optic provider eFive, will pass St Helena relatively closely. There were no plans to land the cable and install a landing station ashore, which could supply St Helena's population with sufficient bandwidth to fully leverage the benefits of today's Information Society. In January 2012, a group of supporters petitioned the UK government to meet the cost of landing the cable at St Helena. On 6 October 2012, eFive agreed to reroute the cable through St. Helena after a successful lobbying campaign by A Human Right, a San Francisco-based NGA working on initiatives to ensure all people are connected to the Internet. Islanders have sought the assistance of the UK Department for International Development and Foreign and Commonwealth Office in funding the £10m required to bridge the connection from a local junction box on the cable to the island. The UK Government have announced that a review of the island's economy would be required before such funding would be agreed to.
The island has two local newspapers, both of which are available on the Internet. The St Helena Independent has been published since November 2005. The Sentinel newspaper was introduced in 2012.
Education is free and compulsory between the ages of 5 and 16  The island has three primary schools for students of age 4 to 11: Harford, Pilling, and St Paul’s. Prince Andrew School provides secondary education for students aged 11 to 18. At the beginning of the academic year 2009-10, 230 students were enrolled in primary school and 286 in secondary school.
The Education and Employment Directorate also offers programmes for students with special needs, vocational training, adult education, evening classes, and distance learning. The island has a public library (the oldest in the Southern Hemisphere) and a mobile library service which operates weekly rural areas.
The UK national curriculum is adapted for local use. A range of qualifications are offered – from GCSE, A/S and A2, to Level 3 Diplomas and VRQ qualifications:
Sports played on the island include football, cricket, volleyball, tennis, golf, motocross, shooting sports and yachting. Saint Helena has sent teams to a number of Commonwealth Games. Saint Helena is a member of the International Island Games Association. The Saint Helena cricket team made its debut in international cricket in Division Three of the African region of the World Cricket League in 2011.
The Governor's Cup is a yacht race between Cape Town and Saint Helena island, held every two years in December/January; the most recent event was in December 2010. In Jamestown a timed run takes place up Jacob's Ladder every year, with people coming from all over the world to take part.
There are scouting and guiding groups on Saint Helena and Ascension Island. Scouting was established on Saint Helena island in 1912. Lord and Lady Baden-Powell visited the Scouts on Saint Helena on the return from their 1937 tour of Africa. The visit is described in Lord Baden-Powell's book entitled African Adventures.
On the Origin of Species, published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology. Darwin's book introduced the scientific theory that populations evolve over the course of generations through a process of natural selection. It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution. Darwin included evidence that he had gathered on the Beagle expedition in the 1830s and his subsequent findings from research, correspondence, and experimentation.
Various evolutionary ideas had already been proposed to explain new findings in biology. There was growing support for such ideas among dissident anatomists and the general public, but during the first half of the 19th century the English scientific establishment was closely tied to the Church of England, while science was part of natural theology. Ideas about the transmutation of species were controversial as they conflicted with the beliefs that species were unchanging parts of a designed hierarchy and that humans were unique, unrelated to other animals. The political and theological implications were intensely debated, but transmutation was not accepted by the scientific mainstream.
The book was written for non-specialist readers and attracted widespread interest upon its publication. As Darwin was an eminent scientist, his findings were taken seriously and the evidence he presented generated scientific, philosophical, and religious discussion. The debate over the book contributed to the campaign by T. H. Huxley and his fellow members of the X Club to secularise science by promoting scientific naturalism. Within two decades there was widespread scientific agreement that evolution, with a branching pattern of common descent, had occurred, but scientists were slow to give natural selection the significance that Darwin thought appropriate. During "the eclipse of Darwinism" from the 1880s to the 1930s, various other mechanisms of evolution were given more credit. With the development of the modern evolutionary synthesis in the 1930s and 1940s, Darwin's concept of evolutionary adaptation through natural selection became central to modern evolutionary theory, and it has now become the unifying concept of the life sciences.
In later editions of the book, Darwin traced evolutionary ideas as far back as Aristotle; the text he cites is a summary by Aristotle of the ideas of the earlier Greek philosopher Empedocles. Early Christian Church Fathers and Medieval European scholars interpreted the Genesis creation narrative allegorically rather than as a literal historical account; organisms were described by their mythological and heraldic significance as well as by their physical form. Nature was widely believed to be unstable and capricious, with monstrous births from union between species, and spontaneous generation of life.
The Protestant Reformation inspired a literal interpretation of the Bible, with concepts of creation that conflicted with the findings of an emerging science seeking explanations congruent with the mechanical philosophy of René Descartes and the empiricism of the Baconian method. After the turmoil of the English Civil War, the Royal Society wanted to show that science did not threaten religious and political stability. John Ray developed an influential natural theology of rational order; in his taxonomy, species were static and fixed, their adaptation and complexity designed by God, and varieties showed minor differences caused by local conditions. In God's benevolent design, carnivores caused mercifully swift death, but the suffering caused by parasitism was a puzzling problem. The biological classification introduced by Carl Linnaeus in 1735 also viewed species as fixed according to the divine plan. In 1766, Georges Buffon suggested that some similar species, such as horses and asses, or lions, tigers, and leopards, might be varieties descended from a common ancestor. The Ussher chronology of the 1650s had calculated creation at 4004 BC, but by the 1780s geologists assumed a much older world. Wernerians thought strata were deposits from shrinking seas, but James Hutton proposed a self-maintaining infinite cycle, anticipating uniformitarianism.
Charles Darwin's grandfather Erasmus Darwin outlined a hypothesis of transmutation of species in the 1790s, and Jean-Baptiste Lamarck published a more developed theory in 1809. Both envisaged that spontaneous generation produced simple forms of life that progressively developed greater complexity, adapting to the environment by inheriting changes in adults caused by use or disuse. This process was later called Lamarckism. Lamarck thought there was an inherent progressive tendency driving organisms continuously towards greater complexity, in parallel but separate lineages with no extinction. Geoffroy contended that embryonic development recapitulated transformations of organisms in past eras when the environment acted on embryos, and that animal structures were determined by a constant plan as demonstrated by homologies. Georges Cuvier strongly disputed such ideas, holding that unrelated, fixed species showed similarities that reflected a design for functional needs. His palæontological work in the 1790s had established the reality of extinction, which he explained by local catastrophes, followed by repopulation of the affected areas by other species.
In Britain, William Paley's Natural Theology saw adaptation as evidence of beneficial "design" by the Creator acting through natural laws. All naturalists in the two English universities (Oxford and Cambridge) were Church of England clergymen, and science became a search for these laws. Geologists adapted catastrophism to show repeated worldwide annihilation and creation of new fixed species adapted to a changed environment, initially identifying the most recent catastrophe as the biblical flood. Some anatomists such as Robert Grant were influenced by Lamarck and Geoffroy, but most naturalists regarded their ideas of transmutation as a threat to divinely appointed social order.
Darwin went to Edinburgh University in 1825 to study medicine. In his second year he neglected his medical studies for natural history and spent four months assisting Robert Grant's research into marine invertebrates. Grant revealed his enthusiasm for the transmutation of species, but Darwin rejected it. Starting in 1827, at Cambridge University, Darwin learnt science as natural theology from botanist John Stevens Henslow, and read Paley, John Herschel and Alexander von Humboldt. Filled with zeal for science, he studied catastrophist geology with Adam Sedgwick.
In December 1831, he joined the Beagle expedition as a gentleman naturalist and geologist. He read Charles Lyell's Principles of Geology and from the first stop ashore, at St. Jago, found Lyell's uniformitarianism a key to the geological history of landscapes. Darwin discovered fossils resembling huge armadillos, and noted the geographical distribution of modern species in hope of finding their "centre of creation". The three Fuegian missionaries the expedition returned to Tierra del Fuego were friendly and civilised, yet to Darwin their relatives on the island seemed "miserable, degraded savages", and he no longer saw an unbridgeable gap between humans and animals. As the Beagle neared England in 1836, he noted that species might not be fixed.
Richard Owen showed that fossils of extinct species Darwin found in South America were allied to living species on the same continent. In March 1837, ornithologist John Gould announced that Darwin's rhea was a separate species from the previously described rhea (though their territories overlapped), that mockingbirds collected on the Galápagos Islands represented three separate species each unique to a particular island, and that several distinct birds from those islands were all classified as finches. Darwin began speculating, in a series of notebooks, on the possibility that "one species does change into another" to explain these findings, and around July sketched a genealogical branching of a single evolutionary tree, discarding Lamarck's independent lineages progressing to higher forms. Unconventionally, Darwin asked questions of fancy pigeon and animal breeders as well as established scientists. At the zoo he had his first sight of an ape, and was profoundly impressed by how human the orangutan seemed.
In late September 1838, he started reading Thomas Malthus's An Essay on the Principle of Population with its statistical argument that human populations, if unrestrained, breed beyond their means and struggle to survive. Darwin related this to the struggle for existence among wildlife and botanist de Candolle's "warring of the species" in plants; he immediately envisioned "a force like a hundred thousand wedges" pushing well-adapted variations into "gaps in the economy of nature", so that the survivors would pass on their form and abilities, and unfavourable variations would be destroyed. By December 1838, he had noted a similarity between the act of breeders selecting traits and a Malthusian Nature selecting among variants thrown up by "chance" so that "every part of newly acquired structure is fully practical and perfected".
Darwin continued to research and extensively revise his theory while focusing on his main work of publishing the scientific results of the Beagle voyage. He tentatively wrote of his ideas to Lyell in January 1842; then in June he roughed out a 35-page "Pencil Sketch" of his theory. Darwin began correspondence about his theorising with the botanist Joseph Dalton Hooker in January 1844, and by July had rounded out his "sketch" into a 230-page "Essay", to be expanded with his research results and published if he died prematurely.
In November 1844, the anonymously published popular science book Vestiges of the Natural History of Creation, written by Scottish journalist Robert Chambers, widened public interest in the concept of transmutation of species. Vestiges used evidence from the fossil record and embryology to support the claim that living things had progressed from the simple to the more complex over time. But it proposed a linear progression rather than the branching common descent theory behind Darwin's work in progress, and it ignored adaptation. Darwin read it soon after publication, and scorned its amateurish geology and zoology, but he carefully reviewed his own arguments after leading scientists, including Adam Sedgwick, attacked its morality and scientific errors. Vestiges had significant influence on public opinion, and the intense debate helped to pave the way for the acceptance of the more scientifically sophisticated Origin by moving evolutionary speculation into the mainstream. While few naturalists were willing to consider transmutation, Herbert Spencer became an active proponent of Lamarckism and progressive development in the 1850s.
Darwin's barnacle studies convinced him that variation arose constantly and not just in response to changed circumstances. In 1854, he completed the last part of his Beagle-related writing and began working full-time on evolution. His thinking changed from the view that species formed in isolated populations only, as on islands, to an emphasis on speciation without isolation; that is, he saw increasing specialisation within large stable populations as continuously exploiting new ecological niches. He conducted empirical research focusing on difficulties with his theory. He studied the developmental and anatomical differences between different breeds of many domestic animals, became actively involved in fancy pigeon breeding, and experimented (with the help of his son Francis) on ways that plant seeds and animals might disperse across oceans to colonise distant islands. By 1856, his theory was much more sophisticated, with a mass of supporting evidence.
An 1855 paper on the "introduction" of species, written by Alfred Russel Wallace, claimed that patterns in the geographical distribution of living and fossil species could be explained if every new species always came into existence near an already existing, closely related species. Charles Lyell recognised the implications of Wallace's paper and its possible connection to Darwin's work, although Darwin did not, and in a letter written on 1–2 May 1856 Lyell urged Darwin to publish his theory to establish priority. Darwin was torn between the desire to set out a full and convincing account and the pressure to quickly produce a short paper. He met Lyell, and in correspondence with Joseph Dalton Hooker affirmed that he did not want to expose his ideas to review by an editor as would have been required to publish in an academic journal. He began a "sketch" account on 14 May 1856, and by July had decided to produce a full technical treatise on species. His theory including the principle of divergence was complete by 5 September 1857 when he sent Asa Gray a brief but detailed abstract of his ideas.
Darwin was hard at work on his "big book" on Natural Selection, when on 18 June 1858 he received a parcel from Wallace, who stayed on the Maluku Islands (Ternate and Gilolo). It enclosed twenty pages describing an evolutionary mechanism, a response to Darwin's recent encouragement, with a request to send it on to Lyell if Darwin thought it worthwhile. The mechanism was similar to Darwin's own theory. Darwin wrote to Lyell that "your words have come true with a vengeance, ... forestalled" and he would "of course, at once write and offer to send [it] to any journal" that Wallace chose, adding that "all my originality, whatever it may amount to, will be smashed". Lyell and Hooker agreed that a joint publication putting together Wallace's pages with extracts from Darwin's 1844 Essay and his 1857 letter to Gray should be presented at the Linnean Society, and on 1 July 1858, the papers entitled On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection, by Wallace and Darwin respectively, were read out but drew little reaction. While Darwin considered Wallace's idea to be identical to his concept of natural selection, historians have pointed out differences. Darwin described natural selection as being analogous to the artificial selection practised by animal breeders, and emphasised competition between individuals; Wallace drew no comparison to selective breeding, and focused on ecological pressures that kept different varieties adapted to local conditions. Some historians have suggested that Wallace was actually discussing group selection rather than selection acting on individual variation.
After the meeting, Darwin decided to write "an abstract of my whole work". He started work on 20 July 1858, while on holiday at Sandown, and wrote parts of it from memory. Lyell discussed arrangements with publisher John Murray III, of the publishing house John Murray, who responded immediately to Darwin's letter of 31 March 1859 with an agreement to publish the book without even seeing the manuscript, and an offer to Darwin of 2⁄3 of the profits. (eventually Murray paid £180 to Darwin for the 1st edition and by Darwin's death in 1882 the book was in its 6th edition, earning Darwin nearly £3000.)
Darwin had initially decided to call his book An abstract of an Essay on the Origin of Species and Varieties Through natural selection, but with Murray's persuasion it was eventually changed to the snappier title: On the Origin of Species, with the title page adding by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. Here the term "races" is used as an alternative for "varieties" and does not carry the modern connotation of human races—the first use in the book refers to "the several races, for instance, of the cabbage" and proceeds to a discussion of "the hereditary varieties or races of our domestic animals and plants".
Darwin had his basic theory of natural selection "by which to work" by December 1838, yet almost twenty years later, when Wallace's letter arrived on 18 June 1858, Darwin was still not ready to publish his theory. It was long thought that Darwin avoided or delayed making his ideas public for personal reasons. Reasons suggested have included fear of religious persecution or social disgrace if his views were revealed, and concern about upsetting his clergymen naturalist friends or his pious wife Emma. Charles Darwin's illness caused repeated delays. His paper on Glen Roy had proved embarrassingly wrong, and he may have wanted to be sure he was correct. David Quammen has suggested all these factors may have contributed, and notes Darwin's large output of books and busy family life during that time.
A more recent study by science historian John van Wyhe has determined that the idea that Darwin delayed publication only dates back to the 1940s, and Darwin's contemporaries thought the time he took was reasonable. Darwin always finished one book before starting another. While he was researching, he told many people about his interest in transmutation without causing outrage. He firmly intended to publish, but it was not until September 1854 that he could work on it full-time. His estimate that writing his "big book" would take five years was optimistic.
On the Origin of Species was first published on Thursday 24 November 1859, priced at fifteen shillings with a first printing of 1250 copies. The book had been offered to booksellers at Murray's autumn sale on Tuesday 22 November, and all available copies had been taken up immediately. In total, 1,250 copies were printed but after deducting presentation and review copies, and five for Stationers' Hall copyright, around 1,170 copies were available for sale. Significantly, 500 were taken by Mudie's Library, ensuring that the book promptly reached a large number of subscribers to the library. The second edition of 3,000 copies was quickly brought out on 7 January 1860, and incorporated numerous corrections as well as a response to religious objections by the addition of a new epigraph on page ii, a quotation from Charles Kingsley, and the phrase "by the Creator" added to the closing sentence. During Darwin's lifetime the book went through six editions, with cumulative changes and revisions to deal with counter-arguments raised. The third edition came out in 1861, with a number of sentences rewritten or added and an introductory appendix, An Historical Sketch of the Recent Progress of Opinion on the Origin of Species, while the fourth in 1866 had further revisions. The fifth edition, published on 10 February 1869, incorporated more changes and for the first time included the phrase "survival of the fittest", which had been coined by the philosopher Herbert Spencer in his Principles of Biology (1864).
In January 1871, George Jackson Mivart's On the Genesis of Species listed detailed arguments against natural selection, and claimed it included false metaphysics. Darwin made extensive revisions to the sixth edition of the Origin (this was the first edition in which he used the word "evolution" which had commonly been associated with embryological development, though all editions concluded with the word "evolved"), and added a new chapter VII, Miscellaneous objections, to address Mivart's arguments.
In the United States, botanist Asa Gray an American colleague of Darwin negotiated with a Boston publisher for publication of an authorised American version, but learnt that two New York publishing firms were already planning to exploit the absence of international copyright to print Origin. Darwin was delighted by the popularity of the book, and asked Gray to keep any profits. Gray managed to negotiate a 5% royalty with Appleton's of New York, who got their edition out in mid January 1860, and the other two withdrew. In a May letter, Darwin mentioned a print run of 2,500 copies, but it is not clear if this referred to the first printing only as there were four that year.
The book was widely translated in Darwin's lifetime, but problems arose with translating concepts and metaphors, and some translations were biased by the translator's own agenda. Darwin distributed presentation copies in France and Germany, hoping that suitable applicants would come forward, as translators were expected to make their own arrangements with a local publisher. He welcomed the distinguished elderly naturalist and geologist Heinrich Georg Bronn, but the German translation published in 1860 imposed Bronn's own ideas, adding controversial themes that Darwin had deliberately omitted. Bronn translated "favoured races" as "perfected races", and added essays on issues including the origin of life, as well as a final chapter on religious implications partly inspired by Bronn's adherence to Naturphilosophie. In 1862, Bronn produced a second edition based on the third English edition and Darwin's suggested additions, but then died of a heart attack. Darwin corresponded closely with Julius Victor Carus, who published an improved translation in 1867. Darwin's attempts to find a translator in France fell through, and the translation by Clémence Royer published in 1862 added an introduction praising Darwin's ideas as an alternative to religious revelation and promoting ideas anticipating social Darwinism and eugenics, as well as numerous explanatory notes giving her own answers to doubts that Darwin expressed. Darwin corresponded with Royer about a second edition published in 1866 and a third in 1870, but he had difficulty getting her to remove her notes and was troubled by these editions. He remained unsatisfied until a translation by Edmond Barbier was published in 1876. A Dutch translation by Tiberius Cornelis Winkler was published in 1860. By 1864, additional translations had appeared in Italian and Russian. In Darwin's lifetime, Origin was published in Swedish in 1871, Danish in 1872, Polish in 1873, Hungarian in 1873–1874, Spanish in 1877 and Serbian in 1878. By 1977, it had appeared in an additional 18 languages.
Page ii contains quotations by William Whewell and Francis Bacon on the theology of natural laws, harmonising science and religion in accordance with Isaac Newton's belief in a rational God who established a law-abiding cosmos. In the second edition, Darwin added an epigraph from Joseph Butler affirming that God could work through scientific laws as much as through miracles, in a nod to the religious concerns of his oldest friends. The Introduction establishes Darwin's credentials as a naturalist and author, then refers to John Herschel's letter suggesting that the origin of species "would be found to be a natural in contradistinction to a miraculous process":
Chapter I covers animal husbandry and plant breeding, going back to ancient Egypt. Darwin discusses contemporary opinions on the origins of different breeds under cultivation to argue that many have been produced from common ancestors by selective breeding. As an illustration of artificial selection, he describes fancy pigeon breeding, noting that "[t]he diversity of the breeds is something astonishing", yet all were descended from one species of rock pigeon. Darwin saw two distinct kinds of variation: (1) rare abrupt changes he called "sports" or "monstrosities" (example: ancon sheep with short legs), and (2) ubiquitous small differences (example: slightly shorter or longer bill of pigeons). Both types of hereditary changes can be used by breeders. However, for Darwin the small changes were most important in evolution.
In Chapter II, Darwin specifies that the distinction between species and varieties is arbitrary, with experts disagreeing and changing their decisions when new forms were found. He concludes that "a well-marked variety may be justly called an incipient species" and that "species are only strongly marked and permanent varieties". He argues for the ubiquity of variation in nature. Historians have noted that naturalists had long been aware that the individuals of a species differed from one another, but had generally considered such variations to be limited and unimportant deviations from the archetype of each species, that archetype being a fixed ideal in the mind of God. Darwin and Wallace made variation among individuals of the same species central to understanding the natural world.
He notes that both A. P. de Candolle and Charles Lyell had stated that all organisms are exposed to severe competition. Darwin emphasizes that he used the phrase "struggle for existence" in "a large and metaphorical sense, including dependence of one being on another"; he gives examples ranging from plants struggling against drought to plants competing for birds to eat their fruit and disseminate their seeds. He describes the struggle resulting from population growth: "It is the doctrine of Malthus applied with manifold force to the whole animal and vegetable kingdoms." He discusses checks to such increase including complex ecological interdependencies, and notes that competition is most severe between closely related forms "which fill nearly the same place in the economy of nature".
Chapter IV details natural selection under the "infinitely complex and close-fitting ... mutual relations of all organic beings to each other and to their physical conditions of life". Darwin takes as an example a country where a change in conditions led to extinction of some species, immigration of others and, where suitable variations occurred, descendants of some species became adapted to new conditions. He remarks that the artificial selection practised by animal breeders frequently produced sharp divergence in character between breeds, and suggests that natural selection might do the same, saying:
Darwin proposes sexual selection, driven by competition between males for mates, to explain sexually dimorphic features such as lion manes, deer antlers, peacock tails, bird songs, and the bright plumage of some male birds. He analysed sexual selection more fully in The Descent of Man, and Selection in Relation to Sex (1871). Natural selection was expected to work very slowly in forming new species, but given the effectiveness of artificial selection, he could "see no limit to the amount of change, to the beauty and infinite complexity of the coadaptations between all organic beings, one with another and with their physical conditions of life, which may be effected in the long course of time by nature's power of selection". Using a tree diagram and calculations, he indicates the "divergence of character" from original species into new species and genera. He describes branches falling off as extinction occurred, while new branches formed in "the great Tree of life ... with its ever branching and beautiful ramifications".
In Darwin's time there was no agreed-upon model of heredity; in Chapter I Darwin admitted, "The laws governing inheritance are quite unknown." He accepted a version of the inheritance of acquired characteristics (which after Darwin's death came to be called Lamarckism), and Chapter V discusses what he called the effects of use and disuse; he wrote that he thought "there can be little doubt that use in our domestic animals strengthens and enlarges certain parts, and disuse diminishes them; and that such modifications are inherited", and that this also applied in nature. Darwin stated that some changes that were commonly attributed to use and disuse, such as the loss of functional wings in some island dwelling insects, might be produced by natural selection. In later editions of Origin, Darwin expanded the role attributed to the inheritance of acquired characteristics. Darwin also admitted ignorance of the source of inheritable variations, but speculated they might be produced by environmental factors. However, one thing was clear: whatever the exact nature and causes of new variations, Darwin knew from observation and experiment that breeders were able to select such variations and produce huge differences in many generations of selection. The observation that selection works in domestic animals is not destroyed by lack of understanding of the underlying hereditary mechanism.
More detail was given in Darwin's 1868 book on The Variation of Animals and Plants under Domestication, which tried to explain heredity through his hypothesis of pangenesis. Although Darwin had privately questioned blending inheritance, he struggled with the theoretical difficulty that novel individual variations would tend to blend into a population. However, inherited variation could be seen, and Darwin's concept of selection working on a population with a range of small variations was workable. It was not until the modern evolutionary synthesis in the 1930s and 1940s that a model of heredity became completely integrated with a model of variation. This modern evolutionary synthesis had been dubbed Neo Darwinian Evolution because it encompasses Charles Darwin's theories of evolution with Gregor Mendel's theories of genetic inheritance.
Chapter VI begins by saying the next three chapters will address possible objections to the theory, the first being that often no intermediate forms between closely related species are found, though the theory implies such forms must have existed. As Darwin noted, "Firstly, why, if species have descended from other species by insensibly fine gradations, do we not everywhere see innumerable transitional forms? Why is not all nature in confusion, instead of the species being, as we see them, well defined?" Darwin attributed this to the competition between different forms, combined with the small number of individuals of intermediate forms, often leading to extinction of such forms. This difficulty can be referred to as the absence or rarity of transitional varieties in habitat space.
His answer was that in many cases animals exist with intermediate structures that are functional. He presented flying squirrels, and flying lemurs as examples of how bats might have evolved from non-flying ancestors. He discussed various simple eyes found in invertebrates, starting with nothing more than an optic nerve coated with pigment, as examples of how the vertebrate eye could have evolved. Darwin concludes: "If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down. But I can find out no such case."
Chapter VII (of the first edition) addresses the evolution of instincts. His examples included two he had investigated experimentally: slave-making ants and the construction of hexagonal cells by honey bees. Darwin noted that some species of slave-making ants were more dependent on slaves than others, and he observed that many ant species will collect and store the pupae of other species as food. He thought it reasonable that species with an extreme dependency on slave workers had evolved in incremental steps. He suggested that bees that make hexagonal cells evolved in steps from bees that made round cells, under pressure from natural selection to economise wax. Darwin concluded:
Chapter VIII addresses the idea that species had special characteristics that prevented hybrids from being fertile in order to preserve separately created species. Darwin said that, far from being constant, the difficulty in producing hybrids of related species, and the viability and fertility of the hybrids, varied greatly, especially among plants. Sometimes what were widely considered to be separate species produced fertile hybrid offspring freely, and in other cases what were considered to be mere varieties of the same species could only be crossed with difficulty. Darwin concluded: "Finally, then, the facts briefly given in this chapter do not seem to me opposed to, but even rather to support the view, that there is no fundamental distinction between species and varieties."
In the sixth edition Darwin inserted a new chapter VII (renumbering the subsequent chapters) to respond to criticisms of earlier editions, including the objection that many features of organisms were not adaptive and could not have been produced by natural selection. He said some such features could have been by-products of adaptive changes to other features, and that often features seemed non-adaptive because their function was unknown, as shown by his book on Fertilisation of Orchids that explained how their elaborate structures facilitated pollination by insects. Much of the chapter responds to George Jackson Mivart's criticisms, including his claim that features such as baleen filters in whales, flatfish with both eyes on one side and the camouflage of stick insects could not have evolved through natural selection because intermediate stages would not have been adaptive. Darwin proposed scenarios for the incremental evolution of each feature.
Chapter IX deals with the fact that the geologic record appears to show forms of life suddenly arising, without the innumerable transitional fossils expected from gradual changes. Darwin borrowed Charles Lyell's argument in Principles of Geology that the record is extremely imperfect as fossilisation is a very rare occurrence, spread over vast periods of time; since few areas had been geologically explored, there could only be fragmentary knowledge of geological formations, and fossil collections were very poor. Evolved local varieties which migrated into a wider area would seem to be the sudden appearance of a new species. Darwin did not expect to be able to reconstruct evolutionary history, but continuing discoveries gave him well founded hope that new finds would occasionally reveal transitional forms. To show that there had been enough time for natural selection to work slowly, he again cited Principles of Geology and other observations based on sedimentation and erosion, including an estimate that erosion of The Weald had taken 300 million years. The initial appearance of entire groups of well developed organisms in the oldest fossil-bearing layers, now known as the Cambrian explosion, posed a problem. Darwin had no doubt that earlier seas had swarmed with living creatures, but stated that he had no satisfactory explanation for the lack of fossils. Fossil evidence of pre-Cambrian life has since been found, extending the history of life back for billions of years.
Chapter X examines whether patterns in the fossil record are better explained by common descent and branching evolution through natural selection, than by the individual creation of fixed species. Darwin expected species to change slowly, but not at the same rate – some organisms such as Lingula were unchanged since the earliest fossils. The pace of natural selection would depend on variability and change in the environment. This distanced his theory from Lamarckian laws of inevitable progress. It has been argued that this anticipated the punctuated equilibrium hypothesis, but other scholars have preferred to emphasise Darwin's commitment to gradualism. He cited Richard Owen's findings that the earliest members of a class were a few simple and generalised species with characteristics intermediate between modern forms, and were followed by increasingly diverse and specialised forms, matching the branching of common descent from an ancestor. Patterns of extinction matched his theory, with related groups of species having a continued existence until extinction, then not reappearing. Recently extinct species were more similar to living species than those from earlier eras, and as he had seen in South America, and William Clift had shown in Australia, fossils from recent geological periods resembled species still living in the same area.
Chapter XI deals with evidence from biogeography, starting with the observation that differences in flora and fauna from separate regions cannot be explained by environmental differences alone; South America, Africa, and Australia all have regions with similar climates at similar latitudes, but those regions have very different plants and animals. The species found in one area of a continent are more closely allied with species found in other regions of that same continent than to species found on other continents. Darwin noted that barriers to migration played an important role in the differences between the species of different regions. The coastal sea life of the Atlantic and Pacific sides of Central America had almost no species in common even though the Isthmus of Panama was only a few miles wide. His explanation was a combination of migration and descent with modification. He went on to say: "On this principle of inheritance with modification, we can understand how it is that sections of genera, whole genera, and even families are confined to the same areas, as is so commonly and notoriously the case." Darwin explained how a volcanic island formed a few hundred miles from a continent might be colonised by a few species from that continent. These species would become modified over time, but would still be related to species found on the continent, and Darwin observed that this was a common pattern. Darwin discussed ways that species could be dispersed across oceans to colonise islands, many of which he had investigated experimentally.
Darwin discusses morphology, including the importance of homologous structures. He says, "What can be more curious than that the hand of a man, formed for grasping, that of a mole for digging, the leg of the horse, the paddle of the porpoise, and the wing of the bat, should all be constructed on the same pattern, and should include the same bones, in the same relative positions?" He notes that animals of the same class often have extremely similar embryos. Darwin discusses rudimentary organs, such as the wings of flightless birds and the rudiments of pelvis and leg bones found in some snakes. He remarks that some rudimentary organs, such as teeth in baleen whales, are found only in embryonic stages.
The final chapter reviews points from earlier chapters, and Darwin concludes by hoping that his theory might produce revolutionary changes in many fields of natural history. Although he avoids the controversial topic of human origins in the rest of the book so as not to prejudice readers against his theory, here he ventures a cautious hint that psychology would be put on a new foundation and that "Light will be thrown on the origin of man". Darwin ends with a passage that became well known and much quoted:
Darwin's aims were twofold: to show that species had not been separately created, and to show that natural selection had been the chief agent of change. He knew that his readers were already familiar with the concept of transmutation of species from Vestiges, and his introduction ridicules that work as failing to provide a viable mechanism. Therefore, the first four chapters lay out his case that selection in nature, caused by the struggle for existence, is analogous to the selection of variations under domestication, and that the accumulation of adaptive variations provides a scientifically testable mechanism for evolutionary speciation.
Later chapters provide evidence that evolution has occurred, supporting the idea of branching, adaptive evolution without directly proving that selection is the mechanism. Darwin presents supporting facts drawn from many disciplines, showing that his theory could explain a myriad of observations from many fields of natural history that were inexplicable under the alternate concept that species had been individually created. The structure of Darwin's argument showed the influence of John Herschel, whose philosophy of science maintained that a mechanism could be called a vera causa (true cause) if three things could be demonstrated: its existence in nature, its ability to produce the effects of interest, and its ability to explain a wide range of observations.
While the book was readable enough to sell, its dryness ensured that it was seen as aimed at specialist scientists and could not be dismissed as mere journalism or imaginative fiction. Unlike the still-popular Vestiges, it avoided the narrative style of the historical novel and cosmological speculation, though the closing sentence clearly hinted at cosmic progression. Darwin had long been immersed in the literary forms and practices of specialist science, and made effective use of his skills in structuring arguments. David Quammen has described the book as written in everyday language for a wide audience, but noted that Darwin's literary style was uneven: in some places he used convoluted sentences that are difficult to read, while in other places his writing was beautiful. Quammen advised that later editions were weakened by Darwin making concessions and adding details to address his critics, and recommended the first edition. James T. Costa said that because the book was an abstract produced in haste in response to Wallace's essay, it was more approachable than the big book on natural selection Darwin had been working on, which would have been encumbered by scholarly footnotes and much more technical detail. He added that some parts of Origin are dense, but other parts are almost lyrical, and the case studies and observations are presented in a narrative style unusual in serious scientific books, which broadened its audience.
The book aroused international interest and a widespread debate, with no sharp line between scientific issues and ideological, social and religious implications. Much of the initial reaction was hostile, but Darwin had to be taken seriously as a prominent and respected name in science. There was much less controversy than had greeted the 1844 publication Vestiges of Creation, which had been rejected by scientists, but had influenced a wide public readership into believing that nature and human society were governed by natural laws. The Origin of Species as a book of wide general interest became associated with ideas of social reform. Its proponents made full use of a surge in the publication of review journals, and it was given more popular attention than almost any other scientific work, though it failed to match the continuing sales of Vestiges. Darwin's book legitimised scientific discussion of evolutionary mechanisms, and the newly coined term Darwinism was used to cover the whole range of evolutionism, not just his own ideas. By the mid-1870s, evolutionism was triumphant.
Scientific readers were already aware of arguments that species changed through processes that were subject to laws of nature, but the transmutational ideas of Lamarck and the vague "law of development" of Vestiges had not found scientific favour. Darwin presented natural selection as a scientifically testable mechanism while accepting that other mechanisms such as inheritance of acquired characters were possible. His strategy established that evolution through natural laws was worthy of scientific study, and by 1875, most scientists accepted that evolution occurred but few thought natural selection was significant. Darwin's scientific method was also disputed, with his proponents favouring the empiricism of John Stuart Mill's A System of Logic, while opponents held to the idealist school of William Whewell's Philosophy of the Inductive Sciences, in which investigation could begin with the intuitive truth that species were fixed objects created by design. Early support for Darwin's ideas came from the findings of field naturalists studying biogeography and ecology, including Joseph Dalton Hooker in 1860, and Asa Gray in 1862. Henry Walter Bates presented research in 1861 that explained insect mimicry using natural selection. Alfred Russel Wallace discussed evidence from his Malay archipelago research, including an 1864 paper with an evolutionary explanation for the Wallace line.
Evolution had less obvious applications to anatomy and morphology, and at first had little impact on the research of the anatomist Thomas Henry Huxley. Despite this, Huxley strongly supported Darwin on evolution; though he called for experiments to show whether natural selection could form new species, and questioned if Darwin's gradualism was sufficient without sudden leaps to cause speciation. Huxley wanted science to be secular, without religious interference, and his article in the April 1860 Westminster Review promoted scientific naturalism over natural theology, praising Darwin for "extending the domination of Science over regions of thought into which she has, as yet, hardly penetrated" and coining the term "Darwinism" as part of his efforts to secularise and professionalise science. Huxley gained influence, and initiated the X Club, which used the journal Nature to promote evolution and naturalism, shaping much of late Victorian science. Later, the German morphologist Ernst Haeckel would convince Huxley that comparative anatomy and palaeontology could be used to reconstruct evolutionary genealogies.
The leading naturalist in Britain was the anatomist Richard Owen, an idealist who had shifted to the view in the 1850s that the history of life was the gradual unfolding of a divine plan. Owen's review of the Origin in the April 1860 Edinburgh Review bitterly attacked Huxley, Hooker and Darwin, but also signalled acceptance of a kind of evolution as a teleological plan in a continuous "ordained becoming", with new species appearing by natural birth. Others that rejected natural selection, but supported "creation by birth", included the Duke of Argyll who explained beauty in plumage by design. Since 1858, Huxley had emphasised anatomical similarities between apes and humans, contesting Owen's view that humans were a separate sub-class. Their disagreement over human origins came to the fore at the British Association for the Advancement of Science meeting featuring the legendary 1860 Oxford evolution debate. In two years of acrimonious public dispute that Charles Kingsley satirised as the "Great Hippocampus Question" and parodied in The Water-Babies as the "great hippopotamus test", Huxley showed that Owen was incorrect in asserting that ape brains lacked a structure present in human brains. Others, including Charles Lyell and Alfred Russel Wallace, thought that humans shared a common ancestor with apes, but higher mental faculties could not have evolved through a purely material process. Darwin published his own explanation in the Descent of Man (1871).
Evolutionary ideas, although not natural selection, were accepted by German biologists accustomed to ideas of homology in morphology from Goethe's Metamorphosis of Plants and from their long tradition of comparative anatomy. Bronn's alterations in his German translation added to the misgivings of conservatives, but enthused political radicals. Ernst Haeckel was particularly ardent, aiming to synthesise Darwin's ideas with those of Lamarck and Goethe while still reflecting the spirit of Naturphilosophie. Their ambitious programme to reconstruct the evolutionary history of life was joined by Huxley and supported by discoveries in palaeontology. Haeckel used embryology extensively in his recapitulation theory, which embodied a progressive, almost linear model of evolution. Darwin was cautious about such histories, and had already noted that von Baer's laws of embryology supported his idea of complex branching.
French-speaking naturalists in several countries showed appreciation of the much modified French translation by Clémence Royer, but Darwin's ideas had little impact in France, where any scientists supporting evolutionary ideas opted for a form of Lamarckism. The intelligentsia in Russia had accepted the general phenomenon of evolution for several years before Darwin had published his theory, and scientists were quick to take it into account, although the Malthusian aspects were felt to be relatively unimportant. The political economy of struggle was criticised as a British stereotype by Karl Marx and by Leo Tolstoy, who had the character Levin in his novel Anna Karenina voice sharp criticism of the morality of Darwin's views.
There were serious scientific objections to the process of natural selection as the key mechanism of evolution, including Karl von Nägeli's insistence that a trivial characteristic with no adaptive advantage could not be developed by selection. Darwin conceded that these could be linked to adaptive characteristics. His estimate that the age of the Earth allowed gradual evolution was disputed by William Thomson (later awarded the title Lord Kelvin), who calculated that it had cooled in less than 100 million years. Darwin accepted blending inheritance, but Fleeming Jenkin calculated that as it mixed traits, natural selection could not accumulate useful traits. Darwin tried to meet these objections in the 5th edition. Mivart supported directed evolution, and compiled scientific and religious objections to natural selection. In response, Darwin made considerable changes to the sixth edition. The problems of the age of the Earth and heredity were only resolved in the 20th century.
By the mid-1870s, most scientists accepted evolution, but relegated natural selection to a minor role as they believed evolution was purposeful and progressive. The range of evolutionary theories during "the eclipse of Darwinism" included forms of "saltationism" in which new species were thought to arise through "jumps" rather than gradual adaptation, forms of orthogenesis claiming that species had an inherent tendency to change in a particular direction, and forms of neo-Lamarckism in which inheritance of acquired characteristics led to progress. The minority view of August Weismann, that natural selection was the only mechanism, was called neo-Darwinism. It was thought that the rediscovery of Mendelian inheritance invalidated Darwin's views.
While some, like Spencer, used analogy from natural selection as an argument against government intervention in the economy to benefit the poor, others, including Alfred Russel Wallace, argued that action was needed to correct social and economic inequities to level the playing field before natural selection could improve humanity further. Some political commentaries, including Walter Bagehot's Physics and Politics (1872), attempted to extend the idea of natural selection to competition between nations and between human races. Such ideas were incorporated into what was already an ongoing effort by some working in anthropology to provide scientific evidence for the superiority of Caucasians over non white races and justify European imperialism. Historians write that most such political and economic commentators had only a superficial understanding of Darwin's scientific theory, and were as strongly influenced by other concepts about social progress and evolution, such as the Lamarckian ideas of Spencer and Haeckel, as they were by Darwin's work. Darwin objected to his ideas being used to justify military aggression and unethical business practices as he believed morality was part of fitness in humans, and he opposed polygenism, the idea that human races were fundamentally distinct and did not share a recent common ancestry.
Natural theology was not a unified doctrine, and while some such as Louis Agassiz were strongly opposed to the ideas in the book, others sought a reconciliation in which evolution was seen as purposeful. In the Church of England, some liberal clergymen interpreted natural selection as an instrument of God's design, with the cleric Charles Kingsley seeing it as "just as noble a conception of Deity". In the second edition of January 1860, Darwin quoted Kingsley as "a celebrated cleric", and added the phrase "by the Creator" to the closing sentence, which from then on read "life, with its several powers, having been originally breathed by the Creator into a few forms or into one". While some commentators have taken this as a concession to religion that Darwin later regretted, Darwin's view at the time was of God creating life through the laws of nature, and even in the first edition there are several references to "creation".
Baden Powell praised "Mr Darwin's masterly volume [supporting] the grand principle of the self-evolving powers of nature". In America, Asa Gray argued that evolution is the secondary effect, or modus operandi, of the first cause, design, and published a pamphlet defending the book in terms of theistic evolution, Natural Selection is not inconsistent with Natural Theology. Theistic evolution became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured over natural selection as being more compatible with purpose.
Even though the book had barely hinted at human evolution, it quickly became central to the debate as mental and moral qualities were seen as spiritual aspects of the immaterial soul, and it was believed that animals did not have spiritual qualities. This conflict could be reconciled by supposing there was some supernatural intervention on the path leading to humans, or viewing evolution as a purposeful and progressive ascent to mankind's position at the head of nature. While many conservative theologians accepted evolution, Charles Hodge argued in his 1874 critique "What is Darwinism?" that "Darwinism", defined narrowly as including rejection of design, was atheism though he accepted that Asa Gray did not reject design. Asa Gray responded that this charge misrepresented Darwin's text. By the early 20th century, four noted authors of The Fundamentals were explicitly open to the possibility that God created through evolution, but fundamentalism inspired the American creation–evolution controversy that began in the 1920s. Some conservative Roman Catholic writers and influential Jesuits opposed evolution in the late 19th and early 20th century, but other Catholic writers, starting with Mivart, pointed out that early Church Fathers had not interpreted Genesis literally in this area. The Vatican stated its official position in a 1950 papal encyclical, which held that evolution was not inconsistent with Catholic teaching.
Modern evolutionary theory continues to develop. Darwin's theory of evolution by natural selection, with its tree-like model of branching common descent, has become the unifying theory of the life sciences. The theory explains the diversity of living organisms and their adaptation to the environment. It makes sense of the geologic record, biogeography, parallels in embryonic development, biological homologies, vestigiality, cladistics, phylogenetics and other fields, with unrivalled explanatory power; it has also become essential to applied sciences such as medicine and agriculture. Despite the scientific consensus, a religion-based political controversy has developed over how evolution is taught in schools, especially in the United States.
Interest in Darwin's writings continues, and scholars have generated an extensive literature, the Darwin Industry, about his life and work. The text of Origin itself has been subject to much analysis including a variorum, detailing the changes made in every edition, first published in 1959, and a concordance, an exhaustive external index published in 1981. Worldwide commemorations of the 150th anniversary of the publication of On the Origin of Species and the bicentenary of Darwin's birth were scheduled for 2009. They celebrated the ideas which "over the last 150 years have revolutionised our understanding of nature and our place within it".
The phrase "in whole or in part" has been subject to much discussion by scholars of international humanitarian law. The International Criminal Tribunal for the Former Yugoslavia found in Prosecutor v. Radislav Krstic – Trial Chamber I – Judgment – IT-98-33 (2001) ICTY8 (2 August 2001) that Genocide had been committed. In Prosecutor v. Radislav Krstic – Appeals Chamber – Judgment – IT-98-33 (2004) ICTY 7 (19 April 2004) paragraphs 8, 9, 10, and 11 addressed the issue of in part and found that "the part must be a substantial part of that group. The aim of the Genocide Convention is to prevent the intentional destruction of entire human groups, and the part targeted must be significant enough to have an impact on the group as a whole." The Appeals Chamber goes into details of other cases and the opinions of respected commentators on the Genocide Convention to explain how they came to this conclusion.
In the same judgement the ECHR reviewed the judgements of several international and municipal courts judgements. It noted that International Criminal Tribunal for the Former Yugoslavia and the International Court of Justice had agreed with the narrow interpretation, that biological-physical destruction was necessary for an act to qualify as genocide. The ECHR also noted that at the time of its judgement, apart from courts in Germany which had taken a broad view, that there had been few cases of genocide under other Convention States municipal laws and that "There are no reported cases in which the courts of these States have defined the type of group destruction the perpetrator must have intended in order to be found guilty of genocide".
After the Holocaust, which had been perpetrated by the Nazi Germany and its allies prior to and during World War II, Lemkin successfully campaigned for the universal acceptance of international laws defining and forbidding genocides. In 1946, the first session of the United Nations General Assembly adopted a resolution that "affirmed" that genocide was a crime under international law, but did not provide a legal definition of the crime. In 1948, the UN General Assembly adopted the Convention on the Prevention and Punishment of the Crime of Genocide (CPPCG) which defined the crime of genocide for the first time.
The first draft of the Convention included political killings, but these provisions were removed in a political and diplomatic compromise following objections from some countries, including the USSR, a permanent security council member. The USSR argued that the Convention's definition should follow the etymology of the term, and may have feared greater international scrutiny of its own Great Purge. Other nations feared that including political groups in the definition would invite international intervention in domestic politics. However leading genocide scholar William Schabas states: “Rigorous examination of the travaux fails to confirm a popular impression in the literature that the opposition to inclusion of political genocide was some Soviet machination. The Soviet views were also shared by a number of other States for whom it is difficult to establish any geographic or social common denominator: Lebanon, Sweden, Brazil, Peru, Venezuela, the Philippines, the Dominican Republic, Iran, Egypt, Belgium, and Uruguay. The exclusion of political groups was in fact originally promoted by a non-governmental organization, the World Jewish Congress, and it corresponded to Raphael Lemkin’s vision of the nature of the crime of genocide.” 
In 2007 the European Court of Human Rights (ECHR), noted in its judgement on Jorgic v. Germany case that in 1992 the majority of legal scholars took the narrow view that "intent to destroy" in the CPPCG meant the intended physical-biological destruction of the protected group and that this was still the majority opinion. But the ECHR also noted that a minority took a broader view and did not consider biological-physical destruction was necessary as the intent to destroy a national, racial, religious or ethnic group was enough to qualify as genocide.
The word genocide was later included as a descriptive term to the process of indictment, but not yet as a formal legal term According to Lemming, genocide was defined as "a coordinated strategy to destroy a group of people, a process that could be accomplished through total annihilation as well as strategies that eliminate key elements of the group's basic existence, including language, culture, and economic infrastructure.” He created a concept of mobilizing much of the international relations and community, to working together and preventing the occurrence of such events happening within history and the international society. Australian anthropologist Peg LeVine coined the term "ritualcide" to describe the destruction of a group's cultural identity without necessarily destroying its members.
The study of genocide has mainly been focused towards the legal aspect of the term. By formally recognizing the act of genocide as a crime, involves the undergoing prosecution that begins with not only seeing genocide as outrageous past any moral standpoint but also may be a legal liability within international relations. When genocide is looked at in a general aspect it is viewed as the deliberate killing of a certain group. Yet is commonly seen to escape the process of trial and prosecution due to the fact that genocide is more often than not committed by the officials in power of a state or area. In 1648 before the term genocide had been coined, the Peace of Westphalia was established to protect ethnic, national, racial and in some instances religious groups. During the 19th century humanitarian intervention was needed due to the fact of conflict and justification of some of the actions executed by the military.
Genocide has become an official term used in international relations. The word genocide was not in use before 1944. Before this, in 1941, Winston Churchill described the mass killing of Russian prisoners of war and civilians as "a crime without a name". In that year, a Polish-Jewish lawyer named Raphael Lemkin, described the policies of systematic murder founded by the Nazis as genocide. The word genocide is the combination of the Greek prefix geno- (meaning tribe or race) and caedere (the Latin word for to kill). The word is defined as a specific set of violent crimes that are committed against a certain group with the attempt to remove the entire group from existence or to destroy them.
The judges continue in paragraph 12, "The determination of when the targeted part is substantial enough to meet this requirement may involve a number of considerations. The numeric size of the targeted part of the group is the necessary and important starting point, though not in all cases the ending point of the inquiry. The number of individuals targeted should be evaluated not only in absolute terms, but also in relation to the overall size of the entire group. In addition to the numeric size of the targeted portion, its prominence within the group can be a useful consideration. If a specific part of the group is emblematic of the overall group, or is essential to its survival, that may support a finding that the part qualifies as substantial within the meaning of Article 4 [of the Tribunal's Statute]."
In paragraph 13 the judges raise the issue of the perpetrators' access to the victims: "The historical examples of genocide also suggest that the area of the perpetrators’ activity and control, as well as the possible extent of their reach, should be considered. ... The intent to destroy formed by a perpetrator of genocide will always be limited by the opportunity presented to him. While this factor alone will not indicate whether the targeted group is substantial, it can—in combination with other factors—inform the analysis."
The Convention came into force as international law on 12 January 1951 after the minimum 20 countries became parties. At that time however, only two of the five permanent members of the UN Security Council were parties to the treaty: France and the Republic of China. The Soviet Union ratified in 1954, the United Kingdom in 1970, the People's Republic of China in 1983 (having replaced the Taiwan-based Republic of China on the UNSC in 1971), and the United States in 1988. This long delay in support for the Convention by the world's most powerful nations caused the Convention to languish for over four decades. Only in the 1990s did the international law on the crime of genocide begin to be enforced.
Writing in 1998 Kurt Jonassohn and Karin Björnson stated that the CPPCG was a legal instrument resulting from a diplomatic compromise. As such the wording of the treaty is not intended to be a definition suitable as a research tool, and although it is used for this purpose, as it has an international legal credibility that others lack, other definitions have also been postulated. Jonassohn and Björnson go on to say that none of these alternative definitions have gained widespread support for various reasons.
Jonassohn and Björnson postulate that the major reason why no single generally accepted genocide definition has emerged is because academics have adjusted their focus to emphasise different periods and have found it expedient to use slightly different definitions to help them interpret events. For example, Frank Chalk and Kurt Jonassohn studied the whole of human history, while Leo Kuper and R. J. Rummel in their more recent works concentrated on the 20th century, and Helen Fein, Barbara Harff and Ted Gurr have looked at post World War II events. Jonassohn and Björnson are critical of some of these studies, arguing that they are too expansive, and conclude that the academic discipline of genocide studies is too young to have a canon of work on which to build an academic paradigm.
The exclusion of social and political groups as targets of genocide in the CPPCG legal definition has been criticized by some historians and sociologists, for example M. Hassan Kakar in his book The Soviet Invasion and the Afghan Response, 1979–1982 argues that the international definition of genocide is too restricted, and that it should include political groups or any group so defined by the perpetrator and quotes Chalk and Jonassohn: "Genocide is a form of one-sided mass killing in which a state or other authority intends to destroy a group, as that group and membership in it are defined by the perpetrator." While there are various definitions of the term, Adam Jones states that the majority of genocide scholars consider that "intent to destroy" is a requirement for any act to be labelled genocide, and that there is growing agreement on the inclusion of the physical destruction criterion.
Barbara Harff and Ted Gurr defined genocide as "the promotion and execution of policies by a state or its agents which result in the deaths of a substantial portion of a group ...[when] the victimized groups are defined primarily in terms of their communal characteristics, i.e., ethnicity, religion or nationality." Harff and Gurr also differentiate between genocides and politicides by the characteristics by which members of a group are identified by the state. In genocides, the victimized groups are defined primarily in terms of their communal characteristics, i.e., ethnicity, religion or nationality. In politicides the victim groups are defined primarily in terms of their hierarchical position or political opposition to the regime and dominant groups. Daniel D. Polsby and Don B. Kates, Jr. state that "... we follow Harff's distinction between genocides and 'pogroms,' which she describes as 'short-lived outbursts by mobs, which, although often condoned by authorities, rarely persist.' If the violence persists for long enough, however, Harff argues, the distinction between condonation and complicity collapses."
According to R. J. Rummel, genocide has 3 different meanings. The ordinary meaning is murder by government of people due to their national, ethnic, racial, or religious group membership. The legal meaning of genocide refers to the international treaty, the Convention on the Prevention and Punishment of the Crime of Genocide. This also includes non-killings that in the end eliminate the group, such as preventing births or forcibly transferring children out of the group to another group. A generalized meaning of genocide is similar to the ordinary meaning but also includes government killings of political opponents or otherwise intentional murder. It is to avoid confusion regarding what meaning is intended that Rummel created the term democide for the third meaning.
Highlighting the potential for state and non-state actors to commit genocide in the 21st century, for example, in failed states or as non-state actors acquire weapons of mass destruction, Adrian Gallagher defined genocide as 'When a source of collective power (usually a state) intentionally uses its power base to implement a process of destruction in order to destroy a group (as defined by the perpetrator), in whole or in substantial part, dependent upon relative group size'. The definition upholds the centrality of intent, the multidimensional understanding of destroy, broadens the definition of group identity beyond that of the 1948 definition yet argues that a substantial part of a group has to be destroyed before it can be classified as genocide (dependent on relative group size).
All signatories to the CPPCG are required to prevent and punish acts of genocide, both in peace and wartime, though some barriers make this enforcement difficult. In particular, some of the signatories—namely, Bahrain, Bangladesh, India, Malaysia, the Philippines, Singapore, the United States, Vietnam, Yemen, and former Yugoslavia—signed with the proviso that no claim of genocide could be brought against them at the International Court of Justice without their consent. Despite official protests from other signatories (notably Cyprus and Norway) on the ethics and legal standing of these reservations, the immunity from prosecution they grant has been invoked from time to time, as when the United States refused to allow a charge of genocide brought against it by former Yugoslavia following the 1999 Kosovo War.
Because the universal acceptance of international laws which in 1948 defined and forbade genocide with the promulgation of the Convention on the Prevention and Punishment of the Crime of Genocide (CPPCG), those criminals who were prosecuted after the war in international courts for taking part in the Holocaust were found guilty of crimes against humanity and other more specific crimes like murder. Nevertheless, the Holocaust is universally recognized to have been a genocide and the term, that had been coined the year before by Raphael Lemkin, appeared in the indictment of the 24 Nazi leaders, Count 3, which stated that all the defendants had "conducted deliberate and systematic genocide—namely, the extermination of racial and national groups..."
On 12 July 2007, European Court of Human Rights when dismissing the appeal by Nikola Jorgić against his conviction for genocide by a German court (Jorgic v. Germany) noted that the German courts wider interpretation of genocide has since been rejected by international courts considering similar cases. The ECHR also noted that in the 21st century "Amongst scholars, the majority have taken the view that ethnic cleansing, in the way in which it was carried out by the Serb forces in Bosnia and Herzegovina in order to expel Muslims and Croats from their homes, did not constitute genocide. However, there are also a considerable number of scholars who have suggested that these acts did amount to genocide, and the ICTY has found in the Momcilo Krajisnik case that the actus reu, of genocide was met in Prijedor "With regard to the charge of genocide, the Chamber found that in spite of evidence of acts perpetrated in the municipalities which constituted the actus reus of genocide".
About 30 people have been indicted for participating in genocide or complicity in genocide during the early 1990s in Bosnia. To date, after several plea bargains and some convictions that were successfully challenged on appeal two men, Vujadin Popović and Ljubiša Beara, have been found guilty of committing genocide, Zdravko Tolimir has been found guilty of committing genocide and conspiracy to commit genocide, and two others, Radislav Krstić and Drago Nikolić, have been found guilty of aiding and abetting genocide. Three others have been found guilty of participating in genocides in Bosnia by German courts, one of whom Nikola Jorgić lost an appeal against his conviction in the European Court of Human Rights. A further eight men, former members of the Bosnian Serb security forces were found guilty of genocide by the State Court of Bosnia and Herzegovina (See List of Bosnian genocide prosecutions).
Slobodan Milošević, as the former President of Serbia and of Yugoslavia, was the most senior political figure to stand trial at the ICTY. He died on 11 March 2006 during his trial where he was accused of genocide or complicity in genocide in territories within Bosnia and Herzegovina, so no verdict was returned. In 1995, the ICTY issued a warrant for the arrest of Bosnian Serbs Radovan Karadžić and Ratko Mladić on several charges including genocide. On 21 July 2008, Karadžić was arrested in Belgrade, and he is currently in The Hague on trial accused of genocide among other crimes. Ratko Mladić was arrested on 26 May 2011 by Serbian special police in Lazarevo, Serbia. Karadzic was convicted of ten of the eleven charges laid against him and sentenced to 40 years in prison on March 24 2016.
The International Criminal Tribunal for Rwanda (ICTR) is a court under the auspices of the United Nations for the prosecution of offenses committed in Rwanda during the genocide which occurred there during April 1994, commencing on 6 April. The ICTR was created on 8 November 1994 by the Security Council of the United Nations in order to judge those people responsible for the acts of genocide and other serious violations of the international law performed in the territory of Rwanda, or by Rwandan citizens in nearby states, between 1 January and 31 December 1994.
There has been much debate over categorizing the situation in Darfur as genocide. The ongoing conflict in Darfur, Sudan, which started in 2003, was declared a "genocide" by United States Secretary of State Colin Powell on 9 September 2004 in testimony before the Senate Foreign Relations Committee. Since that time however, no other permanent member of the UN Security Council followed suit. In fact, in January 2005, an International Commission of Inquiry on Darfur, authorized by UN Security Council Resolution 1564 of 2004, issued a report to the Secretary-General stating that "the Government of the Sudan has not pursued a policy of genocide." Nevertheless, the Commission cautioned that "The conclusion that no genocidal policy has been pursued and implemented in Darfur by the Government authorities, directly or through the militias under their control, should not be taken in any way as detracting from the gravity of the crimes perpetrated in that region. International offences such as the crimes against humanity and war crimes that have been committed in Darfur may be no less serious and heinous than genocide."
In March 2005, the Security Council formally referred the situation in Darfur to the Prosecutor of the International Criminal Court, taking into account the Commission report but without mentioning any specific crimes. Two permanent members of the Security Council, the United States and China, abstained from the vote on the referral resolution. As of his fourth report to the Security Council, the Prosecutor has found "reasonable grounds to believe that the individuals identified [in the UN Security Council Resolution 1593] have committed crimes against humanity and war crimes," but did not find sufficient evidence to prosecute for genocide.
Other authors have focused on the structural conditions leading up to genocide and the psychological and social processes that create an evolution toward genocide. Ervin Staub showed that economic deterioration and political confusion and disorganization were starting points of increasing discrimination and violence in many instances of genocides and mass killing. They lead to scapegoating a group and ideologies that identified that group as an enemy. A history of devaluation of the group that becomes the victim, past violence against the group that becomes the perpetrator leading to psychological wounds, authoritarian cultures and political systems, and the passivity of internal and external witnesses (bystanders) all contribute to the probability that the violence develops into genocide. Intense conflict between groups that is unresolved, becomes intractable and violent can also lead to genocide. The conditions that lead to genocide provide guidance to early prevention, such as humanizing a devalued group, creating ideologies that embrace all groups, and activating bystander responses. There is substantial research to indicate how this can be done, but information is only slowly transformed into action.
Pope Saint John XXIII (Latin: Ioannes XXIII; Italian: Giovanni XXIII) born Angelo Giuseppe Roncalli,[a] Italian pronunciation: [ˈandʒelo dʒuˈzɛppe roŋˈkalli]; 25 November 1881 – 3 June 1963) reigned as Pope from 28 October 1958 to his death in 1963 and was canonized on 27 April 2014. Angelo Giuseppe Roncalli was the fourth of fourteen children born to a family of sharecroppers who lived in a village in Lombardy. He was ordained to the priesthood on 10 August 1904 and served in a number of posts, including papal nuncio in France and a delegate to Bulgaria, Greece and Turkey. In a consistory on 12 January 1953 Pope Pius XII made Roncalli a cardinal as the Cardinal-Priest of Santa Prisca in addition to naming him as the Patriarch of Venice.
Roncalli was elected pope on 28 October 1958 at age 76 after 11 ballots. His selection was unexpected, and Roncalli himself had come to Rome with a return train ticket to Venice. He was the first pope to take the pontifical name of "John" upon election in more than 500 years, and his choice settled the complicated question of official numbering attached to this papal name due to the antipope of this name. Pope John XXIII surprised those who expected him to be a caretaker pope by calling the historic Second Vatican Council (1962–65), the first session opening on 11 October 1962. His passionate views on equality were summed up in his famous statement, "We were all made in God's image, and thus, we are all Godly alike." John XXIII made many passionate speeches during his pontificate, one of which was on the day that he opened the Second Vatican Council in the middle of the night to the crowd gathered in St. Peter's Square: "Dear children, returning home, you will find children: give your children a hug and say: This is a hug from the Pope!"
Pope John XXIII did not live to see the Vatican Council to completion. He died of stomach cancer on 3 June 1963, four and a half years after his election and two months after the completion of his final and famed encyclical, Pacem in terris. He was buried in the Vatican grottoes beneath Saint Peter's Basilica on 6 June 1963 and his cause for canonization was opened on 18 November 1965 by his successor, Pope Paul VI, who declared him a Servant of God. In addition to being named Venerable on 20 December 1999, he was beatified on 3 September 2000 by Pope John Paul II alongside Pope Pius IX and three others. Following his beatification, his body was moved on 3 June 2001 from its original place to the altar of Saint Jerome where it could be seen by the faithful. On 5 July 2013, Pope Francis – bypassing the traditionally required second miracle – declared John XXIII a saint, after unanimous agreement by a consistory, or meeting, of the College of Cardinals, based on the fact that he was considered to have lived a virtuous, model lifestyle, and because of the good for the Church which had come from his having opened the Second Vatican Council. He was canonised alongside Pope Saint John Paul II on 27 April 2014. John XXIII today is affectionately known as the "Good Pope" and in Italian, "il Papa buono".
The Roman Catholic Church celebrates his feast day not on the date of his death, June 3, as is usual, nor even on the day of his papal inauguration (as is sometimes done with Popes who are Saints, such as with John Paul II) but on 11 October, the day of the first session of the Second Vatican Council. This is understandable, since he was the one who had had the idea for it and had convened it. On Thursday, 11 September 2014, Pope Francis added his optional memorial to the worldwide General Roman Calendar of saints' feast days, in response to global requests. He is commemorated on the date of his death, 3 June, by the Evangelical Lutheran Church in America and on the following day, 4 June, by the Anglican Church of Canada and the Episcopal Church (United States).
In February 1925, the Cardinal Secretary of State Pietro Gasparri summoned him to the Vatican and informed him of Pope Pius XI's decision to appoint him as the Apostolic Visitor to Bulgaria (1925–35). On 3 March, Pius XI also named him for consecration as titular archbishop of Areopolis, Jordan. Roncalli was initially reluctant about a mission to Bulgaria, but he would soon relent. His nomination as apostolic visitor was made official on 19 March. Roncalli was consecrated by Giovanni Tacci Porcelli in the church of San Carlo alla Corso in Rome. After he was consecrated, he introduced his family to Pope Pius XI. He chose as his episcopal motto Obedientia et Pax ("Obedience and Peace"), which became his guiding motto. While he was in Bulgaria, an earthquake struck in a town not too far from where he was. Unaffected, he wrote to his sisters Ancilla and Maria and told them both that he was fine.
On 30 November 1934, he was appointed Apostolic Delegate to Turkey and Greece and titular archbishop of Mesembria, Bulgaria. Thus, he is known as "the Turcophile Pope," by the Turkish society which is predominantly Muslim. Roncalli took up this post in 1935 and used his office to help the Jewish underground in saving thousands of refugees in Europe, leading some to consider him to be a Righteous Gentile (see Pope John XXIII and Judaism). In October 1935, he led Bulgarian pilgrims to Rome and introduced them to Pope Pius XI on 14 October.
In February 1939, he received news from his sisters that his mother was dying. On 10 February 1939, Pope Pius XI died. Roncalli was unable to see his mother for the end as the death of a pontiff meant that he would have to stay at his post until the election of a new pontiff. Unfortunately, she died on 20 February 1939, during the nine days of mourning for the late Pius XI. He was sent a letter by Cardinal Eugenio Pacelli, and Roncalli later recalled that it was probably the last letter Pacelli sent until his election as Pope Pius XII on 2 March 1939. Roncalli expressed happiness that Pacelli was elected, and, on radio, listened to the coronation of the new pontiff. 
On 12 January 1953, he was appointed Patriarch of Venice and, accordingly, raised to the rank of Cardinal-Priest of Santa Prisca by Pope Pius XII. Roncalli left France for Venice on 23 February 1953 stopping briefly in Milan and then to Rome. On 15 March 1953, he took possession of his new diocese in Venice. As a sign of his esteem, the President of France, Vincent Auriol, claimed the ancient privilege possessed by French monarchs and bestowed the red biretta on Roncalli at a ceremony in the Élysée Palace. It was around this time that he, with the aid of Monsignor Bruno Heim, formed his coat of arms with a lion of Saint Mark on a white ground. Auriol also awarded Roncalli three months later with the award of Commander of the Legion of Honour.
His sister Ancilla would soon be diagnosed with stomach cancer in the early 1950s. Roncalli's last letter to her was dated on 8 November 1953 where he promised to visit her within the next week. He could not keep that promise, as Ancilla died on 11 November 1953 at the time when he was consecrating a new church in Venice. He attended her funeral back in his hometown. In his will around this time, he mentioned that he wished to be buried in the crypt of Saint Mark's in Venice with some of his predecessors rather than with the family in Sotto il Monte.
Following the death of Pope Pius XII on 9 October 1958, Roncalli watched the live funeral on his last full day in Venice on 11 October. His journal was specifically concerned with the funeral and the abused state of the late pontiff's corpse. Roncalli left Venice for the conclave in Rome well aware that he was papabile,[b] and after eleven ballots, was elected to succeed the late Pius XII, so it came as no surprise to him, though he had arrived at the Vatican with a return train ticket to Venice.[citation needed]
Many had considered Giovanni Battista Montini, the Archbishop of Milan, a possible candidate, but, although he was the archbishop of one of the most ancient and prominent sees in Italy, he had not yet been made a cardinal. Though his absence from the 1958 conclave did not make him ineligible – under Canon Law any Catholic male who is capable of receiving priestly ordination and episcopal consecration may be elected – the College of Cardinals usually chose the new pontiff from among the Cardinals who head archdioceses or departments of the Roman Curia that attend the papal conclave. At the time, as opposed to contemporary practice, the participating Cardinals did not have to be below age 80 to vote, there were few Eastern-rite Cardinals, and no Cardinals who were just priests at the time of their elevation.
Roncalli was summoned to the final ballot of the conclave at 4:00 pm. He was elected pope at 4:30 pm with a total of 38 votes. After the long pontificate of Pope Pius XII, the cardinals chose a man who – it was presumed because of his advanced age – would be a short-term or "stop-gap" pope. They wished to choose a candidate who would do little during the new pontificate. Upon his election, Cardinal Eugene Tisserant asked him the ritual questions of whether he would accept and if so, what name he would take for himself. Roncalli gave the first of his many surprises when he chose "John" as his regnal name. Roncalli's exact words were "I will be called John". This was the first time in over 500 years that this name had been chosen; previous popes had avoided its use since the time of the Antipope John XXIII during the Western Schism several centuries before.
Far from being a mere "stopgap" pope, to great excitement, John XXIII called for an ecumenical council fewer than ninety years after the First Vatican Council (Vatican I's predecessor, the Council of Trent, had been held in the 16th century). This decision was announced on 29 January 1959 at the Basilica of Saint Paul Outside the Walls. Cardinal Giovanni Battista Montini, who later became Pope Paul VI, remarked to Giulio Bevilacqua that "this holy old boy doesn't realise what a hornet's nest he's stirring up". From the Second Vatican Council came changes that reshaped the face of Catholicism: a comprehensively revised liturgy, a stronger emphasis on ecumenism, and a new approach to the world.
John XXIII was an advocate for human rights which included the unborn and the elderly. He wrote about human rights in his Pacem in terris. He wrote, "Man has the right to live. He has the right to bodily integrity and to the means necessary for the proper development of life, particularly food, clothing, shelter, medical care, rest, and, finally, the necessary social services. In consequence, he has the right to be looked after in the event of ill health; disability stemming from his work; widowhood; old age; enforced unemployment; or whenever through no fault of his own he is deprived of the means of livelihood."
Maintaining continuity with his predecessors, John XXIII continued the gradual reform of the Roman liturgy, and published changes that resulted in the 1962 Roman Missal, the last typical edition containing the Tridentine Mass established in 1570 by Pope Pius V at the request of the Council of Trent and whose continued use Pope Benedict XVI authorized in 2007, under the conditions indicated in his motu proprio Summorum Pontificum. In response to the directives of the Second Vatican Council, later editions of the Roman Missal present the 1970 form of the Roman Rite.
On 11 October 1962, the first session of the Second Vatican Council was held in the Vatican. He gave the Gaudet Mater Ecclesia speech, which served as the opening address for the council. The day was basically electing members for several council commissions that would work on the issues presented in the council. On that same night following the conclusion of the first session, the people in Saint Peter's Square chanted and yelled with the sole objective of getting John XXIII to appear at the window to address them.
The first session ended in a solemn ceremony on 8 December 1962 with the next session scheduled to occur in 1963 from 12 May to 29 June – this was announced on 12 November 1962. John XXIII's closing speech made subtle references to Pope Pius IX, and he had expressed the desire to see Pius IX beatified and eventually canonized. In his journal in 1959 during a spiritual retreat, John XXIII made this remark: "I always think of Pius IX of holy and glorious memory, and by imitating him in his sacrifices, I would like to be worthy to celebrate his canonization".
Pope John XXIII offered to mediate between US President John F. Kennedy and Nikita Khrushchev during the Cuban Missile Crisis in October 1962. Both men applauded the pope for his deep commitment to peace. Khrushchev would later send a message via Norman Cousins and the letter expressed his best wishes for the pontiff's ailing health. John XXIII personally typed and sent a message back to him, thanking him for his letter. Cousins, meanwhile, travelled to New York City and ensured that John would become Time magazine's 'Man of the Year'. John XXIII became the first Pope to receive the title, followed by John Paul II in 1994 and Francis in 2013.
On 10 May 1963, John XXIII received the Balzan Prize in private at the Vatican but deflected achievements of himself to the five popes of his lifetime, Pope Leo XIII to Pius XII. On 11 May, the Italian President Antonio Segni officially awarded Pope John XXIII with the Balzan Prize for his engagement for peace. While in the car en route to the official ceremony, he suffered great stomach pains but insisted on meeting with Segni to receive the award in the Quirinal Palace, refusing to do so within the Vatican. He stated that it would have been an insult to honour a pontiff on the remains of the crucified Saint Peter. It was the pope's last public appearance.
On 25 May 1963, the pope suffered another haemorrhage and required several blood transfusions, but the cancer had perforated the stomach wall and peritonitis soon set in. The doctors conferred in a decision regarding this matter and John XXIII's aide Loris F. Capovilla broke the news to him saying that the cancer had done its work and nothing could be done for him. Around this time, his remaining siblings arrived to be with him. By 31 May, it had become clear that the cancer had overcome the resistance of John XXIII – it had left him confined to his bed.
"At 11 am Petrus Canisius Van Lierde as Papal Sacristan was at the bedside of the dying pope, ready to anoint him. The pope began to speak for the very last time: "I had the great grace to be born into a Christian family, modest and poor, but with the fear of the Lord. My time on earth is drawing to a close. But Christ lives on and continues his work in the Church. Souls, souls, ut omnes unum sint."[c] Van Lierde then anointed his eyes, ears, mouth, hands and feet. Overcome by emotion, Van Lierde forgot the right order of anointing. John XXIII gently helped him before bidding those present a last farewell.
John XXIII died of peritonitis caused by a perforated stomach at 19:49 local time on 3 June 1963 at the age of 81, ending a historic pontificate of four years and seven months. He died just as a Mass for him finished in Saint Peter's Square below, celebrated by Luigi Traglia. After he died, his brow was ritually tapped to see if he was dead, and those with him in the room said prayers. Then the room was illuminated, thus informing the people of what had happened. He was buried on 6 June in the Vatican grottos. Two wreaths, placed on the two sides of his tomb, were donated by the prisoners of the Regina Coeli prison and the Mantova jail in Verona. On 22 June 1963, one day after his friend and successor Pope Paul VI was elected, the latter prayed at his tomb.
On 3 December 1963, US President Lyndon B. Johnson posthumously awarded him the Presidential Medal of Freedom, the United States' highest civilian award, in recognition of the good relationship between Pope John XXIII and the United States of America. In his speech on 6 December 1963, Johnson said: "I have also determined to confer the Presidential Medal of Freedom posthumously on another noble man whose death we mourned 6 months ago: His Holiness, Pope John XXIII. He was a man of simple origins, of simple faith, of simple charity. In this exalted office he was still the gentle pastor. He believed in discussion and persuasion. He profoundly respected the dignity of man. He gave the world immortal statements of the rights of man, of the obligations of men to each other, of their duty to strive for a world community in which all can live in peace and fraternal friendship. His goodness reached across temporal boundaries to warm the hearts of men of all nations and of all faiths".
He was known affectionately as "Good Pope John". His cause for canonization was opened under Pope Paul VI during the final session of the Second Vatican Council on 18 November 1965, along with the cause of Pope Pius XII. On 3 September 2000, John XXIII was declared "Blessed" alongside Pope Pius IX by Pope John Paul II, the penultimate step on the road to sainthood after a miracle of curing an ill woman was discovered. He was the first pope since Pope Pius X to receive this honour. Following his beatification, his body was moved from its original burial place in the grottoes below the Vatican to the altar of St. Jerome and displayed for the veneration of the faithful.[citation needed]
The 50th anniversary of his death was celebrated on 3 June 2013 by Pope Francis, who visited his tomb and prayed there, then addressing the gathered crowd and spoke about the late pope. The people that gathered there at the tomb were from Bergamo, the province where the late pope came from. A month later, on 5 July 2013, Francis approved Pope John XXIII for canonization, along with Pope John Paul II without the traditional second miracle required. Instead, Francis based this decision on John XXIII's merits for the Second Vatican Council. On Sunday, 27 April 2014, John XXIII and Pope John Paul II were declared saints on Divine Mercy Sunday.
The United Nations Population Fund (UNFPA), formerly the United Nations Fund for Population Activities, is a UN organization. The UNFPA says it "is the lead UN agency for delivering a world where every pregnancy is wanted, every childbirth is safe and every young person's potential is fulfilled."  Their work involves the improvement of reproductive health; including creation of national strategies and protocols, and providing supplies and services. The organization has recently been known for its worldwide campaign against obstetric fistula and female genital mutilation.
The UNFPA supports programs in more than 150 countries, territories and areas spread across four geographic regions: Arab States and Europe, Asia and the Pacific, Latin America and the Caribbean, and sub-Saharan Africa. Around three quarters of the staff work in the field. It is a member of the United Nations Development Group and part of its Executive Committee.
UNFPA began operations in 1969 as the United Nations Fund for Population Activities (the name was changed in 1987) under the administration of the United Nations Development Fund. In 1971 it was placed under the authority of the United Nations General Assembly.
In September 2015, the 193 member states of the United Nations unanimously adopted the Sustainable Development Goals, a set of 17 goals aiming to transform the world over the next 15 years. These goals are designed to eliminate poverty, discrimination, abuse and preventable deaths, address environmental destruction, and usher in an era of development for all people, everywhere.
The Sustainable Development Goals are ambitious, and they will require enormous efforts across countries, continents, industries and disciplines - but they are achievable. UNFPA is working with governments, partners and other UN agencies to directly tackle many of these goals - in particular Goal 3 on health, Goal 4 on education and Goal 5 on gender equality - and contributes in a variety of ways to achieving many of the rest. 
Executive Directors and Under-Secretaries General of the UN
2011–present Dr Babatunde Osotimehin (Nigeria)
2000–2010 Ms Thoraya Ahmed Obaid (Saudi Arabia)
1987–2000 Dr Nafis Sadik (Pakistan)
1969–87 Mr Rafael M. Salas (Philippines)
UNFPA is the world's largest multilateral source of funding for population and reproductive health programs. The Fund works with governments and non-governmental organizations in over 150 countries with the support of the international community, supporting programs that help women, men and young people:
According to UNFPA these elements promote the right of "reproductive health", that is physical, mental, and social health in matters related to reproduction and the reproductive system.
The Fund raises awareness of and supports efforts to meet these needs in developing countries, advocates close attention to population concerns, and helps developing nations formulate policies and strategies in support of sustainable development. Dr. Osotimehin assumed leadership in January 2011. The Fund is also represented by UNFPA Goodwill Ambassadors and a Patron.
UNFPA works in partnership with governments, along with other United Nations agencies, communities, NGOs, foundations and the private sector, to raise awareness and mobilize the support and resources needed to achieve its mission to promote the rights and health of women and young people.
UNFPA has been falsely accused by anti-family planning groups of providing support for government programs which have promoted forced-abortions and coercive sterilizations. Controversies regarding these claims have resulted in a sometimes shaky relationship between the organization and three presidential administrations, that of Ronald Reagan, George H. W. Bush and George W. Bush, withholding funding from the UNFPA.  
Contributions from governments and the private sector to UNFPA in 2014 exceeded $1 billion. The amount includes $477 million to the organization’s core resources and $529 million earmarked for specific programs and initiatives.
UNFPA provided aid to Peru's reproductive health program in the mid-to-late '90s. When it was discovered a Peruvian program had been engaged in carrying out coercive sterilizations, UNFPA called for reforms and protocols to protect the rights of women seeking assistance. UNFPA was not involved in the scandal, but continued work with the country after the abuses had become public to help end the abuses and reform laws and practices. 
From 2002 through 2008, the Bush Administration denied funding to UNFPA that had already been allocated by the US Congress, partly on the refuted claims that the UNFPA supported Chinese government programs which include forced abortions and coercive sterilizations. In a letter from the Undersecretary of State for Political Affairs Nicholas Burns to Congress, the administration said it had determined that UNFPA’s support for China’s population program “facilitates (its) government’s coercive abortion program”, thus violating the Kemp-Kasten Amendment, which bans the use of United States aid to finance organizations that support or take part in managing a program of coercive abortion of sterilization.
UNFPA's connection to China's administration of forced abortions was refuted by investigations carried out by various US, UK, and UN teams sent to examine UNFPA activities in China. Specifically, a three-person U.S State Department fact-finding team was sent on a two-week tour throughout China. It wrote in a report to the State Department that it found "no evidence that UNFPA has supported or participated in the management of a program of coercive abortion or involuntary sterilization in China," as has been charged by critics.
However, according to then-Secretary of State Colin Powell, the UNFPA contributed vehicles and computers to the Chinese to carry out their population control policies. However, both the Washington Post and the Washington Times reported that Powell simply fell in line, signing a brief written by someone else. 
Rep. Christopher H. Smith (R-NJ), criticized the State Department investigation, saying the investigators were shown "Potemkin Villages" where residents had been intimidated into lying about the family-planning program. Dr. Nafis Sadik, former director of UNFPA said her agency had been pivotal in reversing China's coercive population control methods, but a 2005 report by Amnesty International and a separate report by the United States State Department found that coercive techniques were still regularly employed by the Chinese, casting doubt upon Sadik's statements.
But Amnesty International found no evidence that UNFPA had supported the coercion. A 2001 study conducted by the pro-life Population Research Institute (PRI) falsely claimed that the UNFPA shared an office with the Chinese family planning officials who were carrying out forced abortions. "We located the family planning offices, and in that family planning office, we located the UNFPA office, and we confirmed from family planning officials there that there is no distinction between what the UNFPA does and what the Chinese Family Planning Office does," said Scott Weinberg, a spokesman for PRI. However, United Nations Members disagreed and approved UNFPA’s new country program me in January 2006. The more than 130 members of the “Group of 77” developing countries in the United Nations expressed support for the UNFPA programmes. In addition, speaking for European democracies -- Norway, Denmark, Sweden, Finland, the Netherlands, France, Belgium, Switzerland and Germany -- the United Kingdom stated, ”UNFPA’s activities in China, as in the rest of the world, are in strict conformity with the unanimously adopted Programme of Action of the ICPD, and play a key role in supporting our common endeavor, the promotion and protection of all human rights and fundamental freedoms.” 
President Bush denied funding to the UNFPA. Over the course of the Bush Administration, a total of $244 million in Congressionally approved funding was blocked by the Executive Branch.
In response, the EU decided to fill the gap left behind by the US under the Sandbaek report. According to its Annual Report for 2008, the UNFPA received its funding mainly from European Governments: Of the total income of M845.3 M, $118 was donated by the Netherlands, $67 M by Sweden, $62 M by Norway, $54 M by Denmark, $53 M by the UK, $52 M by Spain, $19 M by Luxembourg. The European Commission donated further $36 M. The most important non-European donor State was Japan ($36 M). The number of donors exceeded 180 in one year.
In America, nonprofit organizations like Friends of UNFPA (formerly Americans for UNFPA) worked to compensate for the loss of United States federal funding by raising private donations.
In January 2009 President Barack Obama restored US funding to UNFPA, saying in a public statement that he would "look forward to working with Congress to restore US financial support for the UN Population Fund. By resuming funding to UNFPA, the US will be joining 180 other donor nations working collaboratively to reduce poverty, improve the health of women and children, prevent HIV/AIDS and provide family planning assistance to women in 154 countries." 
A railway electrification system supplies electric power to railway trains and trams without an on-board prime mover or local fuel supply. Electrification has many advantages but requires significant capital expenditure. Selection of an electrification system is based on economics of energy supply, maintenance, and capital cost compared to the revenue obtained for freight and passenger traffic. Different systems are used for urban and intercity areas; some electric locomotives can switch to different supply voltages to allow flexibility in operation.
Electric railways use electric locomotives to haul passengers or freight in separate cars or electric multiple units, passenger cars with their own motors. Electricity is typically generated in large and relatively efficient generating stations, transmitted to the railway network and distributed to the trains. Some electric railways have their own dedicated generating stations and transmission lines but most purchase power from an electric utility. The railway usually provides its own distribution lines, switches and transformers.
In comparison to the principal alternative, the diesel engine, electric railways offer substantially better energy efficiency, lower emissions and lower operating costs. Electric locomotives are usually quieter, more powerful, and more responsive and reliable than diesels. They have no local emissions, an important advantage in tunnels and urban areas. Some electric traction systems provide regenerative braking that turns the train's kinetic energy back into electricity and returns it to the supply system to be used by other trains or the general utility grid. While diesel locomotives burn petroleum, electricity is generated from diverse sources including many that do not produce carbon dioxide such as nuclear power and renewable forms including hydroelectric, geothermal, wind and solar.
Disadvantages of electric traction include high capital costs that may be uneconomic on lightly trafficked routes; a relative lack of flexibility since electric trains need electrified tracks or onboard supercapacitors and charging infrastructure at stations; and a vulnerability to power interruptions. Different regions may use different supply voltages and frequencies, complicating through service. The limited clearances available under catenaries may preclude efficient double-stack container service. The lethal voltages on contact wires and third rails are a safety hazard to track workers, passengers and trespassers. Overhead wires are safer than third rails, but they are often considered unsightly.
Railways must operate at variable speeds. Until the mid 1980s this was only practical with the brush-type DC motor, although such DC can be supplied from an AC catenary via on-board electric power conversion. Since such conversion was not well developed in the late 19th century and early 20th century, most early electrified railways used DC and many still do, particularly rapid transit (subways) and trams. Speed was controlled by connecting the traction motors in various series-parallel combinations, by varying the traction motors' fields, and by inserting and removing starting resistances to limit motor current.
Motors have very little room for electrical insulation so they generally have low voltage ratings. Because transformers (prior to the development of power electronics) cannot step down DC voltages, trains were supplied with a relatively low DC voltage that the motors can use directly. The most common DC voltages are listed in the previous section. Third (and fourth) rail systems almost always use voltages below 1 kV for safety reasons while overhead wires usually use higher voltages for efficiency. ("Low" voltage is relative; even 600 V can be instantly lethal when touched.)
There has, however, been interest among railroad operators in returning to DC use at higher voltages than previously used. At the same voltage, DC often has less loss than AC, and for this reason high-voltage direct current is already used on some bulk power transmission lines. DC avoids the electromagnetic radiation inherent with AC, and on a railway this also reduces interference with signalling and communications and mitigates hypothetical EMF risks. DC also avoids the power factor problems of AC. Of particular interest to railroading is that DC can supply constant power with a single ungrounded wire. Constant power with AC requires three-phase transmission with at least two ungrounded wires. Another important consideration is that mains-frequency 3-phase AC must be carefully planned to avoid unbalanced phase loads. Parts of the system are supplied from different phases on the assumption that the total loads of the 3 phases will even out. At the phase break points between regions supplied from different phases, long insulated supply breaks are required to avoid them being shorted by rolling stock using more than one pantograph at a time. A few railroads have tried 3-phase but its substantial complexity has made single-phase standard practice despite the interruption in power flow that occurs twice every cycle. An experimental 6 kV DC railway was built in the Soviet Union.
1,500 V DC is used in the Netherlands, Japan, Republic Of Indonesia, Hong Kong (parts), Republic of Ireland, Australia (parts), India (around the Mumbai area alone, has been converted to 25 kV AC like the rest of India), France (also using 25 kV 50 Hz AC), New Zealand (Wellington) and the United States (Chicago area on the Metra Electric district and the South Shore Line interurban line). In Slovakia, there are two narrow-gauge lines in the High Tatras (one a cog railway). In Portugal, it is used in the Cascais Line and in Denmark on the suburban S-train system.
3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.
Most electrification systems use overhead wires, but third rail is an option up to about 1,200 V. Third rail systems exclusively use DC distribution. The use of AC is not feasible because the dimensions of a third rail are physically very large compared with the skin depth that the alternating current penetrates to (0.3 millimetres or 0.012 inches) in a steel rail). This effect makes the resistance per unit length unacceptably high compared with the use of DC. Third rail is more compact than overhead wires and can be used in smaller-diameter tunnels, an important factor for subway systems.
DC systems (especially third-rail systems) are limited to relatively low voltages and this can limit the size and speed of trains and cannot use low-level platform and also limit the amount of air-conditioning that the trains can provide. This may be a factor favouring overhead wires and high-voltage AC, even for urban usage. In practice, the top speed of trains on third-rail systems is limited to 100 mph (160 km/h) because above that speed reliable contact between the shoe and the rail cannot be maintained.
Some street trams (streetcars) used conduit third-rail current collection. The third rail was below street level. The tram picked up the current through a plough (U.S. "plow") accessed through a narrow slot in the road. In the United States, much (though not all) of the former streetcar system in Washington, D.C. (discontinued in 1962) was operated in this manner to avoid the unsightly wires and poles associated with electric traction. The same was true with Manhattan's former streetcar system. The evidence of this mode of running can still be seen on the track down the slope on the northern access to the abandoned Kingsway Tramway Subway in central London, United Kingdom, where the slot between the running rails is clearly visible, and on P and Q Streets west of Wisconsin Avenue in the Georgetown neighborhood of Washington DC, where the abandoned tracks have not been paved over. The slot can easily be confused with the similar looking slot for cable trams/cars (in some cases, the conduit slot was originally a cable slot). The disadvantage of conduit collection included much higher initial installation costs, higher maintenance costs, and problems with leaves and snow getting in the slot. For this reason, in Washington, D.C. cars on some lines converted to overhead wire on leaving the city center, a worker in a "plough pit" disconnecting the plough while another raised the trolley pole (hitherto hooked down to the roof) to the overhead wire. In New York City for the same reasons of cost and operating efficiency outside of Manhattan overhead wire was used. A similar system of changeover from conduit to overhead wire was also used on the London tramways, notably on the southern side; a typical changeover point was at Norwood, where the conduit snaked sideways from between the running rails, to provide a park for detached shoes or ploughs.
A new approach to avoiding overhead wires is taken by the "second generation" tram/streetcar system in Bordeaux, France (entry into service of the first line in December 2003; original system discontinued in 1958) with its APS (alimentation par sol – ground current feed). This involves a third rail which is flush with the surface like the tops of the running rails. The circuit is divided into segments with each segment energized in turn by sensors from the car as it passes over it, the remainder of the third rail remaining "dead". Since each energized segment is completely covered by the lengthy articulated cars, and goes dead before being "uncovered" by the passage of the vehicle, there is no danger to pedestrians. This system has also been adopted in some sections of the new tram systems in Reims, France (opened 2011) and Angers, France (also opened 2011). Proposals are in place for a number of other new services including Dubai, UAE; Barcelona, Spain; Florence, Italy; Marseille, France; Gold Coast, Australia; Washington, D.C., U.S.A.; Brasília, Brazil and Tours, France.
The London Underground in England is one of the few networks that uses a four-rail system. The additional rail carries the electrical return that, on third rail and overhead networks, is provided by the running rails. On the London Underground, a top-contact third rail is beside the track, energized at +420v DC, and a top-contact fourth rail is located centrally between the running rails at −210v DC, which combine to provide a traction voltage of 630v DC. London Underground is now upgrading its fourth rail system to 750v DC with a positive conductor rail energised to +500v DC and a negative conductor rail energised to -250v DC. However, many older sections in tunnels are still energised to 630v DC. The same system was used for Milan's earliest underground line, Milan Metro's line 1, whose more recent lines use an overhead catenary or a third rail.
The key advantage of the four-rail system is that neither running rail carries any current. This scheme was introduced because of the problems of return currents, intended to be carried by the earthed (grounded) running rail, flowing through the iron tunnel linings instead. This can cause electrolytic damage and even arcing if the tunnel segments are not electrically bonded together. The problem was exacerbated because the return current also had a tendency to flow through nearby iron pipes forming the water and gas mains. Some of these, particularly Victorian mains that predated London's underground railways, were not constructed to carry currents and had no adequate electrical bonding between pipe segments. The four-rail system solves the problem. Although the supply has an artificially created earth point, this connection is derived by using resistors which ensures that stray earth currents are kept to manageable levels. Power-only rails can be mounted on strongly insulating ceramic chairs to minimise current leak, but this is not possible for running rails which have to be seated on stronger metal chairs to carry the weight of trains. However, elastomeric rubber pads placed between the rails and chairs can now solve part of the problem by insulating the running rails from the current return should there be a leakage through the running rails.
On tracks that London Underground share with National Rail third-rail stock (the Bakerloo and District lines both have such sections), the centre rail is connected to the running rails, allowing both types of train to operate, at a compromise voltage of 660 V. Underground trains pass from one section to the other at speed; lineside electrical connections and resistances separate the two types of supply. These routes were originally solely electrified on the four-rail system by the LNWR before National Rail trains were rewired to their standard three-rail system to simplify rolling stock use.
A few lines of the Paris Métro in France operate on a four-rail power scheme because they run on rubber tyres which run on a pair of narrow roadways made of steel and, in some places, concrete. Since the tyres do not conduct the return current, the two guide rails provided outside the running 'roadways' double up as conductor rails, so at least electrically it is a four-rail scheme. One of the guide rails is bonded to the return conventional running rails situated inside the roadway so a single polarity supply is required. The trains are designed to operate from either polarity of supply, because some lines use reversing loops at one end, causing the train to be reversed during every complete journey. The loop was originally provided to save the original steam locomotives having to 'run around' the rest of the train saving much time. Today, the driver does not have to change ends at termini provided with such a loop, but the time saving is not so significant as it takes almost as long to drive round the loop as it does to change ends. Many of the original loops have been lost as lines were extended.
An early advantage of AC is that the power-wasting resistors used in DC locomotives for speed control were not needed in an AC locomotive: multiple taps on the transformer can supply a range of voltages. Separate low-voltage transformer windings supply lighting and the motors driving auxiliary machinery. More recently, the development of very high power semiconductors has caused the classic "universal" AC/DC motor to be largely replaced with the three-phase induction motor fed by a variable frequency drive, a special inverter that varies both frequency and voltage to control motor speed. These drives can run equally well on DC or AC of any frequency, and many modern electric locomotives are designed to handle different supply voltages and frequencies to simplify cross-border operation.
DC commutating electric motors, if fitted with laminated pole pieces, become universal motors because they can also operate on AC; reversing the current in both stator and rotor does not reverse the motor. But the now-standard AC distribution frequencies of 50 and 60 Hz caused difficulties with inductive reactance and eddy current losses. Many railways chose low AC frequencies to overcome these problems. They must be converted from utility power by motor-generators or static inverters at the feeding substations or generated at dedicated traction powerstations.
High-voltage AC overhead systems are not only for standard gauge national networks. The meter gauge Rhaetian Railway (RhB) and the neighbouring Matterhorn Gotthard Bahn (MGB) operate on 11 kV at 16.7 Hz frequency. Practice has proven that both Swiss and German 15 kV trains can operate under these lower voltages. The RhB started trials of the 11 kV system in 1913 on the Engadin line (St. Moritz-Scuol/Tarasp). The MGB constituents Furka-Oberalp-Bahn (FO) and Brig-Visp-Zermatt Bahn (BVZ) introduced their electric services in 1941 and 1929 respectively, adopting the already proven RhB system.
In the United States, 25 Hz, a once-common industrial power frequency is used on Amtrak's 25 Hz traction power system at 12 kV on the Northeast Corridor between Washington, D.C. and New York City and on the Keystone Corridor between Harrisburg, Pennsylvania and Philadelphia. SEPTA's 25 Hz traction power system uses the same 12 kV voltage on the catenary in Northeast Philadelphia. This allows for the trains to operate on both the Amtrak and SEPTA power systems. Apart from having an identical catenary voltage, the power distribution systems of Amtrak and SEPTA are very different. The Amtrak power distribution system has a 138 kV transmission network that provides power to substations which then transform the voltage to 12 kV to feed the catenary system. The SEPTA power distribution system uses a 2:1 ratio autotransformer system, with the catenary fed at 12 kV and a return feeder wire fed at 24 kV. The New York, New Haven and Hartford Railroad used an 11 kV system between New York City and New Haven, Connecticut which was converted to 12.5 kV 60 Hz in 1987.
In the UK, the London, Brighton and South Coast Railway pioneered overhead electrification of its suburban lines in London, London Bridge to Victoria being opened to traffic on 1 December 1909. Victoria to Crystal Palace via Balham and West Norwood opened in May 1911. Peckham Rye to West Norwood opened in June 1912. Further extensions were not made owing to the First World War. Two lines opened in 1925 under the Southern Railway serving Coulsdon North and Sutton railway station. The lines were electrified at 6.7 kV 25 Hz. It was announced in 1926 that all lines were to be converted to DC third rail and the last overhead electric service ran in September 1929.
Three-phase AC railway electrification was used in Italy, Switzerland and the United States in the early twentieth century. Italy was the major user, for lines in the mountainous regions of northern Italy from 1901 until 1976. The first lines were the Burgdorf-Thun line in Switzerland (1899), and the lines of the Ferrovia Alta Valtellina from Colico to Chiavenna and Tirano in Italy, which were electrified in 1901 and 1902. Other lines where the three-phase system were used were the Simplon Tunnel in Switzerland from 1906 to 1930, and the Cascade Tunnel of the Great Northern Railway in the United States from 1909 to 1927.
The first attempts to use standard-frequency single-phase AC were made in Hungary as far back as 1923, by the Hungarian Kálmán Kandó on the line between Budapest-Nyugati and Alag, using 16 kV at 50 Hz. The locomotives carried a four-pole rotating phase converter feeding a single traction motor of the polyphase induction type at 600 to 1,100 V. The number of poles on the 2,500 hp motor could be changed using slip rings to run at one of four synchronous speeds. The tests were a success so, from 1932 until the 1960s, trains on the Budapest-Hegyeshalom line (towards Vienna) regularly used the same system. A few decades after the Second World War, the 16 kV was changed to the Russian and later French 25 kV system.
To prevent the risk of out-of-phase supplies mixing, sections of line fed from different feeder stations must be kept strictly isolated. This is achieved by Neutral Sections (also known as Phase Breaks), usually provided at feeder stations and midway between them although, typically, only half are in use at any time, the others being provided to allow a feeder station to be shut down and power provided from adjacent feeder stations. Neutral Sections usually consist of an earthed section of wire which is separated from the live wires on either side by insulating material, typically ceramic beads, designed so that the pantograph will smoothly run from one section to the other. The earthed section prevents an arc being drawn from one live section to the other, as the voltage difference may be higher than the normal system voltage if the live sections are on different phases and the protective circuit breakers may not be able to safely interrupt the considerable current that would flow. To prevent the risk of an arc being drawn across from one section of wire to earth, when passing through the neutral section, the train must be coasting and the circuit breakers must be open. In many cases, this is done manually by the drivers. To help them, a warning board is provided just before both the neutral section and an advance warning some distance before. A further board is then provided after the neutral section to tell drivers to re-close the circuit breaker, although drivers must not do this until the rear pantograph has passed this board. In the UK, a system known as Automatic Power Control (APC) automatically opens and closes the circuit breaker, this being achieved by using sets of permanent magnets alongside the track communicating with a detector on the train. The only action needed by the driver is to shut off power and coast and therefore warning boards are still provided at and on the approach to neutral sections.
Modern electrification systems take AC energy from a power grid which is delivered to a locomotive and converted to a DC voltage to be used by traction motors. These motors may either be DC motors which directly use the DC or they may be 3-phase AC motors which require further conversion of the DC to 3-phase AC (using power electronics). Thus both systems are faced with the same task: converting and transporting high-voltage AC from the power grid to low-voltage DC in the locomotive. Where should this conversion take place and at what voltage and current (AC or DC) should the power flow to the locomotive? And how does all this relate to energy-efficiency? Both the transmission and conversion of electric energy involve losses: ohmic losses in wires and power electronics, magnetic field losses in transformers and smoothing reactors (inductors). Power conversion for a DC system takes place mainly in a railway substation where large, heavy, and more efficient hardware can be used as compared to an AC system where conversion takes place aboard the locomotive where space is limited and losses are significantly higher. Also, the energy used to blow air to cool transformers, power electronics (including rectifiers), and other conversion hardware must be accounted for.
In the Soviet Union, in the 1970s, a comparison was made between systems electrified at 3 kV DC and 25 kV AC (50 Hz). The results showed that percentage losses in the overhead wires (catenary and contact wires) was over 3 times greater for 3 kV DC than for 25 kV AC. But when the conversion losses were all taken into account and added to overhead wire losses (including cooling blower energy) the 25 kV AC lost a somewhat higher percent of energy than for 3 kV DC. Thus in spite of the much higher losses in the catenary, the 3 kV DC was a little more energy efficient than AC in providing energy from the USSR power grid to the terminals of the traction motors (all DC at that time). While both systems use energy in converting higher voltage AC from the USSR's power grid to lower voltage DC, the conversions for the DC system all took place (at higher efficiency) in the railway substation, while most of the conversion for the AC system took place inside the locomotive (at lower efficiency). Consider also that it takes energy to constantly move this mobile conversion hardware over the rails while the stationary hardware in the railway substation doesn't incur this energy cost. For more details see: Wiki: Soviet Union DC vs. AC.
Newly electrified lines often show a "sparks effect", whereby electrification in passenger rail systems leads to significant jumps in patronage / revenue. The reasons may include electric trains being seen as more modern and attractive to ride, faster and smoother service, and the fact that electrification often goes hand in hand with a general infrastructure and rolling stock overhaul / replacement, which leads to better service quality (in a way that theoretically could also be achieved by doing similar upgrades yet without electrification). Whatever the causes of the sparks effect, it is well established for numerous routes that have electrified over decades.
Network effects are a large factor with electrification. When converting lines to electric, the connections with other lines must be considered. Some electrifications have subsequently been removed because of the through traffic to non-electrified lines. If through traffic is to have any benefit, time consuming engine switches must occur to make such connections or expensive dual mode engines must be used. This is mostly an issue for long distance trips, but many lines come to be dominated by through traffic from long-haul freight trains (usually running coal, ore, or containers to or from ports). In theory, these trains could enjoy dramatic savings through electrification, but it can be too costly to extend electrification to isolated areas, and unless an entire network is electrified, companies often find that they need to continue use of diesel trains even if sections are electrified. The increasing demand for container traffic which is more efficient when utilizing the double-stack car also has network effect issues with existing electrifications due to insufficient clearance of overhead electrical lines for these trains, but electrification can be built or modified to have sufficient clearance, at additional cost.
Additionally, there are issues of connections between different electrical services, particularly connecting intercity lines with sections electrified for commuter traffic, but also between commuter lines built to different standards. This can cause electrification of certain connections to be very expensive simply because of the implications on the sections it is connecting. Many lines have come to be overlaid with multiple electrification standards for different trains to avoid having to replace the existing rolling stock on those lines. Obviously, this requires that the economics of a particular connection must be more compelling and this has prevented complete electrification of many lines. In a few cases, there are diesel trains running along completely electrified routes and this can be due to incompatibility of electrification standards along the route.
Central station electricity can often be generated with higher efficiency than a mobile engine/generator. While the efficiency of power plant generation and diesel locomotive generation are roughly the same in the nominal regime, diesel motors decrease in efficiency in non-nominal regimes at low power  while if an electric power plant needs to generate less power it will shut down its least efficient generators, thereby increasing efficiency. The electric train can save energy (as compared to diesel) by regenerative braking and by not needing to consume energy by idling as diesel locomotives do when stopped or coasting. However, electric rolling stock may run cooling blowers when stopped or coasting, thus consuming energy.
Energy sources unsuitable for mobile power plants, such as nuclear power, renewable hydroelectricity, or wind power can be used. According to widely accepted global energy reserve statistics, the reserves of liquid fuel are much less than gas and coal (at 42, 167 and 416 years respectively). Most countries with large rail networks do not have significant oil reserves and those that did, like the United States and Britain, have exhausted much of their reserves and have suffered declining oil output for decades. Therefore, there is also a strong economic incentive to substitute other fuels for oil. Rail electrification is often considered an important route towards consumption pattern reform. However, there are no reliable, peer-reviewed studies available to assist in rational public debate on this critical issue, although there are untranslated Soviet studies from the 1980s.
In the former Soviet Union, electric traction eventually became somewhat more energy-efficient than diesel. Partly due to inefficient generation of electricity in the USSR (only 20.8% thermal efficiency in 1950 vs. 36.2% in 1975), in 1950 diesel traction was about twice as energy efficient as electric traction (in terms of net tonne-km of freight per kg of fuel). But as efficiency of electricity generation (and thus of electric traction) improved, by about 1965 electric railways became more efficient than diesel. After the mid 1970s electrics used about 25% less fuel per ton-km. However diesels were mainly used on single track lines with a fair amount of traffic  so that the lower fuel consumption of electrics may be in part due to better operating conditions on electrified lines (such as double tracking) rather than inherent energy efficiency. Nevertheless, the cost of diesel fuel was about 1.5 times more (per unit of heat energy content) than that of the fuel used in electric power plants (that generated electricity), thus making electric railways even more energy-cost effective.
Besides increased efficiency of power plants, there was an increase in efficiency (between 1950 and 1973) of the railway utilization of this electricity with energy-intensity dropping from 218 to 124 kwh/10,000 gross tonne-km (of both passenger and freight trains) or a 43% drop. Since energy-intensity is the inverse of energy-efficiency it drops as efficiency goes up. But most of this 43% decrease in energy-intensity also benefited diesel traction. The conversion of wheel bearings from plain to roller, increase of train weight, converting single track lines to double track (or partially double track), and the elimination of obsolete 2-axle freight cars increased the energy-efficiency of all types of traction: electric, diesel, and steam. However, there remained a 12–15% reduction of energy-intensity that only benefited electric traction (and not diesel). This was due to improvements in locomotives, more widespread use of regenerative braking (which in 1989 recycled 2.65% of the electric energy used for traction,) remote control of substations, better handling of the locomotive by the locomotive crew, and improvements in automation. Thus the overall efficiency of electric traction as compared to diesel more than doubled between 1950 and the mid-1970s in the Soviet Union. But after 1974 (thru 1980) there was no improvement in energy-intensity (wh/tonne-km) in part due to increasing speeds of passenger and freight trains.
The contemporary Liberal Party generally advocates economic liberalism (see New Right). Historically, the party has supported a higher degree of economic protectionism and interventionism than it has in recent decades. However, from its foundation the party has identified itself as anti-socialist. Strong opposition to socialism and communism in Australia and abroad was one of its founding principles. The party's founder and longest-serving leader Robert Menzies envisaged that Australia's middle class would form its main constituency.
Throughout their history, the Liberals have been in electoral terms largely the party of the middle class (whom Menzies, in the era of the party's formation called "The forgotten people"), though such class-based voting patterns are no longer as clear as they once were. In the 1970s a left-wing middle class emerged that no longer voted Liberal.[citation needed] One effect of this was the success of a breakaway party, the Australian Democrats, founded in 1977 by former Liberal minister Don Chipp and members of minor liberal parties; other members of the left-leaning section of the middle-class became Labor supporters.[citation needed] On the other hand, the Liberals have done increasingly well in recent years among socially conservative working-class voters.[citation needed]However the Liberal Party's key support base remains the upper-middle classes; 16 of the 20 richest federal electorates are held by the Liberals, most of which are safe seats. In country areas they either compete with or have a truce with the Nationals, depending on various factors.
Domestically, Menzies presided over a fairly regulated economy in which utilities were publicly owned, and commercial activity was highly regulated through centralised wage-fixing and high tariff protection. Liberal leaders from Menzies to Malcolm Fraser generally maintained Australia's high tariff levels. At that time the Liberals' coalition partner, the Country Party, the older of the two in the coalition (now known as the "National Party"), had considerable influence over the government's economic policies. It was not until the late 1970s and through their period out of power federally in the 1980s that the party came to be influenced by what was known as the "New Right" – a conservative liberal group who advocated market deregulation, privatisation of public utilities, reductions in the size of government programs and tax cuts.
The Liberals' immediate predecessor was the United Australia Party (UAP). More broadly, the Liberal Party's ideological ancestry stretched back to the anti-Labor groupings in the first Commonwealth parliaments. The Commonwealth Liberal Party was a fusion of the Free Trade Party and the Protectionist Party in 1909 by the second prime minister, Alfred Deakin, in response to Labor's growing electoral prominence. The Commonwealth Liberal Party merged with several Labor dissidents (including Billy Hughes) to form the Nationalist Party of Australia in 1917. That party, in turn, merged with Labor dissidents to form the UAP in 1931.
The UAP had been formed as a new conservative alliance in 1931, with Labor defector Joseph Lyons as its leader. The stance of Lyons and other Labor rebels against the more radical proposals of the Labor movement to deal the Great Depression had attracted the support of prominent Australian conservatives. With Australia still suffering the effects of the Great Depression, the newly formed party won a landslide victory at the 1931 Election, and the Lyons Government went on to win three consecutive elections. It largely avoided Keynesian pump-priming and pursued a more conservative fiscal policy of debt reduction and balanced budgets as a means of stewarding Australia out of the Depression. Lyons' death in 1939 saw Robert Menzies assume the Prime Ministership on the eve of war. Menzies served as Prime Minister from 1939 to 1941 but resigned as leader of the minority World War II government amidst an unworkable parliamentary majority. The UAP, led by Billy Hughes, disintegrated after suffering a heavy defeat in the 1943 election.
Menzies called a conference of conservative parties and other groups opposed to the ruling Australian Labor Party, which met in Canberra on 13 October 1944 and again in Albury, New South Wales in December 1944. From 1942 onward Menzies had maintained his public profile with his series of "The Forgotten People" radio talks–similar to Franklin D. Roosevelt's "fireside chats" of the 1930s–in which he spoke of the middle class as the "backbone of Australia" but as nevertheless having been "taken for granted" by political parties.
The formation of the party was formally announced at Sydney Town Hall on 31 August 1945. It took the name "Liberal" in honour of the old Commonwealth Liberal Party. The new party was dominated by the remains of the old UAP; with few exceptions, the UAP party room became the Liberal party room. The Australian Women's National League, a powerful conservative women's organisation, also merged with the new party. A conservative youth group Menzies had set up, the Young Nationalists, was also merged into the new party. It became the nucleus of the Liberal Party's youth division, the Young Liberals. By September 1945 there were more than 90,000 members, many of whom had not previously been members of any political party.
After an initial loss to Labor at the 1946 election, Menzies led the Liberals to victory at the 1949 election, and the party stayed in office for a record 23 years—still the longest unbroken run in government at the federal level. Australia experienced prolonged economic growth during the post-war boom period of the Menzies Government (1949–1966) and Menzies fulfilled his promises at the 1949 election to end rationing of butter, tea and petrol and provided a five-shilling endowment for first-born children, as well as for others. While himself an unashamed anglophile, Menzies' government concluded a number of major defence and trade treaties that set Australia on its post-war trajectory out of Britain's orbit; opened Australia to multi-ethnic immigration; and instigated important legal reforms regarding Aboriginal Australians.
Menzies came to power the year the Communist Party of Australia had led a coal strike to improve pit miners' working conditions. That same year Joseph Stalin's Soviet Union exploded its first atomic bomb, and Mao Zedong led the Communist Party of China to power in China; a year later came the invasion of South Korea by Communist North Korea. Anti-communism was a key political issue of the 1950s and 1960s. Menzies was firmly anti-Communist; he committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of that war. The Labor Party split over concerns about the influence of the Communist Party over the Trade Union movement, leading to the foundation of the breakaway Democratic Labor Party whose preferences supported the Liberal and Country parties.
In 1951, during the early stages of the Cold War, Menzies spoke of the possibility of a looming third world war. The Menzies Government entered Australia's first formal military alliance outside of the British Commonwealth with the signing of the ANZUS Treaty between Australia, New Zealand and the United States in San Francisco in 1951. External Affairs Minister Percy Spender had put forward the proposal to work along similar lines to the NATO Alliance. The Treaty declared that any attack on one of the three parties in the Pacific area would be viewed as a threat to each, and that the common danger would be met in accordance with each nation's constitutional processes. In 1954 the Menzies Government signed the South East Asia Collective Defence Treaty (SEATO) as a South East Asian counterpart to NATO. That same year, Soviet diplomat Vladimir Petrov and his wife defected from the Soviet embassy in Canberra, revealing evidence of Russian spying activities; Menzies called a Royal Commission to investigate.
Menzies continued the expanded immigration program established under Chifley, and took important steps towards dismantling the White Australia Policy. In the early 1950s, external affairs minister Percy Spender helped to establish the Colombo Plan for providing economic aid to underdeveloped nations in Australia's region. Under that scheme many future Asian leaders studied in Australia. In 1958 the government replaced the Immigration Act's arbitrarily applied European language dictation test with an entry permit system, that reflected economic and skills criteria. In 1962, Menzies' Commonwealth Electoral Act provided that all Indigenous Australians should have the right to enrol and vote at federal elections (prior to this, indigenous people in Queensland, Western Australia and some in the Northern Territory had been excluded from voting unless they were ex-servicemen). In 1949 the Liberals appointed Dame Enid Lyons as the first woman to serve in an Australian Cabinet. Menzies remained a staunch supporter of links to the monarchy and British Commonwealth but formalised an alliance with the United States and concluded the Agreement on Commerce between Australia and Japan which was signed in July 1957 and launched post-war trade with Japan, beginning a growth of Australian exports of coal, iron ore and mineral resources that would steadily climb until Japan became Australia's largest trading partner.
Holt increased Australian commitment to the growing War in Vietnam, which met with some public opposition. His government oversaw conversion to decimal currency. Holt faced Britain's withdrawal from Asia by visiting and hosting many Asian leaders and by expanding ties to the United States, hosting the first visit to Australia by an American president, his friend Lyndon B. Johnson. Holt's government introduced the Migration Act 1966, which effectively dismantled the White Australia Policy and increased access to non-European migrants, including refugees fleeing the Vietnam War. Holt also called the 1967 Referendum which removed the discriminatory clause in the Australian Constitution which excluded Aboriginal Australians from being counted in the census – the referendum was one of the few to be overwhelmingly endorsed by the Australian electorate (over 90% voted 'yes'). By the end of 1967, the Liberals' initially popular support for the war in Vietnam was causing increasing public protest.
The Gorton Government increased funding for the arts, setting up the Australian Council for the Arts, the Australian Film Development Corporation and the National Film and Television Training School. The Gorton Government passed legislation establishing equal pay for men and women and increased pensions, allowances and education scholarships, as well as providing free health care to 250,000 of the nation's poor (but not universal health care). Gorton's government kept Australia in the Vietnam War but stopped replacing troops at the end of 1970.
Gorton maintained good relations with the United States and Britain, but pursued closer ties with Asia. The Gorton government experienced a decline in voter support at the 1969 election. State Liberal leaders saw his policies as too Centralist, while other Liberals didn't like his personal behaviour. In 1971, Defence Minister Malcolm Fraser, resigned and said Gorton was "not fit to hold the great office of Prime Minister". In a vote on the leadership the Liberal Party split 50/50, and although this was insufficient to remove him as the leader, Gorton decided this was also insufficient support for him, and he resigned.
During McMahon's period in office, Neville Bonner joined the Senate and became the first Indigenous Australian in the Australian Parliament. Bonner was chosen by the Liberal Party to fill a Senate vacancy in 1971 and celebrated his maiden parliamentary speech with a boomerang throwing display on the lawns of Parliament. Bonner went on to win election at the 1972 election and served as a Liberal Senator for 12 years. He worked on Indigenous and social welfare issues and proved an independent minded Senator, often crossing the floor on Parliamentary votes.
Following the 1974–75 Loans Affair, the Malcolm Fraser led Liberal-Country Party Coalition argued that the Whitlam Government was incompetent and delayed passage of the Government's money bills in the Senate, until the government would promise a new election. Whitlam refused, Fraser insisted leading to the divisive 1975 Australian constitutional crisis. The deadlock came to an end when the Whitlam government was dismissed by the Governor-General, Sir John Kerr on 11 November 1975 and Fraser was installed as caretaker Prime Minister, pending an election. Fraser won in a landslide at the resulting 1975 election.
Fraser maintained some of the social reforms of the Whitlam era, while seeking increased fiscal restraint. His government included the first Aboriginal federal parliamentarian, Neville Bonner, and in 1976, Parliament passed the Aboriginal Land Rights Act 1976, which, while limited to the Northern Territory, affirmed "inalienable" freehold title to some traditional lands. Fraser established the multicultural broadcaster SBS, accepted Vietnamese refugees, opposed minority white rule in Apartheid South Africa and Rhodesia and opposed Soviet expansionism. A significant program of economic reform however was not pursued. By 1983, the Australian economy was suffering with the early 1980s recession and amidst the effects of a severe drought. Fraser had promoted "states' rights" and his government refused to use Commonwealth powers to stop the construction of the Franklin Dam in Tasmania in 1982. Liberal minister, Don Chipp split off from the party to form a new social liberal party, the Australian Democrats in 1977. Fraser won further substantial majorities at the 1977 and 1980 elections, before losing to the Bob Hawke led Australian Labor Party in the 1983 election.
Howard differed from his Labor predecessor Paul Keating in that he supported traditional Australian institutions like the Monarchy in Australia, the commemoration of ANZAC Day and the design of the Australian flag, but like Keating he pursued privatisation of public utilities and the introduction of a broad based consumption tax (although Keating had dropped support for a GST by the time of his 1993 election victory). Howard's premiership coincided with Al Qaeda's 11 September attacks on the United States. The Howard Government invoked the ANZUS treaty in response to the attacks and supported America's campaigns in Afghanistan and Iraq.
Through 2010, the party improved its vote in the Tasmanian and South Australian state elections and achieved state government in Victoria. In March 2011, the New South Wales Liberal-National Coalition led by Barry O'Farrell won government with the largest election victory in post-war Australian history at the State Election. In Queensland, the Liberal and National parties merged in 2008 to form the new Liberal National Party of Queensland (registered as the Queensland Division of the Liberal Party of Australia). In March 2012, the new party achieved Government in an historic landslide, led by former Brisbane Lord Mayor, Campbell Newman.
Following the 2007 Federal Election, Dr Brendan Nelson was elected leader by the Parliamentary Liberal Party. On 16 September 2008, in a second contest following a spill motion, Nelson lost the leadership to Malcolm Turnbull. On 1 December 2009, a subsequent leadership election saw Turnbull lose the leadership to Tony Abbott by 42 votes to 41 on the second ballot. Abbott led the party to the 2010 federal election, which saw an increase in the Liberal Party vote and resulted in the first hung parliament since the 1940 election.
The party's leader is Malcolm Turnbull and its deputy leader is Julie Bishop. The pair were elected to their positions at the September 2015 Liberal leadership ballot, Bishop as the incumbent deputy leader and Turnbull as a replacement for Tony Abbott, whom he consequently succeeded as Prime Minister of Australia. Now the Turnbull Government, the party had been elected at the 2013 federal election as the Abbott Government which took office on 18 September 2013. At state and territory level, the Liberal Party is in office in three states: Colin Barnett has been Premier of Western Australia since 2008, Will Hodgman Premier of Tasmania since 2014 and Mike Baird Premier of New South Wales since 2014. Adam Giles is also the Chief Minister of the Northern Territory, having led a Country Liberal minority government since 2015. The party is in opposition in Victoria, Queensland, South Australia and the Australian Capital Territory.
Socially, while liberty and freedom of enterprise form the basis of its beliefs, elements of the party have wavered between what is termed "small-l liberalism" and social conservatism. Historically, Liberal Governments have been responsible for the carriage of a number of notable "socially liberal" reforms, including the opening of Australia to multiethnic immigration under Menzies and Harold Holt; Holt's 1967 Referendum on Aboriginal Rights; Sir John Gorton's support for cinema and the arts; selection of the first Aboriginal Senator, Neville Bonner, in 1971; and Malcolm Fraser's Aboriginal Land Rights Act 1976. A West Australian Liberal, Ken Wyatt, became the first Indigenous Australian elected to the House of Representatives in 2010.
The Liberal Party's organisation is dominated by the six state divisions, reflecting the party's original commitment to a federalised system of government (a commitment which was strongly maintained by all Liberal governments until 1983, but was to a large extent abandoned by the Howard Government, which showed strong centralising tendencies). Menzies deliberately created a weak national party machine and strong state divisions. Party policy is made almost entirely by the parliamentary parties, not by the party's rank-and-file members, although Liberal party members do have a degree of influence over party policy.
Menzies ran strongly against Labor's plans to nationalise the Australian banking system and, following victory in the 1949 election, secured a double dissolution election for April 1951, after the Labor-controlled Senate refused to pass his banking legislation. The Liberal-Country Coalition was returned with control of the Senate. The Government was returned again in the 1954 election; the formation of the anti-Communist Democratic Labor Party (DLP) and the consequent split in the Australian Labor Party early in 1955 helped the Liberals to another victory in December 1955. John McEwen replaced Arthur Fadden as leader of the Country Party in March 1958 and the Menzies-McEwen Coalition was returned again at elections in November 1958 – their third victory against Labor's H. V. Evatt. The Coalition was narrowly returned against Labor's Arthur Calwell in the December 1961 election, in the midst of a credit squeeze. Menzies stood for office for the last time in the November 1963 election, again defeating Calwell, with the Coalition winning back its losses in the House of Representatives. Menzies went on to resign from parliament on 26 January 1966.
A period of division for the Liberals followed, with former Treasurer John Howard competing with former Foreign Minister Andrew Peacock for supremacy. The Australian economy was facing the early 1990s recession. Unemployment reached 11.4% in 1992. Under Dr John Hewson, in November 1991, the opposition launched the 650-page Fightback! policy document − a radical collection of "dry", economic liberal measures including the introduction of a Goods and Services Tax (GST), various changes to Medicare including the abolition of bulk billing for non-concession holders, the introduction of a nine-month limit on unemployment benefits, various changes to industrial relations including the abolition of awards, a $13 billion personal income tax cut directed at middle and upper income earners, $10 billion in government spending cuts, the abolition of state payroll taxes and the privatisation of a large number of government owned enterprises − representing the start of a very different future direction to the keynesian economic conservatism practiced by previous Liberal/National Coalition governments. The 15 percent GST was the centerpiece of the policy document. Through 1992, Labor Prime Minister Paul Keating mounted a campaign against the Fightback package, and particularly against the GST, which he described as an attack on the working class in that it shifted the tax burden from direct taxation of the wealthy to indirect taxation as a broad-based consumption tax. Pressure group activity and public opinion was relentless, which led Hewson to exempt food from the proposed GST − leading to questions surrounding the complexity of what food was and wasn't to be exempt from the GST. Hewson's difficulty in explaining this to the electorate was exemplified in the infamous birthday cake interview, considered by some as a turning point in the election campaign. Keating won a record fifth consecutive Labor term at the 1993 election. A number of the proposals were later adopted in to law in some form, to a small extent during the Keating Labor government, and to a larger extent during the Howard Liberal government (most famously the GST), while unemployment benefits and bulk billing were re-targeted for a time by the Abbott Liberal government.
In South Australia, initially a Liberal and Country Party affiliated party, the Liberal and Country League (LCL), mostly led by Premier of South Australia Tom Playford, was in power from the 1933 election to the 1965 election, though with assistance from an electoral malapportionment, or gerrymander, known as the Playmander. The LCL's Steele Hall governed for one term from the 1968 election to the 1970 election and during this time began the process of dismantling the Playmander. David Tonkin, as leader of the South Australian Division of the Liberal Party of Australia, became Premier at the 1979 election for one term, losing office at the 1982 election. The Liberals returned to power at the 1993 election, led by Premiers Dean Brown, John Olsen and Rob Kerin through two terms, until their defeat at the 2002 election. They have since remained in opposition under a record five Opposition Leaders.
Found in applications as diverse as industrial fans, blowers and pumps, machine tools, household appliances, power tools, and disk drives, electric motors can be powered by direct current (DC) sources, such as from batteries, motor vehicles or rectifiers, or by alternating current (AC) sources, such as from the power grid, inverters or generators. Small motors may be found in electric watches. General-purpose motors with highly standardized dimensions and characteristics provide convenient mechanical power for industrial use. The largest of electric motors are used for ship propulsion, pipeline compression and pumped-storage applications with ratings reaching 100 megawatts. Electric motors may be classified by electric power source type, internal construction, application, type of motion output, and so on.
Perhaps the first electric motors were simple electrostatic devices created by the Scottish monk Andrew Gordon in the 1740s. The theoretical principle behind production of mechanical force by the interactions of an electric current and a magnetic field, Ampère's force law, was discovered later by André-Marie Ampère in 1820. The conversion of electrical energy into mechanical energy by electromagnetic means was demonstrated by the British scientist Michael Faraday in 1821. A free-hanging wire was dipped into a pool of mercury, on which a permanent magnet (PM) was placed. When a current was passed through the wire, the wire rotated around the magnet, showing that the current gave rise to a close circular magnetic field around the wire. This motor is often demonstrated in physics experiments, brine substituting for toxic mercury. Though Barlow's wheel was an early refinement to this Faraday demonstration, these and similar homopolar motors were to remain unsuited to practical application until late in the century.
In 1827, Hungarian physicist Ányos Jedlik started experimenting with electromagnetic coils. After Jedlik solved the technical problems of the continuous rotation with the invention of the commutator, he called his early devices "electromagnetic self-rotors". Although they were used only for instructional purposes, in 1828 Jedlik demonstrated the first device to contain the three main components of practical DC motors: the stator, rotor and commutator. The device employed no permanent magnets, as the magnetic fields of both the stationary and revolving components were produced solely by the currents flowing through their windings.
After many other more or less successful attempts with relatively weak rotating and reciprocating apparatus the Prussian Moritz von Jacobi created the first real rotating electric motor in May 1834 that actually developed a remarkable mechanical output power. His motor set a world record which was improved only four years later in September 1838 by Jacobi himself. His second motor was powerful enough to drive a boat with 14 people across a wide river. It was not until 1839/40 that other developers worldwide managed to build motors of similar and later also of higher performance.
The first commutator DC electric motor capable of turning machinery was invented by the British scientist William Sturgeon in 1832. Following Sturgeon's work, a commutator-type direct-current electric motor made with the intention of commercial use was built by the American inventor Thomas Davenport, which he patented in 1837. The motors ran at up to 600 revolutions per minute, and powered machine tools and a printing press. Due to the high cost of primary battery power, the motors were commercially unsuccessful and Davenport went bankrupt. Several inventors followed Sturgeon in the development of DC motors but all encountered the same battery power cost issues. No electricity distribution had been developed at the time. Like Sturgeon's motor, there was no practical commercial market for these motors.
A major turning point in the development of DC machines took place in 1864, when Antonio Pacinotti described for the first time the ring armature with its symmetrically grouped coils closed upon themselves and connected to the bars of a commutator, the brushes of which delivered practically non-fluctuating current. The first commercially successful DC motors followed the invention by Zénobe Gramme who, in 1871, reinvented Pacinotti's design. In 1873, Gramme showed that his dynamo could be used as a motor, which he demonstrated to great effect at exhibitions in Vienna and Philadelphia by connecting two such DC motors at a distance of up to 2 km away from each other, one as a generator. (See also 1873 : l'expérience décisive [Decisive Workaround] .)
In 1886, Frank Julian Sprague invented the first practical DC motor, a non-sparking motor that maintained relatively constant speed under variable loads. Other Sprague electric inventions about this time greatly improved grid electric distribution (prior work done while employed by Thomas Edison), allowed power from electric motors to be returned to the electric grid, provided for electric distribution to trolleys via overhead wires and the trolley pole, and provided controls systems for electric operations. This allowed Sprague to use electric motors to invent the first electric trolley system in 1887–88 in Richmond VA, the electric elevator and control system in 1892, and the electric subway with independently powered centrally controlled cars, which were first installed in 1892 in Chicago by the South Side Elevated Railway where it became popularly known as the "L". Sprague's motor and related inventions led to an explosion of interest and use in electric motors for industry, while almost simultaneously another great inventor was developing its primary competitor, which would become much more widespread. The development of electric motors of acceptable efficiency was delayed for several decades by failure to recognize the extreme importance of a relatively small air gap between rotor and stator. Efficient designs have a comparatively small air gap. [a] The St. Louis motor, long used in classrooms to illustrate motor principles, is extremely inefficient for the same reason, as well as appearing nothing like a modern motor.
Application of electric motors revolutionized industry. Industrial processes were no longer limited by power transmission using line shafts, belts, compressed air or hydraulic pressure. Instead every machine could be equipped with its own electric motor, providing easy control at the point of use, and improving power transmission efficiency. Electric motors applied in agriculture eliminated human and animal muscle power from such tasks as handling grain or pumping water. Household uses of electric motors reduced heavy labor in the home and made higher standards of convenience, comfort and safety possible. Today, electric motors stand for more than half of the electric energy consumption in the US.
In 1824, the French physicist François Arago formulated the existence of rotating magnetic fields, termed Arago's rotations, which, by manually turning switches on and off, Walter Baily demonstrated in 1879 as in effect the first primitive induction motor.  In the 1880s, many inventors were trying to develop workable AC motors because AC's advantages in long-distance high-voltage transmission were counterbalanced by the inability to operate motors on AC. The first alternating-current commutatorless induction motors were independently invented by Galileo Ferraris and Nikola Tesla, a working motor model having been demonstrated by the former in 1885 and by the latter in 1887. In 1888, the Royal Academy of Science of Turin published Ferraris' research detailing the foundations of motor operation while however concluding that "the apparatus based on that principle could not be of any commercial importance as motor." In 1888, Tesla presented his paper A New System for Alternating Current Motors and Transformers to the AIEE that described three patented two-phase four-stator-pole motor types: one with a four-pole rotor forming a non-self-starting reluctance motor, another with a wound rotor forming a self-starting induction motor, and the third a true synchronous motor with separately excited DC supply to rotor winding. One of the patents Tesla filed in 1887, however, also described a shorted-winding-rotor induction motor. George Westinghouse promptly bought Tesla's patents, employed Tesla to develop them, and assigned C. F. Scott to help Tesla, Tesla left for other pursuits in 1889.  The constant speed AC induction motor was found not to be suitable for street cars but Westinghouse engineers successfully adapted it to power a mining operation in Telluride, Colorado in 1891. Steadfast in his promotion of three-phase development, Mikhail Dolivo-Dobrovolsky invented the three-phase cage-rotor induction motor in 1889 and the three-limb transformer in 1890. This type of motor is now used for the vast majority of commercial applications. However, he claimed that Tesla's motor was not practical because of two-phase pulsations, which prompted him to persist in his three-phase work. Although Westinghouse achieved its first practical induction motor in 1892 and developed a line of polyphase 60 hertz induction motors in 1893, these early Westinghouse motors were two-phase motors with wound rotors until B. G. Lamme developed a rotating bar winding rotor. The General Electric Company began developing three-phase induction motors in 1891. By 1896, General Electric and Westinghouse signed a cross-licensing agreement for the bar-winding-rotor design, later called the squirrel-cage rotor. Induction motor improvements flowing from these inventions and innovations were such that a 100 horsepower (HP) induction motor currently has the same mounting dimensions as a 7.5 HP motor in 1897.
A commutator is a mechanism used to switch the input of most DC machines and certain AC machines consisting of slip ring segments insulated from each other and from the electric motor's shaft. The motor's armature current is supplied through the stationary brushes in contact with the revolving commutator, which causes required current reversal and applies power to the machine in an optimal manner as the rotor rotates from pole to pole. In absence of such current reversal, the motor would brake to a stop. In light of significant advances in the past few decades due to improved technologies in electronic controller, sensorless control, induction motor, and permanent magnet motor fields, electromechanically commutated motors are increasingly being displaced by externally commutated induction and permanent-magnet motors.
A commutated DC motor has a set of rotating windings wound on an armature mounted on a rotating shaft. The shaft also carries the commutator, a long-lasting rotary electrical switch that periodically reverses the flow of current in the rotor windings as the shaft rotates. Thus, every brushed DC motor has AC flowing through its rotating windings. Current flows through one or more pairs of brushes that bear on the commutator; the brushes connect an external source of electric power to the rotating armature.
The rotating armature consists of one or more coils of wire wound around a laminated, magnetically "soft" ferromagnetic core. Current from the brushes flows through the commutator and one winding of the armature, making it a temporary magnet (an electromagnet). The magnetic field produced by the armature interacts with a stationary magnetic field produced by either PMs or another winding a field coil, as part of the motor frame. The force between the two magnetic fields tends to rotate the motor shaft. The commutator switches power to the coils as the rotor turns, keeping the magnetic poles of the rotor from ever fully aligning with the magnetic poles of the stator field, so that the rotor never stops (like a compass needle does), but rather keeps rotating as long as power is applied.
Many of the limitations of the classic commutator DC motor are due to the need for brushes to press against the commutator. This creates friction. Sparks are created by the brushes making and breaking circuits through the rotor coils as the brushes cross the insulating gaps between commutator sections. Depending on the commutator design, this may include the brushes shorting together adjacent sections – and hence coil ends – momentarily while crossing the gaps. Furthermore, the inductance of the rotor coils causes the voltage across each to rise when its circuit is opened, increasing the sparking of the brushes. This sparking limits the maximum speed of the machine, as too-rapid sparking will overheat, erode, or even melt the commutator. The current density per unit area of the brushes, in combination with their resistivity, limits the output of the motor. The making and breaking of electric contact also generates electrical noise; sparking generates RFI. Brushes eventually wear out and require replacement, and the commutator itself is subject to wear and maintenance (on larger motors) or replacement (on small motors). The commutator assembly on a large motor is a costly element, requiring precision assembly of many parts. On small motors, the commutator is usually permanently integrated into the rotor, so replacing it usually requires replacing the whole rotor.
Large brushes are desired for a larger brush contact area to maximize motor output, but small brushes are desired for low mass to maximize the speed at which the motor can run without the brushes excessively bouncing and sparking. (Small brushes are also desirable for lower cost.) Stiffer brush springs can also be used to make brushes of a given mass work at a higher speed, but at the cost of greater friction losses (lower efficiency) and accelerated brush and commutator wear. Therefore, DC motor brush design entails a trade-off between output power, speed, and efficiency/wear.
A PM motor does not have a field winding on the stator frame, instead relying on PMs to provide the magnetic field against which the rotor field interacts to produce torque. Compensating windings in series with the armature may be used on large motors to improve commutation under load. Because this field is fixed, it cannot be adjusted for speed control. PM fields (stators) are convenient in miniature motors to eliminate the power consumption of the field winding. Most larger DC motors are of the "dynamo" type, which have stator windings. Historically, PMs could not be made to retain high flux if they were disassembled; field windings were more practical to obtain the needed amount of flux. However, large PMs are costly, as well as dangerous and difficult to assemble; this favors wound fields for large machines.
To minimize overall weight and size, miniature PM motors may use high energy magnets made with neodymium or other strategic elements; most such are neodymium-iron-boron alloy. With their higher flux density, electric machines with high-energy PMs are at least competitive with all optimally designed singly-fed synchronous and induction electric machines. Miniature motors resemble the structure in the illustration, except that they have at least three rotor poles (to ensure starting, regardless of rotor position) and their outer housing is a steel tube that magnetically links the exteriors of the curved field magnets.
Operating at normal power line frequencies, universal motors are often found in a range less than 1000 watts. Universal motors also formed the basis of the traditional railway traction motor in electric railways. In this application, the use of AC to power a motor originally designed to run on DC would lead to efficiency losses due to eddy current heating of their magnetic components, particularly the motor field pole-pieces that, for DC, would have used solid (un-laminated) iron and they are now rarely used.
An advantage of the universal motor is that AC supplies may be used on motors which have some characteristics more common in DC motors, specifically high starting torque and very compact design if high running speeds are used. The negative aspect is the maintenance and short life problems caused by the commutator. Such motors are used in devices such as food mixers and power tools which are used only intermittently, and often have high starting-torque demands. Multiple taps on the field coil provide (imprecise) stepped speed control. Household blenders that advertise many speeds frequently combine a field coil with several taps and a diode that can be inserted in series with the motor (causing the motor to run on half-wave rectified AC). Universal motors also lend themselves to electronic speed control and, as such, are an ideal choice for devices like domestic washing machines. The motor can be used to agitate the drum (both forwards and in reverse) by switching the field winding with respect to the armature.
Whereas SCIMs cannot turn a shaft faster than allowed by the power line frequency, universal motors can run at much higher speeds. This makes them useful for appliances such as blenders, vacuum cleaners, and hair dryers where high speed and light weight are desirable. They are also commonly used in portable power tools, such as drills, sanders, circular and jig saws, where the motor's characteristics work well. Many vacuum cleaner and weed trimmer motors exceed 10,000 rpm, while many similar miniature grinders exceed 30,000 rpm.
Currents induced into this winding provide the rotor magnetic field. The shape of the rotor bars determines the speed-torque characteristics. At low speeds, the current induced in the squirrel cage is nearly at line frequency and tends to be in the outer parts of the rotor cage. As the motor accelerates, the slip frequency becomes lower, and more current is in the interior of the winding. By shaping the bars to change the resistance of the winding portions in the interior and outer parts of the cage, effectively a variable resistance is inserted in the rotor circuit. However, the majority of such motors have uniform bars.
In a WRIM, the rotor winding is made of many turns of insulated wire and is connected to slip rings on the motor shaft. An external resistor or other control devices can be connected in the rotor circuit. Resistors allow control of the motor speed, although significant power is dissipated in the external resistance. A converter can be fed from the rotor circuit and return the slip-frequency power that would otherwise be wasted back into the power system through an inverter or separate motor-generator.
When used with a load that has a torque curve that increases with speed, the motor will operate at the speed where the torque developed by the motor is equal to the load torque. Reducing the load will cause the motor to speed up, and increasing the load will cause the motor to slow down until the load and motor torque are equal. Operated in this manner, the slip losses are dissipated in the secondary resistors and can be very significant. The speed regulation and net efficiency is also very poor.
A common application of a torque motor would be the supply- and take-up reel motors in a tape drive. In this application, driven from a low voltage, the characteristics of these motors allow a relatively constant light tension to be applied to the tape whether or not the capstan is feeding tape past the tape heads. Driven from a higher voltage, (and so delivering a higher torque), the torque motors can also achieve fast-forward and rewind operation without requiring any additional mechanics such as gears or clutches. In the computer gaming world, torque motors are used in force feedback steering wheels.
Another common application is the control of the throttle of an internal combustion engine in conjunction with an electronic governor. In this usage, the motor works against a return spring to move the throttle in accordance with the output of the governor. The latter monitors engine speed by counting electrical pulses from the ignition system or from a magnetic pickup and, depending on the speed, makes small adjustments to the amount of current applied to the motor. If the engine starts to slow down relative to the desired speed, the current will be increased, the motor will develop more torque, pulling against the return spring and opening the throttle. Should the engine run too fast, the governor will reduce the current being applied to the motor, causing the return spring to pull back and close the throttle.
A synchronous electric motor is an AC motor distinguished by a rotor spinning with coils passing magnets at the same rate as the AC and resulting magnetic field which drives it. Another way of saying this is that it has zero slip under usual operating conditions. Contrast this with an induction motor, which must slip to produce torque. One type of synchronous motor is like an induction motor except the rotor is excited by a DC field. Slip rings and brushes are used to conduct current to the rotor. The rotor poles connect to each other and move at the same speed hence the name synchronous motor. Another type, for low load torque, has flats ground onto a conventional squirrel-cage rotor to create discrete poles. Yet another, such as made by Hammond for its pre-World War II clocks, and in the older Hammond organs, has no rotor windings and discrete poles. It is not self-starting. The clock requires manual starting by a small knob on the back, while the older Hammond organs had an auxiliary starting motor connected by a spring-loaded manually operated switch.
Finally, hysteresis synchronous motors typically are (essentially) two-phase motors with a phase-shifting capacitor for one phase. They start like induction motors, but when slip rate decreases sufficiently, the rotor (a smooth cylinder) becomes temporarily magnetized. Its distributed poles make it act like a PMSM. The rotor material, like that of a common nail, will stay magnetized, but can also be demagnetized with little difficulty. Once running, the rotor poles stay in place; they do not drift.
Doubly fed electric motors have two independent multiphase winding sets, which contribute active (i.e., working) power to the energy conversion process, with at least one of the winding sets electronically controlled for variable speed operation. Two independent multiphase winding sets (i.e., dual armature) are the maximum provided in a single package without topology duplication. Doubly-fed electric motors are machines with an effective constant torque speed range that is twice synchronous speed for a given frequency of excitation. This is twice the constant torque speed range as singly-fed electric machines, which have only one active winding set.
Nothing in the principle of any of the motors described above requires that the iron (steel) portions of the rotor actually rotate. If the soft magnetic material of the rotor is made in the form of a cylinder, then (except for the effect of hysteresis) torque is exerted only on the windings of the electromagnets. Taking advantage of this fact is the coreless or ironless DC motor, a specialized form of a PM DC motor. Optimized for rapid acceleration, these motors have a rotor that is constructed without any iron core. The rotor can take the form of a winding-filled cylinder, or a self-supporting structure comprising only the magnet wire and the bonding material. The rotor can fit inside the stator magnets; a magnetically soft stationary cylinder inside the rotor provides a return path for the stator magnetic flux. A second arrangement has the rotor winding basket surrounding the stator magnets. In that design, the rotor fits inside a magnetically soft cylinder that can serve as the housing for the motor, and likewise provides a return path for the flux.
Because the rotor is much lighter in weight (mass) than a conventional rotor formed from copper windings on steel laminations, the rotor can accelerate much more rapidly, often achieving a mechanical time constant under one ms. This is especially true if the windings use aluminum rather than the heavier copper. But because there is no metal mass in the rotor to act as a heat sink, even small coreless motors must often be cooled by forced air. Overheating might be an issue for coreless DC motor designs.
These motors were originally invented to drive the capstan(s) of magnetic tape drives in the burgeoning computer industry, where minimal time to reach operating speed and minimal stopping distance were critical. Pancake motors are still widely used in high-performance servo-controlled systems, robotic systems, industrial automation and medical devices. Due to the variety of constructions now available, the technology is used in applications from high temperature military to low cost pump and basic servos.
A servomotor is a motor, very often sold as a complete module, which is used within a position-control or speed-control feedback control system mainly control valves, such as motor-operated control valves. Servomotors are used in applications such as machine tools, pen plotters, and other process systems. Motors intended for use in a servomechanism must have well-documented characteristics for speed, torque, and power. The speed vs. torque curve is quite important and is high ratio for a servo motor. Dynamic response characteristics such as winding inductance and rotor inertia are also important; these factors limit the overall performance of the servomechanism loop. Large, powerful, but slow-responding servo loops may use conventional AC or DC motors and drive systems with position or speed feedback on the motor. As dynamic response requirements increase, more specialized motor designs such as coreless motors are used. AC motors' superior power density and acceleration characteristics compared to that of DC motors tends to favor PM synchronous, BLDC, induction, and SRM drive applications.
A servo system differs from some stepper motor applications in that the position feedback is continuous while the motor is running; a stepper system relies on the motor not to "miss steps" for short term accuracy, although a stepper system may include a "home" switch or other element to provide long-term stability of control. For instance, when a typical dot matrix computer printer starts up, its controller makes the print head stepper motor drive to its left-hand limit, where a position sensor defines home position and stops stepping. As long as power is on, a bidirectional counter in the printer's microprocessor keeps track of print-head position.
Stepper motors are a type of motor frequently used when precise rotations are required. In a stepper motor an internal rotor containing PMs or a magnetically soft rotor with salient poles is controlled by a set of external magnets that are switched electronically. A stepper motor may also be thought of as a cross between a DC electric motor and a rotary solenoid. As each coil is energized in turn, the rotor aligns itself with the magnetic field produced by the energized field winding. Unlike a synchronous motor, in its application, the stepper motor may not rotate continuously; instead, it "steps"—starts and then quickly stops again—from one position to the next as field windings are energized and de-energized in sequence. Depending on the sequence, the rotor may turn forwards or backwards, and it may change direction, stop, speed up or slow down arbitrarily at any time.
Simple stepper motor drivers entirely energize or entirely de-energize the field windings, leading the rotor to "cog" to a limited number of positions; more sophisticated drivers can proportionally control the power to the field windings, allowing the rotors to position between the cog points and thereby rotate extremely smoothly. This mode of operation is often called microstepping. Computer controlled stepper motors are one of the most versatile forms of positioning systems, particularly when part of a digital servo-controlled system.
Stepper motors can be rotated to a specific angle in discrete steps with ease, and hence stepper motors are used for read/write head positioning in computer floppy diskette drives. They were used for the same purpose in pre-gigabyte era computer disk drives, where the precision and speed they offered was adequate for the correct positioning of the read/write head of a hard disk drive. As drive density increased, the precision and speed limitations of stepper motors made them obsolete for hard drives—the precision limitation made them unusable, and the speed limitation made them uncompetitive—thus newer hard disk drives use voice coil-based head actuator systems. (The term "voice coil" in this connection is historic; it refers to the structure in a typical (cone type) loudspeaker. This structure was used for a while to position the heads. Modern drives have a pivoted coil mount; the coil swings back and forth, something like a blade of a rotating fan. Nevertheless, like a voice coil, modern actuator coil conductors (the magnet wire) move perpendicular to the magnetic lines of force.)
Stepper motors were and still are often used in computer printers, optical scanners, and digital photocopiers to move the optical scanning element, the print head carriage (of dot matrix and inkjet printers), and the platen or feed rollers. Likewise, many computer plotters (which since the early 1990s have been replaced with large-format inkjet and laser printers) used rotary stepper motors for pen and platen movement; the typical alternatives here were either linear stepper motors or servomotors with closed-loop analog control systems.
Since the armature windings of a direct-current or universal motor are moving through a magnetic field, they have a voltage induced in them. This voltage tends to oppose the motor supply voltage and so is called "back electromotive force (emf)". The voltage is proportional to the running speed of the motor. The back emf of the motor, plus the voltage drop across the winding internal resistance and brushes, must equal the voltage at the brushes. This provides the fundamental mechanism of speed regulation in a DC motor. If the mechanical load increases, the motor slows down; a lower back emf results, and more current is drawn from the supply. This increased current provides the additional torque to balance the new load.
All the electromagnetic motors, and that includes the types mentioned here derive the torque from the vector product of the interacting fields. For calculating the torque it is necessary to know the fields in the air gap . Once these have been established by mathematical analysis using FEA or other tools the torque may be calculated as the integral of all the vectors of force multiplied by the radius of each vector. The current flowing in the winding is producing the fields and for a motor using a magnetic material the field is not linearly proportional to the current. This makes the calculation difficult but a computer can do the many calculations needed.
When optimally designed within a given core saturation constraint and for a given active current (i.e., torque current), voltage, pole-pair number, excitation frequency (i.e., synchronous speed), and air-gap flux density, all categories of electric motors or generators will exhibit virtually the same maximum continuous shaft torque (i.e., operating torque) within a given air-gap area with winding slots and back-iron depth, which determines the physical size of electromagnetic core. Some applications require bursts of torque beyond the maximum operating torque, such as short bursts of torque to accelerate an electric vehicle from standstill. Always limited by magnetic core saturation or safe operating temperature rise and voltage, the capacity for torque bursts beyond the maximum operating torque differs significantly between categories of electric motors or generators.
The brushless wound-rotor synchronous doubly-fed (BWRSDF) machine is the only electric machine with a truly dual ported transformer circuit topology (i.e., both ports independently excited with no short-circuited port). The dual ported transformer circuit topology is known to be unstable and requires a multiphase slip-ring-brush assembly to propagate limited power to the rotor winding set. If a precision means were available to instantaneously control torque angle and slip for synchronous operation during motoring or generating while simultaneously providing brushless power to the rotor winding set, the active current of the BWRSDF machine would be independent of the reactive impedance of the transformer circuit and bursts of torque significantly higher than the maximum operating torque and far beyond the practical capability of any other type of electric machine would be realizable. Torque bursts greater than eight times operating torque have been calculated.
The continuous torque density of conventional electric machines is determined by the size of the air-gap area and the back-iron depth, which are determined by the power rating of the armature winding set, the speed of the machine, and the achievable air-gap flux density before core saturation. Despite the high coercivity of neodymium or samarium-cobalt PMs, continuous torque density is virtually the same amongst electric machines with optimally designed armature winding sets. Continuous torque density relates to method of cooling and permissible period of operation before destruction by overheating of windings or PM damage.
An electrostatic motor is based on the attraction and repulsion of electric charge. Usually, electrostatic motors are the dual of conventional coil-based motors. They typically require a high-voltage power supply, although very small motors employ lower voltages. Conventional electric motors instead employ magnetic attraction and repulsion, and require high current at low voltages. In the 1750s, the first electrostatic motors were developed by Benjamin Franklin and Andrew Gordon. Today the electrostatic motor finds frequent use in micro-electro-mechanical systems (MEMS) where their drive voltages are below 100 volts, and where moving, charged plates are far easier to fabricate than coils and iron cores. Also, the molecular machinery which runs living cells is often based on linear and rotary electrostatic motors.[citation needed]
The Royal College of Chemistry was established by private subscription in 1845 as there was a growing awareness that practical aspects of the experimental sciences were not well taught and that in the United Kingdom the teaching of chemistry in particular had fallen behind that in Germany. As a result of a movement earlier in the decade, many politicians donated funds to establish the college, including Benjamin Disraeli, William Gladstone and Robert Peel. It was also supported by Prince Albert, who persuaded August Wilhelm von Hofmann to be the first professor.
City and Guilds College was founded in 1876 from a meeting of 16 of the City of London's livery companies for the Advancement of Technical Education (CGLI), which aimed to improve the training of craftsmen, technicians, technologists, and engineers. The two main objectives were to create a Central Institution in London and to conduct a system of qualifying examinations in technical subjects. Faced with their continuing inability to find a substantial site, the Companies were eventually persuaded by the Secretary of the Science and Art Department, General Sir John Donnelly (who was also a Royal Engineer) to found their institution on the eighty-seven acre (350,000 m²) site at South Kensington bought by the 1851 Exhibition Commissioners (for GBP 342,500) for 'purposes of art and science' in perpetuity. The latter two colleges were incorporated by Royal Charter into the Imperial College of Science and Technology and the CGLI Central Technical College was renamed the City and Guilds College in 1907, but not incorporated into Imperial College until 1910.
In December 2005, Imperial announced a science park programme at the Wye campus, with extensive housing; however, this was abandoned in September 2006 following complaints that the proposal infringed on Areas of Outstanding Natural Beauty, and that the true scale of the scheme, which could have raised £110m for the College, was known to Kent and Ashford Councils and their consultants but concealed from the public. One commentator observed that Imperial's scheme reflected "the state of democracy in Kent, the transformation of a renowned scientific college into a grasping, highly aggressive, neo-corporate institution, and the defence of the status of an Area of Outstanding Natural Beauty – throughout England, not just Wye – against rampant greed backed by the connivance of two important local authorities. Wye College campus was finally closed in September 2009.
The College's endowment is sub-divided into three distinct portfolios: (i) Unitised Scheme – a unit trust vehicle for College, Faculties and Departments to invest endowments and unfettered income to produce returns for the long term; (ii) Non-Core Property – a portfolio containing around 120 operational and developmental properties which College has determined are not core to the academic mission; and (iii) Strategic Asset Investments – containing College’s shareholding in Imperial Innovations and other restricted equity holdings. During the year 2014/15, the market value of the endowment increased by £78 million (18%) to £512.4 million on 31 July 2015.
Imperial submitted a total of 1,257 staff across 14 units of assessment to the 2014 Research Excellence Framework (REF) assessment. In the REF results 46% of Imperial's submitted research was classified as 4*, 44% as 3*, 9% as 2* and 1% as 1*, giving an overall GPA of 3.36. In rankings produced by Times Higher Education based upon the REF results Imperial was ranked 2nd overall for GPA and 8th for "research power" (compared to 6th and 7th respectively in the equivalent rankings for the RAE 2008).
In September 2014, Professor Stefan Grimm, of the Department of Medicine, was found dead after being threatened with dismissal for failure to raise enough grant money. The College made its first public announcement of his death on 4 December 2014. Grimm's last email accused his employers of bullying by demanding that he should get grants worth at least £200,000 per year. His last email was viewed more than 100,000 times in the first four days after it was posted. The College has announced an internal inquiry into Stefan Grimm's death. The inquest on his death has not yet reported.
Imperial College Boat Club
The Imperial College Boat Club was founded on 12 December 1919. The Gold medal winning GB 8+ at the 2000 Sydney Olympics had been based at Imperial College's recently refurbished boathouse and included 3 alumni of the college along with their coach Martin McElroy. The club has been highly successful, with many wins at Henley Royal Regatta including most recently in 2013 with victory in The Prince Albert Challenge Cup event. The club has been home to numerous National Squad oarsmen and women and is open to all rowers not just students of Imperial College London.
The Royal School of Mines was established by Sir Henry de la Beche in 1851, developing from the Museum of Economic Geology, a collection of minerals, maps and mining equipment. He created a school which laid the foundations for the teaching of science in the country, and which has its legacy today at Imperial. Prince Albert was a patron and supporter of the later developments in science teaching, which led to the Royal College of Chemistry becoming part of the Royal School of Mines, to the creation of the Royal College of Science and eventually to these institutions becoming part of his plan for South Kensington being an educational region.
In 2003 Imperial was granted degree-awarding powers in its own right by the Privy Council. The London Centre for Nanotechnology was established in the same year as a joint venture between UCL and Imperial College London. In 2004 the Tanaka Business School (now named the Imperial College Business School) and a new Main Entrance on Exhibition Road were opened by The Queen. The UK Energy Research Centre was also established in 2004 and opened its headquarters at Imperial College. In November 2005 the Faculties of Life Sciences and Physical Sciences merged to become the Faculty of Natural Sciences.
Imperial's main campus is located in the South Kensington area of central London. It is situated in an area of South Kensington, known as Albertopolis, which has a high concentration of cultural and academic institutions, adjacent to the Natural History Museum, the Science Museum, the Victoria and Albert Museum, the Royal College of Music, the Royal College of Art, the Royal Geographical Society and the Royal Albert Hall. Nearby public attractions include the Kensington Palace, Hyde Park and the Kensington Gardens, the National Art Library, and the Brompton Oratory. The expansion of the South Kensington campus in the 1950s & 1960s absorbed the site of the former Imperial Institute, designed by Thomas Collcutt, of which only the 287 foot (87 m) high Queen's Tower remains among the more modern buildings.
The Centre For Co-Curricular Studies provides elective subjects and language courses outside the field of science for students in the other faculties and departments. Students are encouraged to take these classes either for credit or in their own time, and in some departments this is mandatory. Courses exist in a wide range of topics including philosophy, ethics in science and technology, history, modern literature and drama, art in the 20th century, film studies. Language courses are available in French, German, Japanese, Italian, Russian, Spanish, Arabic and Mandarin Chinese. The Centre For Co-Curricular Studies is home to the Science Communication Unit which offers master's degrees in Science Communication and Science Media Production for science graduates.
Furthermore, in terms of job prospects, as of 2014 the average starting salary of an Imperial graduate was the highest of any UK university. In terms of specific course salaries, the Sunday Times ranked Computing graduates from Imperial as earning the second highest average starting salary in the UK after graduation, over all universities and courses. In 2012, the New York Times ranked Imperial College as one of the top 10 most-welcomed universities by the global job market. In May 2014, the university was voted highest in the UK for Job Prospects by students voting in the Whatuni Student Choice Awards Imperial is jointly ranked as the 3rd best university in the UK for the quality of graduates according to recruiters from the UK's major companies.
Imperial College TV
ICTV (formerly STOIC (Student Television of Imperial College)) is Imperial College Union's TV station, founded in 1969 and operated from a small TV studio in the Electrical Engineering block. The department had bought an early AMPEX Type A 1-inch videotape recorder and this was used to produce an occasional short news programme which was then played to students by simply moving the VTR and a monitor into a common room. A cable link to the Southside halls of residence was laid in a tunnel under Exhibition Road in 1972. Besides the news, early productions included a film of the Queen opening what was then called College Block and interview programmes with DJ Mike Raven, Richard O'Brian and Monty Python producer Ian MacNaughton. The society was renamed to ICTV for the start of the 2014/15 academic year.
Non-academic alumni: Author, H. G. Wells, McLaren and Ferrari Chief Designer, Nicholas Tombazis, CEO of Rolls Royce, Ralph Robins, rock band Queen, Brian May, CEO of Singapore Airlines, Chew Choon Seng, Prime Minister of New Zealand, Julius Vogel, Prime Minister of India, Rajiv Gandhi, Deputy Prime Minister of Singapore, Teo Chee Hean, Chief Medical Officer for England, Sir Liam Donaldson, Head Physician to the Queen, Huw Thomas, CEO of Moonfruit, Wendy Tan White, Businessman and philanthropist, Winston Wong, billionaire hedge fund manager Alan Howard.
The Great Exhibition was organised by Prince Albert, Henry Cole, Francis Fuller and other members of the Royal Society for the Encouragement of Arts, Manufactures and Commerce. The Great Exhibition made a surplus of £186,000 used in creating an area in the South of Kensington celebrating the encouragement of the arts, industry, and science. Albert insisted the Great Exhibition surplus should be used as a home for culture and education for everyone. His commitment was to find practical solutions to today's social challenges. Prince Albert's vision built the Victoria and Albert Museum, Science Museum, Natural History Museum, Geological Museum, Royal College of Science, Royal College of Art, Royal School of Mines, Royal School of Music, Royal College of Organists, Royal School of Needlework, Royal Geographical Society, Institute of Recorded Sound, Royal Horticultural Gardens, Royal Albert Hall and the Imperial Institute. Royal colleges and the Imperial Institute merged to form what is now Imperial College London.
In 1907, the newly established Board of Education found that greater capacity for higher technical education was needed and a proposal to merge the City and Guilds College, the Royal School of Mines and the Royal College of Science was approved and passed, creating The Imperial College of Science and Technology as a constituent college of the University of London. Imperial's Royal Charter, granted by Edward VII, was officially signed on 8 July 1907. The main campus of Imperial College was constructed beside the buildings of the Imperial Institute in South Kensington.
In the financial year ended 31 July 2013, Imperial had a total net income of £822.0 million (2011/12 – £765.2 million) and total expenditure of £754.9 million (2011/12 – £702.0 million). Key sources of income included £329.5 million from research grants and contracts (2011/12 – £313.9 million), £186.3 million from academic fees and support grants (2011/12 – £163.1 million), £168.9 million from Funding Council grants (2011/12 – £172.4 million) and £12.5 million from endowment and investment income (2011/12 – £8.1 million). During the 2012/13 financial year Imperial had a capital expenditure of £124 million (2011/12 – £152 million).
In 1988 Imperial merged with St Mary's Hospital Medical School, becoming The Imperial College of Science, Technology and Medicine. In 1995 Imperial launched its own academic publishing house, Imperial College Press, in partnership with World Scientific. Imperial merged with the National Heart and Lung Institute in 1995 and the Charing Cross and Westminster Medical School, Royal Postgraduate Medical School (RPMS) and the Institute of Obstetrics and Gynaecology in 1997. In the same year the Imperial College School of Medicine was formally established and all of the property of Charing Cross and Westminster Medical School, the National Heart and Lung Institute and the Royal Postgraduate Medical School were transferred to Imperial as the result of the Imperial College Act 1997. In 1998 the Sir Alexander Fleming Building was opened by Queen Elizabeth II to provide a headquarters for the College's medical and biomedical research.
The 2008 Research Assessment Exercise returned 26% of the 1225 staff submitted as being world-leading (4*) and a further 47% as being internationally excellent (3*). The 2008 Research Assessment Exercise also showed five subjects – Pure Mathematics, Epidemiology and Public Health, Chemical Engineering, Civil Engineering, and Mechanical, Aeronautical and Manufacturing Engineering – were assessed to be the best[clarification needed] in terms of the proportion of internationally recognised research quality.
Imperial College Healthcare NHS Trust was formed on 1 October 2007 by the merger of Hammersmith Hospitals NHS Trust (Charing Cross Hospital, Hammersmith Hospital and Queen Charlotte's and Chelsea Hospital) and St Mary's NHS Trust (St. Mary's Hospital and Western Eye Hospital) with Imperial College London Faculty of Medicine. It is an academic health science centre and manages five hospitals: Charing Cross Hospital, Queen Charlotte's and Chelsea Hospital, Hammersmith Hospital, St Mary's Hospital, and Western Eye Hospital. The Trust is currently the largest in the UK and has an annual turnover of £800 million, treating more than a million patients a year.[citation needed]
In 2003, it was reported that one third of female academics "believe that discrimination or bullying by managers has held back their careers". It was said then that "A spokesman for Imperial said the college was acting on the recommendations and had already made changes". Nevertheless, allegations of bullying have continued: in 2007, concerns were raised about the methods that were being used to fire people in the Faculty of Medicine. New President of Imperial College, Alice Gast says she sees bright lights in the horizon for female careers at Imperial College London.
Imperial College Union, the students' union at Imperial College, is run by five full-time sabbatical officers elected from the student body for a tenure of one year, and a number of permanent members of staff. The Union is given a large subvention by the university, much of which is spent on maintaining around 300 clubs, projects and societies. Examples of notable student groups and projects are Project Nepal which sends Imperial College students to work on educational development programmes in rural Nepal and the El Salvador Project, a construction based project in Central America. The Union also hosts sports-related clubs such as Imperial College Boat Club and Imperial College Gliding Club.
Imperial College London is a public research university located in London, United Kingdom. It was founded by Prince Albert who envisioned an area composed of the Natural History Museum, Science Museum, Victoria and Albert Museum, Royal Albert Hall and the Imperial Institute. The Imperial Institute was opened by his wife, Queen Victoria, who laid the first stone. In 1907, Imperial College London was formed by Royal Charter, and soon joined the University of London, with a focus on science and technology. The college has expanded its coursework to medicine through mergers with St Mary's Hospital. In 2004, Queen Elizabeth II opened the Imperial College Business School. Imperial became an independent university from the University of London during its one hundred year anniversary.
William Henry Perkin studied and worked at the college under von Hofmann, but resigned his position after discovering the first synthetic dye, mauveine, in 1856. Perkin's discovery was prompted by his work with von Hofmann on the substance aniline, derived from coal tar, and it was this breakthrough which sparked the synthetic dye industry, a boom which some historians have labelled the second chemical revolution. His contribution led to the creation of the Perkin Medal, an award given annually by the Society of Chemical Industry to a scientist residing in the United States for an "innovation in applied chemistry resulting in outstanding commercial development". It is considered the highest honour given in the industrial chemical industry.
Imperial acquired Silwood Park in 1947, to provide a site for research and teaching in those aspects of biology not well suited for the main London campus. Felix, Imperial's student newspaper, was launched on 9 December 1949. On 29 January 1950, the government announced that it was intended that Imperial should expand to meet the scientific and technological challenges of the 20th century and a major expansion of the College followed over the next decade. In 1959 the Wolfson Foundation donated £350,000 for the establishment of a new Biochemistry Department.[citation needed] A special relationship between Imperial and the Indian Institute of Technology Delhi was established in 1963.[citation needed]
YouTube is a global video-sharing website headquartered in San Bruno, California, United States. The service was created by three former PayPal employees in February 2005. In November 2006, it was bought by Google for US$1.65 billion. YouTube now operates as one of Google's subsidiaries. The site allows users to upload, view, rate, share, and comment on videos, and it makes use of WebM, H.264/MPEG-4 AVC, and Adobe Flash Video technology to display a wide variety of user-generated and corporate media video. Available content includes video clips, TV clips, music videos, movie trailers, and other content such as video blogging, short original videos, and educational videos.
According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after they had experienced difficulty sharing videos that had been shot at a dinner party at Chen's apartment in San Francisco. Karim did not attend the party and denied that it had occurred, but Chen commented that the idea that YouTube was founded after a dinner party "was probably very strengthened by marketing ideas around creating a story that was very digestible".
YouTube offered the public a beta test of the site in May 2005. The first video to reach one million views was a Nike advertisement featuring Ronaldinho in September 2005. Following a $3.5 million investment from Sequoia Capital in November, the site launched officially on December 15, 2005, by which time the site was receiving 8 million views a day. The site grew rapidly, and in July 2006 the company announced that more than 65,000 new videos were being uploaded every day, and that the site was receiving 100 million video views per day. According to data published by market research company comScore, YouTube is the dominant provider of online video in the United States, with a market share of around 43% and more than 14 billion views of videos in May 2010.
In 2014 YouTube said that 300 hours of new videos were uploaded to the site every minute, three times more than one year earlier and that around three quarters of the material comes from outside the U.S. The site has 800 million unique users a month. It is estimated that in 2007 YouTube consumed as much bandwidth as the entire Internet in 2000. According to third-party web analytics providers, Alexa and SimilarWeb, YouTube is the third most visited website in the world, as of June 2015; SimilarWeb also lists YouTube as the top TV and video website globally, attracting more than 15 billion visitors per month.
On March 31, 2010, the YouTube website launched a new design, with the aim of simplifying the interface and increasing the time users spend on the site. Google product manager Shiva Rajaraman commented: "We really felt like we needed to step back and remove the clutter." In May 2010, it was reported that YouTube was serving more than two billion videos a day, which it described as "nearly double the prime-time audience of all three major US television networks combined". In May 2011, YouTube reported in its company blog that the site was receiving more than three billion views per day. In January 2012, YouTube stated that the figure had increased to four billion videos streamed per day.
In February 2015, YouTube announced the launch of a new app specifically for use by children visiting the site, called YouTube Kids. It allows parental controls and restrictions on who can upload content, and is available for both Android and iOS devices. Later on August 26, 2015, YouTube Gaming was launched, a platform for video gaming enthusiasts intended to compete with Twitch.tv. 2015 also saw the announcement of a premium YouTube service titled YouTube Red, which provides users with both ad-free content as well as the ability to download videos among other features.
In January 2010, YouTube launched an experimental version of the site that used the built-in multimedia capabilities of web browsers supporting the HTML5 standard. This allowed videos to be viewed without requiring Adobe Flash Player or any other plug-in to be installed. The YouTube site had a page that allowed supported browsers to opt into the HTML5 trial. Only browsers that supported HTML5 Video using the H.264 or WebM formats could play the videos, and not all videos on the site were available.
All YouTube users can upload videos up to 15 minutes each in duration. Users who have a good track record of complying with the site's Community Guidelines may be offered the ability to upload videos up to 12 hours in length, which requires verifying the account, normally through a mobile phone. When YouTube was launched in 2005, it was possible to upload long videos, but a ten-minute limit was introduced in March 2006 after YouTube found that the majority of videos exceeding this length were unauthorized uploads of television shows and films. The 10-minute limit was increased to 15 minutes in July 2010. If an up-to-date browser version is used, videos greater than 20 GB can be uploaded.
In November 2008, 720p HD support was added. At the time of the 720p launch, the YouTube player was changed from a 4:3 aspect ratio to a widescreen 16:9. With this new feature, YouTube began a switchover to H.264/MPEG-4 AVC as its default video compression format. In November 2009, 1080p HD support was added. In July 2010, YouTube announced that it had launched a range of videos in 4K format, which allows a resolution of up to 4096×3072 pixels. In June 2015, support for 8K resolution was added, with the videos playing at 7680×4320 pixels.
In a video posted on July 21, 2009, YouTube software engineer Peter Bradshaw announced that YouTube users can now upload 3D videos. The videos can be viewed in several different ways, including the common anaglyph (cyan/red lens) method which utilizes glasses worn by the viewer to achieve the 3D effect. The YouTube Flash player can display stereoscopic content interleaved in rows, columns or a checkerboard pattern, side-by-side or anaglyph using a red/cyan, green/magenta or blue/yellow combination. In May 2011, an HTML5 version of the YouTube player began supporting side-by-side 3D footage that is compatible with Nvidia 3D Vision.
YouTube offers users the ability to view its videos on web pages outside their website. Each YouTube video is accompanied by a piece of HTML that can be used to embed it on any page on the Web. This functionality is often used to embed YouTube videos in social networking pages and blogs. Users wishing to post a video discussing, inspired by or related to another user's video are able to make a "video response". On August 27, 2013, YouTube announced that it would remove video responses for being an underused feature. Embedding, rating, commenting and response posting can be disabled by the video owner.
YouTube does not usually offer a download link for its videos, and intends for them to be viewed through its website interface. A small number of videos, such as the weekly addresses by President Barack Obama, can be downloaded as MP4 files. Numerous third-party web sites, applications and browser plug-ins allow users to download YouTube videos. In February 2009, YouTube announced a test service, allowing some partners to offer video downloads for free or for a fee paid through Google Checkout. In June 2012, Google sent cease and desist letters threatening legal action against several websites offering online download and conversion of YouTube videos. In response, Zamzar removed the ability to download YouTube videos from its site. The default settings when uploading a video to YouTube will retain a copyright on the video for the uploader, but since July 2012 it has been possible to select a Creative Commons license as the default, allowing other users to reuse and remix the material if it is free of copyright.
Since June 2007, YouTube's videos have been available for viewing on a range of Apple products. This required YouTube's content to be transcoded into Apple's preferred video standard, H.264, a process that took several months. YouTube videos can be viewed on devices including Apple TV, iPod Touch and the iPhone. In July 2010, the mobile version of the site was relaunched based on HTML5, avoiding the need to use Adobe Flash Player and optimized for use with touch screen controls. The mobile version is also available as an app for the Android platform. In September 2012, YouTube launched its first app for the iPhone, following the decision to drop YouTube as one of the preloaded apps in the iPhone 5 and iOS 6 operating system. According to GlobalWebIndex, YouTube was used by 35% of smartphone users between April and June 2013, making it the third most used app.
A TiVo service update in July 2008 allowed the system to search and play YouTube videos. In January 2009, YouTube launched "YouTube for TV", a version of the website tailored for set-top boxes and other TV-based media devices with web browsers, initially allowing its videos to be viewed on the PlayStation 3 and Wii video game consoles. In June 2009, YouTube XL was introduced, which has a simplified interface designed for viewing on a standard television screen. YouTube is also available as an app on Xbox Live. On November 15, 2012, Google launched an official app for the Wii, allowing users to watch YouTube videos from the Wii channel. An app is also available for Wii U and Nintendo 3DS, and videos can be viewed on the Wii U Internet Browser using HTML5. Google made YouTube available on the Roku player on December 17, 2013 and in October 2014, the Sony PlayStation 4.
YouTube Red is YouTube's premium subscription service. It offers advertising-free streaming, access to exclusive content, background and offline video playback on mobile devices, and access to the Google Play Music "All Access" service. YouTube Red was originally announced on November 12, 2014, as "Music Key", a subscription music streaming service, and was intended to integrate with and replace the existing Google Play Music "All Access" service. On October 28, 2015, the service was re-launched as YouTube Red, offering ad-free streaming of all videos, as well as access to exclusive original content.
Both private individuals and large production companies have used YouTube to grow audiences. Independent content creators have built grassroots followings numbering in the thousands at very little cost or effort, while mass retail and radio promotion proved problematic. Concurrently, old media celebrities moved into the website at the invitation of a YouTube management that witnessed early content creators accruing substantial followings, and perceived audience sizes potentially larger than that attainable by television. While YouTube's revenue-sharing "Partner Program" made it possible to earn a substantial living as a video producer—its top five hundred partners each earning more than $100,000 annually and its ten highest-earning channels grossing from $2.5 million to $12 million—in 2012 CMU business editor characterized YouTube as "a free-to-use... promotional platform for the music labels". In 2013 Forbes' Katheryn Thayer asserted that digital-era artists' work must not only be of high quality, but must elicit reactions on the YouTube platform and social media. In 2013, videos of the 2.5% of artists categorized as "mega", "mainstream" and "mid-sized" received 90.3% of the relevant views on YouTube and Vevo. By early 2013 Billboard had announced that it was factoring YouTube streaming data into calculation of the Billboard Hot 100 and related genre charts.
Observing that face-to-face communication of the type that online videos convey has been "fine-tuned by millions of years of evolution", TED curator Chris Anderson referred to several YouTube contributors and asserted that "what Gutenberg did for writing, online video can now do for face-to-face communication". Anderson asserted that it's not far-fetched to say that online video will dramatically accelerate scientific advance, and that video contributors may be about to launch "the biggest learning cycle in human history." In education, for example, the Khan Academy grew from YouTube video tutoring sessions for founder Salman Khan's cousin into what Forbes'  Michael Noer called "the largest school in the world", with technology poised to disrupt how people learn.
YouTube has enabled people to more directly engage with government, such as in the CNN/YouTube presidential debates (2007) in which ordinary people submitted questions to U.S. presidential candidates via YouTube video, with a techPresident co-founder saying that Internet video was changing the political landscape. Describing the Arab Spring (2010- ), sociologist Philip N. Howard quoted an activist's succinct description that organizing the political unrest involved using "Facebook to schedule the protests, Twitter to coordinate, and YouTube to tell the world." In 2012, more than a third of the U.S. Senate introduced a resolution condemning Joseph Kony 16 days after the "Kony 2012" video was posted to YouTube, with resolution co-sponsor Senator Lindsey Graham remarking that the video "will do more to lead to (Kony's) demise than all other action combined."
Conversely, YouTube has also allowed government to more easily engage with citizens, the White House's official YouTube channel being the seventh top news organization producer on YouTube in 2012 and in 2013 a healthcare exchange commissioned Obama impersonator Iman Crosson's YouTube music video spoof to encourage young Americans to enroll in the Affordable Care Act (Obamacare)-compliant health insurance. In February 2014, U.S. President Obama held a meeting at the White House with leading YouTube content creators to not only promote awareness of Obamacare but more generally to develop ways for government to better connect with the "YouTube Generation". Whereas YouTube's inherent ability to allow presidents to directly connect with average citizens was noted, the YouTube content creators' new media savvy was perceived necessary to better cope with the website's distracting content and fickle audience.
The anti-bullying It Gets Better Project expanded from a single YouTube video directed to discouraged or suicidal LGBT teens, that within two months drew video responses from hundreds including U.S. President Barack Obama, Vice President Biden, White House staff, and several cabinet secretaries. Similarly, in response to fifteen-year-old Amanda Todd's video "My story: Struggling, bullying, suicide, self-harm", legislative action was undertaken almost immediately after her suicide to study the prevalence of bullying and form a national anti-bullying strategy.
Google does not provide detailed figures for YouTube's running costs, and YouTube's revenues in 2007 were noted as "not material" in a regulatory filing. In June 2008, a Forbes magazine article projected the 2008 revenue at $200 million, noting progress in advertising sales. In January 2012, it was estimated that visitors to YouTube spent an average of 15 minutes a day on the site, in contrast to the four or five hours a day spent by a typical U.S. citizen watching television. In 2012, YouTube's revenue from its ads program was estimated at 3.7 billion. In 2013 it nearly doubled and estimated to hit 5.6 billion dollars according to eMarketer, others estimated 4.7 billion,
YouTube entered into a marketing and advertising partnership with NBC in June 2006. In November 2008, YouTube reached an agreement with MGM, Lions Gate Entertainment, and CBS, allowing the companies to post full-length films and television episodes on the site, accompanied by advertisements in a section for US viewers called "Shows". The move was intended to create competition with websites such as Hulu, which features material from NBC, Fox, and Disney. In November 2009, YouTube launched a version of "Shows" available to UK viewers, offering around 4,000 full-length shows from more than 60 partners. In January 2010, YouTube introduced an online film rentals service, which is available only to users in the US, Canada and the UK as of 2010. The service offers over 6,000 films.
In May 2007, YouTube launched its Partner Program, a system based on AdSense which allows the uploader of the video to share the revenue produced by advertising on the site. YouTube typically takes 45 percent of the advertising revenue from videos in the Partner Program, with 55 percent going to the uploader. There are over a million members of the YouTube Partner Program. According to TubeMogul, in 2013 a pre-roll advertisement on YouTube (one that is shown before the video starts) cost advertisers on average $7.60 per 1000 views. Usually no more than half of eligible videos have a pre-roll advertisement, due to a lack of interested advertisers. Assuming pre-roll advertisements on half of videos, a YouTube partner would earn 0.5 X $7.60 X 55% = $2.09 per 1000 views in 2013.
Much of YouTube's revenue goes to the copyright holders of the videos. In 2010 it was reported that nearly a third of the videos with advertisements were uploaded without permission of the copyright holders. YouTube gives an option for copyright holders to locate and remove their videos or to have them continue running for revenue. In May 2013, Nintendo began enforcing its copyright ownership and claiming the advertising revenue from video creators who posted screenshots of its games. In February 2015, Nintendo agreed to share the revenue with the video creators.
At the time of uploading a video, YouTube users are shown a message asking them not to violate copyright laws. Despite this advice, there are still many unauthorized clips of copyrighted material on YouTube. YouTube does not view videos before they are posted online, and it is left to copyright holders to issue a DMCA takedown notice pursuant to the terms of the Online Copyright Infringement Liability Limitation Act. Three successful complaints for copyright infringement against a user account will result in the account and all of its uploaded videos being deleted.
Organizations including Viacom, Mediaset, and the English Premier League have filed lawsuits against YouTube, claiming that it has done too little to prevent the uploading of copyrighted material. Viacom, demanding $1 billion in damages, said that it had found more than 150,000 unauthorized clips of its material on YouTube that had been viewed "an astounding 1.5 billion times". YouTube responded by stating that it "goes far beyond its legal obligations in assisting content owners to protect their works".
During the same court battle, Viacom won a court ruling requiring YouTube to hand over 12 terabytes of data detailing the viewing habits of every user who has watched videos on the site. The decision was criticized by the Electronic Frontier Foundation, which called the court ruling "a setback to privacy rights". In June 2010, Viacom's lawsuit against Google was rejected in a summary judgment, with U.S. federal Judge Louis L. Stanton stating that Google was protected by provisions of the Digital Millennium Copyright Act. Viacom announced its intention to appeal the ruling.
In June 2007, YouTube began trials of a system for automatic detection of uploaded videos that infringe copyright. Google CEO Eric Schmidt regarded this system as necessary for resolving lawsuits such as the one from Viacom, which alleged that YouTube profited from content that it did not have the right to distribute. The system, which became known as Content ID, creates an ID File for copyrighted audio and video material, and stores it in a database. When a video is uploaded, it is checked against the database, and flags the video as a copyright violation if a match is found.
An independent test in 2009 uploaded multiple versions of the same song to YouTube, and concluded that while the system was "surprisingly resilient" in finding copyright violations in the audio tracks of videos, it was not infallible. The use of Content ID to remove material automatically has led to controversy in some cases, as the videos have not been checked by a human for fair use. If a YouTube user disagrees with a decision by Content ID, it is possible to fill in a form disputing the decision. YouTube has cited the effectiveness of Content ID as one of the reasons why the site's rules were modified in December 2010 to allow some users to upload videos of unlimited length.
YouTube relies on its users to flag the content of videos as inappropriate, and a YouTube employee will view a flagged video to determine whether it violates the site's terms of service. In July 2008, the Culture and Media Committee of the House of Commons of the United Kingdom stated that it was "unimpressed" with YouTube's system for policing its videos, and argued that "proactive review of content should be standard practice for sites hosting user-generated content". YouTube responded by stating:
Most videos enable users to leave comments, and these have attracted attention for the negative aspects of both their form and content. In 2006, Time praised Web 2.0 for enabling "community and collaboration on a scale never seen before", and added that YouTube "harnesses the stupidity of crowds as well as its wisdom. Some of the comments on YouTube make you weep for the future of humanity just for the spelling alone, never mind the obscenity and the naked hatred". The Guardian in 2009 described users' comments on YouTube as:
On November 6, 2013, Google implemented a new comment system that requires all YouTube users to use a Google+ account in order to comment on videos and making the comment system Google+ oriented. The changes are in large part an attempt to address the frequent criticisms of the quality and tone of YouTube comments. They give creators more power to moderate and block comments, and add new sorting mechanisms to ensure that better, more relevant discussions appear at the top. The new system restored the ability to include URLs in comments, which had previously been removed due to problems with abuse. In response, YouTube co-founder Jawed Karim posted the question "why the fuck do I need a google+ account to comment on a video?" on his YouTube channel to express his negative opinion of the change. The official YouTube announcement received 20,097 "thumbs down" votes and generated more than 32,000 comments in two days. Writing in the Newsday blog Silicon Island, Chase Melvin noted that "Google+ is nowhere near as popular a social media network as Facebook, but it's essentially being forced upon millions of YouTube users who don't want to lose their ability to comment on videos" and "Discussion forums across the Internet are already bursting with outcry against the new comment system". In the same article Melvin goes on to say:
In some countries, YouTube is completely blocked, either through a long term standing ban or for more limited periods of time such as during periods of unrest, the run-up to an election, or in response to upcoming political anniversaries. In other countries access to the website as a whole remains open, but access to specific videos is blocked. In cases where the entire site is banned due to one particular video, YouTube will often agree to remove or limit access to that video in order to restore service.
In May 2014, prior to the launch of YouTube's subscription-based Music Key service, the independent music trade organization Worldwide Independent Network alleged that YouTube was using non-negotiable contracts with independent labels that were "undervalued" in comparison to other streaming services, and that YouTube would block all music content from labels who do not reach a deal to be included on the paid service. In a statement to the Financial Times in June 2014, Robert Kyncl confirmed that YouTube would block the content of labels who do not negotiate deals to be included in the paid service "to ensure that all content on the platform is governed by its new contractual terms." Stating that 90% of labels had reached deals, he went on to say that "while we wish that we had [a] 100% success rate, we understand that is not likely an achievable goal and therefore it is our responsibility to our users and the industry to launch the enhanced music experience." The Financial Times later reported that YouTube had reached an aggregate deal with Merlin Network—a trade group representing over 20,000 independent labels, for their inclusion in the service. However, YouTube itself has not confirmed the deal.
The Warsaw Pact (formally, the Treaty of Friendship, Co-operation, and Mutual Assistance, sometimes, informally WarPac, akin in format to NATO) was a collective defense treaty among Soviet Union and seven Soviet satellite states in Central and Eastern Europe in existence during the Cold War. The Warsaw Pact was the military complement to the Council for Mutual Economic Assistance (CoMEcon), the regional economic organization for the communist states of Central and Eastern Europe. The Warsaw Pact was created in reaction to the integration of West Germany into NATO in 1955 per the Paris Pacts of 1954, but it is also considered to have been motivated by Soviet desires to maintain control over military forces in Central and Eastern Europe.
While the Warsaw Pact was established as a balance of power or counterweight to NATO, there was no direct confrontation between them. Instead, the conflict was fought on an ideological basis. Both NATO and the Warsaw Pact led to the expansion of military forces and their integration into the respective blocs. The Warsaw Pact's largest military engagement was Warsaw Pact invasion of Czechoslovakia (with the participation of all Pact nations except Romania and Albania). The Pact failed to function when the Revolutions of 1989 spread through Eastern Europe, beginning with the Solidarity movement in Poland and its success in June 1989.
On 25 February 1991, the Pact was declared at an end at a meeting of defense and foreign ministers from the remaining member states meeting in Hungary. On 1 July 1991, the Czechoslovak President Václav Havel formally declared an end to the Warsaw Treaty Organization of Friendship, Co-operation, and Mutual Assistance which had been established in 1955. The USSR itself was dissolved in December 1991.
The Warsaw Treaty's organization was two-fold: the Political Consultative Committee handled political matters, and the Combined Command of Pact Armed Forces controlled the assigned multi-national forces, with headquarters in Warsaw, Poland. Furthermore, the Supreme Commander of the Unified Armed Forces of the Warsaw Treaty Organization which commands and controls all the military forces of the member countries was also a First Deputy Minister of Defense of the USSR, and the Chief of Combined Staff of the Unified Armed Forces of the Warsaw Treaty Organization was also a First Deputy Chief of the General Staff of the Armed Forces of the USSR. Therefore, although ostensibly an international collective security alliance, the USSR dominated the Warsaw Treaty armed forces.
The strategy behind the formation of the Warsaw Pact was driven by the desire of the Soviet Union to dominate Central and Eastern Europe. This policy was driven by ideological and geostrategic reasons. Ideologically, the Soviet Union arrogated the right to define socialism and communism and act as the leader of the global socialist movement. A corollary to this idea was the necessity of intervention if a country appeared to be violating core socialist ideas and Communist Party functions, which was explicitly stated in the Brezhnev Doctrine. Geostrategic principles also drove the Soviet Union to prevent invasion of its territory by Western European powers.
Before creation of Warsaw Pact, fearing Germany rearmed, Czechoslovak leadership sought to create security pact with East Germany and Poland. These states protested strongly against re-militarization of West Germany. The Warsaw Pact was primarily put in place as a consequence of the rearming of West Germany inside NATO. Soviet leaders, as many European countries in both western and eastern side, feared Germany being once again a military power as a direct threat and German militarism remained a fresh memory among Soviets and Eastern Europeans. As Soviet Union had already bilateral treaties with all of its eastern satellites, the Pact has been long considered 'superfluous', and because of the rushed way in which it was conceived, NATO officials labeled it as a 'cardboard castle'. Previously, in March 1954, the USSR, fearing the restoration of German Militarism in West Germany, requested admission to NATO.
The Soviet request to join NATO arose in the aftermath of the Berlin Conference of January–February 1954. Soviet foreign minister Molotov made proposals to have Germany reunified and elections for a pan-German government, under conditions of withdrawal of the four powers armies and German neutrality, but all were refused by the other foreign ministers, Dulles (USA), Eden (UK) and Bidault (France). Proposals for the reunification of Germany were nothing new: earlier on 20 March 1952, talks about a German reunification, initiated by the socalled 'Stalin Note', ended after the United Kingdom, France, and the United States insisted that a unified Germany should not be neutral and should be free to join the European Defence Community and rearm. James Dunn (USA), who met in Paris with Eden, Adenauer and Robert Schuman (France), affirmed that "the object should be to avoid discussion with the Russians and to press on the European Defense Community". According to John Gaddis "there was little inclination in Western capitals to explore this offer" from USSR. While historian Rolf Steininger asserts that Adenauer's conviction that “neutralization means sovietization” was the main factor in the rejection of the soviet proposals, Adenauer also feared that unification might have resulted in the end of the CDU's dominance in the Bundestag.
One month later, the proposed European Treaty was rejected not only by supporters of the EDC but also by western opponents of the European Defense Community (like French Gaullist leader Palewski) who perceived it as "unacceptable in its present form because it excludes the USA from participation in the collective security system in Europe". The Soviets then decided to make a new proposal to the governments of the USA, UK and France stating to accept the participation of the USA in the proposed General European Agreement. And considering that another argument deployed against the Soviet proposal was that it was perceived by western powers as "directed against the North Atlantic Pact and its liquidation", the Soviets decided to declare their "readiness to examine jointly with other interested parties the question of the participation of the USSR in the North Atlantic bloc", specifying that "the admittance of the USA into the General European Agreement should not be conditional on the three western powers agreeing to the USSR joining the North Atlantic Pact".
Again all proposals, including the request to join NATO, were rejected by UK, US, and French governments shortly after. Emblematic was the position of British General Hastings Ismay, supporter of NATO expansion, who said that NATO "must grow until the whole free world gets under one umbrella." He opposed the request to join NATO made by the USSR in 1954 saying that "the Soviet request to join NATO is like an unrepentant burglar requesting to join the police force".
In April 1954 Adenauer made his first visit to the USA meeting Nixon, Eisenhower and Dulles. Ratification of EDC was delaying but the US representatives made it clear to Adenauer that EDC would have to become a part of NATO.
Memories of the Nazi occupation were still strong, and the rearmament of Germany was feared by France too. On 30 August 1954 French Parliament rejected the EDC, thus ensuring its failure and blocking a major objective of US policy towards Europe: to associate Germany militarily with the West. The US Department of State started to elaborate alternatives: Germany would be invited to join NATO or, in the case of French obstructionism, strategies to circumvent a French veto would be implemented in order to obtain a German rearmament outside NATO.
On 23 October 1954 – only nine years after Allies (UK, USA and USSR) defeated Nazi Germany ending World War II in Europe – the admission of the Federal Republic of Germany to the North Atlantic Pact was finally decided. The incorporation of West Germany into the organization on 9 May 1955 was described as "a decisive turning point in the history of our continent" by Halvard Lange, Foreign Affairs Minister of Norway at the time. In November 1954, the USSR requested a new European Security Treaty, in order to make a final attempt to not have a remilitarized West Germany potentially opposed to the Soviet Union, with no success.
On 14 May 1955, the USSR and other seven European countries "reaffirming their desire for the establishment of a system of European collective security based on the participation of all European states irrespective of their social and political systems" established the Warsaw Pact in response to the integration of the Federal Republic of Germany into NATO, declaring that: "a remilitarized Western Germany and the integration of the latter in the North-Atlantic bloc [...] increase the danger of another war and constitutes a threat to the national security of the peaceable states; [...] in these circumstances the peaceable European states must take the necessary measures to safeguard their security".
One of the founding members, East Germany was allowed to re-arm by the Soviet Union and the National People's Army was established as the armed forces of the country to counter the rearmament of West Germany.
The eight member countries of the Warsaw Pact pledged the mutual defense of any member who would be attacked. Relations among the treaty signatories were based upon mutual non-intervention in the internal affairs of the member countries, respect for national sovereignty, and political independence. However, almost all governments of those member states were indirectly controlled by the Soviet Union.
In July 1963 the Mongolian People's Republic asked to join the Warsaw Pact under Article 9 of the treaty. For this purpose a special protocol should have been taken since the text of the treaty applied only to Europe. Due to the emerging Sino-Soviet split, Mongolia remained on observer status. Soviet stationing troops were agreed to stay in Mongolia from 1966.
For 36 years, NATO and the Warsaw Pact never directly waged war against each other in Europe; the United States and the Soviet Union and their respective allies implemented strategic policies aimed at the containment of each other in Europe, while working and fighting for influence within the wider Cold War on the international stage.
In 1956, following the declaration of the Imre Nagy government of withdrawal of Hungary from the Warsaw Pact, Soviet troops entered the country and removed the government. Soviet forces crushed the nationwide revolt, leading to the death of an estimated 2,500 Hungarian citizens.
The multi-national Communist armed forces' sole joint action was the Warsaw Pact invasion of Czechoslovakia in August 1968. All member countries, with the exception of the Socialist Republic of Romania and the People's Republic of Albania participated in the invasion.
On 25 February 1991, the Warsaw Pact was declared disbanded at a meeting of defense and foreign ministers from remaining Pact countries meeting in Hungary. On 1 July 1991, in Prague, the Czechoslovak President Václav Havel formally ended the 1955 Warsaw Treaty Organization of Friendship, Cooperation, and Mutual Assistance and so disestablished the Warsaw Treaty after 36 years of military alliance with the USSR. In fact, the treaty was de facto disbanded in December 1989 during the violent revolution in Romania, which toppled the communist government, without military intervention form other member states. The USSR disestablished itself in December 1991.
On 12 March 1999, the Czech Republic, Hungary, and Poland joined NATO; Bulgaria, Estonia, Latvia, Lithuania, Romania, and Slovakia joined in March 2004; Albania joined on 1 April 2009.
In November 2005, the Polish government opened its Warsaw Treaty archives to the Institute of National Remembrance, who published some 1,300 declassified documents in January 2006. Yet the Polish government reserved publication of 100 documents, pending their military declassification. Eventually, 30 of the reserved 100 documents were published; 70 remained secret, and unpublished. Among the documents published is the Warsaw Treaty's nuclear war plan, Seven Days to the River Rhine – a short, swift counter-attack capturing Austria, Denmark, Germany and Netherlands east of River Rhine, using nuclear weapons, in self-defense, after a NATO first strike. The plan originated as a 1979 field training exercise war game, and metamorphosed into official Warsaw Treaty battle doctrine, until the late 1980s – which is why the People's Republic of Poland was a nuclear weapons base, first, to 178, then, to 250 tactical-range rockets. Doctrinally, as a Soviet-style (offensive) battle plan, Seven Days to the River Rhine gave commanders few defensive-war strategies for fighting NATO in Warsaw Treaty territory.[citation needed]
The rivalries between the Arab tribes had caused unrest in the provinces outside Syria, most notably in the Second Muslim Civil War of 680–692 CE and the Berber Revolt of 740–743 CE. During the Second Civil War, leadership of the Umayyad clan shifted from the Sufyanid branch of the family to the Marwanid branch. As the constant campaigning exhausted the resources and manpower of the state, the Umayyads, weakened by the Third Muslim Civil War of 744–747 CE, were finally toppled by the Abbasid Revolution in 750 CE/132 AH. A branch of the family fled across North Africa to Al-Andalus, where they established the Caliphate of Córdoba, which lasted until 1031 before falling due to the Fitna of al-Ándalus.
Ali was assassinated in 661 by a Kharijite partisan. Six months later in the same year, in the interest of peace, Hasan ibn Ali, highly regarded for his wisdom and as a peacemaker, and the Second Imam for the Shias, and the grandson of Muhammad, made a peace treaty with Muawiyah I. In the Hasan-Muawiya treaty, Hasan ibn Ali handed over power to Muawiya on the condition that he be just to the people and keep them safe and secure, and after his death he not establish a dynasty. This brought to an end the era of the Rightly Guided Caliphs for the Sunnis, and Hasan ibn Ali was also the last Imam for the Shias to be a Caliph. Following this, Mu'awiyah broke the conditions of the agreement and began the Umayyad dynasty, with its capital in Damascus.
At the time, the Umayyad taxation and administrative practice were perceived as unjust by some Muslims. The Christian and Jewish population had still autonomy; their judicial matters were dealt with in accordance with their own laws and by their own religious heads or their appointees, although they did pay a poll tax for policing to the central state. Muhammad had stated explicitly during his lifetime that abrahamic religious groups (still a majority in times of the Umayyad Caliphate), should be allowed to practice their own religion, provided that they paid the jizya taxation. The welfare state of both the Muslim and the non-Muslim poor started by Umar ibn al Khattab had also continued. Muawiya's wife Maysum (Yazid's mother) was also a Christian. The relations between the Muslims and the Christians in the state were stable in this time. The Umayyads were involved in frequent battles with the Christian Byzantines without being concerned with protecting themselves in Syria, which had remained largely Christian like many other parts of the empire. Prominent positions were held by Christians, some of whom belonged to families that had served in Byzantine governments. The employment of Christians was part of a broader policy of religious assimilation that was necessitated by the presence of large Christian populations in the conquered provinces, as in Syria. This policy also boosted Muawiya's popularity and solidified Syria as his power base.
The Umayyad Caliphate (Arabic: الخلافة الأموية‎, trans. Al-Khilāfat al-ʾumawiyya) was the second of the four major Islamic caliphates established after the death of Muhammad. This caliphate was centered on the Umayyad dynasty (Arabic: الأمويون‎, al-ʾUmawiyyūn, or بنو أمية, Banū ʾUmayya, "Sons of Umayya"), hailing from Mecca. The Umayyad family had first come to power under the third caliph, Uthman ibn Affan (r. 644–656), but the Umayyad regime was founded by Muawiya ibn Abi Sufyan, long-time governor of Syria, after the end of the First Muslim Civil War in 661 CE/41 AH. Syria remained the Umayyads' main power base thereafter, and Damascus was their capital. The Umayyads continued the Muslim conquests, incorporating the Caucasus, Transoxiana, Sindh, the Maghreb and the Iberian Peninsula (Al-Andalus) into the Muslim world. At its greatest extent, the Umayyad Caliphate covered 15 million km2 (5.79 million square miles), making it the largest empire (in terms of area - not in terms of population) the world had yet seen, and the fifth largest ever to exist.
Most historians[who?] consider Caliph Muawiyah (661–80) to have been the second ruler of the Umayyad dynasty, even though he was the first to assert the Umayyads' right to rule on a dynastic principle. It was really the caliphate of Uthman Ibn Affan (644–656), a member of Umayyad clan himself, that witnessed the revival and then the ascendancy of the Umayyad clan to the corridors of power. Uthman placed some of the trusted members of his clan at prominent and strong positions throughout the state. Most notable was the appointment of Marwan ibn al-Hakam, Uthman's first cousin, as his top advisor, which created a stir among the Hashimite companions of Muhammad, as Marwan along with his father Al-Hakam ibn Abi al-'As had been permanently exiled from Medina by Muhammad during his lifetime. Uthman also appointed as governor of Kufa his half-brother, Walid ibn Uqba, who was accused by Hashmites of leading prayer while under the influence of alcohol. Uthman also consolidated Muawiyah's governorship of Syria by granting him control over a larger area and appointed his foster brother Abdullah ibn Saad as the Governor of Egypt. However, since Uthman never named an heir, he cannot be considered the founder of a dynasty.
Following the death of Husayn, Ibn al-Zubayr, although remaining in Mecca, was associated with two opposition movements, one centered in Medina and the other around Kharijites in Basra and Arabia. Because Medina had been home to Muhammad and his family, including Husayn, word of his death and the imprisonment of his family led to a large opposition movement. In 683, Yazid dispatched an army to subdue both movements. The army suppressed the Medinese opposition at the Battle of al-Harrah. The Grand Mosque in Medina was severely damaged and widespread pillaging caused deep-seated dissent. Yazid's army continued on and laid siege to Mecca. At some point during the siege, the Kaaba was badly damaged in a fire. The destruction of the Kaaba and Grand Mosque became a major cause for censure of the Umayyads in later histories of the period.
According to tradition, the Umayyad family (also known as the Banu Abd-Shams) and Muhammad both descended from a common ancestor, Abd Manaf ibn Qusai, and they originally came from the city of Mecca. Muhammad descended from Abd Manāf via his son Hashim, while the Umayyads descended from Abd Manaf via a different son, Abd-Shams, whose son was Umayya. The two families are therefore considered to be different clans (those of Hashim and of Umayya, respectively) of the same tribe (that of the Quraish). However Muslim Shia historians suspect that Umayya was an adopted son of Abd Shams so he was not a blood relative of Abd Manaf ibn Qusai. Umayya was later discarded from the noble family. Sunni historians disagree with this and view Shia claims as nothing more than outright polemics due to their hostility to the Umayyad family in general. They point to the fact that the grand sons of Uthman, Zaid bin amr bin uthman bin affan and Abdullah bin Amr bin Uthman got married to the Sukaina and Fatima the daughters of Hussein son of Ali to show closeness of Banu hashem and Bani Ummayah.
Following this battle, Ali fought a battle against Muawiyah, known as the Battle of Siffin. The battle was stopped before either side had achieved victory, and the two parties agreed to arbitrate their dispute. After the battle Amr ibn al-As was appointed by Muawiyah as an arbitrator, and Ali appointed Abu Musa Ashaari. Seven months later, in February 658, the two arbitrators met at Adhruh, about 10 miles north west of Maan in Jordon. Amr ibn al-As convinced Abu Musa Ashaari that both Ali and Muawiyah should step down and a new Caliph be elected. Ali and his supporters were stunned by the decision which had lowered the Caliph to the status of the rebellious Muawiyah I. Ali was therefore outwitted by Muawiyah and Amr. Ali refused to accept the verdict and found himself technically in breach of his pledge to abide by the arbitration. This put Ali in a weak position even amongst his own supporters. The most vociferous opponents in Ali's camp were the very same people who had forced Ali into the ceasefire. They broke away from Ali's force, rallying under the slogan, "arbitration belongs to God alone." This group came to be known as the Kharijites ("those who leave"). In 659 Ali's forces and the Kharijites met in the Battle of Nahrawan. Although Ali won the battle, the constant conflict had begun to affect his standing, and in the following years some Syrians seem to have acclaimed Muawiyah as a rival caliph.
The Quran and Muhammad talked about racial equality and justice as in The Farewell Sermon. Tribal and nationalistic differences were discouraged. But after Muhammad's passing, the old tribal differences between the Arabs started to resurface. Following the Roman–Persian Wars and the Byzantine–Sassanid Wars, deep rooted differences between Iraq, formally under the Persian Sassanid Empire, and Syria, formally under the Byzantine Empire, also existed. Each wanted the capital of the newly established Islamic State to be in their area. Previously, the second caliph Umar was very firm on the governors and his spies kept an eye on them. If he felt that a governor or a commander was becoming attracted to wealth, he had him removed from his position.
While the Umayyads and the Hashimites may have had bitterness between the two clans before Muhammad, the rivalry turned into a severe case of tribal animosity after the Battle of Badr. The battle saw three top leaders of the Umayyad clan (Utba ibn Rabi'ah, Walid ibn Utbah and Shaybah) killed by Hashimites (Ali, Hamza ibn ‘Abd al-Muttalib and Ubaydah ibn al-Harith) in a three-on-three melee. This fueled the opposition of Abu Sufyan ibn Harb, the grandson of Umayya, to Muhammad and to Islam. Abu Sufyan sought to exterminate the adherents of the new religion by waging another battle with Muslims based in Medina only a year after the Battle of Badr. He did this to avenge the defeat at Badr. The Battle of Uhud is generally believed by scholars to be the first defeat for the Muslims, as they had incurred greater losses than the Meccans. After the battle, Abu Sufyan's wife Hind, who was also the daughter of Utba ibn Rabi'ah, is reported to have cut open the corpse of Hamza, taking out his liver which she then attempted to eat. Within five years after his defeat in the Battle of Uhud, however, Muhammad took control of Mecca and announced a general amnesty for all. Abu Sufyan and his wife Hind embraced Islam on the eve of the conquest of Mecca, as did their son (the future caliph Muawiyah I).
Umar is honored for his attempt to resolve the fiscal problems attendant upon conversion to Islam. During the Umayyad period, the majority of people living within the caliphate were not Muslim, but Christian, Jewish, Zoroastrian, or members of other small groups. These religious communities were not forced to convert to Islam, but were subject to a tax (jizyah) which was not imposed upon Muslims. This situation may actually have made widespread conversion to Islam undesirable from the point of view of state revenue, and there are reports that provincial governors actively discouraged such conversions. It is not clear how Umar attempted to resolve this situation, but the sources portray him as having insisted on like treatment of Arab and non-Arab (mawali) Muslims, and on the removal of obstacles to the conversion of non-Arabs to Islam.
After the assassination of Uthman in 656, Ali, a member of the Quraysh tribe and the cousin and son-in-law of Muhammad, was elected as the caliph. He soon met with resistance from several factions, owing to his relative political inexperience. Ali moved his capital from Medina to Kufa. The resulting conflict, which lasted from 656 until 661, is known as the First Fitna ("civil war"). Muawiyah I, the governor of Syria, a relative of Uthman ibn al-Affan and Marwan I, wanted the culprits arrested. Marwan I manipulated everyone and created conflict. Aisha, the wife of Muhammad, and Talhah and Al-Zubayr, two of the companions of Muhammad, went to Basra to tell Ali to arrest the culprits who murdered Uthman. Marwan I and other people who wanted conflict manipulated everyone to fight. The two sides clashed at the Battle of the Camel in 656, where Ali won a decisive victory.
Early Muslim armies stayed in encampments away from cities because Umar feared that they might get attracted to wealth and luxury. In the process, they might turn away from the worship of God and start accumulating wealth and establishing dynasties. When Uthman ibn al-Affan became very old, Marwan I, a relative of Muawiyah I, slipped into the vacuum, became his secretary, slowly assumed more control and relaxed some of these restrictions. Marwan I had previously been excluded from positions of responsibility. In 656, Muhammad ibn Abi Bakr, the son of Abu Bakr, the adopted son of Ali ibn Abi Talib, and the great grandfather of Ja'far al-Sadiq, showed some Egyptians the house of Uthman ibn al-Affan. Later the Egyptians ended up killing Uthman ibn al-Affan.
In 680 Ibn al-Zubayr fled Medina for Mecca. Hearing about Husayn's opposition to Yazid I, the people of Kufa sent to Husayn asking him to take over with their support. Al-Husayn sent his cousin Muslim bin Agail to verify if they would rally behind him. When the news reached Yazid I, he sent Ubayd-Allah bin Ziyad, ruler of Basrah, with the instruction to prevent the people of Kufa rallying behind Al-Husayn. Ubayd-Allah bin Ziyad managed to disperse the crowd that gathered around Muslim bin Agail and captured him. Realizing Ubayd-Allah bin Ziyad had been instructed to prevent Husayn from establishing support in Kufa, Muslim bin Agail requested a message to be sent to Husayn to prevent his immigration to Kufa. The request was denied and Ubayd-Allah bin Ziyad killed Muslim bin Agail. While Ibn al-Zubayr would stay in Mecca until his death, Husayn decided to travel on to Kufa with his family, unaware of the lack of support there. Husayn and his family were intercepted by Yazid I's forces led by Amru bin Saad, Shamar bin Thi Al-Joshan, and Hussain bin Tamim, who fought Al-Husayn and his male family members until they were killed. There were 200 people in Husayn's caravan, many of whom were women, including his sisters, wives, daughters and their children. The women and children from Husayn's camp were taken as prisoners of war and led back to Damascus to be presented to Yazid I. They remained imprisoned until public opinion turned against him as word of Husayn's death and his family's capture spread. They were then granted passage back to Medina. The sole adult male survivor from the caravan was Ali ibn Husayn who was with fever too ill to fight when the caravan was attacked.
In the year 712, Muhammad bin Qasim, an Umayyad general, sailed from the Persian Gulf into Sindh in Pakistan and conquered both the Sindh and the Punjab regions along the Indus river. The conquest of Sindh and Punjab, in modern-day Pakistan, although costly, were major gains for the Umayyad Caliphate. However, further gains were halted by Hindu kingdoms in India in the battle of Rajasthan. The Arabs tried to invade India but they were defeated by the north Indian king Nagabhata of the Pratihara Dynasty and by the south Indian Emperor Vikramaditya II of the Chalukya dynasty in the early 8th century. After this the Arab chroniclers admit that the Caliph Mahdi "gave up the project of conquering any part of India."
The second major event of the early reign of Abd al-Malik was the construction of the Dome of the Rock in Jerusalem. Although the chronology remains somewhat uncertain, the building seems to have been completed in 692, which means that it was under construction during the conflict with Ibn al-Zubayr. This had led some historians, both medieval and modern, to suggest that the Dome of the Rock was built as a destination for pilgrimage to rival the Kaaba, which was under the control of Ibn al-Zubayr.
Muawiyah also encouraged peaceful coexistence with the Christian communities of Syria, granting his reign with "peace and prosperity for Christians and Arabs alike", and one of his closest advisers was Sarjun, the father of John of Damascus. At the same time, he waged unceasing war against the Byzantine Roman Empire. During his reign, Rhodes and Crete were occupied, and several assaults were launched against Constantinople. After their failure, and faced with a large-scale Christian uprising in the form of the Mardaites, Muawiyah concluded a peace with Byzantium. Muawiyah also oversaw military expansion in North Africa (the foundation of Kairouan) and in Central Asia (the conquest of Kabul, Bukhara, and Samarkand).
Yazid died while the siege was still in progress, and the Umayyad army returned to Damascus, leaving Ibn al-Zubayr in control of Mecca. Yazid's son Muawiya II (683–84) initially succeeded him but seems to have never been recognized as caliph outside of Syria. Two factions developed within Syria: the Confederation of Qays, who supported Ibn al-Zubayr, and the Quda'a, who supported Marwan, a descendant of Umayya via Wa'il ibn Umayyah. The partisans of Marwan triumphed at a battle at Marj Rahit, near Damascus, in 684, and Marwan became caliph shortly thereafter.
Marwan was succeeded by his son, Abd al-Malik (685–705), who reconsolidated Umayyad control of the caliphate. The early reign of Abd al-Malik was marked by the revolt of Al-Mukhtar, which was based in Kufa. Al-Mukhtar hoped to elevate Muhammad ibn al-Hanafiyyah, another son of Ali, to the caliphate, although Ibn al-Hanafiyyah himself may have had no connection to the revolt. The troops of al-Mukhtar engaged in battles both with the Umayyads in 686, defeating them at the river Khazir near Mosul, and with Ibn al-Zubayr in 687, at which time the revolt of al-Mukhtar was crushed. In 691, Umayyad troops reconquered Iraq, and in 692 the same army captured Mecca. Ibn al-Zubayr was killed in the attack.
Geographically, the empire was divided into several provinces, the borders of which changed numerous times during the Umayyad reign. Each province had a governor appointed by the khalifah. The governor was in charge of the religious officials, army leaders, police, and civil administrators in his province. Local expenses were paid for by taxes coming from that province, with the remainder each year being sent to the central government in Damascus. As the central power of the Umayyad rulers waned in the later years of the dynasty, some governors neglected to send the extra tax revenue to Damascus and created great personal fortunes.
Hisham suffered still worse defeats in the east, where his armies attempted to subdue both Tokharistan, with its center at Balkh, and Transoxiana, with its center at Samarkand. Both areas had already been partially conquered, but remained difficult to govern. Once again, a particular difficulty concerned the question of the conversion of non-Arabs, especially the Sogdians of Transoxiana. Following the Umayyad defeat in the "Day of Thirst" in 724, Ashras ibn 'Abd Allah al-Sulami, governor of Khurasan, promised tax relief to those Sogdians who converted to Islam, but went back on his offer when it proved too popular and threatened to reduce tax revenues. Discontent among the Khurasani Arabs rose sharply after the losses suffered in the Battle of the Defile in 731, and in 734, al-Harith ibn Surayj led a revolt that received broad backing from Arabs and natives alike, capturing Balkh but failing to take Merv. After this defeat, al-Harith's movement seems to have been dissolved, but the problem of the rights of non-Arab Muslims would continue to plague the Umayyads.
The Hashimiyya movement (a sub-sect of the Kaysanites Shia), led by the Abbasid family, overthrew the Umayyad caliphate. The Abbasids were members of the Hashim clan, rivals of the Umayyads, but the word "Hashimiyya" seems to refer specifically to Abu Hashim, a grandson of Ali and son of Muhammad ibn al-Hanafiyya. According to certain traditions, Abu Hashim died in 717 in Humeima in the house of Muhammad ibn Ali, the head of the Abbasid family, and before dying named Muhammad ibn Ali as his successor. This tradition allowed the Abbasids to rally the supporters of the failed revolt of Mukhtar, who had represented themselves as the supporters of Muhammad ibn al-Hanafiyya.
From the caliphate's north-western African bases, a series of raids on coastal areas of the Visigothic Kingdom paved the way to the permanent occupation of most of Iberia by the Umayyads (starting in 711), and on into south-eastern Gaul (last stronghold at Narbonne in 759). Hisham's reign witnessed the end of expansion in the west, following the defeat of the Arab army by the Franks at the Battle of Tours in 732. In 739 a major Berber Revolt broke out in North Africa, which was subdued only with difficulty, but it was followed by the collapse of Umayyad authority in al-Andalus. In India the Arab armies were defeated by the south Indian Chalukya dynasty and by the north Indian Pratiharas Dynasty in the 8th century and the Arabs were driven out of India. In the Caucasus, the confrontation with the Khazars peaked under Hisham: the Arabs established Derbent as a major military base and launched several invasions of the northern Caucasus, but failed to subdue the nomadic Khazars. The conflict was arduous and bloody, and the Arab army even suffered a major defeat at the Battle of Marj Ardabil in 730. Marwan ibn Muhammad, the future Marwan II, finally ended the war in 737 with a massive invasion that is reported to have reached as far as the Volga, but the Khazars remained unsubdued.
The final son of Abd al-Malik to become caliph was Hisham (724–43), whose long and eventful reign was above all marked by the curtailment of military expansion. Hisham established his court at Resafa in northern Syria, which was closer to the Byzantine border than Damascus, and resumed hostilities against the Byzantines, which had lapsed following the failure of the last siege of Constantinople. The new campaigns resulted in a number of successful raids into Anatolia, but also in a major defeat (the Battle of Akroinon), and did not lead to any significant territorial expansion.
With limited resources Muawiyah went about creating allies. Muawiyah married Maysum the daughter of the chief of the Kalb tribe, that was a large Jacobite Christian Arab tribe in Syria. His marriage to Maysum was politically motivated. The Kalb tribe had remained largely neutral when the Muslims first went into Syria. After the plague that killed much of the Muslim Army in Syria, by marrying Maysum, Muawiyah started to use the Jacobite Christians, against the Romans. Muawiya's wife Maysum (Yazid's mother) was also a Jacobite Christian. With limited resources and the Byzantine just over the border, Muawiyah worked in cooperation with the local Christian population. To stop Byzantine harassment from the sea during the Arab-Byzantine Wars, in 649 Muawiyah set up a navy; manned by Monophysitise Christians, Copts and Jacobite Syrian Christians sailors and Muslim troops.
Only Umayyad ruler (Caliphs of Damascus), Umar ibn Abd al-Aziz, is unanimously praised by Sunni sources for his devout piety and justice. In his efforts to spread Islam he established liberties for the Mawali by abolishing the jizya tax for converts to Islam. Imam Abu Muhammad Adbullah ibn Abdul Hakam stated that Umar ibn Abd al-Aziz also stopped the personal allowance offered to his relatives stating that he could only give them an allowance if he gave an allowance to everyone else in the empire. Umar ibn Abd al-Aziz was later poisoned in the year 720. When successive governments tried to reverse Umar ibn Abd al-Aziz's tax policies it created rebellion.
Around 746, Abu Muslim assumed leadership of the Hashimiyya in Khurasan. In 747, he successfully initiated an open revolt against Umayyad rule, which was carried out under the sign of the black flag. He soon established control of Khurasan, expelling its Umayyad governor, Nasr ibn Sayyar, and dispatched an army westwards. Kufa fell to the Hashimiyya in 749, the last Umayyad stronghold in Iraq, Wasit, was placed under siege, and in November of the same year Abu al-Abbas was recognized as the new caliph in the mosque at Kufa.[citation needed] At this point Marwan mobilized his troops from Harran and advanced toward Iraq. In January 750 the two forces met in the Battle of the Zab, and the Umayyads were defeated. Damascus fell to the Abbasids in April, and in August, Marwan was killed in Egypt.
The books written later in the Abbasid period in Iran are more anti Umayyad. Iran was Sunni at the time. There was much anti Arab feeling in Iran after the fall of the Persian empire. This anti Arab feeling also influenced the books on Islamic history. Al-Tabri was also written in Iran during that period. Al-Tabri was a huge collection including all the text that he could find, from all the sources. It was a collection preserving everything for future generations to codify and for future generations to judge if it was true or false.
The Diwan of Umar, assigning annuities to all Arabs and to the Muslim soldiers of other races, underwent a change in the hands of the Umayyads. The Umayyads meddled with the register and the recipients regarded pensions as the subsistence allowance even without being in active service. Hisham reformed it and paid only to those who participated in battle. On the pattern of the Byzantine system the Umayyads reformed their army organization in general and divided it into five corps: the centre, two wings, vanguards and rearguards, following the same formation while on march or on a battle field. Marwan II (740–50) abandoned the old division and introduced Kurdus (cohort), a small compact body. The Umayyad troops were divided into three divisions: infantry, cavalry and artillery. Arab troops were dressed and armed in Greek fashion. The Umayyad cavalry used plain and round saddles. The artillery used arradah (ballista), manjaniq (the mangonel) and dabbabah or kabsh (the battering ram). The heavy engines, siege machines and baggage were carried on camels behind the army.
Mu'awiyah introduced postal service, Abd al-Malik extended it throughout his empire, and Walid made full use of it. The Umayyad Caliph Abd al-Malik developed a regular postal service. Umar bin Abdul-Aziz developed it further by building caravanserais at stages along the Khurasan highway. Relays of horses were used for the conveyance of dispatches between the caliph and his agents and officials posted in the provinces. The main highways were divided into stages of 12 miles (19 km) each and each stage had horses, donkeys or camels ready to carry the post. Primarily the service met the needs of Government officials, but travellers and their important dispatches were also benefitted by the system. The postal carriages were also used for the swift transport of troops. They were able to carry fifty to a hundred men at a time. Under Governor Yusuf bin Umar, the postal department of Iraq cost 4,000,000 dirhams a year.
However many early history books like the Islamic Conquest of Syria Fatuhusham by al-Imam al-Waqidi state that after the conversion to Islam Muawiyah's father Abu Sufyan ibn Harb and his brothers Yazid ibn Abi Sufyan were appointed as commanders in the Muslim armies by Muhammad. Muawiyah, Abu Sufyan ibn Harb, Yazid ibn Abi Sufyan and Hind bint Utbah fought in the Battle of Yarmouk. The defeat of the Byzantine Emperor Heraclius at the Battle of Yarmouk opened the way for the Muslim expansion into Jerusalem and Syria.
Non-Muslim groups in the Umayyad Caliphate, which included Christians, Jews, Zoroastrians, and pagan Berbers, were called dhimmis. They were given a legally protected status as second-class citizens as long as they accepted and acknowledged the political supremacy of the ruling Muslims. They were allowed to have their own courts, and were given freedom of their religion within the empire.[citation needed] Although they could not hold the highest public offices in the empire, they had many bureaucratic positions within the government. Christians and Jews still continued to produce great theological thinkers within their communities, but as time wore on, many of the intellectuals converted to Islam, leading to a lack of great thinkers in the non-Muslim communities.
The Umayyad caliphate was marked both by territorial expansion and by the administrative and cultural problems that such expansion created. Despite some notable exceptions, the Umayyads tended to favor the rights of the old Arab families, and in particular their own, over those of newly converted Muslims (mawali). Therefore, they held to a less universalist conception of Islam than did many of their rivals. As G.R. Hawting has written, "Islam was in fact regarded as the property of the conquering aristocracy."
Many Muslims criticized the Umayyads for having too many non-Muslim, former Roman administrators in their government. St John of Damascus was also a high administrator in the Umayyad administration. As the Muslims took over cities, they left the peoples political representatives and the Roman tax collectors and the administrators. The taxes to the central government were calculated and negotiated by the peoples political representatives. The Central government got paid for the services it provided and the local government got the money for the services it provided. Many Christian cities also used some of the taxes on maintain their churches and run their own organizations. Later the Umayyads were criticized by some Muslims for not reducing the taxes of the people who converted to Islam. These new converts continues to pay the same taxes that were previously negotiated.
The Umayyads have met with a largely negative reception from later Islamic historians, who have accused them of promoting a kingship (mulk, a term with connotations of tyranny) instead of a true caliphate (khilafa). In this respect it is notable that the Umayyad caliphs referred to themselves not as khalifat rasul Allah ("successor of the messenger of God", the title preferred by the tradition), but rather as khalifat Allah ("deputy of God"). The distinction seems to indicate that the Umayyads "regarded themselves as God's representatives at the head of the community and saw no need to share their religious power with, or delegate it to, the emergent class of religious scholars." In fact, it was precisely this class of scholars, based largely in Iraq, that was responsible for collecting and recording the traditions that form the primary source material for the history of the Umayyad period. In reconstructing this history, therefore, it is necessary to rely mainly on sources, such as the histories of Tabari and Baladhuri, that were written in the Abbasid court at Baghdad.
A cappella [a kapˈpɛlla] (Italian for "in the manner of the chapel") music is specifically group or solo singing without instrumental accompaniment, or a piece intended to be performed in this way. It contrasts with cantata, which is accompanied singing. The term "a cappella" was originally intended to differentiate between Renaissance polyphony and Baroque concertato style. In the 19th century a renewed interest in Renaissance polyphony coupled with an ignorance of the fact that vocal parts were often doubled by instrumentalists led to the term coming to mean unaccompanied vocal music. The term is also used, albeit rarely, as a synonym for alla breve.
A cappella music was originally used in religious music, especially church music as well as anasheed and zemirot. Gregorian chant is an example of a cappella singing, as is the majority of secular vocal music from the Renaissance. The madrigal, up until its development in the early Baroque into an instrumentally-accompanied form, is also usually in a cappella form. Jewish and Christian music were originally a cappella,[citation needed] and this practice has continued in both of these religions as well as in Islam.
The polyphony of Christian a cappella music began to develop in Europe around the late 15th century, with compositions by Josquin des Prez. The early a cappella polyphonies may have had an accompanying instrument, although this instrument would merely double the singers' parts and was not independent. By the 16th century, a cappella polyphony had further developed, but gradually, the cantata began to take the place of a cappella forms. 16th century a cappella polyphony, nonetheless, continued to influence church composers throughout this period and to the present day. Recent evidence has shown that some of the early pieces by Palestrina, such as what was written for the Sistine Chapel was intended to be accompanied by an organ "doubling" some or all of the voices. Such is seen in the life of Palestrina becoming a major influence on Bach, most notably in the aforementioned Mass in B Minor. Other composers that utilized the a cappella style, if only for the occasional piece, were Claudio Monteverdi and his masterpiece, Lagrime d'amante al sepolcro dell'amata (A lover's tears at his beloved's grave), which was composed in 1610, and Andrea Gabrieli when upon his death it was discovered many choral pieces, one of which was in the unaccompanied style. Learning from the preceding two composeres, Heinrich Schütz utilized the a cappella style in numerous pieces, chief among these were the pieces in the oratorio style, which were traditionally performed during the Easter week and dealt with the religious subject matter of that week, such as Christ's suffering and the Passion. Five of Schutz's Historien were Easter pieces, and of these the latter three, which dealt with the passion from three different viewpoints, those of Matthew, Luke and John, were all done a cappella style. This was a near requirement for this type of piece, and the parts of the crowd were sung while the solo parts which were the quoted parts from either Christ or the authors were performed in a plainchant.
In the Byzantine Rite of the Eastern Orthodox Church and the Eastern Catholic Churches, the music performed in the liturgies is exclusively sung without instrumental accompaniment. Bishop Kallistos Ware says, "The service is sung, even though there may be no choir... In the Orthodox Church today, as in the early Church, singing is unaccompanied and instrumental music is not found." This a cappella behavior arises from strict interpretation of Psalms 150, which states, Let every thing that hath breath praise the Lord. Praise ye the Lord. In keeping with this philosophy, early Russian musika which started appearing in the late 17th century, in what was known as khorovïye kontsertï (choral concertos) made a cappella adaptations of Venetian-styled pieces, such as the treatise, Grammatika musikiyskaya (1675), by Nikolai Diletsky. Divine Liturgies and Western Rite masses composed by famous composers such as Peter Tchaikovsky, Sergei Rachmaninoff, Alexander Arkhangelsky, and Mykola Leontovych are fine examples of this.
Present-day Christian religious bodies known for conducting their worship services without musical accompaniment include some Presbyterian churches devoted to the regulative principle of worship, Old Regular Baptists, Primitive Baptists, Plymouth Brethren, Churches of Christ, the Old German Baptist Brethren, Doukhobors the Byzantine Rite and the Amish, Old Order Mennonites and Conservative Mennonites. Certain high church services and other musical events in liturgical churches (such as the Roman Catholic Mass and the Lutheran Divine Service) may be a cappella, a practice remaining from apostolic times. Many Mennonites also conduct some or all of their services without instruments. Sacred Harp, a type of folk music, is an a cappella style of religious singing with shape notes, usually sung at singing conventions.
Instruments have divided Christendom since their introduction into worship. They were considered a Catholic innovation, not widely practiced until the 18th century, and were opposed vigorously in worship by a number of Protestant Reformers, including Martin Luther (1483–1546), Ulrich Zwingli, John Calvin (1509–1564) and John Wesley (1703–1791). Alexander Campbell referred to the use of an instrument in worship as "a cow bell in a concert". In Sir Walter Scott's The Heart of Midlothian, the heroine, Jeanie Deans, a Scottish Presbyterian, writes to her father about the church situation she has found in England (bold added):
Those who subscribe to this interpretation believe that since the Christian scriptures never counter instrumental language with any negative judgment on instruments, opposition to instruments instead comes from an interpretation of history. There is no written opposition to musical instruments in any setting in the first century and a half of Christian churches (33 AD to 180AD). The use of instruments for Christian worship during this period is also undocumented. Toward the end of the 2nd century, Christians began condemning the instruments themselves. Those who oppose instruments today believe these Church Fathers had a better understanding of God's desire for the church,[citation needed] but there are significant differences between the teachings of these Church Fathers and Christian opposition to instruments today.
While worship in the Temple in Jerusalem included musical instruments (2 Chronicles 29:25–27), traditional Jewish religious services in the Synagogue, both before and after the last destruction of the Temple, did not include musical instruments given the practice of scriptural cantillation. The use of musical instruments is traditionally forbidden on the Sabbath out of concern that players would be tempted to repair (or tune) their instruments, which is forbidden on those days. (This prohibition has been relaxed in many Reform and some Conservative congregations.) Similarly, when Jewish families and larger groups sing traditional Sabbath songs known as zemirot outside the context of formal religious services, they usually do so a cappella, and Bar and Bat Mitzvah celebrations on the Sabbath sometimes feature entertainment by a cappella ensembles. During the Three Weeks musical instruments are prohibited. Many Jews consider a portion of the 49-day period of the counting of the omer between Passover and Shavuot to be a time of semi-mourning and instrumental music is not allowed during that time. This has led to a tradition of a cappella singing sometimes known as sefirah music.
The popularization of the Jewish chant may be found in the writings of the Jewish philosopher Philo, born 20 BCE. Weaving together Jewish and Greek thought, Philo promoted praise without instruments, and taught that "silent singing" (without even vocal chords) was better still. This view parted with the Jewish scriptures, where Israel offered praise with instruments by God's own command (2 Chronicles 29:25). The shofar is the only temple instrument still being used today in the synagogue, and it is only used from Rosh Chodesh Elul through the end of Yom Kippur. The shofar is used by itself, without any vocal accompaniment, and is limited to a very strictly defined set of sounds and specific places in the synagogue service.
A strong and prominent a cappella tradition was begun in the midwest part of the United States in 1911 by F. Melius Christiansen, a music faculty member at St. Olaf College in Northfield, Minnesota. The St. Olaf College Choir was established as an outgrowth of the local St. John's Lutheran Church, where Christiansen was organist and the choir was composed, at least partially, of students from the nearby St. Olaf campus. The success of the ensemble was emulated by other regional conductors, and a rich tradition of a cappella choral music was born in the region at colleges like Concordia College (Moorhead, Minnesota), Augustana College (Rock Island, Illinois), Wartburg College (Waverly, Iowa), Luther College (Decorah, Iowa), Gustavus Adolphus College (St. Peter, Minnesota), Augustana College (Sioux Falls, South Dakota), and Augsburg College (Minneapolis, Minnesota). The choirs typically range from 40 to 80 singers and are recognized for their efforts to perfect blend, intonation, phrasing and pitch in a large choral setting.
In July 1943, as a result of the American Federation of Musicians boycott of US recording studios, the a cappella vocal group The Song Spinners had a best-seller with "Comin' In On A Wing And A Prayer". In the 1950s several recording groups, notably The Hi-Los and the Four Freshmen, introduced complex jazz harmonies to a cappella performances. The King's Singers are credited with promoting interest in small-group a cappella performances in the 1960s. In 1983 an a cappella group known as The Flying Pickets had a Christmas 'number one' in the UK with a cover of Yazoo's (known in the US as Yaz) "Only You". A cappella music attained renewed prominence from the late 1980s onward, spurred by the success of Top 40 recordings by artists such as The Manhattan Transfer, Bobby McFerrin, Huey Lewis and the News, All-4-One, The Nylons, Backstreet Boys and Boyz II Men.[citation needed]
Contemporary a cappella includes many vocal groups and bands who add vocal percussion or beatboxing to create a pop/rock/gospel sound, in some cases very similar to bands with instruments. Examples of such professional groups include Straight No Chaser, Pentatonix, The House Jacks, Rockapella, Mosaic, and M-pact. There also remains a strong a cappella presence within Christian music, as some denominations purposefully do not use instruments during worship. Examples of such groups are Take 6, Glad and Acappella. Arrangements of popular music for small a cappella ensembles typically include one voice singing the lead melody, one singing a rhythmic bass line, and the remaining voices contributing chordal or polyphonic accompaniment.
A cappella has been used as the sole orchestration for original works of musical theater that have had commercial runs Off-Broadway (theaters in New York City with 99 to 500 seats) only four times. The first was Avenue X which opened on 28 January 1994 and ran for 77 performances. It was produced by Playwrights Horizons with book by John Jiler, music and lyrics by Ray Leslee. The musical style of the show's score was primarily Doo-Wop as the plot revolved around Doo-Wop group singers of the 1960s.
The a cappella musical Perfect Harmony, a comedy about two high school a cappella groups vying to win the National championship, made its Off Broadway debut at Theatre Row’s Acorn Theatre on 42nd Street in New York City in October, 2010 after a successful out-of-town run at the Stoneham Theatre, in Stoneham, Massachusetts. Perfect Harmony features the hit music of The Jackson 5, Pat Benatar, Billy Idol, Marvin Gaye, Scandal, Tiffany, The Romantics, The Pretenders, The Temptations, The Contours, The Commodores, Tommy James & the Shondells and The Partridge Family, and has been compared to a cross between Altar Boyz and The 25th Annual Putnam County Spelling Bee.
The fourth a cappella musical to appear Off-Broadway, In Transit, premiered 5 October 2010 and was produced by Primary Stages with book, music, and lyrics by Kristen Anderson-Lopez, James-Allen Ford, Russ Kaplan, and Sara Wordsworth. Set primarily in the New York City subway system its score features an eclectic mix of musical genres (including jazz, hip hop, Latin, rock, and country). In Transit incorporates vocal beat boxing into its contemporary a cappella arrangements through the use of a subway beat boxer character. Beat boxer and actor Chesney Snow performed this role for the 2010 Primary Stages production. According to the show's website, it is scheduled to reopen for an open-ended commercial run in the Fall of 2011. In 2011 the production received four Lucille Lortel Award nominations including Outstanding Musical, Outer Critics Circle and Drama League nominations, as well as five Drama Desk nominations including Outstanding Musical and won for Outstanding Ensemble Performance.
Barbershop music is one of several uniquely American art forms. The earliest reports of this style of a cappella music involved African Americans. The earliest documented quartets all began in barbershops. In 1938, the first formal men's barbershop organization was formed, known as the Society for the Preservation and Encouragement of Barber Shop Quartet Singing in America (S.P.E.B.S.Q.S.A), and in 2004 rebranded itself and officially changed its public name to the Barbershop Harmony Society (BHS). Today the BHS has over 22,000 members in approximately 800 chapters across the United States, and the barbershop style has spread around the world with organizations in many other countries. The Barbershop Harmony Society provides a highly organized competition structure for a cappella quartets and choruses singing in the barbershop style.
In 1945, the first formal women's barbershop organization, Sweet Adelines, was formed. In 1953 Sweet Adelines became an international organization, although it didn't change its name to Sweet Adelines International until 1991. The membership of nearly 25,000 women, all singing in English, includes choruses in most of the fifty United States as well as in Australia, Canada, England, Finland, Germany, Ireland, Japan, New Zealand, Scotland, Sweden, Wales and the Netherlands. Headquartered in Tulsa, Oklahoma, the organization encompasses more than 1,200 registered quartets and 600 choruses.
The reasons for the strong Swedish dominance are as explained by Richard Sparks manifold; suffice to say here that there is a long-standing tradition, an unsusually large proportion of the populations (5% is often cited) regularly sing in choirs, the Swedish choral director Eric Ericson had an enormous impact on a cappella choral development not only in Sweden but around the world, and finally there are a large number of very popular primary and secondary schools (music schools) with high admission standards based on auditions that combine a rigid academic regimen with high level choral singing on every school day, a system that started with Adolf Fredrik's Music School in Stockholm in 1939 but has spread over the country.
It is not clear exactly where collegiate a cappella began. The Rensselyrics of Rensselaer Polytechnic Institute (formerly known as the RPI Glee Club), established in 1873 is perhaps the oldest known collegiate a cappella group.[citation needed] However the longest continuously-singing group is probably The Whiffenpoofs of Yale University, which was formed in 1909 and once included Cole Porter as a member. Collegiate a cappella groups grew throughout the 20th century. Some notable historical groups formed along the way include Princeton University's Tigertones (1946), Colgate University's The Colgate 13 (1942), Dartmouth College's Aires (1946), Cornell University's Cayuga's Waiters (1949) and The Hangovers (1968), the University of Maine Maine Steiners (1958), the Columbia University Kingsmen (1949), the Jabberwocks of Brown University (1949), and the University of Rochester YellowJackets (1956). All-women a cappella groups followed shortly, frequently as a parody of the men's groups: the Smiffenpoofs of Smith College (1936), The Shwiffs of Connecticut College (The She-Whiffenpoofs, 1944), and The Chattertocks of Brown University (1951). A cappella groups exploded in popularity beginning in the 1990s, fueled in part by a change in style popularized by the Tufts University Beelzebubs and the Boston University Dear Abbeys. The new style used voices to emulate modern rock instruments, including vocal percussion/"beatboxing". Some larger universities now have multiple groups. Groups often join one another in on-campus concerts, such as the Georgetown Chimes' Cherry Tree Massacre, a 3-weekend a cappella festival held each February since 1975, where over a hundred collegiate groups have appeared, as well as International Quartet Champions The Boston Common and the contemporary commercial a cappella group Rockapella. Co-ed groups have produced many up-and-coming and major artists, including John Legend, an alumnus of the Counterparts at the University of Pennsylvania, and Sara Bareilles, an alumna of Awaken A Cappella at University of California, Los Angeles. Mira Sorvino is an alumna of the Harvard-Radcliffe Veritones of Harvard College where she had the solo on Only You by Yaz.
A cappella is gaining popularity among South Asians with the emergence of primarily Hindi-English College groups. The first South Asian a cappella group was Penn Masala, founded in 1996 at the University of Pennsylvania. Co-ed South Asian a cappella groups are also gaining in popularity. The first co-ed south Asian a cappella was Anokha, from the University of Maryland, formed in 2001. Also, Dil se, another co-ed a cappella from UC Berkeley, hosts the "Anahat" competition at the University of California, Berkeley annually. Maize Mirchi, the co-ed a cappella group from the University of Michigan hosts "Sa Re Ga Ma Pella", an annual South Asian a cappella invitational with various groups from the Midwest.
Increased interest in modern a cappella (particularly collegiate a cappella) can be seen in the growth of awards such as the Contemporary A Cappella Recording Awards (overseen by the Contemporary A Cappella Society) and competitions such as the International Championship of Collegiate A Cappella for college groups and the Harmony Sweepstakes for all groups. In December 2009, a new television competition series called The Sing-Off aired on NBC. The show featured eight a cappella groups from the United States and Puerto Rico vying for the prize of $100,000 and a recording contract with Epic Records/Sony Music. The show was judged by Ben Folds, Shawn Stockman, and Nicole Scherzinger and was won by an all-male group from Puerto Rico called Nota. The show returned for a second and third season, won by Committed and Pentatonix, respectively.
In addition to singing words, some a cappella singers also emulate instrumentation by reproducing instrumental sounds with their vocal cords and mouth. One of the earliest 20th century practitioners of this method were The Mills Brothers whose early recordings of the 1930s clearly stated on the label that all instrumentation was done vocally. More recently, "Twilight Zone" by 2 Unlimited was sung a cappella to the instrumentation on the comedy television series Tompkins Square. Another famous example of emulating instrumentation instead of singing the words is the theme song for The New Addams Family series on Fox Family Channel (now ABC Family). Groups such as Vocal Sampling and Undivided emulate Latin rhythms a cappella. In the 1960s, the Swingle Singers used their voices to emulate musical instruments to Baroque and Classical music. Vocal artist Bobby McFerrin is famous for his instrumental emulation. A cappella group Naturally Seven recreates entire songs using vocal tones for every instrument.
The Swingle Singers used nonsense words to sound like instruments, but have been known to produce non-verbal versions of musical instruments. Like the other groups, examples of their music can be found on YouTube. Beatboxing, more accurately known as vocal percussion, is a technique used in a cappella music popularized by the hip-hop community, where rap is often performed a cappella also. The advent of vocal percussion added new dimensions to the a cappella genre and has become very prevalent in modern arrangements. Petra Haden used a four-track recorder to produce an a cappella version of The Who Sell Out including the instruments and fake advertisements on her album Petra Haden Sings: The Who Sell Out in 2005. Haden has also released a cappella versions of Journey's "Don't Stop Believin'", The Beach Boys' "God Only Knows" and Michael Jackson's "Thriller". In 2009, Toyota commissioned Haden to perform three songs for television commercials for the third-generation Toyota Prius, including an a cappella version of The Bellamy Brothers' 1970s song "Let Your Love Flow".[citation needed]
Florida i/ˈflɒrɪdə/ (Spanish for "flowery land") is a state located in the southeastern region of the United States. The state is bordered to the west by the Gulf of Mexico, to the north by Alabama and Georgia, to the east by the Atlantic Ocean, and to the south by the Straits of Florida and the sovereign state of Cuba. Florida is the 22nd most extensive, the 3rd most populous, and the 8th most densely populated of the United States. Jacksonville is the most populous city in Florida, and the largest city by area in the contiguous United States. The Miami metropolitan area is the eighth-largest metropolitan area in the United States. Tallahassee is the state capital.
A peninsula between the Gulf of Mexico, the Atlantic Ocean, and the Straits of Florida, it has the longest coastline in the contiguous United States, approximately 1,350 miles (2,170 km), and is the only state that borders both the Gulf of Mexico and the Atlantic Ocean. Much of the state is at or near sea level and is characterized by sedimentary soil. The climate varies from subtropical in the north to tropical in the south. The American alligator, American crocodile, Florida panther, and manatee can be found in the Everglades National Park.
"By May 1539, Conquistador Hernando de Soto skirted the coast of Florida, searching for a deep harbor to land. He described seeing a thick wall of red mangroves spread mile after mile, some reaching as high as 70 feet (21 m), with intertwined and elevated roots making landing difficult. Very soon, 'many smokes' appeared 'along the whole coast', billowing against the sky, when the Native ancestors of the Seminole spotted the newcomers and spread the alarm by signal fires". The Spanish introduced Christianity, cattle, horses, sheep, the Spanish language, and more to Florida.[full citation needed] Both the Spanish and French established settlements in Florida, with varying degrees of success. In 1559, Don Tristán de Luna y Arellano established a colony at present-day Pensacola, one of the first European settlements in the continental United States, but it was abandoned by 1561.
In 1763, Spain traded Florida to the Kingdom of Great Britain for control of Havana, Cuba, which had been captured by the British during the Seven Years' War. It was part of a large expansion of British territory following the country's victory in the Seven Years' War. Almost the entire Spanish population left, taking along most of the remaining indigenous population to Cuba. The British soon constructed the King's Road connecting St. Augustine to Georgia. The road crossed the St. Johns River at a narrow point, which the Seminole called Wacca Pilatka and the British named "Cow Ford", both names ostensibly reflecting the fact that cattle were brought across the river there.
The British divided Florida into the two colonies of British East Florida and British West Florida. The British government gave land grants to officers and soldiers who had fought in the French and Indian War in order to encourage settlement. In order to induce settlers to move to the two new colonies reports of the natural wealth of Florida were published in England. A large number of British colonists who were "energetic and of good character" moved to Florida, mostly coming from South Carolina, Georgia and England though there was also a group of settlers who came from the colony of Bermuda. This would be the first permanent English-speaking population in what is now Duval County, Baker County, St. Johns County and Nassau County. The British built good public roads and introduced the cultivation of sugar cane, indigo and fruits as well the export of lumber.
As a result of these initiatives northeastern Florida prospered economically in a way it never did under Spanish rule. Furthermore, the British governors were directed to call general assemblies as soon as possible in order to make laws for the Floridas and in the meantime they were, with the advice of councils, to establish courts. This would be the first introduction of much of the English-derived legal system which Florida still has today including trial by jury, habeas corpus and county-based government. Neither East Florida nor West Florida would send any representatives to Philadelphia to draft the Declaration of Independence. Florida would remain a Loyalist stronghold for the duration of the American Revolution.
Americans of English descent and Americans of Scots-Irish descent began moving into northern Florida from the backwoods of Georgia and South Carolina. Though technically not allowed by the Spanish authorities, the Spanish were never able to effectively police the border region and the backwoods settlers from the United States would continue to migrate into Florida unchecked. These migrants, mixing with the already present British settlers who had remained in Florida since the British period, would be the progenitors of the population known as Florida Crackers.
These American settlers established a permanent foothold in the area and ignored Spanish officials. The British settlers who had remained also resented Spanish rule, leading to a rebellion in 1810 and the establishment for ninety days of the so-called Free and Independent Republic of West Florida on September 23. After meetings beginning in June, rebels overcame the Spanish garrison at Baton Rouge (now in Louisiana), and unfurled the flag of the new republic: a single white star on a blue field. This flag would later become known as the "Bonnie Blue Flag".
Seminole Indians based in East Florida began raiding Georgia settlements, and offering havens for runaway slaves. The United States Army led increasingly frequent incursions into Spanish territory, including the 1817–1818 campaign against the Seminole Indians by Andrew Jackson that became known as the First Seminole War. The United States now effectively controlled East Florida. Control was necessary according to Secretary of State John Quincy Adams because Florida had become "a derelict open to the occupancy of every enemy, civilized or savage, of the United States, and serving no other earthly purpose than as a post of annoyance to them.".
Florida had become a burden to Spain, which could not afford to send settlers or garrisons. Madrid therefore decided to cede the territory to the United States through the Adams-Onís Treaty, which took effect in 1821. President James Monroe was authorized on March 3, 1821 to take possession of East Florida and West Florida for the United States and provide for initial governance. Andrew Jackson served as military governor of the newly acquired territory, but only for a brief period. On March 30, 1822, the United States merged East Florida and part of West Florida into the Florida Territory.
By the early 1800s, Indian removal was a significant issue throughout the southeastern U.S. and also in Florida. In 1830, the U.S. Congress passed the Indian Removal Act and as settlement increased, pressure grew on the United States government to remove the Indians from Florida. Seminoles harbored runaway blacks, known as the Black Seminoles, and clashes between whites and Indians grew with the influx of new settlers. In 1832, the Treaty of Payne's Landing promised to the Seminoles lands west of the Mississippi River if they agreed to leave Florida. Many Seminole left at this time.
The climate of Florida is tempered somewhat by the fact that no part of the state is distant from the ocean. North of Lake Okeechobee, the prevalent climate is humid subtropical (Köppen: Cfa), while areas south of the lake (including the Florida Keys) have a true tropical climate (Köppen: Aw). Mean high temperatures for late July are primarily in the low 90s Fahrenheit (32–34 °C). Mean low temperatures for early to mid January range from the low 40s Fahrenheit (4–7 °C) in northern Florida to above 60 °F (16 °C) from Miami on southward. With an average daily temperature of 70.7 °F (21.5 °C), it is the warmest state in the country.
Florida's nickname is the "Sunshine State", but severe weather is a common occurrence in the state. Central Florida is known as the lightning capital of the United States, as it experiences more lightning strikes than anywhere else in the country. Florida has one of the highest average precipitation levels of any state, in large part because afternoon thunderstorms are common in much of the state from late spring until early autumn. A narrow eastern part of the state including Orlando and Jacksonville receives between 2,400 and 2,800 hours of sunshine annually. The rest of the state, including Miami, receives between 2,800 and 3,200 hours annually.
Hurricanes pose a severe threat each year during the June 1 to November 30 hurricane season, particularly from August to October. Florida is the most hurricane-prone state, with subtropical or tropical water on a lengthy coastline. Of the category 4 or higher storms that have struck the United States, 83% have either hit Florida or Texas. From 1851 to 2006, Florida was struck by 114 hurricanes, 37 of them major—category 3 and above. It is rare for a hurricane season to pass without any impact in the state by at least a tropical storm.[citation needed]
Extended systems of underwater caves, sinkholes and springs are found throughout the state and supply most of the water used by residents. The limestone is topped with sandy soils deposited as ancient beaches over millions of years as global sea levels rose and fell. During the last glacial period, lower sea levels and a drier climate revealed a much wider peninsula, largely savanna. The Everglades, an enormously wide, slow-flowing river encompasses the southern tip of the peninsula. Sinkhole damage claims on property in the state exceeded a total of $2 billion from 2006 through 2010.
The United States Census Bureau estimates that the population of Florida was 20,271,272 on July 1, 2015, a 7.82% increase since the 2010 United States Census. The population of Florida in the 2010 census was 18,801,310. Florida was the seventh fastest-growing state in the U.S. in the 12-month period ending July 1, 2012. In 2010, the center of population of Florida was located between Fort Meade and Frostproof. The center of population has moved less than 5 miles (8 km) to the east and approximately 1 mile (1.6 km) to the north between 1980 and 2010 and has been located in Polk County since the 1960 census. The population exceeded 19.7 million by December 2014, surpassing the population of the state of New York for the first time.
Florida is among the three states with the most severe felony disenfranchisement laws. Florida requires felons to have completed sentencing, parole and/or probation, and then seven years later, to apply individually for restoration of voting privileges. As in other aspects of the criminal justice system, this law has disproportionate effects for minorities. As a result, according to Brent Staples, based on data from The Sentencing Project, the effect of Florida's law is such that in 2014 "[m]ore than one in ten Floridians – and nearly one in four African-American Floridians – are shut out of the polls because of felony convictions."
In 2010, 6.9% of the population (1,269,765) considered themselves to be of only American ancestry (regardless of race or ethnicity). Many of these were of English or Scotch-Irish descent; however, their families have lived in the state for so long, that they choose to identify as having "American" ancestry or do not know their ancestry. In the 1980 United States census the largest ancestry group reported in Florida was English with 2,232,514 Floridians claiming that they were of English or mostly English American ancestry. Some of their ancestry went back to the original thirteen colonies.
As of 2010, those of (non-Hispanic white) European ancestry accounted for 57.9% of Florida's population. Out of the 57.9%, the largest groups were 12.0% German (2,212,391), 10.7% Irish (1,979,058), 8.8% English (1,629,832), 6.6% Italian (1,215,242), 2.8% Polish (511,229), and 2.7% French (504,641). White Americans of all European backgrounds are present in all areas of the state. In 1970, non-Hispanic whites were nearly 80% of Florida's population. Those of English and Irish ancestry are present in large numbers in all the urban/suburban areas across the state. Some native white Floridians, especially those who have descended from long-time Florida families, may refer to themselves as "Florida crackers"; others see the term as a derogatory one. Like whites in most of the other Southern states, they descend mainly from English and Scots-Irish settlers, as well as some other British American settlers.
As of 2010, those of Hispanic or Latino ancestry ancestry accounted for 22.5% (4,223,806) of Florida's population. Out of the 22.5%, the largest groups were 6.5% (1,213,438) Cuban, 4.5% (847,550) Puerto Rican, 3.3% (629,718) Mexican, and 1.6% (300,414) Colombian. Florida's Hispanic population includes large communities of Cuban Americans in Miami and Tampa, Puerto Ricans in Orlando and Tampa, and Mexican/Central American migrant workers. The Hispanic community continues to grow more affluent and mobile. As of 2011, 57.0% of Florida's children under the age of 1 belonged to minority groups. Florida has a large and diverse Hispanic population, with Cubans and Puerto Ricans being the largest groups in the state. Nearly 80% of Cuban Americans live in Florida, especially South Florida where there is a long-standing and affluent Cuban community. Florida has the second largest Puerto Rican population after New York, as well as the fastest-growing in the nation. Puerto Ricans are more widespread throughout the state, though the heaviest concentrations are in the Orlando area of Central Florida.
As of 2010, those of African ancestry accounted for 16.0% of Florida's population, which includes African Americans. Out of the 16.0%, 4.0% (741,879) were West Indian or Afro-Caribbean American. During the early 1900s, black people made up nearly half of the state's population. In response to segregation, disfranchisement and agricultural depression, many African Americans migrated from Florida to northern cities in the Great Migration, in waves from 1910 to 1940, and again starting in the later 1940s. They moved for jobs, better education for their children and the chance to vote and participate in society. By 1960 the proportion of African Americans in the state had declined to 18%. Conversely large numbers of northern whites moved to the state.[citation needed] Today, large concentrations of black residents can be found in northern and central Florida. Aside from blacks descended from African slaves brought to the US south, there are also large numbers of blacks of West Indian, recent African, and Afro-Latino immigrant origins, especially in the Miami/South Florida area. In 2010, Florida had the highest percentage of West Indians in the United States, with 2.0% (378,926) from Haitian ancestry, and 1.3% (236,950) Jamaican. All other (non-Hispanic) Caribbean nations were well below 0.1% of Florida residents.
From 1952 to 1964, most voters were registered Democrats, but the state voted for the Republican presidential candidate in every election except for 1964. The following year, Congress passed and President Lyndon B. Johnson signed the Voting Rights Act of 1965, providing for oversight of state practices and enforcement of constitutional voting rights for African Americans and other minorities in order to prevent the discrimination and disenfranchisement that had excluded most of them for decades from the political process.
From the 1930s through much of the 1960s, Florida was essentially a one-party state dominated by white conservative Democrats, who together with other Democrats of the Solid South, exercised considerable control in Congress. They gained federal money from national programs; like other southern states, Florida residents have received more federal monies than they pay in taxes: the state is a net beneficiary. Since the 1970s, the conservative white majority of voters in the state has largely shifted from the Democratic to the Republican Party. It has continued to support Republican presidential candidates through the 20th century, except in 1976 and 1996, when the Democratic nominee was from the South. They have had "the luxury of voting for presidential candidates who pledge to cut taxes and halt the expansion of government while knowing that their congressional delegations will continue to protect federal spending."
The first post-Reconstruction era Republican elected to Congress from Florida was William C. Cramer in 1954 from Pinellas County on the Gulf Coast, where demographic changes were underway. In this period, African Americans were still disenfranchised by the state's constitution and discriminatory practices; in the 19th century they had made up most of the Republican Party. Cramer built a different Republican Party in Florida, attracting local white conservatives and transplants from northern and midwestern states. In 1966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election. In 1968 Edward J. Gurney, also a white conservative, was elected as the state's first post-reconstruction Republican US Senator. In 1970 Democrats took the governorship and the open US Senate seat, and maintained dominance for years.
In 1998, Democratic voters dominated areas of the state with a high percentage of racial minorities and transplanted white liberals from the northeastern United States, known colloquially as "snowbirds". South Florida and the Miami metropolitan area are dominated by both racial minorities and white liberals. Because of this, the area has consistently voted as one of the most Democratic areas of the state. The Daytona Beach area is similar demographically and the city of Orlando has a large Hispanic population, which has often favored Democrats. Republicans, made up mostly of white conservatives, have dominated throughout much of the rest of Florida, particularly in the more rural and suburban areas. This is characteristic of its voter base throughout the Deep South.
The fast-growing I-4 corridor area, which runs through Central Florida and connects the cities of Daytona Beach, Orlando, and Tampa/St. Petersburg, has had a fairly even breakdown of Republican and Democratic voters. The area is often seen as a merging point of the conservative northern portion of the state and the liberal southern portion, making it the biggest swing area in the state. Since the late 20th century, the voting results in this area, containing 40% of Florida voters, has often determined who will win the state of Florida in presidential elections.
Reapportionment following the 2010 United States Census gave the state two more seats in the House of Representatives. The legislature's redistricting, announced in 2012, was quickly challenged in court, on the grounds that it had unfairly benefited Republican interests. In 2015, the Florida Supreme Court ruled on appeal that the congressional districts had to be redrawn because of the legislature's violation of the Fair District Amendments to the state constitution passed in 2010; it accepted a new map in early December 2015.
In the closely contested 2000 election, the state played a pivotal role. Out of more than 5.8 million votes for the two main contenders Bush and Al Gore, around 500 votes separated the two candidates for the all-decisive Florida electoral votes that landed Bush the election win. Florida's felony disenfranchisement law is more severe than most European nations or other American states. A 2002 study in the American Sociological Review concluded that "if the state’s 827,000 disenfranchised felons had voted at the same rate as other Floridians, Democratic candidate Al Gore would have won Florida—and the presidency—by more than 80,000 votes."
The court ruled in 2014, after lengthy testimony, that at least two districts had to be redrawn because of gerrymandering. After this was appealed, in July 2015 the Florida Supreme Court ruled that lawmakers had followed an illegal and unconstitutional process overly influenced by party operatives, and ruled that at least eight districts had to be redrawn. On December 2, 2015, a 5-2 majority of the Court accepted a new map of congressional districts, some of which was drawn by challengers. Their ruling affirmed the map previously approved by Leon County Judge Terry Lewis, who had overseen the original trial. It particularly makes changes in South Florida. There are likely to be additional challenges to the map and districts.
The Gross Domestic Product (GDP) of Florida in 2010 was $748 billion. Its GDP is the fourth largest economy in the United States. In 2010, it became the fourth largest exporter of trade goods. The major contributors to the state's gross output in 2007 were general services, financial services, trade, transportation and public utilities, manufacturing and construction respectively. In 2010–11, the state budget was $70.5 billion, having reached a high of $73.8 billion in 2006–07. Chief Executive Magazine name Florida the third "Best State for Business" in 2011.
At the end of the third quarter in 2008, Florida had the highest mortgage delinquency rate in the country, with 7.8% of mortgages delinquent at least 60 days. A 2009 list of national housing markets that were hard hit in the real estate crash included a disproportionate number in Florida. The early 21st-century building boom left Florida with 300,000 vacant homes in 2009, according to state figures. In 2009, the US Census Bureau estimated that Floridians spent an average 49.1% of personal income on housing-related costs, the third highest percentage in the country.
After the watershed events of Hurricane Andrew in 1992, the state of Florida began investing in economic development through the Office of Trade, Tourism, and Economic Development. Governor Jeb Bush realized that watershed events such as Andrew negatively impacted Florida's backbone industry of tourism severely. The office was directed to target Medical/Bio-Sciences among others. Three years later, The Scripps Research Institute (TSRI) announced it had chosen Florida for its newest expansion. In 2003, TSRI announced plans to establish a major science center in Palm Beach, a 364,000 square feet (33,800 m2) facility on 100 acres (40 ha), which TSRI planned to occupy in 2006.
Some sections of the state feature architectural styles including Spanish revival, Florida vernacular, and Mediterranean Revival Style. It has the largest collection of Art Deco and Streamline Moderne buildings in both the United States and the entire world, most of which are located in the Miami metropolitan area, especially Miami Beach's Art Deco District, constructed as the city was becoming a resort destination. A unique architectural design found only in Florida is the post-World War II Miami Modern, which can be seen in areas such as Miami's MiMo Historic District.
Florida is served by Amtrak, operating numerous lines throughout, connecting the state's largest cities to points north in the United States and Canada. The busiest Amtrak train stations in Florida in 2011 were: Sanford (259,944), Orlando (179,142), Tampa Union Station (140,785), Miami (94,556), and Jacksonville (74,733). Sanford, in Greater Orlando, is the southern terminus of the Auto Train, which originates at Lorton, Virginia, south of Washington, D.C.. Until 2005, Orlando was also the eastern terminus of the Sunset Limited, which travels across the southern United States via New Orleans, Houston, and San Antonio to its western terminus of Los Angeles. Florida is served by two additional Amtrak trains (the Silver Star and the Silver Meteor), which operate between New York City and Miami. Miami Central Station, the city's rapid transit, commuter rail, intercity rail, and bus hub, is under construction.
NASCAR (headquartered in Daytona Beach) begins all three of its major auto racing series in Florida at Daytona International Speedway in February, featuring the Daytona 500, and ends all three Series in November at Homestead-Miami Speedway. Daytona also has the Coke Zero 400 NASCAR race weekend around Independence Day in July. The 24 Hours of Daytona is one of the world's most prestigious endurance auto races. The Grand Prix of St. Petersburg and Grand Prix of Miami have held IndyCar races as well.
The priesthoods of public religion were held by members of the elite classes. There was no principle analogous to separation of church and state in ancient Rome. During the Roman Republic (509–27 BC), the same men who were elected public officials might also serve as augurs and pontiffs. Priests married, raised families, and led politically active lives. Julius Caesar became pontifex maximus before he was elected consul. The augurs read the will of the gods and supervised the marking of boundaries as a reflection of universal order, thus sanctioning Roman expansionism as a matter of divine destiny. The Roman triumph was at its core a religious procession in which the victorious general displayed his piety and his willingness to serve the public good by dedicating a portion of his spoils to the gods, especially Jupiter, who embodied just rule. As a result of the Punic Wars (264–146 BC), when Rome struggled to establish itself as a dominant power, many new temples were built by magistrates in fulfillment of a vow to a deity for assuring their military success.
Roman religion was thus practical and contractual, based on the principle of do ut des, "I give that you might give." Religion depended on knowledge and the correct practice of prayer, ritual, and sacrifice, not on faith or dogma, although Latin literature preserves learned speculation on the nature of the divine and its relation to human affairs. Even the most skeptical among Rome's intellectual elite such as Cicero, who was an augur, saw religion as a source of social order. For ordinary Romans, religion was a part of daily life. Each home had a household shrine at which prayers and libations to the family's domestic deities were offered. Neighborhood shrines and sacred places such as springs and groves dotted the city. The Roman calendar was structured around religious observances. Women, slaves, and children all participated in a range of religious activities. Some public rituals could be conducted only by women, and women formed what is perhaps Rome's most famous priesthood, the state-supported Vestals, who tended Rome's sacred hearth for centuries, until disbanded under Christian domination.
The Romans are known for the great number of deities they honored, a capacity that earned the mockery of early Christian polemicists. The presence of Greeks on the Italian peninsula from the beginning of the historical period influenced Roman culture, introducing some religious practices that became as fundamental as the cult of Apollo. The Romans looked for common ground between their major gods and those of the Greeks (interpretatio graeca), adapting Greek myths and iconography for Latin literature and Roman art. Etruscan religion was also a major influence, particularly on the practice of augury.
Imported mystery religions, which offered initiates salvation in the afterlife, were a matter of personal choice for an individual, practiced in addition to carrying on one's family rites and participating in public religion. The mysteries, however, involved exclusive oaths and secrecy, conditions that conservative Romans viewed with suspicion as characteristic of "magic", conspiratorial (coniuratio), or subversive activity. Sporadic and sometimes brutal attempts were made to suppress religionists who seemed to threaten traditional morality and unity, as with the senate's efforts to restrict the Bacchanals in 186 BC.
As the Romans extended their dominance throughout the Mediterranean world, their policy in general was to absorb the deities and cults of other peoples rather than try to eradicate them, since they believed that preserving tradition promoted social stability. One way that Rome incorporated diverse peoples was by supporting their religious heritage, building temples to local deities that framed their theology within the hierarchy of Roman religion. Inscriptions throughout the Empire record the side-by-side worship of local and Roman deities, including dedications made by Romans to local gods. By the height of the Empire, numerous international deities were cultivated at Rome and had been carried to even the most remote provinces, among them Cybele, Isis, Epona, and gods of solar monism such as Mithras and Sol Invictus, found as far north as Roman Britain. Because Romans had never been obligated to cultivate one god or one cult only, religious tolerance was not an issue in the sense that it is for competing monotheistic systems. The monotheistic rigor of Judaism posed difficulties for Roman policy that led at times to compromise and the granting of special exemptions, but sometimes to intractable conflict. For example, religious disputes helped cause the First Jewish–Roman War and the Bar Kokhba revolt.
In the wake of the Republic's collapse, state religion had adapted to support the new regime of the emperors. Augustus, the first Roman emperor, justified the novelty of one-man rule with a vast program of religious revivalism and reform. Public vows formerly made for the security of the republic now were directed at the wellbeing of the emperor. So-called "emperor worship" expanded on a grand scale the traditional Roman veneration of the ancestral dead and of the Genius, the divine tutelary of every individual. Imperial cult became one of the major ways in which Rome advertised its presence in the provinces and cultivated shared cultural identity and loyalty throughout the Empire. Rejection of the state religion was tantamount to treason. This was the context for Rome's conflict with Christianity, which Romans variously regarded as a form of atheism and novel superstitio.
Rome had a semi-divine ancestor in the Trojan refugee Aeneas, son of Venus, who was said to have established the nucleus of Roman religion when he brought the Palladium, Lares and Penates from Troy to Italy. These objects were believed in historical times to remain in the keeping of the Vestals, Rome's female priesthood. Aeneas had been given refuge by King Evander, a Greek exile from Arcadia, to whom were attributed other religious foundations: he established the Ara Maxima, "Greatest Altar," to Hercules at the site that would become the Forum Boarium, and he was the first to celebrate the Lupercalia, an archaic festival in February that was celebrated as late as the 5th century of the Christian era.
The myth of a Trojan founding with Greek influence was reconciled through an elaborate genealogy (the Latin kings of Alba Longa) with the well-known legend of Rome's founding by Romulus and Remus. The most common version of the twins' story displays several aspects of hero myth. Their mother, Rhea Silvia, had been ordered by her uncle the king to remain a virgin, in order to preserve the throne he had usurped from her father. Through divine intervention, the rightful line was restored when Rhea Silvia was impregnated by the god Mars. She gave birth to twins, who were duly exposed by order of the king but saved through a series of miraculous events.
Romulus was credited with several religious institutions. He founded the Consualia festival, inviting the neighbouring Sabines to participate; the ensuing rape of the Sabine women by Romulus's men further embedded both violence and cultural assimilation in Rome's myth of origins. As a successful general, Romulus is also supposed to have founded Rome's first temple to Jupiter Feretrius and offered the spolia opima, the prime spoils taken in war, in the celebration of the first Roman triumph. Spared a mortal's death, Romulus was mysteriously spirited away and deified.
Each of Rome's legendary or semi-legendary kings was associated with one or more religious institutions still known to the later Republic. Tullus Hostilius and Ancus Marcius instituted the fetial priests. The first "outsider" Etruscan king, Lucius Tarquinius Priscus, founded a Capitoline temple to the triad Jupiter, Juno and Minerva which served as the model for the highest official cult throughout the Roman world. The benevolent, divinely fathered Servius Tullius established the Latin League, its Aventine Temple to Diana, and the Compitalia to mark his social reforms. Servius Tullius was murdered and succeeded by the arrogant Tarquinius Superbus, whose expulsion marked the beginning of Rome as a republic with annually elected magistrates.
Rome offers no native creation myth, and little mythography to explain the character of its deities, their mutual relationships or their interactions with the human world, but Roman theology acknowledged that di immortales (immortal gods) ruled all realms of the heavens and earth. There were gods of the upper heavens, gods of the underworld and a myriad of lesser deities between. Some evidently favoured Rome because Rome honoured them, but none were intrinsically, irredeemably foreign or alien. The political, cultural and religious coherence of an emergent Roman super-state required a broad, inclusive and flexible network of lawful cults. At different times and in different places, the sphere of influence, character and functions of a divine being could expand, overlap with those of others, and be redefined as Roman. Change was embedded within existing traditions.
Several versions of a semi-official, structured pantheon were developed during the political, social and religious instability of the Late Republican era. Jupiter, the most powerful of all gods and "the fount of the auspices upon which the relationship of the city with the gods rested", consistently personified the divine authority of Rome's highest offices, internal organization and external relations. During the archaic and early Republican eras, he shared his temple, some aspects of cult and several divine characteristics with Mars and Quirinus, who were later replaced by Juno and Minerva. A conceptual tendency toward triads may be indicated by the later agricultural or plebeian triad of Ceres, Liber and Libera, and by some of the complementary threefold deity-groupings of Imperial cult. Other major and minor deities could be single, coupled, or linked retrospectively through myths of divine marriage and sexual adventure. These later Roman pantheistic hierarchies are part literary and mythographic, part philosophical creations, and often Greek in origin. The Hellenization of Latin literature and culture supplied literary and artistic models for reinterpreting Roman deities in light of the Greek Olympians, and promoted a sense that the two cultures had a shared heritage.
The impressive, costly, and centralised rites to the deities of the Roman state were vastly outnumbered in everyday life by commonplace religious observances pertaining to an individual's domestic and personal deities, the patron divinities of Rome's various neighborhoods and communities, and the often idiosyncratic blends of official, unofficial, local and personal cults that characterised lawful Roman religion. In this spirit, a provincial Roman citizen who made the long journey from Bordeaux to Italy to consult the Sibyl at Tibur did not neglect his devotion to his own goddess from home:
Roman calendars show roughly forty annual religious festivals. Some lasted several days, others a single day or less: sacred days (dies fasti) outnumbered "non-sacred" days (dies nefasti). A comparison of surviving Roman religious calendars suggests that official festivals were organized according to broad seasonal groups that allowed for different local traditions. Some of the most ancient and popular festivals incorporated ludi ("games," such as chariot races and theatrical performances), with examples including those held at Palestrina in honour of Fortuna Primigenia during Compitalia, and the Ludi Romani in honour of Liber. Other festivals may have required only the presence and rites of their priests and acolytes, or particular groups, such as women at the Bona Dea rites.
Other public festivals were not required by the calendar, but occasioned by events. The triumph of a Roman general was celebrated as the fulfillment of religious vows, though these tended to be overshadowed by the political and social significance of the event. During the late Republic, the political elite competed to outdo each other in public display, and the ludi attendant on a triumph were expanded to include gladiator contests. Under the Principate, all such spectacular displays came under Imperial control: the most lavish were subsidised by emperors, and lesser events were provided by magistrates as a sacred duty and privilege of office. Additional festivals and games celebrated Imperial accessions and anniversaries. Others, such as the traditional Republican Secular Games to mark a new era (saeculum), became imperially funded to maintain traditional values and a common Roman identity. That the spectacles retained something of their sacral aura even in late antiquity is indicated by the admonitions of the Church Fathers that Christians should not take part.
The meaning and origin of many archaic festivals baffled even Rome's intellectual elite, but the more obscure they were, the greater the opportunity for reinvention and reinterpretation — a fact lost neither on Augustus in his program of religious reform, which often cloaked autocratic innovation, nor on his only rival as mythmaker of the era, Ovid. In his Fasti, a long-form poem covering Roman holidays from January to June, Ovid presents a unique look at Roman antiquarian lore, popular customs, and religious practice that is by turns imaginative, entertaining, high-minded, and scurrilous; not a priestly account, despite the speaker's pose as a vates or inspired poet-prophet, but a work of description, imagination and poetic etymology that reflects the broad humor and burlesque spirit of such venerable festivals as the Saturnalia, Consualia, and feast of Anna Perenna on the Ides of March, where Ovid treats the assassination of the newly deified Julius Caesar as utterly incidental to the festivities among the Roman people. But official calendars preserved from different times and places also show a flexibility in omitting or expanding events, indicating that there was no single static and authoritative calendar of required observances. In the later Empire under Christian rule, the new Christian festivals were incorporated into the existing framework of the Roman calendar, alongside at least some of the traditional festivals.
The Latin word templum originally referred not to the temple building itself, but to a sacred space surveyed and plotted ritually through augury: "The architecture of the ancient Romans was, from first to last, an art of shaping space around ritual." The Roman architect Vitruvius always uses the word templum to refer to this sacred precinct, and the more common Latin words aedes, delubrum, or fanum for a temple or shrine as a building. The ruins of temples are among the most visible monuments of ancient Roman culture.
All sacrifices and offerings required an accompanying prayer to be effective. Pliny the Elder declared that "a sacrifice without prayer is thought to be useless and not a proper consultation of the gods." Prayer by itself, however, had independent power. The spoken word was thus the single most potent religious action, and knowledge of the correct verbal formulas the key to efficacy. Accurate naming was vital for tapping into the desired powers of the deity invoked, hence the proliferation of cult epithets among Roman deities. Public prayers (prex) were offered loudly and clearly by a priest on behalf of the community. Public religious ritual had to be enacted by specialists and professionals faultlessly; a mistake might require that the action, or even the entire festival, be repeated from the start. The historian Livy reports an occasion when the presiding magistrate at the Latin festival forgot to include the "Roman people" among the list of beneficiaries in his prayer; the festival had to be started over. Even private prayer by an individual was formulaic, a recitation rather than a personal expression, though selected by the individual for a particular purpose or occasion.
Sacrifice to deities of the heavens (di superi, "gods above") was performed in daylight, and under the public gaze. Deities of the upper heavens required white, infertile victims of their own sex: Juno a white heifer (possibly a white cow); Jupiter a white, castrated ox (bos mas) for the annual oath-taking by the consuls. Di superi with strong connections to the earth, such as Mars, Janus, Neptune and various genii – including the Emperor's – were offered fertile victims. After the sacrifice, a banquet was held; in state cults, the images of honoured deities took pride of place on banqueting couches and by means of the sacrificial fire consumed their proper portion (exta, the innards). Rome's officials and priests reclined in order of precedence alongside and ate the meat; lesser citizens may have had to provide their own.
Chthonic gods such as Dis pater, the di inferi ("gods below"), and the collective shades of the departed (di Manes) were given dark, fertile victims in nighttime rituals. Animal sacrifice usually took the form of a holocaust or burnt offering, and there was no shared banquet, as "the living cannot share a meal with the dead". Ceres and other underworld goddesses of fruitfulness were sometimes offered pregnant female animals; Tellus was given a pregnant cow at the Fordicidia festival. Color had a general symbolic value for sacrifices. Demigods and heroes, who belonged to the heavens and the underworld, were sometimes given black-and-white victims. Robigo (or Robigus) was given red dogs and libations of red wine at the Robigalia for the protection of crops from blight and red mildew.
The same divine agencies who caused disease or harm also had the power to avert it, and so might be placated in advance. Divine consideration might be sought to avoid the inconvenient delays of a journey, or encounters with banditry, piracy and shipwreck, with due gratitude to be rendered on safe arrival or return. In times of great crisis, the Senate could decree collective public rites, in which Rome's citizens, including women and children, moved in procession from one temple to the next, supplicating the gods.
Extraordinary circumstances called for extraordinary sacrifice: in one of the many crises of the Second Punic War, Jupiter Capitolinus was promised every animal born that spring (see ver sacrum), to be rendered after five more years of protection from Hannibal and his allies. The "contract" with Jupiter is exceptionally detailed. All due care would be taken of the animals. If any died or were stolen before the scheduled sacrifice, they would count as already sacrificed, since they had already been consecrated. Normally, if the gods failed to keep their side of the bargain, the offered sacrifice would be withheld. In the imperial period, sacrifice was withheld following Trajan's death because the gods had not kept the Emperor safe for the stipulated period. In Pompeii, the Genius of the living emperor was offered a bull: presumably a standard practise in Imperial cult, though minor offerings (incense and wine) were also made.
 The exta were the entrails of a sacrificed animal, comprising in Cicero's enumeration the gall bladder (fel), liver (iecur), heart (cor), and lungs (pulmones). The exta were exposed for litatio (divine approval) as part of Roman liturgy, but were "read" in the context of the disciplina Etrusca. As a product of Roman sacrifice, the exta and blood are reserved for the gods, while the meat (viscera) is shared among human beings in a communal meal. The exta of bovine victims were usually stewed in a pot (olla or aula), while those of sheep or pigs were grilled on skewers. When the deity's portion was cooked, it was sprinkled with mola salsa (ritually prepared salted flour) and wine, then placed in the fire on the altar for the offering; the technical verb for this action was porricere.
Human sacrifice in ancient Rome was rare but documented. After the Roman defeat at Cannae two Gauls and two Greeks were buried under the Forum Boarium, in a stone chamber "which had on a previous occasion [228 BC] also been polluted by human victims, a practice most repulsive to Roman feelings". Livy avoids the word "sacrifice" in connection with this bloodless human life-offering; Plutarch does not. The rite was apparently repeated in 113 BC, preparatory to an invasion of Gaul. Its religious dimensions and purpose remain uncertain.
In the early stages of the First Punic War (264 BC) the first known Roman gladiatorial munus was held, described as a funeral blood-rite to the manes of a Roman military aristocrat. The gladiator munus was never explicitly acknowledged as a human sacrifice, probably because death was not its inevitable outcome or purpose. Even so, the gladiators swore their lives to the infernal gods, and the combat was dedicated as an offering to the di manes or other gods. The event was therefore a sacrificium in the strict sense of the term, and Christian writers later condemned it as human sacrifice.
The small woolen dolls called Maniae, hung on the Compitalia shrines, were thought a symbolic replacement for child-sacrifice to Mania, as Mother of the Lares. The Junii took credit for its abolition by their ancestor L. Junius Brutus, traditionally Rome's Republican founder and first consul. Political or military executions were sometimes conducted in such a way that they evoked human sacrifice, whether deliberately or in the perception of witnesses; Marcus Marius Gratidianus was a gruesome example.
Officially, human sacrifice was obnoxious "to the laws of gods and men." The practice was a mark of the "Other", attributed to Rome's traditional enemies such as the Carthaginians and Gauls. Rome banned it on several occasions under extreme penalty. A law passed in 81 BC characterised human sacrifice as murder committed for magical purposes. Pliny saw the ending of human sacrifice conducted by the druids as a positive consequence of the conquest of Gaul and Britain. Despite an empire-wide ban under Hadrian, human sacrifice may have continued covertly in North Africa and elsewhere.
A pater familias was the senior priest of his household. He offered daily cult to his lares and penates, and to his di parentes/divi parentes at his domestic shrines and in the fires of the household hearth. His wife (mater familias) was responsible for the household's cult to Vesta. In rural estates, bailiffs seem to have been responsible for at least some of the household shrines (lararia) and their deities. Household cults had state counterparts. In Vergil's Aeneid, Aeneas brought the Trojan cult of the lares and penates from Troy, along with the Palladium which was later installed in the temple of Vesta.
Religious law centered on the ritualised system of honours and sacrifice that brought divine blessings, according to the principle do ut des ("I give, that you might give"). Proper, respectful religio brought social harmony and prosperity. Religious neglect was a form of atheism: impure sacrifice and incorrect ritual were vitia (impious errors). Excessive devotion, fearful grovelling to deities and the improper use or seeking of divine knowledge were superstitio. Any of these moral deviations could cause divine anger (ira deorum) and therefore harm the State. The official deities of the state were identified with its lawful offices and institutions, and Romans of every class were expected to honour the beneficence and protection of mortal and divine superiors. Participation in public rites showed a personal commitment to their community and its values.
Official cults were state funded as a "matter of public interest" (res publica). Non-official but lawful cults were funded by private individuals for the benefit of their own communities. The difference between public and private cult is often unclear. Individuals or collegial associations could offer funds and cult to state deities. The public Vestals prepared ritual substances for use in public and private cults, and held the state-funded (thus public) opening ceremony for the Parentalia festival, which was otherwise a private rite to household ancestors. Some rites of the domus (household) were held in public places but were legally defined as privata in part or whole. All cults were ultimately subject to the approval and regulation of the censor and pontifices.
Rome had no separate priestly caste or class. The highest authority within a community usually sponsored its cults and sacrifices, officiated as its priest and promoted its assistants and acolytes. Specialists from the religious colleges and professionals such as haruspices and oracles were available for consultation. In household cult, the paterfamilias functioned as priest, and members of his familia as acolytes and assistants. Public cults required greater knowledge and expertise. The earliest public priesthoods were probably the flamines (the singular is flamen), attributed to king Numa: the major flamines, dedicated to Jupiter, Mars and Quirinus, were traditionally drawn from patrician families. Twelve lesser flamines were each dedicated to a single deity, whose archaic nature is indicated by the relative obscurity of some. Flamines were constrained by the requirements of ritual purity; Jupiter's flamen in particular had virtually no simultaneous capacity for a political or military career.
In the Regal era, a rex sacrorum (king of the sacred rites) supervised regal and state rites in conjunction with the king (rex) or in his absence, and announced the public festivals. He had little or no civil authority. With the abolition of monarchy, the collegial power and influence of the Republican pontifices increased. By the late Republican era, the flamines were supervised by the pontifical collegia. The rex sacrorum had become a relatively obscure priesthood with an entirely symbolic title: his religious duties still included the daily, ritual announcement of festivals and priestly duties within two or three of the latter but his most important priestly role – the supervision of the Vestals and their rites – fell to the more politically powerful and influential pontifex maximus.
Public priests were appointed by the collegia. Once elected, a priest held permanent religious authority from the eternal divine, which offered him lifetime influence, privilege and immunity. Therefore, civil and religious law limited the number and kind of religious offices allowed an individual and his family. Religious law was collegial and traditional; it informed political decisions, could overturn them, and was difficult to exploit for personal gain. Priesthood was a costly honour: in traditional Roman practice, a priest drew no stipend. Cult donations were the property of the deity, whose priest must provide cult regardless of shortfalls in public funding – this could mean subsidy of acolytes and all other cult maintenance from personal funds. For those who had reached their goal in the Cursus honorum, permanent priesthood was best sought or granted after a lifetime's service in military or political life, or preferably both: it was a particularly honourable and active form of retirement which fulfilled an essential public duty. For a freedman or slave, promotion as one of the Compitalia seviri offered a high local profile, and opportunities in local politics; and therefore business. During the Imperial era, priesthood of the Imperial cult offered provincial elites full Roman citizenship and public prominence beyond their single year in religious office; in effect, it was the first step in a provincial cursus honorum. In Rome, the same Imperial cult role was performed by the Arval Brethren, once an obscure Republican priesthood dedicated to several deities, then co-opted by Augustus as part of his religious reforms. The Arvals offered prayer and sacrifice to Roman state gods at various temples for the continued welfare of the Imperial family on their birthdays, accession anniversaries and to mark extraordinary events such as the quashing of conspiracy or revolt. Every January 3 they consecrated the annual vows and rendered any sacrifice promised in the previous year, provided the gods had kept the Imperial family safe for the contracted time.
The Vestals were a public priesthood of six women devoted to the cultivation of Vesta, goddess of the hearth of the Roman state and its vital flame. A girl chosen to be a Vestal achieved unique religious distinction, public status and privileges, and could exercise considerable political influence. Upon entering her office, a Vestal was emancipated from her father's authority. In archaic Roman society, these priestesses were the only women not required to be under the legal guardianship of a man, instead answering directly to the Pontifex Maximus.
A Vestal's dress represented her status outside the usual categories that defined Roman women, with elements of both virgin bride and daughter, and Roman matron and wife. Unlike male priests, Vestals were freed of the traditional obligations of marrying and producing children, and were required to take a vow of chastity that was strictly enforced: a Vestal polluted by the loss of her chastity while in office was buried alive. Thus the exceptional honor accorded a Vestal was religious rather than personal or social; her privileges required her to be fully devoted to the performance of her duties, which were considered essential to the security of Rome.
The Vestals embody the profound connection between domestic cult and the religious life of the community. Any householder could rekindle their own household fire from Vesta's flame. The Vestals cared for the Lares and Penates of the state that were the equivalent of those enshrined in each home. Besides their own festival of Vestalia, they participated directly in the rites of Parilia, Parentalia and Fordicidia. Indirectly, they played a role in every official sacrifice; among their duties was the preparation of the mola salsa, the salted flour that was sprinkled on every sacrificial victim as part of its immolation.
Augustus' religious reformations raised the funding and public profile of the Vestals. They were given high-status seating at games and theatres. The emperor Claudius appointed them as priestesses to the cult of the deified Livia, wife of Augustus. They seem to have retained their religious and social distinctions well into the 4th century, after political power within the Empire had shifted to the Christians. When the Christian emperor Gratian refused the office of pontifex maximus, he took steps toward the dissolution of the order. His successor Theodosius I extinguished Vesta's sacred fire and vacated her temple.
Public religion took place within a sacred precinct that had been marked out ritually by an augur. The original meaning of the Latin word templum was this sacred space, and only later referred to a building. Rome itself was an intrinsically sacred space; its ancient boundary (pomerium) had been marked by Romulus himself with oxen and plough; what lay within was the earthly home and protectorate of the gods of the state. In Rome, the central references for the establishment of an augural templum appear to have been the Via Sacra (Sacred Way) and the pomerium. Magistrates sought divine opinion of proposed official acts through an augur, who read the divine will through observations made within the templum before, during and after an act of sacrifice. Divine disapproval could arise through unfit sacrifice, errant rites (vitium) or an unacceptable plan of action. If an unfavourable sign was given, the magistrate could repeat the sacrifice until favourable signs were seen, consult with his augural colleagues, or abandon the project. Magistrates could use their right of augury (ius augurum) to adjourn and overturn the process of law, but were obliged to base their decision on the augur's observations and advice. For Cicero, himself an augur, this made the augur the most powerful authority in the Late Republic. By his time (mid 1st century BC) augury was supervised by the college of pontifices, whose powers were increasingly woven into the magistracies of the cursus honorum.
Haruspicy was also used in public cult, under the supervision of the augur or presiding magistrate. The haruspices divined the will of the gods through examination of entrails after sacrifice, particularly the liver. They also interpreted omens, prodigies and portents, and formulated their expiation. Most Roman authors describe haruspicy as an ancient, ethnically Etruscan "outsider" religious profession, separate from Rome's internal and largely unpaid priestly hierarchy, essential but never quite respectable. During the mid-to-late Republic, the reformist Gaius Gracchus, the populist politician-general Gaius Marius and his antagonist Sulla, and the "notorious Verres" justified their very different policies by the divinely inspired utterances of private diviners. The senate and armies used the public haruspices: at some time during the late Republic, the Senate decreed that Roman boys of noble family be sent to Etruria for training in haruspicy and divination. Being of independent means, they would be better motivated to maintain a pure, religious practice for the public good. The motives of private haruspices – especially females – and their clients were officially suspect: none of this seems to have troubled Marius, who employed a Syrian prophetess.
Prodigies were transgressions in the natural, predictable order of the cosmos – signs of divine anger that portended conflict and misfortune. The Senate decided whether a reported prodigy was false, or genuine and in the public interest, in which case it was referred to the public priests, augurs and haruspices for ritual expiation. In 207 BC, during one of the Punic Wars' worst crises, the Senate dealt with an unprecedented number of confirmed prodigies whose expiation would have involved "at least twenty days" of dedicated rites.
Livy presents these as signs of widespread failure in Roman religio. The major prodigies included the spontaneous combustion of weapons, the apparent shrinking of the sun's disc, two moons in a daylit sky, a cosmic battle between sun and moon, a rain of red-hot stones, a bloody sweat on statues, and blood in fountains and on ears of corn: all were expiated by sacrifice of "greater victims". The minor prodigies were less warlike but equally unnatural; sheep become goats, a hen become a cock (and vice versa) – these were expiated with "lesser victims". The discovery of an androgynous four-year-old child was expiated by its drowning and the holy procession of 27 virgins to the temple of Juno Regina, singing a hymn to avert disaster: a lightning strike during the hymn rehearsals required further expiation. Religious restitution is proved only by Rome's victory.
Roman beliefs about an afterlife varied, and are known mostly for the educated elite who expressed their views in terms of their chosen philosophy. The traditional care of the dead, however, and the perpetuation after death of their status in life were part of the most archaic practices of Roman religion. Ancient votive deposits to the noble dead of Latium and Rome suggest elaborate and costly funeral offerings and banquets in the company of the deceased, an expectation of afterlife and their association with the gods. As Roman society developed, its Republican nobility tended to invest less in spectacular funerals and extravagant housing for their dead, and more on monumental endowments to the community, such as the donation of a temple or public building whose donor was commemorated by his statue and inscribed name. Persons of low or negligible status might receive simple burial, with such grave goods as relatives could afford.
Funeral and commemorative rites varied according to wealth, status and religious context. In Cicero's time, the better-off sacrificed a sow at the funeral pyre before cremation. The dead consumed their portion in the flames of the pyre, Ceres her portion through the flame of her altar, and the family at the site of the cremation. For the less well-off, inhumation with "a libation of wine, incense, and fruit or crops was sufficient". Ceres functioned as an intermediary between the realms of the living and the dead: the deceased had not yet fully passed to the world of the dead and could share a last meal with the living. The ashes (or body) were entombed or buried. On the eighth day of mourning, the family offered further sacrifice, this time on the ground; the shade of the departed was assumed to have passed entirely into the underworld. They had become one of the di Manes, who were collectively celebrated and appeased at the Parentalia, a multi-day festival of remembrance in February.
In the later Imperial era, the burial and commemorative practises of Christian and non-Christians overlapped. Tombs were shared by Christian and non-Christian family members, and the traditional funeral rites and feast of novemdialis found a part-match in the Christian Constitutio Apostolica. The customary offers of wine and food to the dead continued; St Augustine (following St Ambrose) feared that this invited the "drunken" practices of Parentalia but commended funeral feasts as a Christian opportunity to give alms of food to the poor. Christians attended Parentalia and its accompanying Feralia and Caristia in sufficient numbers for the Council of Tours to forbid them in AD 567. Other funerary and commemorative practices were very different. Traditional Roman practice spurned the corpse as a ritual pollution; inscriptions noted the day of birth and duration of life. The Christian Church fostered the veneration of saintly relics, and inscriptions marked the day of death as a transition to "new life".
Roman camps followed a standard pattern for defense and religious ritual; in effect they were Rome in miniature. The commander's headquarters stood at the centre; he took the auspices on a dais in front. A small building behind housed the legionary standards, the divine images used in religious rites and in the Imperial era, the image of the ruling emperor. In one camp, this shrine is even called Capitolium. The most important camp-offering appears to have been the suovetaurilia performed before a major, set battle. A ram, a boar and a bull were ritually garlanded, led around the outer perimeter of the camp (a lustratio exercitus) and in through a gate, then sacrificed: Trajan's column shows three such events from his Dacian wars. The perimeter procession and sacrifice suggest the entire camp as a divine templum; all within are purified and protected.
Each camp had its own religious personnel; standard bearers, priestly officers and their assistants, including a haruspex, and housekeepers of shrines and images. A senior magistrate-commander (sometimes even a consul) headed it, his chain of subordinates ran it and a ferocious system of training and discipline ensured that every citizen-soldier knew his duty. As in Rome, whatever gods he served in his own time seem to have been his own business; legionary forts and vici included shrines to household gods, personal deities and deities otherwise unknown. From the earliest Imperial era, citizen legionaries and provincial auxiliaries gave cult to the emperor and his familia on Imperial accessions, anniversaries and their renewal of annual vows. They celebrated Rome's official festivals in absentia, and had the official triads appropriate to their function – in the Empire, Jupiter, Victoria and Concordia were typical. By the early Severan era, the military also offered cult to the Imperial divi, the current emperor's numen, genius and domus (or familia), and special cult to the Empress as "mother of the camp." The near ubiquitous legionary shrines to Mithras of the later Imperial era were not part of official cult until Mithras was absorbed into Solar and Stoic Monism as a focus of military concordia and Imperial loyalty.
The devotio was the most extreme offering a Roman general could make, promising to offer his own life in battle along with the enemy as an offering to the underworld gods. Livy offers a detailed account of the devotio carried out by Decius Mus; family tradition maintained that his son and grandson, all bearing the same name, also devoted themselves. Before the battle, Decius is granted a prescient dream that reveals his fate. When he offers sacrifice, the victim's liver appears "damaged where it refers to his own fortunes". Otherwise, the haruspex tells him, the sacrifice is entirely acceptable to the gods. In a prayer recorded by Livy, Decius commits himself and the enemy to the dii Manes and Tellus, charges alone and headlong into the enemy ranks, and is killed; his action cleanses the sacrificial offering. Had he failed to die, his sacrificial offering would have been tainted and therefore void, with possibly disastrous consequences. The act of devotio is a link between military ethics and those of the Roman gladiator.
The efforts of military commanders to channel the divine will were on occasion less successful. In the early days of Rome's war against Carthage, the commander Publius Claudius Pulcher (consul 249 BC) launched a sea campaign "though the sacred chickens would not eat when he took the auspices." In defiance of the omen, he threw them into the sea, "saying that they might drink, since they would not eat. He was defeated, and on being bidden by the senate to appoint a dictator, he appointed his messenger Glycias, as if again making a jest of his country's peril." His impiety not only lost the battle but ruined his career.
Roman women were present at most festivals and cult observances. Some rituals specifically required the presence of women, but their active participation was limited. As a rule women did not perform animal sacrifice, the central rite of most major public ceremonies. In addition to the public priesthood of the Vestals, some cult practices were reserved for women only. The rites of the Bona Dea excluded men entirely. Because women enter the public record less frequently than men, their religious practices are less known, and even family cults were headed by the paterfamilias. A host of deities, however, are associated with motherhood. Juno, Diana, Lucina, and specialized divine attendants presided over the life-threatening act of giving birth and the perils of caring for a baby at a time when the infant mortality rate was as high as 40 percent.
Excessive devotion and enthusiasm in religious observance were superstitio, in the sense of "doing or believing more than was necessary", to which women and foreigners were considered particularly prone. The boundaries between religio and superstitio are perhaps indefinite. The famous tirade of Lucretius, the Epicurean rationalist, against what is usually translated as "superstition" was in fact aimed at excessive religio. Roman religion was based on knowledge rather than faith, but superstitio was viewed as an "inappropriate desire for knowledge"; in effect, an abuse of religio.
In the everyday world, many individuals sought to divine the future, influence it through magic, or seek vengeance with help from "private" diviners. The state-sanctioned taking of auspices was a form of public divination with the intent of ascertaining the will of the gods, not foretelling the future. Secretive consultations between private diviners and their clients were thus suspect. So were divinatory techniques such as astrology when used for illicit, subversive or magical purposes. Astrologers and magicians were officially expelled from Rome at various times, notably in 139 BC and 33 BC. In 16 BC Tiberius expelled them under extreme penalty because an astrologer had predicted his death. "Egyptian rites" were particularly suspect: Augustus banned them within the pomerium to doubtful effect; Tiberius repeated and extended the ban with extreme force in AD 19. Despite several Imperial bans, magic and astrology persisted among all social classes. In the late 1st century AD, Tacitus observed that astrologers "would always be banned and always retained at Rome".
In the Graeco-Roman world, practitioners of magic were known as magi (singular magus), a "foreign" title of Persian priests. Apuleius, defending himself against accusations of casting magic spells, defined the magician as "in popular tradition (more vulgari)... someone who, because of his community of speech with the immortal gods, has an incredible power of spells (vi cantaminum) for everything he wishes to." Pliny the Elder offers a thoroughly skeptical "History of magical arts" from their supposed Persian origins to Nero's vast and futile expenditure on research into magical practices in an attempt to control the gods. Philostratus takes pains to point out that the celebrated Apollonius of Tyana was definitely not a magus, "despite his special knowledge of the future, his miraculous cures, and his ability to vanish into thin air".
Lucan depicts Sextus Pompeius, the doomed son of Pompey the Great, as convinced "the gods of heaven knew too little" and awaiting the Battle of Pharsalus by consulting with the Thessalian witch Erichtho, who practices necromancy and inhabits deserted graves, feeding on rotting corpses. Erichtho, it is said, can arrest "the rotation of the heavens and the flow of rivers" and make "austere old men blaze with illicit passions". She and her clients are portrayed as undermining the natural order of gods, mankind and destiny. A female foreigner from Thessaly, notorious for witchcraft, Erichtho is the stereotypical witch of Latin literature, along with Horace's Canidia.
The Twelve Tables forbade any harmful incantation (malum carmen, or 'noisome metrical charm'); this included the "charming of crops from one field to another" (excantatio frugum) and any rite that sought harm or death to others. Chthonic deities functioned at the margins of Rome's divine and human communities; although sometimes the recipients of public rites, these were conducted outside the sacred boundary of the pomerium. Individuals seeking their aid did so away from the public gaze, during the hours of darkness. Burial grounds and isolated crossroads were among the likely portals. The barrier between private religious practices and "magic" is permeable, and Ovid gives a vivid account of rites at the fringes of the public Feralia festival that are indistinguishable from magic: an old woman squats among a circle of younger women, sews up a fish-head, smears it with pitch, then pierces and roasts it to "bind hostile tongues to silence". By this she invokes Tacita, the "Silent One" of the underworld.
Archaeology confirms the widespread use of binding spells (defixiones), magical papyri and so-called "voodoo dolls" from a very early era. Around 250 defixiones have been recovered just from Roman Britain, in both urban and rural settings. Some seek straightforward, usually gruesome revenge, often for a lover's offense or rejection. Others appeal for divine redress of wrongs, in terms familiar to any Roman magistrate, and promise a portion of the value (usually small) of lost or stolen property in return for its restoration. None of these defixiones seem produced by, or on behalf of the elite, who had more immediate recourse to human law and justice. Similar traditions existed throughout the empire, persisting until around the 7th century AD, well into the Christian era.
Rome's government, politics and religion were dominated by an educated, male, landowning military aristocracy. Approximately half Rome's population were slave or free non-citizens. Most others were plebeians, the lowest class of Roman citizens. Less than a quarter of adult males had voting rights; far fewer could actually exercise them. Women had no vote. However, all official business was conducted under the divine gaze and auspices, in the name of the senate and people of Rome. "In a very real sense the senate was the caretaker of the Romans’ relationship with the divine, just as it was the caretaker of their relationship with other humans".
The links between religious and political life were vital to Rome's internal governance, diplomacy and development from kingdom, to Republic and to Empire. Post-regal politics dispersed the civil and religious authority of the kings more or less equitably among the patrician elite: kingship was replaced by two annually elected consular offices. In the early Republic, as presumably in the regal era, plebeians were excluded from high religious and civil office, and could be punished for offenses against laws of which they had no knowledge. They resorted to strikes and violence to break the oppressive patrician monopolies of high office, public priesthood, and knowledge of civil and religious law. The senate appointed Camillus as dictator to handle the emergency; he negotiated a settlement, and sanctified it by the dedication of a temple to Concordia. The religious calendars and laws were eventually made public. Plebeian tribunes were appointed, with sacrosanct status and the right of veto in legislative debate. In principle, the augural and pontifical colleges were now open to plebeians. In reality, the patrician and to a lesser extent, plebeian nobility dominated religious and civil office throughout the Republican era and beyond.
While the new plebeian nobility made social, political and religious inroads on traditionally patrician preserves, their electorate maintained their distinctive political traditions and religious cults. During the Punic crisis, popular cult to Dionysus emerged from southern Italy; Dionysus was equated with Father Liber, the inventor of plebeian augury and personification of plebeian freedoms, and with Roman Bacchus. Official consternation at these enthusiastic, unofficial Bacchanalia cults was expressed as moral outrage at their supposed subversion, and was followed by ferocious suppression. Much later, a statue of Marsyas, the silen of Dionysus flayed by Apollo, became a focus of brief symbolic resistance to Augustus' censorship. Augustus himself claimed the patronage of Venus and Apollo; but his settlement appealed to all classes. Where loyalty was implicit, no divine hierarchy need be politically enforced; Liber's festival continued.
The Augustan settlement built upon a cultural shift in Roman society. In the middle Republican era, even Scipio's tentative hints that he might be Jupiter's special protege sat ill with his colleagues. Politicians of the later Republic were less equivocal; both Sulla and Pompey claimed special relationships with Venus. Julius Caesar went further, and claimed her as his ancestress. Such claims suggested personal character and policy as divinely inspired; an appointment to priesthood offered divine validation. In 63 BC, Julius Caesar's appointment as pontifex maximus "signaled his emergence as a major player in Roman politics". Likewise, political candidates could sponsor temples, priesthoods and the immensely popular, spectacular public ludi and munera whose provision became increasingly indispensable to the factional politics of the Late Republic. Under the principate, such opportunities were limited by law; priestly and political power were consolidated in the person of the princeps ("first citizen").
By the end of the regal period Rome had developed into a city-state, with a large plebeian, artisan class excluded from the old patrician gentes and from the state priesthoods. The city had commercial and political treaties with its neighbours; according to tradition, Rome's Etruscan connections established a temple to Minerva on the predominantly plebeian Aventine; she became part of a new Capitoline triad of Jupiter, Juno and Minerva, installed in a Capitoline temple, built in an Etruscan style and dedicated in a new September festival, Epulum Jovis. These are supposedly the first Roman deities whose images were adorned, as if noble guests, at their own inaugural banquet.
Rome's diplomatic agreement with her neighbours of Latium confirmed the Latin league and brought the cult of Diana from Aricia to the Aventine. and established on the Aventine in the "commune Latinorum Dianae templum": At about the same time, the temple of Jupiter Latiaris was built on the Alban mount, its stylistic resemblance to the new Capitoline temple pointing to Rome's inclusive hegemony. Rome's affinity to the Latins allowed two Latin cults within the pomoerium: and the cult to Hercules at the ara maxima in the Forum Boarium was established through commercial connections with Tibur. and the Tusculan cult of Castor as the patron of cavalry found a home close to the Forum Romanum: Juno Sospita and Juno Regina were brought from Italy, and Fortuna Primigenia from Praeneste. In 217, Venus was brought from Sicily and installed in a temple on the Capitoline hill.
The introduction of new or equivalent deities coincided with Rome's most significant aggressive and defensive military forays. In 206 BC the Sibylline books commended the introduction of cult to the aniconic Magna Mater (Great Mother) from Pessinus, installed on the Palatine in 191 BC. The mystery cult to Bacchus followed; it was suppressed as subversive and unruly by decree of the Senate in 186 BC. Greek deities were brought within the sacred pomerium: temples were dedicated to Juventas (Hebe) in 191 BC, Diana (Artemis) in 179 BC, Mars (Ares) in 138 BC), and to Bona Dea, equivalent to Fauna, the female counterpart of the rural Faunus, supplemented by the Greek goddess Damia. Further Greek influences on cult images and types represented the Roman Penates as forms of the Greek Dioscuri. The military-political adventurers of the Later Republic introduced the Phrygian goddess Ma (identified with Roman Bellona, the Egyptian mystery-goddess Isis and Persian Mithras.)
The spread of Greek literature, mythology and philosophy offered Roman poets and antiquarians a model for the interpretation of Rome's festivals and rituals, and the embellishment of its mythology. Ennius translated the work of Graeco-Sicilian Euhemerus, who explained the genesis of the gods as apotheosized mortals. In the last century of the Republic, Epicurean and particularly Stoic interpretations were a preoccupation of the literate elite, most of whom held - or had held - high office and traditional Roman priesthoods; notably, Scaevola and the polymath Varro. For Varro - well versed in Euhemerus' theory - popular religious observance was based on a necessary fiction; what the people believed was not itself the truth, but their observance led them to as much higher truth as their limited capacity could deal with. Whereas in popular belief deities held power over mortal lives, the skeptic might say that mortal devotion had made gods of mortals, and these same gods were only sustained by devotion and cult.
Just as Rome itself claimed the favour of the gods, so did some individual Romans. In the mid-to-late Republican era, and probably much earlier, many of Rome's leading clans acknowledged a divine or semi-divine ancestor and laid personal claim to their favour and cult, along with a share of their divinity. Most notably in the very late Republic, the Julii claimed Venus Genetrix as ancestor; this would be one of many foundations for the Imperial cult. The claim was further elaborated and justified in Vergil's poetic, Imperial vision of the past.
Towards the end of the Republic, religious and political offices became more closely intertwined; the office of pontifex maximus became a de facto consular prerogative. Augustus was personally vested with an extraordinary breadth of political, military and priestly powers; at first temporarily, then for his lifetime. He acquired or was granted an unprecedented number of Rome's major priesthoods, including that of pontifex maximus; as he invented none, he could claim them as traditional honours. His reforms were represented as adaptive, restorative and regulatory, rather than innovative; most notably his elevation (and membership) of the ancient Arvales, his timely promotion of the plebeian Compitalia shortly before his election and his patronage of the Vestals as a visible restoration of Roman morality. Augustus obtained the pax deorum, maintained it for the rest of his reign and adopted a successor to ensure its continuation. This remained a primary religious and social duty of emperors.
The Roman Empire expanded to include different peoples and cultures; in principle, Rome followed the same inclusionist policies that had recognised Latin, Etruscan and other Italian peoples, cults and deities as Roman. Those who acknowledged Rome's hegemony retained their own cult and religious calendars, independent of Roman religious law. Newly municipal Sabratha built a Capitolium near its existing temple to Liber Pater and Serapis. Autonomy and concord were official policy, but new foundations by Roman citizens or their Romanised allies were likely to follow Roman cultic models. Romanisation offered distinct political and practical advantages, especially to local elites. All the known effigies from the 2nd century AD forum at Cuicul are of emperors or Concordia. By the middle of the 1st century AD, Gaulish Vertault seems to have abandoned its native cultic sacrifice of horses and dogs in favour of a newly established, Romanised cult nearby: by the end of that century, Sabratha’s so-called tophet was no longer in use. Colonial and later Imperial provincial dedications to Rome's Capitoline Triad were a logical choice, not a centralised legal requirement. Major cult centres to "non-Roman" deities continued to prosper: notable examples include the magnificent Alexandrian Serapium, the temple of Aesculapeus at Pergamum and Apollo's sacred wood at Antioch.
Military settlement within the empire and at its borders broadened the context of Romanitas. Rome's citizen-soldiers set up altars to multiple deities, including their traditional gods, the Imperial genius and local deities – sometimes with the usefully open-ended dedication to the diis deabusque omnibus (all the gods and goddesses). They also brought Roman "domestic" deities and cult practices with them. By the same token, the later granting of citizenship to provincials and their conscription into the legions brought their new cults into the Roman military.
The first and last Roman known as a living divus was Julius Caesar, who seems to have aspired to divine monarchy; he was murdered soon after. Greek allies had their own traditional cults to rulers as divine benefactors, and offered similar cult to Caesar's successor, Augustus, who accepted with the cautious proviso that expatriate Roman citizens refrain from such worship; it might prove fatal. By the end of his reign, Augustus had appropriated Rome's political apparatus – and most of its religious cults – within his "reformed" and thoroughly integrated system of government. Towards the end of his life, he cautiously allowed cult to his numen. By then the Imperial cult apparatus was fully developed, first in the Eastern Provinces, then in the West. Provincial Cult centres offered the amenities and opportunities of a major Roman town within a local context; bathhouses, shrines and temples to Roman and local deities, amphitheatres and festivals. In the early Imperial period, the promotion of local elites to Imperial priesthood gave them Roman citizenship.
In Rome, state cult to a living emperor acknowledged his rule as divinely approved and constitutional. As princeps (first citizen) he must respect traditional Republican mores; given virtually monarchic powers, he must restrain them. He was not a living divus but father of his country (pater patriae), its pontifex maximus (greatest priest) and at least notionally, its leading Republican. When he died, his ascent to heaven, or his descent to join the dii manes was decided by a vote in the Senate. As a divus, he could receive much the same honours as any other state deity – libations of wine, garlands, incense, hymns and sacrificial oxen at games and festivals. What he did in return for these favours is unknown, but literary hints and the later adoption of divus as a title for Christian Saints suggest him as a heavenly intercessor. In Rome, official cult to a living emperor was directed to his genius; a small number refused this honour and there is no evidence of any emperor receiving more than that. In the crises leading up to the Dominate, Imperial titles and honours multiplied, reaching a peak under Diocletian. Emperors before him had attempted to guarantee traditional cults as the core of Roman identity and well-being; refusal of cult undermined the state and was treasonous.
For at least a century before the establishment of the Augustan principate, Jews and Judaism were tolerated in Rome by diplomatic treaty with Judaea's Hellenised elite. Diaspora Jews had much in common with the overwhelmingly Hellenic or Hellenised communities that surrounded them. Early Italian synagogues have left few traces; but one was dedicated in Ostia around the mid-1st century BC and several more are attested during the Imperial period. Judaea's enrollment as a client kingdom in 63 BC increased the Jewish diaspora; in Rome, this led to closer official scrutiny of their religion. Their synagogues were recognised as legitimate collegia by Julius Caesar. By the Augustan era, the city of Rome was home to several thousand Jews. In some periods under Roman rule, Jews were legally exempt from official sacrifice, under certain conditions. Judaism was a superstitio to Cicero, but the Church Father Tertullian described it as religio licita (an officially permitted religion) in contrast to Christianity.
After the Great Fire of Rome in 64 AD, Emperor Nero accused the Christians as convenient scapegoats, who were later persecuted and killed. From that point on, Roman official policy towards Christianity tended towards persecution. During the various Imperial crises of the 3rd century, “contemporaries were predisposed to decode any crisis in religious terms”, regardless of their allegiance to particular practices or belief systems. Christianity drew its traditional base of support from the powerless, who seemed to have no religious stake in the well-being of the Roman State, and therefore threatened its existence. The majority of Rome’s elite continued to observe various forms of inclusive Hellenistic monism; Neoplatonism in particular accommodated the miraculous and the ascetic within a traditional Graeco-Roman cultic framework. Christians saw these ungodly practices as a primary cause of economic and political crisis.
In the wake of religious riots in Egypt, the emperor Decius decreed that all subjects of the Empire must actively seek to benefit the state through witnessed and certified sacrifice to "ancestral gods" or suffer a penalty: only Jews were exempt. Decius' edict appealed to whatever common mos maiores might reunite a politically and socially fractured Empire and its multitude of cults; no ancestral gods were specified by name. The fulfillment of sacrificial obligation by loyal subjects would define them and their gods as Roman. Roman oaths of loyalty were traditionally collective; the Decian oath has been interpreted as a design to root out individual subversives and suppress their cults, but apostasy was sought, rather than capital punishment. A year after its due deadline, the edict expired.
Valerian's first religious edict singled out Christianity as a particularly self-interested and subversive foreign cult, outlawed its assemblies and urged Christians to sacrifice to Rome's traditional gods. His second edict acknowledged a Christian threat to the Imperial system – not yet at its heart but close to it, among Rome’s equites and Senators. Christian apologists interpreted his disgraceful capture and death as divine judgement. The next forty years were peaceful; the Christian church grew stronger and its literature and theology gained a higher social and intellectual profile, due in part to its own search for political toleration and theological coherence. Origen discussed theological issues with traditionalist elites in a common Neoplatonist frame of reference – he had written to Decius' predecessor Philip the Arab in similar vein – and Hippolytus recognised a “pagan” basis in Christian heresies. The Christian churches were disunited; Paul of Samosata, Bishop of Antioch was deposed by a synod of 268 for "dogmatic reasons – his doctrine on the human nature of Christ was rejected – and for his lifestyle, which reminded his brethren of the habits of the administrative elite". The reasons for his deposition were widely circulated among the churches. Meanwhile, Aurelian (270-75) appealed for harmony among his soldiers (concordia militum), stabilised the Empire and its borders and successfully established an official, Hellenic form of unitary cult to the Palmyrene Sol Invictus in Rome's Campus Martius.
In 295, a certain Maximilian refused military service; in 298 Marcellus renounced his military oath. Both were executed for treason; both were Christians. At some time around 302, a report of ominous haruspicy in Diocletian's domus and a subsequent (but undated) dictat of placatory sacrifice by the entire military triggered a series of edicts against Christianity. The first (303 AD) "ordered the destruction of church buildings and Christian texts, forbade services to be held, degraded ofﬁcials who were Christians, re-enslaved imperial freedmen who were Christians, and reduced the legal rights of all Christians... [Physical] or capital punishments were not imposed on them" but soon after, several Christians suspected of attempted arson in the palace were executed. The second edict threatened Christian priests with imprisonment and the third offered them freedom if they performed sacrifice. An edict of 304 enjoined universal sacrifice to traditional gods, in terms that recall the Decian edict.
In some cases and in some places the edicts were strictly enforced: some Christians resisted and were imprisoned or martyred. Others complied. Some local communities were not only pre-dominantly Christian, but powerful and influential; and some provincial authorities were lenient, notably the Caesar in Gaul, Constantius Chlorus, the father of Constantine I. Diocletian's successor Galerius maintained anti-Christian policy until his deathbed revocation in 311, when he asked Christians to pray for him. "This meant an ofﬁcial recognition of their importance in the religious world of the Roman empire, although one of the tetrarchs, Maximinus Daia, still oppressed Christians in his part of the empire up to 313."
With the abatement of persecution, St. Jerome acknowledged the Empire as a bulwark against evil but insisted that "imperial honours" were contrary to Christian teaching. His was an authoritative but minority voice: most Christians showed no qualms in the veneration of even "pagan" emperors. The peace of the emperors was the peace of God; as far as the Church was concerned, internal dissent and doctrinal schism were a far greater problem. The solution came from a hitherto unlikely source: as pontifex maximus Constantine I favoured the "Catholic Church of the Christians" against the Donatists because:
Constantine successfully balanced his own role as an instrument of the pax deorum with the power of the Christian priesthoods in determining what was (in traditional Roman terms) auspicious - or in Christian terms, what was orthodox. The edict of Milan (313) redefined Imperial ideology as one of mutual toleration. Constantine had triumphed under the signum (sign) of the Christ: Christianity was therefore officially embraced along with traditional religions and from his new Eastern capital, Constantine could be seen to embody both Christian and Hellenic religious interests. He may have officially ended – or attempted to end – blood sacrifices to the genius of living emperors but his Imperial iconography and court ceremonial outstripped Diocletian's in their supra-human elevation of the Imperial hierarch. His later direct intervention in Church affairs proved a political masterstroke. Constantine united the empire as an absolute head of state, and on his death, he was honored as a Christian, Imperial, and "divus".
At the time, there were many varying opinions about Christian doctrine, and no centralized way of enforcing orthodoxy. Constantine called all the Christian bishops throughout the Roman Empire to a meeting, and some 318 bishops (very few from the Western Empire) attended the First Council of Nicaea. The purpose of this meeting was to define Christian orthodoxy and clearly differentiate it from Christian heresies. The meeting reached consensus on the Nicene Creed and other statements. Later, Philostorgius criticized Christians who offered sacrifice at statues of the divus Constantine. Constantine nevertheless took great pains to assuage traditionalist and Christian anxieties.
The emperor Julian made a short-lived attempt to revive traditional and Hellenistic religion and to affirm the special status of Judaism, but in 380 under Theodosius I, Nicene Christianity became the official state religion of the Roman Empire. Pleas for religious tolerance from traditionalists such as the senator Symmachus (d. 402) were rejected. Christianity became increasingly popular. Heretics as well as non-Christians were subject to exclusion from public life or persecution, but Rome's original religious hierarchy and many aspects of its ritual influenced Christian forms, and many pre-Christian beliefs and practices survived in Christian festivals and local traditions.
Constantine's nephew Julian rejected the "Galilean madness" of his upbringing for an idiosyncratic synthesis of neo-Platonism, Stoic asceticism and universal solar cult. Julian became Augustus in 361 and actively but vainly fostered a religious and cultural pluralism, attempting a restitution of non-Christian practices and rights. He proposed the rebuilding of Jerusalem's temple as an Imperial project and argued against the "irrational impieties" of Christian doctrine. His attempt to restore an Augustan form of principate, with himself as primus inter pares ended with his death in 363 in Persia, after which his reforms were reversed or abandoned. The empire once again fell under Christian control, this time permanently.
The Western emperor Gratian refused the office of pontifex maximus, and against the protests of the senate, removed the altar of Victory from the senate house and began the disestablishment of the Vestals. Theodosius I briefly re-united the Empire: in 391 he officially adopted Nicene Christianity as the Imperial religion and ended official support for all other creeds and cults. He not only refused to restore Victory to the senate-house, but extinguished the Sacred fire of the Vestals and vacated their temple: the senatorial protest was expressed in a letter by Quintus Aurelius Symmachus to the Western and Eastern emperors. Ambrose, the influential Bishop of Milan and future saint, wrote urging the rejection of Symmachus's request for tolerance. Yet Theodosius accepted comparison with Hercules and Jupiter as a living divinity in the panegyric of Pacatus, and despite his active dismantling of Rome's traditional cults and priesthoods could commend his heirs to its overwhelmingly Hellenic senate in traditional Hellenic terms.[clarification needed] He was the last emperor of both East and West.
Hunting is the practice of killing or trapping any animal, or pursuing or tracking it with the intent of doing so. Hunting wildlife or feral animals is most commonly done by humans for food, recreation, to remove predators which are dangerous to humans or domestic animals, or for trade. In the 2010s, lawful hunting is distinguished from poaching, which is the illegal killing, trapping or capture of the hunted species. The species that are hunted are referred to as game or prey and are usually mammals and birds.
Furthermore, evidence exists that hunting may have been one of the multiple environmental factors leading to extinctions of the holocene megafauna and their replacement by smaller herbivores. North American megafauna extinction was coincidental with the Younger Dryas impact event, possibly making hunting a less critical factor in prehistoric species loss than had been previously thought. However, in other locations such as Australia, humans are thought to have played a very significant role in the extinction of the Australian megafauna that was widespread prior to human occupation.
While it is undisputed that early humans were hunters, the importance of this for the emergence of the Homo genus from the earlier Australopithecines, including the production of stone tools and eventually the control of fire, are emphasised in the hunting hypothesis and de-emphasised in scenarios that stress omnivory and social interaction, including mating behaviour, as essential in the emergence of human behavioural modernity. With the establishment of language, culture, and religion, hunting became a theme of stories and myths, as well as rituals such as dance and animal sacrifice.
Hunter-gathering lifestyles remained prevalent in some parts of the New World, Sub-Saharan Africa, and Siberia, as well as all of Australia, until the European Age of Discovery. They still persist in some tribal societies, albeit in rapid decline. Peoples that preserved paleolithic hunting-gathering until the recent past include some indigenous peoples of the Amazonas (Aché), some Central and Southern African (San people), some peoples of New Guinea (Fayu), the Mlabri of Thailand and Laos, the Vedda people of Sri Lanka, and a handful of uncontacted peoples. In Africa, the only remaining full-time hunter-gatherers are the Hadza of Tanzania.[citation needed]
Archaeologist Louis Binford criticised the idea that early hominids and early humans were hunters. On the basis of the analysis of the skeletal remains of the consumed animals, he concluded that hominids and early humans were mostly scavengers, not hunters, and this idea is popular among some archaeologists and paleoanthropologists. Robert Blumenschine proposed the idea of confrontational scavenging, which involves challenging and scaring off other predators after they have made a kill, which he suggests could have been the leading method of obtaining protein-rich meat by early humans.
Even as animal domestication became relatively widespread and after the development of agriculture, hunting was usually a significant contributor to the human food supply. The supplementary meat and materials from hunting included protein, bone for implements, sinew for cordage, fur, feathers, rawhide and leather used in clothing. Man's earliest hunting weapons would have included rocks, spears, the atlatl, and bows and arrows. Hunting is still vital in marginal climates, especially those unsuited for pastoral uses or agriculture.[citation needed] For example, Inuit people in the Arctic trap and hunt animals for clothing and use the skins of sea mammals to make kayaks, clothing, and footwear.
On ancient reliefs, especially from Mesopotamia, kings are often depicted as hunters of big game such as lions and are often portrayed hunting from a war chariot. The cultural and psychological importance of hunting in ancient societies is represented by deities such as the horned god Cernunnos and lunar goddesses of classical antiquity, the Greek Artemis or Roman Diana. Taboos are often related to hunting, and mythological association of prey species with a divinity could be reflected in hunting restrictions such as a reserve surrounding a temple. Euripides' tale of Artemis and Actaeon, for example, may be seen as a caution against disrespect of prey or impudent boasting.
In most parts of medieval Europe, the upper class obtained the sole rights to hunt in certain areas of a feudal territory. Game in these areas was used as a source of food and furs, often provided via professional huntsmen, but it was also expected to provide a form of recreation for the aristocracy. The importance of this proprietary view of game can be seen in the Robin Hood legends, in which one of the primary charges against the outlaws is that they "hunt the King's deer". In contrast, settlers in Anglophone colonies gloried democratically in hunting for all.
Hindu scriptures describe hunting as an acceptable occupation, as well as a sport of the kingly. Even figures considered godly are described to have engaged in hunting. One of the names of the god Shiva is Mrigavyadha, which translates as "the deer hunter" (mriga means deer; vyadha means hunter). The word Mriga, in many Indian languages including Malayalam, not only stands for deer, but for all animals and animal instincts (Mriga Thrishna). Shiva, as Mrigavyadha, is the one who destroys the animal instincts in human beings. In the epic Ramayana, Dasharatha, the father of Rama, is said to have the ability to hunt in the dark. During one of his hunting expeditions, he accidentally killed Shravana, mistaking him for game. During Rama's exile in the forest, Ravana kidnapped his wife, Sita, from their hut, while Rama was asked by Sita to capture a golden deer, and his brother Lakshman went after him. According to the Mahabharat, Pandu, the father of the Pandavas, accidentally killed the sage Kindama and his wife with an arrow, mistaking them for a deer. Krishna is said to have died after being accidentally wounded by an arrow of a hunter.
From early Christian times, hunting has been forbidden to Roman Catholic Church clerics. Thus the Corpus Juris Canonici (C. ii, X, De cleric. venat.) says, "We forbid to all servants of God hunting and expeditions through the woods with hounds; and we also forbid them to keep hawks or falcons." The Fourth Council of the Lateran, held under Pope Innocent III, decreed (canon xv): "We interdict hunting or hawking to all clerics." The decree of the Council of Trent is worded more mildly: "Let clerics abstain from illicit hunting and hawking" (Sess. XXIV, De reform., c. xii), which seems to imply that not all hunting is illicit, and canonists generally make a distinction declaring noisy (clamorosa) hunting unlawful, but not quiet (quieta) hunting.
Nevertheless, although a distinction between lawful and unlawful hunting is undoubtedly permissible, it is certain that a bishop can absolutely prohibit all hunting to the clerics of his diocese, as was done by synods at Milan, Avignon, Liège, Cologne, and elsewhere. Benedict XIV (De synodo diœces., l. II, c. x) declared that such synodal decrees are not too severe, as an absolute prohibition of hunting is more conformable to the ecclesiastical law. In practice, therefore, the synodal statutes of various localities must be consulted to discover whether they allow quiet hunting or prohibit it altogether.
New Zealand has a strong hunting culture. The islands making up New Zealand originally had no land mammals apart from bats. However, once Europeans arrived, game animals were introduced by acclimatisation societies to provide New Zealanders with sport and a hunting resource. Deer, pigs, goats, rabbits, hare, tahr and chamois all adapted well to the New Zealand terrain, and with no natural predators, their population exploded. Government agencies view the animals as pests due to their effects on the natural environment and on agricultural production, but hunters view them as a resource.
During the feudal and colonial times in British India, hunting was regarded as a regal sport in the numerous princely states, as many maharajas and nawabs, as well as British officers, maintained a whole corps of shikaris (big-game hunters), who were native professional hunters. They would be headed by a master of the hunt, who might be styled mir-shikar. Often, they recruited the normally low-ranking local tribes because of their traditional knowledge of the environment and hunting techniques. Big game, such as Bengal tigers, might be hunted from the back of an elephant.
Regional social norms are generally antagonistic to hunting, while a few sects, such as the Bishnoi, lay special emphasis on the conservation of particular species, such as the antelope. India's Wildlife Protection Act of 1972 bans the killing of all wild animals. However, the Chief Wildlife Warden may, if satisfied that any wild animal from a specified list has become dangerous to human life, or is so disabled or diseased as to be beyond recovery, permit any person to hunt such an animal. In this case, the body of any wild animal killed or wounded becomes government property.
Unarmed fox hunting on horseback with hounds is the type of hunting most closely associated with the United Kingdom; in fact, "hunting" without qualification implies fox hunting. What in other countries is called "hunting" is called "shooting" (birds) or "stalking" (deer) in Britain. Originally a form of vermin control to protect livestock, fox hunting became a popular social activity for newly wealthy upper classes in Victorian times and a traditional rural activity for riders and foot followers alike. Similar to fox hunting in many ways is the chasing of hares with hounds. Pairs of Sight hounds (or long-dogs), such as greyhounds, may be used to pursue a hare in coursing, where the greyhounds are marked as to their skill in coursing the hare (but are not intended to actually catch it), or the hare may be pursued with scent hounds such as beagles or harriers. Other sorts of foxhounds may also be used for hunting stags (deer) or mink. Deer stalking with rifles is carried out on foot without hounds, using stealth.
Shooting as practised in Britain, as opposed to traditional hunting, requires little questing for game—around thirty-five million birds are released onto shooting estates every year, some having been factory farmed. Shoots can be elaborate affairs with guns placed in assigned positions and assistants to help load shotguns. When in position, "beaters" move through the areas of cover, swinging sticks or flags to drive the game out. Such events are often called "drives". The open season for grouse in the UK begins on 12 August, the so-called Glorious Twelfth. The definition of game in the United Kingdom is governed by the Game Act 1831.
Hunting is primarily regulated by state law; additional regulations are imposed through United States environmental law in the case of migratory birds and endangered species. Regulations vary widely from state to state and govern the areas, time periods, techniques and methods by which specific game animals may be hunted. Some states make a distinction between protected species and unprotected species (often vermin or varmints for which there are no hunting regulations). Hunters of protected species require a hunting license in all states, for which completion of a hunting safety course is sometimes a prerequisite.
Hunting big game typically requires a "tag" for each animal harvested. Tags must be purchased in addition to the hunting license, and the number of tags issued to an individual is typically limited. In cases where there are more prospective hunters than the quota for that species, tags are usually assigned by lottery. Tags may be further restricted to a specific area, or wildlife management unit. Hunting migratory waterfowl requires a duck stamp from the Fish and Wildlife Service in addition to the appropriate state hunting license.
Gun usage in hunting is typically regulated by game category, area within the state, and time period. Regulations for big-game hunting often specify a minimum caliber or muzzle energy for firearms. The use of rifles is often banned for safety reasons in areas with high population densities or limited topographic relief. Regulations may also limit or ban the use of lead in ammunition because of environmental concerns. Specific seasons for bow hunting or muzzle-loading black-powder guns are often established to limit competition with hunters using more effective weapons.
Hunting in the United States is not associated with any particular class or culture; a 2006 poll showed seventy-eight percent of Americans supported legal hunting, although relatively few Americans actually hunt. At the beginning of the 21st century, just six percent of Americans hunted. Southerners in states along the eastern seaboard hunted at a rate of five percent, slightly below the national average, and while hunting was more common in other parts of the South at nine percent, these rates did not surpass those of the Plains states, where twelve percent of Midwesterners hunted. Hunting in other areas of the country fell below the national average. Overall, in the 1996–2006 period, the number of hunters over the age of sixteen declined by ten percent, a drop attributable to a number of factors including habitat loss and changes in recreation habits.
Regulation of hunting within the United States dates from the 19th century. Some modern hunters see themselves as conservationists and sportsmen in the mode of Theodore Roosevelt and the Boone and Crockett Club. Local hunting clubs and national organizations provide hunter education and help protect the future of the sport by buying land for future hunting use. Some groups represent a specific hunting interest, such as Ducks Unlimited, Pheasants Forever, or the Delta Waterfowl Foundation. Many hunting groups also participate in lobbying the federal government and state government.
Each year, nearly $200 million in hunters' federal excise taxes are distributed to state agencies to support wildlife management programs, the purchase of lands open to hunters, and hunter education and safety classes. Since 1934, the sale of Federal Duck Stamps, a required purchase for migratory waterfowl hunters over sixteen years old, has raised over $700 million to help purchase more than 5,200,000 acres (8,100 sq mi; 21,000 km2) of habitat for the National Wildlife Refuge System lands that support waterfowl and many other wildlife species and are often open to hunting. States also collect money from hunting licenses to assist with management of game animals, as designated by law. A key task of federal and state park rangers and game wardens is to enforce laws and regulations related to hunting, including species protection, hunting seasons, and hunting bans.
Varmint hunting is an American phrase for the selective killing of non-game animals seen as pests. While not always an efficient form of pest control, varmint hunting achieves selective control of pests while providing recreation and is much less regulated. Varmint species are often responsible for detrimental effects on crops, livestock, landscaping, infrastructure, and pets. Some animals, such as wild rabbits or squirrels, may be utilised for fur or meat, but often no use is made of the carcass. Which species are varmints depends on the circumstance and area. Common varmints may include various rodents, coyotes, crows, foxes, feral cats, and feral hogs. Some animals once considered varmints are now protected, such as wolves. In the US state of Louisiana, a non-native rodent known as a nutria has become so destructive to the local ecosystem that the state has initiated a bounty program to help control the population.
When Internet hunting was introduced in 2005, allowing people to hunt over the Internet using remotely controlled guns, the practice was widely criticised by hunters as violating the principles of fair chase. As a representative of the National Rifle Association (NRA) explained, "The NRA has always maintained that fair chase, being in the field with your firearm or bow, is an important element of hunting tradition. Sitting at your desk in front of your computer, clicking at a mouse, has nothing to do with hunting."
There is a very active tradition of hunting of small to medium-sized wild game in Trinidad and Tobago. Hunting is carried out with firearms, and aided by the use of hounds, with the illegal use of trap guns, trap cages and snare nets. With approximately 12,000 sport hunters applying for hunting licences in recent years (in a very small country of about the size of the state of Delaware at about 5128 square kilometers and 1.3 million inhabitants), there is some concern that the practice might not be sustainable. In addition there are at present no bag limits and the open season is comparatively very long (5 months - October to February inclusive). As such hunting pressure from legal hunters is very high. Added to that, there is a thriving and very lucrative black market for poached wild game (sold and enthusiastically purchased as expensive luxury delicacies) and the numbers of commercial poachers in operation is unknown but presumed to be fairly high. As a result, the populations of the five major mammalian game species (red-rumped agouti, lowland paca, nine-banded armadillo, collared peccary, and red brocket deer) are thought to be quite low (although scientifically conducted population studies are only just recently being conducted as of 2013). It appears that the red brocket deer population has been extirpated on Tobago as a result of over-hunting. Various herons, ducks, doves, the green iguana, the gold tegu, the spectacled caiman and the common opossum are also commonly hunted and poached. There is also some poaching of 'fully protected species', including red howler monkeys and capuchin monkeys, southern tamanduas, Brazilian porcupines, yellow-footed tortoises, Trinidad piping guans and even one of the national birds, the scarlet ibis. Legal hunters pay very small fees to obtain hunting licences and undergo no official basic conservation biology or hunting-ethics training. There is presumed to be relatively very little subsistence hunting in the country (with most hunting for either sport or commercial profit). The local wildlife management authority is under-staffed and under-funded, and as such very little in the way of enforcement is done to uphold existing wildlife management laws, with hunting occurring both in and out of season, and even in wildlife sanctuaries. There is some indication that the government is beginning to take the issue of wildlife management more seriously, with well drafted legislation being brought before Parliament in 2015. It remains to be seen if the drafted legislation will be fully adopted and financially supported by the current and future governments, and if the general populace will move towards a greater awareness of the importance of wildlife conservation and change the culture of wanton consumption to one of sustainable management.
Hunting is claimed to give resource managers an important tool in managing populations that might exceed the carrying capacity of their habitat and threaten the well-being of other species, or, in some instances, damage human health or safety.[citation needed] However, in most circumstances carrying capacity is determined by a combination habitat and food availability, and hunting for 'population control' has no effect on the annual population of species.[citation needed] In some cases, it can increase the population of predators such as coyotes by removing territorial bounds that would otherwise be established, resulting in excess neighbouring migrations into an area, thus artificially increasing the population. Hunting advocates[who?] assert that hunting reduces intraspecific competition for food and shelter, reducing mortality among the remaining animals. Some environmentalists assert[who?] that (re)introducing predators would achieve the same end with greater efficiency and less negative effect, such as introducing significant amounts of free lead into the environment and food chain.
In the 19th century, southern and central European sport hunters often pursued game only for a trophy, usually the head or pelt of an animal, which was then displayed as a sign of prowess. The rest of the animal was typically discarded. Some cultures, however, disapprove of such waste. In Nordic countries, hunting for trophies was—and still is—frowned upon. Hunting in North America in the 19th century was done primarily as a way to supplement food supplies, although it is now undertaken mainly for sport.[citation needed] The safari method of hunting was a development of sport hunting that saw elaborate travel in Africa, India and other places in pursuit of trophies. In modern times, trophy hunting persists and is a significant industry in some areas.[citation needed]
A scientific study in the journal, Biological Conservation, states that trophy hunting is of "major importance to conservation in Africa by creating economic incentives for conservation over vast areas, including areas which may be unsuitable for alternative wildlife-based land uses such as photographic ecotourism." However, another study states that less than 3% of a trophy hunters' expenditures reach the local level, meaning that the economic incentive and benefit is "minimal, particularly when we consider the vast areas of land that hunting concessions occupy."
A variety of industries benefit from hunting and support hunting on economic grounds. In Tanzania, it is estimated that a safari hunter spends fifty to one hundred times that of the average ecotourist. While the average photo tourist may seek luxury accommodation, the average safari hunter generally stays in tented camps. Safari hunters are also more likely to use remote areas, uninviting to the typical ecotourist. Advocates argue that these hunters allow for anti-poaching activities and revenue for local communities.[citation needed]
Hunting also has a significant financial impact in the United States, with many companies specialising in hunting equipment or speciality tourism. Many different technologies have been created to assist hunters, even including iPhone applications. Today's hunters come from a broad range of economic, social, and cultural backgrounds. In 2001, over thirteen million hunters averaged eighteen days hunting, and spent over $20.5 billion on their sport.[citation needed] In the US, proceeds from hunting licenses contribute to state game management programs, including preservation of wildlife habitat.
However, excessive hunting and poachers have also contributed heavily to the endangerment, extirpation and extinction of many animals, such as the quagga, the great auk, Steller's sea cow, the thylacine, the bluebuck, the Arabian oryx, the Caspian and Javan tigers, the markhor, the Sumatran rhinoceros, the bison, the North American cougar, the Altai argali sheep, the Asian elephant and many more, primarily for commercial sale or sport. All these animals have been hunted to endangerment or extinction.
On 16 March 1934, President Franklin D. Roosevelt signed the Migratory Bird Hunting Stamp Act, which requires an annual stamp purchase by all hunters over the age of sixteen. The stamps are created on behalf of the program by the US Postal Service and depict wildlife artwork chosen through an annual contest. They play an important role in habitat conservation because ninety-eight percent of all funds generated by their sale go directly toward the purchase or lease of wetland habitat for protection in the National Wildlife Refuge System.[citation needed] In addition to waterfowl, it is estimated that one third of the nation's endangered species seek food and shelter in areas protected using Duck Stamp funds.[citation needed]
Since 1934, the sale of Federal Duck Stamps has generated $670 million, and helped to purchase or lease 5,200,000 acres (8,100 sq mi; 21,000 km2) of habitat. The stamps serve as a license to hunt migratory birds, an entrance pass for all National Wildlife Refuge areas, and are also considered collectors items often purchased for aesthetic reasons outside of the hunting and birding communities. Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them. Distribution of funds is managed by the Migratory Bird Conservation Commission (MBCC).
The Arabian oryx, a species of large antelope, once inhabited much of the desert areas of the Middle East. However, the species' striking appearance made it (along with the closely related scimitar-horned oryx and addax) a popular quarry for sport hunters, especially foreign executives of oil companies working in the region.[citation needed] The use of automobiles and high-powered rifles destroyed their only advantage: speed, and they became extinct in the wild exclusively due to sport hunting in 1972. The scimitar-horned oryx followed suit, while the addax became critically endangered. However, the Arabian oryx has now made a comeback and been upgraded from “extinct in the wild” to “vulnerable” due to conservation efforts like captive breeding
The American bison is a large bovid which inhabited much of western North America prior to the 1800s, living on the prairies in large herds. However, the vast herds of bison attracted market hunters, who killed dozens of bison for their hides only, leaving the rest to rot. Thousands of these hunters quickly eliminated the bison herds, bringing the population from several million in the early 1800s to a few hundred by the 1880s. Conservation efforts have allowed the population to increase, but the bison remains near-threatened.
In contrast, Botswana has recently been forced to ban trophy hunting following a precipitous wildlife decline. The numbers of antelope plummeted across Botswana, with a resultant decline in predator numbers, while elephant numbers remained stable and hippopotamus numbers rose. According to the government of Botswana, trophy hunting is at least partly to blame for this, but many other factors, such as poaching, drought and habitat loss are also to blame. Uganda recently did the same, arguing that "the share of benefits of sport hunting were lopsided and unlikely to deter poaching or improve [Uganda's] capacity to manage the wildlife reserves."
A psychological identity relates to self-image (one's mental model of oneself), self-esteem, and individuality. Consequently, Weinreich gives the definition "A person's identity is defined as the totality of one's self-construal, in which how one construes oneself in the present expresses the continuity between how one construes oneself as one was in the past and how one construes oneself as one aspires to be in the future"; this allows for definitions of aspects of identity, such as: "One's ethnic identity is defined as that part of the totality of one's self-construal made up of those dimensions that express the continuity between one's construal of past ancestry and one's future aspirations in relation to ethnicity" (Weinreich, 1986a).
The description or representation of individual and group identity is a central task for psychologists, sociologists and anthropologists and those of other disciplines where "identity" needs to be mapped and defined. How should one describe the identity of another, in ways which encompass both their idiosyncratic qualities and their group memberships or identifications, both of which can shift according to circumstance? Following on from the work of Kelly, Erikson, Tajfel and others Weinreich's Identity Structure Analysis (ISA), is "a structural representation of the individual's existential experience, in which the relationships between self and other agents are organised in relatively stable structures over time … with the emphasis on the socio-cultural milieu in which self relates to other agents and institutions" (Weinreich and Saunderson, (eds) 2003, p1). Using constructs drawn from the salient discourses of the individual, the group and cultural norms, the practical operationalisation of ISA provides a methodology that maps how these are used by the individual, applied across time and milieus by the "situated self" to appraise self and other agents and institutions (for example, resulting in the individual's evaluation of self and significant others and institutions).[citation needed]
Weinreich's identity variant similarly includes the categories of identity diffusion, foreclosure and crisis, but with a somewhat different emphasis. Here, with respect to identity diffusion for example, an optimal level is interpreted as the norm, as it is unrealistic to expect an individual to resolve all their conflicted identifications with others; therefore we should be alert to individuals with levels which are much higher or lower than the norm – highly diffused individuals are classified as diffused, and those with low levels as foreclosed or defensive. (Weinreich & Saunderson, 2003, pp 65–67; 105-106). Weinreich applies the identity variant in a framework which also allows for the transition from one to another by way of biographical experiences and resolution of conflicted identifications situated in various contexts – for example, an adolescent going through family break-up may be in one state, whereas later in a stable marriage with a secure professional role may be in another. Hence, though there is continuity, there is also development and change. (Weinreich & Saunderson, 2003, pp 22–23).
Anthropologists have contributed to the debate by shifting the focus of research: One of the first challenges for the researcher wishing to carry out empirical research in this area is to identify an appropriate analytical tool. The concept of boundaries is useful here for demonstrating how identity works. In the same way as Barth, in his approach to ethnicity, advocated the critical focus for investigation as being "the ethnic boundary that defines the group rather than the cultural stuff that it encloses" (1969:15), social anthropologists such as Cohen and Bray have shifted the focus of analytical study from identity to the boundaries that are used for purposes of identification. If identity is a kind of virtual site in which the dynamic processes and markers used for identification are made apparent, boundaries provide the framework on which this virtual site is built. They concentrated on how the idea of community belonging is differently constructed by individual members and how individuals within the group conceive ethnic boundaries.
The inclusiveness of Weinreich's definition (above) directs attention to the totality of one's identity at a given phase in time, and assists in elucidating component aspects of one's total identity, such as one's gender identity, ethnic identity, occupational identity and so on. The definition readily applies to the young child, to the adolescent, to the young adult, and to the older adult in various phases of the life cycle. Depending on whether one is a young child or an adult at the height of one's powers, how one construes oneself as one was in the past will refer to very different salient experiential markers. Likewise, how one construes oneself as one aspires to be in the future will differ considerably according to one's age and accumulated experiences. (Weinreich & Saunderson, (eds) 2003, pp 26–34).
Although the self is distinct from identity, the literature of self-psychology can offer some insight into how identity is maintained (Cote & Levin 2002, p. 24). From the vantage point of self-psychology, there are two areas of interest: the processes by which a self is formed (the "I"), and the actual content of the schemata which compose the self-concept (the "Me"). In the latter field, theorists have shown interest in relating the self-concept to self-esteem, the differences between complex and simple ways of organizing self-knowledge, and the links between those organizing principles and the processing of information (Cote & Levin 2002).
At a general level, self-psychology is compelled to investigate the question of how the personal self relates to the social environment. To the extent that these theories place themselves in the tradition of "psychological" social psychology, they focus on explaining an individual's actions within a group in terms of mental events and states. However, some "sociological" social psychology theories go further by attempting to deal with the issue of identity at both the levels of individual cognition and of collective behavior.
Anthropologists have most frequently employed the term 'identity' to refer to this idea of selfhood in a loosely Eriksonian way (Erikson 1972) properties based on the uniqueness and individuality which makes a person distinct from others. Identity became of more interest to anthropologists with the emergence of modern concerns with ethnicity and social movements in the 1970s. This was reinforced by an appreciation, following the trend in sociological thought, of the manner in which the individual is affected by and contributes to the overall social context. At the same time, the Eriksonian approach to identity remained in force, with the result that identity has continued until recently to be used in a largely socio-historical way to refer to qualities of sameness in relation to a person's connection to others and to a particular group of people.
Boundaries can be inclusive or exclusive depending on how they are perceived by other people. An exclusive boundary arises, for example, when a person adopts a marker that imposes restrictions on the behaviour of others. An inclusive boundary is created, by contrast, by the use of a marker with which other people are ready and able to associate. At the same time, however, an inclusive boundary will also impose restrictions on the people it has included by limiting their inclusion within other boundaries. An example of this is the use of a particular language by a newcomer in a room full of people speaking various languages. Some people may understand the language used by this person while others may not. Those who do not understand it might take the newcomer's use of this particular language merely as a neutral sign of identity. But they might also perceive it as imposing an exclusive boundary that is meant to mark them off from her. On the other hand, those who do understand the newcomer's language could take it as an inclusive boundary, through which the newcomer associates herself with them to the exclusion of the other people present. Equally, however, it is possible that people who do understand the newcomer but who also speak another language may not want to speak the newcomer's language and so see her marker as an imposition and a negative boundary. It is possible that the newcomer is either aware or unaware of this, depending on whether she herself knows other languages or is conscious of the plurilingual quality of the people there and is respectful of it or not.
The "Neo-Eriksonian" identity status paradigm emerged in later years[when?], driven largely by the work of James Marcia. This paradigm focuses upon the twin concepts of exploration and commitment. The central idea is that any individual's sense of identity is determined in large part by the explorations and commitments that he or she makes regarding certain personal and social traits. It follows that the core of the research in this paradigm investigates the degrees to which a person has made certain explorations, and the degree to which he or she displays a commitment to those explorations.
Many people gain a sense of positive self-esteem from their identity groups, which furthers a sense of community and belonging. Another issue that researchers have attempted to address is the question of why people engage in discrimination, i.e., why they tend to favor those they consider a part of their "in-group" over those considered to be outsiders. Both questions have been given extensive attention by researchers working in the social identity tradition. For example, in work relating to social identity theory it has been shown that merely crafting cognitive distinction between in- and out-groups can lead to subtle effects on people's evaluations of others (Cote & Levine 2002).
The first favours a primordialist approach which takes the sense of self and belonging to a collective group as a fixed thing, defined by objective criteria such as common ancestry and common biological characteristics. The second, rooted in social constructionist theory, takes the view that identity is formed by a predominantly political choice of certain characteristics. In so doing, it questions the idea that identity is a natural given, characterised by fixed, supposedly objective criteria. Both approaches need to be understood in their respective political and historical contexts, characterised by debate on issues of class, race and ethnicity. While they have been criticized, they continue to exert an influence on approaches to the conceptualisation of identity today.
The implications are multiple as various research traditions are now[when?] heavily utilizing the lens of identity to examine phenomena.[citation needed] One implication of identity and of identity construction can be seen in occupational settings. This becomes increasing challenging in stigmatized jobs or "dirty work" (Hughes, 1951). Tracy and Trethewey (2005) state that "individuals gravitate toward and turn away from particular jobs depending in part, on the extent to which they validate a "preferred organizational self" (Tracy & Tretheway 2005, p. 169). Some jobs carry different stigmas or acclaims. In her analysis Tracy uses the example of correctional officers trying to shake the stigma of "glorified maids" (Tracy & Tretheway 2005). "The process by which people arrive at justifications of and values for various occupational choices." Among these are workplace satisfaction and overall quality of life (Tracy & Scott 2006, p. 33). People in these types of jobs are forced to find ways in order to create an identity they can live with. "Crafting a positive sense of self at work is more challenging when one's work is considered "dirty" by societal standards" (Tracy & Scott 2006, p. 7). "In other words, doing taint management is not just about allowing the employee to feel good in that job. "If employees must navigate discourses that question the viability of their work, and/ or experience obstacles in managing taint through transforming dirty work into a badge of honor, it is likely they will find blaming the client to be an efficacious route in affirming their identity" (Tracy & Scott 2006, p. 33).
However, the formation of one's identity occurs through one's identifications with significant others (primarily with parents and other individuals during one's biographical experiences, and also with "groups" as they are perceived). These others may be benign - such that one aspires to their characteristics, values and beliefs (a process of idealistic-identification), or malign - when one wishes to dissociate from their characteristics (a process of defensive contra-identification) (Weinreich & Saunderson 2003, Chapter 1, pp 54–61).
A person may display either relative weakness or relative strength in terms of both exploration and commitments. When assigned categories, four possible permutations result: identity diffusion, identity foreclosure, identity moratorium, and identity achievement. Diffusion is when a person lacks both exploration in life and interest in committing even to those unchosen roles that he or she occupies. Foreclosure is when a person has not chosen extensively in the past, but seems willing to commit to some relevant values, goals, or roles in the future. Moratorium is when a person displays a kind of flightiness, ready to make choices but unable to commit to them. Finally, achievement is when a person makes identity choices and commits to them.
These different explorations of 'identity' demonstrate how difficult a concept it is to pin down. Since identity is a virtual thing, it is impossible to define it empirically. Discussions of identity use the term with different meanings, from fundamental and abiding sameness, to fluidity, contingency, negotiated and so on. Brubaker and Cooper note a tendency in many scholars to confuse identity as a category of practice and as a category of analysis (Brubaker & Cooper 2000, p. 5). Indeed, many scholars demonstrate a tendency to follow their own preconceptions of identity, following more or less the frameworks listed above, rather than taking into account the mechanisms by which the concept is crystallised as reality. In this environment, some analysts, such as Brubaker and Cooper, have suggested doing away with the concept completely (Brubaker & Cooper 2000, p. 1). Others, by contrast, have sought to introduce alternative concepts in an attempt to capture the dynamic and fluid qualities of human social self-expression. Hall (1992, 1996), for example, suggests treating identity as a process, to take into account the reality of diverse and ever-changing social experience. Some scholars have introduced the idea of identification, whereby identity is perceived as made up of different components that are 'identified' and interpreted by individuals. The construction of an individual sense of self is achieved by personal choices regarding who and what to associate with. Such approaches are liberating in their recognition of the role of the individual in social interaction and the construction of identity.
Gender identity forms an important part of identity in psychology, as it dictates to a significant degree how one views oneself both as a person and in relation to other people, ideas and nature. Other aspects of identity, such as racial, religious, ethnic, occupational… etc. may also be more or less significant – or significant in some situations but not in others (Weinreich & Saunderson 2003 pp26–34). In cognitive psychology, the term "identity" refers to the capacity for self-reflection and the awareness of self.(Leary & Tangney 2003, p. 3)
Erik Erikson (1902-1994) became one of the earliest psychologists to take an explicit interest in identity. The Eriksonian framework rests upon a distinction among the psychological sense of continuity, known as the ego identity (sometimes identified simply as "the self"); the personal idiosyncrasies that separate one person from the next, known as the personal identity; and the collection of social roles that a person might play, known as either the social identity or the cultural identity. Erikson's work, in the psychodynamic tradition, aimed to investigate the process of identity formation across a lifespan. Progressive strength in the ego identity, for example, can be charted in terms of a series of stages in which identity is formed in response to increasingly sophisticated challenges. The process of forming a viable sense of identity for the culture is conceptualized as an adolescent task, and those who do not manage a resynthesis of childhood identifications are seen as being in a state of 'identity diffusion' whereas those who retain their initially given identities unquestioned have 'foreclosed' identities (Weinreich & Saunderson 2003 p7-8). On some readings of Erikson, the development of a strong ego identity, along with the proper integration into a stable society and culture, lead to a stronger sense of identity in general. Accordingly, a deficiency in either of these factors may increase the chance of an identity crisis or confusion (Cote & Levine 2002, p. 22).
Laing's definition of identity closely follows Erikson's, in emphasising the past, present and future components of the experienced self. He also develops the concept of the "metaperspective of self", i.e. the self's perception of the other's view of self, which has been found to be extremely important in clinical contexts such as anorexia nervosa. (Saunderson and O'Kane, 2005). Harré also conceptualises components of self/identity – the "person" (the unique being I am to myself and others) along with aspects of self (including a totality of attributes including beliefs about one's characteristics including life history), and the personal characteristics displayed to others.
Kenneth Gergen formulated additional classifications, which include the strategic manipulator, the pastiche personality, and the relational self. The strategic manipulator is a person who begins to regard all senses of identity merely as role-playing exercises, and who gradually becomes alienated from his or her social "self". The pastiche personality abandons all aspirations toward a true or "essential" identity, instead viewing social interactions as opportunities to play out, and hence become, the roles they play. Finally, the relational self is a perspective by which persons abandon all sense of exclusive self, and view all sense of identity in terms of social engagement with others. For Gergen, these strategies follow one another in phases, and they are linked to the increase in popularity of postmodern culture and the rise of telecommunications technology.
As a non-directive and flexible analytical tool, the concept of boundaries helps both to map and to define the changeability and mutability that are characteristic of people's experiences of the self in society. While identity is a volatile, flexible and abstract 'thing', its manifestations and the ways in which it is exercised are often open to view. Identity is made evident through the use of markers such as language, dress, behaviour and choice of space, whose effect depends on their recognition by other social beings. Markers help to create the boundaries that define similarities or differences between the marker wearer and the marker perceivers, their effectiveness depends on a shared understanding of their meaning. In a social context, misunderstandings can arise due to a misinterpretation of the significance of specific markers. Equally, an individual can use markers of identity to exert influence on other people without necessarily fulfilling all the criteria that an external observer might typically associate with such an abstract identity.
In Canada, the term "football" may refer to Canadian football and American football collectively, or to either sport specifically, depending on context. The two sports have shared origins and are closely related but have significant differences. In particular, Canadian football has 12 players on the field per team rather than 11; the field is roughly 10 yards wider, and 10 yards longer between end-zones that are themselves 10 yards deeper; and a team has only three downs to gain 10 yards, which results in less offensive rushing than in the American game. In the Canadian game all players on the defending team, when a down begins, must be at least 1 yard from the line of scrimmage. (The American game has a similar "neutral zone" but it is only the length of the football.)
Canadian football is also played at the high school, junior, collegiate, and semi-professional levels: the Canadian Junior Football League, formed May 8, 1974, and Quebec Junior Football League are leagues for players aged 18–22, many post-secondary institutions compete in Canadian Interuniversity Sport for the Vanier Cup, and senior leagues such as the Alberta Football League have grown in popularity in recent years. Great achievements in Canadian football are enshrined in the Canadian Football Hall of Fame.
The first written account of a game played was on October 15, 1862, on the Montreal Cricket Grounds. It was between the First Battalion Grenadier Guards and the Second Battalion Scots Fusilier Guards resulting in a win by the Grenadier Guards 3 goals, 2 rouges to nothing.[citation needed] In 1864, at Trinity College, Toronto, F. Barlow Cumberland, Frederick A. Bethune, and Christopher Gwynn, one of the founders of Milton, Massachusetts, devised rules based on rugby football. The game gradually gained a following, with the Hamilton Football Club formed on November 3, 1869, (the oldest football club in Canada). Montreal formed a team April 8, 1872, Toronto was formed on October 4, 1873, and the Ottawa FBC on September 20, 1876.
The first attempt to establish a proper governing body and adopted the current set of Rugby rules was the Foot Ball Association of Canada, organized on March 24, 1873 followed by the Canadian Rugby Football Union (CRFU) founded June 12, 1880, which included teams from Ontario and Quebec. Later both the Ontario and Quebec Rugby Football Union (ORFU and QRFU) were formed (January 1883), and then the Interprovincial (1907) and Western Interprovincial Football Union (1936) (IRFU and WIFU). The CRFU reorganized into an umbrella organization forming the Canadian Rugby Union (CRU) in 1891. The original forerunners to the current Canadian Football League, was established in 1956 when the IRFU and WIFU formed an umbrella organization, The Canadian Football Council (CFC). And then in 1958 the CFC left The CRFU to become The CFL.
The Burnside rules closely resembling American Football that were incorporated in 1903 by The ORFU, was an effort to distinguish it from a more rugby-oriented game. The Burnside Rules had teams reduced to 12 men per side, introduced the Snap-Back system, required the offensive team to gain 10 yards on three downs, eliminated the Throw-In from the sidelines, allowed only six men on the line, stated that all goals by kicking were to be worth two points and the opposition was to line up 10 yards from the defenders on all kicks. The rules were an attempt to standardize the rules throughout the country. The CIRFU, QRFU and CRU refused to adopt the new rules at first. Forward passes were not allowed in the Canadian game until 1929, and touchdowns, which had been five points, were increased to six points in 1956, in both cases several decades after the Americans had adopted the same changes. The primary differences between the Canadian and American games stem from rule changes that the American side of the border adopted but the Canadian side did not (originally, both sides had three downs, goal posts on the goal lines and unlimited forward motion, but the American side modified these rules and the Canadians did not). The Canadian field width was one rule that was not based on American rules, as the Canadian game played in wider fields and stadiums that were not as narrow as the American stadiums.
The Grey Cup was established in 1909 after being donated by Albert Grey, 4th Earl Grey, The Governor General of Canada as the championship of teams under the CRU for the Rugby Football Championship of Canada. Initially an amateur competition, it eventually became dominated by professional teams in the 1940s and early 1950s. The Ontario Rugby Football Union, the last amateur organization to compete for the trophy, withdrew from competition in 1954. The move ushered in the modern era of Canadian professional football.
Canadian football has mostly been confined to Canada, with the United States being the only other country to have hosted a high-level Canadian football game. The CFL's controversial "South Division" as it would come to be officially known attempted to put CFL teams in the United States playing under Canadian rules between 1992 and 1995. The move was aborted after three years; the Baltimore Stallions were the most successful of the numerous Americans teams to play in the CFL, winning the 83rd Grey Cup. Continuing financial losses, a lack of proper Canadian football venues, a pervasive belief that the American teams were simply pawns to provide the struggling Canadian teams with expansion fee revenue, and the return of the NFL to Baltimore prompted the end of Canadian football on the American side of the border.
Amateur football is governed by Football Canada. At the university level, 26 teams play in four conferences under the auspices of Canadian Interuniversity Sport; the CIS champion is awarded the Vanier Cup. Junior football is played by many after high school before joining the university ranks. There are 20 junior teams in three divisions in the Canadian Junior Football League competing for the Canadian Bowl. The Quebec Junior Football League includes teams from Ontario and Quebec who battle for the Manson Cup.
The Canadian football field is 150 yards (137 m) long and 65 yards (59 m) wide with end zones 20 yards (18 m) deep, and goal lines 110 yards (101 m) apart. At each goal line is a set of 40-foot-high (12 m) goalposts, which consist of two uprights joined by an 18 1⁄2-foot-long (5.6 m) crossbar which is 10 feet (3 m) above the goal line. The goalposts may be H-shaped (both posts fixed in the ground) although in the higher-calibre competitions the tuning-fork design (supported by a single curved post behind the goal line, so that each post starts 10 feet (3 m) above the ground) is preferred. The sides of the field are marked by white sidelines, the goal line is marked in white, and white lines are drawn laterally across the field every 5 yards (4.6 m) from the goal line. These lateral lines are called "yard lines" and often marked with the distance in yards from and an arrow pointed toward the nearest goal line. In previous decades, arrows were not used and every yard line was usually marked with the distance to the goal line, including the goal line itself which was marked with a "0"; in most stadiums today, the 10-, 20-, 30-, 40-, and 50-yard lines are marked with numbers, with the goal line sometimes being marked with a "G". The centre (55-yard) line usually is marked with a "C". "Hash marks" are painted in white, parallel to the yardage lines, at 1 yard (0.9 m) intervals, 24 yards (21.9 m) from the sidelines. On fields that have a surrounding running track, such as Commonwealth Stadium, Molson Stadium, and many universities, the end zones are often cut off in the corners to accommodate the track. In 2014, Edmonton placed turf over the track to create full end zones. This was particularly common among U.S.-based teams during the CFL's American expansion, where few American stadiums were able to accommodate the much longer and noticeably wider CFL field.
At the beginning of a match, an official tosses a coin and allows the captain of the visiting team call heads or tails. The captain of the team winning the coin toss is given the option of having first choice, or of deferring first choice to the other captain. The captain making first choice may either choose a) to kick off or receive the kick and the beginning of the half, or b) which direction of the field to play in. The remaining choice is given to the opposing captain. Before the resumption of play in the second half, the captain that did not have first choice in the first half is given first choice. Teams usually choose to defer, so it is typical for the team that wins the coin toss to kick to begin the first half and receive to begin the second.
Play stops when the ball carrier's knee, elbow, or any other body part aside from the feet and hands, is forced to the ground (a tackle); when a forward pass is not caught on the fly (during a scrimmage); when a touchdown (see below) or a field goal is scored; when the ball leaves the playing area by any means (being carried, thrown, or fumbled out of bounds); or when the ball carrier is in a standing position but can no longer move forwards (called forward progress). If no score has been made, the next play starts from scrimmage.
Before scrimmage, an official places the ball at the spot it was at the stop of clock, but no nearer than 24 yards from the sideline or 1 yard from the goal line. The line parallel to the goal line passing through the ball (line from sideline to sideline for the length of the ball) is referred to as the line of scrimmage. This line is similar to "no-man's land"; players must stay on their respective sides of this line until the play has begun again. For a scrimmage to be valid the team in possession of the football must have seven players, excluding the quarterback, within one yard of the line of scrimmage. The defending team must stay a yard or more back from the line of scrimmage.
On the field at the beginning of a play are two teams of 12 (unlike 11 in American football). The team in possession of the ball is the offence and the team defending is referred to as the defence. Play begins with a backwards pass through the legs (the snap) by a member of the offensive team, to another member of the offensive team. This is usually the quarterback or punter, but a "direct snap" to a running back is also not uncommon. If the quarterback or punter receives the ball, he may then do any of the following:
Each play constitutes a down. The offence must advance the ball at least ten yards towards the opponents' goal line within three downs or forfeit the ball to their opponents. Once ten yards have been gained the offence gains a new set of three downs (rather than the four downs given in American football). Downs do not accumulate. If the offensive team completes 10 yards on their first play, they lose the other two downs and are granted another set of three. If a team fails to gain ten yards in two downs they usually punt the ball on third down or try to kick a field goal (see below), depending on their position on the field. The team may, however use its third down in an attempt to advance the ball and gain a cumulative 10 yards.
There are many rules to contact in this type of football. First, the only player on the field who may be legally tackled is the player currently in possession of the football (the ball carrier). Second, a receiver, that is to say, an offensive player sent down the field to receive a pass, may not be interfered with (have his motion impeded, be blocked, etc.) unless he is within one yard of the line of scrimmage (instead of 5 yards (4.6 m) in American football). Any player may block another player's passage, so long as he does not hold or trip the player he intends to block. The kicker may not be contacted after the kick but before his kicking leg returns to the ground (this rule is not enforced upon a player who has blocked a kick), and the quarterback, having already thrown the ball, may not be hit or tackled.
Infractions of the rules are punished with penalties, typically a loss of yardage of 5, 10 or 15 yards against the penalized team. Minor violations such as offside (a player from either side encroaching into scrimmage zone before the play starts) are penalized five yards, more serious penalties (such as holding) are penalized 10 yards, and severe violations of the rules (such as face-masking) are typically penalized 15 yards. Depending on the penalty, the penalty yardage may be assessed from the original line of scrimmage, from where the violation occurred (for example, for a pass interference infraction), or from where the ball ended after the play. Penalties on the offence may, or may not, result in a loss of down; penalties on the defence may result in a first down being automatically awarded to the offence. For particularly severe conduct, the game official(s) may eject players (ejected players may be substituted for), or in exceptional cases, declare the game over and award victory to one side or the other. Penalties do not affect the yard line which the offence must reach to gain a first down (unless the penalty results in a first down being awarded); if a penalty against the defence results in the first down yardage being attained, then the offence is awarded a first down.
Penalties never result in a score for the offence. For example, a point-of-foul infraction committed by the defence in their end zone is not ruled a touchdown, but instead advances the ball to the one-yard line with an automatic first down. For a distance penalty, if the yardage is greater than half the distance to the goal line, then the ball is advanced half the distance to the goal line, though only up to the one-yard line (unlike American football, in Canadian football no scrimmage may start inside either one-yard line). If the original penalty yardage would have resulted in a first down or moving the ball past the goal line, a first down is awarded.
In most cases, the non-penalized team will have the option of declining the penalty; in which case the results of the previous play stand as if the penalty had not been called. One notable exception to this rule is if the kicking team on a 3rd down punt play is penalized before the kick occurs: the receiving team may not decline the penalty and take over on downs. After the kick is made, change of possession occurs and subsequent penalties are assessed against either the spot where the ball is caught, or the runback.
During the last three minutes of a half, the penalty for failure to place the ball in play within the 20-second play clock, known as "time count" (this foul is known as "delay of game" in American football), is dramatically different from during the first 27 minutes. Instead of the penalty being 5 yards with the down repeated, the base penalty (except during convert attempts) becomes loss of down on first or second down, and 10 yards on third down with the down repeated. In addition, as noted previously, the referee can give possession to the defence for repeated deliberate time count violations on third down.
The clock does not run during convert attempts in the last three minutes of a half. If the 15 minutes of a quarter expire while the ball is live, the quarter is extended until the ball becomes dead. If a quarter's time expires while the ball is dead, the quarter is extended for one more scrimmage. A quarter cannot end while a penalty is pending: after the penalty yardage is applied, the quarter is extended one scrimmage. Note that the non-penalized team has the option to decline any penalty it considers disadvantageous, so a losing team cannot indefinitely prolong a game by repeatedly committing infractions.
In the CFL, if the game is tied at the end of regulation play, then each team is given an equal number of chances to break the tie. A coin toss is held to determine which team will take possession first; the first team scrimmages the ball at the opponent's 35-yard line and advances through a series of downs until it scores or loses possession. If the team scores a touchdown, starting with the 2010 season, it is required to attempt a 2-point conversion. The other team then scrimmages the ball at the same 35-yard line and has the same opportunity to score. After the teams have completed their possessions, if one team is ahead, then it is declared the winner; otherwise, the two teams each get another chance to score, scrimmaging from the other 35-yard line. After this second round, if there is still no winner, during the regular season the game ends as a tie. In a playoff or championship game, the teams continue to attempt to score from alternating 35-yard lines, until one team is leading after both have had an equal number of possessions.
Muslims believe the Quran was verbally revealed by God to Muhammad through the angel Gabriel (Jibril), gradually over a period of approximately 23 years, beginning on 22 December 609 CE, when Muhammad was 40, and concluding in 632, the year of his death. Muslims regard the Quran as the most important miracle of Muhammad, a proof of his prophethood, and the culmination of a series of divine messages that started with the messages revealed to Adam and ended with Muhammad. The word "Quran" occurs some 70 times in the text of the Quran, although different names and words are also said to be references to the Quran.
According to the traditional narrative, several companions of Muhammad served as scribes and were responsible for writing down the revelations. Shortly after Muhammad's death, the Quran was compiled by his companions who wrote down and memorized parts of it. These codices had differences that motivated the Caliph Uthman to establish a standard version now known as Uthman's codex, which is generally considered the archetype of the Quran known today. There are, however, variant readings, with mostly minor differences in meaning.
The Quran assumes familiarity with major narratives recounted in the Biblical scriptures. It summarizes some, dwells at length on others and, in some cases, presents alternative accounts and interpretations of events. The Quran describes itself as a book of guidance. It sometimes offers detailed accounts of specific historical events, and it often emphasizes the moral significance of an event over its narrative sequence. The Quran is used along with the hadith to interpret sharia law. During prayers, the Quran is recited only in Arabic.
The word qurʼān appears about 70 times in the Quran itself, assuming various meanings. It is a verbal noun (maṣdar) of the Arabic verb qaraʼa (قرأ), meaning "he read" or "he recited". The Syriac equivalent is (ܩܪܝܢܐ) qeryānā, which refers to "scripture reading" or "lesson". While some Western scholars consider the word to be derived from the Syriac, the majority of Muslim authorities hold the origin of the word is qaraʼa itself. Regardless, it had become an Arabic term by Muhammad's lifetime. An important meaning of the word is the "act of reciting", as reflected in an early Quranic passage: "It is for Us to collect it and to recite it (qurʼānahu)."
The term also has closely related synonyms that are employed throughout the Quran. Each synonym possesses its own distinct meaning, but its use may converge with that of qurʼān in certain contexts. Such terms include kitāb (book); āyah (sign); and sūrah (scripture). The latter two terms also denote units of revelation. In the large majority of contexts, usually with a definite article (al-), the word is referred to as the "revelation" (waḥy), that which has been "sent down" (tanzīl) at intervals. Other related words are: dhikr (remembrance), used to refer to the Quran in the sense of a reminder and warning, and ḥikmah (wisdom), sometimes referring to the revelation or part of it.
The Quran describes itself as "the discernment" (al-furqān), "the mother book" (umm al-kitāb), "the guide" (huda), "the wisdom" (hikmah), "the remembrance" (dhikr) and "the revelation" (tanzīl; something sent down, signifying the descent of an object from a higher place to lower place). Another term is al-kitāb (The Book), though it is also used in the Arabic language for other scriptures, such as the Torah and the Gospels. The adjective of "Quran" has multiple transliterations including "quranic", "koranic", and "qur'anic", or capitalised as "Qur'anic", "Koranic", and "Quranic". The term mus'haf ('written work') is often used to refer to particular Quranic manuscripts but is also used in the Quran to identify earlier revealed books. Other transliterations of "Quran" include "al-Coran", "Coran", "Kuran", and "al-Qurʼan".
Islamic tradition relates that Muhammad received his first revelation in the Cave of Hira during one of his isolated retreats to the mountains. Thereafter, he received revelations over a period of 23 years. According to hadith and Muslim history, after Muhammad immigrated to Medina and formed an independent Muslim community, he ordered many of his companions to recite the Quran and to learn and teach the laws, which were revealed daily. It is related that some of the Quraysh who were taken prisoners at the battle of Badr regained their freedom after they had taught some of the Muslims the simple writing of the time. Thus a group of Muslims gradually became literate. As it was initially spoken, the Quran was recorded on tablets, bones, and the wide, flat ends of date palm fronds. Most suras were in use amongst early Muslims since they are mentioned in numerous sayings by both Sunni and Shia sources, relating Muhammad's use of the Quran as a call to Islam, the making of prayer and the manner of recitation. However, the Quran did not exist in book form at the time of Muhammad's death in 632. There is agreement among scholars that Muhammad himself did not write down the revelation.
The Quran describes Muhammad as "ummi", which is traditionally interpreted as "illiterate," but the meaning is rather more complex. Medieval commentators such as Al-Tabari maintained that the term induced two meanings: first, the inability to read or write in general; second, the inexperience or ignorance of the previous books or scriptures (but they gave priority to the first meaning). Muhammad's illiteracy was taken as a sign of the genuineness of his prophethood. For example, according to Fakhr al-Din al-Razi, if Muhammad had mastered writing and reading he possibly would have been suspected of having studied the books of the ancestors. Some scholars such as Watt prefer the second meaning of "ummi" - they take it to indicate unfamiliarity with earlier sacred texts.
Based on earlier transmitted reports, in the year 632, after the demise of Muhammad a number of his companions who knew the Quran by heart were killed in a battle by Musaylimah, the first caliph Abu Bakr (d. 634) decided to collect the book in one volume so that it could be preserved. Zayd ibn Thabit (d. 655) was the person to collect the Quran since "he used to write the Divine Inspiration for Allah's Apostle". Thus, a group of scribes, most importantly Zayd, collected the verses and produced a hand-written manuscript of the complete book. The manuscript according to Zayd remained with Abu Bakr until he died. Zayd's reaction to the task and the difficulties in collecting the Quranic material from parchments, palm-leaf stalks, thin stones and from men who knew it by heart is recorded in earlier narratives. After Abu Bakr, Hafsa bint Umar, Muhammad's widow, was entrusted with the manuscript. In about 650, the third Caliph Uthman ibn Affan (d. 656) began noticing slight differences in pronunciation of the Quran as Islam expanded beyond the Arabian Peninsula into Persia, the Levant, and North Africa. In order to preserve the sanctity of the text, he ordered a committee headed by Zayd to use Abu Bakr's copy and prepare a standard copy of the Quran. Thus, within 20 years of Muhammad's death, the Quran was committed to written form. That text became the model from which copies were made and promulgated throughout the urban centers of the Muslim world, and other versions are believed to have been destroyed. The present form of the Quran text is accepted by Muslim scholars to be the original version compiled by Abu Bakr.
In 1972, in a mosque in the city of Sana'a, Yemen, manuscripts were discovered that were later proved to be the most ancient Quranic text known to exist at the time. The Sana'a manuscripts contain palimpsests, a manuscript page from which the text has been washed off to make the parchment reusable again—a practice which was common in ancient times due to scarcity of writing material. However, the faint washed-off underlying text (scriptio inferior) is still barely visible and believed to be "pre-Uthmanic" Quranic content, while the text written on top (scriptio superior) is believed to belong to Uthmanic time. Studies using radiocarbon dating indicate that the parchments are dated to the period before 671 AD with a 99 percent probability.
In 2015, fragments of a very early Quran, dating back to 1370 years ago, were discovered in the library of the University of Birmingham, England. According to the tests carried out by Oxford University Radiocarbon Accelerator Unit, "with a probability of more than 95%, the parchment was from between 568 and 645". The manuscript is written in Hijazi script, an early form of written Arabic. This is possibly the earliest extant exemplar of the Quran, but as the tests allow a range of possible dates, it cannot be said with certainty which of the existing versions is the oldest. Saudi scholar Saud al-Sarhan has expressed doubt over the age of the fragments as they contain dots and chapter separators that are believed to have originated later.
Respect for the written text of the Quran is an important element of religious faith by many Muslims, and the Quran is treated with reverence. Based on tradition and a literal interpretation of Quran 56:79 ("none shall touch but those who are clean"), some Muslims believe that they must perform a ritual cleansing with water before touching a copy of the Quran, although this view is not universal. Worn-out copies of the Quran are wrapped in a cloth and stored indefinitely in a safe place, buried in a mosque or a Muslim cemetery, or burned and the ashes buried or scattered over water.
Inimitability of the Quran (or "I'jaz") is the belief that no human speech can match the Quran in its content and form. The Quran is considered an inimitable miracle by Muslims, effective until the Day of Resurrection—and, thereby, the central proof granted to Muhammad in authentication of his prophetic status. The concept of inimitability originates in the Quran where in five different verses opponents are challenged to produce something like the Quran: "If men and sprites banded together to produce the like of this Quran they would never produce its like not though they backed one another." So the suggestion is that if there are doubts concerning the divine authorship of the Quran, come forward and create something like it. From the ninth century, numerous works appeared which studied the Quran and examined its style and content. Medieval Muslim scholars including al-Jurjani (d. 1078) and al-Baqillani (d. 1013) have written treatises on the subject, discussed its various aspects, and used linguistic approaches to study the Quran. Others argue that the Quran contains noble ideas, has inner meanings, maintained its freshness through the ages and has caused great transformations in individual level and in the history. Some scholars state that the Quran contains scientific information that agrees with modern science. The doctrine of miraculousness of the Quran is further emphasized by Muhammad's illiteracy since the unlettered prophet could not have been suspected of composing the Quran.
The Quran consists of 114 chapters of varying lengths, each known as a sura. Suras are classified as Meccan or Medinan, depending on whether the verses were revealed before or after the migration of Muhammad to the city of Medina. However, a sura classified as Medinan may contain Meccan verses in it and vice versa. Sura titles are derived from a name or quality discussed in the text, or from the first letters or words of the sura. Suras are arranged roughly in order of decreasing size. The sura arrangement is thus not connected to the sequence of revelation. Each sura except the ninth starts with the Bismillah (بسم الله الرحمن الرحيم), an Arabic phrase meaning "In the name of God". There are, however, still 114 occurrences of the Bismillah in the Quran, due to its presence in Quran 27:30 as the opening of Solomon's letter to the Queen of Sheba.
In addition to and independent of the division into suras, there are various ways of dividing the Quran into parts of approximately equal length for convenience in reading. The 30 juz' (plural ajzāʼ) can be used to read through the entire Quran in a month. Some of these parts are known by names—which are the first few words by which the juzʼ starts. A juz' is sometimes further divided into two ḥizb (plural aḥzāb), and each hizb subdivided into four rubʻ al-ahzab. The Quran is also divided into seven approximately equal parts, manzil (plural manāzil), for it to be recited in a week.
The Quranic content is concerned with basic Islamic beliefs including the existence of God and the resurrection. Narratives of the early prophets, ethical and legal subjects, historical events of Muhammad's time, charity and prayer also appear in the Quran. The Quranic verses contain general exhortations regarding right and wrong and historical events are related to outline general moral lessons. Verses pertaining to natural phenomena have been interpreted by Muslims as an indication of the authenticity of the Quranic message.
The doctrine of the last day and eschatology (the final fate of the universe) may be reckoned as the second great doctrine of the Quran. It is estimated that approximately one-third of the Quran is eschatological, dealing with the afterlife in the next world and with the day of judgment at the end of time. There is a reference to the afterlife on most pages of the Quran and belief in the afterlife is often referred to in conjunction with belief in God as in the common expression: "Believe in God and the last day". A number of suras such as 44, 56, 75, 78, 81 and 101 are directly related to the afterlife and its preparations. Some suras indicate the closeness of the event and warn people to be prepared for the imminent day. For instance, the first verses of Sura 22, which deal with the mighty earthquake and the situations of people on that day, represent this style of divine address: "O People! Be respectful to your Lord. The earthquake of the Hour is a mighty thing."
According to the Quran, God communicated with man and made his will known through signs and revelations. Prophets, or 'Messengers of God', received revelations and delivered them to humanity. The message has been identical and for all humankind. "Nothing is said to you that was not said to the messengers before you, that your lord has at his Command forgiveness as well as a most Grievous Penalty." The revelation does not come directly from God to the prophets. Angels acting as God's messengers deliver the divine revelation to them. This comes out in Quran 42:51, in which it is stated: "It is not for any mortal that God should speak to them, except by revelation, or from behind a veil, or by sending a messenger to reveal by his permission whatsoever He will."
Belief is a fundamental aspect of morality in the Quran, and scholars have tried to determine the semantic contents of "belief" and "believer" in the Quran. The ethico-legal concepts and exhortations dealing with righteous conduct are linked to a profound awareness of God, thereby emphasizing the importance of faith, accountability, and the belief in each human's ultimate encounter with God. People are invited to perform acts of charity, especially for the needy. Believers who "spend of their wealth by night and by day, in secret and in public" are promised that they "shall have their reward with their Lord; on them shall be no fear, nor shall they grieve". It also affirms family life by legislating on matters of marriage, divorce, and inheritance. A number of practices, such as usury and gambling, are prohibited. The Quran is one of the fundamental sources of Islamic law (sharia). Some formal religious practices receive significant attention in the Quran including the formal prayers (salat) and fasting in the month of Ramadan. As for the manner in which the prayer is to be conducted, the Quran refers to prostration. The term for charity, zakat, literally means purification. Charity, according to the Quran, is a means of self-purification.
The astrophysicist Nidhal Guessoum while being highly critical of pseudo-scientific claims made about the Quran, has highlighted the encouragement for sciences that the Quran provides by developing "the concept of knowledge.". He writes: "The Qur'an draws attention to the danger of conjecturing without evidence (And follow not that of which you have not the (certain) knowledge of... 17:36) and in several different verses asks Muslims to require proofs (Say: Bring your proof if you are truthful 2:111), both in matters of theological belief and in natural science." Guessoum cites Ghaleb Hasan on the definition of "proof" according the Quran being "clear and strong... convincing evidence or argument." Also, such a proof cannot rely on an argument from authority, citing verse 5:104. Lastly, both assertions and rejections require a proof, according to verse 4:174. Ismail al-Faruqi and Taha Jabir Alalwani are of the view that any reawakening of the Muslim civilization must start with the Quran; however, the biggest obstacle on this route is the "centuries old heritage of tafseer (exegesis) and other classical disciplines" which inhibit a "universal, epidemiological and systematic conception" of the Quran's message. The philosopher Muhammad Iqbal, considered the Quran's methodology and epistemology to be empirical and rational.
It's generally accepted that there are around 750 verses in the Quran dealing with natural phenomenon. In many of these verses the study of nature is "encouraged and highly recommended," and historical Islamic scientists like Al-Biruni and Al-Battani derived their inspiration from verses of the Quran. Mohammad Hashim Kamali has the stated that "scientific observation, experimental knowledge and rationality" are the primary tools with which humanity can achieve the goals laid out for it in the Quran. Ziauddin Sardar built a case for Muslims having developed the foundations of modern science, by highlighting the repeated calls of the Quran to observe and reflect upon natural phenomenon. "The 'scientific method,' as it is understood today, was first developed by Muslim scientists" like Ibn al-Haytham and Al-Biruni, along with numerous other Muslim scientists.
The physicist Abdus Salam, in his Nobel Prize banquet address, quoted a well known verse from the Quran (67:3-4) and then stated: "This in effect is the faith of all physicists: the deeper we seek, the more is our wonder excited, the more is the dazzlement of our gaze". One of Salam's core beliefs was that there is no contradiction between Islam and the discoveries that science allows humanity to make about nature and the universe. Salam also held the opinion that the Quran and the Islamic spirit of study and rational reflection was the source of extraordinary civilizational development. Salam highlights, in particular, the work of Ibn al-Haytham and Al-Biruni as the pioneers of empiricism who introduced the experimental approach, breaking way from Aristotle's influence, and thus giving birth to modern science. Salam was also careful to differentiate between metaphysics and physics, and advised against empirically probing certain matters on which "physics is silent and will remain so," such as the doctrine of "creation from nothing" which in Salam's view is outside the limits of science and thus "gives way" to religious considerations.
The language of the Quran has been described as "rhymed prose" as it partakes of both poetry and prose; however, this description runs the risk of failing to convey the rhythmic quality of Quranic language, which is more poetic in some parts and more prose-like in others. Rhyme, while found throughout the Quran, is conspicuous in many of the earlier Meccan suras, in which relatively short verses throw the rhyming words into prominence. The effectiveness of such a form is evident for instance in Sura 81, and there can be no doubt that these passages impressed the conscience of the hearers. Frequently a change of rhyme from one set of verses to another signals a change in the subject of discussion. Later sections also preserve this form but the style is more expository.
The Quranic text seems to have no beginning, middle, or end, its nonlinear structure being akin to a web or net. The textual arrangement is sometimes considered to exhibit lack of continuity, absence of any chronological or thematic order and repetitiousness. Michael Sells, citing the work of the critic Norman O. Brown, acknowledges Brown's observation that the seeming disorganization of Quranic literary expression – its scattered or fragmented mode of composition in Sells's phrase – is in fact a literary device capable of delivering profound effects as if the intensity of the prophetic message were shattering the vehicle of human language in which it was being communicated. Sells also addresses the much-discussed repetitiveness of the Quran, seeing this, too, as a literary device.
A text is self-referential when it speaks about itself and makes reference to itself. According to Stefan Wild, the Quran demonstrates this metatextuality by explaining, classifying, interpreting and justifying the words to be transmitted. Self-referentiality is evident in those passages where the Quran refers to itself as revelation (tanzil), remembrance (dhikr), news (naba'), criterion (furqan) in a self-designating manner (explicitly asserting its Divinity, "And this is a blessed Remembrance that We have sent down; so are you now denying it?"), or in the frequent appearance of the "Say" tags, when Muhammad is commanded to speak (e.g., "Say: 'God's guidance is the true guidance' ", "Say: 'Would you then dispute with us concerning God?' "). According to Wild the Quran is highly self-referential. The feature is more evident in early Meccan suras.
Tafsir is one of the earliest academic activities of Muslims. According to the Quran, Muhammad was the first person who described the meanings of verses for early Muslims. Other early exegetes included a few Companions of Muhammad, like ʻAli ibn Abi Talib, ʻAbdullah ibn Abbas, ʻAbdullah ibn Umar and Ubayy ibn Kaʻb. Exegesis in those days was confined to the explanation of literary aspects of the verse, the background of its revelation and, occasionally, interpretation of one verse with the help of the other. If the verse was about a historical event, then sometimes a few traditions (hadith) of Muhammad were narrated to make its meaning clear.
Because the Quran is spoken in classical Arabic, many of the later converts to Islam (mostly non-Arabs) did not always understand the Quranic Arabic, they did not catch allusions that were clear to early Muslims fluent in Arabic and they were concerned with reconciling apparent conflict of themes in the Quran. Commentators erudite in Arabic explained the allusions, and perhaps most importantly, explained which Quranic verses had been revealed early in Muhammad's prophetic career, as being appropriate to the very earliest Muslim community, and which had been revealed later, canceling out or "abrogating" (nāsikh) the earlier text (mansūkh). Other scholars, however, maintain that no abrogation has taken place in the Quran. The Ahmadiyya Muslim Community has published a ten-volume Urdu commentary on the Quran, with the name Tafseer e Kabir.
Esoteric or Sufi interpretation attempts to unveil the inner meanings of the Quran. Sufism moves beyond the apparent (zahir) point of the verses and instead relates Quranic verses to the inner or esoteric (batin) and metaphysical dimensions of consciousness and existence. According to Sands, esoteric interpretations are more suggestive than declarative, they are allusions (isharat) rather than explanations (tafsir). They indicate possibilities as much as they demonstrate the insights of each writer.
Moses, in 7:143, comes the way of those who are in love, he asks for a vision but his desire is denied, he is made to suffer by being commanded to look at other than the Beloved while the mountain is able to see God. The mountain crumbles and Moses faints at the sight of God's manifestation upon the mountain. In Qushayri's words, Moses came like thousands of men who traveled great distances, and there was nothing left to Moses of Moses. In that state of annihilation from himself, Moses was granted the unveiling of the realities. From the Sufi point of view, God is the always the beloved and the wayfarer's longing and suffering lead to realization of the truths.
One of the notable authors of esoteric interpretation prior to the 12th century is Sulami (d. 1021) without whose work the majority of very early Sufi commentaries would not have been preserved. Sulami's major commentary is a book named haqaiq al-tafsir ("Truths of Exegesis") which is a compilation of commentaries of earlier Sufis. From the 11th century onwards several other works appear, including commentaries by Qushayri (d. 1074), Daylami (d. 1193), Shirazi (d. 1209) and Suhrawardi (d. 1234). These works include material from Sulami's books plus the author's contributions. Many works are written in Persian such as the works of Maybudi (d. 1135) kash al-asrar ("the unveiling of the secrets"). Rumi (d. 1273) wrote a vast amount of mystical poetry in his book Mathnawi. Rumi makes heavy use of the Quran in his poetry, a feature that is sometimes omitted in translations of Rumi's work. A large number of Quranic passages can be found in Mathnawi, which some consider a kind of Sufi interpretation of the Quran. Rumi's book is not exceptional for containing citations from and elaboration on the Quran, however, Rumi does mention Quran more frequently. Simnani (d. 1336) wrote two influential works of esoteric exegesis on the Quran. He reconciled notions of God's manifestation through and in the physical world with the sentiments of Sunni Islam. Comprehensive Sufi commentaries appear in the 18th century such as the work of Ismail Hakki Bursevi (d. 1725). His work ruh al-Bayan (the Spirit of Elucidation) is a voluminous exegesis. Written in Arabic, it combines the author's own ideas with those of his predecessors (notably Ibn Arabi and Ghazali), all woven together in Hafiz, a Persian poetry form.
Commentaries dealing with the zahir (outward aspects) of the text are called tafsir, and hermeneutic and esoteric commentaries dealing with the batin are called ta'wil ("interpretation" or "explanation"), which involves taking the text back to its beginning. Commentators with an esoteric slant believe that the ultimate meaning of the Quran is known only to God. In contrast, Quranic literalism, followed by Salafis and Zahiris, is the belief that the Quran should only be taken at its apparent meaning.[citation needed]
The first fully attested complete translations of the Quran were done between the 10th and 12th centuries in Persian. The Samanid king, Mansur I (961-976), ordered a group of scholars from Khorasan to translate the Tafsir al-Tabari, originally in Arabic, into Persian. Later in the 11th century, one of the students of Abu Mansur Abdullah al-Ansari wrote a complete tafsir of the Quran in Persian. In the 12th century, Najm al-Din Abu Hafs al-Nasafi translated the Quran into Persian. The manuscripts of all three books have survived and have been published several times.[citation needed]
Robert of Ketton's 1143 translation of the Quran for Peter the Venerable, Lex Mahumet pseudoprophete, was the first into a Western language (Latin). Alexander Ross offered the first English version in 1649, from the French translation of L'Alcoran de Mahomet (1647) by Andre du Ryer. In 1734, George Sale produced the first scholarly translation of the Quran into English; another was produced by Richard Bell in 1937, and yet another by Arthur John Arberry in 1955. All these translators were non-Muslims. There have been numerous translations by Muslims. The Ahmadiyya Muslim Community has published translations of the Quran in 50 different languages besides a five-volume English commentary and an English translation of the Quran.
The proper recitation of the Quran is the subject of a separate discipline named tajwid which determines in detail how the Quran should be recited, how each individual syllable is to be pronounced, the need to pay attention to the places where there should be a pause, to elisions, where the pronunciation should be long or short, where letters should be sounded together and where they should be kept separate, etc. It may be said that this discipline studies the laws and methods of the proper recitation of the Quran and covers three main areas: the proper pronunciation of consonants and vowels (the articulation of the Quranic phonemes), the rules of pause in recitation and of resumption of recitation, and the musical and melodious features of recitation.
Vocalization markers indicating specific vowel sounds were introduced into the Arabic language by the end of the 9th century. The first Quranic manuscripts lacked these marks, therefore several recitations remain acceptable. The variation in readings of the text permitted by the nature of the defective vocalization led to an increase in the number of readings during the 10th century. The 10th-century Muslim scholar from Baghdad, Ibn Mujāhid, is famous for establishing seven acceptable textual readings of the Quran. He studied various readings and their trustworthiness and chose seven 8th-century readers from the cities of Mecca, Medina, Kufa, Basra and Damascus. Ibn Mujahid did not explain why he chose seven readers, rather than six or ten, but this may be related to a prophetic tradition (Muhammad's saying) reporting that the Quran had been revealed in seven "ahruf" (meaning seven letters or modes). Today, the most popular readings are those transmitted by Ḥafṣ (d.796) and Warsh (d. 812) which are according to two of Ibn Mujahid's reciters, Aasim ibn Abi al-Najud (Kufa, d. 745) and Nafi‘ al-Madani (Medina, d. 785), respectively. The influential standard Quran of Cairo (1924) uses an elaborate system of modified vowel-signs and a set of additional symbols for minute details and is based on ʻAsim's recitation, the 8th-century recitation of Kufa. This edition has become the standard for modern printings of the Quran.
Before printing was widely adopted in the 19th century, the Quran was transmitted in manuscripts made by calligraphers and copyists. The earliest manuscripts were written in Ḥijāzī-type script. The Hijazi style manuscripts nevertheless confirm that transmission of the Quran in writing began at an early stage. Probably in the ninth century, scripts began to feature thicker strokes, which are traditionally known as Kufic scripts. Toward the end of the ninth century, new scripts began to appear in copies of the Quran and replace earlier scripts. The reason for discontinuation in the use of the earlier style was that it took too long to produce and the demand for copies was increasing. Copyists would therefore chose simpler writing styles. Beginning in the 11th century, the styles of writing employed were primarily the naskh, muhaqqaq, rayḥānī and, on rarer occasions, the thuluth script. Naskh was in very widespread use. In North Africa and Spain, the Maghribī style was popular. More distinct is the Bihari script which was used solely in the north of India. Nastaʻlīq style was also rarely used in Persian world.
In the beginning, the Quran did not have vocalization markings. The system of vocalization, as we know it today, seems to have been introduced towards the end of the ninth century. Since it would have been too costly for most Muslims to purchase a manuscript, copies of the Quran were held in mosques in order to make them accessible to people. These copies frequently took the form of a series of 30 parts or juzʼ. In terms of productivity, the Ottoman copyists provide the best example. This was in response to widespread demand, unpopularity of printing methods and for aesthetic reasons.
According to Sahih al-Bukhari, the Quran was recited among Levantines and Iraqis, and discussed by Christians and Jews, before it was standardized. Its language was similar to the Syriac language.[citation needed] The Quran recounts stories of many of the people and events recounted in Jewish and Christian sacred books (Tanakh, Bible) and devotional literature (Apocrypha, Midrash), although it differs in many details. Adam, Enoch, Noah, Eber, Shelah, Abraham, Lot, Ishmael, Isaac, Jacob, Joseph, Job, Jethro, David, Solomon, Elijah, Elisha, Jonah, Aaron, Moses, Zechariah, John the Baptist and Jesus are mentioned in the Quran as prophets of God (see Prophets of Islam). In fact, Moses is mentioned more in the Quran than any other individual. Jesus is mentioned more often in the Quran than Muhammad, while Mary is mentioned in the Quran more than the New Testament. Muslims believe the common elements or resemblances between the Bible and other Jewish and Christian writings and Islamic dispensations is due to their common divine source,[citation needed] and that the original Christian or Jewish texts were authentic divine revelations given to prophets.
According to Tabatabaei, there are acceptable and unacceptable esoteric interpretations. Acceptable ta'wil refers to the meaning of a verse beyond its literal meaning; rather the implicit meaning, which ultimately is known only to God and can't be comprehended directly through human thought alone. The verses in question here refer to the human qualities of coming, going, sitting, satisfaction, anger and sorrow, which are apparently attributed to God. Unacceptable ta'wil is where one "transfers" the apparent meaning of a verse to a different meaning by means of a proof; this method is not without obvious inconsistencies. Although this unacceptable ta'wil has gained considerable acceptance, it is incorrect and cannot be applied to the Quranic verses. The correct interpretation is that reality a verse refers to. It is found in all verses, the decisive and the ambiguous alike; it is not a sort of a meaning of the word; it is a fact that is too sublime for words. God has dressed them with words to bring them a bit nearer to our minds; in this respect they are like proverbs that are used to create a picture in the mind, and thus help the hearer to clearly grasp the intended idea.
The Quran most likely existed in scattered written form during Muhammad's lifetime. Several sources indicate that during Muhammad's lifetime a large number of his companions had memorized the revelations. Early commentaries and Islamic historical sources support the above-mentioned understanding of the Quran's early development. The Quran in its present form is generally considered by academic scholars to record the words spoken by Muhammad because the search for variants has not yielded any differences of great significance.[page needed] University of Chicago professor Fred Donner states that "...there was a very early attempt to establish a uniform consonantal text of the Qurʾān from what was probably a wider and more varied group of related texts in early transmission. [...] After the creation of this standardized canonical text, earlier authoritative texts were suppressed, and all extant manuscripts—despite their numerous variants—seem to date to a time after this standard consonantal text was established." Although most variant readings of the text of the Quran have ceased to be transmitted, some still are. There has been no critical text produced on which a scholarly reconstruction of the Quranic text could be based. Historically, controversy over the Quran's content has rarely become an issue, although debates continue on the subject.
Sahih al-Bukhari narrates Muhammad describing the revelations as, "Sometimes it is (revealed) like the ringing of a bell" and Aisha reported, "I saw the Prophet being inspired Divinely on a very cold day and noticed the sweat dropping from his forehead (as the Inspiration was over)." Muhammad's first revelation, according to the Quran, was accompanied with a vision. The agent of revelation is mentioned as the "one mighty in power", the one who "grew clear to view when he was on the uppermost horizon. Then he drew nigh and came down till he was (distant) two bows' length or even nearer." The Islamic studies scholar Welch states in the Encyclopaedia of Islam that he believes the graphic descriptions of Muhammad's condition at these moments may be regarded as genuine, because he was severely disturbed after these revelations. According to Welch, these seizures would have been seen by those around him as convincing evidence for the superhuman origin of Muhammad's inspirations. However, Muhammad's critics accused him of being a possessed man, a soothsayer or a magician since his experiences were similar to those claimed by such figures well known in ancient Arabia. Welch additionally states that it remains uncertain whether these experiences occurred before or after Muhammad's initial claim of prophethood.
Muhammad Husayn Tabatabaei says that according to the popular explanation among the later exegetes, ta'wil indicates the particular meaning a verse is directed towards. The meaning of revelation (tanzil), as opposed to ta'wil, is clear in its accordance to the obvious meaning of the words as they were revealed. But this explanation has become so widespread that, at present, it has become the primary meaning of ta'wil, which originally meant "to return" or "the returning place". In Tabatabaei's view, what has been rightly called ta'wil, or hermeneutic interpretation of the Quran, is not concerned simply with the denotation of words. Rather, it is concerned with certain truths and realities that transcend the comprehension of the common run of men; yet it is from these truths and realities that the principles of doctrine and the practical injunctions of the Quran issue forth. Interpretation is not the meaning of the verse—rather it transpires through that meaning, in a special sort of transpiration. There is a spiritual reality—which is the main objective of ordaining a law, or the basic aim in describing a divine attribute—and then there is an actual significance that a Quranic story refers to.
According to Shia beliefs, those who are firmly rooted in knowledge like Muhammad and the imams know the secrets of the Quran. According to Tabatabaei, the statement "none knows its interpretation except God" remains valid, without any opposing or qualifying clause. Therefore, so far as this verse is concerned, the knowledge of the Quran's interpretation is reserved for God. But Tabatabaei uses other verses and concludes that those who are purified by God know the interpretation of the Quran to a certain extent.
Kievan Rus' begins with the rule (882–912) of Prince Oleg, who extended his control from Novgorod south along the Dnieper river valley in order to protect trade from Khazar incursions from the east and moved his capital to the more strategic Kiev. Sviatoslav I (died 972) achieved the first major expansion of Kievan Rus' territorial control, fighting a war of conquest against the Khazar Empire. Vladimir the Great (980–1015) introduced Christianity with his own baptism and, by decree, that of all the inhabitants of Kiev and beyond. Kievan Rus' reached its greatest extent under Yaroslav I (1019–1054); his sons assembled and issued its first written legal code, the Rus' Justice, shortly after his death.
The term "Kievan Rus'" (Ки́евская Русь Kievskaya Rus’) was coined in the 19th century in Russian historiography to refer to the period when the centre was in Kiev. In English, the term was introduced in the early 20th century, when it was found in the 1913 English translation of Vasily Klyuchevsky's A History of Russia, to distinguish the early polity from successor states, which were also named Rus. Later, the Russian term was rendered into Belarusian and Ukrainian as Кіеўская Русь Kijeŭskaja Rus’ and Ки́ївська Русь Kyivs'ka Rus’, respectively.
Prior to the emergence of Kievan Rus' in the 9th century AD, the lands between the Baltic Sea and Black Sea were primarily populated by eastern Slavic tribes. In the northern region around Novgorod were the Ilmen Slavs and neighboring Krivichi, who occupied territories surrounding the headwaters of the West Dvina, Dnieper, and Volga Rivers. To their north, in the Ladoga and Karelia regions, were the Finnic Chud tribe. In the south, in the area around Kiev, were the Poliane, a group of Slavicized tribes with Iranian origins, the Drevliane to the west of the Dnieper, and the Severiane to the east. To their north and east were the Vyatichi, and to their south was forested land settled by Slav farmers, giving way to steppelands populated by nomadic herdsmen.
Controversy persists over whether the Rus’ were Varangians (Vikings) or Slavs. This uncertainty is due largely to a paucity of contemporary sources. Attempts to address this question instead rely on archaeological evidence, the accounts of foreign observers, legends and literature from centuries later. To some extent the controversy is related to the foundation myths of modern states in the region. According to the "Normanist" view, the Rus' were Scandinavians, while Russian and Ukrainian nationalist historians generally argue that the Rus' were themselves Slavs. Normanist theories focus on the earliest written source for the East Slavs, the Russian Primary Chronicle, although even this account was not produced until the 12th century. Nationalist accounts have suggested that the Rus' were present before the arrival of the Varangians, noting that only a handful of Scandinavian words can be found in modern Russian and that Scandinavian names in the early chronicles were soon replaced by Slavic names. Nevertheless, archaeological evidence from the area suggests that a Scandinavian population was present during the 10th century at the latest. On balance, it seems likely that the Rus' proper were a small minority of Scandinavians who formed an elite ruling class, while the great majority of their subjects were Slavs. Considering the linguistic arguments mounted by nationalist scholars, if the proto-Rus' were Scandinavians, they must have quickly become nativized, adopting Slavic languages and other cultural practices.
Ahmad ibn Fadlan, an Arab traveler during the 10th century, provided one of the earliest written descriptions of the Rus': "They are as tall as a date palm, blond and ruddy, so that they do not need to wear a tunic nor a cloak; rather the men among them wear garments that only cover half of his body and leaves one of his hands free." Liutprand of Cremona, who was twice an envoy to the Byzantine court (949 and 968), identifies the "Russi" with the Norse ("the Russi, whom we call Norsemen by another name") but explains the name as a Greek term referring to their physical traits ("A certain people made up of a part of the Norse, whom the Greeks call [...] the Russi on account of their physical features, we designate as Norsemen because of the location of their origin."). Leo the Deacon, a 10th-century Byzantine historian and chronicler, refers to the Rus' as "Scythians" and notes that they tended to adopt Greek rituals and customs.
According to the Primary Chronicle, the territories of the East Slavs in the 9th century were divided between the Varangians and the Khazars. The Varangians are first mentioned imposing tribute from Slavic and Finnic tribes in 859. In 862, the Finnic and Slavic tribes in the area of Novgorod rebelled against the Varangians, driving them "back beyond the sea and, refusing them further tribute, set out to govern themselves." The tribes had no laws, however, and soon began to make war with one another, prompting them to invite the Varangians back to rule them and bring peace to the region:
The three brothers—Rurik, Sineus, and Truvor—established themselves in Novgorod, Beloozero, and Izborsk, respectively. Two of the brothers died, and Rurik became the sole ruler of the territory and progenitor of the Rurik Dynasty. A short time later, two of Rurik’s men, Askold and Dir, asked him for permission to go to Tsargrad (Constantinople). On their way south, they discovered "a small city on a hill," Kiev, captured it and the surrounding country from the Khazars, populated the region with more Varangians, and "established their dominion over the country of the Polyanians."
The Chronicle reports that Askold and Dir continued to Constantinople with a navy to attack the city in 863–66, catching the Byzantines by surprise and ravaging the surrounding area, though other accounts date the attack in 860. Patriarch Photius vividly describes the "universal" devastation of the suburbs and nearby islands, and another account further details the destruction and slaughter of the invasion. The Rus' turned back before attacking the city itself, due either to a storm dispersing their boats, the return of the Emperor, or in a later account, due to a miracle after a ceremonial appeal by the Patriarch and the Emperor to the Virgin. The attack was the first encounter between the Rus' and Byzantines and led the Patriarch to send missionaries north to engage and attempt to convert the Rus' and the Slavs.
Rurik led the Rus' until his death in about 879, bequeathing his kingdom to his kinsman, Prince Oleg, as regent for his young son, Igor. In 880-82, Oleg led a military force south along the Dnieper river, capturing Smolensk and Lyubech before reaching Kiev, where he deposed and killed Askold and Dir, proclaimed himself prince, and declared Kiev the "mother of Rus' cities." Oleg set about consolidating his power over the surrounding region and the riverways north to Novgorod, imposing tribute on the East Slav tribes. In 883, he conquered the Drevlians, imposing a fur tribute on them. By 885 he had subjugated the Poliane, Severiane, Vyatichi, and Radimichs, forbidding them to pay further tribute to the Khazars. Oleg continued to develop and expand a network of Rus' forts in Slav lands, begun by Rurik in the north.
The new Kievan state prospered due to its abundant supply of furs, beeswax, honey, and slaves for export, and because it controlled three main trade routes of Eastern Europe. In the north, Novgorod served as a commercial link between the Baltic Sea and the Volga trade route to the lands of the Volga Bulgars, the Khazars, and across the Caspian Sea as far as Baghdad, providing access to markets and products from Central Asia and the Middle East. Trade from the Baltic also moved south on a network of rivers and short portages along the Dnieper known as the "route from the Varangians to the Greeks," continuing to the Black Sea and on to Constantinople. Kiev was a central outpost along the Dnieper route and a hub with the east-west overland trade route between the Khazars and the Germanic lands of Central Europe. These commercial connections enriched Rus' merchants and princes, funding military forces and the construction of churches, palaces, fortifications, and further towns. Demand for luxury goods fostered production of expensive jewelry and religious wares, allowing their export, and an advanced credit and money-lending system may have also been in place.
The rapid expansion of the Rus' to the south led to conflict and volatile relationships with the Khazars and other neighbors on the Pontic steppe. The Khazars dominated the Black Sea steppe during the 8th century, trading and frequently allying with the Byzantine Empire against Persians and Arabs. In the late 8th century, the collapse of the Göktürk Khaganate led the Magyars and the Pechenegs, Ugric and Turkic peoples from Central Asia, to migrate west into the steppe region, leading to military conflict, disruption of trade, and instability within the Khazar Khaganate. The Rus' and Slavs had earlier allied with the Khazars against Arab raids on the Caucasus, but they increasingly worked against them to secure control of the trade routes.
The Byzantine Empire was able to take advantage of the turmoil to expand its political influence and commercial relationships, first with the Khazars and later with the Rus' and other steppe groups. The Byzantines established the Theme of Cherson, formally known as Klimata, in the Crimea in the 830s to defend against raids by the Rus' and to protect vital grain shipments supplying Constantinople. Cherson also served as a key diplomatic link with the Khazars and others on the steppe, and it became the centre of Black Sea commerce. The Byzantines also helped the Khazars build a fortress at Sarkel on the Don river to protect their northwest frontier against incursions by the Turkic migrants and the Rus', and to control caravan trade routes and the portage between the Don and Volga rivers.
The expansion of the Rus' put further military and economic pressure on the Khazars, depriving them of territory, tributaries, and trade. In around 890, Oleg waged an indecisive war in the lands of the lower Dniester and Dnieper rivers with the Tivertsi and the Ulichs, who were likely acting as vassals of the Magyars, blocking Rus' access to the Black Sea. In 894, the Magyars and Pechenegs were drawn into the wars between the Byzantines and the Bulgarian Empire. The Byzantines arranged for the Magyars to attack Bulgarian territory from the north, and Bulgaria in turn persuaded the Pechenegs to attack the Magyars from their rear. Boxed in, the Magyars were forced to migrate further west across the Carpathian Mountains into the Hungarian plain, depriving the Khazars of an important ally and a buffer from the Rus'. The migration of the Magyars allowed Rus' access to the Black Sea, and they soon launched excursions into Khazar territory along the sea coast, up the Don river, and into the lower Volga region. The Rus' were raiding and plundering into the Caspian Sea region from 864, with the first large-scale expedition in 913, when they extensively raided Baku, Gilan, Mazandaran and penetrated into the Caucasus.
As the 10th century progressed, the Khazars were no longer able to command tribute from the Volga Bulgars, and their relationship with the Byzantines deteriorated, as Byzantium increasingly allied with the Pechenegs against them. The Pechenegs were thus secure to raid the lands of the Khazars from their base between the Volga and Don rivers, allowing them to expand to the west. Rus' relations with the Pechenegs were complex, as the groups alternately formed alliances with and against one another. The Pechenegs were nomads roaming the steppe raising livestock which they traded with the Rus' for agricultural goods and other products. The lucrative Rus' trade with the Byzantine Empire had to pass through Pecheneg-controlled territory, so the need for generally peaceful relations was essential. Nevertheless, while the Primary Chronicle reports the Pechenegs entering Rus' territory in 915 and then making peace, they were waging war with one another again in 920. Pechenegs are reported assisting the Rus' in later campaigns against the Byzantines, yet allied with the Byzantines against the Rus' at other times.
After the Rus' attack on Constantinople in 860, the Byzantine Patriarch Photius sent missionaries north to convert the Rus' and the Slavs. Prince Rastislav of Moravia had requested the Emperor to provide teachers to interpret the holy scriptures, so in 863 the brothers Cyril and Methodius were sent as missionaries, due to their knowledge of the Slavonic language. The Slavs had no written language, so the brothers devised the Glagolitic alphabet, later developed into Cyrillic, and standardized the language of the Slavs, later known as Old Church Slavonic. They translated portions of the Bible and drafted the first Slavic civil code and other documents, and the language and texts spread throughout Slavic territories, including Kievan Rus’. The mission of Cyril and Methodius served both evangelical and diplomatic purposes, spreading Byzantine cultural influence in support of imperial foreign policy. In 867 the Patriarch announced that the Rus' had accepted a bishop, and in 874 he speaks of an "Archbishop of the Rus'."
Relations between the Rus' and Byzantines became more complex after Oleg took control over Kiev, reflecting commercial, cultural, and military concerns. The wealth and income of the Rus' depended heavily upon trade with Byzantium. Constantine Porphyrogenitus described the annual course of the princes of Kiev, collecting tribute from client tribes, assembling the product into a flotilla of hundreds of boats, conducting them down the Dnieper to the Black Sea, and sailing to the estuary of the Dniester, the Danube delta, and on to Constantinople. On their return trip they would carry silk fabrics, spices, wine, and fruit. The importance of this trade relationship led to military action when disputes arose. The Primary Chronicle reports that the Rus' attacked Constantinople again in 907, probably to secure trade access. The Chronicle glorifies the military prowess and shrewdness of Oleg, an account imbued with legendary detail. Byzantine sources do not mention the attack, but a pair of treaties in 907 and 911 set forth a trade agreement with the Rus', the terms suggesting pressure on the Byzantines, who granted the Rus' quarters and supplies for their merchants and tax-free trading privileges in Constantinople.
The Chronicle provides a mythic tale of Oleg's death. A sorcerer prophesies that the death of the Grand Prince would be associated with a certain horse. Oleg has the horse sequestered, and it later dies. Oleg goes to visit the horse and stands over the carcass, gloating that he had outlived the threat, when a snake strikes him from among the bones, and he soon becomes ill and dies. The Chronicle reports that Prince Igor succeeded Oleg in 913, and after some brief conflicts with the Drevlians and the Pechenegs, a period of peace ensued for over twenty years.
In 941, Igor led another major Rus' attack on Constantinople, probably over trading rights again. A navy of 10,000 vessels, including Pecheneg allies, landed on the Bithynian coast and devastated the Asiatic shore of the Bosphorus. The attack was well-timed, perhaps due to intelligence, as the Byzantine fleet was occupied with the Arabs in the Mediterranean, and the bulk of its army was stationed in the east. The Rus’ burned towns, churches, and monasteries, butchering the people and amassing booty. The emperor arranged for a small group of retired ships to be outfitted with Greek fire throwers and sent them out to meet the Rus’, luring them into surrounding the contingent before unleashing the Greek fire. Liutprand of Cremona wrote that "the Rus', seeing the flames, jumped overboard, preferring water to fire. Some sank, weighed down by the weight of their breastplates and helmets; others caught fire." Those captured were beheaded. The ploy dispelled the Rus’ fleet, but their attacks continued into the hinterland as far as Nicomedia, with many atrocities reported as victims were crucified and set up for use as targets. At last a Byzantine army arrived from the Balkans to drive the Rus' back, and a naval contingent reportedly destroyed much of the Rus' fleet on its return voyage (possibly an exaggeration since the Rus' soon mounted another attack). The outcome indicates increased military might by Byzantium since 911, suggesting a shift in the balance of power.
Igor returned to Kiev keen for revenge. He assembled a large force of warriors from among neighboring Slavs and Pecheneg allies, and sent for reinforcements of Varangians from “beyond the sea.” In 944 the Rus' force advanced again on the Greeks, by land and sea, and a Byzantine force from Cherson responded. The Emperor sent gifts and offered tribute in lieu of war, and the Rus' accepted. Envoys were sent between the Rus’, the Byzantines, and the Bulgarians in 945, and a peace treaty was completed. The agreement again focused on trade, but this time with terms less favorable to the Rus’, including stringent regulations on the conduct of Rus’ merchants in Cherson and Constantinople and specific punishments for violations of the law. The Byzantines may have been motivated to enter the treaty out of concern of a prolonged alliance of the Rus', Pechenegs, and Bulgarians against them, though the more favorable terms further suggest a shift in power.
Following the death of Grand Prince Igor in 945, his wife Olga ruled as regent in Kiev until their son Sviatoslav reached maturity (ca. 963). His decade-long reign over Rus' was marked by rapid expansion through the conquest of the Khazars of the Pontic steppe and the invasion of the Balkans. By the end of his short life, Sviatoslav carved out for himself the largest state in Europe, eventually moving his capital from Kiev to Pereyaslavets on the Danube in 969. In contrast with his mother's conversion to Christianity, Sviatoslav, like his druzhina, remained a staunch pagan. Due to his abrupt death in an ambush in 972, Sviatoslav's conquests, for the most part, were not consolidated into a functioning empire, while his failure to establish a stable succession led to a fratricidal feud among his sons, which resulted in two of his three sons being killed.
It is not clearly documented when the title of the Grand Duke was first introduced, but the importance of the Kiev principality was recognized after the death of Sviatoslav I in 972 and the ensuing struggle between Vladimir the Great and Yaropolk I. The region of Kiev dominated the state of Kievan Rus' for the next two centuries. The Grand Prince ("velikiy kniaz'") of Kiev controlled the lands around the city, and his formally subordinate relatives ruled the other cities and paid him tribute. The zenith of the state's power came during the reigns of Vladimir the Great (980–1015) and Prince Yaroslav I the Wise (1019–1054). Both rulers continued the steady expansion of Kievan Rus' that had begun under Oleg.
Vladimir had been prince of Novgorod when his father Sviatoslav I died in 972. He was forced to flee to Scandinavia in 976 after his half-brother Yaropolk had murdered his other brother Oleg and taken control of Rus. In Scandinavia, with the help of his relative Earl Håkon Sigurdsson, ruler of Norway, Vladimir assembled a Viking army and reconquered Novgorod and Kiev from Yaropolk. As Prince of Kiev, Vladimir's most notable achievement was the Christianization of Kievan Rus', a process that began in 988. The Primary Chronicle states that when Vladimir had decided to accept a new faith instead of the traditional idol-worship (paganism) of the Slavs, he sent out some of his most valued advisors and warriors as emissaries to different parts of Europe. They visited the Christians of the Latin Rite, the Jews, and the Muslims before finally arriving in Constantinople. They rejected Islam because, among other things, it prohibited the consumption of alcohol, and Judaism because the god of the Jews had permitted his chosen people to be deprived of their country. They found the ceremonies in the Roman church to be dull. But at Constantinople, they were so astounded by the beauty of the cathedral of Hagia Sophia and the liturgical service held there that they made up their minds there and then about the faith they would like to follow. Upon their arrival home, they convinced Vladimir that the faith of the Byzantine Rite was the best choice of all, upon which Vladimir made a journey to Constantinople and arranged to marry Princess Anna, the sister of Byzantine emperor Basil II.
Vladimir's choice of Eastern Christianity may also have reflected his close personal ties with Constantinople, which dominated the Black Sea and hence trade on Kiev's most vital commercial route, the Dnieper River. Adherence to the Eastern Church had long-range political, cultural, and religious consequences. The church had a liturgy written in Cyrillic and a corpus of translations from Greek that had been produced for the Slavic peoples. This literature facilitated the conversion to Christianity of the Eastern Slavs and introduced them to rudimentary Greek philosophy, science, and historiography without the necessity of learning Greek (there were some merchants who did business with Greeks and likely had an understanding of contemporary business Greek). In contrast, educated people in medieval Western and Central Europe learned Latin. Enjoying independence from the Roman authority and free from tenets of Latin learning, the East Slavs developed their own literature and fine arts, quite distinct from those of other Eastern Orthodox countries.[citation needed] (See Old East Slavic language and Architecture of Kievan Rus for details ). Following the Great Schism of 1054, the Rus' church maintained communion with both Rome and Constantinople for some time, but along with most of the Eastern churches it eventually split to follow the Eastern Orthodox. That being said, unlike other parts of the Greek world, Kievan Rus' did not have a strong hostility to the Western world.
Yaroslav, known as "the Wise", struggled for power with his brothers. A son of Vladimir the Great, he was vice-regent of Novgorod at the time of his father's death in 1015. Subsequently, his eldest surviving brother, Svyatopolk the Accursed, killed three of his other brothers and seized power in Kiev. Yaroslav, with the active support of the Novgorodians and the help of Viking mercenaries, defeated Svyatopolk and became the grand prince of Kiev in 1019. Although he first established his rule over Kiev in 1019, he did not have uncontested rule of all of Kievan Rus' until 1036. Like Vladimir, Yaroslav was eager to improve relations with the rest of Europe, especially the Byzantine Empire. Yaroslav's granddaughter, Eupraxia the daughter of his son Vsevolod I, Prince of Kiev, was married to Henry III, Holy Roman Emperor. Yaroslav also arranged marriages for his sister and three daughters to the kings of Poland, France, Hungary and Norway. Yaroslav promulgated the first East Slavic law code, Russkaya Pravda; built Saint Sophia Cathedral in Kiev and Saint Sophia Cathedral in Novgorod; patronized local clergy and monasticism; and is said to have founded a school system. Yaroslav's sons developed the great Kiev Pechersk Lavra (monastery), which functioned in Kievan Rus' as an ecclesiastical academy.
An unconventional power succession system was established (rota system) whereby power was transferred to the eldest member of the ruling dynasty rather than from father to son, i.e. in most cases to the eldest brother of the ruler, fomenting constant hatred and rivalry within the royal family.[citation needed] Familicide was frequently deployed in obtaining power and can be traced particularly during the time of the Yaroslavichi rule (sons of Yaroslav) when the established system was skipped in the establishment of Vladimir II Monomakh as the Grand Prince of Kiev,[clarification needed] in turn creating major squabbles between Olegovichi from Chernihiv, Monomakhs from Pereyaslav, Izyaslavichi from Turov/Volhynia, and Polotsk Princes.[citation needed]
The most prominent struggle for power was the conflict that erupted after the death of Yaroslav the Wise. The rivaling Principality of Polotsk was contesting the power of the Grand Prince by occupying Novgorod, while Rostislav Vladimirovich was fighting for the Black Sea port of Tmutarakan belonging to Chernihiv.[citation needed] Three of Yaroslav's sons that first allied together found themselves fighting each other especially after their defeat to the Cuman forces in 1068 at the Battle of the Alta River. At the same time an uprising took place in Kiev, bringing to power Vseslav of Polotsk who supported the traditional Slavic paganism.[citation needed] The ruling Grand Prince Iziaslav fled to Poland asking for support and in couple of years returned to establish the order.[citation needed] The affairs became even more complicated by the end of the 11th century driving the state into chaos and constant warfare. On the initiative of Vladimir II Monomakh in 1097 the first federal council of Kievan Rus took place near Chernihiv]in the city of Liubech with the main intention to find an understanding among the fighting sides. However even though that did not really stop the fighting, it certainly cooled things off.[citation needed]
The decline of Constantinople – a main trading partner of Kievan Rus' – played a significant role in the decline of the Kievan Rus'. The trade route from the Varangians to the Greeks, along which the goods were moving from the Black Sea (mainly Byzantine) through eastern Europe to the Baltic, was a cornerstone of Kiev wealth and prosperity. Kiev was the main power and initiator in this relationship, once the Byzantine Empire fell into turmoil and the supplies became erratic, profits dried out, and Kiev lost its appeal.[citation needed]
The last ruler to maintain united state was Mstislav the Great. After his death in 1132 the Kievan Rus' fell into recession and a rapid decline, and Mstislav's successor Yaropolk II of Kiev instead of focussing on the external threat of the Cumans was embroiled in conflicts with the growing power of the Novgorod Republic. In 1169, as the Kievan Rus' state was full of internal conflict, Andrei Bogolyubsky of Vladimir sacked the city of Kiev. The sack of the city fundamentally changed the perception of Kiev and was evidence of the fragmentation of the Kievan Rus'. By the end of the 12th century, the Kievan state became even further fragmented and had been divided into roughly twelve different principalities.
The Crusades brought a shift in European trade routes that accelerated the decline of Kievan Rus'. In 1204 the forces of the Fourth Crusade sacked Constantinople, making the Dnieper trade route marginal. At the same time the Teutonic Knights (of the Northern Crusades) were conquering the Baltic region and threatening the Lands of Novgorod. Concurrently with it the Ruthenian Federation of Kievan Rus' started to disintegrate into smaller principalities as the Rurik dynasty grew. The local Orthodox Christianity of Kievan Rus', while struggling to establish itself in the predominantly pagan state and losing its main base in Constantinople was on the brink of extinction. Some of the main regional centres that developed later were Novgorod, Chernigov, Galich, Kiev, Ryazan, Vladimir-upon-Klyazma, Vladimir of Volyn and Polotsk.
In the north, the Republic of Novgorod prospered because it controlled trade routes from the River Volga to the Baltic Sea. As Kievan Rus' declined, Novgorod became more independent. A local oligarchy ruled Novgorod; major government decisions were made by a town assembly, which also elected a prince as the city's military leader. In the 12th century, Novgorod acquired its own archbishop Ilya in 1169, a sign of increased importance and political independence, while about 30 years prior to that in 1136 in Novgorod was established a republican form of government - elective monarchy. Since then Novgorod enjoyed a wide degree of autonomy although being closely associated with the Kievan Rus.
In the northeast, Slavs from the Kievan region colonized the territory that later would become the Grand Duchy of Moscow by subjugating and merging with the Finnic tribes already occupying the area. The city of Rostov, the oldest centre of the northeast, was supplanted first by Suzdal and then by the city of Vladimir, which become the capital of Vladimir-Suzdal'. The combined principality of Vladimir-Suzdal asserted itself as a major power in Kievan Rus' in the late 12th century. In 1169 Prince Andrey Bogolyubskiy of Vladimir-Suzdal sacked the city of Kiev and took over the title of the (Великий Князь/Velikiy Knyaz/Grand Prince or Grand Duke) to Vladimir, this way claiming the primacy in Rus'. Prince Andrey then installed his younger brother, who ruled briefly in Kiev while Andrey continued to rule his realm from Suzdal. In 1299, in the wake of the Mongol invasion, the metropolitan moved from Kiev to the city of Vladimir and Vladimir-Suzdal.
To the southwest, the principality of Halych had developed trade relations with its Polish, Hungarian and Lithuanian neighbours and emerged as the local successor to Kievan Rus'. In 1199, Prince Roman Mstislavich united the two previously separate principalities. In 1202 he conquered Kiev, and assumed the title of Grand Duke of Kievan Rus', which was held by the rulers of Vladimir-Suzdal since 1169. His son, Prince Daniil (r. 1238–1264) looked for support from the West. He accepted a crown as a "Rex Rusiae" ("King of Russia") from the Roman papacy, apparently doing so without breaking with Constantinople. In 1370, the patriarch of the Eastern Orthodox Church in Constantinople granted the King of Poland a metropolitan for his Russian subjects. Lithuanian rulers also requested and received a metropolitan for Novagrudok shortly afterwards. Cyprian, a candidate pushed by the Lithuanian rulers, became Metropolitan of Kiev in 1375 and metropolitan of Moscow in 1382; this way the church in the Russian countries was reunited for some time. In 1439, Kiev became the seat of a separate "Metropolitan of Kiev, Galich and all Rus'" for all Greek Orthodox Christians under Polish-Lithuanian rule.
Due to the expansion of trade and its geographical proximity, Kiev became the most important trade centre and chief among the communes; therefore the leader of Kiev gained political "control" over the surrounding areas. This princedom emerged from a coalition of traditional patriarchic family communes banded together in an effort to increase the applicable workforce and expand the productivity of the land. This union developed the first major cities in the Rus' and was the first notable form of self-government. As these communes became larger, the emphasis was taken off the family holdings and placed on the territory that surrounded. This shift in ideology became known as the verv'.
In the 11th century and the 12th century, the princes and their retinues, which were a mixture of Slavic and Scandinavian elites, dominated the society of Kievan Rus'. Leading soldiers and officials received income and land from the princes in return for their political and military services. Kievan society lacked the class institutions and autonomous towns that were typical of Western European feudalism. Nevertheless, urban merchants, artisans and labourers sometimes exercised political influence through a city assembly, the veche (council), which included all the adult males in the population. In some cases, the veche either made agreements with their rulers or expelled them and invited others to take their place. At the bottom of society was a stratum of slaves. More important was a class of tribute-paying peasants, who owed labour duty to the princes. The widespread personal serfdom characteristic of Western Europe did not exist in Kievan Rus'.
The change in political structure led to the inevitable development of the peasant class or smerdy. The smerdy were free un-landed people that found work by labouring for wages on the manors which began to develop around 1031 as the verv' began to dominate socio-political structure. The smerdy were initially given equality in the Kievian law code, they were theoretically equal to the prince, so they enjoyed as much freedom as can be expected of manual labourers. However, in the 13th century they began to slowly lose their rights and became less equal in the eyes of the law.
Kievan Rus', although sparsely populated compared to Western Europe, was not only the largest contemporary European state in terms of area but also culturally advanced. Literacy in Kiev, Novgorod and other large cities was high. As birch bark documents attest, they exchanged love letters and prepared cheat sheets for schools. Novgorod had a sewage system and wood paving not often found in other cities at the time. The Russkaya Pravda confined punishments to fines and generally did not use capital punishment. Certain rights were accorded to women, such as property and inheritance rights.
Kievan Rus' also played an important genealogical role in European politics. Yaroslav the Wise, whose stepmother belonged to the Macedonian dynasty, the greatest one to rule Byzantium, married the only legitimate daughter of the king who Christianized Sweden. His daughters became queens of Hungary, France and Norway, his sons married the daughters of a Polish king and a Byzantine emperor (not to mention a niece of the Pope), while his granddaughters were a German Empress and (according to one theory) the queen of Scotland. A grandson married the only daughter of the last Anglo-Saxon king of England. Thus the Rurikids were a well-connected royal family of the time.
From the 9th century, the Pecheneg nomads began an uneasy relationship with Kievan Rus′. For over two centuries they launched sporadic raids into the lands of Rus′, which sometimes escalated into full-scale wars (such as the 920 war on the Pechenegs by Igor of Kiev reported in the Primary Chronicle), but there were also temporary military alliances (e.g. the 943 Byzantine campaign by Igor). In 968, the Pechenegs attacked and besieged the city of Kiev. Some speculation exists that the Pechenegs drove off the Tivertsi and the Ulichs to the regions of the upper Dniester river in Bukovina. The Byzantine Empire was known to support the Pechenegs in their military campaigns against the Eastern Slavic states.[citation needed]
The exact date of creation of the Kiev Metropolis is uncertain, as well as who was the first leader of the church. Predominantly it is considered that the first head was Michael I of Kiev, however some sources also claim Leontiy who is often placed after Michael or Anastas Chersonesos, became the first bishop of the Church of the Tithes. The first metropolitan to be confirmed by historical sources is Theopemp, who was appointed by Patriarch Alexius of Constantinople in 1038. Before 1015 there were five dioceses: Kiev, Chernihiv, Bilhorod, Volodymyr, Novgorod, and soon thereafter Yuriy-upon-Ros. The Kiev Metropolitan sent his own delegation to the Council of Bari in 1098.
Universal Studios Inc. (also known as Universal Pictures) is an American film studio, owned by Comcast through its wholly owned subsidiary NBCUniversal, and is one of Hollywood's "Big Six" film studios. Its production studios are at 100 Universal City Plaza Drive in Universal City, California. Distribution and other corporate offices are in New York City. Universal Studios is a member of the Motion Picture Association of America (MPAA). Universal was founded in 1912 by the German Carl Laemmle (pronounced "LEM-lee"), Mark Dintenfass, Charles O. Baumann, Adam Kessel, Pat Powers, William Swanson, David Horsley, Robert H. Cochrane, and Jules Brulatour.
It is the world's fourth oldest major film studio, after the renowned French studios Gaumont Film Company and Pathé, and the Danish Nordisk Film company.
Universal Studios was founded by Carl Laemmle, Mark Dintenfass, Charles O. Baumann, Adam Kessel, Pat Powers, William Swanson, David Horsley, Robert H. Cochrane[a] and Jules Brulatour. One story has Laemmle watching a box office for hours, counting patrons and calculating the day's takings. Within weeks of his Chicago trip, Laemmle gave up dry goods to buy the first several nickelodeons. For Laemmle and other such entrepreneurs, the creation in 1908 of the Edison-backed Motion Picture Trust meant that exhibitors were expected to pay fees for Trust-produced films they showed. Based on the Latham Loop used in cameras and projectors, along with other patents, the Trust collected fees on all aspects of movie production and exhibition, and attempted to enforce a monopoly on distribution.
Soon, Laemmle and other disgruntled nickelodeon owners decided to avoid paying Edison by producing their own pictures. In June 1909, Laemmle started the Yankee Film Company with partners Abe Stern and Julius Stern. That company quickly evolved into the Independent Moving Pictures Company (IMP), with studios in Fort Lee, New Jersey, where many early films in America's first motion picture industry were produced in the early 20th century. Laemmle broke with Edison's custom of refusing to give billing and screen credits to performers. By naming the movie stars, he attracted many of the leading players of the time, contributing to the creation of the star system. In 1910, he promoted Florence Lawrence, formerly known as "The Biograph Girl", and actor King Baggot, in what may be the first instance of a studio using stars in its marketing.
The Universal Film Manufacturing Company was incorporated in New York on April 30, 1912. Laemmle, who emerged as president in July 1912, was the primary figure in the partnership with Dintenfass, Baumann, Kessel, Powers, Swanson, Horsley, and Brulatour. Eventually all would be bought out by Laemmle. The new Universal studio was a vertically integrated company, with movie production, distribution and exhibition venues all linked in the same corporate entity, the central element of the Studio system era.
On March 15, 1915,:8 Laemmle opened the world's largest motion picture production facility, Universal City Studios, on a 230-acre (0.9-km²) converted farm just over the Cahuenga Pass from Hollywood. Studio management became the third facet of Universal's operations, with the studio incorporated as a distinct subsidiary organization. Unlike other movie moguls, Laemmle opened his studio to tourists. Universal became the largest studio in Hollywood, and remained so for a decade. However, it sought an audience mostly in small towns, producing mostly inexpensive melodramas, westerns and serials.
In its early years Universal released three brands of feature films — Red Feather, low-budget programmers; Bluebird, more ambitious productions; and Jewel, their prestige motion pictures. Directors included Jack Conway, John Ford, Rex Ingram, Robert Z. Leonard, George Marshall and Lois Weber, one of the few women directing films in Hollywood.:13
Despite Laemmle's role as an innovator, he was an extremely cautious studio chief. Unlike rivals Adolph Zukor, William Fox, and Marcus Loew, Laemmle chose not to develop a theater chain. He also financed all of his own films, refusing to take on debt. This policy nearly bankrupted the studio when actor-director Erich von Stroheim insisted on excessively lavish production values for his films Blind Husbands (1919) and Foolish Wives (1922), but Universal shrewdly gained a return on some of the expenditure by launching a sensational ad campaign that attracted moviegoers. Character actor Lon Chaney became a drawing card for Universal in the 1920s, appearing steadily in dramas. His two biggest hits for Universal were The Hunchback of Notre Dame (1923) and The Phantom of the Opera (1925). During this period Laemmle entrusted most of the production policy decisions to Irving Thalberg. Thalberg had been Laemmle's personal secretary, and Laemmle was impressed by his cogent observations of how efficiently the studio could be operated. Promoted to studio chief, Thalberg was giving Universal's product a touch of class, but MGM's head of production Louis B. Mayer lured Thalberg away from Universal with a promise of better pay. Without his guidance Universal became a second-tier studio, and would remain so for several decades.
In 1926, Universal opened a production unit in Germany, Deutsche Universal-Film AG, under the direction of Joe Pasternak. This unit produced three to four films per year until 1936, migrating to Hungary and then Austria in the face of Hitler's increasing domination of central Europe. With the advent of sound, these productions were made in the German language or, occasionally, Hungarian or Polish. In the U.S., Universal Pictures did not distribute any of this subsidiary's films, but at least some of them were exhibited through other, independent, foreign-language film distributors based in New York, without benefit of English subtitles. Nazi persecution and a change in ownership for the parent Universal Pictures organization resulted in the dissolution of this subsidiary.
In the early years, Universal had a "clean picture" policy. However, by April 1927, Carl Laemmle considered this to be a mistake as "unclean pictures" from other studios were generating more profit while Universal was losing money.
Universal owned the rights to the "Oswald the Lucky Rabbit" character, although Walt Disney and Ub Iwerks had created Oswald, and their films had enjoyed a successful theatrical run. After Charles Mintz had unsuccessfully demanded that Disney accept a lower fee for producing the property, Mintz produced the films with his own group of animators. Instead, Disney and Iwerks created Mickey Mouse who in 1928 stared in the first "sync" sound animated short, Steamboat Willie. This moment effectively launched Walt Disney Studios' foothold, while Universal became a minor player in film animation. Universal subsequently severed its link to Mintz and formed its own in-house animation studio to produce Oswald cartoons headed by Walter Lantz.
In 2006, after almost 80 years, NBC Universal sold all Walt Disney-produced Oswald cartoons, along with the rights to the character himself, back to Disney. In return, Disney released ABC sportscaster Al Michaels from his contract so he could work on NBC's Sunday night NFL football package. However, Universal retained ownership of Oswald cartoons produced for them by Walter Lantz from 1929 to 1943.
In 1928, Laemmle, Sr. made his son, Carl, Jr. head of Universal Pictures as a 21st birthday present. Universal already had a reputation for nepotism—at one time, 70 of Carl, Sr.'s relatives were supposedly on the payroll. Many of them were nephews, resulting in Carl, Sr. being known around the studios as "Uncle Carl." Ogden Nash famously quipped in rhyme, "Uncle Carl Laemmle/Has a very large faemmle." Among these relatives was future Academy Award winning director/producer William Wyler.
"Junior" Laemmle persuaded his father to bring Universal up to date. He bought and built theaters, converted the studio to sound production, and made several forays into high-quality production. His early efforts included the critically mauled part-talkie version of Edna Ferber's novel Show Boat (1929), the lavish musical Broadway (1929) which included Technicolor sequences; and the first all-color musical feature (for Universal), King of Jazz (1930). The more serious All Quiet on the Western Front (1930), won its year's Best Picture Oscar.
Laemmle, Jr. created a niche for the studio, beginning a series of horror films which extended into the 1940s, affectionately dubbed Universal Horror. Among them are Frankenstein (1931), Dracula ( also in 1931), The Mummy (1932) and The Invisible Man (1933). Other Laemmle productions of this period include Imitation of Life (1934) and My Man Godfrey (1936).
Universal's forays into high-quality production spelled the end of the Laemmle era at the studio. Taking on the task of modernizing and upgrading a film conglomerate in the depths of the depression was risky, and for a time Universal slipped into receivership. The theater chain was scrapped, but Carl, Jr. held fast to distribution, studio and production operations.
The end for the Laemmles came with a lavish version of Show Boat (1936), a remake of its earlier 1929 part-talkie production, and produced as a high-quality, big-budget film rather than as a B-picture. The new film featured several stars from the Broadway stage version, which began production in late 1935, and unlike the 1929 film was based on the Broadway musical rather than the novel. Carl, Jr.'s spending habits alarmed company stockholders. They would not allow production to start on Show Boat unless the Laemmles obtained a loan. Universal was forced to seek a $750,000 production loan from the Standard Capital Corporation, pledging the Laemmle family's controlling interest in Universal as collateral. It was the first time Universal had borrowed money for a production in its 26-year history. The production went $300,000 over budget; Standard called in the loan, cash-strapped Universal could not pay, Standard foreclosed and seized control of the studio on April 2, 1936.
Universal's 1936 Show Boat (released a little over a month later) became a critical and financial success, it was not enough to save the Laemmles' involvement with the studio. They were unceremoniously removed from the company they had founded. Because the Laemmles personally oversaw production, Show Boat was released (despite the takeover) with Carl Laemmle and Carl Laemmle Jr.'s names on the credits and in the advertising campaign of the film. Standard Capital's J. Cheever Cowdin had taken over as president and chairman of the board of directors, and instituted severe cuts in production budgets. Gone were the big ambitions, and though Universal had a few big names under contract, those it had been cultivating, like William Wyler and Margaret Sullavan, left.
Meanwhile, producer Joe Pasternak, who had been successfully producing light musicals with young sopranos for Universal's German subsidiary, repeated his formula in America. Teenage singer Deanna Durbin starred in Pasternak's first American film, Three Smart Girls (1936). The film was a box-office hit and reputedly restored the studio's solvency. The success of the film led Universal to offer her a contract, which for the first five years of her career produced her most successful pictures.
When Pasternak stopped producing Durbin's pictures, and she outgrew her screen persona and pursued more dramatic roles, the studio signed 13-year-old Gloria Jean for her own series of Pasternak musicals from 1939; she went on to star with Bing Crosby, W. C. Fields, and Donald O'Connor. A popular Universal film of the late 1930s was Destry Rides Again (1939), starring James Stewart as Destry and Marlene Dietrich in her comeback role after leaving Paramount Studios.
By the early 1940s, the company was concentrating on lower-budget productions that were the company's main staple: westerns, melodramas, serials and sequels to the studio's horror pictures, the latter now solely B pictures. The studio fostered many series: The Dead End Kids and Little Tough Guys action features and serials (1938–43); the comic adventures of infant Baby Sandy (1938–41); comedies with Hugh Herbert (1938–42) and The Ritz Brothers (1940–43); musicals with Robert Paige, Jane Frazee, The Andrews Sisters, and The Merry Macs (1938–45); and westerns with Tom Mix (1932–33), Buck Jones (1933–36), Bob Baker (1938–39), Johnny Mack Brown (1938–43); Rod Cameron (1944–45), and Kirby Grant (1946–47).
Universal could seldom afford its own stable of stars, and often borrowed talent from other studios, or hired freelance actors. In addition to Stewart and Dietrich, Margaret Sullavan, and Bing Crosby were two of the major names that made a couple of pictures for Universal during this period. Some stars came from radio, including Edgar Bergen, W. C. Fields, and the comedy team of Abbott and Costello (Bud Abbott and Lou Costello). Abbott and Costello's military comedy Buck Privates (1941) gave the former burlesque comedians a national and international profile.
During the war years Universal did have a co-production arrangement with producer Walter Wanger and his partner, director Fritz Lang, lending the studio some amount of prestige productions. Universal's core audience base was still found in the neighborhood movie theaters, and the studio continued to please the public with low- to medium-budget films. Basil Rathbone and Nigel Bruce in new Sherlock Holmes mysteries (1942–46), teenage musicals with Gloria Jean, Donald O'Connor, and Peggy Ryan (1942–43), and screen adaptations of radio's Inner Sanctum Mysteries with Lon Chaney, Jr. (1943–45). Alfred Hitchcock was also borrowed for two films from Selznick International Pictures: Saboteur (1942) and Shadow of a Doubt (1943).
As Universal's main product had always been low-budget film, it was one of the last major studios to have a contract with Technicolor. The studio did not make use of the three-strip Technicolor process until Arabian Nights (1942), starring Jon Hall and Maria Montez. The following year, Technicolor was also used in Universal's remake of their 1925 horror melodrama, Phantom of the Opera with Claude Rains and Nelson Eddy. With the success of their first two pictures, a regular schedule of high-budget, Technicolor films followed.
In 1945, the British entrepreneur J. Arthur Rank, hoping to expand his American presence, bought into a four-way merger with Universal, the independent company International Pictures, and producer Kenneth Young. The new combine, United World Pictures, was a failure and was dissolved within one year. Rank and International remained interested in Universal, however, culminating in the studio's reorganization as Universal-International. William Goetz, a founder of International, was made head of production at the renamed Universal-International Pictures Inc., which also served as an import-export subsidiary, and copyright holder for the production arm's films. Goetz, a son-in-law of Louis B. Mayer decided to bring "prestige" to the new company. He stopped the studio's low-budget production of B movies, serials and curtailed Universal's horror and "Arabian Nights" cycles. Distribution and copyright control remained under the name of Universal Pictures Company Inc.
Goetz set out an ambitious schedule. Universal-International became responsible for the American distribution of Rank's British productions, including such classics as David Lean's Great Expectations (1946) and Laurence Olivier's Hamlet (1948). Broadening its scope further, Universal-International branched out into the lucrative non-theatrical field, buying a majority stake in home-movie dealer Castle Films in 1947, and taking the company over entirely in 1951. For three decades, Castle would offer "highlights" reels from the Universal film library to home-movie enthusiasts and collectors. Goetz licensed Universal's pre–Universal-International film library to Jack Broeder's Realart Pictures for cinema re-release but Realart was not allowed to show the films on television.
The production arm of the studio still struggled. While there were to be a few hits like The Killers (1946) and The Naked City (1948), Universal-International's new theatrical films often met with disappointing response at the box office. By the late 1940s, Goetz was out, and the studio returned to low-budget films. The inexpensive Francis (1950), the first film of a series about a talking mule and Ma and Pa Kettle (1949), part of a series, became mainstays of the company. Once again, the films of Abbott and Costello, including Abbott and Costello Meet Frankenstein (1948), were among the studio's top-grossing productions. But at this point Rank lost interest and sold his shares to the investor Milton Rackmil, whose Decca Records would take full control of Universal in 1952. Besides Abbott and Costello, the studio retained the Walter Lantz cartoon studio, whose product was released with Universal-International's films.
In the 1950s, Universal-International resumed their series of Arabian Nights films, many starring Tony Curtis. The studio also had a success with monster and science fiction films produced by William Alland, with many directed by Jack Arnold. Other successes were the melodramas directed by Douglas Sirk and produced by Ross Hunter, although for film critics they were not so well thought of on first release as they have since become. Among Universal-International's stable of stars were Rock Hudson, Tony Curtis, Jeff Chandler, Audie Murphy, and John Gavin.
Though Decca would continue to keep picture budgets lean, it was favored by changing circumstances in the film business, as other studios let their contract actors go in the wake of the 1948 U.S. vs. Paramount Pictures, et al. decision. Leading actors were increasingly free to work where and when they chose, and in 1950 MCA agent Lew Wasserman made a deal with Universal for his client James Stewart that would change the rules of the business. Wasserman's deal gave Stewart a share in the profits of three pictures in lieu of a large salary. When one of those films, Winchester '73, proved to be a hit, the arrangement would become the rule for many future productions at Universal, and eventually at other studios as well.
By the late 1950s, the motion picture business was again changing. The combination of the studio/theater-chain break-up and the rise of television saw the reduced audience size for cinema productions. The Music Corporation of America (MCA), then predominately a talent agency, had also become a powerful television producer, renting space at Republic Studios for its Revue Productions subsidiary. After a period of complete shutdown, a moribund Universal agreed to sell its 360-acre (1.5 km²) studio lot to MCA in 1958, for $11 million, renamed Revue Studios. MCA owned the studio lot, but not Universal Pictures, yet was increasingly influential on Universal's product. The studio lot was upgraded and modernized, while MCA clients like Doris Day, Lana Turner, Cary Grant, and director Alfred Hitchcock were signed to Universal Pictures contracts.
The long-awaited takeover of Universal Pictures by MCA, Inc. happened in mid-1962 as part of the MCA-Decca Records merger. The company reverted in name to Universal Pictures. As a final gesture before leaving the talent agency business, virtually every MCA client was signed to a Universal contract. In 1964 MCA formed Universal City Studios, Inc., merging the motion pictures and television arms of Universal Pictures Company and Revue Productions (officially renamed as Universal Television in 1966). And so, with MCA in charge, Universal became a full-blown, A-film movie studio, with leading actors and directors under contract; offering slick, commercial films; and a studio tour subsidiary launched in 1964. Television production made up much of the studio's output, with Universal heavily committed, in particular, to deals with NBC (which later merged with Universal to form NBC Universal; see below) providing up to half of all prime time shows for several seasons. An innovation during this period championed by Universal was the made-for-television movie.
At this time, Hal B. Wallis, who had latterly worked as a major producer at Paramount, moved over to Universal, where he produced several films, among them a lavish version of Maxwell Anderson's Anne of the Thousand Days (1969), and the equally lavish Mary, Queen of Scots (1971). Though neither could claim to be a big financial hit, both films received Academy Award nominations, and Anne was nominated for Best Picture, Best Actor (Richard Burton), Best Actress (Geneviève Bujold), and Best Supporting Actor (Anthony Quayle). Wallis retired from Universal after making the film Rooster Cogburn (1975), a sequel to True Grit (1969), which Wallis had produced at Paramount. Rooster Cogburn co-starred John Wayne, reprising his Oscar-winning role from the earlier film, and Katharine Hepburn, their only film together. The film was only a moderate success.
In the early 1970s, Universal teamed up with Paramount Pictures to form Cinema International Corporation, which distributed films by Paramount and Universal worldwide. Though Universal did produce occasional hits, among them Airport (1970), The Sting (1973), American Graffiti (also 1973), Earthquake (1974), and a big box-office success which restored the company's fortunes: Jaws (1975), Universal during the decade was primarily a television studio. When Metro-Goldwyn-Mayer purchased United Artists in 1981, MGM could not drop out of the CIC venture to merge with United Artists overseas operations. However, with future film productions from both names being released through the MGM/UA Entertainment plate, CIC decided to merge UA's international units with MGM and reformed as United International Pictures. There would be other film hits like E.T. the Extra-Terrestrial (1982), Back to the Future (1985), Field of Dreams (1989), and Jurassic Park (1993), but the film business was financially unpredictable. UIP began distributing films by start-up studio DreamWorks in 1997, due to connections the founders have with Paramount, Universal, and Amblin Entertainment. In 2001, MGM dropped out of the UIP venture, and went with 20th Century Fox's international arm to handle distribution of their titles to this day.
Anxious to expand the company's broadcast and cable presence, longtime MCA head Lew Wasserman sought a rich partner. He located Japanese electronics manufacturer Matsushita Electric (now known as Panasonic), which agreed to acquire MCA for $6.6 billion in 1990. Meanwhile, around this time, the production subsidiary was renamed Universal Studios Inc., and (in 1990) MCA created MCA/Universal Home Video Inc. for the VHS video cassette (later DVD) sales industry.
Matsushita provided a cash infusion, but the clash of cultures was too great to overcome, and five years later Matsushita sold an 80% stake in MCA/Universal to Canadian drinks distributor Seagram for $5.7 billion. Seagram sold off its stake in DuPont to fund this expansion into the entertainment industry. Hoping to build an entertainment empire around Universal, Seagram bought PolyGram in 1999 and other entertainment properties, but the fluctuating profits characteristic of Hollywood were no substitute for the reliable income stream gained from the previously held shares in DuPont.
To raise money, Seagram head Edgar Bronfman Jr. sold Universal's television holdings, including cable network USA, to Barry Diller (these same properties would be bought back later at greatly inflated prices). In June 2000, Seagram was sold to French water utility and media company Vivendi, which owned StudioCanal; the conglomerate then became known as Vivendi Universal. Afterward, Universal Pictures acquired the United States distribution rights of several of StudioCanal's films, such as Mulholland Drive (which received an Oscar nomination) and Brotherhood of the Wolf (which became the second-highest-grossing French-language film in the United States since 1980). Universal Pictures and StudioCanal also co-produced several films, such as Love Actually (an $40 million-budgeted film that eventually grossed $246 million worldwide). In late 2000, the New York Film Academy was permitted to use the Universal Studios backlot for student film projects in an unofficial partnership.
Burdened with debt, in 2004 Vivendi Universal sold 80% of Vivendi Universal Entertainment (including the studio and theme parks) to General Electric, parent of NBC. The resulting media super-conglomerate was renamed NBCUniversal, while Universal Studios Inc. remained the name of the production subsidiary. After that deal, GE owned 80% of NBC Universal; Vivendi held the remaining 20%, with an option to sell its share in 2006. GE purchased Vivendi's share in NBCU in 2011 and in turn sold 51% of the company to cable provider Comcast. Comcast merged the former GE subsidiary with its own cable-television programming assets, creating the current NBCUniversal. Following Federal Communications Commission (FCC) approval, the Comcast-GE deal was closed on Jan 29, 2011. In March 2013, Comcast bought the remaining 49% of NBCUniversal for $16.7 billion.
In late 2005, Viacom's Paramount Pictures acquired DreamWorks SKG after acquisition talks between GE and DreamWorks stalled. Universal's long time chairperson, Stacey Snider, left the company in early 2006 to head up DreamWorks. Snider was replaced by then-Vice Chairman Marc Shmuger and Focus Features head David Linde. On October 5, 2009, Marc Shmuger and David Linde were ousted and their co-chairperson jobs consolidated under former president of worldwide marketing and distribution Adam Fogelson becoming the single chairperson. Donna Langley was also upped to co-chairperson. In 2009, Stephanie Sperber founded Universal Partnerships & Licensing within Universal to license consumer products for Universal. In September 2013, Adam Fogelson was ousted as co-chairman of Universal Pictures, promoting Donna Langley to sole-chairman. In addition, NBCUniversal International Chairman, Jeff Shell, would be appointed as Chairman of the newly created Filmed Entertainment Group. Longtime studio head Ron Meyer would give up oversight of the film studio and appointed Vice Chairman of NBCUniversal, providing consultation to CEO Steve Burke on all of the company's operations. Meyers still retains oversight of Universal Parks and Resorts.
Universal's multi-year film financing deal with Elliott Management expired in 2013. In July 2013, Universal made an agreement with Legendary Pictures to market, co-finance, and distribute Legendary's films for five years starting in 2014, the year that Legendary's similar agreement with Warner Bros. expires.
In June 2014, Universal Partnerships took over licensing consumer products for NBC and Sprout with expectation that all licensing would eventually be centralized within NBCUniversal. In May 2015, Gramercy Pictures was revived by Focus Features as a genre label, that concentrated on action, sci-fi, and horror films.
As of 2015, Universal is the only studio to have released three billion-dollar films in one year; this distinction was achieved in 2015 with Furious 7, Jurassic World and Minions.
In the early 1950s, Universal set up its own distribution company in France, and in the late 1960s, the company also started a production company in Paris, Universal Productions France S.A., although sometimes credited by the name of the distribution company, Universal Pictures France. Except for the two first films it produced, Claude Chabrol's Le scandale (English title The Champagne Murders) and Romain Gary's Les oiseaux vont mourir au Pérou (English title Birds in Peru), it was only involved in French or other European co-productions, the most noticeable ones being Louis Malle's Lacombe, Lucien, Bertrand Blier's Les Valseuses (English title Going Places), and Fred Zinnemann's The Day of the Jackal. It was only involved in approximately 20 French film productions. In the early 1970s, the unit was incorporated into the French Cinema International Corporation arm.
The motif of the England national football team has three lions passant guardant, the emblem of King Richard I, who reigned from 1189 to 1199. The lions, often blue, have had minor changes to colour and appearance. Initially topped by a crown, this was removed in 1949 when the FA was given an official coat of arms by the College of Arms; this introduced ten Tudor roses, one for each of the regional branches of the FA. Since 2003, England top their logo with a star to recognise their World Cup win in 1966; this was first embroidered onto the left sleeve of the home kit, and a year later was moved to its current position, first on the away shirt.
Although England's first away kits were blue, England's traditional away colours are red shirts, white shorts and red socks. In 1996, England's away kit was changed to grey shirts, shorts and socks. This kit was only worn three times, including against Germany in the semi-final of Euro 96 but the deviation from the traditional red was unpopular with supporters and the England away kit remained red until 2011, when a navy blue away kit was introduced. The away kit is also sometimes worn during home matches, when a new edition has been released to promote it.
England failed to qualify for the World Cup in 1974, 1978 and 1994. The team's earliest exit in the competition itself was its elimination in the first round in 1950, 1958 and most recently in the 2014 FIFA World Cup, after being defeated in both their opening two matches for the first time, versus Italy and Uruguay in Group D. In 1950, four teams remained after the first round, in 1958 eight teams remained and in 2014 sixteen teams remained. In 2010, England suffered its most resounding World Cup defeat (4–1 to Germany) in the Round of 16, after drawing with the United States and Algeria and defeating Slovenia 1–0 in the group stage.
Their first ever defeat on home soil to a foreign team was an 0–2 loss to the Republic of Ireland, on 21 September 1949 at Goodison Park. A 6–3 loss in 1953 to Hungary, was their second defeat by a foreign team at Wembley. In the return match in Budapest, Hungary won 7–1. This still stands as England's worst ever defeat. After the game, a bewildered Syd Owen said, "it was like playing men from outer space". In the 1954 FIFA World Cup, England reached the quarter-finals for the first time, and lost 4–2 to reigning champions Uruguay.
In February 2012, Capello resigned from his role as England manager, following a disagreement with the FA over their request to remove John Terry from team captaincy after accusations of racial abuse concerning the player. Following this, there was media speculation that Harry Redknapp would take the job. However, on 1 May 2012, Roy Hodgson was announced as the new manager, just six weeks before UEFA Euro 2012. England managed to finish top of their group, winning two and drawing one of their fixtures, but exited the Championships in the quarter-finals via a penalty shoot-out, this time to Italy.
The England national football team represents England and the Crown Dependencies of Jersey, Guernsey and the Isle of Man for football matches as part of FIFA-authorised events, and is controlled by The Football Association, the governing body for football in England. England are one of the two oldest national teams in football; alongside Scotland, whom they played in the world's first international football match in 1872. England's home ground is Wembley Stadium, London, and the current team manager is Roy Hodgson.
To begin with, England had no permanent home stadium. They joined FIFA in 1906 and played their first ever games against countries other than the Home Nations on a tour of Central Europe in 1908. Wembley Stadium was opened in 1923 and became their home ground. The relationship between England and FIFA became strained, and this resulted in their departure from FIFA in 1928, before they rejoined in 1946. As a result, they did not compete in a World Cup until 1950, in which they were beaten in a 1–0 defeat by the United States, failing to get past the first round in one of the most embarrassing defeats in the team's history.
England is quite a successful nation at the UEFA European Football Championship, having finished in third place in 1968 and reached the semi-final in 1996. England hosted Euro 96 and have appeared in eight UEFA European Championship Finals tournaments, tied for ninth-best. The team has also reached the quarter-final on two recent occasions in 2004 and 2012. The team's worst result in the competition was a first-round elimination in 1980, 1988, 1992 and 2000. The team did not enter in 1960, and they failed to qualify in 1964, 1972, 1976, 1984, and 2008.
England qualified for the 1970 FIFA World Cup in Mexico as reigning champions, and reached the quarter-finals, where they were knocked out by West Germany. England had been 2–0 up, but were eventually beaten 3–2 after extra time. They failed in qualification for the 1974, leading to Ramsey's dismissal, and 1978 FIFA World Cups. Under Ron Greenwood, they managed to qualify for the 1982 FIFA World Cup in Spain (the first time competitively since 1962); despite not losing a game, they were eliminated in the second group stage.
Sven-Göran Eriksson took charge of the team between 2001 and 2006, and was the first non–English manager of England. Despite controversial press coverage of his personal life, Eriksson was consistently popular with the majority of fans.[citation needed] He guided England to the quarter-finals of the 2002 FIFA World Cup, UEFA Euro 2004, and the 2006 FIFA World Cup. He lost only five competitive matches during his tenure, and England rose to an No.4 world ranking under his guidance. His contract was extended by the Football Association by two years, to include UEFA Euro 2008. However, it was terminated by them at the 2006 FIFA World Cup's conclusion.
All England matches are broadcast with full commentary on BBC Radio 5 Live. From the 2008–09 season until the 2017–18 season, England's home and away qualifiers, and friendlies both home and away are broadcast live on ITV (often with the exception of STV, the ITV affiliate in central and northern Scotland). England's away qualifiers for the 2010 World Cup were shown on Setanta Sports until that company's collapse. As a result of Setanta Sports's demise, England's World Cup qualifier in Ukraine on 10 October 2009 was shown in the United Kingdom on a pay-per-view basis via the internet only. This one-off event was the first time an England game had been screened in such a way. The number of subscribers, paying between £4.99 and £11.99 each, was estimated at between 250,000 and 300,000 and the total number of viewers at around 500,000.
England first appeared at the 1950 FIFA World Cup and have appeared in 14 FIFA World Cups, they are tied for sixth-best in terms of number of wins alongside France and Spain. The national team is one of eight national teams to have won at least one FIFA World Cup title. The England team won their first and only World Cup title in 1966. The tournament was played on home soil and England defeated Germany 4–2 in the final. In 1990, England finished in fourth place, losing 2–1 to host nation Italy in the third place play-off after losing on penalties to champions Germany in the semi-final. The team has also reached the quarter-final on two recent occasions in 2002 and 2006. Previously, they reached this stage in 1954, 1962, 1970 and 1986.
Armenia is a unitary, multi-party, democratic nation-state with an ancient cultural heritage. Urartu was established in 860 BC and by the 6th century BC it was replaced by the Satrapy of Armenia. In the 1st century BC the Kingdom of Armenia reached its height under Tigranes the Great. Armenia became the first state in the world to adopt Christianity as its official religion. In between the late 3rd century to early years of the 4th century, the state became the first Christian nation. The official date of state adoption of Christianity is 301 AD. The ancient Armenian kingdom was split between the Byzantine and Sasanian Empires around the early 5th century.
Between the 16th century and 19th century, the traditional Armenian homeland composed of Eastern Armenia and Western Armenia came under the rule of the Ottoman and successive Iranian empires, repeatedly ruled by either of the two over the centuries. By the 19th century, Eastern Armenia had been conquered by the Russian Empire, while most of the western parts of the traditional Armenian homeland remained under Ottoman rule. During World War I, Armenians living in their ancestral lands in the Ottoman Empire were systematically exterminated in the Armenian Genocide. In 1918, after the Russian Revolution, all non-Russian countries declared their independence from the Russian empire, leading to the establishment of the First Republic of Armenia. By 1920, the state was incorporated into the Transcaucasian Socialist Federative Soviet Republic, and in 1922 became a founding member of the Soviet Union. In 1936, the Transcaucasian state was dissolved, transforming its constituent states, including the Armenian Soviet Socialist Republic, into full Union republics. The modern Republic of Armenia became independent in 1991 during the dissolution of the Soviet Union.
The exonym Armenia is attested in the Old Persian Behistun Inscription (515 BC) as Armina (    ). The ancient Greek terms Ἀρμενία (Armenía) and Ἀρμένιοι (Arménioi, "Armenians") are first mentioned by Hecataeus of Miletus (c. 550 BC – c. 476 BC). Xenophon, a Greek general serving in some of the Persian expeditions, describes many aspects of Armenian village life and hospitality in around 401 BC. He relates that the people spoke a language that to his ear sounded like the language of the Persians. According to the histories of both Moses of Chorene and Michael Chamchian, Armenia derives from the name of Aram, a lineal descendant of Hayk.
Several bronze-era states flourished in the area of Greater Armenia, including the Hittite Empire (at the height of its power), Mitanni (South-Western historical Armenia), and Hayasa-Azzi (1500–1200 BC). The Nairi people (12th to 9th centuries BC) and the Kingdom of Urartu (1000–600 BC) successively established their sovereignty over the Armenian Highland. Each of the aforementioned nations and tribes participated in the ethnogenesis of the Armenian people. A large cuneiform lapidary inscription found in Yerevan established that the modern capital of Armenia was founded in the summer of 782 BC by King Argishti I. Yerevan is the world's oldest city to have documented the exact date of its foundation.
During the late 6th century BC, the first geographical entity that was called Armenia by neighboring populations was established under the Orontid Dynasty within the Achaemenid Empire, as part of the latters' territories. The kingdom became fully sovereign from the sphere of influence of the Seleucid Empire in 190 BC under King Artaxias I and begun the rule of the Artaxiad dynasty. Armenia reached its height between 95 and 66 BC under Tigranes the Great, becoming the most powerful kingdom of its time east of the Roman Republic.
In the next centuries, Armenia was in the Persian Empire's sphere of influence during the reign of Tiridates I, the founder of the Arsacid dynasty of Armenia, which itself was a branch of the eponymous Arsacid dynasty of Parthia. Throughout its history, the kingdom of Armenia enjoyed both periods of independence and periods of autonomy subject to contemporary empires. Its strategic location between two continents has subjected it to invasions by many peoples, including the Assyrians (under Ashurbanipal, at around 669–627 BC, the boundaries of the Assyrian Empire reached as far as Armenia & the Caucasus Mountains), Medes, Achaemenid Persians, Greeks, Parthians, Romans, Sassanid Persians, Byzantines, Arabs, Seljuks, Mongols, Ottomans, successive Iranian Safavids, Afsharids, and Qajars, and the Russians.
After the Marzpanate period (428–636), Armenia emerged as the Emirate of Armenia, an autonomous principality within the Arabic Empire, reuniting Armenian lands previously taken by the Byzantine Empire as well. The principality was ruled by the Prince of Armenia, and recognized by the Caliph and the Byzantine Emperor. It was part of the administrative division/emirate Arminiya created by the Arabs, which also included parts of Georgia and Caucasian Albania, and had its center in the Armenian city, Dvin. The Principality of Armenia lasted until 884, when it regained its independence from the weakened Arab Empire under King Ashot I Bagratuni.
In 1045, the Byzantine Empire conquered Bagratid Armenia. Soon, the other Armenian states fell under Byzantine control as well. The Byzantine rule was short lived, as in 1071 Seljuk Turks defeated the Byzantines and conquered Armenia at the Battle of Manzikert, establishing the Seljuk Empire. To escape death or servitude at the hands of those who had assassinated his relative, Gagik II, King of Ani, an Armenian named Roupen, went with some of his countrymen into the gorges of the Taurus Mountains and then into Tarsus of Cilicia. The Byzantine governor of the palace gave them shelter where the Armenian Kingdom of Cilicia was eventually established on 6 January 1198 under King Leo I, a descendant of Prince Roupen.
The Seljuk Empire soon started to collapse. In the early 12th century, Armenian princes of the Zakarid noble family drove out the Seljuk Turks and established a semi-independent Armenian principality in Northern and Eastern Armenia, known as Zakarid Armenia, which lasted under the patronage of the Georgian Kingdom. The noble family of Orbelians shared control with the Zakarids in various parts of the country, especially in Syunik and Vayots Dzor, while the Armenian family of Hasan-Jalalians controlled provinces of Artsakh and Utik as the Kingdom of Artsakh.
In the 16th century, the Ottoman Empire and Safavid Empire divided Armenia. From the early 16th century, both Western Armenia and Eastern Armenia fell under Iranian Safavid rule. Owing to the century long Turco-Iranian geo-political rivalry that would last in Western Asia, significant parts of the region were frequently fought over between the two rivalling empires. From the mid 16th century with the Peace of Amasya, and decisively from the first half of the 17th century with the Treaty of Zuhab until the first half of the 19th century, Eastern Armenia was ruled by the successive Iranian Safavid, Afsharid and Qajar empires, while Western Armenia remained under Ottoman rule.
While Western Armenia still remained under Ottoman rule, the Armenians were granted considerable autonomy within their own enclaves and lived in relative harmony with other groups in the empire (including the ruling Turks). However, as Christians under a strict Muslim social system, Armenians faced pervasive discrimination. When they began pushing for more rights within the Ottoman Empire, Sultan ‘Abdu’l-Hamid II, in response, organized state-sponsored massacres against the Armenians between 1894 and 1896, resulting in an estimated death toll of 80,000 to 300,000 people. The Hamidian massacres, as they came to be known, gave Hamid international infamy as the "Red Sultan" or "Bloody Sultan."
During the 1890s, the Armenian Revolutionary Federation, commonly known as Dashnaktsutyun, became active within the Ottoman Empire with the aim of unifying the various small groups in the empire that were advocating for reform and defending Armenian villages from massacres that were widespread in some of the Armenian-populated areas of the empire. Dashnaktsutyun members also formed fedayi groups that defended Armenian civilians through armed resistance. The Dashnaks also worked for the wider goal of creating a "free, independent and unified" Armenia, although they sometimes set aside this goal in favor of a more realistic approach, such as advocating autonomy.
The Ottoman Empire began to collapse, and in 1908, the Young Turk Revolution overthrew the government of Sultan Hamid. In April 1909, the Adana massacre occurred in the Adana Vilayet of the Ottoman Empire resulting in the deaths of as many as 20,000–30,000 Armenians. The Armenians living in the empire hoped that the Committee of Union and Progress would change their second-class status. Armenian reform package (1914) was presented as a solution by appointing an inspector general over Armenian issues.
When World War I broke out leading to confrontation between the Ottoman Empire and the Russian Empire in the Caucasus and Persian Campaigns, the new government in Istanbul began to look on the Armenians with distrust and suspicion. This was because the Imperial Russian Army contained a contingent of Armenian volunteers. On 24 April 1915, Armenian intellectuals were arrested by Ottoman authorities and, with the Tehcir Law (29 May 1915), eventually a large proportion of Armenians living in Anatolia perished in what has become known as the Armenian Genocide.
The genocide was implemented in two phases: the wholesale killing of the able-bodied male population through massacre and subjection of army conscripts to forced labour, followed by the deportation of women, children, the elderly and infirm on death marches leading to the Syrian desert. Driven forward by military escorts, the deportees were deprived of food and water and subjected to periodic robbery, rape, and massacre. There was local Armenian resistance in the region, developed against the activities of the Ottoman Empire. The events of 1915 to 1917 are regarded by Armenians and the vast majority of Western historians to have been state-sponsored mass killings, or genocide.
Turkish authorities deny the genocide took place to this day. The Armenian Genocide is acknowledged to have been one of the first modern genocides. According to the research conducted by Arnold J. Toynbee, an estimated 600,000 Armenians died during deportation from 1915–16). This figure, however, accounts for solely the first year of the Genocide and does not take into account those who died or were killed after the report was compiled on the 24th May 1916. The International Association of Genocide Scholars places the death toll at "more than a million". The total number of people killed has been most widely estimated at between 1 and 1.5 million.
Although the Russian Caucasus Army of Imperial forces commanded by Nikolai Yudenich and Armenians in volunteer units and Armenian militia led by Andranik Ozanian and Tovmas Nazarbekian succeeded in gaining most of Ottoman Armenia during World War I, their gains were lost with the Bolshevik Revolution of 1917.[citation needed] At the time, Russian-controlled Eastern Armenia, Georgia, and Azerbaijan attempted to bond together in the Transcaucasian Democratic Federative Republic. This federation, however, lasted from only February to May 1918, when all three parties decided to dissolve it. As a result, the Dashnaktsutyun government of Eastern Armenia declared its independence on 28 May as the First Republic of Armenia under the leadership of Aram Manukian.
At the end of the war, the victorious powers sought to divide up the Ottoman Empire. Signed between the Allied and Associated Powers and Ottoman Empire at Sèvres on 10 August 1920, the Treaty of Sèvres promised to maintain the existence of the Armenian republic and to attach the former territories of Ottoman Armenia to it. Because the new borders of Armenia were to be drawn by United States President Woodrow Wilson, Ottoman Armenia was also referred to as "Wilsonian Armenia." In addition, just days prior, on 5 August 1920, Mihran Damadian of the Armenian National Union, the de facto Armenian administration in Cilicia, declared the independence of Cilicia as an Armenian autonomous republic under French protectorate.
In 1920, Turkish nationalist forces invaded the fledgling Armenian republic from the east. Turkish forces under the command of Kazım Karabekir captured Armenian territories that Russia had annexed in the aftermath of the 1877–1878 Russo-Turkish War and occupied the old city of Alexandropol (present-day Gyumri). The violent conflict finally concluded with the Treaty of Alexandropol on 2 December 1920. The treaty forced Armenia to disarm most of its military forces, cede all former Ottoman territory granted to it by the Treaty of Sèvres, and to give up all the "Wilsonian Armenia" granted to it at the Sèvres treaty. Simultaneously, the Soviet Eleventh Army, under the command of Grigoriy Ordzhonikidze, invaded Armenia at Karavansarai (present-day Ijevan) on 29 November. By 4 December, Ordzhonikidze's forces entered Yerevan and the short-lived Armenian republic collapsed.
Armenia was annexed by Bolshevist Russia and along with Georgia and Azerbaijan, it was incorporated into the Soviet Union as part of the Transcaucasian SFSR (TSFSR) on 4 March 1922. With this annexation, the Treaty of Alexandropol was superseded by the Turkish-Soviet Treaty of Kars. In the agreement, Turkey allowed the Soviet Union to assume control over Adjara with the port city of Batumi in return for sovereignty over the cities of Kars, Ardahan, and Iğdır, all of which were part of Russian Armenia.
The TSFSR existed from 1922 to 1936, when it was divided up into three separate entities (Armenian SSR, Azerbaijan SSR, and Georgian SSR). Armenians enjoyed a period of relative stability under Soviet rule. They received medicine, food, and other provisions from Moscow, and communist rule proved to be a soothing balm in contrast to the turbulent final years of the Ottoman Empire. The situation was difficult for the church, which struggled under Soviet rule. After the death of Vladimir Lenin, Joseph Stalin took the reins of power and began an era of renewed fear and terror for Armenians.
Fears decreased when Stalin died in 1953 and Nikita Khruschev emerged as the Soviet Union's new leader. Soon, life in Soviet Armenia began to see rapid improvement. The church, which suffered greatly under Stalin, was revived when Catholicos Vazgen I assumed the duties of his office in 1955. In 1967, a memorial to the victims of the Armenian Genocide was built at the Tsitsernakaberd hill above the Hrazdan gorge in Yerevan. This occurred after mass demonstrations took place on the tragic event's fiftieth anniversary in 1965.
During the Gorbachev era of the 1980s, with the reforms of Glasnost and Perestroika, Armenians began to demand better environmental care for their country, opposing the pollution that Soviet-built factories brought. Tensions also developed between Soviet Azerbaijan and its autonomous district of Nagorno-Karabakh, a majority-Armenian region separated by Stalin from Armenia in 1923. About 484,000 Armenians lived in Azerbaijan in 1970. The Armenians of Karabakh demanded unification with Soviet Armenia. Peaceful protests in Yerevan supporting the Karabakh Armenians were met with anti-Armenian pogroms in the Azerbaijani city of Sumgait. Compounding Armenia's problems was a devastating earthquake in 1988 with a moment magnitude of 7.2.
Gorbachev's inability to alleviate any of Armenia's problems created disillusionment among the Armenians and fed a growing hunger for independence. In May 1990, the New Armenian Army (NAA) was established, serving as a defence force separate from the Soviet Red Army. Clashes soon broke out between the NAA and Soviet Internal Security Forces (MVD) troops based in Yerevan when Armenians decided to commemorate the establishment of the 1918 First Republic of Armenia. The violence resulted in the deaths of five Armenians killed in a shootout with the MVD at the railway station. Witnesses there claimed that the MVD used excessive force and that they had instigated the fighting.
Further firefights between Armenian militiamen and Soviet troops occurred in Sovetashen, near the capital and resulted in the deaths of over 26 people, mostly Armenians. The pogrom of Armenians in Baku in January 1990 forced almost all of the 200,000 Armenians in the Azerbaijani capital Baku to flee to Armenia. On 17 March 1991, Armenia, along with the Baltic states, Georgia and Moldova, boycotted a nationwide referendum in which 78% of all voters voted for the retention of the Soviet Union in a reformed form.
Ter-Petrosyan led Armenia alongside Defense Minister Vazgen Sargsyan through the Nagorno-Karabakh War with neighboring Azerbaijan. The initial post-Soviet years were marred by economic difficulties, which had their roots early in the Karabakh conflict when the Azerbaijani Popular Front managed to pressure the Azerbaijan SSR to instigate a railway and air blockade against Armenia. This move effectively crippled Armenia's economy as 85% of its cargo and goods arrived through rail traffic. In 1993, Turkey joined the blockade against Armenia in support of Azerbaijan.
The Karabakh war ended after a Russian-brokered cease-fire was put in place in 1994. The war was a success for the Karabakh Armenian forces who managed to capture 16% of Azerbaijan's internationally recognised territory including Nagorno-Karabakh itself. Since then, Armenia and Azerbaijan have held peace talks, mediated by the Organisation for Security and Co-operation in Europe (OSCE). The status of Karabakh has yet to be determined. The economies of both countries have been hurt in the absence of a complete resolution and Armenia's borders with Turkey and Azerbaijan remain closed. By the time both Azerbaijan and Armenia had finally agreed to a ceasefire in 1994, an estimated 30,000 people had been killed and over a million had been displaced.
International observers of Council of Europe and US Department of State have questioned the fairness of Armenia's parliamentary and presidential elections and constitutional referendum since 1995, citing polling deficiencies, lack of cooperation by the Electoral Commission, and poor maintenance of electoral lists and polling places. Freedom House categorized Armenia in its 2008 report as a "Semi-consolidated Authoritarian Regime" (along with Moldova, Kosovo, Kyrgyzstan, and Russia) and ranked Armenia 20th among 29 nations in transition, with a Democracy Score of 5.21 out of 7 (7 represents the lowest democratic progress).
Armenia presently maintains good relations with almost every country in the world, with two major exceptions being its immediate neighbours, Turkey and Azerbaijan. Tensions were running high between Armenians and Azerbaijanis during the final years of the Soviet Union. The Nagorno-Karabakh War dominated the region's politics throughout the 1990s. The border between the two rival countries remains closed up to this day, and a permanent solution for the conflict has not been reached despite the mediation provided by organisations such as the OSCE.
Turkey also has a long history of poor relations with Armenia over its refusal to acknowledge the Armenian Genocide. Turkey was one of the first countries to recognize the Republic of Armenia (the 3rd republic) after its independence from the USSR in 1991. Despite this, for most of the 20th century and early 21st century, relations remain tense and there are no formal diplomatic relations between the two countries due to Turkey's refusal to establish them for numerous reasons. During the Nagorno-Karabakh War and citing it as the reason, Turkey illegally closed its land border with Armenia in 1993. It has not lifted its blockade despite pressure from the powerful Turkish business lobby interested in Armenian markets.
On 10 October 2009, Armenia and Turkey signed protocols on normalisation of relationships, which set a timetable for restoring diplomatic ties and reopening their joint border. The ratification of those had to be made in the national parliaments. In Armenia it passed through the required by legislation approval of the Constitutional Court and was sent to the parliament for the final ratification. The President had made multiple public announcements, both in Armenia and abroad, that as the leader of the political majority of Armenia he assured the ratification of the protocols if Turkey also ratified them. Despite this, the process stopped, as Turkey continuously added more preconditions to its ratification and also "delayed it beyond any reasonable time-period".
Due to its position between two unfriendly neighbours, Armenia has close security ties with Russia. At the request of the Armenian government, Russia maintains a military base in the northwestern Armenian city of Gyumri as a deterrent against Turkey.[citation needed] Despite this, Armenia has also been looking toward Euro-Atlantic structures in recent years. It maintains good relations with the United States especially through its Armenian diaspora. According to the US Census Bureau, there are 427,822 Armenians living in the country.
Armenia is also a member of the Council of Europe, maintaining friendly relations with the European Union, especially with its member states such as France and Greece. A 2005 survey reported that 64% of Armenia's population would be in favor of joining the EU. Several Armenian officials have also expressed the desire for their country to eventually become an EU member state, some[who?] predicting that it will make an official bid for membership in a few years.[citation needed] In 2004 its forces joined KFOR, a NATO-led international force in Kosovo. It is also an observer member of the Eurasian Economic Community and the Non-Aligned Movement.
The Armenian Army, Air Force, Air Defence, and Border Guard comprise the four branches of the Armed Forces of the Republic of Armenia. The Armenian military was formed after the collapse of the Soviet Union in 1991 and with the establishment of the Ministry of Defence in 1992. The Commander-in-Chief of the military is the President of Armenia, Serzh Sargsyan. The Ministry of Defence is in charge of political leadership, currently headed by Colonel General Seyran Ohanyan, while military command remains in the hands of the General Staff, headed by the Chief of Staff, who is currently Colonel General Yuri Khatchaturov.
Armenia is member of Collective Security Treaty Organisation (CSTO) along with Belarus, Kazakhstan, Kyrgyzstan, Russia, Tajikistan and Uzbekistan. It participates in NATO's Partnership for Peace (PiP) program and is in a NATO organisation called Euro-Atlantic Partnership Council (EAPC). Armenia has engaged in a peacekeeping mission in Kosovo as part of non-NATO KFOR troops under Greek command. Armenia also had 46 members of its military peacekeeping forces as a part of the Coalition Forces in Iraq War until October 2008.
Within each province are communities (hamaynkner, singular hamaynk). Each community is self-governing and consists of one or more settlements (bnakavayrer, singular bnakavayr). Settlements are classified as either towns (kaghakner, singular kaghak) or villages (gyugher, singular gyugh). As of 2007[update], Armenia includes 915 communities, of which 49 are considered urban and 866 are considered rural. The capital, Yerevan, also has the status of a community. Additionally, Yerevan is divided into twelve semi-autonomous districts.
The economy relies heavily on investment and support from Armenians abroad. Before independence, Armenia's economy was largely industry-based – chemicals, electronics, machinery, processed food, synthetic rubber, and textile – and highly dependent on outside resources. The republic had developed a modern industrial sector, supplying machine tools, textiles, and other manufactured goods to sister republics in exchange for raw materials and energy. Recently, the Intel Corporation agreed to open a research center in Armenia, in addition to other technology companies, signalling the growth of the technology industry in Armenia.
Agriculture accounted for less than 20% of both net material product and total employment before the dissolution of the Soviet Union in 1991. After independence, the importance of agriculture in the economy increased markedly, its share at the end of the 1990s rising to more than 30% of GDP and more than 40% of total employment. This increase in the importance of agriculture was attributable to food security needs of the population in the face of uncertainty during the first phases of transition and the collapse of the non-agricultural sectors of the economy in the early 1990s. As the economic situation stabilized and growth resumed, the share of agriculture in GDP dropped to slightly over 20% (2006 data), although the share of agriculture in employment remained more than 40%.
Like other newly independent states of the former Soviet Union, Armenia's economy suffers from the breakdown of former Soviet trading patterns. Soviet investment in and support of Armenian industry has virtually disappeared, so that few major enterprises are still able to function. In addition, the effects of the 1988 Spitak earthquake, which killed more than 25,000 people and made 500,000 homeless, are still being felt. The conflict with Azerbaijan over Nagorno-Karabakh has not been resolved. The closure of Azerbaijani and Turkish borders has devastated the economy, because Armenia depends on outside supplies of energy and most raw materials. Land routes through Georgia and Iran are inadequate or unreliable. The GDP fell nearly 60% between 1989 and 1993, but then resumed robust growth. The national currency, the dram, suffered hyperinflation for the first years after its introduction in 1993.
Nevertheless, the government was able to make wide-ranging economic reforms that paid off in dramatically lower inflation and steady growth. The 1994 cease-fire in the Nagorno-Karabakh conflict has also helped the economy. Armenia has had strong economic growth since 1995, building on the turnaround that began the previous year, and inflation has been negligible for the past several years. New sectors, such as precious-stone processing and jewellery making, information and communication technology, and even tourism are beginning to supplement more traditional sectors of the economy, such as agriculture.
This steady economic progress has earned Armenia increasing support from international institutions. The International Monetary Fund (IMF), World Bank, European Bank for Reconstruction and Development (EBRD), and other international financial institutions (IFIs) and foreign countries are extending considerable grants and loans. Loans to Armenia since 1993 exceed $1.1 billion. These loans are targeted at reducing the budget deficit and stabilizing the currency; developing private businesses; energy; agriculture; food processing; transportation; the health and education sectors; and ongoing rehabilitation in the earthquake zone. The government joined the World Trade Organization on 5 February 2003. But one of the main sources of foreign direct investments remains the Armenian diaspora, which finances major parts of the reconstruction of infrastructure and other public projects. Being a growing democratic state, Armenia also hopes to get more financial aid from the Western World.
A liberal foreign investment law was approved in June 1994, and a law on privatisation was adopted in 1997, as well as a program of state property privatisation. Continued progress will depend on the ability of the government to strengthen its macroeconomic management, including increasing revenue collection, improving the investment climate, and making strides against corruption. However, unemployment, which currently stands at around 15%, still remains a major problem due to the influx of thousands of refugees from the Karabakh conflict.
In the 1988–89 school year, 301 students per 10,000 population were in specialized secondary or higher education, a figure slightly lower than the Soviet average. In 1989 some 58% of Armenians over age fifteen had completed their secondary education, and 14% had a higher education. In the 1990–91 school year, the estimated 1,307 primary and secondary schools were attended by 608,800 students. Another seventy specialized secondary institutions had 45,900 students, and 68,400 students were enrolled in a total of ten postsecondary institutions that included universities. In addition, 35% of eligible children attended preschools. In 1992 Armenia's largest institution of higher learning, Yerevan State University, had eighteen departments, including ones for social sciences, sciences, and law. Its faculty numbered about 1,300 teachers and its student population about 10,000 students. The National Polytechnic University of Armenia is operating since 1933.
On the basis of the expansion and development of Yerevan State University a number of higher educational independent Institutions were formed including Medical Institute separated in 1930 which was set up on the basis of medical faculty. In 1980 Yerevan State Medical University was awarded one of the main rewards of the former USSR – the Order of Labor red Banner for training qualified specialists in health care and valuable service in the development of Medical Science. In 1995 YSMI was renamed to YSMU and since 1989 it has been named after Mkhitar Heratsi, the famous medieval doctor. Mkhitar Heratsi was the founder of Armenian Medical school in Cilician Armenia. The great doctor played the same role in Armenian Medical Science as Hippocrates in Western, Galen in Roman, Ibn Sīnā in Arabic medicine.
Foreign students' department for Armenian diaspora established in 1957 later was enlarged and the enrollment of foreign students began. Nowadays the YSMU is a Medical Institution corresponding to international requirements, trains medical staff for not only Armenia and neighbor countries, i.e. Iran, Syria, Lebanon, Georgia, but also many other leading countries all over the world. A great number of foreign students from India, Nepal, Sri Lanka, the USA and Russian Federation study together with Armenian students. Nowadays the university is ranked among famous higher Medical Institutions and takes its honorable place in the World Directory of Medical Schools published by the WHO.
Other educational institutions in Armenia include the American University of Armenia and the QSI International School of Yerevan. The American University of Armenia has graduate programs in Business and Law, among others. The institution owes its existence to the combined efforts of the Government of Armenia, the Armenian General Benevolent Union, U.S. Agency for International Development, and the University of California. The extension programs and the library at AUA form a new focal point for English-language intellectual life in the city. Armenia also hosts a deployment of OLPC – One Laptopschool Per child XO laptop-tablet schools.
Instruments like the duduk, the dhol, the zurna, and the kanun are commonly found in Armenian folk music. Artists such as Sayat Nova are famous due to their influence in the development of Armenian folk music. One of the oldest types of Armenian music is the Armenian chant which is the most common kind of religious music in Armenia. Many of these chants are ancient in origin, extending to pre-Christian times, while others are relatively modern, including several composed by Saint Mesrop Mashtots, the inventor of the Armenian alphabet. Whilst under Soviet rule, Armenian classical music composer Aram Khatchaturian became internationally well known for his music, for various ballets and the Sabre Dance from his composition for the ballet Gayane.
The Armenian Genocide caused widespread emigration that led to the settlement of Armenians in various countries in the world. Armenians kept to their traditions and certain diasporans rose to fame with their music. In the post-Genocide Armenian community of the United States, the so-called "kef" style Armenian dance music, using Armenian and Middle Eastern folk instruments (often electrified/amplified) and some western instruments, was popular. This style preserved the folk songs and dances of Western Armenia, and many artists also played the contemporary popular songs of Turkey and other Middle Eastern countries from which the Armenians emigrated.
Richard Hagopian is perhaps the most famous artist of the traditional "kef" style and the Vosbikian Band was notable in the 1940s and 1950s for developing their own style of "kef music" heavily influenced by the popular American Big Band Jazz of the time. Later, stemming from the Middle Eastern Armenian diaspora and influenced by Continental European (especially French) pop music, the Armenian pop music genre grew to fame in the 1960s and 1970s with artists such as Adiss Harmandian and Harout Pamboukjian performing to the Armenian diaspora and Armenia; also with artists such as Sirusho, performing pop music combined with Armenian folk music in today's entertainment industry.
Other Armenian diasporans that rose to fame in classical or international music circles are world-renowned French-Armenian singer and composer Charles Aznavour, pianist Sahan Arzruni, prominent opera sopranos such as Hasmik Papian and more recently Isabel Bayrakdarian and Anna Kasyan. Certain Armenians settled to sing non-Armenian tunes such as the heavy metal band System of a Down (which nonetheless often incorporates traditional Armenian instrumentals and styling into their songs) or pop star Cher. In the Armenian diaspora, Armenian revolutionary songs are popular with the youth. These songs encourage Armenian patriotism and are generally about Armenian history and national heroes.
Yerevan Vernissage (arts and crafts market), close to Republic Square, bustles with hundreds of vendors selling a variety of crafts on weekends and Wednesdays (though the selection is much reduced mid-week). The market offers woodcarving, antiques, fine lace, and the hand-knotted wool carpets and kilims that are a Caucasus specialty. Obsidian, which is found locally, is crafted into assortment of jewellery and ornamental objects. Armenian gold smithery enjoys a long tradition, populating one corner of the market with a selection of gold items. Soviet relics and souvenirs of recent Russian manufacture – nesting dolls, watches, enamel boxes and so on – are also available at the Vernisage.
The National Art Gallery in Yerevan has more than 16,000 works that date back to the Middle Ages, which indicate Armenia's rich tales and stories of the times. It houses paintings by many European masters as well. The Modern Art Museum, the Children’s Picture Gallery, and the Martiros Saryan Museum are only a few of the other noteworthy collections of fine art on display in Yerevan. Moreover, many private galleries are in operation, with many more opening every year, featuring rotating exhibitions and sales.
A wide array of sports are played in Armenia, the most popular among them being wrestling, weightlifting, judo, association football, chess, and boxing. Armenia's mountainous terrain provides great opportunities for the practice of sports like skiing and climbing. Being a landlocked country, water sports can only be practiced on lakes, notably Lake Sevan. Competitively, Armenia has been successful in chess, weightlifting and wrestling at the international level. Armenia is also an active member of the international sports community, with full membership in the Union of European Football Associations (UEFA) and International Ice Hockey Federation (IIHF). It also hosts the Pan-Armenian Games.
Prior to 1992, Armenians would participate in the Olympics representing the USSR. As part of the Soviet Union, Armenia was very successful, winning plenty of medals and helping the USSR win the medal standings at the Olympics on numerous occasions. The first medal won by an Armenian in modern Olympic history was by Hrant Shahinyan (sometimes spelled as Grant Shaginyan), who won two golds and two silvers in gymnastics at the 1952 Summer Olympics in Helsinki. To highlight the level of success of Armenians in the Olympics, Shahinyan was quoted as saying:
Football is also popular in Armenia. The most successful team was the FC Ararat Yerevan team of the 1970s who won the Soviet Cup in 1973 and 1975 and the Soviet Top League in 1973. The latter achievement saw FC Ararat gain entry to the European Cup where – despite a home victory in the second leg – they lost on aggregate at the quarter final stage to eventual winner FC Bayern Munich. Armenia competed internationally as part of the USSR national football team until the Armenian national football team was formed in 1992 after the split of the Soviet Union. Armenia have never qualified for a major tournament although recent improvements saw the team to achieve 44th position in the FIFA World Rankings in September 2011. The national team is controlled by the Football Federation of Armenia. The Armenian Premier League is the highest level football competition in Armenia, and has been dominated by FC Pyunik in recent seasons. The league currently consists of eight teams and relegates to the Armenian First League.
Due to the lack of success lately on the international level, in recent years, Armenia has rebuilt 16 Soviet-era sports schools and furnished them with new equipment for a total cost of $1.9 million. The rebuilding of the regional schools was financed by the Armenian government. $9.3 million has been invested in the resort town of Tsaghkadzor to improve the winter sports infrastructure because of dismal performances at recent winter sports events. In 2005, a cycling center was opened in Yerevan with the aim of helping produce world class Armenian cyclists. The government has also promised a cash reward of $700,000 to Armenians who win a gold medal at the Olympics.
Armenian cuisine is as ancient as the history of Armenia, a combination of different tastes and aromas. The food often has quite a distinct aroma. Closely related to eastern and Mediterranean cuisine, various spices, vegetables, fish, and fruits combine to present unique dishes. The main characteristics of Armenian cuisine are a reliance on the quality of the ingredients rather than heavily spicing food, the use of herbs, the use of wheat in a variety of forms, of legumes, nuts, and fruit (as a main ingredient as well as to sour food), and the stuffing of a wide variety of leaves.
Hydrogen is a chemical element with chemical symbol H and atomic number 1. With an atomic weight of 7000100794000000000♠1.00794 u, hydrogen is the lightest element on the periodic table. Its monatomic form (H) is the most abundant chemical substance in the Universe, constituting roughly 75% of all baryonic mass.[note 1] Non-remnant stars are mainly composed of hydrogen in its plasma state. The most common isotope of hydrogen, termed protium (name rarely used, symbol 1H), has one proton and no neutrons.
The universal emergence of atomic hydrogen first occurred during the recombination epoch. At standard temperature and pressure, hydrogen is a colorless, odorless, tasteless, non-toxic, nonmetallic, highly combustible diatomic gas with the molecular formula H2. Since hydrogen readily forms covalent compounds with most non-metallic elements, most of the hydrogen on Earth exists in molecular forms such as in the form of water or organic compounds. Hydrogen plays a particularly important role in acid–base reactions as many acid-base reactions involve the exchange of protons between soluble molecules. In ionic compounds, hydrogen can take the form of a negative charge (i.e., anion) when it is known as a hydride, or as a positively charged (i.e., cation) species denoted by the symbol H+. The hydrogen cation is written as though composed of a bare proton, but in reality, hydrogen cations in ionic compounds are always more complex species than that would suggest. As the only neutral atom for which the Schrödinger equation can be solved analytically, study of the energetics and bonding of the hydrogen atom has played a key role in the development of quantum mechanics.
Hydrogen gas was first artificially produced in the early 16th century, via the mixing of metals with acids. In 1766–81, Henry Cavendish was the first to recognize that hydrogen gas was a discrete substance, and that it produces water when burned, a property which later gave it its name: in Greek, hydrogen means "water-former".
Industrial production is mainly from the steam reforming of natural gas, and less often from more energy-intensive hydrogen production methods like the electrolysis of water. Most hydrogen is employed near its production site, with the two largest uses being fossil fuel processing (e.g., hydrocracking) and ammonia production, mostly for the fertilizer market. Hydrogen is a concern in metallurgy as it can embrittle many metals, complicating the design of pipelines and storage tanks.
Hydrogen gas (dihydrogen or molecular hydrogen) is highly flammable and will burn in air at a very wide range of concentrations between 4% and 75% by volume. The enthalpy of combustion for hydrogen is −286 kJ/mol:
Hydrogen gas forms explosive mixtures with air if it is 4–74% concentrated and with chlorine if it is 5–95% concentrated. The mixtures may be ignited by spark, heat or sunlight. The hydrogen autoignition temperature, the temperature of spontaneous ignition in air, is 500 °C (932 °F). Pure hydrogen-oxygen flames emit ultraviolet light and with high oxygen mix are nearly invisible to the naked eye, as illustrated by the faint plume of the Space Shuttle Main Engine compared to the highly visible plume of a Space Shuttle Solid Rocket Booster. The detection of a burning hydrogen leak may require a flame detector; such leaks can be very dangerous. Hydrogen flames in other conditions are blue, resembling blue natural gas flames. The destruction of the Hindenburg airship was an infamous example of hydrogen combustion; the cause is debated, but the visible orange flames were the result of a rich mixture of hydrogen to oxygen combined with carbon compounds from the airship skin.
H2 reacts with every oxidizing element. Hydrogen can react spontaneously and violently at room temperature with chlorine and fluorine to form the corresponding hydrogen halides, hydrogen chloride and hydrogen fluoride, which are also potentially dangerous acids.
The energy levels of hydrogen can be calculated fairly accurately using the Bohr model of the atom, which conceptualizes the electron as "orbiting" the proton in analogy to the Earth's orbit of the Sun. However, the electromagnetic force attracts electrons and protons to one another, while planets and celestial objects are attracted to each other by gravity. Because of the discretization of angular momentum postulated in early quantum mechanics by Bohr, the electron in the Bohr model can only occupy certain allowed distances from the proton, and therefore only certain allowed energies.
A more accurate description of the hydrogen atom comes from a purely quantum mechanical treatment that uses the Schrödinger equation, Dirac equation or even the Feynman path integral formulation to calculate the probability density of the electron around the proton. The most complicated treatments allow for the small effects of special relativity and vacuum polarization. In the quantum mechanical treatment, the electron in a ground state hydrogen atom has no angular momentum at all—an illustration of how the "planetary orbit" conception of electron motion differs from reality.
There exist two different spin isomers of hydrogen diatomic molecules that differ by the relative spin of their nuclei. In the orthohydrogen form, the spins of the two protons are parallel and form a triplet state with a molecular spin quantum number of 1 (1⁄2+1⁄2); in the parahydrogen form the spins are antiparallel and form a singlet with a molecular spin quantum number of 0 (1⁄2–1⁄2). At standard temperature and pressure, hydrogen gas contains about 25% of the para form and 75% of the ortho form, also known as the "normal form". The equilibrium ratio of orthohydrogen to parahydrogen depends on temperature, but because the ortho form is an excited state and has a higher energy than the para form, it is unstable and cannot be purified. At very low temperatures, the equilibrium state is composed almost exclusively of the para form. The liquid and gas phase thermal properties of pure parahydrogen differ significantly from those of the normal form because of differences in rotational heat capacities, as discussed more fully in spin isomers of hydrogen. The ortho/para distinction also occurs in other hydrogen-containing molecules or functional groups, such as water and methylene, but is of little significance for their thermal properties.
The uncatalyzed interconversion between para and ortho H2 increases with increasing temperature; thus rapidly condensed H2 contains large quantities of the high-energy ortho form that converts to the para form very slowly. The ortho/para ratio in condensed H2 is an important consideration in the preparation and storage of liquid hydrogen: the conversion from ortho to para is exothermic and produces enough heat to evaporate some of the hydrogen liquid, leading to loss of liquefied material. Catalysts for the ortho-para interconversion, such as ferric oxide, activated carbon, platinized asbestos, rare earth metals, uranium compounds, chromic oxide, or some nickel compounds, are used during hydrogen cooling.
While H2 is not very reactive under standard conditions, it does form compounds with most elements. Hydrogen can form compounds with elements that are more electronegative, such as halogens (e.g., F, Cl, Br, I), or oxygen; in these compounds hydrogen takes on a partial positive charge. When bonded to fluorine, oxygen, or nitrogen, hydrogen can participate in a form of medium-strength noncovalent bonding with other similar molecules between their hydrogens called hydrogen bonding, which is critical to the stability of many biological molecules. Hydrogen also forms compounds with less electronegative elements, such as the metals and metalloids, in which it takes on a partial negative charge. These compounds are often known as hydrides.
Hydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, "organic" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word "organic" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways, which seldom involve elementary hydrogen.
Compounds of hydrogen are often called hydrides, a term that is used fairly loosely. The term "hydride" suggests that the H atom has acquired a negative or anionic character, denoted H−, and is used when hydrogen forms a compound with a more electropositive element. The existence of the hydride anion, suggested by Gilbert N. Lewis in 1916 for group I and II salt-like hydrides, was demonstrated by Moers in 1920 by the electrolysis of molten lithium hydride (LiH), producing a stoichiometry quantity of hydrogen at the anode. For hydrides other than group I and II metals, the term is quite misleading, considering the low electronegativity of hydrogen. An exception in group II hydrides is BeH
2, which is polymeric. In lithium aluminium hydride, the AlH−
4 anion carries hydridic centers firmly attached to the Al(III).
Although hydrides can be formed with almost all main-group elements, the number and combination of possible compounds varies widely; for example, there are over 100 binary borane hydrides known, but only one binary aluminium hydride. Binary indium hydride has not yet been identified, although larger complexes exist.
In inorganic chemistry, hydrides can also serve as bridging ligands that link two metal centers in a coordination complex. This function is particularly common in group 13 elements, especially in boranes (boron hydrides) and aluminium complexes, as well as in clustered carboranes.
Oxidation of hydrogen removes its electron and gives H+, which contains no electrons and a nucleus which is usually composed of one proton. That is why H+ is often called a proton. This species is central to discussion of acids. Under the Bronsted-Lowry theory, acids are proton donors, while bases are proton acceptors.
A bare proton, H+, cannot exist in solution or in ionic crystals, because of its unstoppable attraction to other atoms or molecules with electrons. Except at the high temperatures associated with plasmas, such protons cannot be removed from the electron clouds of atoms and molecules, and will remain attached to them. However, the term 'proton' is sometimes used loosely and metaphorically to refer to positively charged or cationic hydrogen attached to other species in this fashion, and as such is denoted "H+" without any implication that any single protons exist freely as a species.
To avoid the implication of the naked "solvated proton" in solution, acidic aqueous solutions are sometimes considered to contain a less unlikely fictitious species, termed the "hydronium ion" (H
3O+). However, even in this case, such solvated hydrogen cations are more realistically conceived as being organized into clusters that form species closer to H
9O+
4. Other oxonium ions are found when water is in acidic solution with other solvents.
Although exotic on Earth, one of the most common ions in the universe is the H+
3 ion, known as protonated molecular hydrogen or the trihydrogen cation.
Hydrogen has three naturally occurring isotopes, denoted 1H, 2H and 3H. Other, highly unstable nuclei (4H to 7H) have been synthesized in the laboratory but not observed in nature.
Hydrogen is the only element that has different names for its isotopes in common use today. During the early study of radioactivity, various heavy radioactive isotopes were given their own names, but such names are no longer used, except for deuterium and tritium. The symbols D and T (instead of 2H and 3H) are sometimes used for deuterium and tritium, but the corresponding symbol for protium, P, is already in use for phosphorus and thus is not available for protium. In its nomenclatural guidelines, the International Union of Pure and Applied Chemistry allows any of D, T, 2H, and 3H to be used, although 2H and 3H are preferred.
In 1671, Robert Boyle discovered and described the reaction between iron filings and dilute acids, which results in the production of hydrogen gas. In 1766, Henry Cavendish was the first to recognize hydrogen gas as a discrete substance, by naming the gas from a metal-acid reaction "flammable air". He speculated that "flammable air" was in fact identical to the hypothetical substance called "phlogiston" and further finding in 1781 that the gas produces water when burned. He is usually given credit for its discovery as an element. In 1783, Antoine Lavoisier gave the element the name hydrogen (from the Greek ὑδρο- hydro meaning "water" and -γενής genes meaning "creator") when he and Laplace reproduced Cavendish's finding that water is produced when hydrogen is burned.
Lavoisier produced hydrogen for his experiments on mass conservation by reacting a flux of steam with metallic iron through an incandescent iron tube heated in a fire. Anaerobic oxidation of iron by the protons of water at high temperature can be schematically represented by the set of following reactions:
Hydrogen was liquefied for the first time by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. He produced solid hydrogen the next year. Deuterium was discovered in December 1931 by Harold Urey, and tritium was prepared in 1934 by Ernest Rutherford, Mark Oliphant, and Paul Harteck. Heavy water, which consists of deuterium in the place of regular hydrogen, was discovered by Urey's group in 1932. François Isaac de Rivaz built the first de Rivaz engine, an internal combustion engine powered by a mixture of hydrogen and oxygen in 1806. Edward Daniel Clarke invented the hydrogen gas blowpipe in 1819. The Döbereiner's lamp and limelight were invented in 1823.
The first hydrogen-filled balloon was invented by Jacques Charles in 1783. Hydrogen provided the lift for the first reliable form of air-travel following the 1852 invention of the first hydrogen-lifted airship by Henri Giffard. German count Ferdinand von Zeppelin promoted the idea of rigid airships lifted by hydrogen that later were called Zeppelins; the first of which had its maiden flight in 1900. Regularly scheduled flights started in 1910 and by the outbreak of World War I in August 1914, they had carried 35,000 passengers without a serious incident. Hydrogen-lifted airships were used as observation platforms and bombers during the war.
The first non-stop transatlantic crossing was made by the British airship R34 in 1919. Regular passenger service resumed in the 1920s and the discovery of helium reserves in the United States promised increased safety, but the U.S. government refused to sell the gas for this purpose. Therefore, H2 was used in the Hindenburg airship, which was destroyed in a midair fire over New Jersey on 6 May 1937. The incident was broadcast live on radio and filmed. Ignition of leaking hydrogen is widely assumed to be the cause, but later investigations pointed to the ignition of the aluminized fabric coating by static electricity. But the damage to hydrogen's reputation as a lifting gas was already done.
In the same year the first hydrogen-cooled turbogenerator went into service with gaseous hydrogen as a coolant in the rotor and the stator in 1937 at Dayton, Ohio, by the Dayton Power & Light Co.; because of the thermal conductivity of hydrogen gas, this is the most common type in its field today.
The nickel hydrogen battery was used for the first time in 1977 aboard the U.S. Navy's Navigation technology satellite-2 (NTS-2). For example, the ISS, Mars Odyssey and the Mars Global Surveyor are equipped with nickel-hydrogen batteries. In the dark part of its orbit, the Hubble Space Telescope is also powered by nickel-hydrogen batteries, which were finally replaced in May 2009, more than 19 years after launch, and 13 years over their design life.
Because of its simple atomic structure, consisting only of a proton and an electron, the hydrogen atom, together with the spectrum of light produced from it or absorbed by it, has been central to the development of the theory of atomic structure. Furthermore, the corresponding simplicity of the hydrogen molecule and the corresponding cation H+
2 allowed fuller understanding of the nature of the chemical bond, which followed shortly after the quantum mechanical treatment of the hydrogen atom had been developed in the mid-1920s.
One of the first quantum effects to be explicitly noticed (but not understood at the time) was a Maxwell observation involving hydrogen, half a century before full quantum mechanical theory arrived. Maxwell observed that the specific heat capacity of H2 unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H2 because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect.
Hydrogen, as atomic H, is the most abundant chemical element in the universe, making up 75% of normal matter by mass and over 90% by number of atoms (most of the mass of the universe, however, is not in the form of chemical-element type matter, but rather is postulated to occur as yet-undetected forms of mass such as dark matter and dark energy). This element is found in great abundance in stars and gas giant planets. Molecular clouds of H2 are associated with star formation. Hydrogen plays a vital role in powering stars through the proton-proton reaction and the CNO cycle nuclear fusion.
Throughout the universe, hydrogen is mostly found in the atomic and plasma states whose properties are quite different from molecular hydrogen. As a plasma, hydrogen's electron and proton are not bound together, resulting in very high electrical conductivity and high emissivity (producing the light from the Sun and other stars). The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind they interact with the Earth's magnetosphere giving rise to Birkeland currents and the aurora. Hydrogen is found in the neutral atomic state in the interstellar medium. The large amount of neutral hydrogen found in the damped Lyman-alpha systems is thought to dominate the cosmological baryonic density of the Universe up to redshift z=4.
Under ordinary conditions on Earth, elemental hydrogen exists as the diatomic gas, H2. However, hydrogen gas is very rare in the Earth's atmosphere (1 ppm by volume) because of its light weight, which enables it to escape from Earth's gravity more easily than heavier gases. However, hydrogen is the third most abundant element on the Earth's surface, mostly in the form of chemical compounds such as hydrocarbons and water. Hydrogen gas is produced by some bacteria and algae and is a natural component of flatus, as is methane, itself a hydrogen source of increasing importance.
A molecular form called protonated molecular hydrogen (H+
3) is found in the interstellar medium, where it is generated by ionization of molecular hydrogen from cosmic rays. This charged ion has also been observed in the upper atmosphere of the planet Jupiter. The ion is relatively stable in the environment of outer space due to the low temperature and density. H+
3 is one of the most abundant ions in the Universe, and it plays a notable role in the chemistry of the interstellar medium. Neutral triatomic hydrogen H3 can only exist in an excited form and is unstable. By contrast, the positive hydrogen molecular ion (H+
2) is a rare molecule in the universe.
H
2 is produced in chemistry and biology laboratories, often as a by-product of other reactions; in industry for the hydrogenation of unsaturated substrates; and in nature as a means of expelling reducing equivalents in biochemical reactions.
The electrolysis of water is a simple method of producing hydrogen. A low voltage current is run through the water, and gaseous oxygen forms at the anode while gaseous hydrogen forms at the cathode. Typically the cathode is made from platinum or another inert metal when producing hydrogen for storage. If, however, the gas is to be burnt on site, oxygen is desirable to assist the combustion, and so both electrodes would be made from inert metals. (Iron, for instance, would oxidize, and thus decrease the amount of oxygen given off.) The theoretical maximum efficiency (electricity used vs. energetic value of hydrogen produced) is in the range 80–94%.
An alloy of aluminium and gallium in pellet form added to water can be used to generate hydrogen. The process also produces alumina, but the expensive gallium, which prevents the formation of an oxide skin on the pellets, can be re-used. This has important potential implications for a hydrogen economy, as hydrogen can be produced on-site and does not need to be transported.
Hydrogen can be prepared in several different ways, but economically the most important processes involve removal of hydrogen from hydrocarbons. Commercial bulk hydrogen is usually produced by the steam reforming of natural gas. At high temperatures (1000–1400 K, 700–1100 °C or 1300–2000 °F), steam (water vapor) reacts with methane to yield carbon monoxide and H
2.
This reaction is favored at low pressures but is nonetheless conducted at high pressures (2.0  MPa, 20 atm or 600 inHg). This is because high-pressure H
2 is the most marketable product and Pressure Swing Adsorption (PSA) purification systems work better at higher pressures. The product mixture is known as "synthesis gas" because it is often used directly for the production of methanol and related compounds. Hydrocarbons other than methane can be used to produce synthesis gas with varying product ratios. One of the many complications to this highly optimized technology is the formation of coke or carbon:
Consequently, steam reforming typically employs an excess of H
2O. Additional hydrogen can be recovered from the steam by use of carbon monoxide through the water gas shift reaction, especially with an iron oxide catalyst. This reaction is also a common industrial source of carbon dioxide:
Hydrogen is sometimes produced and consumed in the same industrial process, without being separated. In the Haber process for the production of ammonia, hydrogen is generated from natural gas. Electrolysis of brine to yield chlorine also produces hydrogen as a co-product.
There are more than 200 thermochemical cycles which can be used for water splitting, around a dozen of these cycles such as the iron oxide cycle, cerium(IV) oxide–cerium(III) oxide cycle, zinc zinc-oxide cycle, sulfur-iodine cycle, copper-chlorine cycle and hybrid sulfur cycle are under research and in testing phase to produce hydrogen and oxygen from water and heat without using electricity. A number of laboratories (including in France, Germany, Greece, Japan, and the USA) are developing thermochemical methods to produce hydrogen from solar energy and water.
Under anaerobic conditions, iron and steel alloys are slowly oxidized by the protons of water concomitantly reduced in molecular hydrogen (H
2). The anaerobic corrosion of iron leads first to the formation of ferrous hydroxide (green rust) and can be described by the following reaction:
In its turn, under anaerobic conditions, the ferrous hydroxide (Fe(OH)
2 ) can be oxidized by the protons of water to form magnetite and molecular hydrogen. This process is described by the Schikorr reaction:
In the absence of atmospheric oxygen (O
2), in deep geological conditions prevailing far away from Earth atmosphere, hydrogen (H
2) is produced during the process of serpentinization by the anaerobic oxidation by the water protons (H+) of the ferrous (Fe2+) silicate present in the crystal lattice of the fayalite (Fe
2SiO
4, the olivine iron-endmember). The corresponding reaction leading to the formation of magnetite (Fe
3O
4), quartz (SiO
2) and hydrogen (H
2) is the following:
From all the fault gases formed in power transformers, hydrogen is the most common and is generated under most fault conditions; thus, formation of hydrogen is an early indication of serious problems in the transformer's life cycle.
Large quantities of H
2 are needed in the petroleum and chemical industries. The largest application of H
2 is for the processing ("upgrading") of fossil fuels, and in the production of ammonia. The key consumers of H
2 in the petrochemical plant include hydrodealkylation, hydrodesulfurization, and hydrocracking. H
2 has several other important uses. H
2 is used as a hydrogenating agent, particularly in increasing the level of saturation of unsaturated fats and oils (found in items such as margarine), and in the production of methanol. It is similarly the source of hydrogen in the manufacture of hydrochloric acid. H
2 is also used as a reducing agent of metallic ores.
Hydrogen is highly soluble in many rare earth and transition metals and is soluble in both nanocrystalline and amorphous metals. Hydrogen solubility in metals is influenced by local distortions or impurities in the crystal lattice. These properties may be useful when hydrogen is purified by passage through hot palladium disks, but the gas's high solubility is a metallurgical problem, contributing to the embrittlement of many metals, complicating the design of pipelines and storage tanks.
Apart from its use as a reactant, H
2 has wide applications in physics and engineering. It is used as a shielding gas in welding methods such as atomic hydrogen welding. H2 is used as the rotor coolant in electrical generators at power stations, because it has the highest thermal conductivity of any gas. Liquid H2 is used in cryogenic research, including superconductivity studies. Because H
2 is lighter than air, having a little more than 1⁄14 of the density of air, it was once widely used as a lifting gas in balloons and airships.
In more recent applications, hydrogen is used pure or mixed with nitrogen (sometimes called forming gas) as a tracer gas for minute leak detection. Applications can be found in the automotive, chemical, power generation, aerospace, and telecommunications industries. Hydrogen is an authorized food additive (E 949) that allows food package leak testing among other anti-oxidizing properties.
Hydrogen's rarer isotopes also each have specific applications. Deuterium (hydrogen-2) is used in nuclear fission applications as a moderator to slow neutrons, and in nuclear fusion reactions. Deuterium compounds have applications in chemistry and biology in studies of reaction isotope effects. Tritium (hydrogen-3), produced in nuclear reactors, is used in the production of hydrogen bombs, as an isotopic label in the biosciences, and as a radiation source in luminous paints.
Hydrogen is commonly used in power stations as a coolant in generators due to a number of favorable properties that are a direct result of its light diatomic molecules. These include low density, low viscosity, and the highest specific heat and thermal conductivity of all gases.
Hydrogen is not an energy resource, except in the hypothetical context of commercial nuclear fusion power plants using deuterium or tritium, a technology presently far from development. The Sun's energy comes from nuclear fusion of hydrogen, but this process is difficult to achieve controllably on Earth. Elemental hydrogen from solar, biological, or electrical sources require more energy to make it than is obtained by burning it, so in these cases hydrogen functions as an energy carrier, like a battery. Hydrogen may be obtained from fossil sources (such as methane), but these sources are unsustainable.
The energy density per unit volume of both liquid hydrogen and compressed hydrogen gas at any practicable pressure is significantly less than that of traditional fuel sources, although the energy density per unit fuel mass is higher. Nevertheless, elemental hydrogen has been widely discussed in the context of energy, as a possible future carrier of energy on an economy-wide scale. For example, CO
2 sequestration followed by carbon capture and storage could be conducted at the point of H
2 production from fossil fuels. Hydrogen used in transportation would burn relatively cleanly, with some NOx emissions, but without carbon emissions. However, the infrastructure costs associated with full conversion to a hydrogen economy would be substantial. Fuel cells can convert hydrogen and oxygen directly to electricity more efficiently than internal combustion engines.
Hydrogen is employed to saturate broken ("dangling") bonds of amorphous silicon and amorphous carbon that helps stabilizing material properties. It is also a potential electron donor in various oxide materials, including ZnO, SnO2, CdO, MgO, ZrO2, HfO2, La2O3, Y2O3, TiO2, SrTiO3, LaAlO3, SiO2, Al2O3, ZrSiO4, HfSiO4, and SrZrO3.
H2 is a product of some types of anaerobic metabolism and is produced by several microorganisms, usually via reactions catalyzed by iron- or nickel-containing enzymes called hydrogenases. These enzymes catalyze the reversible redox reaction between H2 and its component two protons and two electrons. Creation of hydrogen gas occurs in the transfer of reducing equivalents produced during pyruvate fermentation to water. The natural cycle of hydrogen production and consumption by organisms is called the hydrogen cycle.
Water splitting, in which water is decomposed into its component protons, electrons, and oxygen, occurs in the light reactions in all photosynthetic organisms. Some such organisms, including the alga Chlamydomonas reinhardtii and cyanobacteria, have evolved a second step in the dark reactions in which protons and electrons are reduced to form H2 gas by specialized hydrogenases in the chloroplast. Efforts have been undertaken to genetically modify cyanobacterial hydrogenases to efficiently synthesize H2 gas even in the presence of oxygen. Efforts have also been undertaken with genetically modified alga in a bioreactor.
Hydrogen poses a number of hazards to human safety, from potential detonations and fires when mixed with air to being an asphyxiant in its pure, oxygen-free form. In addition, liquid hydrogen is a cryogen and presents dangers (such as frostbite) associated with very cold liquids. Hydrogen dissolves in many metals, and, in addition to leaking out, may have adverse effects on them, such as hydrogen embrittlement, leading to cracks and explosions. Hydrogen gas leaking into external air may spontaneously ignite. Moreover, hydrogen fire, while being extremely hot, is almost invisible, and thus can lead to accidental burns.
Even interpreting the hydrogen data (including safety data) is confounded by a number of phenomena. Many physical and chemical properties of hydrogen depend on the parahydrogen/orthohydrogen ratio (it often takes days or weeks at a given temperature to reach the equilibrium ratio, for which the data is usually given). Hydrogen detonation parameters, such as critical detonation pressure and temperature, strongly depend on the container geometry.
Many applications of silicate glasses derive from their optical transparency, which gives rise to one of silicate glasses' primary uses as window panes. Glass will transmit, reflect and refract light; these qualities can be enhanced by cutting and polishing to make optical lenses, prisms, fine glassware, and optical fibers for high speed data transmission by light. Glass can be colored by adding metallic salts, and can also be painted and printed with vitreous enamels. These qualities have led to the extensive use of glass in the manufacture of art objects and in particular, stained glass windows. Although brittle, silicate glass is extremely durable, and many examples of glass fragments exist from early glass-making cultures. Because glass can be formed or molded into any shape, and also because it is a sterile product, it has been traditionally used for vessels: bowls, vases, bottles, jars and drinking glasses. In its most solid forms it has also been used for paperweights, marbles, and beads. When extruded as glass fiber and matted as glass wool in a way to trap air, it becomes a thermal insulating material, and when these glass fibers are embedded into an organic polymer plastic, they are a key structural reinforcement part of the composite material fiberglass. Some objects historically were so commonly made of silicate glass that they are simply called by the name of the material, such as drinking glasses and reading glasses.
Most common glass contains other ingredients to change its properties. Lead glass or flint glass is more 'brilliant' because the increased refractive index causes noticeably more specular reflection and increased optical dispersion. Adding barium also increases the refractive index. Thorium oxide gives glass a high refractive index and low dispersion and was formerly used in producing high-quality lenses, but due to its radioactivity has been replaced by lanthanum oxide in modern eyeglasses.[citation needed] Iron can be incorporated into glass to absorb infrared energy, for example in heat absorbing filters for movie projectors, while cerium(IV) oxide can be used for glass that absorbs UV wavelengths.
Fused quartz is a glass made from chemically-pure SiO2 (silica). It has excellent thermal shock characteristics, being able to survive immersion in water while red hot. However, its high melting-temperature (1723 °C) and viscosity make it difficult to work with. Normally, other substances are added to simplify processing. One is sodium carbonate (Na2CO3, "soda"), which lowers the glass transition temperature. The soda makes the glass water-soluble, which is usually undesirable, so lime (calcium oxide [CaO], generally obtained from limestone), some magnesium oxide (MgO) and aluminium oxide (Al2O3) are added to provide for a better chemical durability. The resulting glass contains about 70 to 74% silica by weight and is called a soda-lime glass. Soda-lime glasses account for about 90% of manufactured glass.
Following the glass batch preparation and mixing, the raw materials are transported to the furnace. Soda-lime glass for mass production is melted in gas fired units. Smaller scale furnaces for specialty glasses include electric melters, pot furnaces, and day tanks. After melting, homogenization and refining (removal of bubbles), the glass is formed. Flat glass for windows and similar applications is formed by the float glass process, developed between 1953 and 1957 by Sir Alastair Pilkington and Kenneth Bickerstaff of the UK's Pilkington Brothers, who created a continuous ribbon of glass using a molten tin bath on which the molten glass flows unhindered under the influence of gravity. The top surface of the glass is subjected to nitrogen under pressure to obtain a polished finish. Container glass for common bottles and jars is formed by blowing and pressing methods. This glass is often slightly modified chemically (with more alumina and calcium oxide) for greater water resistance. Further glass forming techniques are summarized in the table Glass forming techniques.
Glass has the ability to refract, reflect, and transmit light following geometrical optics, without scattering it. It is used in the manufacture of lenses and windows. Common glass has a refraction index around 1.5. This may be modified by adding low-density materials such as boron, which lowers the index of refraction (see crown glass), or increased (to as much as 1.8) with high-density materials such as (classically) lead oxide (see flint glass and lead glass), or in modern uses, less toxic oxides of zirconium, titanium, or barium. These high-index glasses (inaccurately known as "crystal" when used in glass vessels) cause more chromatic dispersion of light, and are prized for their diamond-like optical properties.
The most familiar, and historically the oldest, types of glass are "silicate glasses" based on the chemical compound silica (silicon dioxide, or quartz), the primary constituent of sand. The term glass, in popular usage, is often used to refer only to this type of material, which is familiar from use as window glass and in glass bottles. Of the many silica-based glasses that exist, ordinary glazing and container glass is formed from a specific type called soda-lime glass, composed of approximately 75% silicon dioxide (SiO2), sodium oxide (Na2O) from sodium carbonate (Na2CO3), calcium oxide, also called lime (CaO), and several minor additives. A very clear and durable quartz glass can be made from pure silica, but the high melting point and very narrow glass transition of quartz make glassblowing and hot working difficult. In glasses like soda lime, the compounds added to quartz are used to lower the melting temperature and improve workability, at a cost in the toughness, thermal stability, and optical transmittance.
Glass is in widespread use largely due to the production of glass compositions that are transparent to visible light. In contrast, polycrystalline materials do not generally transmit visible light. The individual crystallites may be transparent, but their facets (grain boundaries) reflect or scatter light resulting in diffuse reflection. Glass does not contain the internal subdivisions associated with grain boundaries in polycrystals and hence does not scatter light in the same manner as a polycrystalline material. The surface of a glass is often smooth since during glass formation the molecules of the supercooled liquid are not forced to dispose in rigid crystal geometries and can follow surface tension, which imposes a microscopically smooth surface. These properties, which give glass its clearness, can be retained even if glass is partially light-absorbing—i.e., colored.
Naturally occurring glass, especially the volcanic glass obsidian, has been used by many Stone Age societies across the globe for the production of sharp cutting tools and, due to its limited source areas, was extensively traded. But in general, archaeological evidence suggests that the first true glass was made in coastal north Syria, Mesopotamia or ancient Egypt. The earliest known glass objects, of the mid third millennium BCE, were beads, perhaps initially created as accidental by-products of metal-working (slags) or during the production of faience, a pre-glass vitreous material made by a process similar to glazing.
Color in glass may be obtained by addition of electrically charged ions (or color centers) that are homogeneously distributed, and by precipitation of finely dispersed particles (such as in photochromic glasses). Ordinary soda-lime glass appears colorless to the naked eye when it is thin, although iron(II) oxide (FeO) impurities of up to 0.1 wt% produce a green tint, which can be viewed in thick pieces or with the aid of scientific instruments. Further FeO and Cr2O3 additions may be used for the production of green bottles. Sulfur, together with carbon and iron salts, is used to form iron polysulfides and produce amber glass ranging from yellowish to almost black. A glass melt can also acquire an amber color from a reducing combustion atmosphere. Manganese dioxide can be added in small amounts to remove the green tint given by iron(II) oxide. When used in art glass or studio glass is colored using closely guarded recipes that involve specific combinations of metal oxides, melting temperatures and 'cook' times. Most colored glass used in the art market is manufactured in volume by vendors who serve this market although there are some glassmakers with the ability to make their own color from raw materials.
Glass remained a luxury material, and the disasters that overtook Late Bronze Age civilizations seem to have brought glass-making to a halt. Indigenous development of glass technology in South Asia may have begun in 1730 BCE. In ancient China, though, glassmaking seems to have a late start, compared to ceramics and metal work. The term glass developed in the late Roman Empire. It was in the Roman glassmaking center at Trier, now in modern Germany, that the late-Latin term glesum originated, probably from a Germanic word for a transparent, lustrous substance. Glass objects have been recovered across the Roman empire in domestic, industrial and funerary contexts.[citation needed]
Glass was used extensively during the Middle Ages. Anglo-Saxon glass has been found across England during archaeological excavations of both settlement and cemetery sites. Glass in the Anglo-Saxon period was used in the manufacture of a range of objects including vessels, beads, windows and was also used in jewelry. From the 10th-century onwards, glass was employed in stained glass windows of churches and cathedrals, with famous examples at Chartres Cathedral and the Basilica of Saint Denis. By the 14th-century, architects were designing buildings with walls of stained glass such as Sainte-Chapelle, Paris, (1203–1248) and the East end of Gloucester Cathedral. Stained glass had a major revival with Gothic Revival architecture in the 19th-century. With the Renaissance, and a change in architectural style, the use of large stained glass windows became less prevalent. The use of domestic stained glass increased until most substantial houses had glass windows. These were initially small panes leaded together, but with the changes in technology, glass could be manufactured relatively cheaply in increasingly larger sheets. This led to larger window panes, and, in the 20th-century, to much larger windows in ordinary domestic and commercial buildings.
In the 20th century, new types of glass such as laminated glass, reinforced glass and glass bricks have increased the use of glass as a building material and resulted in new applications of glass. Multi-storey buildings are frequently constructed with curtain walls made almost entirely of glass. Similarly, laminated glass has been widely applied to vehicles for windscreens. While glass containers have always been used for storage and are valued for their hygienic properties, glass has been utilized increasingly in industry. Optical glass for spectacles has been used since the late Middle Ages. The production of lenses has become increasingly proficient, aiding astronomers as well as having other application in medicine and science. Glass is also employed as the aperture cover in many solar energy systems.
From the 19th century, there was a revival in many ancient glass-making techniques including cameo glass, achieved for the first time since the Roman Empire and initially mostly used for pieces in a neo-classical style. The Art Nouveau movement made great use of glass, with René Lalique, Émile Gallé, and Daum of Nancy producing colored vases and similar pieces, often in cameo glass, and also using luster techniques. Louis Comfort Tiffany in America specialized in stained glass, both secular and religious, and his famous lamps. The early 20th-century saw the large-scale factory production of glass art by firms such as Waterford and Lalique. From about 1960 onwards there have been an increasing number of small studios hand-producing glass artworks, and glass artists began to class themselves as in effect sculptors working in glass, and their works as part fine arts.
Addition of lead(II) oxide lowers melting point, lowers viscosity of the melt, and increases refractive index. Lead oxide also facilitates solubility of other metal oxides and is used in colored glasses. The viscosity decrease of lead glass melt is very significant (roughly 100 times in comparison with soda glasses); this allows easier removal of bubbles and working at lower temperatures, hence its frequent use as an additive in vitreous enamels and glass solders. The high ionic radius of the Pb2+ ion renders it highly immobile in the matrix and hinders the movement of other ions; lead glasses therefore have high electrical resistance, about two orders of magnitude higher than soda-lime glass (108.5 vs 106.5 Ohm·cm, DC at 250 °C). For more details, see lead glass.
There are three classes of components for oxide glasses: network formers, intermediates, and modifiers. The network formers (silicon, boron, germanium) form a highly cross-linked network of chemical bonds. The intermediates (titanium, aluminium, zirconium, beryllium, magnesium, zinc) can act as both network formers and modifiers, according to the glass composition. The modifiers (calcium, lead, lithium, sodium, potassium) alter the network structure; they are usually present as ions, compensated by nearby non-bridging oxygen atoms, bound by one covalent bond to the glass network and holding one negative charge to compensate for the positive ion nearby. Some elements can play multiple roles; e.g. lead can act both as a network former (Pb4+ replacing Si4+), or as a modifier.
The alkali metal ions are small and mobile; their presence in glass allows a degree of electrical conductivity, especially in molten state or at high temperature. Their mobility decreases the chemical resistance of the glass, allowing leaching by water and facilitating corrosion. Alkaline earth ions, with their two positive charges and requirement for two non-bridging oxygen ions to compensate for their charge, are much less mobile themselves and also hinder diffusion of other ions, especially the alkalis. The most common commercial glasses contain both alkali and alkaline earth ions (usually sodium and calcium), for easier processing and satisfying corrosion resistance. Corrosion resistance of glass can be achieved by dealkalization, removal of the alkali ions from the glass surface by reaction with e.g. sulfur or fluorine compounds. Presence of alkaline metal ions has also detrimental effect to the loss tangent of the glass, and to its electrical resistance; glasses for electronics (sealing, vacuum tubes, lamps...) have to take this in account.
New chemical glass compositions or new treatment techniques can be initially investigated in small-scale laboratory experiments. The raw materials for laboratory-scale glass melts are often different from those used in mass production because the cost factor has a low priority. In the laboratory mostly pure chemicals are used. Care must be taken that the raw materials have not reacted with moisture or other chemicals in the environment (such as alkali or alkaline earth metal oxides and hydroxides, or boron oxide), or that the impurities are quantified (loss on ignition). Evaporation losses during glass melting should be considered during the selection of the raw materials, e.g., sodium selenite may be preferred over easily evaporating SeO2. Also, more readily reacting raw materials may be preferred over relatively inert ones, such as Al(OH)3 over Al2O3. Usually, the melts are carried out in platinum crucibles to reduce contamination from the crucible material. Glass homogeneity is achieved by homogenizing the raw materials mixture (glass batch), by stirring the melt, and by crushing and re-melting the first melt. The obtained glass is usually annealed to prevent breakage during processing.
In the past, small batches of amorphous metals with high surface area configurations (ribbons, wires, films, etc.) have been produced through the implementation of extremely rapid rates of cooling. This was initially termed "splat cooling" by doctoral student W. Klement at Caltech, who showed that cooling rates on the order of millions of degrees per second is sufficient to impede the formation of crystals, and the metallic atoms become "locked into" a glassy state. Amorphous metal wires have been produced by sputtering molten metal onto a spinning metal disk. More recently a number of alloys have been produced in layers with thickness exceeding 1 millimeter. These are known as bulk metallic glasses (BMG). Liquidmetal Technologies sell a number of zirconium-based BMGs. Batches of amorphous steel have also been produced that demonstrate mechanical properties far exceeding those found in conventional steel alloys.
In 2004, NIST researchers presented evidence that an isotropic non-crystalline metallic phase (dubbed "q-glass") could be grown from the melt. This phase is the first phase, or "primary phase," to form in the Al-Fe-Si system during rapid cooling. Interestingly, experimental evidence indicates that this phase forms by a first-order transition. Transmission electron microscopy (TEM) images show that the q-glass nucleates from the melt as discrete particles, which grow spherically with a uniform growth rate in all directions. The diffraction pattern shows it to be an isotropic glassy phase. Yet there is a nucleation barrier, which implies an interfacial discontinuity (or internal surface) between the glass and the melt.
Glass-ceramic materials share many properties with both non-crystalline glass and crystalline ceramics. They are formed as a glass, and then partially crystallized by heat treatment. For example, the microstructure of whiteware ceramics frequently contains both amorphous and crystalline phases. Crystalline grains are often embedded within a non-crystalline intergranular phase of grain boundaries. When applied to whiteware ceramics, vitreous means the material has an extremely low permeability to liquids, often but not always water, when determined by a specified test regime.
The term mainly refers to a mix of lithium and aluminosilicates that yields an array of materials with interesting thermomechanical properties. The most commercially important of these have the distinction of being impervious to thermal shock. Thus, glass-ceramics have become extremely useful for countertop cooking. The negative thermal expansion coefficient (CTE) of the crystalline ceramic phase can be balanced with the positive CTE of the glassy phase. At a certain point (~70% crystalline) the glass-ceramic has a net CTE near zero. This type of glass-ceramic exhibits excellent mechanical properties and can sustain repeated and quick temperature changes up to 1000 °C.
Mass production of glass window panes in the early twentieth century caused a similar effect. In glass factories, molten glass was poured onto a large cooling table and allowed to spread. The resulting glass is thicker at the location of the pour, located at the center of the large sheet. These sheets were cut into smaller window panes with nonuniform thickness, typically with the location of the pour centered in one of the panes (known as "bull's-eyes") for decorative effect. Modern glass intended for windows is produced as float glass and is very uniform in thickness.
The observation that old windows are sometimes found to be thicker at the bottom than at the top is often offered as supporting evidence for the view that glass flows over a timescale of centuries, the assumption being that the glass has exhibited the liquid property of flowing from one shape to another. This assumption is incorrect, as once solidified, glass stops flowing. The reason for the observation is that in the past, when panes of glass were commonly made by glassblowers, the technique used was to spin molten glass so as to create a round, mostly flat and even plate (the crown glass process, described above). This plate was then cut to fit a window. The pieces were not absolutely flat; the edges of the disk became a different thickness as the glass spun. When installed in a window frame, the glass would be placed with the thicker side down both for the sake of stability and to prevent water accumulating in the lead cames at the bottom of the window. Occasionally such glass has been found installed with the thicker side at the top, left or right.
In physics, the standard definition of a glass (or vitreous solid) is a solid formed by rapid melt quenching. The term glass is often used to describe any amorphous solid that exhibits a glass transition temperature Tg. If the cooling is sufficiently rapid (relative to the characteristic crystallization time) then crystallization is prevented and instead the disordered atomic configuration of the supercooled liquid is frozen into the solid state at Tg. The tendency for a material to form a glass while quenched is called glass-forming ability. This ability can be predicted by the rigidity theory. Generally, the structure of a glass exists in a metastable state with respect to its crystalline form, although in certain circumstances, for example in atactic polymers, there is no crystalline analogue of the amorphous phase.
Some people consider glass to be a liquid due to its lack of a first-order phase transition where certain thermodynamic variables such as volume, entropy and enthalpy are discontinuous through the glass transition range. The glass transition may be described as analogous to a second-order phase transition where the intensive thermodynamic variables such as the thermal expansivity and heat capacity are discontinuous. Nonetheless, the equilibrium theory of phase transformations does not entirely hold for glass, and hence the glass transition cannot be classed as one of the classical equilibrium phase transformations in solids.
Although the atomic structure of glass shares characteristics of the structure in a supercooled liquid, glass tends to behave as a solid below its glass transition temperature. A supercooled liquid behaves as a liquid, but it is below the freezing point of the material, and in some cases will crystallize almost instantly if a crystal is added as a core. The change in heat capacity at a glass transition and a melting transition of comparable materials are typically of the same order of magnitude, indicating that the change in active degrees of freedom is comparable as well. Both in a glass and in a crystal it is mostly only the vibrational degrees of freedom that remain active, whereas rotational and translational motion is arrested. This helps to explain why both crystalline and non-crystalline solids exhibit rigidity on most experimental time scales.
The term "Great Plains", for the region west of about the 96th or 98th meridian and east of the Rocky Mountains, was not generally used before the early 20th century. Nevin Fenneman's 1916 study, Physiographic Subdivision of the United States, brought the term Great Plains into more widespread usage. Before that the region was almost invariably called the High Plains, in contrast to the lower Prairie Plains of the Midwestern states. Today the term "High Plains" is used for a subregion of the Great Plains.
Much of the Great Plains became open range, or rangeland where cattle roamed free, hosting ranching operations where anyone was theoretically free to run cattle. In the spring and fall, ranchers held roundups where their cowboys branded new calves, treated animals and sorted the cattle for sale. Such ranching began in Texas and gradually moved northward. In 1866-95, cowboys herded 10 million cattle north to rail heads such as Dodge City, Kansas and Ogallala, Nebraska; from there, cattle were shipped eastward.
With the arrival of Francisco Vázquez de Coronado, a Spanish conquistador, the first recorded history of encounter between Europeans and Native Americans in the Great Plains occurred in Texas, Kansas and Nebraska from 1540-1542. In that same time period, Hernando de Soto crossed a west-northwest direction in what is now Oklahoma and Texas. Today this is known as the De Soto Trail. The Spanish thought the Great Plains were the location of the mythological Quivira and Cíbola, a place said to be rich in gold.
The 100th meridian roughly corresponds with the line that divides the Great Plains into an area that receive 20 inches (510 millimetres) or more of rainfall per year and an area that receives less than 20 in (510 mm). In this context, the High Plains, as well as Southern Alberta, south-western Saskatchewan and Eastern Montana are mainly semi hot steppe land and are generally characterised by rangeland or marginal farmland. The region (especially the High Plains) is periodically subjected to extended periods of drought; high winds in the region may then generate devastating dust storms. The eastern Great Plains near the eastern boundary falls in the humid subtropical climate zone in the southern areas, and the northern and central areas fall in the humid continental climate.
After 1870, the new railroads across the Plains brought hunters who killed off almost all the bison for their hides. The railroads offered attractive packages of land and transportation to European farmers, who rushed to settle the land. They (and Americans as well) also took advantage of the homestead laws to obtain free farms. Land speculators and local boosters identified many potential towns, and those reached by the railroad had a chance, while the others became ghost towns. In Kansas, for example, nearly 5000 towns were mapped out, but by 1970 only 617 were actually operating. In the mid-20th century, closeness to an interstate exchange determined whether a town would flourish or struggle for business.
The rural Plains have lost a third of their population since 1920. Several hundred thousand square miles (several hundred thousand square kilometers) of the Great Plains have fewer than 6 inhabitants per square mile (2.3 inhabitants per square kilometer)—the density standard Frederick Jackson Turner used to declare the American frontier "closed" in 1893. Many have fewer than 2 inhabitants per square mile (0.77 inhabitants per square kilometer). There are more than 6,000 ghost towns in the state of Kansas alone, according to Kansas historian Daniel Fitzgerald. This problem is often exacerbated by the consolidation of farms and the difficulty of attracting modern industry to the region. In addition, the smaller school-age population has forced the consolidation of school districts and the closure of high schools in some communities. The continuing population loss has led some to suggest that the current use of the drier parts of the Great Plains is not sustainable, and there has been a proposal - the "Buffalo Commons" - to return approximately 139,000 square miles (360,000 km2) of these drier parts to native prairie land.
Although the eastern image of farm life in the prairies emphasized the isolation of the lonely farmer and wife, plains residents created busy social lives for themselves. They often sponsored activities that combined work, food and entertainment such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities and school functions. Women organized shared meals and potluck events, as well as extended visits between families. The Grange was a nationwide farmers' organization, they reserved high offices for women, and gave them a voice in public affairs.
From the 1950s on, many areas of the Great Plains have become productive crop-growing areas because of extensive irrigation on large landholdings. The United States is a major exporter of agricultural products. The southern portion of the Great Plains lies over the Ogallala Aquifer, a huge underground layer of water-bearing strata dating from the last ice age. Center pivot irrigation is used extensively in drier sections of the Great Plains, resulting in aquifer depletion at a rate that is greater than the ground's ability to recharge.
The Great Plains is the broad expanse of flat land (a plain), much of it covered in prairie, steppe and grassland, that lies west of the Mississippi River tallgrass prairie states and east of the Rocky Mountains in the United States and Canada. This area covers parts, but not all, of the states of Colorado, Kansas, Montana, Nebraska, New Mexico, North Dakota, Oklahoma, South Dakota, Texas, and Wyoming, and the Canadian provinces of Alberta, Manitoba and Saskatchewan. The region is known for supporting extensive cattle ranching and dry farming.
The North American Environmental Atlas, produced by the Commission for Environmental Cooperation, a NAFTA agency composed of the geographical agencies of the Mexican, American, and Canadian governments uses the "Great Plains" as an ecoregion synonymous with predominant prairies and grasslands rather than as physiographic region defined by topography. The Great Plains ecoregion includes five sub-regions: Temperate Prairies, West-Central Semi-Arid Prairies, South-Central Semi-Arid Prairies, Texas Louisiana Coastal Plains, and Tamaulipus-Texas Semi-Arid Plain, which overlap or expand upon other Great Plains designations.
The railroads opened up the Great Plains for settlement, for now it was possible to ship wheat and other crops at low cost to the urban markets in the East, and Europe. Homestead land was free for American settlers. Railroads sold their land at cheap rates to immigrants in expectation they would generate traffic as soon as farms were established. Immigrants poured in, especially from Germany and Scandinavia. On the plains, very few single men attempted to operate a farm or ranch by themselves; they clearly understood the need for a hard-working wife, and numerous children, to handle the many chores, including child-rearing, feeding and clothing the family, managing the housework, feeding the hired hands, and, especially after the 1930s, handling paperwork and financial details. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After approximately one generation, women increasingly left the fields, thus redefining their roles within the family. New technology including sewing and washing machines encouraged women to turn to domestic roles. The scientific housekeeping movement, promoted across the land by the media and government extension agents, as well as county fairs which featured achievements in home cookery and canning, advice columns for women regarding farm bookkeeping, and home economics courses in the schools.
During the Cenozoic era, specifically about 25 million years ago during the Miocene and Pliocene epochs, the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. The grasslands provided a new niche for mammals, including many ungulates and glires, that switched from browsing diets to grazing diets. Traditionally, the spread of grasslands and the development of grazers have been strongly linked. However, an examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes in mammals, giving rise to the "grit, not grass" hypothesis.
To allow for agricultural development of the Great Plains and house a growing population, the US passed the Homestead Acts of 1862: it allowed a settler to claim up to 160 acres (65 ha) of land, provided that he lived on it for a period of five years and cultivated it. The provisions were expanded under the Kinkaid Act of 1904 to include a homestead of an entire section. Hundreds of thousands of people claimed such homesteads, sometimes building sod houses out of the very turf of their land. Many of them were not skilled dryland farmers and failures were frequent. Much of the Plains were settled during relatively wet years. Government experts did not understand how farmers should cultivate the prairies and gave advice counter to what would have worked[citation needed]. Germans from Russia who had previously farmed, under similar circumstances, in what is now Ukraine were marginally more successful than other homesteaders. The Dominion Lands Act of 1871 served a similar function for establishing homesteads on the prairies in Canada.