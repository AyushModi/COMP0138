{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDqtOovDfgv8OV9Kui6zaB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers\n","## scatter 1.12+cu113\n","# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n","# scatter 1.13+cu116\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.0+cu116.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"180adOJqfWTG","executionInfo":{"status":"ok","timestamp":1674496080896,"user_tz":0,"elapsed":17786,"user":{"displayName":"Ayush Modi","userId":"15866713891896926467"}},"outputId":"9f8ba085-4f97-4a11-91fb-2501f705efec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.1.0+pt113cu116\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch_scatter import scatter_max\n","from transformers import BertModel, BertTokenizer"],"metadata":{"id":"3dcoNqwPfpes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def return_mask_lengths(ids):\n","    mask = torch.sign(ids).float()\n","    lengths = torch.sum(mask, 1)\n","    return mask, lengths\n","\n","\n","def cal_attn(query, memories, mask):\n","    ## memories is c_hs, the per-state output\n","    \n","    ## mask is 0 at the paddings\n","    ## line below sets padding to -10000\n","    # mask=1-mask; mask[mask==1] = -float(\"inf\")\n","    mask = (1.0 - mask.float()) * -10000.0\n","    attn_logits = torch.matmul(query, memories.transpose(-1, -2).contiguous())\n","    attn_logits = attn_logits + mask\n","    ## padding goes to 0, because we do softmax of -10000,\n","    attn_weights = F.softmax(attn_logits, dim=-1)\n","    attn_outputs = torch.matmul(attn_weights, memories)\n","    return attn_outputs, attn_logits\n","\n","\n","def gumbel_softmax(logits, tau=1, hard=False, eps=1e-20, dim=-1):\n","    # type: (Tensor, float, bool, float, int) -> Tensor\n","\n","    gumbels = -(torch.empty_like(logits).exponential_() +\n","                eps).log()  # ~Gumbel(0,1)\n","    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n","    y_soft = gumbels.softmax(dim)\n","\n","    if hard:\n","        # Straight through.\n","        index = y_soft.max(dim, keepdim=True)[1]\n","        y_hard = torch.zeros_like(logits).scatter_(dim, index, 1.0)\n","        ret = y_hard - y_soft.detach() + y_soft\n","    else:\n","        # Re-parametrization trick.\n","        ret = y_soft\n","    return ret"],"metadata":{"id":"vN-MgS2cfy-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class CategoricalKLLoss(nn.Module):\n","    def __init__(self):\n","        super(CategoricalKLLoss, self).__init__()\n","\n","    def forward(self, P, Q):\n","        log_P = P.log()\n","        log_Q = Q.log()\n","        kl = (P * (log_P - log_Q)).sum(dim=-1).sum(dim=-1)\n","        return kl.mean(dim=0)\n","\n","\n","class GaussianKLLoss(nn.Module):\n","    def __init__(self):\n","        super(GaussianKLLoss, self).__init__()\n","\n","    def forward(self, mu1, logvar1, mu2, logvar2):\n","        numerator = logvar1.exp() + torch.pow(mu1 - mu2, 2)\n","        fraction = torch.div(numerator, (logvar2.exp()))\n","        kl = 0.5 * torch.sum(logvar2 - logvar1 + fraction - 1, dim=1)\n","        return kl.mean(dim=0)\n","\n","\n","class Embedding(nn.Module):\n","    def __init__(self, bert_model):\n","        super(Embedding, self).__init__()\n","        bert_embeddings = BertModel.from_pretrained(bert_model).embeddings\n","        self.word_embeddings = bert_embeddings.word_embeddings\n","        self.token_type_embeddings = bert_embeddings.token_type_embeddings\n","        self.position_embeddings = bert_embeddings.position_embeddings\n","        self.LayerNorm = bert_embeddings.LayerNorm\n","        self.dropout = bert_embeddings.dropout\n","\n","    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","        if position_ids is None:\n","            seq_length = input_ids.size(1)\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device)\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","\n","        embeddings = words_embeddings + token_type_embeddings + position_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","\n","        return embeddings\n","\n","\n","class ContextualizedEmbedding(nn.Module):\n","    def __init__(self, bert_model):\n","        super(ContextualizedEmbedding, self).__init__()\n","        bert = BertModel.from_pretrained(bert_model)\n","        self.embedding = bert.embeddings\n","        self.encoder = bert.encoder\n","        self.num_hidden_layers = bert.config.num_hidden_layers\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(\n","            seq_length, dtype=torch.long, device=input_ids.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","\n","        extended_attention_mask = attention_mask.unsqueeze(\n","            1).unsqueeze(2).float()\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","        head_mask = [None] * self.num_hidden_layers\n","\n","        embedding_output = self.embedding(\n","            input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n","        encoder_outputs = self.encoder(embedding_output,\n","                                       extended_attention_mask,\n","                                       head_mask=head_mask)\n","        sequence_output = encoder_outputs[0]\n","\n","        return sequence_output\n","\n","\n","class CustomLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional=False):\n","        super(CustomLSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.bidirectional = bidirectional\n","        self.dropout = nn.Dropout(dropout)\n","        if dropout > 0.0 and num_layers == 1:\n","            dropout = 0.0\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                            num_layers=num_layers, dropout=dropout,\n","                            bidirectional=bidirectional, batch_first=True)\n","\n","    def forward(self, inputs, input_lengths, state=None):\n","        _, total_length, _ = inputs.size()\n","\n","        input_packed = pack_padded_sequence(inputs, input_lengths.cpu(),\n","                                            batch_first=True, enforce_sorted=False)\n","\n","        self.lstm.flatten_parameters()\n","        output_packed, state = self.lstm(input_packed, state)\n","\n","        output = pad_packed_sequence(\n","            output_packed, batch_first=True, total_length=total_length)[0]\n","        output = self.dropout(output)\n","\n","        return output, state"],"metadata":{"id":"rRxZRPnTf1aP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosteriorEncoder(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 nzqdim, nza, nzadim,nzddim,\n","                 dropout=0.0):\n","        super(PosteriorEncoder, self).__init__()\n","\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.nlayers = nlayers\n","        self.nzqdim = nzqdim\n","        self.nzddim = nzddim\n","        self.nza = nza\n","        self.nzadim = nzadim\n","\n","        self.encoder = CustomLSTM(input_size=emsize,\n","                                  hidden_size=nhidden,\n","                                  num_layers=nlayers,\n","                                  dropout=dropout,\n","                                  bidirectional=True)\n","\n","        \n","\n","        self.question_attention = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.context_attention = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.distractor_attention = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.zq_attention = nn.Linear(nzddim, 2 * nhidden)\n","\n","        self.zq_linear = nn.Linear(4 * 2 * nhidden, 2 * nzqdim)\n","        self.zd_linear = nn.Linear(nzqdim + 5 * 2 * nhidden, 2 * nzddim)\n","        self.za_linear = nn.Linear(nzqdim + 2 * 2 * nhidden, nza * nzadim)\n","\n","    def forward(self, c_ids, q_ids, a_ids, d_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        q_mask, q_lengths = return_mask_lengths(q_ids)\n","        d_mask, d_lengths = return_mask_lengths(d_ids)\n","\n","        # question enc\n","        q_embeddings = self.embedding(q_ids)\n","        q_hs, q_state = self.encoder(q_embeddings, q_lengths)\n","        q_h = q_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        q_h = q_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","        \n","        ## distractor enc \n","        d_embeddings = self.embedding(d_ids)\n","        d_hs, d_state = self.encoder(d_embeddings, d_lengths)\n","        d_h = d_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        d_h = d_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","        ##\n","\n","        # context enc\n","        c_embeddings = self.embedding(c_ids)\n","        c_hs, c_state = self.encoder(c_embeddings, c_lengths)\n","        c_h = c_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_h = c_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        # context and answer enc\n","        c_a_embeddings = self.embedding(c_ids, a_ids, None)\n","        c_a_hs, c_a_state = self.encoder(c_a_embeddings, c_lengths)\n","        c_a_h = c_a_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_a_h = c_a_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        \n","        # ## context and distractor enc\n","        # c_d_embeddings = self.embedding(c_ids, d_ids, None)\n","        # c_d_hs, c_d_state = self.encoder(c_d_embeddings, c_lengths)\n","        # c_d_h = c_d_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        # c_d_h = c_d_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","        # ##\n","\n","        # attetion q, c\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_q, _ = cal_attn(self.question_attention(q_h).unsqueeze(1),\n","                                    c_hs,\n","                                    mask)\n","        c_attned_by_q = c_attned_by_q.squeeze(1)\n","\n","        # attetion c, q\n","        mask = q_mask.unsqueeze(1)\n","        q_attned_by_c, _ = cal_attn(self.context_attention(c_h).unsqueeze(1),\n","                                    q_hs,\n","                                    mask)\n","        q_attned_by_c = q_attned_by_c.squeeze(1)\n","\n","        h = torch.cat([q_h, q_attned_by_c, c_h, c_attned_by_q], dim=-1)\n","\n","        zq_mu, zq_logvar = torch.split(self.zq_linear(h), self.nzqdim, dim=1)\n","        zq = zq_mu + torch.randn_like(zq_mu) * torch.exp(0.5 * zq_logvar)\n","\n","        ## attention d, c\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_d, _ = cal_attn(self.distractor_attention(d_h).unsqueeze(1),\n","                                       d_hs,\n","                                       mask)\n","        c_attned_by_d = c_attned_by_d.squeeze(1)\n","        ## attetion c, d\n","        mask = d_mask.unsqueeze(1)\n","        d_attned_by_c, _ = cal_attn(self.context_attention(c_h).unsqueeze(1),\n","                                    d_hs,\n","                                    mask)\n","        d_attned_by_c = d_attned_by_c.squeeze(1)\n","\n","        ## attention zq, c\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_zq, _ = cal_attn(self.zq_attention(zq).unsqueeze(1),\n","                                       c_hs,\n","                                       mask)\n","        c_attned_by_zq = c_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([d_h, d_attned_by_c,c_h, c_attned_by_d,zq,c_attned_by_zq], dim=-1)\n","\n","        zd_mu, zd_logvar = torch.split(self.zd_linear(h), self.nzddim, dim=1)\n","        zd = zd_mu + torch.randn_like(zd_mu) * torch.exp(0.5 * zd_logvar)\n","        ##\n","\n","        # attention zq, c_a\n","        mask = c_mask.unsqueeze(1)\n","        c_a_attned_by_zq, _ = cal_attn(self.zq_attention(zq).unsqueeze(1),\n","                                       c_a_hs,\n","                                       mask)\n","        c_a_attned_by_zq = c_a_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([zq, c_a_attned_by_zq, c_a_h], dim=-1)\n","\n","        za_logits = self.za_linear(h).view(-1, self.nza, self.nzadim)\n","        za_prob = F.softmax(za_logits, dim=-1)\n","        za = gumbel_softmax(za_logits, hard=True)\n","\n","        return zq_mu, zq_logvar, zq, za_prob, za, zd_mu, zd_logvar, zd\n"],"metadata":{"id":"QiuwzvMff9Ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PriorEncoder(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 nzqdim, nza, nzadim,nzddim,\n","                 dropout=0):\n","        super(PriorEncoder, self).__init__()\n","\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.nlayers = nlayers\n","        self.nzqdim = nzqdim\n","        self.nzddim = nzddim\n","        self.nza = nza\n","        self.nzadim = nzadim\n","\n","        self.context_encoder = CustomLSTM(input_size=emsize,\n","                                          hidden_size=nhidden,\n","                                          num_layers=nlayers,\n","                                          dropout=dropout,\n","                                          bidirectional=True)\n","\n","        self.zq_attention = nn.Linear(nzqdim, 2 * nhidden)\n","\n","        self.zq_linear = nn.Linear(2 * nhidden, 2 * nzqdim)\n","        self.zd_linear = nn.Linear(nzqdim + 2 * 2 * nhidden, 2 * nzddim)\n","        self.za_linear = nn.Linear(nzqdim + 2 * 2 * nhidden, nza * nzadim)\n","\n","    def forward(self, c_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","\n","        c_embeddings = self.embedding(c_ids)\n","        c_hs, c_state = self.context_encoder(c_embeddings, c_lengths)\n","        c_h = c_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_h = c_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        zq_mu, zq_logvar = torch.split(self.zq_linear(c_h), self.nzqdim, dim=1)\n","        zq = zq_mu + torch.randn_like(zq_mu)*torch.exp(0.5*zq_logvar)\n","\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_zq, _ = cal_attn(self.zq_attention(zq).unsqueeze(1),\n","                                     c_hs,\n","                                     mask)\n","        c_attned_by_zq = c_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([zq, c_attned_by_zq, c_h], dim=-1)\n","\n","        zd_mu, zd_logvar = torch.split(self.zd_linear(h), self.nzddim, dim=1)\n","        zd = zd_mu + torch.randn_like(zd_mu)*torch.exp(0.5*zd_logvar)\n","\n","        za_logits = self.za_linear(h).view(-1, self.nza, self.nzadim)\n","        za_prob = F.softmax(za_logits, dim=-1)\n","        za = gumbel_softmax(za_logits, hard=True)\n","\n","        return zq_mu, zq_logvar, zq, za_prob, za,zd_mu, zd_logvar, zd\n","\n","    def interpolation(self, c_ids, zq):\n","\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","\n","        c_embeddings = self.embedding(c_ids)\n","        c_hs, c_state = self.context_encoder(c_embeddings, c_lengths)\n","        c_h = c_state[0].view(self.nlayers, 2, -1, self.nhidden)[-1]\n","        c_h = c_h.transpose(0, 1).contiguous().view(-1, 2 * self.nhidden)\n","\n","        mask = c_mask.unsqueeze(1)\n","        c_attned_by_zq, _ = cal_attn(\n","            self.zq_attention(zq).unsqueeze(1), c_hs, mask)\n","        c_attned_by_zq = c_attned_by_zq.squeeze(1)\n","\n","        h = torch.cat([zq, c_attned_by_zq, c_h], dim=-1)\n","\n","        za_logits = self.za_linear(h).view(-1, self.nza, self.nzadim)\n","        za = gumbel_softmax(za_logits, hard=True)\n","\n","        return za\n"],"metadata":{"id":"Fq8_BS7vgALB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AnswerDecoder(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 dropout=0.0):\n","        super(AnswerDecoder, self).__init__()\n","\n","        self.embedding = embedding\n","\n","        self.context_lstm = CustomLSTM(input_size=4 * emsize,\n","                                       hidden_size=nhidden,\n","                                       num_layers=nlayers,\n","                                       dropout=dropout,\n","                                       bidirectional=True)\n","\n","        self.start_linear = nn.Linear(2 * nhidden, 1)\n","        self.end_linear = nn.Linear(2 * nhidden, 1)\n","        self.ls = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, init_state, c_ids):\n","        _, max_c_len = c_ids.size()\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","\n","        H = self.embedding(c_ids, c_mask)\n","        U = init_state.unsqueeze(1).repeat(1, max_c_len, 1)\n","        G = torch.cat([H, U, H * U, torch.abs(H - U)], dim=-1)\n","        M, _ = self.context_lstm(G, c_lengths)\n","\n","        start_logits = self.start_linear(M).squeeze(-1)\n","        end_logits = self.end_linear(M).squeeze(-1)\n","\n","        start_end_mask = (c_mask == 0)\n","        masked_start_logits = start_logits.masked_fill(\n","            start_end_mask, -10000.0)\n","        masked_end_logits = end_logits.masked_fill(start_end_mask, -10000.0)\n","\n","        return masked_start_logits, masked_end_logits\n","\n","    def generate(self, init_state, c_ids):\n","        start_logits, end_logits = self.forward(init_state, c_ids)\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        batch_size, max_c_len = c_ids.size()\n","\n","        mask = torch.matmul(c_mask.unsqueeze(2).float(),\n","                            c_mask.unsqueeze(1).float())\n","        mask = torch.triu(mask) == 0\n","        score = (self.ls(start_logits).unsqueeze(2)\n","                 + self.ls(end_logits).unsqueeze(1))\n","        score = score.masked_fill(mask, -10000.0)\n","        score, start_positions = score.max(dim=1)\n","        score, end_positions = score.max(dim=1)\n","        start_positions = torch.gather(start_positions,\n","                                       1,\n","                                       end_positions.view(-1, 1)).squeeze(1)\n","\n","        idxes = torch.arange(0, max_c_len, out=torch.LongTensor(max_c_len))\n","        idxes = idxes.unsqueeze(0).to(\n","            start_logits.device).repeat(batch_size, 1)\n","\n","        start_positions = start_positions.unsqueeze(1)\n","        start_mask = (idxes >= start_positions).long()\n","        end_positions = end_positions.unsqueeze(1)\n","        end_mask = (idxes <= end_positions).long()\n","        a_ids = start_mask + end_mask - 1\n","\n","        return a_ids, start_positions.squeeze(1), end_positions.squeeze(1)"],"metadata":{"id":"mDltTuEWgDKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ContextEncoderforQG(nn.Module):\n","    def __init__(self, embedding, emsize,\n","                 nhidden, nlayers,\n","                 dropout=0.0):\n","        super(ContextEncoderforQG, self).__init__()\n","        self.embedding = embedding\n","        self.context_lstm = CustomLSTM(input_size=emsize,\n","                                       hidden_size=nhidden,\n","                                       num_layers=nlayers,\n","                                       dropout=dropout,\n","                                       bidirectional=True)\n","        self.context_linear = nn.Linear(2 * nhidden, 2 * nhidden)\n","        self.fusion = nn.Linear(4 * nhidden, 2 * nhidden, bias=False)\n","        self.gate = nn.Linear(4 * nhidden, 2 * nhidden, bias=False)\n","\n","    def forward(self, c_ids, a_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        c_embeddings = self.embedding(c_ids, c_mask, a_ids)\n","        c_outputs, _ = self.context_lstm(c_embeddings, c_lengths)\n","        # attention\n","        mask = torch.matmul(c_mask.unsqueeze(2), c_mask.unsqueeze(1))\n","        c_attned_by_c, _ = cal_attn(self.context_linear(c_outputs),\n","                                    c_outputs,\n","                                    mask)\n","        c_concat = torch.cat([c_outputs, c_attned_by_c], dim=2)\n","        c_fused = self.fusion(c_concat).tanh()\n","        c_gate = self.gate(c_concat).sigmoid()\n","        c_outputs = c_gate * c_fused + (1 - c_gate) * c_outputs\n","        return c_outputs"],"metadata":{"id":"0FBi4VEIgGF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QuestionDecoder(nn.Module):\n","    def __init__(self, sos_id, eos_id,\n","                 embedding, contextualized_embedding, emsize,\n","                 nhidden, ntokens, nlayers,\n","                 dropout=0.0,\n","                 max_q_len=64):\n","        super(QuestionDecoder, self).__init__()\n","\n","        self.sos_id = sos_id\n","        self.eos_id = eos_id\n","        self.emsize = emsize\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.ntokens = ntokens\n","        self.nlayers = nlayers\n","        # this max_len include sos eos\n","        self.max_q_len = max_q_len\n","\n","        self.context_lstm = ContextEncoderforQG(contextualized_embedding, emsize,\n","                                                nhidden // 2, nlayers, dropout)\n","\n","        self.question_lstm = CustomLSTM(input_size=emsize,\n","                                        hidden_size=nhidden,\n","                                        num_layers=nlayers,\n","                                        dropout=dropout,\n","                                        bidirectional=False)\n","\n","        self.question_linear = nn.Linear(nhidden, nhidden)\n","\n","        self.concat_linear = nn.Sequential(nn.Linear(2*nhidden, 2*nhidden),\n","                                           nn.Dropout(dropout),\n","                                           nn.Linear(2*nhidden, 2*emsize))\n","\n","        self.logit_linear = nn.Linear(emsize, ntokens, bias=False)\n","\n","        # fix output word matrix\n","        self.logit_linear.weight = embedding.word_embeddings.weight\n","        for param in self.logit_linear.parameters():\n","            param.requires_grad = False\n","\n","        self.discriminator = nn.Bilinear(emsize, nhidden, 1)\n","\n","    def postprocess(self, q_ids):\n","        eos_mask = q_ids == self.eos_id\n","        no_eos_idx_sum = (eos_mask.sum(dim=1) == 0).long() * \\\n","            (self.max_q_len - 1)\n","        eos_mask = eos_mask.cpu().numpy()\n","        q_lengths = np.argmax(eos_mask, axis=1) + 1\n","        q_lengths = torch.tensor(q_lengths).to(\n","            q_ids.device).long() + no_eos_idx_sum\n","        batch_size, max_len = q_ids.size()\n","        idxes = torch.arange(0, max_len).to(q_ids.device)\n","        idxes = idxes.unsqueeze(0).repeat(batch_size, 1)\n","        q_mask = (idxes < q_lengths.unsqueeze(1))\n","        q_ids = q_ids.long() * q_mask.long()\n","        return q_ids\n","\n","    def forward(self, init_state, c_ids, q_ids, a_ids):\n","        batch_size, max_q_len = q_ids.size()\n","\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        q_mask, q_lengths = return_mask_lengths(q_ids)\n","\n","        # question dec\n","        q_embeddings = self.embedding(q_ids)\n","        q_outputs, _ = self.question_lstm(q_embeddings, q_lengths, init_state)\n","\n","        # attention\n","        mask = torch.matmul(q_mask.unsqueeze(2), c_mask.unsqueeze(1))\n","        c_attned_by_q, attn_logits = cal_attn(self.question_linear(q_outputs),\n","                                              c_outputs,\n","                                              mask)\n","\n","        # gen logits\n","        q_concated = torch.cat([q_outputs, c_attned_by_q], dim=2)\n","        q_concated = self.concat_linear(q_concated)\n","        q_maxouted, _ = q_concated.view(\n","            batch_size, max_q_len, self.emsize, 2).max(dim=-1)\n","        gen_logits = self.logit_linear(q_maxouted)\n","\n","        # copy logits\n","        bq = batch_size * max_q_len\n","        c_ids = c_ids.unsqueeze(1).repeat(\n","            1, max_q_len, 1).view(bq, -1).contiguous()\n","        attn_logits = attn_logits.view(bq, -1).contiguous()\n","        copy_logits = torch.zeros(bq, self.ntokens).to(c_ids.device)\n","        copy_logits = copy_logits - 10000.0\n","        copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","        copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","        copy_logits = copy_logits.view(batch_size, max_q_len, -1).contiguous()\n","\n","        logits = gen_logits + copy_logits\n","\n","        # mutual information btw answer and question\n","        a_emb = c_outputs * a_ids.float().unsqueeze(2)\n","        a_mean_emb = torch.sum(a_emb, 1) / a_ids.sum(1).unsqueeze(1).float()\n","        fake_a_mean_emb = torch.cat([a_mean_emb[-1].unsqueeze(0),\n","                                     a_mean_emb[:-1]], dim=0)\n","\n","        q_emb = q_maxouted * q_mask.unsqueeze(2)\n","        q_mean_emb = torch.sum(q_emb, 1) / q_lengths.unsqueeze(1).float()\n","        fake_q_mean_emb = torch.cat([q_mean_emb[-1].unsqueeze(0),\n","                                     q_mean_emb[:-1]], dim=0)\n","\n","        bce_loss = nn.BCEWithLogitsLoss()\n","        true_logits = self.discriminator(q_mean_emb, a_mean_emb)\n","        true_labels = torch.ones_like(true_logits)\n","\n","        fake_a_logits = self.discriminator(q_mean_emb, fake_a_mean_emb)\n","        fake_q_logits = self.discriminator(fake_q_mean_emb, a_mean_emb)\n","        fake_logits = torch.cat([fake_a_logits, fake_q_logits], dim=0)\n","        fake_labels = torch.zeros_like(fake_logits)\n","\n","        true_loss = bce_loss(true_logits, true_labels)\n","        fake_loss = 0.5 * bce_loss(fake_logits, fake_labels)\n","        loss_info = 0.5 * (true_loss + fake_loss)\n","\n","        return logits, loss_info\n","\n","    def generate(self, init_state, c_ids, a_ids):\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        batch_size = c_ids.size(0)\n","\n","        q_ids = torch.LongTensor([self.sos_id] * batch_size).unsqueeze(1)\n","        q_ids = q_ids.to(c_ids.device)\n","        token_type_ids = torch.zeros_like(q_ids)\n","        position_ids = torch.zeros_like(q_ids)\n","        q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        state = init_state\n","\n","        # unroll\n","        all_q_ids = list()\n","        all_q_ids.append(q_ids)\n","        for _ in range(self.max_q_len - 1):\n","            position_ids = position_ids + 1\n","            q_outputs, state = self.question_lstm.lstm(q_embeddings, state)\n","\n","            # attention\n","            mask = c_mask.unsqueeze(1)\n","            c_attned_by_q, attn_logits = cal_attn(self.question_linear(q_outputs),\n","                                                  c_outputs,\n","                                                  mask)\n","\n","            # gen logits\n","            q_concated = torch.cat([q_outputs, c_attned_by_q], dim=2)\n","            q_concated = self.concat_linear(q_concated)\n","            q_maxouted, _ = q_concated.view(\n","                batch_size, 1, self.emsize, 2).max(dim=-1)\n","            gen_logits = self.logit_linear(q_maxouted)\n","\n","            # copy logits\n","            attn_logits = attn_logits.squeeze(1)\n","            copy_logits = torch.zeros(\n","                batch_size, self.ntokens).to(c_ids.device)\n","            copy_logits = copy_logits - 10000.0\n","            copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","            copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","\n","            logits = gen_logits + copy_logits.unsqueeze(1)\n","\n","            q_ids = torch.argmax(logits, 2)\n","            all_q_ids.append(q_ids)\n","\n","            q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        q_ids = torch.cat(all_q_ids, 1)\n","        q_ids = self.postprocess(q_ids)\n","\n","        return q_ids\n","\n","    def sample(self, init_state, c_ids, a_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        batch_size = c_ids.size(0)\n","\n","        q_ids = torch.LongTensor([self.sos_id] * batch_size).unsqueeze(1)\n","        q_ids = q_ids.to(c_ids.device)\n","        token_type_ids = torch.zeros_like(q_ids)\n","        position_ids = torch.zeros_like(q_ids)\n","        q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        state = init_state\n","\n","        # unroll\n","        all_q_ids = list()\n","        all_q_ids.append(q_ids)\n","        for _ in range(self.max_q_len - 1):\n","            position_ids = position_ids + 1\n","            q_outputs, state = self.question_lstm.lstm(q_embeddings, state)\n","\n","            # attention\n","            mask = c_mask.unsqueeze(1)\n","            c_attned_by_q, attn_logits = cal_attn(self.question_linear(q_outputs),\n","                                                  c_outputs,\n","                                                  mask)\n","\n","            # gen logits\n","            q_concated = torch.cat([q_outputs, c_attned_by_q], dim=2)\n","            q_concated = self.concat_linear(q_concated)\n","            q_maxouted, _ = q_concated.view(batch_size, 1, self.emsize, 2).max(dim=-1)\n","            gen_logits = self.logit_linear(q_maxouted)\n","\n","            # copy logits\n","            attn_logits = attn_logits.squeeze(1)\n","            copy_logits = torch.zeros(batch_size, self.ntokens).to(c_ids.device)\n","            copy_logits = copy_logits - 10000.0\n","            copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","            copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","\n","            logits = gen_logits + copy_logits.unsqueeze(1)\n","            logits = logits.squeeze(1)\n","            logits = self.top_k_top_p_filtering(logits, 2, top_p=0.8)\n","            probs = F.softmax(logits, dim=-1)\n","            q_ids = torch.multinomial(probs, num_samples=1)  # [b,1]\n","            all_q_ids.append(q_ids)\n","\n","            q_embeddings = self.embedding(q_ids, token_type_ids, position_ids)\n","\n","        q_ids = torch.cat(all_q_ids, 1)\n","        q_ids = self.postprocess(q_ids)\n","\n","        return q_ids\n"],"metadata":{"id":"1W0l6wP5gJH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DistractorDecoder(nn.Module):\n","    def __init__(self, sos_id, eos_id,\n","                 embedding, contextualized_embedding, emsize,\n","                 nhidden, ntokens, nlayers,\n","                 dropout=0.0,\n","                 max_d_len=64):\n","        super(DistractorDecoder, self).__init__()\n","\n","        self.sos_id = sos_id\n","        self.eos_id = eos_id\n","        self.emsize = emsize\n","        self.embedding = embedding\n","        self.nhidden = nhidden\n","        self.ntokens = ntokens\n","        self.nlayers = nlayers\n","        # this max_len include sos eos\n","        self.max_d_len = max_d_len \n","\n","        self.context_lstm = ContextEncoderforQG(contextualized_embedding, emsize,\n","                                                nhidden // 2, nlayers, dropout)\n","\n","        self.distractor_lstm = CustomLSTM(input_size=emsize,\n","                                        hidden_size=nhidden,\n","                                        num_layers=nlayers,\n","                                        dropout=dropout,\n","                                        bidirectional=False) ## make true?\n","\n","        self.distractor_linear = nn.Linear(nhidden, nhidden)\n","\n","        self.concat_linear = nn.Sequential(nn.Linear(2*nhidden, 2*nhidden),\n","                                           nn.Dropout(dropout),\n","                                           nn.Linear(2*nhidden, 2*emsize))\n","\n","        self.logit_linear = nn.Linear(emsize, ntokens, bias=False)\n","\n","        # fix output word matrix\n","        self.logit_linear.weight = embedding.word_embeddings.weight\n","        for param in self.logit_linear.parameters():\n","            param.requires_grad = False\n","\n","        self.discriminator = nn.Bilinear(emsize, nhidden, 1)\n","\n","    def postprocess(self, d_ids):\n","        eos_mask = d_ids == self.eos_id\n","        no_eos_idx_sum = (eos_mask.sum(dim=1) == 0).long() * \\\n","            (self.max_q_len - 1)\n","        eos_mask = eos_mask.cpu().numpy()\n","        d_lengths = np.argmax(eos_mask, axis=1) + 1\n","        d_lengths = torch.tensor(d_lengths).to(\n","            d_ids.device).long() + no_eos_idx_sum\n","        batch_size, max_len = d_ids.size()\n","        idxes = torch.arange(0, max_len).to(d_ids.device)\n","        idxes = idxes.unsqueeze(0).repeat(batch_size, 1)\n","        d_mask = (idxes < d_lengths.unsqueeze(1))\n","        d_ids = d_ids.long() * d_mask.long()\n","        return d_ids\n","\n","    def forward(self, init_state, c_ids, q_ids, a_ids,d_ids,q_maxouted):\n","        batch_size, max_d_len = d_ids.size()\n","\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        d_mask, d_lengths = return_mask_lengths(d_ids)\n","        q_mask, q_lengths = return_mask_lengths(q_ids)\n","\n","        # distractor dec\n","        d_embeddings = self.embedding(d_ids)\n","        d_outputs, _ = self.distractor_lstm(d_embeddings, d_lengths, init_state)\n","\n","        # attention\n","        mask = torch.matmul(d_mask.unsqueeze(2), c_mask.unsqueeze(1))\n","        c_attned_by_d, attn_logits = cal_attn(self.distractor_linear(d_outputs),\n","                                              c_outputs,\n","                                              mask)\n","\n","        # gen logits\n","        d_concated = torch.cat([d_outputs, c_attned_by_d], dim=2)\n","        d_concated = self.concat_linear(d_concated)\n","        d_maxouted, _ = d_concated.view(\n","            batch_size, max_d_len, self.emsize, 2).max(dim=-1)\n","        gen_logits = self.logit_linear(d_maxouted)\n","\n","        # copy logits\n","        bd = batch_size * max_d_len\n","        c_ids = c_ids.unsqueeze(1).repeat(\n","            1, max_d_len, 1).view(bd, -1).contiguous()\n","        attn_logits = attn_logits.view(bd, -1).contiguous()\n","        copy_logits = torch.zeros(bd, self.ntokens).to(c_ids.device)\n","        copy_logits = copy_logits - 10000.0\n","        copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","        copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","        copy_logits = copy_logits.view(batch_size, max_d_len, -1).contiguous()\n","\n","        logits = gen_logits + copy_logits\n","\n","        # mutual information btw answer and distractor\n","        a_emb = c_outputs * a_ids.float().unsqueeze(2)\n","        a_mean_emb = torch.sum(a_emb, 1) / a_ids.sum(1).unsqueeze(1).float()\n","        fake_a_mean_emb = torch.cat([a_mean_emb[-1].unsqueeze(0),\n","                                     a_mean_emb[:-1]], dim=0)\n","\n","        d_emb = d_maxouted * d_mask.unsqueeze(2)\n","        d_mean_emb = torch.sum(d_emb, 1) / d_lengths.unsqueeze(1).float()\n","        fake_d_mean_emb = torch.cat([d_mean_emb[-1].unsqueeze(0),\n","                                     d_mean_emb[:-1]], dim=0)\n","\n","        bce_loss = nn.BCEWithLogitsLoss()\n","        true_logits = self.discriminator(d_mean_emb, a_mean_emb)\n","        true_labels = torch.ones_like(true_logits)\n","\n","        fake_a_logits = self.discriminator(d_mean_emb, fake_a_mean_emb)\n","        fake_d_logits = self.discriminator(fake_d_mean_emb, a_mean_emb)\n","        fake_logits = torch.cat([fake_a_logits, fake_d_logits], dim=0)\n","        fake_labels = torch.zeros_like(fake_logits)\n","\n","        true_loss = bce_loss(true_logits, true_labels)\n","        fake_loss = 0.5 * bce_loss(fake_logits, fake_labels)\n","        loss_info_with_answer = 0.5 * (true_loss + fake_loss)\n","\n","        # mutual information btw distractor and question\n","        d_emb = d_maxouted * d_mask.unsqueeze(2)\n","        d_mean_emb = torch.sum(d_emb, 1) / d_lengths.unsqueeze(1).float()\n","        fake_d_mean_emb = torch.cat([q_mean_emb[-1].unsqueeze(0),\n","                                     q_mean_emb[:-1]], dim=0)\n","\n","        q_emb = q_maxouted * q_mask.unsqueeze(2)\n","        q_mean_emb = torch.sum(q_emb, 1) / q_lengths.unsqueeze(1).float()\n","        fake_q_mean_emb = torch.cat([q_mean_emb[-1].unsqueeze(0),\n","                                     q_mean_emb[:-1]], dim=0)\n","\n","        bce_loss = nn.BCEWithLogitsLoss()\n","        true_logits = self.discriminator(q_mean_emb, d_mean_emb)\n","        true_labels = torch.ones_like(true_logits)\n","\n","        fake_d_logits = self.discriminator(q_mean_emb, fake_d_mean_emb)\n","        fake_q_logits = self.discriminator(fake_q_mean_emb, d_mean_emb)\n","        fake_logits = torch.cat([fake_d_logits, fake_q_logits], dim=0)\n","        fake_labels = torch.zeros_like(fake_logits)\n","\n","        true_loss = bce_loss(true_logits, true_labels)\n","        fake_loss = 0.5 * bce_loss(fake_logits, fake_labels)\n","        loss_info_with_question = 0.5 * (true_loss + fake_loss)\n","        \n","        return logits, -loss_info_with_answer,loss_info_with_question\n","\n","    def generate(self, init_state, c_ids, a_ids,q_ids):\n","        c_mask, _ = return_mask_lengths(c_ids)\n","        c_outputs = self.context_lstm(c_ids, a_ids) ## here\n","\n","        batch_size = c_ids.size(0)\n","\n","        d_ids = torch.LongTensor([self.sos_id] * batch_size).unsqueeze(1)\n","        d_ids = d_ids.to(c_ids.device)\n","        token_type_ids = torch.zeros_like(d_ids)\n","        position_ids = torch.zeros_like(d_ids)\n","        d_embeddings = self.embedding(d_ids, token_type_ids, position_ids)\n","\n","        state = init_state\n","\n","        # unroll\n","        all_d_ids = list()\n","        all_d_ids.append(d_ids)\n","        for _ in range(self.max_q_len - 1):\n","            position_ids = position_ids + 1\n","            d_outputs, state = self.distractor_lstm.lstm(d_embeddings, state)\n","\n","            # attention\n","            mask = c_mask.unsqueeze(1)\n","            c_attned_by_d, attn_logits = cal_attn(self.question_linear(d_outputs),\n","                                                  c_outputs,\n","                                                  mask)\n","\n","            # gen logits\n","            d_concated = torch.cat([d_outputs, c_attned_by_d], dim=2)\n","            d_concated = self.concat_linear(d_concated)\n","            d_maxouted, _ = d_concated.view(\n","                batch_size, 1, self.emsize, 2).max(dim=-1)\n","            gen_logits = self.logit_linear(d_maxouted)\n","\n","            # copy logits\n","            attn_logits = attn_logits.squeeze(1)\n","            copy_logits = torch.zeros(\n","                batch_size, self.ntokens).to(c_ids.device)\n","            copy_logits = copy_logits - 10000.0\n","            copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","            copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","\n","            logits = gen_logits + copy_logits.unsqueeze(1)\n","\n","            d_ids = torch.argmax(logits, 2)\n","            all_d_ids.append(d_ids)\n","\n","            d_embeddings = self.embedding(d_ids, token_type_ids, position_ids)\n","\n","        d_ids = torch.cat(all_d_ids, 1)\n","        d_ids = self.postprocess(d_ids)\n","\n","        return d_ids\n","\n","    def sample(self, init_state, c_ids, a_ids,q_ids):\n","        c_mask, c_lengths = return_mask_lengths(c_ids)\n","        c_outputs = self.context_lstm(c_ids, a_ids)\n","\n","        batch_size = c_ids.size(0)\n","\n","        d_ids = torch.LongTensor([self.sos_id] * batch_size).unsqueeze(1)\n","        d_ids = d_ids.to(c_ids.device)\n","        token_type_ids = torch.zeros_like(d_ids)\n","        position_ids = torch.zeros_like(d_ids)\n","        d_embeddings = self.embedding(d_ids, token_type_ids, position_ids)\n","\n","        state = init_state\n","\n","        # unroll\n","        all_d_ids = list()\n","        all_d_ids.append(d_ids)\n","        for _ in range(self.max_q_len - 1):\n","            position_ids = position_ids + 1\n","            d_outputs, state = self.distractor_lstm.lstm(d_embeddings, state)\n","\n","            # attention\n","            mask = c_mask.unsqueeze(1)\n","            c_attned_by_d, attn_logits = cal_attn(self.question_linear(d_outputs),\n","                                                  c_outputs,\n","                                                  mask)\n","\n","            # gen logits\n","            d_concated = torch.cat([d_outputs, c_attned_by_d], dim=2)\n","            d_concated = self.concat_linear(d_concated)\n","            d_maxouted, _ = d_concated.view(batch_size, 1, self.emsize, 2).max(dim=-1)\n","            gen_logits = self.logit_linear(d_maxouted)\n","\n","            # copy logits\n","            attn_logits = attn_logits.squeeze(1)\n","            copy_logits = torch.zeros(batch_size, self.ntokens).to(c_ids.device)\n","            copy_logits = copy_logits - 10000.0\n","            copy_logits, _ = scatter_max(attn_logits, c_ids, out=copy_logits)\n","            copy_logits = copy_logits.masked_fill(copy_logits == -10000.0, 0)\n","\n","            logits = gen_logits + copy_logits.unsqueeze(1)\n","            logits = logits.squeeze(1)\n","            logits = self.top_k_top_p_filtering(logits, 2, top_p=0.8)\n","            probs = F.softmax(logits, dim=-1)\n","            d_ids = torch.multinomial(probs, num_samples=1)  # [b,1]\n","            all_d_ids.append(d_ids)\n","\n","            d_embeddings = self.embedding(d_ids, token_type_ids, position_ids)\n","\n","        d_ids = torch.cat(all_d_ids, 1)\n","        d_ids = self.postprocess(d_ids)\n","\n","        return d_ids\n"],"metadata":{"id":"Jo4KG9HivTXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DiscreteVAE(nn.Module):\n","    def __init__(self, args):\n","        super(DiscreteVAE, self).__init__()\n","        tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n","        padding_idx = tokenizer.vocab['[PAD]']\n","        sos_id = tokenizer.vocab['[CLS]']\n","        eos_id = tokenizer.vocab['[SEP]']\n","        ntokens = len(tokenizer.vocab)\n","\n","        bert_model = args.bert_model\n","        if \"large\" in bert_model:\n","            emsize = 1024\n","        else:\n","            emsize = 768\n","\n","        enc_nhidden = args.enc_nhidden\n","        enc_nlayers = args.enc_nlayers\n","        enc_dropout = args.enc_dropout\n","        dec_a_nhidden = args.dec_a_nhidden\n","        dec_a_nlayers = args.dec_a_nlayers\n","        dec_a_dropout = args.dec_a_dropout\n","        self.dec_q_nhidden = dec_q_nhidden = args.dec_q_nhidden\n","        self.dec_q_nlayers = dec_q_nlayers = args.dec_q_nlayers\n","        self.dec_d_nhidden = dec_d_nhidden = args.dec_d_nhidden\n","        self.dec_d_nlayers = dec_d_nlayers = args.dec_d_nlayers\n","        dec_q_dropout = args.dec_q_dropout\n","        dec_d_dropout = args.dec_d_dropout\n","        self.nzqdim = nzqdim = args.nzqdim\n","        self.nzddim = nzddim = args.nzddim\n","        self.nza = nza = args.nza\n","        self.nzadim = nzadim = args.nzadim\n","\n","        self.lambda_kl = args.lambda_kl\n","        self.lambda_info = args.lambda_info\n","\n","        max_q_len = args.max_q_len\n","        max_d_len = args.max_d_len\n","\n","        embedding = Embedding(bert_model)\n","        contextualized_embedding = ContextualizedEmbedding(bert_model)\n","        # freeze embedding\n","        for param in embedding.parameters():\n","            param.requires_grad = False\n","        for param in contextualized_embedding.parameters():\n","            param.requires_grad = False\n","\n","        self.posterior_encoder = PosteriorEncoder(embedding, emsize,\n","                                                  enc_nhidden, enc_nlayers,\n","                                                  nzqdim, nza, nzadim,nzddim,\n","                                                  enc_dropout)\n","\n","        self.prior_encoder = PriorEncoder(embedding, emsize,\n","                                          enc_nhidden, enc_nlayers,\n","                                          nzqdim, nza, nzadim,nzddim, enc_dropout)\n","\n","        self.answer_decoder = AnswerDecoder(contextualized_embedding, emsize,\n","                                            dec_a_nhidden, dec_a_nlayers,\n","                                            dec_a_dropout)\n","\n","        self.question_decoder = QuestionDecoder(sos_id, eos_id,\n","                                                embedding, contextualized_embedding, emsize,\n","                                                dec_q_nhidden, ntokens, dec_q_nlayers,\n","                                                dec_q_dropout,\n","                                                max_q_len)\n","\n","        self.distractor_decoder = DistractorDecoder(sos_id, eos_id,\n","                                                embedding, contextualized_embedding, emsize,\n","                                                dec_d_nhidden, ntokens, dec_d_nlayers,\n","                                                dec_d_dropout,\n","                                                max_d_len)\n","\n","        self.q_h_linear = nn.Linear(nzqdim, dec_q_nlayers * dec_q_nhidden)\n","        self.q_c_linear = nn.Linear(nzqdim, dec_q_nlayers * dec_q_nhidden)\n","        self.d_h_linear = nn.Linear(nzddim, dec_d_nlayers * dec_d_nhidden)\n","        self.d_c_linear = nn.Linear(nzddim, dec_d_nlayers * dec_d_nhidden)\n","        self.a_linear = nn.Linear(nza * nzadim, emsize, False)\n","\n","        self.q_rec_criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n","        self.d_rec_criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n","        self.gaussian_kl_criterion = GaussianKLLoss()\n","        self.categorical_kl_criterion = CategoricalKLLoss()\n","\n","    def return_init_state(self, zq, za, zd):\n","\n","        q_init_h = self.q_h_linear(zq)\n","        q_init_c = self.q_c_linear(zq)\n","        d_init_h = self.d_h_linear(zd)\n","        d_init_c = self.d_c_linear(zd)\n","\n","        q_init_h = q_init_h.view(-1, self.dec_q_nlayers,\n","                                 self.dec_q_nhidden).transpose(0, 1).contiguous()\n","        q_init_c = q_init_c.view(-1, self.dec_q_nlayers,\n","                                 self.dec_q_nhidden).transpose(0, 1).contiguous()\n","        q_init_state = (q_init_h, q_init_c)\n","\n","        ##\n","        d_init_h = d_init_h.view(-1, self.dec_d_nlayers,\n","                                 self.dec_d_nhidden).transpose(0, 1).contiguous()\n","        d_init_c = d_init_c.view(-1, self.dec_d_nlayers,\n","                                 self.dec_d_nhidden).transpose(0, 1).contiguous()\n","        d_init_state = (d_init_h, d_init_c)\n","        ##\n","\n","        za_flatten = za.view(-1, self.nza * self.nzadim)\n","        a_init_state = self.a_linear(za_flatten)\n","\n","        return q_init_state, a_init_state,d_init_state\n","\n","\n","    def forward(self, c_ids, q_ids, a_ids, start_positions, end_positions,d_ids):\n","\n","        posterior_zq_mu, posterior_zq_logvar, posterior_zq, \\\n","            posterior_za_prob, posterior_za, \\\n","            posterior_zd_mu, posterior_zd_logvar, posterior_zd \\\n","            = self.posterior_encoder(c_ids, q_ids, a_ids,d_ids)\n","\n","        prior_zq_mu, prior_zq_logvar, _, \\\n","            prior_za_prob, _, \\\n","            prior_zd_mu, prior_zd_logvar, _ \\\n","            = self.prior_encoder(c_ids)\n","\n","        q_init_state, a_init_state, d_init_state = self.return_init_state(\n","            posterior_zq, posterior_za,posterior_zd)\n","\n","        # answer decoding\n","        start_logits, end_logits = self.answer_decoder(a_init_state, c_ids)\n","        # question decoding\n","        q_logits, loss_info = self.question_decoder(\n","            q_init_state, c_ids, q_ids, a_ids)        \n","        # distractor decoding\n","        d_logits, d_loss_info = self.distractor_decoder(\n","            d_init_state, c_ids, q_ids, a_ids,d_ids)\n","\n","        # q rec loss\n","        loss_q_rec = self.q_rec_criterion(q_logits[:, :-1, :].transpose(1, 2).contiguous(),\n","                                          q_ids[:, 1:])\n","\n","        # a rec loss\n","        max_c_len = c_ids.size(1)\n","        a_rec_criterion = nn.CrossEntropyLoss(ignore_index=max_c_len)\n","        start_positions.clamp_(0, max_c_len)\n","        end_positions.clamp_(0, max_c_len)\n","        loss_start_a_rec = a_rec_criterion(start_logits, start_positions)\n","        loss_end_a_rec = a_rec_criterion(end_logits, end_positions)\n","        loss_a_rec = 0.5 * (loss_start_a_rec + loss_end_a_rec)\n","\n","        # d rec loss\n","        loss_d_rec = self.d_rec_criterion(d_logits[:, :-1, :].transpose(1, 2).contiguous(),\n","                                          d_ids[:, 1:])\n","\n","        # kl loss\n","        loss_zq_kl = self.gaussian_kl_criterion(posterior_zq_mu,\n","                                                posterior_zq_logvar,\n","                                                prior_zq_mu,\n","                                                prior_zq_logvar)\n","        # kl loss for d\n","        loss_zd_kl = self.gaussian_kl_criterion(posterior_zd_mu,\n","                                                posterior_zd_logvar,\n","                                                prior_zd_mu,\n","                                                prior_zd_logvar)\n","\n","        loss_za_kl = self.categorical_kl_criterion(posterior_za_prob,\n","                                                   prior_za_prob)\n","\n","        loss_kl = self.lambda_kl * (loss_zq_kl + loss_za_kl+loss_zd_kl)\n","        loss_info = self.lambda_info * (loss_info+d_loss_info)\n","\n","        loss = loss_q_rec + loss_a_rec + loss_kl + loss_info + loss_d_rec\n","\n","        return loss, \\\n","            loss_q_rec, loss_a_rec, \\\n","            loss_zq_kl, loss_za_kl, \\\n","            loss_info, loss_d_rec\n","\n","    def generate(self, zq, za, c_ids):\n","        q_init_state, a_init_state, d_init_state = self.return_init_state(zq, za)\n","\n","        a_ids, start_positions, end_positions = self.answer_decoder.generate(   ## answer generation\n","            a_init_state, c_ids)\n","\n","        q_ids = self.question_decoder.generate(q_init_state, c_ids, a_ids) \n","        d_ids = self.distractor_decoder.generate(d_init_state, c_ids, a_ids, q_ids)       ## question generation\n","\n","        return q_ids, start_positions, end_positions, d_ids\n","\n","    def return_answer_logits(self, zq, za, c_ids):\n","        _, a_init_state = self.return_init_state(zq, za)\n","\n","        start_logits, end_logits = self.answer_decoder(a_init_state, c_ids)\n","\n","        return start_logits, end_logits\n"],"metadata":{"id":"3Q_7yZuGgT_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"n9-VPOLngXDV"},"execution_count":null,"outputs":[]}]}