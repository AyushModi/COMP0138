Immunology is a branch of biomedical science that covers the study of immune systems in all organisms. It charts, measures, and contextualizes the: physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (such as autoimmune diseases, hypersensitivities, immune deficiency, and transplant rejection); the physical, chemical and physiological characteristics of the components of the immune system in vitro, in situ, and in vivo. Immunology has applications in numerous disciplines of medicine, particularly in the fields of organ transplantation, oncology, virology, bacteriology, parasitology, psychiatry, and dermatology.
Prior to the designation of immunity from the etymological root immunis, which is Latin for "exempt"; early physicians characterized organs that would later be proven as essential components of the immune system. The important lymphoid organs of the immune system are the thymus and bone marrow, and chief lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and liver. When health conditions worsen to emergency status, portions of immune system organs including the thymus, spleen, bone marrow, lymph nodes and other lymphatic tissues can be surgically excised for examination while patients are still alive.
Many components of the immune system are typically cellular in nature and not associated with any specific organ; but rather are embedded or circulating in various tissues located throughout the body.
Classical immunology ties in with the fields of epidemiology and medicine. It studies the relationship between the body systems, pathogens, and immunity. The earliest written mention of immunity can be traced back to the plague of Athens in 430 BCE. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. Many other ancient societies have references to this phenomenon, but it was not until the 19th and 20th centuries before the concept developed into scientific theory.
The study of the molecular and cellular components that comprise the immune system, including their function and interaction, is the central science of immunology. The immune system has been divided into a more primitive innate immune system and, in vertebrates, an acquired or adaptive immune system. The latter is further divided into humoral (or antibody) and cell-mediated components.
The humoral (antibody) response is defined as the interaction between antibodies and antigens. Antibodies are specific proteins released from a certain class of immune cells known as B lymphocytes, while antigens are defined as anything that elicits the generation of antibodies ("anti"body "gen"erators). Immunology rests on an understanding of the properties of these two biological entities and the cellular response to both.
Immunological research continues to become more specialized, pursuing non-classical models of immunity and functions of cells, organs and systems not previously associated with the immune system (Yemeserach 2010).
Clinical immunology is the study of diseases caused by disorders of the immune system (failure, aberrant action, and malignant growth of the cellular elements of the system). It also involves diseases of other systems, where immune reactions play a part in the pathology and clinical features.
Other immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.
The most well-known disease that affects the immune system itself is AIDS, an immunodeficiency characterized by the suppression of CD4+ ("helper") T cells, dendritic cells and macrophages by the Human Immunodeficiency Virus (HIV).
The body’s capability to react to antigen depends on a person's age, antigen type, maternal factors and the area where the antigen is presented. Neonates are said to be in a state of physiological immunodeficiency, because both their innate and adaptive immunological responses are greatly suppressed. Once born, a child’s immune system responds favorably to protein antigens while not as well to glycoproteins and polysaccharides. In fact, many of the infections acquired by neonates are caused by low virulence organisms like Staphylococcus and Pseudomonas. In neonates, opsonic activity and the ability to activate the complement cascade is very limited. For example, the mean level of C3 in a newborn is approximately 65% of that found in the adult. Phagocytic activity is also greatly impaired in newborns. This is due to lower opsonic activity, as well as diminished up-regulation of integrin and selectin receptors, which limit the ability of neutrophils to interact with adhesion molecules in the endothelium. Their monocytes are slow and have a reduced ATP production, which also limits the newborn's phagocytic activity. Although, the number of total lymphocytes is significantly higher than in adults, the cellular and humoral immunity is also impaired. Antigen-presenting cells in newborns have a reduced capability to activate T cells. Also, T cells of a newborn proliferate poorly and produce very small amounts of cytokines like IL-2, IL-4, IL-5, IL-12, and IFN-g which limits their capacity to activate the humoral response as well as the phagocitic activity of macrophage. B cells develop early during gestation but are not fully active.
Maternal factors also play a role in the body’s immune response. At birth, most of the immunoglobulin present is maternal IgG. Because IgM, IgD, IgE and IgA don’t cross the placenta, they are almost undetectable at birth. Some IgA is provided by breast milk. These passively-acquired antibodies can protect the newborn for up to 18 months, but their response is usually short-lived and of low affinity. These antibodies can also produce a negative response. If a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response. Passively acquired maternal antibodies can suppress the antibody response to active immunization. Similarly the response of T-cells to vaccination differs in children compared to adults, and vaccines that induce Th1 responses in adults do not readily elicit these same responses in neonates. Between six to nine months after birth, a child’s immune system begins to respond more strongly to glycoproteins, but there is usually no marked improvement in their response to polysaccharides until they are at least one year old. This can be the reason for distinct time frames found in vaccination schedules.
During adolescence, the human body undergoes various physical, physiological and immunological changes triggered and mediated by hormones, of which the most significant in females is 17-β-oestradiol (an oestrogen) and, in males, is testosterone. Oestradiol usually begins to act around the age of 10 and testosterone some months later. There is evidence that these steroids act directly not only on the primary and secondary sexual characteristics but also have an effect on the development and regulation of the immune system, including an increased risk in developing pubescent and post-pubescent autoimmunity. There is also some evidence that cell surface receptors on B cells and macrophages may detect sex hormones in the system.
Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between "cellular" and "humoral" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held by Robert Koch and Emil von Behring, among others, stated that the active immune agents were soluble components (molecules) found in the organism’s “humors” rather than its cells.
In the mid-1950s, Frank Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: "self" constituents (constituents of the body) do not trigger destructive immune responses, while "nonself" entities (e.g., pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex "two-signal" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.
Bioscience is the overall major in which undergraduate students who are interested in general well-being take in college. Immunology is a branch of bioscience for undergraduate programs but the major gets specified as students move on for graduate program in immunology. The aim of immunology is to study the health of humans and animals through effective yet consistent research, (AAAAI, 2013). The most important thing about being immunologists is the research because it is the biggest portion of their jobs.
Most graduate immunology schools follow the AAI courses immunology which are offered throughout numerous schools in the United States. For example, in New York State, there are several universities that offer the AAI courses immunology: Albany Medical College, Cornell University, Icahn School of Medicine at Mount Sinai, New York University Langone Medical Center, University at Albany (SUNY), University at Buffalo (SUNY), University of Rochester Medical Center and Upstate Medical University (SUNY). The AAI immunology courses include an Introductory Course and an Advance Course. The Introductory Course is a course that gives students an overview of the basics of immunology.
In addition, this Introductory Course gives students more information to complement general biology or science training. It also has two different parts: Part I is an introduction to the basic principles of immunology and Part II is a clinically-oriented lecture series. On the other hand, the Advanced Course is another course for those who are willing to expand or update their understanding of immunology. It is advised for students who want to attend the Advanced Course to have a background of the principles of immunology. Most schools require students to take electives in other to complete their degrees. A Master’s degree requires two years of study following the attainment of a bachelor's degree. For a doctoral programme it is required to take two additional years of study.
Madrasa (Arabic: مدرسة‎, madrasah, pl. مدارس, madāris, Turkish: Medrese) is the Arabic word for any type of educational institution, whether secular or religious (of any religion). The word is variously transliterated madrasah, madarasaa, medresa, madrassa, madraza, medrese, etc. In the West, the word usually refers to a specific type of religious school or college for the study of the Islamic religion, though this may not be the only subject studied. Not all students in madaris are Muslims; there is also a modern curriculum.
The word madrasah derives from the triconsonantal Semitic root د-ر-س D-R-S 'to learn, study', through the wazn (form/stem) مفعل(ة)‎; mafʻal(ah), meaning "a place where something is done". Therefore, madrasah literally means "a place where learning and studying take place". The word is also present as a loanword with the same innocuous meaning in many Arabic-influenced languages, such as: Urdu, Bengali, Hindi, Persian, Turkish, Azeri, Kurdish, Indonesian, Malay and Bosnian / Croatian. In the Arabic language, the word مدرسة madrasah simply means the same as school does in the English language, whether that is private, public or parochial school, as well as for any primary or secondary school whether Muslim, non-Muslim, or secular. Unlike the use of the word school in British English, the word madrasah more closely resembles the term school in American English, in that it can refer to a university-level or post-graduate school as well as to a primary or secondary school. For example, in the Ottoman Empire during the Early Modern Period, madaris had lower schools and specialised schools where the students became known as danişmends. The usual Arabic word for a university, however, is جامعة (jāmiʻah). The Hebrew cognate midrasha also connotes the meaning of a place of learning; the related term midrash literally refers to study or learning, but has acquired mystical and religious connotations.
However, in English, the term madrasah usually refers to the specifically Islamic institutions. A typical Islamic school usually offers two courses of study: a ḥifẓ course teaching memorization of the Qur'an (the person who commits the entire Qurʼan to memory is called a ḥāfiẓ); and an ʻālim course leading the candidate to become an accepted scholar in the community. A regular curriculum includes courses in Arabic, tafsir (Qur'anic interpretation), sharīʻah (Islamic law), hadiths (recorded sayings and deeds of Muhammad), mantiq (logic), and Muslim history. In the Ottoman Empire, during the Early Modern Period, the study of hadiths was introduced by Süleyman I. Depending on the educational demands, some madaris also offer additional advanced courses in Arabic literature, English and other foreign languages, as well as science and world history. Ottoman madaris along with religious teachings also taught "styles of writing, grammary, syntax, poetry, composition, natural sciences, political sciences, and etiquette."
People of all ages attend, and many often move on to becoming imams.[citation needed] The certificate of an ʻālim, for example, requires approximately twelve years of study.[citation needed] A good number of the ḥuffāẓ (plural of ḥāfiẓ) are the product of the madaris. The madaris also resemble colleges, where people take evening classes and reside in dormitories. An important function of the madaris is to admit orphans and poor children in order to provide them with education and training. Madaris may enroll female students; however, they study separately from the men.[citation needed]
The term "Islamic education" means education in the light of Islam itself, which is rooted in the teachings of the Quran - holy book of Muslims. Islamic education and Muslim education are not the same. Because Islamic education has epistemological integration which is founded on Tawhid - Oneness or monotheism. For details Read "A Qur’anic Methodology for Integrating Knowledge and Education: Implications for Malaysia’s Islamic Education Strategy" written Tareq M Zayed  and "Knowledge of Shariah and Knowledge to Manage ‘Self’ and ‘System’: Integration of Islamic Epistemology with the Knowledge and Education" authored by Tareq M Zayed
The first institute of madrasa education was at the estate of Hazrat Zaid bin Arkam near a hill called Safa, where Hazrat Muhammad was the teacher and the students were some of his followers.[citation needed] After Hijrah (migration) the madrasa of "Suffa" was established in Madina on the east side of the Al-Masjid an-Nabawi mosque. Hazrat 'Ubada bin Samit was appointed there by Hazrat Muhammad as teacher and among the students.[citation needed] In the curriculum of the madrasa, there were teachings of The Qur'an,The Hadith, fara'iz, tajweed, genealogy, treatises of first aid, etc. There were also trainings of horse-riding, art of war, handwriting and calligraphy, athletics and martial arts. The first part of madrasa based education is estimated from the first day of "nabuwwat" to the first portion of the "Umaiya" caliphate.[citation needed]
During the rule of the Fatimid and Mamluk dynasties and their successor states in the medieval Middle East, many of the ruling elite founded madaris through a religious endowment known as the waqf. Not only was the madrasa a potent symbol of status but it was an effective means of transmitting wealth and status to their descendants. Especially during the Mamlūk period, when only former slaves could assume power, the sons of the ruling Mamlūk elite were unable to inherit. Guaranteed positions within the new madaris thus allowed them to maintain status. Madaris built in this period include the Mosque-Madrasah of Sultan Ḥasan in Cairo.
At the beginning of the Caliphate or Islamic Empire, the reliance on courts initially confined sponsorship and scholarly activities to major centres. Within several centuries, the development of Muslim educational institutions such as the madrasah and masjid eventually introduced such activities to provincial towns and dispersed them across the Islamic legal schools and Sufi orders. In addition to religious subjects, they also taught the "rational sciences," as varied as mathematics, astronomy, astrology, geography, alchemy, philosophy, magic, and occultism, depending on the curriculum of the specific institution in question. The madaris, however, were not centres of advanced scientific study; scientific advances in Islam were usually carried out by scholars working under the patronage of royal courts. During this time,[when?] the Caliphate experienced a growth in literacy, having the highest literacy rate of the Middle Ages, comparable to classical Athens' literacy in antiquity but on a much larger scale. The emergence of the maktab and madrasa institutions played a fundamental role in the relatively high literacy rates of the medieval Islamic world.
In the medieval Islamic world, an elementary school was known as a maktab, which dates back to at least the 10th century. Like madaris (which referred to higher education), a maktab was often attached to an endowed mosque. In the 11th century, the famous Persian Islamic philosopher and teacher Ibn Sīnā (known as Avicenna in the West), in one of his books, wrote a chapter about the maktab entitled "The Role of the Teacher in the Training and Upbringing of Children," as a guide to teachers working at maktab schools. He wrote that children can learn better if taught in classes instead of individual tuition from private tutors, and he gave a number of reasons for why this is the case, citing the value of competition and emulation among pupils, as well as the usefulness of group discussions and debates. Ibn Sīnā described the curriculum of a maktab school in some detail, describing the curricula for two stages of education in a maktab school.
Ibn Sīnā refers to the secondary education stage of maktab schooling as a period of specialisation when pupils should begin to acquire manual skills, regardless of their social status. He writes that children after the age of 14 should be allowed to choose and specialise in subjects they have an interest in, whether it was reading, manual skills, literature, preaching, medicine, geometry, trade and commerce, craftsmanship, or any other subject or profession they would be interested in pursuing for a future career. He wrote that this was a transitional stage and that there needs to be flexibility regarding the age in which pupils graduate, as the student's emotional development and chosen subjects need to be taken into account.
During its formative period, the term madrasah referred to a higher education institution, whose curriculum initially included only the "religious sciences", whilst philosophy and the secular sciences were often excluded. The curriculum slowly began to diversify, with many later madaris teaching both the religious and the "secular sciences", such as logic, mathematics and philosophy. Some madaris further extended their curriculum to history, politics, ethics, music, metaphysics, medicine, astronomy and chemistry. The curriculum of a madrasah was usually set by its founder, but most generally taught both the religious sciences and the physical sciences. Madaris were established throughout the Islamic world, examples being the 9th century University of al-Qarawiyyin, the 10th century al-Azhar University (the most famous), the 11th century Niẓāmīyah, as well as 75 madaris in Cairo, 51 in Damascus and up to 44 in Aleppo between 1155 and 1260. Many more were also established in the Andalusian cities of Córdoba, Seville, Toledo, Granada (Madrasah of Granada), Murcia, Almería, Valencia and Cádiz during the Caliphate of Córdoba.
Madaris were largely centred on the study of fiqh (Islamic jurisprudence). The ijāzat al-tadrīs wa-al-iftāʼ ("licence to teach and issue legal opinions") in the medieval Islamic legal education system had its origins in the 9th century after the formation of the madhāhib (schools of jurisprudence). George Makdisi considers the ijāzah to be the origin of the European doctorate. However, in an earlier article, he considered the ijāzah to be of "fundamental difference" to the medieval doctorate, since the former was awarded by an individual teacher-scholar not obliged to follow any formal criteria, whereas the latter was conferred on the student by the collective authority of the faculty. To obtain an ijāzah, a student "had to study in a guild school of law, usually four years for the basic undergraduate course" and ten or more years for a post-graduate course. The "doctorate was obtained after an oral examination to determine the originality of the candidate's theses", and to test the student's "ability to defend them against all objections, in disputations set up for the purpose." These were scholarly exercises practised throughout the student's "career as a graduate student of law." After students completed their post-graduate education, they were awarded ijazas giving them the status of faqīh 'scholar of jurisprudence', muftī 'scholar competent in issuing fatwās', and mudarris 'teacher'.
The Arabic term ijāzat al-tadrīs was awarded to Islamic scholars who were qualified to teach. According to Makdisi, the Latin title licentia docendi 'licence to teach' in the European university may have been a translation of the Arabic, but the underlying concept was very different. A significant difference between the ijāzat al-tadrīs and the licentia docendi was that the former was awarded by the individual scholar-teacher, while the latter was awarded by the chief official of the university, who represented the collective faculty, rather than the individual scholar-teacher.
Much of the study in the madrasah college centred on examining whether certain opinions of law were orthodox. This scholarly process of "determining orthodoxy began with a question which the Muslim layman, called in that capacity mustaftī, presented to a jurisconsult, called mufti, soliciting from him a response, called fatwa, a legal opinion (the religious law of Islam covers civil as well as religious matters). The mufti (professor of legal opinions) took this question, studied it, researched it intensively in the sacred scriptures, in order to find a solution to it. This process of scholarly research was called ijtihād, literally, the exertion of one's efforts to the utmost limit."
There is disagreement whether madaris ever became universities. Scholars like Arnold H. Green and Seyyed Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madaris indeed became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Darleen Pryds questions this view, pointing out that madaris and European universities in the Mediterranean region shared similar foundations by princely patrons and were intended to provide loyal administrators to further the rulers' agenda. Other scholars regard the university as uniquely European in origin and characteristics.
al-Qarawīyīn University in Fez, Morocco is recognised by many historians as the oldest degree-granting university in the world, having been founded in 859 by Fatima al-Fihri. While the madrasa college could also issue degrees at all levels, the jāmiʻahs (such as al-Qarawīyīn and al-Azhar University) differed in the sense that they were larger institutions, more universal in terms of their complete source of studies, had individual faculties for different subjects, and could house a number of mosques, madaris, and other institutions within them. Such an institution has thus been described as an "Islamic university".
Al-Azhar University, founded in Cairo, Egypt in 975 by the Ismaʻīlī Shīʻī Fatimid dynasty as a jāmiʻah, had individual faculties for a theological seminary, Islamic law and jurisprudence, Arabic grammar, Islamic astronomy, early Islamic philosophy and logic in Islamic philosophy. The postgraduate doctorate in law was only obtained after "an oral examination to determine the originality of the candidate's theses", and to test the student's "ability to defend them against all objections, in disputations set up for the purpose." ‘Abd al-Laṭīf al-Baghdādī also delivered lectures on Islamic medicine at al-Azhar, while Maimonides delivered lectures on medicine and astronomy there during the time of Saladin. Another early jāmiʻah was the Niẓāmīyah of Baghdād (founded 1091), which has been called the "largest university of the Medieval world." Mustansiriya University, established by the ʻAbbāsid caliph al-Mustanṣir in 1233, in addition to teaching the religious subjects, offered courses dealing with philosophy, mathematics and the natural sciences.
However, the classification of madaris as "universities" is disputed on the question of understanding of each institution on its own terms. In madaris, the ijāzahs were only issued in one field, the Islamic religious law of sharīʻah, and in no other field of learning. Other academic subjects, including the natural sciences, philosophy and literary studies, were only treated "ancillary" to the study of the Sharia. For example, a natural science like astronomy was only studied (if at all) to supply religious needs, like the time for prayer. This is why Ptolemaic astronomy was considered adequate, and is still taught in some modern day madaris. The Islamic law undergraduate degree from al-Azhar, the most prestigious madrasa, was traditionally granted without final examinations, but on the basis of the students' attentive attendance to courses. In contrast to the medieval doctorate which was granted by the collective authority of the faculty, the Islamic degree was not granted by the teacher to the pupil based on any formal criteria, but remained a "personal matter, the sole prerogative of the person bestowing it; no one could force him to give one".
Medievalist specialists who define the university as a legally autonomous corporation disagree with the term "university" for the Islamic madaris and jāmi‘ahs because the medieval university (from Latin universitas) was structurally different, being a legally autonomous corporation rather than a waqf institution like the madrasa and jāmiʻah. Despite the many similarities, medieval specialists have coined the term "Islamic college" for madrasa and jāmiʻah to differentiate them from the legally autonomous corporations that the medieval European universities were. In a sense, the madrasa resembles a university college in that it has most of the features of a university, but lacks the corporate element. Toby Huff summarises the difference as follows:
As Muslim institutions of higher learning, the madrasa had the legal designation of waqf. In central and eastern Islamic lands, the view that the madrasa, as a charitable endowment, will remain under the control of the donor (and their descendent), resulted in a "spurt" of establishment of madaris in the 11th and 12th centuries. However, in Western Islamic lands, where the Maliki views prohibited donors from controlling their endowment, madaris were not as popular. Unlike the corporate designation of Western institutions of higher learning, the waqf designation seemed to have led to the exclusion of non-orthodox religious subjects such a philosophy and natural science from the curricula. The madrasa of al-Qarawīyīn, one of the two surviving madaris that predate the founding of the earliest medieval universities and are thus claimed to be the "first universities" by some authors, has acquired official university status as late as 1947. The other, al-Azhar, did acquire this status in name and essence only in the course of numerous reforms during the 19th and 20th century, notably the one of 1961 which introduced non-religious subjects to its curriculum, such as economics, engineering, medicine, and agriculture. It should also be noted that many medieval universities were run for centuries as Christian cathedral schools or monastic schools prior to their formal establishment as universitas scholarium; evidence of these immediate forerunners of the university dates back to the 6th century AD, thus well preceding the earliest madaris. George Makdisi, who has published most extensively on the topic concludes in his comparison between the two institutions:
Nevertheless, Makdisi has asserted that the European university borrowed many of its features from the Islamic madrasa, including the concepts of a degree and doctorate. Makdisi and Hugh Goddard have also highlighted other terms and concepts now used in modern universities which most likely have Islamic origins, including "the fact that we still talk of professors holding the 'Chair' of their subject" being based on the "traditional Islamic pattern of teaching where the professor sits on a chair and the students sit around him", the term 'academic circles' being derived from the way in which Islamic students "sat in a circle around their professor", and terms such as "having 'fellows', 'reading' a subject, and obtaining 'degrees', can all be traced back" to the Islamic concepts of aṣḥāb ('companions, as of Muhammad'), qirāʼah ('reading aloud the Qur'an') and ijāzah ('licence [to teach]') respectively. Makdisi has listed eighteen such parallels in terminology which can be traced back to their roots in Islamic education. Some of the practices now common in modern universities which Makdisi and Goddard trace back to an Islamic root include "practices such as delivering inaugural lectures, wearing academic robes, obtaining doctorates by defending a thesis, and even the idea of academic freedom are also modelled on Islamic custom." The Islamic scholarly system of fatwá and ijmāʻ, meaning opinion and consensus respectively, formed the basis of the "scholarly system the West has practised in university scholarship from the Middle Ages down to the present day." According to Makdisi and Goddard, "the idea of academic freedom" in universities was also "modelled on Islamic custom" as practised in the medieval Madrasa system from the 9th century. Islamic influence was "certainly discernible in the foundation of the first deliberately planned university" in Europe, the University of Naples Federico II founded by Frederick II, Holy Roman Emperor in 1224.
However, all of these facets of medieval university life are considered by standard scholarship to be independent medieval European developments with no tracable Islamic influence. Generally, some reviewers have pointed out the strong inclination of Makdisi of overstating his case by simply resting on "the accumulation of close parallels", but all the while failing to point to convincing channels of transmission between the Muslim and Christian world. Norman Daniel points out that the Arab equivalent of the Latin disputation, the taliqa, was reserved for the ruler's court, not the madrasa, and that the actual differences between Islamic fiqh and medieval European civil law were profound. The taliqa only reached Islamic Spain, the only likely point of transmission, after the establishment of the first medieval universities. In fact, there is no Latin translation of the taliqa and, most importantly, no evidence of Latin scholars ever showing awareness of Arab influence on the Latin method of disputation, something they would have certainly found noteworthy. Rather, it was the medieval reception of the Greek Organon which set the scholastic sic et non in motion. Daniel concludes that resemblances in method had more to with the two religions having "common problems: to reconcile the conflicting statements of their own authorities, and to safeguard the data of revelation from the impact of Greek philosophy"; thus Christian scholasticism and similar Arab concepts should be viewed in terms of a parallel occurrence, not of the transmission of ideas from one to the other, a view shared by Hugh Kennedy.
Prior to the 12th century, women accounted for less than one percent of the world’s Islamic scholars. However, al-Sakhawi and Mohammad Akram Nadwi have since found evidence of over 8,000 female scholars since the 15th century. al-Sakhawi devotes an entire volume of his 12-volume biographical dictionary al-Ḍawʾ al-lāmiʻ to female scholars, giving information on 1,075 of them. More recently, the scholar Mohammad Akram Nadwi, currently a researcher from the Oxford Centre for Islamic Studies, has written 40 volumes on the muḥaddithāt (the women scholars of ḥadīth), and found at least 8,000 of them.
From around 750, during the Abbasid Caliphate, women “became renowned for their brains as well as their beauty”. In particular, many well known women of the time were trained from childhood in music, dancing and poetry. Mahbuba was one of these. Another feminine figure to be remembered for her achievements was Tawaddud, "a slave girl who was said to have been bought at great cost by Hārūn al-Rashīd because she had passed her examinations by the most eminent scholars in astronomy, medicine, law, philosophy, music, history, Arabic grammar, literature, theology and chess". Moreover, among the most prominent feminine figures was Shuhda who was known as "the Scholar" or "the Pride of Women" during the 12th century in Baghdad. Despite the recognition of women's aptitudes during the Abbasid dynasty, all these came to an end in Iraq with the sack of Baghdad in 1258.
According to the Sunni scholar Ibn ʻAsākir in the 12th century, there were opportunities for female education in the medieval Islamic world, writing that women could study, earn ijazahs (academic degrees), and qualify as scholars and teachers. This was especially the case for learned and scholarly families, who wanted to ensure the highest possible education for both their sons and daughters. Ibn ʻAsakir had himself studied under 80 different female teachers in his time. Female education in the Islamic world was inspired by Muhammad's wives, such as Khadijah, a successful businesswoman. According to a hadith attributed to Muhammad, he praised the women of Medina because of their desire for religious knowledge:
"The first Ottoman Medrese was created in İznik in 1331 and most Ottoman medreses followed the traditions of Sunni Islam." "When an Ottoman sultan established a new medrese, he would invite scholars from the Islamic world—for example, Murad II brought scholars from Persia, such as ʻAlāʼ al-Dīn and Fakhr al-Dīn who helped enhance the reputation of the Ottoman medrese". This reveals that the Islamic world was interconnected in the early modern period as they travelled around to other Islamic states exchanging knowledge. This sense that the Ottoman Empire was becoming modernised through globalization is also recognised by Hamadeh who says: "Change in the eighteenth century as the beginning of a long and unilinear march toward westernisation reflects the two centuries of reformation in sovereign identity." İnalcık also mentions that while scholars from for example Persia travelled to the Ottomans in order to share their knowledge, Ottomans travelled as well to receive education from scholars of these Islamic lands, such as Egypt, Persia and Turkestan. Hence, this reveals that similar to today's modern world, individuals from the early modern society travelled abroad to receive education and share knowledge and that the world was more interconnected than it seems. Also, it reveals how the system of "schooling" was also similar to today's modern world where students travel abroad to different countries for studies. Examples of Ottoman madaris are the ones built by Mehmed the Conqueror. He built eight madaris that were built "on either side of the mosque where there were eight higher madaris for specialised studies and eight lower medreses, which prepared students for these." The fact that they were built around, or near mosques reveals the religious impulses behind madrasa building and it reveals the interconnectedness between institutions of learning and religion. The students who completed their education in the lower medreses became known as danismends. This reveals that similar to the education system today, the Ottomans' educational system involved different kinds of schools attached to different kinds of levels. For example, there were lower madaris and specialised ones, and for one to get into the specialised area meant that he had to complete the classes in the lower one in order to adequately prepare himself for higher learning.
Although Ottoman madaris had a number of different branches of study, such as calligraphic sciences, oral sciences, and intellectual sciences, they primarily served the function of an Islamic centre for spiritual learning. "The goal of all knowledge and in particular, of the spiritual sciences is knowledge of God." Religion, for the most part, determines the significance and importance of each science. As İnalcık mentions: "Those which aid religion are good and sciences like astrology are bad." However, even though mathematics, or studies in logic were part of the madrasa's curriculum, they were all centred around religion. Even mathematics had a religious impulse behind its teachings. "The Ulema of the Ottoman medreses held the view that hostility to logic and mathematics was futile since these accustomed the mind to correct thinking and thus helped to reveal divine truths" – key word being "divine". İnalcık also mentions that even philosophy was only allowed to be studied so that it helped to confirm the doctrines of Islam." Hence, madaris – schools were basically religious centres for religious teachings and learning in the Ottoman world. Although scholars such as Goffman have argued that the Ottomans were highly tolerant and lived in a pluralistic society, it seems that schools that were the main centres for learning were in fact heftily religious and were not religiously pluralistic, but centred around Islam. Similarly, in Europe "Jewish children learned the Hebrew letters and texts of basic prayers at home, and then attended a school organised by the synagogue to study the Torah." Wiesner-Hanks also says that Protestants also wanted to teach "proper religious values." This shows that in the early modern period, Ottomans and Europeans were similar in their ideas about how schools should be managed and what they should be primarily focused on. Thus, Ottoman madaris were very similar to present day schools in the sense that they offered a wide range of studies; however, these studies, in their ultimate objective, aimed to further solidify and consolidate Islamic practices and theories.
As with any other country during the Early Modern Period, such as Italy and Spain in Europe, the Ottoman social life was interconnected with the medrese. Medreses were built in as part of a Mosque complex where many programmes, such as aid to the poor through soup kitchens, were held under the infrastructure of a mosque, which reveals the interconnectedness of religion and social life during this period. "The mosques to which medreses were attached, dominated the social life in Ottoman cities." Social life was not dominated by religion only in the Muslim world of the Ottoman Empire; it was also quite similar to the social life of Europe during this period. As Goffman says: "Just as mosques dominated social life for the Ottomans, churches and synagogues dominated life for the Christians and Jews as well." Hence, social life and the medrese were closely linked, since medreses taught many curricula, such as religion, which highly governed social life in terms of establishing orthodoxy. "They tried moving their developing state toward Islamic orthodoxy." Overall, the fact that mosques contained medreses comes to show the relevance of education to religion in the sense that education took place within the framework of religion and religion established social life by trying to create a common religious orthodoxy. Hence, medreses were simply part of the social life of society as students came to learn the fundamentals of their societal values and beliefs.
In India the majority of these schools follow the Hanafi school of thought. The religious establishment forms part of the mainly two large divisions within the country, namely the Deobandis, who dominate in numbers (of whom the Darul Uloom Deoband constitutes one of the biggest madaris) and the Barelvis, who also make up a sizeable portion (Sufi-oriented). Some notable establishments include: Al Jamiatul Ashrafia, Mubarakpur, Manzar Islam Bareilly, Jamia Nizamdina New Delhi, Jamia Nayeemia Muradabad which is one of the largest learning centres for the Barelvis. The HR[clarification needed] ministry of the government of India has recently[when?] declared that a Central Madrasa Board would be set up. This will enhance the education system of madaris in India. Though the madaris impart Quranic education mainly, efforts are on to include Mathematics, Computers and science in the curriculum. In July 2015, the state government of Maharashtra created a stir de-recognised madrasa education, receiving critisicm from several political parties with the NCP accusing the ruling BJP of creating Hindu-Muslim friction in the state, and Kamal Farooqui of the All India Muslim Personal Law Board saying it was "ill-designed" 
Today, the system of Arabic and Islamic education has grown and further integrated with Kerala government administration. In 2005, an estimated 6,000 Muslim Arabic teachers taught in Kerala government schools, with over 500,000 Muslim students. State-appointed committees, not private mosques or religious scholars outside the government, determine the curriculum and accreditation of new schools and colleges. Primary education in Arabic and Islamic studies is available to Kerala Muslims almost entirely in after-school madrasa programs - sharply unlike full-time madaris common in north India, which may replace formal schooling. Arabic colleges (over eleven of which exist within the state-run University of Calicut and the Kannur University) provide B.A. and Masters' level degrees. At all levels, instruction is co-educational, with many women instructors and professors. Islamic education boards are independently run by the following organizations, accredited by the Kerala state government: Samastha Kerala Islamic Education Board, Kerala Nadvathul Mujahideen, Jamaat-e-Islami Hind, and Jamiat Ulema-e-Hind.
In Southeast Asia, Muslim students have a choice of attending a secular government or an Islamic school. Madaris or Islamic schools are known as Sekolah Agama (Malay: religious school) in Malaysia and Indonesia, โรงเรียนศาสนาอิสลาม (Thai: school of Islam) in Thailand and madaris in the Philippines. In countries where Islam is not the majority or state religion, Islamic schools are found in regions such as southern Thailand (near the Thai-Malaysian border) and the southern Philippines in Mindanao, where a significant Muslim population can be found.
In Singapore, madrasahs are private schools which are overseen by Majlis Ugama Islam Singapura (MUIS, English: Islamic Religious Council of Singapore). There are six Madrasahs in Singapore, catering to students from Primary 1 to Secondary 4. Four Madrasahs are coeducational and two are for girls. Students take a range of Islamic Studies subjects in addition to mainstream MOE curriculum subjects and sit for the PSLE and GCE 'O' Levels like their peers. In 2009, MUIS introduced the "Joint Madrasah System" (JMS), a joint collaboration of Madrasah Al-Irsyad Al-Islamiah primary school and secondary schools Madrasah Aljunied Al-Islamiah (offering the ukhrawi, or religious stream) and Madrasah Al-Arabiah Al-Islamiah (offering the academic stream). The JMS aims to introduce the International Baccalaureate (IB) programme into the Madrasah Al-Arabiah Al-Islamiah by 2019. Students attending a madrasah are required to wear the traditional Malay attire, including the songkok for boys and tudong for girls, in contrast to mainstream government schools which ban religious headgear as Singapore is officially a secular state. For students who wish to attend a mainstream school, they may opt to take classes on weekends at the madrasah instead of enrolling full-time.
In 2004, madaris were mainstreamed in 16 Regions nationwide, primarily in Muslim-majority areas in Mindanao under the auspices of the Department of Education (DepEd). The DepEd adopted Department Order No. 51, which instituted Arabic-language and Islamic Values instruction for Muslim children in state schools, and authorised implementation of the Standard Madrasa Curriculum (SMC) in private-run madaris. While there are state-recognised Islamic schools, such as Ibn Siena Integrated School in the Islamic City of Marawi, Sarang Bangun LC in Zamboanga and SMIE in Jolo, their Islamic studies programmes initially varied in application and content.
The first Madressa established in North America, Al-Rashid Islamic Institute, was established in Cornwall, Ontario in 1983 and has graduates who are Hafiz (Quran) and Ulama. The seminary was established by Mazhar Alam under the direction of his teacher the leading Indian Tablighi scholar Muhammad Zakariya Kandhlawi and focuses on the traditional Hanafi school of thought and shuns Salafist / Wahabi teachings. Due to its proximity to the US border city of Messina the school has historically had a high ratio of US students. Their most prominent graduate Shaykh Muhammad Alshareef completed his Hifz in the early 1990s then went on to deviate from his traditional roots and form the Salafist organization the AlMaghrib Institute.
Western commentators post-9/11 often perceive madaris as places of radical revivalism with a connotation of anti-Americanism and radical extremism, frequently associated in the Western press with Wahhabi attitudes toward non-Muslims. In Arabic the word madrasa simply means "school" and does not imply a political or religious affiliation, radical or otherwise. Madaris have varied curricula, and are not all religious. Some madaris in India, for example, have a secularised identity. Although early madaris were founded primarily to gain "knowledge of God" they also taught subjects such as mathematics and poetry. For example, in the Ottoman Empire, "Madrasahs had seven categories of sciences that were taught, such as: styles of writing, oral sciences like the Arabic language, grammar, rhetoric, and history and intellectual sciences, such as logic." This is similar to the Western world, in which universities began as institutions of the Catholic church.
The Sun had the largest circulation of any daily newspaper in the United Kingdom, but in late 2013 slipped to second largest Saturday newspaper behind the Daily Mail. It had an average daily circulation of 2.2 million copies in March 2014. Between July and December 2013 the paper had an average daily readership of approximately 5.5 million, with approximately 31% of those falling into the ABC1 demographic and 68% in the C2DE demographic. Approximately 41% of readers are women. The Sun has been involved in many controversies in its history, including its coverage of the 1989 Hillsborough football stadium disaster. Regional editions of the newspaper for Scotland, Northern Ireland and the Republic of Ireland are published in Glasgow (The Scottish Sun), Belfast (The Sun) and Dublin (The Irish Sun) respectively.
On 26 February 2012, The Sun on Sunday was launched to replace the closed News of the World, employing some of its former journalists. In late 2013, it was given a new look, with a new typeface. The average circulation for The Sun on Sunday in March 2014 was 1,686,840; but in May 2015 The Mail on Sunday sold more copies for the first time, an average of 28,650 over those of its rival: 1,497,855 to 1,469,195. Roy Greenslade issued some caveats over the May 2015 figures, but believes the weekday Daily Mail will overtake The Sun in circulation during 2016.
Research commissioned by Cecil King from Mark Abrams of Sussex University, The Newspaper Reading Public of Tomorrow, identified demographic changes which suggested reasons why the Herald might be in decline. The new paper was intended to add a readership of 'social radicals' to the Herald's 'political radicals'. Launched with an advertising budget of £400,000 the brash new paper "burst forth with tremendous energy", according to The Times. Its initial print run of 3.5 million was attributed to 'curiosity' and the 'advantage of novelty', and had declined to the previous circulation of the Daily Herald (1.2 million) within a few weeks.
Seizing the opportunity to increase his presence on Fleet Street, he made an agreement with the print unions, promising fewer redundancies if he acquired the newspaper. He assured IPC that he would publish a "straightforward, honest newspaper" which would continue to support Labour. IPC, under pressure from the unions, rejected Maxwell's offer, and Murdoch bought the paper for £800,000, to be paid in instalments. He would later remark: "I am constantly amazed at the ease with which I entered British newspapers."
Murdoch found he had such a rapport with Larry Lamb over lunch that other potential recruits as editor were not interviewed and Lamb was appointed as the first editor of the new Sun. He was scathing in his opinion of the Mirror, where he had recently been employed as a senior sub-editor, and shared Murdoch's view that a paper's quality was best measured by its sales, and he regarded the Mirror as overstaffed, and primarily aimed at an ageing readership. Lamb hastily recruited a staff of about 125 reporters, who were mostly selected for their availability rather than their ability.
Sex was used as an important element in the content and marketing the paper from the start, which Lamb believed was the most important part of his readers' lives. The first topless Page 3 model appeared on 17 November 1970, German-born Stephanie Rahn; she was tagged as a "Birthday Suit Girl" to mark the first anniversary of the relaunched Sun. A topless Page 3 model gradually became a regular fixture, and with increasingly risqué poses. Both feminists and many cultural conservatives saw the pictures as pornographic and misogynistic. Lamb expressed some regret at introducing the feature, although denied it was sexist. A Conservative council in Sowerby Bridge, Yorkshire, was the first to ban the paper from its public library, shortly after Page 3 began, because of its excessive sexual content. This decision was reversed after a sustained campaign by the newspaper itself lasting 16 months, and the election of a Labour-led council in 1971.
Politically, The Sun in the early Murdoch years, remained nominally Labour. It supported the Labour Party led by Harold Wilson in the 1970 General Election, with the headline "Why It Must Be Labour" but by February 1974 it was calling for a vote for the Conservative Party led by Edward Heath while suggesting that it might support a Labour Party led by James Callaghan or Roy Jenkins. In the October election an editorial asserted: "ALL our instincts are left rather than right and we would vote for any able politician who would describe himself as a Social Democrat."
The editor, Larry Lamb, was originally from a Labour background, with a socialist upbringing while his temporary replacement Bernard Shrimsley (1972–75) was a middle-class uncommitted Conservative. An extensive advertising campaign on the ITV network in this period, voiced by actor Christopher Timothy, may have helped The Sun to overtake the Daily Mirror's circulation in 1978. Despite the industrial relations of the 1970s – the so-called "Spanish practices" of the print unions – The Sun was very profitable, enabling Murdoch to expand his operations to the United States from 1973.
The Daily Star had been launched in 1978 by Express Newspaper, and by 1981 had begun to affect sales of The Sun. Bingo was introduced as a marketing tool and a 2p drop in cover price removed the Daily Star's competitive advantage opening a new circulation battle which resulted in The Sun neutralising the threat of the new paper. The new editor of The Sun, Kelvin MacKenzie, took up his post in 1981 just after these developments, and "changed the British tabloid concept more profoundly than [Larry] Lamb did", according to Bruce Page, MacKenzie The paper became "more outrageous, opinionated and irreverent than anything ever produced in Britain".
On 1 May, The Sun claimed to have 'sponsored' a British missile. Under the headline "Stick This Up Your Junta: A Sun missile for Galtieri’s gauchos", the newspaper published a photograph of a missile, (actually a Polaris missile stock shot from the Ministry of Defence) which had a large Sun logo printed on its side with the caption "Here It Comes, Senors..." underneath. The paper explained that it was 'sponsoring' the missile by contributing to the eventual victory party on HMS Invincible when the war ended. In copy written by Wendy Henry, the paper said that the missile would shortly be used against Argentinian forces. Despite this, it was not well received by the troops and copies of The Sun were soon burnt. Tony Snow, The Sun journalist on HMS Invincible who had 'signed' the missile, reported a few days later that it had hit an Argentinian target.
One of the paper's best known front pages, published on 4 May 1982, commemorated the torpedoing of the Argentine ship the General Belgrano by running the story under the headline "GOTCHA". At MacKenzie's insistence, and against the wishes of Murdoch (the mogul was present because almost all the journalists were on strike), the headline was changed for later editions after the extent of Argentinian casualties became known. John Shirley, a reporter for The Sunday Times, witnessed copies of this edition of The Sun being thrown overboard by sailors and marines on HMS Fearless.
After HMS Sheffield was wrecked by an Argentinian attack, The Sun was heavily criticised and even mocked for its coverage of the war in The Daily Mirror and The Guardian, and the wider media queried the veracity of official information and worried about the number of casualties, The Sun gave its response. "There are traitors in our midst", wrote leader writer Ronald Spark on 7 May, accusing commentators on Daily Mirror and The Guardian, plus the BBC's defence correspondent Peter Snow, of "treason" for aspects of their coverage.
These years included what was called "spectacularly malicious coverage" of the Labour Party by The Sun and other newspapers. During the general election of 1983 The Sun ran a front page featuring an unflattering photograph of Michael Foot, then aged almost 70, claiming he was unfit to be Prime Minister on grounds of his age, appearance and policies, alongside the headline "Do You Really Want This Old Fool To Run Britain?" A year later, in 1984, The Sun made clear its enthusiastic support for the re-election of Ronald Reagan as president in the USA. Reagan was two weeks off his 74th birthday when he started his second term, in January 1985.
The Sun, during the Miners' strike of 1984–85, supported the police and the Thatcher government against the striking NUM miners, and in particular the union's president, Arthur Scargill. On 23 May 1984, The Sun prepared a front page with the headline "Mine Führer" and a photograph of Scargill with his arm in the air, a pose which made him look as though he was giving a Nazi salute. The print workers at The Sun refused to print it. The Sun strongly supported the April 1986 bombing of Libya by the US, which was launched from British bases. Several civilians were killed during the bombing. Their leader was "Right Ron, Right Maggie". That year, Labour MP Clare Short attempted in vain to persuade Parliament to outlaw the pictures on Page Three and gained opprobrium from the newspaper for her stand.
Murdoch has responded to some of the arguments against the newspaper by saying that critics are "snobs" who want to "impose their tastes on everyone else", while MacKenzie claims the same critics are people who, if they ever had a "popular idea", would have to "go and lie down in a dark room for half an hour". Both have pointed to the huge commercial success of the Sun in this period and its establishment as Britain's top-selling newspaper, claiming that they are "giving the public what they want". This conclusion is disputed by critics. John Pilger has said that a late-1970s edition of the Daily Mirror, which replaced the usual celebrity and domestic political news items with an entire issue devoted to his own front-line reporting of the genocide in Pol Pot's Cambodia, not only outsold The Sun on the day it was issued but became the only edition of the Daily Mirror to ever sell every single copy issued throughout the country, something never achieved by The Sun.
According to Max Clifford: Read All About It, written by Clifford and Angela Levin, La Salle invented the story out of frustration with Starr who had been working on a book with McCaffrey. She contacted an acquaintance who worked for The Sun in Manchester. The story reportedly delighted MacKenzie, who was keen to run it, and Max Clifford, who had been Starr's public relations agent. Starr had to be persuaded that the apparent revelation would not damage him; the attention helped to revive his career. In his 2001 autobiography Unwrapped, Starr wrote that the incident was a complete fabrication: "I have never eaten or even nibbled a live hamster, gerbil, guinea pig, mouse, shrew, vole or any other small mammal."
Eventually resulting in 17 libel writs in total, The Sun ran a series of false stories about the pop musician Elton John from 25 February 1987. They began with an invented account of the singer having sexual relationships with rent boys. The singer-songwriter was abroad on the day indicated in the story, as former Sun journalist John Blake, recently poached by the Daily Mirror, soon discovered. After further stories, in September 1987, The Sun accused John of having his Rottweiler guard dogs voice boxes surgically removed. In November, the Daily Mirror found their rival's only source for the rent boy story and he admitted it was a totally fictitious concoction created for money. The inaccurate story about his dogs, actually Alsatians, put pressure on The Sun, and John received £1 million in an out of court settlement, then the largest damages payment in British history. The Sun ran a front-page apology on 12 December 1988, under the banner headline "SORRY, ELTON". In May 1987 gay men were offered free one-way airline tickets to Norway to leave Britain for good: "Fly Away Gays - And We Will Pay" was the paper's headline. Gay Church of England clergymen were described in one headline in November 1987 as "Pulpit poofs".
Television personality Piers Morgan, a former editor of the Daily Mirror and of The Sun's Bizarre pop column, has said that during the late 1980s, at Kelvin MacKenzie's behest, he was ordered to speculate on the sexuality of male pop stars for a feature headlined "The Poofs of Pop". He also recalls MacKenzie headlining a January 1989 story about the first same-sex kiss on the BBC television soap opera EastEnders "EastBenders", describing the kiss between Colin Russell and Guido Smith as "a homosexual love scene between yuppie poofs ... when millions of children were watching".
On 17 November 1989, The Sun headlined a page 2 news story titled "STRAIGHT SEX CANNOT GIVE YOU AIDS – OFFICIAL." The Sun favourably cited the opinions of Lord Kilbracken, a member of the All Parliamentary Group on AIDS. Lord Kilbracken said that only one person out of the 2,372 individuals with HIV/AIDS mentioned in a specific Department of Health report was not a member of a "high risk group", such as homosexuals and recreational drug users. The Sun also ran an editorial further arguing that "At last the truth can be told... the risk of catching AIDS if you are heterosexual is "statistically invisible". In other words, impossible. So now we know – everything else is homosexual propaganda." Although many other British press services covered Lord Kilbracken's public comments, none of them made the argument that the Sun did in its editorial and none of them presented Lord Kilbracken's ideas without context or criticism.
Critics stated that both The Sun and Lord Kilbracken cherry-picked the results from one specific study while ignoring other data reports on HIV infection and not just AIDS infection, which the critics viewed as unethical politicisation of a medical issue. Lord Kilbracken himself criticised The Sun's editorial and the headline of its news story; he stated that while he thought that gay people were more at risk of developing AIDS it was still wrong to imply that no one else could catch the disease. The Press Council condemned The Sun for committing what it called a "gross distortion". The Sun later ran an apology, which they ran on Page 28. Journalist David Randall argued in the textbook The Universal Journalist that The Sun's story was one of the worst cases of journalistic malpractice in recent history, putting its own readers in harm's way.
Under a front page headline "The Truth", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable "whilst he was administering the kiss of life to a patient." Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.
The front page caused outrage in Liverpool, where the paper lost more than three-quarters of its estimated 55,000 daily sales and still sells poorly in the city more than 25 years later (around 12,000). It is unavailable in many parts of the city, as many newsagents refuse to stock it. It was revealed in a documentary called Alexei Sayle's Liverpool, aired in September 2008, that many Liverpudlians will not even take the newspaper for free, and those who do may simply burn or tear it up. Liverpudlians refer to the paper as 'The Scum' with campaigners believing it handicapped their fight for justice.
On 7 July 2004, in response to verbal attacks in Liverpool on Wayne Rooney, just before his transfer from Everton to Manchester United, who had sold his life story to The Sun, the paper devoted a full-page editorial to an apology for the "awful error" of its Hillsborough coverage and argued that Rooney (who was still only three years old at the time of Hillsborough) should not be punished for its "past sins". In January 2005, The Sun's managing editor Graham Dudman admitting the Hillsborough coverage was "the worst mistake in our history", added: "What we did was a terrible mistake. It was a terrible, insensitive, horrible article, with a dreadful headline; but what we'd also say is: we have apologised for it, and the entire senior team here now is completely different from the team that put the paper out in 1989."
The Sun remained loyal to Thatcher right up to her resignation in November 1990, despite the party's fall in popularity over the previous year following the introduction of the Poll tax (officially known as the Community Charge). This change to the way local government is funded was vociferously supported by the newspaper, despite widespread opposition, (some from Conservative MPs), which is seen as having contributed to Thatcher's own downfall. The tax was quickly repealed by her successor John Major, whom The Sun initially supported enthusiastically, believing he was a radical Thatcherite – despite the economy having entered recession at this time.
Despite its initial opposition to the closures, until 1997, the newspaper repeatedly called for the implementation of further Thatcherite policies, such as Royal Mail privatisation,[verification needed] and social security cutbacks, with leaders such as "Peter Lilley is right, we can't carry on like this",[verification needed] The paper showed hostility to the EU and approval of public spending cuts, tax cuts, and promotion of right-wing ministers to the cabinet, with leaders such as "More of the Redwood, not Deadwood".
The Sun switched support to the Labour party on 18 March 1997, six weeks before the General Election victory which saw the New Labour leader Tony Blair become Prime Minister with a large parliamentary majority, despite the paper having attacked Blair and New Labour up to a month earlier. Its front page headline read THE SUN BACKS BLAIR and its front page editorial made clear that while it still opposed some New Labour policies, such as the Minimum Wage and Devolution, it believed Blair to be "the breath of fresh air this great country needs". John Major's Conservatives, it said, were "tired, divided and rudderless". Blair, who had radically altered his party's image and policies, noting the influence the paper could have over its readers' political thinking, had courted it (and Murdoch) for some time by granting exclusive interviews and writing columns.
In exchange for Rupert Murdoch's support, Blair agreed not to join the European Exchange Rate Mechanism – which John Major had withdrawn the country from in September 1992 after barely two years. Cabinet Minister Peter Mandelson was "outed" by Matthew Parris (a former Sun columnist) on BBC TV's Newsnight in November 1998. Misjudging public response, The Sun's editor David Yelland demanded to know in a front page editorial whether Britain was governed by a "gay mafia" of a "closed world of men with a mutual self-interest". Three days later the paper apologised in another editorial which said The Sun would never again reveal a person's sexuality unless it could be defended on the grounds of "overwhelming public interest".
In 2003 the paper was accused of racism by the Government over its criticisms of what it perceived as the "open door" policy on immigration. The attacks came from the Prime Minister's press spokesman Alastair Campbell and the Home Secretary David Blunkett (later a Sun columnist). The paper rebutted the claim, believing that it was not racist to suggest that a "tide" of unchecked illegal immigrants was increasing the risk of terrorist attacks and infectious diseases. It did not help its argument by publishing a front page story on 4 July 2003, under the headline "Swan Bake", which claimed that asylum seekers were slaughtering and eating swans. It later proved to have no basis in fact. Subsequently The Sun published a follow-up headlined "Now they're after our fish!". Following a Press Complaints Commission adjudication a "clarification" was eventually printed, on page 41. In 2005 The Sun published photographs of Prince Harry sporting a Nazi costume to a fancy dress party. The photographs caused outrage across the world and Clarence House was forced to issue a statement in response apologising for any offence or embarrassment caused.
Despite being a persistent critic of some of the government's policies, the paper supported Labour in both subsequent elections the party won. For the 2005 general election, The Sun backed Blair and Labour for a third consecutive election win and vowed to give him "one last chance" to fulfil his promises, despite berating him for several weaknesses including a failure to control immigration. However, it did speak of its hope that the Conservatives (led by Michael Howard) would one day be fit for a return to government. This election (Blair had declared it would be his last as prime minister) resulted in Labour's third successive win but with a much reduced majority.
On 22 September 2003 the newspaper appeared to misjudge the public mood surrounding mental health, as well as its affection for former world heavyweight champion boxer Frank Bruno, who had been admitted to hospital, when the headline "Bonkers Bruno Locked Up" appeared on the front page of early editions. The adverse reaction, once the paper had hit the streets on the evening of 21 September, led to the headline being changed for the paper's second edition to the more sympathetic "Sad Bruno In Mental Home".
The Sun has been openly antagonistic towards other European nations, particularly the French and Germans. During the 1980s and 1990s, the nationalities were routinely described in copy and headlines as "frogs", "krauts" or "hun". As the paper is opposed to the EU it has referred to foreign leaders who it deemed hostile to the UK in unflattering terms. Former President Jacques Chirac of France, for instance, was branded "le Worm". An unflattering picture of German chancellor Angela Merkel, taken from the rear, bore the headline "I'm Big in the Bumdestag" (17 April 2006).
On 7 January 2009, The Sun ran an exclusive front page story claiming that participants in a discussion on Ummah.com, a British Muslim internet forum, had made a "hate hit list" of British Jews to be targeted by extremists over the Gaza War. It was claimed that "Those listed [on the forum] should treat it very seriously. Expect a hate campaign and intimidation by 20 or 30 thugs." The UK magazine Private Eye claimed that Glen Jenvey, a man quoted by The Sun as a terrorism expert, who had been posting to the forum under the pseudonym "Abuislam", was the only forum member promoting a hate campaign while other members promoted peaceful advocacy, such as writing 'polite letters'. The story has since been removed from The Sun's website following complaints to the UK's Press Complaints Commission.
On 9 December 2010, The Sun published a front-page story claiming that terrorist group Al-Qaeda had threatened a terrorist attack on Granada Television in Manchester to disrupt the episode of the soap opera Coronation Street to be transmitted live that evening. The paper cited unnamed sources, claiming "cops are throwing a ring of steel around tonight's live episode of Coronation Street over fears it has been targeted by Al-Qaeda." Later that morning, however, Greater Manchester Police categorically denied having "been made aware of any threat from Al-Qaeda or any other proscribed organisation." The Sun published a small correction on 28 December, admitting "that while cast and crew were subject to full body searches, there was no specific threat from Al-Qaeda as we reported." The apology had been negotiated by the Press Complaints Commission. For the day following the 2011 Norway attacks The Sun produced an early edition blaming the massacre on al-Qaeda. Later the perpetrator was revealed to be Anders Behring Breivik, a Norwegian nationalist.
In January 2008 the Wapping presses printed The Sun for the last time and London printing was transferred to Waltham Cross in the Borough of Broxbourne in Hertfordshire, where News International had built what is claimed to be the largest printing centre in Europe with 12 presses. The site also produces The Times and Sunday Times, Daily Telegraph and Sunday Telegraph, Wall Street Journal Europe (also now a Murdoch newspaper), London's Evening Standard and local papers. Northern printing had earlier been switched to a new plant at Knowsley on Merseyside and the Scottish Sun to another new plant at Motherwell near Glasgow. The three print centres represent a £600 million investment by NI and allowed all the titles to be produced with every page in full colour from 2008. The Waltham Cross plant is capable of producing one million copies an hour of a 120-page tabloid newspaper.
Politically, the paper's stance was less clear under Prime Minister Gordon Brown who succeeded Blair in June 2007. Its editorials were critical of many of Brown's policies and often more supportive of those of Conservative leader David Cameron. Rupert Murdoch, head of The Sun's parent company News Corporation, speaking at a 2007 meeting with the House of Lords Select Committee on Communications, which was investigating media ownership and the news, said that he acts as a "traditional proprietor". This means he exercises editorial control on major issues such as which political party to back in a general election or which policy to adopt on Europe.
During the campaign for the United Kingdom general election, 2010, The Independent ran ads declaring that "Rupert Murdoch won't decide this election – you will." In response James Murdoch and Rebekah Wade "appeared unannounced and uninvited on the editorial floor" of the Independent, and had an energetic conversation with its editor Simon Kelner. Several days later the Independent reported The Sun's failure to report its own YouGov poll result which said that "if people thought Mr Clegg's party had a significant chance of winning the election" the Liberal Democrats would win 49% of the vote, and with it a landslide majority.
On election day (6 May 2010), The Sun urged its readers to vote for David Cameron's "modern and positive" Conservatives in order to save Britain from "disaster" which the paper thought the country would face if the Labour government was re-elected. The election ended in the first hung parliament after an election for 36 years, with the Tories gaining the most seats and votes but being 20 seats short of an overall majority. They finally came to power on 11 May when Gordon Brown stepped down as prime minister, paving the way for David Cameron to become prime minister by forming a coalition with the Liberal Democrats.
On 28 January 2012, police arrested four current and former staff members of The Sun, as part of a probe in which journalists paid police officers for information; a police officer was also arrested in the probe. The Sun staffers arrested were crime editor Mike Sullivan, head of news Chris Pharo, former deputy editor Fergus Shanahan, and former managing editor Graham Dudman, who since became a columnist and media writer. All five arrested were held on suspicion of corruption. Police also searched the offices of News International, the publishers of The Sun, as part of a continuing investigation into the News of the World scandal.
The main party leaders, David Cameron, Nick Clegg and Ed Miliband, were all depicted holding a copy of the special issue in publicity material. Miliband's decision to pose with a copy of The Sun received a strong response. Organisations representing the relatives of Hillsborough victims described Miliband's action as an "absolute disgrace" and he faced criticism too from Liverpool Labour MPs and the city's Labour Mayor, Joe Anderson. A statement was issued on 13 June explaining that Miliband "was promoting England's bid to win the World Cup", although "he understands the anger that is felt towards the Sun over Hillsborough by many people in Merseyside and he is sorry to those who feel offended."
On 2 June 2013, The Sun on Sunday ran a front page story on singer-songwriter Tulisa Contostavlos. The front page read: "Tulisa's cocaine deal shame"; this story was written by The Sun On Sunday's undercover reporter Mahzer Mahmood, who had previously worked for the News of the World. It was claimed that Tulisa introduced three film producers (actually Mahmood and two other Sun journalists) to a drug dealer and set up a £800 deal. The subterfuge involved conning the singer into believing that she was being considered for a role in an £8 million Bollywood film.
At her subsequent trial, the case against Tulisa collapsed at Southwark Crown Court in July 2014, with the judge commenting that there were "strong grounds" to believe that Mahmood had lied at a pre-trial hearing and tried to manipulate evidence against the co-defendant Tulisa. Tulisa was cleared of supplying Class A drugs. After these events, The Sun released a statement saying that the newspaper "takes the Judge's remarks very seriously. Mahmood has been suspended pending an immediate internal investigation."
In October 2014, the trial of six senior staff and journalists at The Sun newspaper began. All six were charged with conspiring to commit misconduct in a public office. They included The Sun's head of news Chris Pharo, who faced six charges, while ex-managing editor Graham Dudman and ex-Sun deputy news editor Ben O'Driscoll were accused of four charges each. Thames Valley district reporter Jamie Pyatt and picture editor John Edwards were charged with three counts each, while ex-reporter John Troup was accused of two counts. The trial related to illegal payments allegedly made to public officials, with prosecutors saying the men conspired to pay officials from 2002–11, including police, prison officers and soldiers. They were accused of buying confidential information about the Royal Family, public figures and prison inmates. They all denied the charges. On 16 January 2015, Troup and Edwards were cleared by the jury of all charges against them. The jury also partially cleared O'Driscoll and Dudman but continued deliberating over other counts faced by them, as well as the charges against Pharo and Pyatt. On 21 January 2015, the jury told the court that it was unable to reach unanimous verdicts on any of the outstanding charges and was told by the judge, Richard Marks, that he would accept majority verdicts. Shortly afterwards, one of the jurors sent a note to the judge and was discharged. The judge told the remaining 11 jurors that their colleague had been "feeling unwell and feeling under a great deal of pressure and stress from the situation you are in", and that under the circumstances he was prepared to accept majority verdicts of "11 to zero or 10 to 1". On 22 January 2015, the jury was discharged after failing to reach verdicts on the outstanding charges. The Crown Prosecution Service (CPS) announced that it would seek a retrial.
On 6 February 2015, it was announced that Judge Richard Marks is to be replaced by Judge Charles Wide at the retrial. Two days earlier, Marks had emailed counsel for the defendants telling them: "It has been decided (not by me but by my elders and betters) that I am not going to be doing the retrial". Reporting the decision in UK newspaper The Guardian, Lisa O’Carroll wrote: "Wide is the only judge so far to have presided in a case which has seen a conviction of a journalist in relation to allegations of unlawful payments to public officials for stories. The journalist, who cannot be named for legal reasons, is appealing the verdict". Defence counsel for the four journalists threatened to take the decision to judicial review, with the barrister representing Pharo, Nigel Rumfitt QC, saying: "The way this has come about gives rise to the impression that something has been going on behind the scenes which should not have been going on behind the scenes and which should have been dealt with transparently". He added that the defendants were "extremely concerned" and "entitled" to know why Marks was being replaced by Wide.
On 22 May 2015, Sun reporter Anthony France was found guilty of aiding and abetting misconduct in a public office between 2008 and 2011. France’s trial followed the London Metropolitan Police's Operation Elveden, an ongoing investigation into alleged payments to police and officials in exchange for information. He had paid a total of more than £22,000 to PC Timothy Edwards, an anti-terrorism police officer based at Heathrow Airport. The police officer had already pleaded guilty to misconduct in a public office and given a two-year gaol sentence in 2014, but the jury in France’s trial was not informed of this. Following the passing of the guilty verdict, the officer leading Operation Elveden, Detective Chief Superintendent Gordon Briggs said France and Edwards had been in a "long-term, corrupt relationship".
The BBC reported that France was the first journalist to face trial and be convicted under Operation Elveden since the Crown Prosecution Service (CPS) had revised its guidance in April 2015 so that prosecutions would only be brought against journalists who had made payments to police officers over a period of time. As a result of the change in the CPS’ policy, charges against several journalists who had made payments to other types of public officials – including civil servants, health workers and prison staff - had been dropped. In July 2015, Private Eye magazine reported that at a costs hearing at the Old Bailey The Sun's parent company had refused to pay for the prosecution costs relating to France’s trial, leading the presiding judge to express his "considerable disappointment" at this state of affairs. Judge Timothy Pontius said in court that France’s illegal actions had been part of a "clearly recognised procedure at The Sun", adding that, "There can be no doubt that News International bears some measure of moral responsibility if not legal culpability for the acts of the defendant". The Private Eye report noted that despite this The Sun's parent organisation was "considering disciplinary actions" against France whilst at the same time it was also preparing to bring a case to the Investigatory Powers Tribunal against the London Metropolitan Police Service for its actions relating to him and two other journalists.
In August 2013, The Irish Sun ended the practice of featuring topless models on Page 3. The main newspaper was reported to have followed in 2015 with the edition of 16 January supposedly the last to carry such photographs after a report in The Times made such an assertion. After substantial coverage in the media about an alleged change in editorial policy, Page 3 returned to its usual format on 22 January 2015. A few hours before the issue was published, the Head of PR at the newspaper said the reputed end of Page 3 had been "speculation" only.
On 17 April 2015, The Sun's columnist Katie Hopkins called migrants to Britain "cockroaches" and "feral humans" and said they were "spreading like the norovirus". Her remarks were condemned by the United Nations High Commission for Human Rights. In a statement released on 24 April 2015, High Commissioner Zeid Ra'ad Al Hussein stated that Hopkins' used "language very similar to that employed by Rwanda's Kangura newspaper and Radio Mille Collines during the run up to the 1994 genocide", and noted that both media organizations were subsequently convicted by an international tribunal of public incitement to commit genocide.
Hopkins' column also drew criticism on Twitter, including from Russell Brand, to whom Hopkins responded by accusing Brand's "champagne socialist humanity" of neglecting taxpayers. Simon Usborne, writing in The Independent, compared her use of the word "cockroach" to previous uses by the Nazis and just before the Rwandan Genocide by its perpetrators. He suspected that if any other contributor had written the piece it would not have been published and questioned her continued employment by the newspaper. Zoe Williams commented in The Guardian: "It is no joke when people start talking like this. We are not 'giving her what she wants' when we make manifest our disgust. It is not a free speech issue. I’m not saying gag her: I’m saying fight her".
On 9 March 2016, The Sun's front page proclaimed that Queen Elizabeth II was backing "Brexit", a common term for a British withdrawal from the European Union. It claimed that in 2011 at Windsor Castle, while having lunch with Deputy Prime Minister Nick Clegg, the monarch criticised the union. Clegg denied that the Queen made such a statement, and a Buckingham Palace spokesperson confirmed that a complaint had been made to the Independent Press Standards Organisation over a breach of guidelines relating to accuracy.
Cambridge English Dictionary states that culture is, "the way of life, especially the general customs and beliefs, of a particular group of people at a particular time." Terror Management Theory posits that culture is a series of activities and worldviews that provide humans with the illusion of being individuals of value in a world meaning—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that Homo Sapiens became aware of when they acquired a larger brain.
As a defining aspect of what it means to be human, culture is a central concept in anthropology, encompassing the range of phenomena that are transmitted through social learning in human societies. The word is used in a general sense as the evolved ability to categorize and represent experiences with symbols and to act imaginatively and creatively. This ability arose with the evolution of behavioral modernity in humans around 50,000 years ago.[citation needed] This capacity is often thought to be unique to humans, although some other species have demonstrated similar, though much less complex abilities for social learning. It is also used to denote the complex networks of practices and accumulated knowledge and ideas that is transmitted through social interaction and exist in specific human groups, or cultures, using the plural form. Some aspects of human behavior, such as language, social practices such as kinship, gender and marriage, expressive forms such as art, music, dance, ritual, religion, and technologies such as cooking, shelter, clothing are said to be cultural universals, found in all human societies. The concept material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including, practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science make up the intangible cultural heritage of a society.
In the humanities, one sense of culture, as an attribute of the individual, has been the degree to which they have cultivated a particular level of sophistication, in the arts, sciences, education, or manners. The level of cultural sophistication has also sometimes been seen to distinguish civilizations from less complex societies. Such hierarchical perspectives on culture are also found in class-based distinctions between a high culture of the social elite and a low culture, popular culture or folk culture of the lower classes, distinguished by the stratified access to cultural capital. In common parlance, culture is often used to refer specifically to the symbolic markers used by ethnic groups to distinguish themselves visibly from each other such as body modification, clothing or jewelry.[dubious – discuss] Mass culture refers to the mass-produced and mass mediated forms of consumer culture that emerged in the 20th century. Some schools of philosophy, such as Marxism and critical theory, have argued that culture is often used politically as a tool of the elites to manipulate the lower classes and create a false consciousness, such perspectives common in the discipline of cultural studies. In the wider social sciences, the theoretical perspective of cultural materialism holds that human symbolic culture arises from the material conditions of human life, as humans create the conditions for physical survival, and that the basis of culture is found in evolved biological dispositions.
When used as a count noun "a culture", is the set of customs, traditions and values of a society or community, such as an ethnic group or nation. In this sense, multiculturalism is a concept that values the peaceful coexistence and mutual respect between different cultures inhabiting the same territory. Sometimes "culture" is also used to describe specific practices within a subgroup of a society, a subculture (e.g. "bro culture"), or a counter culture. Within cultural anthropology, the ideology and analytical stance of cultural relativism holds that cultures cannot easily be objectively ranked or evaluated because any evaluation is necessarily situated within the value system of a given culture.
The modern term "culture" is based on a term used by the Ancient Roman orator Cicero in his Tusculanae Disputationes, where he wrote of a cultivation of the soul or "cultura animi", using an agricultural metaphor for the development of a philosophical soul, understood teleologically as the highest possible ideal for human development. Samuel Pufendorf took over this metaphor in a modern context, meaning something similar, but no longer assuming that philosophy was man's natural perfection. His use, and that of many writers after him "refers to all the ways in which human beings overcome their original barbarism, and through artifice, become fully human".
Social conflict and the development of technologies can produce changes within a society by altering social dynamics and promoting new cultural models, and spurring or enabling generative action. These social shifts may accompany ideological shifts and other types of cultural change. For example, the U.S. feminist movement involved new practices that produced a shift in gender relations, altering both gender and economic structures. Environmental conditions may also enter as factors. For example, after tropical forests returned at the end of the last ice age, plants suitable for domestication were available, leading to the invention of agriculture, which in turn brought about many cultural innovations and shifts in social dynamics.
Cultures are externally affected via contact between societies, which may also produce—or inhibit—social shifts and changes in cultural practices. War or competition over resources may impact technological development or social dynamics. Additionally, cultural ideas may transfer from one society to another, through diffusion or acculturation. In diffusion, the form of something (though not necessarily its meaning) moves from one culture to another. For example, hamburgers, fast food in the United States, seemed exotic when introduced into China. "Stimulus diffusion" (the sharing of ideas) refers to an element of one culture leading to an invention or propagation in another. "Direct Borrowing" on the other hand tends to refer to technological or tangible diffusion from one culture to another. Diffusion of innovations theory presents a research-based model of why and when individuals and cultures adopt new ideas, practices, and products.
Immanuel Kant (1724–1804) has formulated an individualist definition of "enlightenment" similar to the concept of bildung: "Enlightenment is man's emergence from his self-incurred immaturity." He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: Sapere aude, "Dare to be wise!" In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of bildung: "For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people."
In 1795, the Prussian linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a "Germany" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as "worldview" (Weltanschauung). According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between "civilized" and "primitive" or "tribal" cultures.
In 1860, Adolf Bastian (1826–1905) argued for "the psychic unity of mankind". He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of "elementary ideas" (Elementargedanken); different cultures, or different "folk ideas" (Völkergedanken), are local modifications of the elementary ideas. This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.
In practice, culture referred to an élite ideal and was associated with such activities as art, classical music, and haute cuisine. As these forms were associated with urban life, "culture" was identified with "civilization" (from lat. civitas, city). Another facet of the Romantic movement was an interest in folklore, which led to identifying a "culture" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling social group, and low culture. In other words, the idea of "culture" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.
Matthew Arnold contrasted "culture" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted "culture" with "the state of nature". According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between "civilized" and "uncivilized." According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures is really an expression of the conflict between European elites and non-elites, some critics have argued that the distinction between civilized and uncivilized people is really an expression of the conflict between European colonial powers and their colonial subjects.
Other 19th-century critics, following Rousseau have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by "the folk", i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as "noble savages" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of the West.
Although anthropologists worldwide refer to Tylor's definition of culture, in the 20th century "culture" emerged as the central and unifying concept of American anthropology, where it most commonly refers to the universal human capacity to classify and encode human experiences symbolically, and to communicate symbolically encoded experiences socially.[citation needed] American anthropology is organized into four fields, each of which plays an important role in research on culture: biological anthropology, linguistic anthropology, cultural anthropology, and archaeology.
The sociology of culture concerns culture—usually understood as the ensemble of symbolic codes used by a society—as manifested in society. For Georg Simmel (1858–1918), culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history". Culture in the sociological field can be defined as the ways of thinking, the ways of acting, and the material objects that together shape a people's way of life. Culture can be any of two types, non-material culture or material culture. Non-material culture refers to the non physical ideas that individuals have about their culture, including values, belief system, rules, norms, morals, language, organizations, and institutions. While Material culture is the physical evidence of a culture in the objects and architecture they make, or have made. The term tends to be relevant only in archeological and anthropological studies, but it specifically means all material evidence which can be attributed to culture past or present.
Cultural sociology first emerged in Weimar Germany (1918–1933), where sociologists such as Alfred Weber used the term Kultursoziologie (cultural sociology). Cultural sociology was then "reinvented" in the English-speaking world as a product of the "cultural turn" of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may loosely be regarded as an approach incorporating cultural analysis and critical theory. Cultural sociologists tend to reject scientific methods,[citation needed] instead hermeneutically focusing on words, artifacts and symbols. "Culture" has since become an important concept across many branches of sociology, including resolutely scientific fields like social stratification and social network analysis. As a result, there has been a recent influx of quantitative sociologists to the field. Thus there is now a growing group of sociologists of culture who are, confusingly, not cultural sociologists. These scholars reject the abstracted postmodern aspects of cultural sociology, and instead look for a theoretical backing in the more scientific vein of social psychology and cognitive science. "Cultural sociology" is one of the largest sections of the American Sociological Association. The British establishment of cultural studies means the latter is often taught as a loosely distinct discipline in the UK.
The sociology of culture grew from the intersection between sociology (as shaped by early theorists like Marx, Durkheim, and Weber) with the growing discipline of anthropology, where in researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field lingers in the methods (much of cultural sociological research is qualitative), in the theories (a variety of critical approaches to sociology are central to current research communities), and in the substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.
In the United Kingdom, sociologists and other scholars influenced by Marxism, such as Stuart Hall (1932–2014) and Raymond Williams (1921–1988), developed cultural studies. Following nineteenth-century Romantics, they identified "culture" with consumption goods and leisure activities (such as art, music, film, food, sports, and clothing). Nevertheless, they saw patterns of consumption and leisure as determined by relations of production, which led them to focus on class relations and the organization of production.
In the United States, "Cultural Studies" focuses largely on the study of popular culture, that is, on the social meanings of mass-produced consumer and leisure goods. Richard Hoggart coined the term in 1964 when he founded the Birmingham Centre for Contemporary Cultural Studies or CCCS. It has since become strongly associated with Stuart Hall, who succeeded Hoggart as Director. Cultural studies in this sense, then, can be viewed as a limited concentration scoped on the intricacies of consumerism, which belongs to a wider culture sometimes referred to as "Western Civilization" or as "Globalism."
From the 1970s onward, Stuart Hall's pioneering work, along with that of his colleagues Paul Willis, Dick Hebdige, Tony Jefferson, and Angela McRobbie, created an international intellectual movement. As the field developed it began to combine political economy, communication, sociology, social theory, literary theory, media theory, film/video studies, cultural anthropology, philosophy, museum studies and art history to study cultural phenomena or cultural texts. In this field researchers often concentrate on how particular phenomena relate to matters of ideology, nationality, ethnicity, social class, and/or gender.[citation needed] Cultural studies has a concern with the meaning and practices of everyday life. These practices comprise the ways people do particular things (such as watching television, or eating out) in a given culture. This field studies the meanings and uses people attribute to various objects and practices. Specifically, culture involves those meanings and practices held independently of reason. Watching television in order to view a public perspective on a historical event should not be thought of as culture, unless referring to the medium of television itself, which may have been selected culturally; however, schoolchildren watching television after school with their friends in order to "fit in" certainly qualifies, since there is no grounded reason for one's participation in this practice. Recently, as capitalism has spread throughout the world (a process called globalization), cultural studies has begun[when?] to analyze local and global forms of resistance to Western hegemony.[citation needed] Globalization in this context can be defined as western civilization in other ways, it undermines the cultural integrity of other culture and it is therefore repressive, exploitative and harmful to most people in different places.
In the context of cultural studies, the idea of a text includes not only written language, but also films, photographs, fashion or hairstyles: the texts of cultural studies comprise all the meaningful artifacts of culture.[citation needed] Similarly, the discipline widens the concept of "culture". "Culture" for a cultural-studies researcher not only includes traditional high culture (the culture of ruling social groups) and popular culture, but also everyday meanings and practices. The last two, in fact, have become the main focus of cultural studies. A further and recent approach is comparative cultural studies, based on the disciplines of comparative literature and cultural studies.[citation needed]
Scholars in the United Kingdom and the United States developed somewhat different versions of cultural studies after the late 1970s. The British version of cultural studies had originated in the 1950s and 1960s, mainly under the influence first of Richard Hoggart, E. P. Thompson, and Raymond Williams, and later that of Stuart Hall and others at the Centre for Contemporary Cultural Studies at the University of Birmingham. This included overtly political, left-wing views, and criticisms of popular culture as "capitalist" mass culture; it absorbed some of the ideas of the Frankfurt School critique of the "culture industry" (i.e. mass culture). This emerges in the writings of early British cultural-studies scholars and their influences: see the work of (for example) Raymond Williams, Stuart Hall, Paul Willis, and Paul Gilroy.
In the United States, Lindlof and Taylor write, "Cultural studies [were] grounded in a pragmatic, liberal-pluralist tradition". The American version of cultural studies initially concerned itself more with understanding the subjective and appropriative side of audience reactions to, and uses of, mass culture; for example, American cultural-studies advocates wrote about the liberatory aspects of fandom.[citation needed] The distinction between American and British strands, however, has faded.[citation needed] Some researchers, especially in early British cultural studies, apply a Marxist model to the field. This strain of thinking has some influence from the Frankfurt School, but especially from the structuralist Marxism of Louis Althusser and others. The main focus of an orthodox Marxist approach concentrates on the production of meaning. This model assumes a mass production of culture and identifies power as residing with those producing cultural artifacts. In a Marxist view, those who control the means of production (the economic base) essentially control a culture.[citation needed] Other approaches to cultural studies, such as feminist cultural studies and later American developments of the field, distance themselves from this view. They criticize the Marxist assumption of a single, dominant meaning, shared by all, for any cultural product. The non-Marxist approaches suggest that different ways of consuming cultural artifacts affect the meaning of the product. This view comes through in the book Doing Cultural Studies: The Story of the Sony Walkman (by Paul du Gay et al.), which seeks to challenge the notion that those who produce commodities control the meanings that people attribute to them. Feminist cultural analyst, theorist and art historian Griselda Pollock contributed to cultural studies from viewpoints of art history and psychoanalysis. The writer Julia Kristeva is among influential voices at the turn of the century, contributing to cultural studies from the field of art and psychoanalytical French feminism.[citation needed]
Raimon Panikkar pointed out 29 ways in which cultural change can be brought about. Some of these are: growth, development, evolution, involution, renovation, reconception, reform, innovation, revivalism, revolution, mutation, progress, diffusion, osmosis, borrowing, eclecticism, syncretism, modernization, indigenization, and transformation. Hence Modernization could be similar or related to the enlightenment but a 'looser' term set to ideal and values that flourish. a belief in objectivity progress. Also seen as a belief in a secular society (free from religious influences) example objective and rational, science vs religion and finally been modern means not being religious.
The Super Nintendo Entertainment System (officially abbreviated the Super NES[b] or SNES[c], and commonly shortened to Super Nintendo[d]) is a 16-bit home video game console developed by Nintendo that was released in 1990 in Japan and South Korea, 1991 in North America, 1992 in Europe and Australasia (Oceania), and 1993 in South America. In Japan, the system is called the Super Famicom (Japanese: スーパーファミコン, Hepburn: Sūpā Famikon?, officially adopting the abbreviated name of its predecessor, the Family Computer), or SFC for short. In South Korea, it is known as the Super Comboy (슈퍼 컴보이 Syupeo Keomboi) and was distributed by Hyundai Electronics. Although each version is essentially the same, several forms of regional lockout prevent the different versions from being compatible with one another. It was released in Brazil on September 2, 1992, by Playtronic.
To compete with the popular Family Computer in Japan, NEC Home Electronics launched the PC Engine in 1987, and Sega Enterprises followed suit with the Mega Drive in 1988. The two platforms were later launched in North America in 1989 as the TurboGrafx-16 and the Genesis respectively. Both systems were built on 16-bit architectures and offered improved graphics and sound over the 8-bit NES. However, it took several years for Sega's system to become successful. Nintendo executives were in no rush to design a new system, but they reconsidered when they began to see their dominance in the market slipping.
Designed by Masayuki Uemura, the designer of the original Famicom, the Super Famicom was released in Japan on Wednesday, November 21, 1990 for ¥25,000 (US$210). It was an instant success; Nintendo's initial shipment of 300,000 units sold out within hours, and the resulting social disturbance led the Japanese government to ask video game manufacturers to schedule future console releases on weekends. The system's release also gained the attention of the Yakuza, leading to a decision to ship the devices at night to avoid robbery.
On August 23, 1991,[a] Nintendo released the Super Nintendo Entertainment System, a redesigned version of the Super Famicom, in North America for US$199. The SNES was released in the United Kingdom and Ireland in April 1992 for GB£150, with a German release following a few weeks later. Most of the PAL region versions of the console use the Japanese Super Famicom design, except for labeling and the length of the joypad leads. The Playtronic Super NES in Brazil, although PAL, uses the North American design. Both the NES and SNES were released in Brazil in 1993 by Playtronic, a joint venture between the toy company Estrela and consumer electronics company Gradiente.
The rivalry between Nintendo and Sega resulted in what has been described as one of the most notable console wars in video game history, in which Sega positioned the Genesis as the "cool" console, with more mature titles aimed at older gamers, and edgy advertisements that occasionally attacked the competition. Nintendo however, scored an early public relations advantage by securing the first console conversion of Capcom's arcade classic Street Fighter II for SNES, which took over a year to make the transition to Genesis. Despite the Genesis's head start, much larger library of games, and lower price point, the Genesis only represented an estimated 60% of the American 16-bit console market in June 1992, and neither console could maintain a definitive lead for several years. Donkey Kong Country is said to have helped establish the SNES's market prominence in the latter years of the 16-bit generation, and for a time, maintain against the PlayStation and Saturn. According to Nintendo, the company had sold more than 20 million SNES units in the U.S. According to a 2014 Wedbush Securities report based on NPD sales data, the SNES ultimately outsold the Genesis in the U.S. market.
During the NES era, Nintendo maintained exclusive control over titles released for the system—the company had to approve every game, each third-party developer could only release up to five games per year (but some third parties got around this by using different names, for example Konami's "Ultra Games" brand), those games could not be released on another console within two years, and Nintendo was the exclusive manufacturer and supplier of NES cartridges. However, competition from Sega's console brought an end to this practice; in 1991, Acclaim began releasing games for both platforms, with most of Nintendo's other licensees following suit over the next several years; Capcom (which licensed some games to Sega instead of producing them directly) and Square were the most notable holdouts.
The company continued to carefully review submitted titles, giving them scores using a 40-point scale and allocating Nintendo's marketing resources accordingly. Each region performed separate evaluations. Nintendo of America also maintained a policy that, among other things, limited the amount of violence in the games on its systems. One game, Mortal Kombat, would challenge this policy. A surprise hit in arcades in 1992, Mortal Kombat features splashes of blood and finishing moves that often depict one character dismembering the other. Because the Genesis version retained the gore while the SNES version did not, it outsold the SNES version by a ratio of three or four-to-one.
Game players were not the only ones to notice the violence in this game; US Senators Herb Kohl and Joe Lieberman convened a Congressional hearing on December 9, 1993 to investigate the marketing of violent video games to children.[e] While Nintendo took the high ground with moderate success, the hearings led to the creation of the Interactive Digital Software Association and the Entertainment Software Rating Board, and the inclusion of ratings on all video games. With these ratings in place, Nintendo decided its censorship policies were no longer needed.
While other companies were moving on to 32-bit systems, Rare and Nintendo proved that the SNES was still a strong contender in the market. In November 1994, Rare released Donkey Kong Country, a platform game featuring 3D models and textures pre-rendered on SGI workstations. With its detailed graphics, fluid animation and high-quality music, Donkey Kong Country rivaled the aesthetic quality of games that were being released on newer 32-bit CD-based consoles. In the last 45 days of 1994, the game sold 6.1 million units, making it the fastest-selling video game in history to that date. This game sent a message that early 32-bit systems had little to offer over the SNES, and helped make way for the more advanced consoles on the horizon.
In October 1997, Nintendo released a redesigned model of the SNES (the SNS-101 model) in North America for US$99, which sometimes included the pack-in game Super Mario World 2: Yoshi's Island. Like the earlier redesign of the NES (the NES-101 model), the new model was slimmer and lighter than its predecessor, but it lacked S-Video and RGB output, and it was among the last major SNES-related releases in the region. A similarly redesigned Super Famicom Jr. was released in Japan at around the same time.
Internally, a regional lockout chip (CIC) within the console and in each cartridge prevents PAL region games from being played on Japanese or North American consoles and vice versa. The Japanese and North American machines have the same region chip. This can be overcome through the use of adapters, typically by inserting the imported cartridge in one slot and a cartridge with the correct region chip in a second slot. Alternatively, disconnecting one pin of the console's lockout chip will prevent it from locking the console; hardware in later games can detect this situation, so it later became common to install a switch to reconnect the lockout chip as needed.
PAL consoles face another incompatibility when playing out-of-region cartridges: the NTSC video standard specifies video at 60 Hz while PAL operates at 50 Hz, resulting in approximately 16.7% slower gameplay. Additionally, PAL's higher resolution results in letterboxing of the output image. Some commercial PAL region releases exhibit this same problem and, therefore, can be played in NTSC systems without issue while others will face a 20% speedup if played in an NTSC console. To mostly correct this issue, a switch can be added to place the SNES PPU into a 60 Hz mode supported by most newer PAL televisions. Later games will detect this setting and refuse to run, requiring the switch to be thrown only after the check completes.
All versions of the SNES are predominantly gray, although the exact shade may differ. The original North American version, designed by Nintendo of America industrial designer Lance Barr (who previously redesigned the Famicom to become the NES), has a boxy design with purple sliding switches and a dark gray eject lever. The loading bay surface is curved, both to invite interaction and to prevent food or drinks from being placed on the console and spilling as had happened with the flat surfaced NES. The Japanese and European versions are more rounded, with darker gray accents and buttons. The North American SNS-101 model and the Japanese Super Famicom Jr. (the SHVC-101 model), all designed by Barr, are both smaller with a rounded contour; however, the SNS-101 buttons are purple where the Super Famicom Jr. buttons are gray. The European and American versions of the SNES controllers have much longer cables compared to the Japanese Super Famicom controllers.
All versions incorporate a top-loading slot for game cartridges, although the shape of the slot differs between regions to match the different shapes of the cartridges. The MULTI OUT connector (later used on the Nintendo 64 and GameCube) can output composite video, S-Video and RGB signals, as well as RF with an external RF modulator. Original versions additionally include a 28-pin expansion port under a small cover on the bottom of the unit and a standard RF output with channel selection switch on the back; the redesigned models output composite video only, requiring an external modulator for RF.
The ABS plastic used in the casing of some older SNES and Super Famicom consoles is particularly susceptible to oxidization on exposure to air, likely due to an incorrect mixture of the stabilizing or flame retarding additives. This, along with the particularly light color of the original plastic, causes affected consoles to quickly become yellow; if the sections of the casing came from different batches of plastic, a "two-tone" effect results. The color can sometimes be restored with UV light and a hydrogen peroxide solution.
The cartridge media of the console is officially referred to as Game Pak in most Western regions, and as Cassette (カセット, Kasetto?) in Japan and parts of Latin America. While the SNES can address 128 Mbit,[f] only 117.75 Mbit are actually available for cartridge use. A fairly normal mapping could easily address up to 95 Mbit of ROM data (48 Mbit at FastROM speed) with 8 Mbit of battery-backed RAM. However, most available memory access controllers only support mappings of up to 32 Mbit. The largest games released (Tales of Phantasia and Star Ocean) contain 48 Mbit of ROM data, while the smallest games contain only 2 Mbit.
The standard SNES controller adds two additional face buttons (X and Y) to the design of the NES iteration, arranging the four in a diamond shape, and introduces two shoulder buttons. It also features an ergonomic design by Lance Barr, later used for the NES-102 model controllers, also designed by Barr. The Japanese and PAL region versions incorporate the colors of the four action buttons into system's logo. The North American version's buttons are colored to match the redesigned console; the X and Y buttons are lavender with concave faces, and the A and B buttons are purple with convex faces. Several later consoles derive elements of their controller design from the SNES, including the PlayStation, Dreamcast, Xbox, and Wii Classic Controller.
Throughout the course of its life, a number of peripherals were released which added to the functionality of the SNES. Many of these devices were modeled after earlier add-ons for the NES: the Super Scope is a light gun functionally similar to the NES Zapper (though the Super Scope features wireless capabilities) and the Super Advantage is an arcade-style joystick with adjustable turbo settings akin to the NES Advantage. Nintendo also released the SNES Mouse in conjunction with its Mario Paint title. Hudson Soft, under license from Nintendo, released the Super Multitap, a multiplayer adapter for use with its popular series of Bomberman games. Some of the more unusual controllers include the BatterUP baseball bat, the Life Fitness Entertainment System (an exercise bike controller with built-in monitoring software), and the TeeV Golf golf club.
While Nintendo never released an adapter for playing NES games on the SNES (though the instructions included a way to connect both consoles to the same TV by either daisy chaining the RF switches or using AV outputs for one or both systems), the Super Game Boy adapter cartridge allows games designed for Nintendo's portable Game Boy system to be played on the SNES. The Super Game Boy touted several feature enhancements over the Game Boy, including palette substitution, custom screen borders, and (for specially enhanced games) access to the SNES console. Japan also saw the release of the Super Game Boy 2, which added a communication port to enable a second Game Boy to connect for multiplayer games.
Japan saw the release of the Satellaview, a modem which attached to the Super Famicom's expansion port and connected to the St.GIGA satellite radio station. Users of the Satellaview could download gaming news and specially designed games, which were frequently either remakes of or sequels to older Famicom titles, released in installments. Satellaview signals were broadcast from April 23, 1995 through June 30, 2000. In the United States, the similar but relatively short-lived XBAND allowed users to connect to a network via a dial-up modem to compete against other players around the country.
During the SNES's life, Nintendo contracted with two different companies to develop a CD-ROM-based peripheral for the console to compete with Sega's CD-ROM based addon, Mega-CD. Ultimately, deals with both Sony and Philips fell through, (although a prototype console was produced by Sony) with Philips gaining the right to release a series of titles based on Nintendo franchises for its CD-i multimedia player and Sony going on to develop its own console based on its initial dealings with Nintendo (the PlayStation).
Nintendo of America took the same stance against the distribution of SNES ROM image files and the use of emulators as it did with the NES, insisting that they represented flagrant software piracy. Proponents of SNES emulation cite discontinued production of the SNES constituting abandonware status, the right of the owner of the respective game to make a personal backup via devices such as the Retrode, space shifting for private use, the desire to develop homebrew games for the system, the frailty of SNES ROM cartridges and consoles, and the lack of certain foreign imports.
Emulation of the SNES is now available on handheld units, such as Android devices, Apple's iPhone and iPad, Sony's PlayStation Portable (PSP), the Nintendo DS and Game Boy Advance, the Gizmondo, the Dingoo and the GP2X by GamePark Holdings, as well as PDAs. While individual games have been included with emulators on some GameCube discs, Nintendo's Virtual Console service for the Wii marks the introduction of officially sanctioned general SNES emulation, though SNES9x GX, a port of SNES9x, has been made for the Wii.
In 2007, GameTrailers named the SNES as the second-best console of all time in their list of top ten consoles that "left their mark on the history of gaming", citing its graphic, sound, and library of top-quality games. In 2015, they also named it the best Nintendo console of all time, saying, "The list of games we love from this console completely annihilates any other roster from the Big N." Technology columnist Don Reisinger proclaimed "The SNES is the greatest console of all time" in January 2008, citing the quality of the games and the console's dramatic improvement over its predecessor; fellow technology columnist Will Greenwald replied with a more nuanced view, giving the SNES top marks with his heart, the NES with his head, and the PlayStation (for its controller) with his hands. GamingExcellence also gave the SNES first place in 2008, declaring it "simply the most timeless system ever created" with many games that stand the test of time and citing its innovation in controller design, graphics capabilities, and game storytelling. At the same time, GameDaily rated it fifth of ten for its graphics, audio, controllers, and games. In 2009, IGN named the Super Nintendo Entertainment System the fourth best video game console, complimenting its audio and "concentration of AAA titles".
The Seven Years' War was fought between 1755 and 1764, the main conflict occurring in the seven-year period from 1756 to 1763. It involved every great power of the time except the Ottoman Empire, and affected Europe, the Americas, West Africa, India, and the Philippines. Considered a prelude to the two world wars and the greatest European war since the Thirty Years War of the 17th century, it once again split Europe into two coalitions, led by Great Britain on one side and France on the other. For the first time, aiming to curtail Britain and Prussia's ever-growing might, France formed a grand coalition of its own, which ended with failure as Britain rose as the world's predominant power, altering the European balance of power.
Realizing that war was imminent, Prussia preemptively struck Saxony and quickly overran it. The result caused uproar across Europe. Because of Prussia's alliance with Britain, Austria formed an alliance with France, seeing an opportunity to recapture Silesia, which had been lost in a previous war. Reluctantly, by following the imperial diet, most of the states of the empire joined Austria's cause. The Anglo-Prussian alliance was joined by smaller German states (especially Hanover). Sweden, fearing Prussia's expansionist tendencies, went to war in 1757 to protect its Baltic dominions, seeing its chance when virtually all of Europe opposed Prussia. Spain, bound by the Pacte de Famille, intervened on behalf of France and together they launched an utterly unsuccessful invasion of Portugal in 1762. The Russian Empire was originally aligned with Austria, fearing Prussia's ambition on the Polish-Lithuanian Commonwealth, but switched sides upon the succession of Tsar Peter III in 1762.
Many middle and small powers in Europe, unlike in the previous wars, tried to steer clear away from the escalating conflict, even though they had interests in the conflict or with the belligerents, like Denmark-Norway. The Dutch Republic, long-time British ally, kept its neutrality intact, fearing the odds against Britain and Prussia fighting the great powers of Europe, even tried to prevent Britain's domination in India. Naples, Sicily, and Savoy, although sided with Franco-Spanish party, declined to join the coalition under the fear of British power. The taxation needed for war caused the Russian people considerable hardship, being added to the taxation of salt and alcohol begun by Empress Elizabeth in 1759 to complete her addition to the Winter Palace. Like Sweden, Russia concluded a separate peace with Prussia.
The war was successful for Great Britain, which gained the bulk of New France in North America, Spanish Florida, some individual Caribbean islands in the West Indies, the colony of Senegal on the West African coast, and superiority over the French trading outposts on the Indian subcontinent. The Native American tribes were excluded from the settlement; a subsequent conflict, known as Pontiac's War, was also unsuccessful in returning them to their pre-war status. In Europe, the war began disastrously for Prussia, but a combination of good luck and successful strategy saw King Frederick the Great manage to retrieve the Prussian position and retain the status quo ante bellum. Prussia emerged as a new European great power. Although Austria failed to retrieve the territory of Silesia from Prussia (its original goal) its military prowess was also noted by the other powers. The involvement of Portugal, Spain and Sweden did not return them to their former status as great powers. France was deprived of many of its colonies and had saddled itself with heavy war debts that its inefficient financial system could barely handle. Spain lost Florida but gained French Louisiana and regained control of its colonies, e.g., Cuba and the Philippines, which had been captured by the British during the war. France and other European powers avenged their defeat in 1778 when the American Revolutionary War broke out, with hopes of destroying Britain's dominance once and for all.
The war has been described as the first "world war", although this label was also given to various earlier conflicts like the Eighty Years' War, the Thirty Years' War, the War of the Spanish Succession and the War of the Austrian Succession, and to later conflicts like the Napoleonic Wars. The term "Second Hundred Years' War" has been used in order to describe the almost continuous level of world-wide conflict during the entire 18th century, reminiscent of the more famous and compact struggle of the 14th century.
Realizing that war was imminent, Prussia preemptively struck Saxony and quickly overran it. The result caused uproar across Europe. Because of Prussia's alliance with Britain, Austria formed an alliance with France, seeing an opportunity to recapture Silesia, which had been lost in a previous war. Reluctantly, by following the imperial diet, most of the states of the empire joined Austria's cause. The Anglo-Prussian alliance was joined by smaller German states (especially Hanover). Sweden, fearing Prussia's expansionist tendencies, went to war in 1757 to protect its Baltic dominions, seeing its chance when virtually all of Europe opposed Prussia. Spain, bound by the Pacte de Famille, intervened on behalf of France and together they launched a disastrous invasion of Portugal in 1762. The Russian Empire was originally aligned with Austria, fearing Prussia's ambition on the Polish-Lithuanian Commonwealth, but switched sides upon the succession of Tsar Peter III in 1762.
Many middle and small powers in Europe, unlike in the previous wars, tried to steer clear away from the escalating conflict, even though they had interests in the conflict or with the belligerents, like Denmark-Norway. The Dutch Republic, long-time British ally, kept its neutrality intact, fearing the odds against Britain and Prussia fighting the great powers of Europe, even tried to prevent Britain's domination in India. Naples, Sicily, and Savoy, although sided with Franco-Spanish party, declined to join the coalition under the fear of British power. The taxation needed for war caused the Russian people considerable hardship, being added to the taxation of salt and alcohol begun by Empress Elizabeth in 1759 to complete her addition to the Winter Palace. Like Sweden, Russia concluded a separate peace with Prussia.
The war was successful for Great Britain, which gained the bulk of New France in North America, Spanish Florida, some individual Caribbean islands in the West Indies, the colony of Senegal on the West African coast, and superiority over the French trading outposts on the Indian subcontinent. The Native American tribes were excluded from the settlement; a subsequent conflict, known as Pontiac's War, was also unsuccessful in returning them to their pre-war status. In Europe, the war began disastrously for Prussia, but a combination of good luck and successful strategy saw King Frederick the Great manage to retrieve the Prussian position and retain the status quo ante bellum. Prussia emerged as a new European great power. The involvement of Portugal, Spain and Sweden did not return them to their former status as great powers. France was deprived of many of its colonies and had saddled itself with heavy war debts that its inefficient financial system could barely handle. Spain lost Florida but gained French Louisiana and regained control of its colonies, e.g., Cuba and the Philippines, which had been captured by the British during the war. France and other European powers will soon avenge their defeat in 1778 when American Revolutionary War broke out, with hopes of destroying Britain's dominance once and for all.
The War of the Austrian Succession had seen the belligerents aligned on a time-honoured basis. France’s traditional enemies, Great Britain and Austria, had coalesced just as they had done against Louis XIV. Prussia, the leading anti-Austrian state in Germany, had been supported by France. Neither group, however, found much reason to be satisfied with its partnership: British subsidies to Austria had produced nothing of much help to the British, while the British military effort had not saved Silesia for Austria. Prussia, having secured Silesia, had come to terms with Austria in disregard of French interests. Even so, France had concluded a defensive alliance with Prussia in 1747, and the maintenance of the Anglo-Austrian alignment after 1748 was deemed essential by the Duke of Newcastle, British secretary of state in the ministry of his brother Henry Pelham. The collapse of that system and the aligning of France with Austria and of Great Britain with Prussia constituted what is known as the “diplomatic revolution” or the “reversal of alliances.”
In 1756 Austria was making military preparations for war with Prussia and pursuing an alliance with Russia for this purpose. On June 2, 1746, Austria and Russia concluded a defensive alliance that covered their own territory and Poland against attack by Prussia or the Ottoman Empire. They also agreed to a secret clause that promised the restoration of Silesia and the countship of Glatz (now Kłodzko, Poland) to Austria in the event of hostilities with Prussia. Their real desire, however, was to destroy Frederick’s power altogether, reducing his sway to his electorate of Brandenburg and giving East Prussia to Poland, an exchange that would be accompanied by the cession of the Polish Duchy of Courland to Russia. Aleksey Petrovich, Graf (count) Bestuzhev-Ryumin, grand chancellor of Russia under Empress Elizabeth, was hostile to both France and Prussia, but he could not persuade Austrian statesman Wenzel Anton von Kaunitz to commit to offensive designs against Prussia so long as Prussia was able to rely on French support.
The Hanoverian king George II of Great Britain was passionately devoted to his family’s continental holdings, but his commitments in Germany were counterbalanced by the demands of the British colonies overseas. If war against France for colonial expansion was to be resumed, then Hanover had to be secured against Franco-Prussian attack. France was very much interested in colonial expansion and was willing to exploit the vulnerability of Hanover in war against Great Britain, but it had no desire to divert forces to central Europe for Prussia's interest.
French policy was, moreover, complicated by the existence of the le Secret du roi—a system of private diplomacy conducted by King Louis XV. Unbeknownst to his foreign minister, Louis had established a network of agents throughout Europe with the goal of pursuing personal political objectives that were often at odds with France’s publicly stated policies. Louis’s goals for le Secret du roi included an attempt to win the Polish crown for his kinsman Louis François de Bourbon, prince de Conti, and the maintenance of Poland, Sweden, and Turkey as French client states in opposition to Russian and Austrian interests.
Frederick saw Saxony and Polish west Prussia as potential fields for expansion but could not expect French support if he started an aggressive war for them. If he joined the French against the British in the hope of annexing Hanover, he might fall victim to an Austro-Russian attack. The hereditary elector of Saxony, Augustus III, was also elective King of Poland as Augustus III, but the two territories were physically separated by Brandenburg and Silesia. Neither state could pose as a great power. Saxony was merely a buffer between Prussia and Austrian Bohemia, whereas Poland, despite its union with the ancient lands of Lithuania, was prey to pro-French and pro-Russian factions. A Prussian scheme for compensating Frederick Augustus with Bohemia in exchange for Saxony obviously presupposed further spoliation of Austria.
In the attempt to satisfy Austria at the time, Britain gave their electoral vote in Hanover for the candidacy of Maria Theresa's son, Joseph, as the Holy Roman Emperor, much to the dismay of Frederick and Prussia. Not only that, Britain would soon join the Austro-Russian alliance, but complications arose. Britain's basic framework for the alliance itself was to protect Hanover's interests against France. While at the same time, Kaunitz kept approaching the French in the hope of establishing such alliance with Austria. Not only that, France had no intention to ally with Russia, who meddled with their affairs in Austria's succession war, years earlier, and saw the complete dismemberment of Prussia as unacceptable to the stability of Central Europe.
Years later, Kaunitz kept trying to establish France's alliance with Austria. He tried as hard as he could for Austria to not get entangled in Hanover's political affairs, and was even willing to trade Austrian Netherlands for France's aid in recapturing Silesia. Frustrated by this decision and by the Dutch Republic's insistence on neutrality, Britain soon turned to Russia. On September 30, 1755, Britain pledged financial aid to Russia in order to station 50,000 troops on the Livonian-Lithunian border, so they could defend Britain's interests in Hanover immediately. Besthuzev, assuming the preparation was directed against Prussia, was more than happy to obey the request of the British. Unbeknownst to the other powers, King George II also made overtures to the Prussian king; Frederick, who began fearing the Austro-Russian intentions, and was excited to welcome a rapprochement with Britain. On January 16, 1756, the Convention of Westminster was signed wherein Britain and Prussia promised to aid one another in order to achieve lasting peace and stability in Europe.
The carefully coded word in the agreement proved no less catalytic for the other European powers. The results were absolute chaos. Empress Elizabeth of Russia was outraged at the duplicity of Britain's position. Not only that France was so enraged, and terrified, by the sudden betrayal of its only ally. Austria, particularly Kaunitz, used this situation to their utmost advantage. The now-isolated France was forced to accede to the Austro-Russian alliance or face ruin. Thereafter, on May 1, 1756, the First Treaty of Versailles was signed, in which both nations pledged 24.000 troops to defend each other in the case of an attack. This diplomatic revolution proved to be an important cause of the war; although both treaties were self-defensive in nature, the actions of both coalitions made the war virtually inevitable.
The most important French fort planned was intended to occupy a position at "the Forks" where the Allegheny and Monongahela Rivers meet to form the Ohio River (present day Pittsburgh, Pennsylvania). Peaceful British attempts to halt this fort construction were unsuccessful, and the French proceeded to build the fort they named Fort Duquesne. British colonial militia from Virginia were then sent to drive them out. Led by George Washington, they ambushed a small French force at Jumonville Glen on 28 May 1754 killing ten, including commander Jumonville. The French retaliated by attacking Washington's army at Fort Necessity on 3 July 1754 and forced Washington to surrender.
News of this arrived in Europe, where Britain and France unsuccessfully attempted to negotiate a solution. The two nations eventually dispatched regular troops to North America to enforce their claims. The first British action was the assault on Acadia on 16 June 1755 in the Battle of Fort Beauséjour, which was immediately followed by their expulsion of the Acadians. In July British Major General Edward Braddock led about 2,000 army troops and provincial militia on an expedition to retake Fort Duquesne, but the expedition ended in disastrous defeat. In further action, Admiral Edward Boscawen fired on the French ship Alcide on 8 June 1755, capturing it and two troop ships. In September 1755, French and British troops met in the inconclusive Battle of Lake George.
For much of the eighteenth century, France approached its wars in the same way. It would let colonies defend themselves or would offer only minimal help (sending them limited numbers of troops or inexperienced soldiers), anticipating that fights for the colonies would most likely be lost anyway. This strategy was to a degree forced upon France: geography, coupled with the superiority of the British navy, made it difficult for the French navy to provide significant supplies and support to French colonies. Similarly, several long land borders made an effective domestic army imperative for any French ruler. Given these military necessities, the French government, unsurprisingly, based its strategy overwhelmingly on the army in Europe: it would keep most of its army on the continent, hoping for victories closer to home. The plan was to fight to the end of hostilities and then, in treaty negotiations, to trade territorial acquisitions in Europe to regain lost overseas possessions. This approach did not serve France well in the war, as the colonies were indeed lost, but although much of the European war went well, by its end France had few counterbalancing European successes.
The British—by inclination as well as for practical reasons—had tended to avoid large-scale commitments of troops on the Continent. They sought to offset the disadvantage of this in Europe by allying themselves with one or more Continental powers whose interests were antithetical to those of their enemies, particularly France.:15–16 By subsidising the armies of continental allies, Britain could turn London's enormous financial power to military advantage. In the Seven Years' War, the British chose as their principal partner the greatest general of the day, Frederick the Great of Prussia, then the rising power in central Europe, and paid Frederick substantial subsidies for his campaigns.:106 This was accomplished in the Diplomatic Revolution of 1756, in which Britain ended its long-standing alliance with Austria in favor of Prussia, leaving Austria to side with France. In marked contrast to France, Britain strove to prosecute the war actively in the colonies, taking full advantage of its naval power. :64–66 The British pursued a dual strategy – naval blockade and bombardment of enemy ports, and rapid movement of troops by sea. They harassed enemy shipping and attacked enemy colonies, frequently using colonists from nearby British colonies in the effort.
William Pitt, who entered the cabinet in 1756, had a grand vision for the war that made it entirely different from previous wars with France. As prime minister Pitt committed Britain to a grand strategy of seizing the entire French Empire, especially its possessions in North America and India. Britain's main weapon was the Royal Navy, which could control the seas and bring as many invasion troops as were needed. He also planned to use colonial forces from the Thirteen American colonies, working under the command of British regulars, to invade new France. In order to tie the French army down he subsidized his European allies. Pitt Head of the government from 1756 to 1761, and even after that the British continued his strategy. It proved completely successful. Pitt had a clear appreciation of the enormous value of imperial possessions, and realized how vulnerable was the French Empire.
The British Prime Minister, the Duke of Newcastle, was optimistic that the new series of alliances could prevent war from breaking out in Europe. However, a large French force was assembled at Toulon, and the French opened the campaign against the British by an attack on Minorca in the Mediterranean. A British attempt at relief was foiled at the Battle of Minorca, and the island was captured on 28 June (for which Admiral Byng was court-martialed and executed). War between Britain and France had been formally declared on 18 May nearly two years after fighting had broken out in the Ohio Country.
Frederick II of Prussia had received reports of the clashes in North America and had formed an alliance with Great Britain. On 29 August 1756, he led Prussian troops across the border of Saxony, one of the small German states in league with Austria. He intended this as a bold pre-emption of an anticipated Austro-French invasion of Silesia. He had three goals in his new war on Austria. First, he would seize Saxony and eliminate it as a threat to Prussia, then using the Saxon army and treasury to aid the Prussian war effort. His second goal was to advance into Bohemia where he might set up winter quarters at Austria's expense. Thirdly, he wanted to invade Moravia from Silesia, seize the fortress at Olmütz, and advance on Vienna to force an end to the war.
Accordingly, leaving Field Marshal Count Kurt von Schwerin in Silesia with 25,000 soldiers to guard against incursions from Moravia or Hungary, and leaving Field Marshal Hans von Lehwaldt in East Prussia to guard against Russian invasion from the east, Frederick set off with his army for Saxony. The Prussian army marched in three columns. On the right was a column of about 15,000 men under the command of Prince Ferdinand of Brunswick. On the left was a column of 18,000 men under the command of the Duke of Brunswick-Bevern. In the centre was Frederick II, himself with Field Marshal James Keith commanding a corps of 30,000 troops. Ferdinand of Brunswick was to close in on the town of Chemnitz. The Duke of Brunswick-Bevern was to traverse Lusatia to close in on Bautzen. Meanwhile, Frederick and Field Marshal Keith would make for Dresden.
The Saxon and Austrian armies were unprepared, and their forces were scattered. Frederick occupied Dresden with little or no opposition from the Saxons. At the Battle of Lobositz on 1 October 1756, Frederick prevented the isolated Saxon army from being reinforced by an Austrian army under General Browne. The Prussians then occupied Saxony; after the Siege of Pirna, the Saxon army surrendered in October 1756, and was forcibly incorporated into the Prussian army. The attack on neutral Saxony caused outrage across Europe and led to the strengthening of the anti-Prussian coalition. The only significant Austrian success was the partial occupation of Silesia. Far from being easy, Frederick's early successes proved indecisive and very costly for Prussia's smaller army. This led him to remark that he did not fight the same Austrians as he had during the previous war.
Britain had been surprised by the sudden Prussian offensive but now began shipping supplies and ₤670,000 (equivalent to ₤89.9 million in 2015) to its new ally. A combined force of allied German states was organised by the British to protect Hanover from French invasion, under the command of the Duke of Cumberland. The British attempted to persuade the Dutch Republic to join the alliance, but the request was rejected, as the Dutch wished to remain fully neutral. Despite the huge disparity in numbers, the year had been successful for the Prussian-led forces on the continent, in contrast to disappointing British campaigns in North America.
In early 1757, Frederick II again took the initiative by marching into the Kingdom of Bohemia, hoping to inflict a decisive defeat on Austrian forces. After winning the bloody Battle of Prague on 6 May 1757, in which both forces suffered major casualties, the Prussians forced the Austrians back into the fortifications of Prague. The Prussian army then laid siege to the city. Following the battle at Prague, Frederick took 5,000 troops from the siege at Prague and sent them to reinforce the 19,000-man army under the Duke of Brunswick-Bevern at Kolin in Bohemia. Austrian Marshal Daun arrived too late to participate in the battle of Prague, but picked up 16,000 men who had escaped from the battle. With this army he slowly moved to relieve Prague. The Prussian army was too weak to simultaneously besiege Prague and keep Daun away, and Frederick was forced to attack prepared positions. The resulting Battle of Kolin was a sharp defeat for Frederick, his first military defeat. His losses further forced him to lift the siege and withdraw from Bohemia altogether.
Later that summer, the Russians invaded Memel with 75,000 troops. Memel had one of the strongest fortresses in Prussia. However, after five days of artillery bombardment the Russian army was able to storm it. The Russians then used Memel as a base to invade East Prussia and defeated a smaller Prussian force in the fiercely contested Battle of Gross-Jägersdorf on 30 August 1757. However, it was not yet able to take Königsberg and retreated soon afterward. Still, it was a new threat to Prussia. Not only was Frederick forced to break off his invasion of Bohemia, he was now forced to withdraw further into Prussian-controlled territory. His defeats on the battlefield brought still more opportunist nations into the war. Sweden declared war on Prussia and invaded Pomerania with 17,000 men. Sweden felt this small army was all that was needed to occupy Pomerania and felt the Swedish army would not need to engage with the Prussians because the Prussians were occupied on so many other fronts.
Things were looking grim for Prussia now, with the Austrians mobilising to attack Prussian-controlled soil and a French army under Soubise approaching from the west. However, in November and December of 1757, the whole situation in Germany was reversed. First, Frederick devastated Prince Soubise's French force at the Battle of Rossbach on 5 November 1757 and then routed a vastly superior Austrian force at the Battle of Leuthen on 5 December 1757 With these victories, Frederick once again established himself as Europe's premier general and his men as Europe's most accomplished soldiers. In spite of this, the Prussians were now facing the prospect of four major powers attacking on four fronts (France from the West, Austria from the South, Russia from the East and Sweden from the North). Meanwhile, a combined force from a number of smaller German states such as Bavaria had been established under Austrian leadership, thus threatening Prussian control of Saxony.
This problem was compounded when the main Hanoverian army under Cumberland was defeated at the Battle of Hastenbeck and forced to surrender entirely at the Convention of Klosterzeven following a French Invasion of Hanover. The Convention removed Hanover and Brunswick from the war, leaving the Western approach to Prussian territory extremely vulnerable. Frederick sent urgent requests to Britain for more substantial assistance, as he was now without any outside military support for his forces in Germany.
Calculating that no further Russian advance was likely until 1758, Frederick moved the bulk of his eastern forces to Pomerania under the command of Marshal Lehwaldt where they were to repel the Swedish invasion. In short order, the Prussian army drove the Swedes back, occupied most of Swedish Pomerania, and blockaded its capital Stralsund. George II of Great Britain, on the advice of his British ministers, revoked the Convention of Klosterzeven, and Hanover reentered the war. Over the winter the new commander of the Hanoverian forces, Duke Ferdinand of Brunswick, regrouped his army and launched a series of offensives that drove the French back across the River Rhine. The British had suffered further defeats in North America, particularly at Fort William Henry. At home, however, stability had been established. Since 1756, successive governments led by Newcastle and Pitt had fallen. In August 1757, the two men agreed to a political partnership and formed a coalition government that gave new, firmer direction to the war effort. The new strategy emphasised both Newcastle's commitment to British involvement on the Continent, particularly in defence of Germany, and William Pitt's determination to use naval power to seize French colonies around the globe. This "dual strategy" would dominate British policy for the next five years.
Between 10 and 17 October 1757, a Hungarian general, Count András Hadik, serving in the Austrian army, executed what may be the most famous hussar action in history. When the Prussian King Frederick was marching south with his powerful armies, the Hungarian general unexpectedly swung his force of 5,000, mostly hussars, around the Prussians and occupied part of their capital, Berlin, for one night. The city was spared for a negotiated ransom of 200,000 thalers. When Frederick heard about this humiliating occupation, he immediately sent a larger force to free the city. Hadik, however, left the city with his Hussars and safely reached the Austrian lines. Subsequently, Hadik was promoted to the rank of Marshal in the Austrian army.
In early 1758, Frederick launched an invasion of Moravia, and laid siege to Olmütz (now Olomouc, Czech Republic). Following an Austrian victory at the Battle of Domstadtl that wiped out a supply convoy destined for Olmütz, Frederick broke off the siege and withdrew from Moravia. It marked the end of his final attempt to launch a major invasion of Austrian territory. East Prussia had been occupied by Russian forces over the winter and would remain under their control until 1762, although Frederick did not see the Russians as an immediate threat and instead entertained hopes of first fighting a decisive battle against Austria that would knock them out of the war.
In April 1758, the British concluded the Anglo-Prussian Convention with Frederick in which they committed to pay him an annual subsidy of £670,000. Britain also dispatched 9,000 troops to reinforce Ferdinand's Hanoverian army, the first British troop commitment on the continent and a reversal in the policy of Pitt. Ferdinand had succeeded in driving the French from Hanover and Westphalia and re-captured the port of Emden in March 1758 before crossing the Rhine with his own forces, which caused alarm in France. Despite Ferdinand's victory over the French at the Battle of Krefeld and the brief occupation of Düsseldorf, he was compelled by the successful manoeuvering of larger French forces to withdraw across the Rhine.
By this point Frederick was increasingly concerned by the Russian advance from the east and marched to counter it. Just east of the Oder in Brandenburg-Neumark, at the Battle of Zorndorf (now Sarbinowo, Poland), a Prussian army of 35,000 men under Frederick on Aug. 25, 1758, fought a Russian army of 43,000 commanded by Count William Fermor. Both sides suffered heavy casualties – the Prussians 12,800, the Russians 18,000 – but the Russians withdrew, and Frederick claimed victory. In the undecided Battle of Tornow on 25 September, a Swedish army repulsed six assaults by a Prussian army but did not push on Berlin following the Battle of Fehrbellin.
The war was continuing indecisively when on 14 October Marshal Daun's Austrians surprised the main Prussian army at the Battle of Hochkirch in Saxony. Frederick lost much of his artillery but retreated in good order, helped by dense woods. The Austrians had ultimately made little progress in the campaign in Saxony despite Hochkirch and had failed to achieve a decisive breakthrough. After a thwarted attempt to take Dresden, Daun's troops were forced to withdraw to Austrian territory for the winter, so that Saxony remained under Prussian occupation. At the same time, the Russians failed in an attempt to take Kolberg in Pomerania (now Kołobrzeg, Poland) from the Prussians.
The year 1759 saw several Prussian defeats. At the Battle of Kay, or Paltzig, the Russian Count Saltykov with 47,000 Russians defeated 26,000 Prussians commanded by General Carl Heinrich von Wedel. Though the Hanoverians defeated an army of 60,000 French at Minden, Austrian general Daun forced the surrender of an entire Prussian corps of 13,000 in the Battle of Maxen. Frederick himself lost half his army in the Battle of Kunersdorf (now Kunowice Poland), the worst defeat in his military career and one that drove him to the brink of abdication and thoughts of suicide. The disaster resulted partly from his misjudgment of the Russians, who had already demonstrated their strength at Zorndorf and at Gross-Jägersdorf (now Motornoye, Russia), and partly from good cooperation between the Russian and Austrian forces.
The French planned to invade the British Isles during 1759 by accumulating troops near the mouth of the Loire and concentrating their Brest and Toulon fleets. However, two sea defeats prevented this. In August, the Mediterranean fleet under Jean-François de La Clue-Sabran was scattered by a larger British fleet under Edward Boscawen at the Battle of Lagos. In the Battle of Quiberon Bay on 20 November, the British admiral Edward Hawke with 23 ships of the line caught the French Brest fleet with 21 ships of the line under Marshal de Conflans and sank, captured, or forced many of them aground, putting an end to the French plans.
Despite this, the Austrians, under the command of General Laudon, captured Glatz (now Kłodzko, Poland) in Silesia. In the Battle of Liegnitz Frederick scored a strong victory despite being outnumbered three to one. The Russians under General Saltykov and Austrians under General Lacy briefly occupied his capital, Berlin, in October, but could not hold it for long. The end of that year saw Frederick once more victorious, defeating the able Daun in the Battle of Torgau; but he suffered very heavy casualties, and the Austrians retreated in good order.
1762 brought two new countries into the war. Britain declared war against Spain on 4 January 1762; Spain reacted by issuing their own declaration of war against Britain on 18 January. Portugal followed by joining the war on Britain's side. Spain, aided by the French, launched an invasion of Portugal and succeeded in capturing Almeida. The arrival of British reinforcements stalled a further Spanish advance, and the Battle of Valencia de Alcántara saw British-Portuguese forces overrun a major Spanish supply base. The invaders were stopped on the heights in front of Abrantes (called the pass to Lisbon) where the Anglo-Portuguese were entrenched. Eventually the Anglo-Portuguese army, aided by guerrillas and practicing a scorched earth strategy, chased the greatly reduced Franco-Spanish army back to Spain, recovering almost all the lost towns, among them the Spanish headquarters in Castelo Branco full of wounded and sick that had been left behind.
On the eastern front, progress was very slow. The Russian army was heavily dependent upon its main magazines in Poland, and the Prussian army launched several successful raids against them. One of them, led by general Platen in September resulted in the loss of 2,000 Russians, mostly captured, and the destruction of 5,000 wagons. Deprived of men, the Prussians had to resort to this new sort of warfare, raiding, to delay the advance of their enemies. Nonetheless, at the end of the year, they suffered two critical setbacks. The Russians under Zakhar Chernyshev and Pyotr Rumyantsev stormed Kolberg in Pomerania, while the Austrians captured Schweidnitz. The loss of Kolberg cost Prussia its last port on the Baltic Sea. In Britain, it was speculated that a total Prussian collapse was now imminent.
Britain now threatened to withdraw its subsidies if Prussia didn't consider offering concessions to secure peace. As the Prussian armies had dwindled to just 60,000 men and with Berlin itself under siege, Frederick's survival was severely threatened. Then on 5 January 1762 the Russian Empress Elizabeth died. Her Prussophile successor, Peter III, at once recalled Russian armies from Berlin (see: the Treaty of Saint Petersburg (1762)) and mediated Frederick's truce with Sweden. He also placed a corps of his own troops under Frederick's command This turn of events has become known as the Miracle of the House of Brandenburg. Frederick was then able to muster a larger army of 120,000 men and concentrate it against Austria. He drove them from much of Saxony, while his brother Henry won a victory in Silesia in the Battle of Freiberg (29 October 1762). At the same time, his Brunswick allies captured the key town of Göttingen and compounded this by taking Cassel.
By 1763, the war in Central Europe was essentially a stalemate. Frederick had retaken most of Silesia and Saxony but not the latter's capital, Dresden. His financial situation was not dire, but his kingdom was devastated and his army severely weakened. His manpower had dramatically decreased, and he had lost so many effective officers and generals that a new offensive was perhaps impossible. British subsidies had been stopped by the new Prime Minister Lord Bute, and the Russian Emperor had been overthrown by his wife, Catherine, who ended Russia's alliance with Prussia and withdrew from the war. Austria, however, like most participants, was facing a severe financial crisis and had to decrease the size of its army, something which greatly affected its offensive power. Indeed, after having effectively sustained a long war, its administration was in disarray. By that time, it still held Dresden, the southeastern parts of Saxony, the county of Glatz, and southern Silesia, but the prospect of victory was dim without Russian support. In 1763 a peace settlement was reached at the Treaty of Hubertusburg, ending the war in central Europe.
Despite the debatable strategic success and the operational failure of the descent on Rochefort, William Pitt—who saw purpose in this type of asymmetric enterprise—prepared to continue such operations. An army was assembled under the command of Charles Spencer, 3rd Duke of Marlborough; he was aided by Lord George Sackville. The naval squadron and transports for the expedition were commanded by Richard Howe. The army landed on 5 June 1758 at Cancalle Bay, proceeded to St. Malo, and, finding that it would take prolonged siege to capture it, instead attacked the nearby port of St. Servan. It burned shipping in the harbor, roughly 80 French privateers and merchantmen, as well as four warships which were under construction. The force then re-embarked under threat of the arrival of French relief forces. An attack on Havre de Grace was called off, and the fleet sailed on to Cherbourg; the weather being bad and provisions low, that too was abandoned, and the expedition returned having damaged French privateering and provided further strategic demonstration against the French coast.
Pitt now prepared to send troops into Germany; and both Marlborough and Sackville, disgusted by what they perceived as the futility of the "descents", obtained commissions in that army. The elderly General Bligh was appointed to command a new "descent", escorted by Howe. The campaign began propitiously with the Raid on Cherbourg. Covered by naval bombardment, the army drove off the French force detailed to oppose their landing, captured Cherbourg, and destroyed its fortifications, docks, and shipping.
The troops were reembarked and moved to the Bay of St. Lunaire in Brittany where, on 3 September, they were landed to operate against St. Malo; however, this action proved impractical. Worsening weather forced the two armies to separate: the ships sailed for the safer anchorage of St. Cast, while the army proceeded overland. The tardiness of Bligh in moving his forces allowed a French force of 10,000 from Brest to catch up with him and open fire on the reembarkation troops. A rear-guard of 1,400 under General Dury held off the French while the rest of the army embarked. They could not be saved; 750, including Dury, were killed and the rest captured.
Great Britain lost Minorca in the Mediterranean to the French in 1756 but captured the French colonies in Senegal in 1758. The British Royal Navy took the French sugar colonies of Guadeloupe in 1759 and Martinique in 1762 as well as the Spanish cities of Havana in Cuba, and Manila in the Philippines, both prominent Spanish colonial cities. However, expansion into the hinterlands of both cities met with stiff resistance. In the Philippines, the British were confined to Manila until their agreed upon withdrawal at the war's end.
During the war, the Seven Nations of Canada were allied with the French. These were Native Americans of the Laurentian valley—the Algonquin, the Abenaki, the Huron, and others. Although the Algonquin tribes and the Seven Nations were not directly concerned with the fate of the Ohio River Valley, they had been victims of the Iroquois Confederation. The Iroquois had encroached on Algonquin territory and pushed the Algonquins west beyond Lake Michigan. Therefore, the Algonquin and the Seven Nations were interested in fighting against the Iroquois. Throughout New England, New York, and the North-west Native American tribes formed differing alliances with the major belligerents. The Iroquois, dominant in what is now Upstate New York, sided with the British but did not play a large role in the war.
In 1756 and 1757 the French captured forts Oswego and William Henry from the British. The latter victory was marred when France's native allies broke the terms of capitulation and attacked the retreating British column, which was under French guard, slaughtering and scalping soldiers and taking captive many men, women and children while the French refused to protect their captives. French naval deployments in 1757 also successfully defended the key Fortress of Louisbourg on Cape Breton Island, securing the seaward approaches to Quebec.
British Prime Minister William Pitt's focus on the colonies for the 1758 campaign paid off with the taking of Louisbourg after French reinforcements were blocked by British naval victory in the Battle of Cartagena and in the successful capture of Fort Duquesne and Fort Frontenac. The British also continued the process of deporting the Acadian population with a wave of major operations against Île Saint-Jean (present-day Prince Edward Island), the St. John River valley, and the Petitcodiac River valley. The celebration of these successes was dampened by their embarrassing defeat in the Battle of Carillon (Ticonderoga), in which 4,000 French troops repulsed 16,000 British.
All of Britain's campaigns against New France succeeded in 1759, part of what became known as an Annus Mirabilis. Fort Niagara and Fort Carillon on 8 July 1758 fell to sizable British forces, cutting off French frontier forts further west. On 13 September 1759, following a three-month siege of Quebec, General James Wolfe defeated the French on the Plains of Abraham outside the city. The French staged a counteroffensive in the spring of 1760, with initial success at the Battle of Sainte-Foy, but they were unable to retake Quebec, due to British naval superiority following the battle of Neuville. The French forces retreated to Montreal, where on 8 September they surrendered to overwhelming British numerical superiority.
In 1762, towards the end of the war, French forces attacked St. John's, Newfoundland. If successful, the expedition would have strengthened France's hand at the negotiating table. Although they took St. John's and raided nearby settlements, the French forces were eventually defeated by British troops at the Battle of Signal Hill. This was the final battle of the war in North America, and it forced the French to surrender to Lieutenant Colonel William Amherst. The victorious British now controlled all of eastern North America.
The history of the Seven Years' War in North America, particularly the expulsion of the Acadians, the siege of Quebec, the death of Wolfe, and the Battle of Fort William Henry generated a vast number of ballads, broadsides, images, and novels (see Longfellow's Evangeline, Benjamin West's The Death of General Wolfe, James Fenimore Cooper's The Last of the Mohicans), maps and other printed materials, which testify to how this event held the imagination of the British and North American public long after Wolfe's death in 1759.
The Anglo-French hostilities were ended in 1763 by the Treaty of Paris, which involved a complex series of land exchanges, the most important being France's cession to Spain of Louisiana, and to Great Britain the rest of New France except for the islands of St. Pierre and Miquelon. Faced with the choice of retrieving either New France or its Caribbean island colonies of Guadeloupe and Martinique, France chose the latter to retain these lucrative sources of sugar, writing off New France as an unproductive, costly territory. France also returned Minorca to the British. Spain lost control of Florida to Great Britain, but it received from the French the Île d'Orléans and all of the former French holdings west of the Mississippi River. The exchanges suited the British as well, as their own Caribbean islands already supplied ample sugar, and, with the acquisition of New France and Florida, they now controlled all of North America east of the Mississippi.
In India, the British retained the Northern Circars, but returned all the French trading ports. The treaty, however, required that the fortifications of these settlements be destroyed and never rebuilt, while only minimal garrisons could be maintained there, thus rendering them worthless as military bases. Combined with the loss of France's ally in Bengal and the defection of Hyderabad to the British as a result of the war, this effectively brought French power in India to an end, making way for British hegemony and eventual control of the subcontinent.
The Treaty of Hubertusburg, between Austria, Prussia, and Saxony, was signed on February 15, 1763, at a hunting lodge between Dresden and Leipzig. Negotiations had started there on December 31, 1762. Frederick, who had considered ceding East Prussia to Russia if Peter III helped him secure Saxony, finally insisted on excluding Russia (in fact, no longer a belligerent) from the negotiations. At the same time, he refused to evacuate Saxony until its elector had renounced any claim to reparation. The Austrians wanted at least to retain Glatz, which they had in fact reconquered, but Frederick would not allow it. The treaty simply restored the status quo of 1748, with Silesia and Glatz reverting to Frederick and Saxony to its own elector. The only concession that Prussia made to Austria was to consent to the election of Archduke Joseph as Holy Roman emperor.
Austria was not able to retake Silesia or make any significant territorial gain. However, it did prevent Prussia from invading parts of Saxony. More significantly, its military performance proved far better than during the War of the Austrian Succession and seemed to vindicate Maria Theresa's administrative and military reforms. Hence, Austria's prestige was restored in great part and the empire secured its position as a major player in the European system. Also, by promising to vote for Joseph II in the Imperial elections, Frederick II accepted the Habsburg preeminence in the Holy Roman Empire. The survival of Prussia as a first-rate power and the enhanced prestige of its king and its army, however, was potentially damaging in the long run to Austria's influence in Germany.
Not only that, Austria now found herself estranged with the new developments within the empire itself. Beside the rise of Prussia, Augustus III, although ineffective, could mustered up an army not only from Saxony, but also Poland, considering the elector was also the King of Poland. Bavaria's growing power and independence was also apparent as she had more voices on the path that its army should have taken, and managed to slip out of the war at its own will. Most importantly, with the now somehow-belligerent Hanover united personally under George III of Great Britain, It can amassed a considerable power, even brought Britain in, on the future conflicts. This power dynamic is important to the future and the latter conflicts of the empire. The war also proved that Maria Theresa's reforms were still not enough to compete with Prussia: unlike its enemy, the Austrians went almost bankrupt at the end of war. Hence, she dedicated the next two decades to the consolidation of her administration.
Prussia emerged from the war as a great power whose importance could no longer be challenged. Frederick the Great’s personal reputation was enormously enhanced, as his debt to fortune (Russia’s volte-face after Elizabeth’s death) and to the British subsidy were soon forgotten while the memory of his energy and his military genius was strenuously kept alive. Russia, on the other hand, made one great invisible gain from the war: the elimination of French influence in Poland. The First Partition of Poland (1772) was to be a Russo-Prussian transaction, with Austria only reluctantly involved and with France simply ignored.
The British government was close to bankruptcy, and Britain now faced the delicate task of pacifying its new French-Canadian subjects as well as the many American Indian tribes who had supported France. George III's Proclamation of 1763, which forbade white settlement beyond the crest of the Appalachians, was intended to appease the latter but led to considerable outrage in the Thirteen Colonies, whose inhabitants were eager to acquire native lands. The Quebec Act of 1774, similarly intended to win over the loyalty of French Canadians, also spurred resentment among American colonists. The act protected Catholic religion and French language, which enraged the Americans, but the Québécois remained loyal and did not rebel.
The war had also brought to an end the "Old System" of alliances in Europe, In the years after the war, under the direction of Lord Sandwich, the British did try to re-establish this system. But after her surprising grand success against a coalition of great powers, European states such as Austria, The Dutch Republic, Sweden, Denmark-Norway, Ottoman Empire, and Russia now saw Britain as a greater threat than France and did not join them, while the Prussians were angered by what they considered a British betrayal in 1762. Consequently, when the American War of Independence turned into a global war between 1778–83, Britain found itself opposed by a strong coalition of European powers, and lacking any substantial ally.
The British Empire comprised the dominions, colonies, protectorates, mandates and other territories ruled or administered by the United Kingdom. It originated with the overseas possessions and trading posts established by England between the late 16th and early 18th centuries. At its height, it was the largest empire in history and, for over a century, was the foremost global power. By 1922 the British Empire held sway over about 458 million people, one-fifth of the world's population at the time, and covered more than 13,000,000 sq mi (33,670,000 km2), almost a quarter of the Earth's total land area. As a result, its political, legal, linguistic and cultural legacy is widespread. At the peak of its power, the phrase "the empire on which the sun never sets" was often used to describe the British Empire, because its expanse around the globe meant that the sun was always shining on at least one of its territories.
During the Age of Discovery in the 15th and 16th centuries, Portugal and Spain pioneered European exploration of the globe, and in the process established large overseas empires. Envious of the great wealth these empires generated, England, France, and the Netherlands began to establish colonies and trade networks of their own in the Americas and Asia. A series of wars in the 17th and 18th centuries with the Netherlands and France left England (and then, following union between England and Scotland in 1707, Great Britain) the dominant colonial power in North America and India.
The independence of the Thirteen Colonies in North America in 1783 after the American War of Independence caused Britain to lose some of its oldest and most populous colonies. British attention soon turned towards Asia, Africa, and the Pacific. After the defeat of France in the Revolutionary and Napoleonic Wars (1792–1815), Britain emerged as the principal naval and imperial power of the 19th century (with London the largest city in the world from about 1830). Unchallenged at sea, British dominance was later described as Pax Britannica ("British Peace"), a period of relative peace in Europe and the world (1815–1914) during which the British Empire became the global hegemon and adopted the role of global policeman. In the early 19th century, the Industrial Revolution began to transform Britain; by the time of the Great Exhibition in 1851 the country was described as the "workshop of the world". The British Empire expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During this century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia, and New Zealand became self-governing dominions.
By the start of the 20th century, Germany and the United States challenged Britain's economic lead. Subsequent military and economic tensions between Britain and Germany were major causes of the First World War, during which Britain relied heavily upon its empire. The conflict placed enormous strain on the military, financial and manpower resources of Britain. Although the British Empire achieved its largest territorial extent immediately after World War I, Britain was no longer the world's pre-eminent industrial or military power. In the Second World War, Britain's colonies in South-East Asia were occupied by Imperial Japan. Despite the final victory of Britain and its allies, the damage to British prestige helped to accelerate the decline of the empire. India, Britain's most valuable and populous possession, achieved independence as part of a larger decolonisation movement in which Britain granted independence to most territories of the Empire. The transfer of Hong Kong to China in 1997 marked for many the end of the British Empire. Fourteen overseas territories remain under British sovereignty. After independence, many former British colonies joined the Commonwealth of Nations, a free association of independent states. The United Kingdom is now one of 16 Commonwealth nations, a grouping known informally as the Commonwealth realms, that share one monarch—Queen Elizabeth II.
The foundations of the British Empire were laid when England and Scotland were separate kingdoms. In 1496 King Henry VII of England, following the successes of Spain and Portugal in overseas exploration, commissioned John Cabot to lead a voyage to discover a route to Asia via the North Atlantic. Cabot sailed in 1497, five years after the European discovery of America, and although he successfully made landfall on the coast of Newfoundland (mistakenly believing, like Christopher Columbus, that he had reached Asia), there was no attempt to found a colony. Cabot led another voyage to the Americas the following year but nothing was heard of his ships again.
No further attempts to establish English colonies in the Americas were made until well into the reign of Queen Elizabeth I, during the last decades of the 16th century. In the meantime the Protestant Reformation had turned England and Catholic Spain into implacable enemies . In 1562, the English Crown encouraged the privateers John Hawkins and Francis Drake to engage in slave-raiding attacks against Spanish and Portuguese ships off the coast of West Africa with the aim of breaking into the Atlantic trade system. This effort was rebuffed and later, as the Anglo-Spanish Wars intensified, Elizabeth I gave her blessing to further privateering raids against Spanish ports in the Americas and shipping that was returning across the Atlantic, laden with treasure from the New World. At the same time, influential writers such as Richard Hakluyt and John Dee (who was the first to use the term "British Empire") were beginning to press for the establishment of England's own empire. By this time, Spain had become the dominant power in the Americas and was exploring the Pacific ocean, Portugal had established trading posts and forts from the coasts of Africa and Brazil to China, and France had begun to settle the Saint Lawrence River area, later to become New France.
In 1578, Elizabeth I granted a patent to Humphrey Gilbert for discovery and overseas exploration. That year, Gilbert sailed for the West Indies with the intention of engaging in piracy and establishing a colony in North America, but the expedition was aborted before it had crossed the Atlantic. In 1583 he embarked on a second attempt, on this occasion to the island of Newfoundland whose harbour he formally claimed for England, although no settlers were left behind. Gilbert did not survive the return journey to England, and was succeeded by his half-brother, Walter Raleigh, who was granted his own patent by Elizabeth in 1584. Later that year, Raleigh founded the colony of Roanoke on the coast of present-day North Carolina, but lack of supplies caused the colony to fail.
In 1603, James VI, King of Scots, ascended (as James I) to the English throne and in 1604 negotiated the Treaty of London, ending hostilities with Spain. Now at peace with its main rival, English attention shifted from preying on other nations' colonial infrastructures to the business of establishing its own overseas colonies. The British Empire began to take shape during the early 17th century, with the English settlement of North America and the smaller islands of the Caribbean, and the establishment of private companies, most notably the English East India Company, to administer colonies and overseas trade. This period, until the loss of the Thirteen Colonies after the American War of Independence towards the end of the 18th century, has subsequently been referred to by some historians as the "First British Empire".
The Caribbean initially provided England's most important and lucrative colonies, but not before several attempts at colonisation failed. An attempt to establish a colony in Guiana in 1604 lasted only two years, and failed in its main objective to find gold deposits. Colonies in St Lucia (1605) and Grenada (1609) also rapidly folded, but settlements were successfully established in St. Kitts (1624), Barbados (1627) and Nevis (1628). The colonies soon adopted the system of sugar plantations successfully used by the Portuguese in Brazil, which depended on slave labour, and—at first—Dutch ships, to sell the slaves and buy the sugar. To ensure that the increasingly healthy profits of this trade remained in English hands, Parliament decreed in 1651 that only English ships would be able to ply their trade in English colonies. This led to hostilities with the United Dutch Provinces—a series of Anglo-Dutch Wars—which would eventually strengthen England's position in the Americas at the expense of the Dutch. In 1655, England annexed the island of Jamaica from the Spanish, and in 1666 succeeded in colonising the Bahamas.
England's first permanent settlement in the Americas was founded in 1607 in Jamestown, led by Captain John Smith and managed by the Virginia Company. Bermuda was settled and claimed by England as a result of the 1609 shipwreck there of the Virginia Company's flagship, and in 1615 was turned over to the newly formed Somers Isles Company. The Virginia Company's charter was revoked in 1624 and direct control of Virginia was assumed by the crown, thereby founding the Colony of Virginia. The London and Bristol Company was created in 1610 with the aim of creating a permanent settlement on Newfoundland, but was largely unsuccessful. In 1620, Plymouth was founded as a haven for puritan religious separatists, later known as the Pilgrims. Fleeing from religious persecution would become the motive of many English would-be colonists to risk the arduous trans-Atlantic voyage: Maryland was founded as a haven for Roman Catholics (1634), Rhode Island (1636) as a colony tolerant of all religions and Connecticut (1639) for Congregationalists. The Province of Carolina was founded in 1663. With the surrender of Fort Amsterdam in 1664, England gained control of the Dutch colony of New Netherland, renaming it New York. This was formalised in negotiations following the Second Anglo-Dutch War, in exchange for Suriname. In 1681, the colony of Pennsylvania was founded by William Penn. The American colonies were less financially successful than those of the Caribbean, but had large areas of good agricultural land and attracted far larger numbers of English emigrants who preferred their temperate climates.
Two years later, the Royal African Company was inaugurated, receiving from King Charles a monopoly of the trade to supply slaves to the British colonies of the Caribbean. From the outset, slavery was the basis of the British Empire in the West Indies. Until the abolition of the slave trade in 1807, Britain was responsible for the transportation of 3.5 million African slaves to the Americas, a third of all slaves transported across the Atlantic. To facilitate this trade, forts were established on the coast of West Africa, such as James Island, Accra and Bunce Island. In the British Caribbean, the percentage of the population of African descent rose from 25 percent in 1650 to around 80 percent in 1780, and in the 13 Colonies from 10 percent to 40 percent over the same period (the majority in the southern colonies). For the slave traders, the trade was extremely profitable, and became a major economic mainstay for such western British cities as Bristol and Liverpool, which formed the third corner of the so-called triangular trade with Africa and the Americas. For the transported, harsh and unhygienic conditions on the slaving ships and poor diets meant that the average mortality rate during the Middle Passage was one in seven.
In 1695, the Scottish Parliament granted a charter to the Company of Scotland, which established a settlement in 1698 on the isthmus of Panama. Besieged by neighbouring Spanish colonists of New Granada, and afflicted by malaria, the colony was abandoned two years later. The Darien scheme was a financial disaster for Scotland—a quarter of Scottish capital was lost in the enterprise—and ended Scottish hopes of establishing its own overseas empire. The episode also had major political consequences, persuading the governments of both England and Scotland of the merits of a union of countries, rather than just crowns. This occurred in 1707 with the Treaty of Union, establishing the Kingdom of Great Britain.
At the end of the 16th century, England and the Netherlands began to challenge Portugal's monopoly of trade with Asia, forming private joint-stock companies to finance the voyages—the English, later British, East India Company and the Dutch East India Company, chartered in 1600 and 1602 respectively. The primary aim of these companies was to tap into the lucrative spice trade, an effort focused mainly on two regions; the East Indies archipelago, and an important hub in the trade network, India. There, they competed for trade supremacy with Portugal and with each other. Although England ultimately eclipsed the Netherlands as a colonial power, in the short term the Netherlands' more advanced financial system and the three Anglo-Dutch Wars of the 17th century left it with a stronger position in Asia. Hostilities ceased after the Glorious Revolution of 1688 when the Dutch William of Orange ascended the English throne, bringing peace between the Netherlands and England. A deal between the two nations left the spice trade of the East Indies archipelago to the Netherlands and the textiles industry of India to England, but textiles soon overtook spices in terms of profitability, and by 1720, in terms of sales, the British company had overtaken the Dutch.
Peace between England and the Netherlands in 1688 meant that the two countries entered the Nine Years' War as allies, but the conflict—waged in Europe and overseas between France, Spain and the Anglo-Dutch alliance—left the English a stronger colonial power than the Dutch, who were forced to devote a larger proportion of their military budget on the costly land war in Europe. The 18th century saw England (after 1707, Britain) rise to be the world's dominant colonial power, and France becoming its main rival on the imperial stage.
At the concluding Treaty of Utrecht, Philip renounced his and his descendants' right to the French throne and Spain lost its empire in Europe. The British Empire was territorially enlarged: from France, Britain gained Newfoundland and Acadia, and from Spain, Gibraltar and Minorca. Gibraltar became a critical naval base and allowed Britain to control the Atlantic entry and exit point to the Mediterranean. Spain also ceded the rights to the lucrative asiento (permission to sell slaves in Spanish America) to Britain.
During the middle decades of the 18th century, there were several outbreaks of military conflict on the Indian subcontinent, the Carnatic Wars, as the English East India Company (the Company) and its French counterpart, the Compagnie française des Indes orientales, struggled alongside local rulers to fill the vacuum that had been left by the decline of the Mughal Empire. The Battle of Plassey in 1757, in which the British, led by Robert Clive, defeated the Nawab of Bengal and his French allies, left the Company in control of Bengal and as the major military and political power in India. France was left control of its enclaves but with military restrictions and an obligation to support British client states, ending French hopes of controlling India. In the following decades the Company gradually increased the size of the territories under its control, either ruling directly or via local rulers under the threat of force from the British Indian Army, the vast majority of which was composed of Indian sepoys.
The British and French struggles in India became but one theatre of the global Seven Years' War (1756–1763) involving France, Britain and the other major European powers. The signing of the Treaty of Paris (1763) had important consequences for the future of the British Empire. In North America, France's future as a colonial power there was effectively ended with the recognition of British claims to Rupert's Land, and the ceding of New France to Britain (leaving a sizeable French-speaking population under British control) and Louisiana to Spain. Spain ceded Florida to Britain. Along with its victory over France in India, the Seven Years' War therefore left Britain as the world's most powerful maritime power.
During the 1760s and early 1770s, relations between the Thirteen Colonies and Britain became increasingly strained, primarily because of resentment of the British Parliament's attempts to govern and tax American colonists without their consent. This was summarised at the time by the slogan "No taxation without representation", a perceived violation of the guaranteed Rights of Englishmen. The American Revolution began with rejection of Parliamentary authority and moves towards self-government. In response Britain sent troops to reimpose direct rule, leading to the outbreak of war in 1775. The following year, in 1776, the United States declared independence. The entry of France to the war in 1778 tipped the military balance in the Americans' favour and after a decisive defeat at Yorktown in 1781, Britain began negotiating peace terms. American independence was acknowledged at the Peace of Paris in 1783.
The loss of such a large portion of British America, at the time Britain's most populous overseas possession, is seen by some historians as the event defining the transition between the "first" and "second" empires, in which Britain shifted its attention away from the Americas to Asia, the Pacific and later Africa. Adam Smith's Wealth of Nations, published in 1776, had argued that colonies were redundant, and that free trade should replace the old mercantilist policies that had characterised the first period of colonial expansion, dating back to the protectionism of Spain and Portugal. The growth of trade between the newly independent United States and Britain after 1783 seemed to confirm Smith's view that political control was not necessary for economic success.
Events in America influenced British policy in Canada, where between 40,000 and 100,000 defeated Loyalists had migrated from America following independence. The 14,000 Loyalists who went to the Saint John and Saint Croix river valleys, then part of Nova Scotia, felt too far removed from the provincial government in Halifax, so London split off New Brunswick as a separate colony in 1784. The Constitutional Act of 1791 created the provinces of Upper Canada (mainly English-speaking) and Lower Canada (mainly French-speaking) to defuse tensions between the French and British communities, and implemented governmental systems similar to those employed in Britain, with the intention of asserting imperial authority and not allowing the sort of popular control of government that was perceived to have led to the American Revolution.
Since 1718, transportation to the American colonies had been a penalty for various criminal offences in Britain, with approximately one thousand convicts transported per year across the Atlantic. Forced to find an alternative location after the loss of the 13 Colonies in 1783, the British government turned to the newly discovered lands of Australia. The western coast of Australia had been discovered for Europeans by the Dutch explorer Willem Jansz in 1606 and was later named New Holland by the Dutch East India Company, but there was no attempt to colonise it. In 1770 James Cook discovered the eastern coast of Australia while on a scientific voyage to the South Pacific Ocean, claimed the continent for Britain, and named it New South Wales. In 1778, Joseph Banks, Cook's botanist on the voyage, presented evidence to the government on the suitability of Botany Bay for the establishment of a penal settlement, and in 1787 the first shipment of convicts set sail, arriving in 1788. Britain continued to transport convicts to New South Wales until 1840. The Australian colonies became profitable exporters of wool and gold, mainly because of gold rushes in the colony of Victoria, making its capital Melbourne the richest city in the world and the largest city after London in the British Empire.
During his voyage, Cook also visited New Zealand, first discovered by Dutch explorer Abel Tasman in 1642, and claimed the North and South islands for the British crown in 1769 and 1770 respectively. Initially, interaction between the indigenous Māori population and Europeans was limited to the trading of goods. European settlement increased through the early decades of the 19th century, with numerous trading stations established, especially in the North. In 1839, the New Zealand Company announced plans to buy large tracts of land and establish colonies in New Zealand. On 6 February 1840, Captain William Hobson and around 40 Maori chiefs signed the Treaty of Waitangi. This treaty is considered by many to be New Zealand's founding document, but differing interpretations of the Maori and English versions of the text have meant that it continues to be a source of dispute.
The Napoleonic Wars were therefore ones in which Britain invested large amounts of capital and resources to win. French ports were blockaded by the Royal Navy, which won a decisive victory over a Franco-Spanish fleet at Trafalgar in 1805. Overseas colonies were attacked and occupied, including those of the Netherlands, which was annexed by Napoleon in 1810. France was finally defeated by a coalition of European armies in 1815. Britain was again the beneficiary of peace treaties: France ceded the Ionian Islands, Malta (which it had occupied in 1797 and 1798 respectively), Mauritius, St Lucia, and Tobago; Spain ceded Trinidad; the Netherlands Guyana, and the Cape Colony. Britain returned Guadeloupe, Martinique, French Guiana, and Réunion to France, and Java and Suriname to the Netherlands, while gaining control of Ceylon (1795–1815).
With support from the British abolitionist movement, Parliament enacted the Slave Trade Act in 1807, which abolished the slave trade in the empire. In 1808, Sierra Leone was designated an official British colony for freed slaves. The Slavery Abolition Act passed in 1833 abolished slavery in the British Empire on 1 August 1834 (with the exception of St. Helena, Ceylon and the territories administered by the East India Company, though these exclusions were later repealed). Under the Act, slaves were granted full emancipation after a period of 4 to 6 years of "apprenticeship".
Between 1815 and 1914, a period referred to as Britain's "imperial century" by some historians, around 10,000,000 square miles (26,000,000 km2) of territory and roughly 400 million people were added to the British Empire. Victory over Napoleon left Britain without any serious international rival, other than Russia in central Asia. Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the Pax Britannica, and a foreign policy of "splendid isolation". Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many countries, such as China, Argentina and Siam, which has been characterised by some historians as "Informal Empire".
From its base in India, the Company had also been engaged in an increasingly profitable opium export trade to China since the 1730s. This trade, illegal since it was outlawed by the Qing dynasty in 1729, helped reverse the trade imbalances resulting from the British imports of tea, which saw large outflows of silver from Britain to China. In 1839, the confiscation by the Chinese authorities at Canton of 20,000 chests of opium led Britain to attack China in the First Opium War, and resulted in the seizure by Britain of Hong Kong Island, at that time a minor settlement.
During the late 18th and early 19th centuries the British Crown began to assume an increasingly large role in the affairs of the Company. A series of Acts of Parliament were passed, including the Regulating Act of 1773, Pitt's India Act of 1784 and the Charter Act of 1813 which regulated the Company's affairs and established the sovereignty of the Crown over the territories that it had acquired. The Company's eventual end was precipitated by the Indian Rebellion, a conflict that had begun with the mutiny of sepoys, Indian troops under British officers and discipline. The rebellion took six months to suppress, with heavy loss of life on both sides. The following year the British government dissolved the Company and assumed direct control over India through the Government of India Act 1858, establishing the British Raj, where an appointed governor-general administered India and Queen Victoria was crowned the Empress of India. India became the empire's most valuable possession, "the Jewel in the Crown", and was the most important source of Britain's strength.
During the 19th century, Britain and the Russian Empire vied to fill the power vacuums that had been left by the declining Ottoman Empire, Qajar dynasty and Qing Dynasty. This rivalry in Eurasia came to be known as the "Great Game". As far as Britain was concerned, defeats inflicted by Russia on Persia and Turkey demonstrated its imperial ambitions and capabilities and stoked fears in Britain of an overland invasion of India. In 1839, Britain moved to pre-empt this by invading Afghanistan, but the First Anglo-Afghan War was a disaster for Britain.
When Russia invaded the Turkish Balkans in 1853, fears of Russian dominance in the Mediterranean and Middle East led Britain and France to invade the Crimean Peninsula to destroy Russian naval capabilities. The ensuing Crimean War (1854–56), which involved new techniques of modern warfare, and was the only global war fought between Britain and another imperial power during the Pax Britannica, was a resounding defeat for Russia. The situation remained unresolved in Central Asia for two more decades, with Britain annexing Baluchistan in 1876 and Russia annexing Kirghizia, Kazakhstan, and Turkmenistan. For a while it appeared that another war would be inevitable, but the two countries reached an agreement on their respective spheres of influence in the region in 1878 and on all outstanding matters in 1907 with the signing of the Anglo-Russian Entente. The destruction of the Russian Navy by the Japanese at the Battle of Port Arthur during the Russo-Japanese War of 1904–05 also limited its threat to the British.
The Dutch East India Company had founded the Cape Colony on the southern tip of Africa in 1652 as a way station for its ships travelling to and from its colonies in the East Indies. Britain formally acquired the colony, and its large Afrikaner (or Boer) population in 1806, having occupied it in 1795 to prevent its falling into French hands, following the invasion of the Netherlands by France. British immigration began to rise after 1820, and pushed thousands of Boers, resentful of British rule, northwards to found their own—mostly short-lived—independent republics, during the Great Trek of the late 1830s and early 1840s. In the process the Voortrekkers clashed repeatedly with the British, who had their own agenda with regard to colonial expansion in South Africa and with several African polities, including those of the Sotho and the Zulu nations. Eventually the Boers established two republics which had a longer lifespan: the South African Republic or Transvaal Republic (1852–77; 1881–1902) and the Orange Free State (1854–1902). In 1902 Britain occupied both republics, concluding a treaty with the two Boer Republics following the Second Boer War (1899–1902).
In 1869 the Suez Canal opened under Napoleon III, linking the Mediterranean with the Indian Ocean. Initially the Canal was opposed by the British; but once opened, its strategic value was quickly recognised and became the "jugular vein of the Empire". In 1875, the Conservative government of Benjamin Disraeli bought the indebted Egyptian ruler Isma'il Pasha's 44 percent shareholding in the Suez Canal for £4 million (£340 million in 2013). Although this did not grant outright control of the strategic waterway, it did give Britain leverage. Joint Anglo-French financial control over Egypt ended in outright British occupation in 1882. The French were still majority shareholders and attempted to weaken the British position, but a compromise was reached with the 1888 Convention of Constantinople, which made the Canal officially neutral territory.
With French, Belgian and Portuguese activity in the lower Congo River region undermining orderly incursion of tropical Africa, the Berlin Conference of 1884–85 was held to regulate the competition between the European powers in what was called the "Scramble for Africa" by defining "effective occupation" as the criterion for international recognition of territorial claims. The scramble continued into the 1890s, and caused Britain to reconsider its decision in 1885 to withdraw from Sudan. A joint force of British and Egyptian troops defeated the Mahdist Army in 1896, and rebuffed a French attempted invasion at Fashoda in 1898. Sudan was nominally made an Anglo-Egyptian Condominium, but a British colony in reality.
The path to independence for the white colonies of the British Empire began with the 1839 Durham Report, which proposed unification and self-government for Upper and Lower Canada, as a solution to political unrest there. This began with the passing of the Act of Union in 1840, which created the Province of Canada. Responsible government was first granted to Nova Scotia in 1848, and was soon extended to the other British North American colonies. With the passage of the British North America Act, 1867 by the British Parliament, Upper and Lower Canada, New Brunswick and Nova Scotia were formed into the Dominion of Canada, a confederation enjoying full self-government with the exception of international relations. Australia and New Zealand achieved similar levels of self-government after 1900, with the Australian colonies federating in 1901. The term "dominion status" was officially introduced at the Colonial Conference of 1907.
The last decades of the 19th century saw concerted political campaigns for Irish home rule. Ireland had been united with Britain into the United Kingdom of Great Britain and Ireland with the Act of Union 1800 after the Irish Rebellion of 1798, and had suffered a severe famine between 1845 and 1852. Home rule was supported by the British Prime minister, William Gladstone, who hoped that Ireland might follow in Canada's footsteps as a Dominion within the empire, but his 1886 Home Rule bill was defeated in Parliament. Although the bill, if passed, would have granted Ireland less autonomy within the UK than the Canadian provinces had within their own federation, many MPs feared that a partially independent Ireland might pose a security threat to Great Britain or mark the beginning of the break-up of the empire. A second Home Rule bill was also defeated for similar reasons. A third bill was passed by Parliament in 1914, but not implemented because of the outbreak of the First World War leading to the 1916 Easter Rising.
By the turn of the 20th century, fears had begun to grow in Britain that it would no longer be able to defend the metropole and the entirety of the empire while at the same time maintaining the policy of "splendid isolation". Germany was rapidly rising as a military and industrial power and was now seen as the most likely opponent in any future war. Recognising that it was overstretched in the Pacific and threatened at home by the Imperial German Navy, Britain formed an alliance with Japan in 1902 and with its old enemies France and Russia in 1904 and 1907, respectively.
Britain's fears of war with Germany were realised in 1914 with the outbreak of the First World War. Britain quickly invaded and occupied most of Germany's overseas colonies in Africa. In the Pacific, Australia and New Zealand occupied German New Guinea and Samoa respectively. Plans for a post-war division of the Ottoman Empire, which had joined the war on Germany's side, were secretly drawn up by Britain and France under the 1916 Sykes–Picot Agreement. This agreement was not divulged to the Sharif of Mecca, who the British had been encouraging to launch an Arab revolt against their Ottoman rulers, giving the impression that Britain was supporting the creation of an independent Arab state.
The British declaration of war on Germany and its allies also committed the colonies and Dominions, which provided invaluable military, financial and material support. Over 2.5 million men served in the armies of the Dominions, as well as many thousands of volunteers from the Crown colonies. The contributions of Australian and New Zealand troops during the 1915 Gallipoli Campaign against the Ottoman Empire had a great impact on the national consciousness at home, and marked a watershed in the transition of Australia and New Zealand from colonies to nations in their own right. The countries continue to commemorate this occasion on Anzac Day. Canadians viewed the Battle of Vimy Ridge in a similar light. The important contribution of the Dominions to the war effort was recognised in 1917 by the British Prime Minister David Lloyd George when he invited each of the Dominion Prime Ministers to join an Imperial War Cabinet to co-ordinate imperial policy.
Under the terms of the concluding Treaty of Versailles signed in 1919, the empire reached its greatest extent with the addition of 1,800,000 square miles (4,700,000 km2) and 13 million new subjects. The colonies of Germany and the Ottoman Empire were distributed to the Allied powers as League of Nations mandates. Britain gained control of Palestine, Transjordan, Iraq, parts of Cameroon and Togo, and Tanganyika. The Dominions themselves also acquired mandates of their own: the Union of South Africa gained South-West Africa (modern-day Namibia), Australia gained German New Guinea, and New Zealand Western Samoa. Nauru was made a combined mandate of Britain and the two Pacific Dominions.
The changing world order that the war had brought about, in particular the growth of the United States and Japan as naval powers, and the rise of independence movements in India and Ireland, caused a major reassessment of British imperial policy. Forced to choose between alignment with the United States or Japan, Britain opted not to renew its Japanese alliance and instead signed the 1922 Washington Naval Treaty, where Britain accepted naval parity with the United States. This decision was the source of much debate in Britain during the 1930s as militaristic governments took hold in Japan and Germany helped in part by the Great Depression, for it was feared that the empire could not survive a simultaneous attack by both nations. Although the issue of the empire's security was a serious concern in Britain, at the same time the empire was vital to the British economy.
In 1919, the frustrations caused by delays to Irish home rule led members of Sinn Féin, a pro-independence party that had won a majority of the Irish seats at Westminster in the 1918 British general election, to establish an Irish assembly in Dublin, at which Irish independence was declared. The Irish Republican Army simultaneously began a guerrilla war against the British administration. The Anglo-Irish War ended in 1921 with a stalemate and the signing of the Anglo-Irish Treaty, creating the Irish Free State, a Dominion within the British Empire, with effective internal independence but still constitutionally linked with the British Crown. Northern Ireland, consisting of six of the 32 Irish counties which had been established as a devolved region under the 1920 Government of Ireland Act, immediately exercised its option under the treaty to retain its existing status within the United Kingdom.
A similar struggle began in India when the Government of India Act 1919 failed to satisfy demand for independence. Concerns over communist and foreign plots following the Ghadar Conspiracy ensured that war-time strictures were renewed by the Rowlatt Acts. This led to tension, particularly in the Punjab region, where repressive measures culminated in the Amritsar Massacre. In Britain public opinion was divided over the morality of the event, between those who saw it as having saved India from anarchy, and those who viewed it with revulsion. The subsequent Non-Co-Operation movement was called off in March 1922 following the Chauri Chaura incident, and discontent continued to simmer for the next 25 years.
In 1922, Egypt, which had been declared a British protectorate at the outbreak of the First World War, was granted formal independence, though it continued to be a British client state until 1954. British troops remained stationed in Egypt until the signing of the Anglo-Egyptian Treaty in 1936, under which it was agreed that the troops would withdraw but continue to occupy and defend the Suez Canal zone. In return, Egypt was assisted to join the League of Nations. Iraq, a British mandate since 1920, also gained membership of the League in its own right after achieving independence from Britain in 1932. In Palestine, Britain was presented with the problem of mediating between the Arab and Jewish communities. The 1917 Balfour Declaration, which had been incorporated into the terms of the mandate, stated that a national home for the Jewish people would be established in Palestine, and Jewish immigration allowed up to a limit that would be determined by the mandatory power. This led to increasing conflict with the Arab population, who openly revolted in 1936. As the threat of war with Germany increased during the 1930s, Britain judged the support of the Arab population in the Middle East as more important than the establishment of a Jewish homeland, and shifted to a pro-Arab stance, limiting Jewish immigration and in turn triggering a Jewish insurgency.
The ability of the Dominions to set their own foreign policy, independent of Britain, was recognised at the 1923 Imperial Conference. Britain's request for military assistance from the Dominions at the outbreak of the Chanak Crisis the previous year had been turned down by Canada and South Africa, and Canada had refused to be bound by the 1923 Treaty of Lausanne. After pressure from Ireland and South Africa, the 1926 Imperial Conference issued the Balfour Declaration, declaring the Dominions to be "autonomous Communities within the British Empire, equal in status, in no way subordinate one to another" within a "British Commonwealth of Nations". This declaration was given legal substance under the 1931 Statute of Westminster. The parliaments of Canada, Australia, New Zealand, the Union of South Africa, the Irish Free State and Newfoundland were now independent of British legislative control, they could nullify British laws and Britain could no longer pass laws for them without their consent. Newfoundland reverted to colonial status in 1933, suffering from financial difficulties during the Great Depression. Ireland distanced itself further from Britain with the introduction of a new constitution in 1937, making it a republic in all but name.
After the German occupation of France in 1940, Britain and the empire stood alone against Germany, until the entry of the Soviet Union to the war in 1941. British Prime Minister Winston Churchill successfully lobbied President Franklin D. Roosevelt for military aid from the United States, but Roosevelt was not yet ready to ask Congress to commit the country to war. In August 1941, Churchill and Roosevelt met and signed the Atlantic Charter, which included the statement that "the rights of all peoples to choose the form of government under which they live" should be respected. This wording was ambiguous as to whether it referred to European countries invaded by Germany, or the peoples colonised by European nations, and would later be interpreted differently by the British, Americans, and nationalist movements.
In December 1941, Japan launched, in quick succession, attacks on British Malaya, the United States naval base at Pearl Harbor, and Hong Kong. Churchill's reaction to the entry of the United States into the war was that Britain was now assured of victory and the future of the empire was safe, but the manner in which British forces were rapidly defeated in the Far East irreversibly harmed Britain's standing and prestige as an imperial power. Most damaging of all was the fall of Singapore, which had previously been hailed as an impregnable fortress and the eastern equivalent of Gibraltar. The realisation that Britain could not defend its entire empire pushed Australia and New Zealand, which now appeared threatened by Japanese forces, into closer ties with the United States. This resulted in the 1951 ANZUS Pact between Australia, New Zealand and the United States of America.
Though Britain and the empire emerged victorious from the Second World War, the effects of the conflict were profound, both at home and abroad. Much of Europe, a continent that had dominated the world for several centuries, was in ruins, and host to the armies of the United States and the Soviet Union, who now held the balance of global power. Britain was left essentially bankrupt, with insolvency only averted in 1946 after the negotiation of a $US 4.33 billion loan (US$56 billion in 2012) from the United States, the last instalment of which was repaid in 2006. At the same time, anti-colonial movements were on the rise in the colonies of European nations. The situation was complicated further by the increasing Cold War rivalry of the United States and the Soviet Union. In principle, both nations were opposed to European colonialism. In practice, however, American anti-communism prevailed over anti-imperialism, and therefore the United States supported the continued existence of the British Empire to keep Communist expansion in check. The "wind of change" ultimately meant that the British Empire's days were numbered, and on the whole, Britain adopted a policy of peaceful disengagement from its colonies once stable, non-Communist governments were available to transfer power to. This was in contrast to other European powers such as France and Portugal, which waged costly and ultimately unsuccessful wars to keep their empires intact. Between 1945 and 1965, the number of people under British rule outside the UK itself fell from 700 million to five million, three million of whom were in Hong Kong.
The pro-decolonisation Labour government, elected at the 1945 general election and led by Clement Attlee, moved quickly to tackle the most pressing issue facing the empire: that of Indian independence. India's two major political parties—the Indian National Congress and the Muslim League—had been campaigning for independence for decades, but disagreed as to how it should be implemented. Congress favoured a unified secular Indian state, whereas the League, fearing domination by the Hindu majority, desired a separate Islamic state for Muslim-majority regions. Increasing civil unrest and the mutiny of the Royal Indian Navy during 1946 led Attlee to promise independence no later than 1948. When the urgency of the situation and risk of civil war became apparent, the newly appointed (and last) Viceroy, Lord Mountbatten, hastily brought forward the date to 15 August 1947. The borders drawn by the British to broadly partition India into Hindu and Muslim areas left tens of millions as minorities in the newly independent states of India and Pakistan. Millions of Muslims subsequently crossed from India to Pakistan and Hindus vice versa, and violence between the two communities cost hundreds of thousands of lives. Burma, which had been administered as part of the British Raj, and Sri Lanka gained their independence the following year in 1948. India, Pakistan and Sri Lanka became members of the Commonwealth, while Burma chose not to join.
The British Mandate of Palestine, where an Arab majority lived alongside a Jewish minority, presented the British with a similar problem to that of India. The matter was complicated by large numbers of Jewish refugees seeking to be admitted to Palestine following the Holocaust, while Arabs were opposed to the creation of a Jewish state. Frustrated by the intractability of the problem, attacks by Jewish paramilitary organisations and the increasing cost of maintaining its military presence, Britain announced in 1947 that it would withdraw in 1948 and leave the matter to the United Nations to solve. The UN General Assembly subsequently voted for a plan to partition Palestine into a Jewish and an Arab state.
Following the defeat of Japan in the Second World War, anti-Japanese resistance movements in Malaya turned their attention towards the British, who had moved to quickly retake control of the colony, valuing it as a source of rubber and tin. The fact that the guerrillas were primarily Malayan-Chinese Communists meant that the British attempt to quell the uprising was supported by the Muslim Malay majority, on the understanding that once the insurgency had been quelled, independence would be granted. The Malayan Emergency, as it was called, began in 1948 and lasted until 1960, but by 1957, Britain felt confident enough to grant independence to the Federation of Malaya within the Commonwealth. In 1963, the 11 states of the federation together with Singapore, Sarawak and North Borneo joined to form Malaysia, but in 1965 Chinese-majority Singapore was expelled from the union following tensions between the Malay and Chinese populations. Brunei, which had been a British protectorate since 1888, declined to join the union and maintained its status until independence in 1984.
In 1951, the Conservative Party returned to power in Britain, under the leadership of Winston Churchill. Churchill and the Conservatives believed that Britain's position as a world power relied on the continued existence of the empire, with the base at the Suez Canal allowing Britain to maintain its pre-eminent position in the Middle East in spite of the loss of India. However, Churchill could not ignore Gamal Abdul Nasser's new revolutionary government of Egypt that had taken power in 1952, and the following year it was agreed that British troops would withdraw from the Suez Canal zone and that Sudan would be granted self-determination by 1955, with independence to follow. Sudan was granted independence on 1 January 1956.
In July 1956, Nasser unilaterally nationalised the Suez Canal. The response of Anthony Eden, who had succeeded Churchill as Prime Minister, was to collude with France to engineer an Israeli attack on Egypt that would give Britain and France an excuse to intervene militarily and retake the canal. Eden infuriated US President Dwight D. Eisenhower, by his lack of consultation, and Eisenhower refused to back the invasion. Another of Eisenhower's concerns was the possibility of a wider war with the Soviet Union after it threatened to intervene on the Egyptian side. Eisenhower applied financial leverage by threatening to sell US reserves of the British pound and thereby precipitate a collapse of the British currency. Though the invasion force was militarily successful in its objectives, UN intervention and US pressure forced Britain into a humiliating withdrawal of its forces, and Eden resigned.
The Suez Crisis very publicly exposed Britain's limitations to the world and confirmed Britain's decline on the world stage, demonstrating that henceforth it could no longer act without at least the acquiescence, if not the full support, of the United States. The events at Suez wounded British national pride, leading one MP to describe it as "Britain's Waterloo" and another to suggest that the country had become an "American satellite". Margaret Thatcher later described the mindset she believed had befallen the British political establishment as "Suez syndrome", from which Britain did not recover until the successful recapture of the Falkland Islands from Argentina in 1982.
While the Suez Crisis caused British power in the Middle East to weaken, it did not collapse. Britain again deployed its armed forces to the region, intervening in Oman (1957), Jordan (1958) and Kuwait (1961), though on these occasions with American approval, as the new Prime Minister Harold Macmillan's foreign policy was to remain firmly aligned with the United States. Britain maintained a military presence in the Middle East for another decade. In January 1968, a few weeks after the devaluation of the pound, Prime Minister Harold Wilson and his Defence Secretary Denis Healey announced that British troops would be withdrawn from major military bases East of Suez, which included the ones in the Middle East, and primarily from Malaysia and Singapore. The British withdrew from Aden in 1967, Bahrain in 1971, and Maldives in 1976.
Britain's remaining colonies in Africa, except for self-governing Southern Rhodesia, were all granted independence by 1968. British withdrawal from the southern and eastern parts of Africa was not a peaceful process. Kenyan independence was preceded by the eight-year Mau Mau Uprising. In Rhodesia, the 1965 Unilateral Declaration of Independence by the white minority resulted in a civil war that lasted until the Lancaster House Agreement of 1979, which set the terms for recognised independence in 1980, as the new nation of Zimbabwe.
Most of the UK's Caribbean territories achieved independence after the departure in 1961 and 1962 of Jamaica and Trinidad from the West Indies Federation, established in 1958 in an attempt to unite the British Caribbean colonies under one government, but which collapsed following the loss of its two largest members. Barbados achieved independence in 1966 and the remainder of the eastern Caribbean islands in the 1970s and 1980s, but Anguilla and the Turks and Caicos Islands opted to revert to British rule after they had already started on the path to independence. The British Virgin Islands, Cayman Islands and Montserrat opted to retain ties with Britain, while Guyana achieved independence in 1966. Britain's last colony on the American mainland, British Honduras, became a self-governing colony in 1964 and was renamed Belize in 1973, achieving full independence in 1981. A dispute with Guatemala over claims to Belize was left unresolved.
In 1980, Rhodesia, Britain's last African colony, became the independent nation of Zimbabwe. The New Hebrides achieved independence (as Vanuatu) in 1980, with Belize following suit in 1981. The passage of the British Nationality Act 1981, which reclassified the remaining Crown colonies as "British Dependent Territories" (renamed British Overseas Territories in 2002) meant that, aside from a scattering of islands and outposts the process of decolonisation that had begun after the Second World War was largely complete. In 1982, Britain's resolve in defending its remaining overseas territories was tested when Argentina invaded the Falkland Islands, acting on a long-standing claim that dated back to the Spanish Empire. Britain's ultimately successful military response to retake the islands during the ensuing Falklands War was viewed by many to have contributed to reversing the downward trend in Britain's status as a world power. The same year, the Canadian government severed its last legal link with Britain by patriating the Canadian constitution from Britain. The 1982 Canada Act passed by the British parliament ended the need for British involvement in changes to the Canadian constitution. Similarly, the Constitution Act 1986 reformed the constitution of New Zealand to sever its constitutional link with Britain, and the Australia Act 1986 severed the constitutional link between Britain and the Australian states. In 1984, Brunei, Britain's last remaining Asian protectorate, gained its independence.
In September 1982, Prime minister Margaret Thatcher travelled to Beijing to negotiate with the Chinese government on the future of Britain's last major and most populous overseas territory, Hong Kong. Under the terms of the 1842 Treaty of Nanking, Hong Kong Island itself had been ceded to Britain in perpetuity, but the vast majority of the colony was constituted by the New Territories, which had been acquired under a 99-year lease in 1898, due to expire in 1997. Thatcher, seeing parallels with the Falkland Islands, initially wished to hold Hong Kong and proposed British administration with Chinese sovereignty, though this was rejected by China. A deal was reached in 1984—under the terms of the Sino-British Joint Declaration, Hong Kong would become a special administrative region of the People's Republic of China, maintaining its way of life for at least 50 years. The handover ceremony in 1997 marked for many, including Charles, Prince of Wales, who was in attendance, "the end of Empire".
Britain retains sovereignty over 14 territories outside the British Isles, which were renamed the British Overseas Territories in 2002. Some are uninhabited except for transient military or scientific personnel; the remainder are self-governing to varying degrees and are reliant on the UK for foreign relations and defence. The British government has stated its willingness to assist any Overseas Territory that wishes to proceed to independence, where that is an option. British sovereignty of several of the overseas territories is disputed by their geographical neighbours: Gibraltar is claimed by Spain, the Falkland Islands and South Georgia and the South Sandwich Islands are claimed by Argentina, and the British Indian Ocean Territory is claimed by Mauritius and Seychelles. The British Antarctic Territory is subject to overlapping claims by Argentina and Chile, while many countries do not recognise any territorial claims in Antarctica.
Most former British colonies and protectorates are among the 53 member states of the Commonwealth of Nations, a non-political, voluntary association of equal members, comprising a population of around 2.2 billion people. Sixteen Commonwealth realms voluntarily continue to share the British monarch, Queen Elizabeth II, as their head of state. These sixteen nations are distinct and equal legal entities – the United Kingdom, Australia, Canada, New Zealand, Papua New Guinea, Antigua and Barbuda, The Bahamas, Barbados, Belize, Grenada, Jamaica, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands and Tuvalu.
Political boundaries drawn by the British did not always reflect homogeneous ethnicities or religions, contributing to conflicts in formerly colonised areas. The British Empire was also responsible for large migrations of peoples. Millions left the British Isles, with the founding settler populations of the United States, Canada, Australia and New Zealand coming mainly from Britain and Ireland. Tensions remain between the white settler populations of these countries and their indigenous minorities, and between white settler minorities and indigenous majorities in South Africa and Zimbabwe. Settlers in Ireland from Great Britain have left their mark in the form of divided nationalist and unionist communities in Northern Ireland. Millions of people moved to and from British colonies, with large numbers of Indians emigrating to other parts of the empire, such as Malaysia and Fiji, and Chinese people to Malaysia, Singapore and the Caribbean. The demographics of Britain itself was changed after the Second World War owing to immigration to Britain from its former colonies.
Of approximately 100 million native speakers of German in the world, roughly 80 million consider themselves Germans.[citation needed] There are an additional 80 million people of German ancestry mainly in the United States, Brazil (mainly in the South Region of the country), Argentina, Canada, South Africa, the post-Soviet states (mainly in Russia and Kazakhstan), and France, each accounting for at least 1 million.[note 2] Thus, the total number of Germans lies somewhere between 100 and more than 150 million, depending on the criteria applied (native speakers, single-ancestry ethnic Germans, partial German ancestry, etc.).
Conflict between the Germanic tribes and the forces of Rome under Julius Caesar forced major Germanic tribes to retreat to the east bank of the Rhine. Roman emperor Augustus in 12 BC ordered the conquest of the Germans, but the catastrophic Roman defeat at the Battle of the Teutoburg Forest resulted in the Roman Empire abandoning its plans to completely conquer Germany. Germanic peoples in Roman territory were culturally Romanized, and although much of Germany remained free of direct Roman rule, Rome deeply influenced the development of German society, especially the adoption of Christianity by the Germans who obtained it from the Romans. In Roman-held territories with Germanic populations, the Germanic and Roman peoples intermarried, and Roman, Germanic, and Christian traditions intermingled. The adoption of Christianity would later become a major influence in the development of a common German identity.
The Germanic peoples during the Migrations Period came into contact with other peoples; in the case of the populations settling in the territory of modern Germany, they encountered Celts to the south, and Balts and Slavs towards the east. The Limes Germanicus was breached in AD 260. Migrating Germanic tribes commingled with the local Gallo-Roman populations in what is now Swabia and Bavaria. The arrival of the Huns in Europe resulted in Hun conquest of large parts of Eastern Europe, the Huns initially were allies of the Roman Empire who fought against Germanic tribes, but later the Huns cooperated with the Germanic tribe of the Ostrogoths, and large numbers of Germans lived within the lands of the Hunnic Empire of Attila. Attila had both Hunnic and Germanic families and prominent Germanic chiefs amongst his close entourage in Europe. The Huns living in Germanic territories in Eastern Europe adopted an East Germanic language as their lingua franca. A major part of Attila's army were Germans, during the Huns' campaign against the Roman Empire. After Attila's unexpected death the Hunnic Empire collapsed with the Huns disappearing as a people in Europe – who either escaped into Asia, or otherwise blended in amongst Europeans.
The migration-period peoples who later coalesced into a "German" ethnicity were the Germanic tribes of the Saxons, Franci, Thuringii, Alamanni and Bavarii. These five tribes, sometimes with inclusion of the Frisians, are considered as the major groups to take part in the formation of the Germans. The varieties of the German language are still divided up into these groups. Linguists distinguish low Saxon, Franconian, Bavarian, Thuringian and Alemannic varieties in modern German. By the 9th century, the large tribes which lived on the territory of modern Germany had been united under the rule of the Frankish king Charlemagne, known in German as Karl der Große. Much of what is now Eastern Germany became Slavonic-speaking (Sorbs and Veleti), after these areas were vacated by Germanic tribes (Vandals, Lombards, Burgundians and Suebi amongst others) which had migrated into the former areas of the Roman Empire.
A German ethnicity emerged in the course of the Middle Ages, ultimately as a result of the formation of the kingdom of Germany within East Francia and later the Holy Roman Empire, beginning in the 9th century. The process was gradual and lacked any clear definition, and the use of exonyms designating "the Germans" develops only during the High Middle Ages. The title of rex teutonicum "King of the Germans" is first used in the late 11th century, by the chancery of Pope Gregory VII, to describe the future Holy Roman Emperor of the German Nation Henry IV. Natively, the term ein diutscher ("a German") is used for the people of Germany from the 12th century.
After Christianization, the Roman Catholic Church and local rulers led German expansion and settlement in areas inhabited by Slavs and Balts, known as Ostsiedlung. During the wars waged in the Baltic by the Catholic German Teutonic Knights; the lands inhabited by the ethnic group of the Old Prussians (the current reference to the people known then simply as the "Prussians"), were conquered by the Germans. The Old Prussians were an ethnic group related to the Latvian and Lithuanian Baltic peoples. The former German state of Prussia took its name from the Baltic Prussians, although it was led by Germans who had assimilated the Old Prussians; the old Prussian language was extinct by the 17th or early 18th century. The Slavic people of the Teutonic-controlled Baltic were assimilated into German culture and eventually there were many intermarriages of Slavic and German families, including amongst the Prussia's aristocracy known as the Junkers. Prussian military strategist Karl von Clausewitz is a famous German whose surname is of Slavic origin. Massive German settlement led to the assimilation of Baltic (Old Prussians) and Slavic (Wends) populations, who were exhausted by previous warfare.
At the same time, naval innovations led to a German domination of trade in the Baltic Sea and parts of Eastern Europe through the Hanseatic League. Along the trade routes, Hanseatic trade stations became centers of the German culture. German town law (Stadtrecht) was promoted by the presence of large, relatively wealthy German populations, their influence and political power. Thus people who would be considered "Germans", with a common culture, language, and worldview different from that of the surrounding rural peoples, colonized trading towns as far north of present-day Germany as Bergen (in Norway), Stockholm (in Sweden), and Vyborg (now in Russia). The Hanseatic League was not exclusively German in any ethnic sense: many towns who joined the league were outside the Holy Roman Empire and a number of them may only loosely be characterized as German. The Empire itself was not entirely German either. It had a multi-ethnic and multi-lingual structure, some of the smaller ethnicities and languages used at different times were Dutch, Italian, French, Czech and Polish.
By the Middle Ages, large numbers of Jews lived in the Holy Roman Empire and had assimilated into German culture, including many Jews who had previously assimilated into French culture and had spoken a mixed Judeo-French language. Upon assimilating into German culture, the Jewish German peoples incorporated major parts of the German language and elements of other European languages into a mixed language known as Yiddish. However tolerance and assimilation of Jews in German society suddenly ended during the Crusades with many Jews being forcefully expelled from Germany and Western Yiddish disappeared as a language in Germany over the centuries, with German Jewish people fully adopting the German language.
The Napoleonic Wars were the cause of the final dissolution of the Holy Roman Empire, and ultimately the cause for the quest for a German nation state in 19th-century German nationalism. After the Congress of Vienna, Austria and Prussia emerged as two competitors. Austria, trying to remain the dominant power in Central Europe, led the way in the terms of the Congress of Vienna. The Congress of Vienna was essentially conservative, assuring that little would change in Europe and preventing Germany from uniting. These terms came to a sudden halt following the Revolutions of 1848 and the Crimean War in 1856, paving the way for German unification in the 1860s. By the 1820s, large numbers of Jewish German women had intermarried with Christian German men and had converted to Christianity. Jewish German Eduard Lasker was a prominent German nationalist figure who promoted the unification of Germany in the mid-19th century.
In 1866, the feud between Austria and Prussia finally came to a head. There were several reasons behind this war. As German nationalism grew strongly inside the German Confederation and neither could decide on how Germany was going to be unified into a nation-state. The Austrians favoured the Greater Germany unification but were not willing to give up any of the non-German-speaking land inside of the Austrian Empire and take second place to Prussia. The Prussians however wanted to unify Germany as Little Germany primarily by the Kingdom of Prussia, whilst excluding Austria. In the final battle of the German war (Battle of Königgrätz) the Prussians successfully defeated the Austrians and succeeded in creating the North German Confederation.
In 1870, after France attacked Prussia, Prussia and its new allies in Southern Germany (among them Bavaria) were victorious in the Franco-Prussian War. It created the German Empire in 1871 as a German nation-state, effectively excluding the multi-ethnic Austrian Habsburg monarchy and Liechtenstein. Integrating the Austrians nevertheless remained a strong desire for many people of Germany and Austria, especially among the liberals, the social democrats and also the Catholics who were a minority within the Protestant Germany.
The Nazis, led by Adolf Hitler, attempted to unite all the people they claimed were "Germans" (Volksdeutsche) into one realm, including ethnic Germans in eastern Europe, many of whom had emigrated more than one hundred fifty years before and developed separate cultures in their new lands. This idea was initially welcomed by many ethnic Germans in Sudetenland, Austria, Poland, Danzig and western Lithuania, particularly the Germans from Klaipeda (Memel). The Swiss resisted the idea. They had viewed themselves as a distinctly separate nation since the Peace of Westphalia of 1648.
After World War II, eastern European countries such as the Soviet Union, Poland, Czechoslovakia, Hungary, Romania and Yugoslavia expelled the Germans from their territories. Many of those had inhabited these lands for centuries, developing a unique culture. Germans were also forced to leave the former eastern territories of Germany, which were annexed by Poland (Silesia, Pomerania, parts of Brandenburg and southern part of East Prussia) and the Soviet Union (northern part of East Prussia). Between 12 and 16,5 million ethnic Germans and German citizens were expelled westwards to allied-occupied Germany.
The event of the Protestant Reformation and the politics that ensued has been cited as the origins of German identity that arose in response to the spread of a common German language and literature. Early German national culture was developed through literary and religious figures including Martin Luther, Johann Wolfgang von Goethe and Friedrich Schiller. The concept of a German nation was developed by German philosopher Johann Gottfried Herder. The popularity of German identity arose in the aftermath of the French Revolution.
Persons who speak German as their first language, look German and whose families have lived in Germany for generations are considered "most German", followed by categories of diminishing Germanness such as Aussiedler (people of German ancestry whose families have lived in Eastern Europe but who have returned to Germany), Restdeutsche (people living in lands that have historically belonged to Germany but which is currently outside of Germany), Auswanderer (people whose families have emigrated from Germany and who still speak German), German speakers in German-speaking nations such as Austrians, and finally people of German emigrant background who no longer speak German.
The native language of Germans is German, a West Germanic language, related to and classified alongside English and Dutch, and sharing many similarities with the North Germanic and Scandinavian languages. Spoken by approximately 100 million native speakers, German is one of the world's major languages and the most widely spoken first language in the European Union. German has been replaced by English as the dominant language of science-related Nobel Prize laureates during the second half of the 20th century. It was a lingua franca in the Holy Roman Empire.
People of German origin are found in various places around the globe. United States is home to approximately 50 million German Americans or one third of the German diaspora, making it the largest centre of German-descended people outside Germany. Brazil is the second largest with 5 million people claiming German ancestry. Other significant centres are Canada, Argentina, South Africa and France each accounting for at least 1 million. While the exact number of German-descended people is difficult to calculate, the available data makes it safe to claim the number is exceeding 100 million people.
German philosophers have helped shape western philosophy from as early as the Middle Ages (Albertus Magnus). Later, Leibniz (17th century) and most importantly Kant played central roles in the history of philosophy. Kantianism inspired the work of Schopenhauer and Nietzsche as well as German idealism defended by Fichte and Hegel. Engels helped develop communist theory in the second half of the 19th century while Heidegger and Gadamer pursued the tradition of German philosophy in the 20th century. A number of German intellectuals were also influential in sociology, most notably Adorno, Habermas, Horkheimer, Luhmann, Simmel, Tönnies, and Weber. The University of Berlin founded in 1810 by linguist and philosopher Wilhelm von Humboldt served as an influential model for a number of modern western universities.
The work of David Hilbert and Max Planck was crucial to the foundation of modern physics, which Werner Heisenberg and Erwin Schrödinger developed further. They were preceded by such key physicists as Hermann von Helmholtz, Joseph von Fraunhofer, and Gabriel Daniel Fahrenheit, among others. Wilhelm Conrad Röntgen discovered X-rays, an accomplishment that made him the first winner of the Nobel Prize in Physics in 1901. The Walhalla temple for "laudable and distinguished Germans", features a number of scientists, and is located east of Regensburg, in Bavaria.
In the field of music, Germany claims some of the most renowned classical composers of the world including Bach, Mozart and Beethoven, who marked the transition between the Classical and Romantic eras in Western classical music. Other composers of the Austro-German tradition who achieved international fame include Brahms, Wagner, Haydn, Schubert, Händel, Schumann, Liszt, Mendelssohn Bartholdy, Johann Strauss II, Bruckner, Mahler, Telemann, Richard Strauss, Schoenberg, Orff, and most recently, Henze, Lachenmann, and Stockhausen.
As of 2008[update], Germany is the fourth largest music market in the world and has exerted a strong influence on Dance and Rock music, and pioneered trance music. Artists such as Herbert Grönemeyer, Scorpions, Rammstein, Nena, Dieter Bohlen, Tokio Hotel and Modern Talking have enjoyed international fame. German musicians and, particularly, the pioneering bands Tangerine Dream and Kraftwerk have also contributed to the development of electronic music. Germany hosts many large rock music festivals annually. The Rock am Ring festival is the largest music festival in Germany, and among the largest in the world. German artists also make up a large percentage of Industrial music acts, which is called Neue Deutsche Härte. Germany hosts some of the largest Goth scenes and festivals in the entire world, with events like Wave-Gothic-Treffen and M'era Luna Festival easily attracting up to 30,000 people. Amongst Germany's famous artists there are various Dutch entertainers, such as Johannes Heesters.
German cinema dates back to the very early years of the medium with the work of Max Skladanowsky. It was particularly influential during the years of the Weimar Republic with German expressionists such as Robert Wiene and Friedrich Wilhelm Murnau. The Nazi era produced mostly propaganda films although the work of Leni Riefenstahl still introduced new aesthetics in film. From the 1960s, New German Cinema directors such as Volker Schlöndorff, Werner Herzog, Wim Wenders, Rainer Werner Fassbinder placed West-German cinema back onto the international stage with their often provocative films, while the Deutsche Film-Aktiengesellschaft controlled film production in the GDR.
More recently, films such as Das Boot (1981), The Never Ending Story (1984) Run Lola Run (1998), Das Experiment (2001), Good Bye Lenin! (2003), Gegen die Wand (Head-on) (2004) and Der Untergang (Downfall) (2004) have enjoyed international success. In 2002 the Academy Award for Best Foreign Language Film went to Caroline Link's Nowhere in Africa, in 2007 to Florian Henckel von Donnersmarck's The Lives of Others. The Berlin International Film Festival, held yearly since 1951, is one of the world's foremost film and cinema festivals.
Roman Catholicism was the sole established religion in the Holy Roman Empire until the Reformation changed this drastically. In 1517, Martin Luther challenged the Catholic Church as he saw it as a corruption of Christian faith. Through this, he altered the course of European and world history and established Protestantism. The Thirty Years' War (1618–1648) was one of the most destructive conflicts in European history. The war was fought primarily in what is now Germany, and at various points involved most of the countries of Europe. The war was fought largely as a religious conflict between Protestants and Catholics in the Holy Roman Empire.
According to the latest nationwide census, Roman Catholics constituted 30.8% of the total population of Germany, followed by the Evangelical Protestants at 30.3%. Other religions, atheists or not specified constituted 38.8% of the population at the time. Among "others" are Protestants not included in Evangelical Church of Germany, and other Christians such as the Restorationist New Apostolic Church. Protestantism was more common among the citizens of Germany. The North and East Germany is predominantly Protestant, the South and West rather Catholic. Nowadays there is a non-religious majority in Hamburg and the East German states.
Sport forms an integral part of German life, as demonstrated by the fact that 27 million Germans are members of a sports club and an additional twelve million pursue such an activity individually. Football is by far the most popular sport, and the German Football Federation (Deutscher Fußballbund) with more than 6.3 million members is the largest athletic organisation in the country. It also attracts the greatest audience, with hundreds of thousands of spectators attending Bundesliga matches and millions more watching on television.
Since the 2006 FIFA World Cup, the internal and external evaluation of Germany's national image has changed. In the annual Nation Brands Index global survey, Germany became significantly and repeatedly more highly ranked after the tournament. People in 20 different states assessed the country's reputation in terms of culture, politics, exports, its people and its attractiveness to tourists, immigrants and investments. Germany has been named the world's second most valued nation among 50 countries in 2010. Another global opinion poll, for the BBC, revealed that Germany is recognised for the most positive influence in the world in 2010. A majority of 59% have a positive view of the country, while 14% have a negative view.
Pan-Germanism's origins began in the early 19th century following the Napoleonic Wars. The wars launched a new movement that was born in France itself during the French Revolution. Nationalism during the 19th century threatened the old aristocratic regimes. Many ethnic groups of Central and Eastern Europe had been divided for centuries, ruled over by the old Monarchies of the Romanovs and the Habsburgs. Germans, for the most part, had been a loose and disunited people since the Reformation when the Holy Roman Empire was shattered into a patchwork of states. The new German nationalists, mostly young reformers such as Johann Tillmann of East Prussia, sought to unite all the German-speaking and ethnic-German (Volksdeutsche) people.
By the 1860s the Kingdom of Prussia and the Austrian Empire were the two most powerful nations dominated by German-speaking elites. Both sought to expand their influence and territory. The Austrian Empire – like the Holy Roman Empire – was a multi-ethnic state, but German-speaking people there did not have an absolute numerical majority; the creation of the Austro-Hungarian Empire was one result of the growing nationalism of other ethnicities especially the Hungarians. Prussia under Otto von Bismarck would ride on the coat-tails of nationalism to unite all of modern-day Germany. The German Empire ("Second Reich") was created in 1871 following the proclamation of Wilhelm I as head of a union of German-speaking states, while disregarding millions of its non-German subjects who desired self-determination from German rule.
Following the defeat in World War I, influence of German-speaking elites over Central and Eastern Europe was greatly limited. At the treaty of Versailles Germany was substantially reduced in size. Austria-Hungary was split up. Rump-Austria, which to a certain extent corresponded to the German-speaking areas of Austria-Hungary (a complete split into language groups was impossible due to multi-lingual areas and language-exclaves) adopted the name "German-Austria" (German: Deutschösterreich). The name German-Austria was forbidden by the victorious powers of World War I. Volga Germans living in the Soviet Union were interned in gulags or forcibly relocated during the Second World War.
For decades after the Second World War, any national symbol or expression was a taboo. However, the Germans are becoming increasingly patriotic. During a study in 2009, in which some 2,000 German citizens age 14 and upwards filled out a questionnaire, nearly 60% of those surveyed agreed with the sentiment "I'm proud to be German." And 78%, if free to choose their nation, would opt for German nationality with "near or absolute certainty". Another study in 2009, carried out by the Identity Foundation in Düsseldorf, showed that 73% of the Germans were proud of their country, twice more than 8 years earlier. According to Eugen Buss, a sociology professor at the University of Hohenheim, there's an ongoing normalisation and more and more Germans are becoming openly proud of their country.
In the midst of the European sovereign-debt crisis, Radek Sikorski, Poland's Foreign Minister, stated in November 2011, "I will probably be the first Polish foreign minister in history to say so, but here it is: I fear German power less than I am beginning to fear German inactivity. You have become Europe's indispensable nation." According to Jacob Heilbrunn, a senior editor at The National Interest, such a statement is unprecedented when taking into consideration Germany's history. "This was an extraordinary statement from a top official of a nation that was ravaged by Germany during World War II. And it reflects a profound shift taking place throughout Germany and Europe about Berlin's position at the center of the Continent." Heilbrunn believes that the adage, "what was good for Germany was bad for the European Union" has been supplanted by a new mentality—what is in the interest of Germany is also in the interest of its neighbors. The evolution in Germany's national identity stems from focusing less on its Nazi past and more on its Prussian history, which many Germans believe was betrayed—and not represented—by Nazism. The evolution is further precipitated by Germany's conspicuous position as Europe's strongest economy. Indeed, this German sphere of influence has been welcomed by the countries that border it, as demonstrated by Polish foreign minister Radek Sikorski's effusive praise for his country's western neighbor. This shift in thinking is boosted by a newer generation of Germans who see World War II as a distant memory.
Dwight David "Ike" Eisenhower (/ˈaɪzənˌhaʊ.ər/ EYE-zən-HOW-ər; October 14, 1890 – March 28, 1969) was an American politician and general who served as the 34th President of the United States from 1953 until 1961. He was a five-star general in the United States Army during World War II and served as Supreme Commander of the Allied Forces in Europe. He was responsible for planning and supervising the invasion of North Africa in Operation Torch in 1942–43 and the successful invasion of France and Germany in 1944–45 from the Western Front. In 1951, he became the first Supreme Commander of NATO.
Eisenhower's main goals in office were to keep pressure on the Soviet Union and reduce federal deficits. In the first year of his presidency, he threatened the use of nuclear weapons in an effort to conclude the Korean War; his New Look policy of nuclear deterrence prioritized inexpensive nuclear weapons while reducing funding for conventional military forces. He ordered coups in Iran and Guatemala. Eisenhower refused to give major aid to help France in Vietnam. He gave strong financial support to the new nation of South Vietnam. Congress agreed to his request in 1955 for the Formosa Resolution, which obliged the U.S. to militarily support the pro-Western Republic of China in Taiwan and continue the isolation of the People's Republic of China.
After the Soviet Union launched the world's first artificial satellite in 1957, Eisenhower authorized the establishment of NASA, which led to the space race. During the Suez Crisis of 1956, Eisenhower condemned the Israeli, British and French invasion of Egypt, and forced them to withdraw. He also condemned the Soviet invasion during the Hungarian Revolution of 1956 but took no action. In 1958, Eisenhower sent 15,000 U.S. troops to Lebanon to prevent the pro-Western government from falling to a Nasser-inspired revolution. Near the end of his term, his efforts to set up a summit meeting with the Soviets collapsed because of the U-2 incident. In his January 17, 1961 farewell address to the nation, Eisenhower expressed his concerns about the dangers of massive military spending, particularly deficit spending and government contracts to private military manufacturers, and coined the term "military–industrial complex".
On the domestic front, he covertly opposed Joseph McCarthy and contributed to the end of McCarthyism by openly invoking the modern expanded version of executive privilege. He otherwise left most political activity to his Vice President, Richard Nixon. He was a moderate conservative who continued New Deal agencies and expanded Social Security. He also launched the Interstate Highway System, the Defense Advanced Research Projects Agency (DARPA), the establishment of strong science education via the National Defense Education Act, and encouraged peaceful use of nuclear power via amendments to the Atomic Energy Act.
His parents set aside specific times at breakfast and at dinner for daily family Bible reading. Chores were regularly assigned and rotated among all the children, and misbehavior was met with unequivocal discipline, usually from David. His mother, previously a member (with David) of the River Brethren sect of the Mennonites, joined the International Bible Students Association, later known as Jehovah's Witnesses. The Eisenhower home served as the local meeting hall from 1896 to 1915, though Eisenhower never joined the International Bible Students. His later decision to attend West Point saddened his mother, who felt that warfare was "rather wicked," but she did not overrule him. While speaking of himself in 1948, Eisenhower said he was "one of the most deeply religious men I know" though unattached to any "sect or organization". He was baptized in the Presbyterian Church in 1953.
Eisenhower attended Abilene High School and graduated with the class of 1909. As a freshman, he injured his knee and developed a leg infection that extended into his groin, and which his doctor diagnosed as life-threatening. The doctor insisted that the leg be amputated but Dwight refused to allow it, and miraculously recovered, though he had to repeat his freshman year. He and brother Edgar both wanted to attend college, though they lacked the funds. They made a pact to take alternate years at college while the other worked to earn the tuitions.
Edgar took the first turn at school, and Dwight was employed as a night supervisor at the Belle Springs Creamery. Edgar asked for a second year, Dwight consented and worked for a second year. At that time, a friend "Swede" Hazlet was applying to the Naval Academy and urged Dwight to apply to the school, since no tuition was required. Eisenhower requested consideration for either Annapolis or West Point with his U.S. Senator, Joseph L. Bristow. Though Eisenhower was among the winners of the entrance-exam competition, he was beyond the age limit for the Naval Academy. He then accepted an appointment to West Point in 1911.
The Eisenhowers had two sons. Doud Dwight "Icky" Eisenhower was born September 24, 1917, and died of scarlet fever on January 2, 1921, at the age of three; Eisenhower was mostly reticent to discuss his death. Their second son, John Eisenhower (1922–2013), was born in Denver Colorado. John served in the United States Army, retired as a brigadier general, became an author and served as U.S. Ambassador to Belgium from 1969 to 1971. Coincidentally, John graduated from West Point on D-Day, June 6, 1944. He married Barbara Jean Thompson on June 10, 1947. John and Barbara had four children: David, Barbara Ann, Susan Elaine and Mary Jean. David, after whom Camp David is named, married Richard Nixon's daughter Julie in 1968. John died on December 21, 2013.
Eisenhower was a golf enthusiast later in life, and joined the Augusta National Golf Club in 1948. He played golf frequently during and after his presidency and was unreserved in expressing his passion for the game, to the point of golfing during winter; he ordered his golf balls painted black so he could see them better against snow on the ground. He had a small, basic golf facility installed at Camp David, and became close friends with the Augusta National Chairman Clifford Roberts, inviting Roberts to stay at the White House on several occasions. Roberts, an investment broker, also handled the Eisenhower family's investments. Roberts also advised Eisenhower on tax aspects of publishing his memoirs, which proved financially lucrative.
After golf, oil painting was Eisenhower's second hobby. While at Columbia University, Eisenhower began the art after watching Thomas E. Stephens paint Mamie's portrait. Eisenhower painted about 260 oils during the last 20 years of his life to relax, mostly landscapes but also portraits of subjects such as Mamie, their grandchildren, General Montgomery, George Washington, and Abraham Lincoln. Wendy Beckett stated that Eisenhower's work, "simple and earnest, rather cause us to wonder at the hidden depths of this reticent president". A conservative in both art and politics, he in a 1962 speech denounced modern art as "a piece of canvas that looks like a broken-down Tin Lizzie, loaded with paint, has been driven over it."
Angels in the Outfield was Eisenhower's favorite movie. His favorite reading material for relaxation were the Western novels of Zane Grey. With his excellent memory and ability to focus, Eisenhower was skilled at card games. He learned poker, which he called his "favorite indoor sport," in Abilene. Eisenhower recorded West Point classmates' poker losses for payment after graduation, and later stopped playing because his opponents resented having to pay him. A classmate reported that after learning to play contract bridge at West Point, Eisenhower played the game six nights a week for five months.
When the U.S. entered World War I he immediately requested an overseas assignment but was again denied and then assigned to Ft. Leavenworth, Kansas. In February 1918 he was transferred to Camp Meade in Maryland with the 65th Engineers. His unit was later ordered to France but to his chagrin he received orders for the new tank corps, where he was promoted to brevet Lieutenant Colonel in the National Army. He commanded a unit that trained tank crews at Camp Colt – his first command – at the site of "Pickett's Charge" on the Gettysburg, Pennsylvania Civil War battleground. Though Eisenhower and his tank crews never saw combat, he displayed excellent organizational skills, as well as an ability to accurately assess junior officers' strengths and make optimal placements of personnel.
Once again his spirits were raised when the unit under his command received orders overseas to France. This time his wishes were thwarted when the armistice was signed, just a week before departure. Completely missing out on the warfront left him depressed and bitter for a time, despite being given the Distinguished Service Medal for his work at home.[citation needed] In World War II, rivals who had combat service in the first great war (led by Gen. Bernard Montgomery) sought to denigrate Eisenhower for his previous lack of combat duty, despite his stateside experience establishing a camp, completely equipped, for thousands of troops, and developing a full combat training schedule.
He assumed duties again at Camp Meade, Maryland, commanding a battalion of tanks, where he remained until 1922. His schooling continued, focused on the nature of the next war and the role of the tank in it. His new expertise in tank warfare was strengthened by a close collaboration with George S. Patton, Sereno E. Brett, and other senior tank leaders. Their leading-edge ideas of speed-oriented offensive tank warfare were strongly discouraged by superiors, who considered the new approach too radical and preferred to continue using tanks in a strictly supportive role for the infantry. Eisenhower was even threatened with court martial for continued publication of these proposed methods of tank deployment, and he relented.
From 1920, Eisenhower served under a succession of talented generals – Fox Conner, John J. Pershing, Douglas MacArthur and George Marshall. He first became executive officer to General Conner in the Panama Canal Zone, where, joined by Mamie, he served until 1924. Under Conner's tutelage, he studied military history and theory (including Carl von Clausewitz's On War), and later cited Conner's enormous influence on his military thinking, saying in 1962 that "Fox Conner was the ablest man I ever knew." Conner's comment on Eisenhower was, "[He] is one of the most capable, efficient and loyal officers I have ever met." On Conner's recommendation, in 1925–26 he attended the Command and General Staff College at Fort Leavenworth, Kansas, where he graduated first in a class of 245 officers. He then served as a battalion commander at Fort Benning, Georgia, until 1927.
During the late 1920s and early 1930s, Eisenhower's career in the post-war army stalled somewhat, as military priorities diminished; many of his friends resigned for high-paying business jobs. He was assigned to the American Battle Monuments Commission directed by General Pershing, and with the help of his brother Milton Eisenhower, then a journalist at the Agriculture Department, he produced a guide to American battlefields in Europe. He then was assigned to the Army War College and graduated in 1928. After a one-year assignment in France, Eisenhower served as executive officer to General George V. Mosely, Assistant Secretary of War, from 1929 to February 1933. Major Dwight D. Eisenhower graduated from the Army Industrial College (Washington, DC) in 1933 and later served on the faculty (it was later expanded to become the Industrial College of the Armed Services and is now known as the Dwight D. Eisenhower School for National Security and Resource Strategy).
His primary duty was planning for the next war, which proved most difficult in the midst of the Great Depression. He then was posted as chief military aide to General MacArthur, Army Chief of Staff. In 1932, he participated in the clearing of the Bonus March encampment in Washington, D.C. Although he was against the actions taken against the veterans and strongly advised MacArthur against taking a public role in it, he later wrote the Army's official incident report, endorsing MacArthur's conduct.
Historians have concluded that this assignment provided valuable preparation for handling the challenging personalities of Winston Churchill, George S. Patton, George Marshall, and General Montgomery during World War II. Eisenhower later emphasized that too much had been made of the disagreements with MacArthur, and that a positive relationship endured. While in Manila, Mamie suffered a life-threatening stomach ailment but recovered fully. Eisenhower was promoted to the rank of permanent lieutenant colonel in 1936. He also learned to fly, making a solo flight over the Philippines in 1937 and obtained his private pilot's license in 1939 at Fort Lewis. Also around this time, he was offered a post by the Philippine Commonwealth Government, namely by then Philippine President Manuel L. Quezon on recommendations by MacArthur, to become the chief of police of a new capital being planned, now named Quezon City, but he declined the offer.
Eisenhower returned to the U.S. in December 1939 and was assigned as a battalion commander and regimental executive officer of the 15th Infantry at Fort Lewis, Washington. In March 1941 he was promoted to colonel and assigned as chief of staff of the newly activated IX Corps under Major General Kenyon Joyce. In June 1941, he was appointed Chief of Staff to General Walter Krueger, Commander of the 3rd Army, at Fort Sam Houston in San Antonio, Texas. After successfully participating in the Louisiana Maneuvers, he was promoted to brigadier general on October 3, 1941. Although his administrative abilities had been noticed, on the eve of the U.S. entry into World War II he had never held an active command above a battalion and was far from being considered by many as a potential commander of major operations.
After the Japanese attack on Pearl Harbor, Eisenhower was assigned to the General Staff in Washington, where he served until June 1942 with responsibility for creating the major war plans to defeat Japan and Germany. He was appointed Deputy Chief in charge of Pacific Defenses under the Chief of War Plans Division (WPD), General Leonard T. Gerow, and then succeeded Gerow as Chief of the War Plans Division. Next, he was appointed Assistant Chief of Staff in charge of the new Operations Division (which replaced WPD) under Chief of Staff General George C. Marshall, who spotted talent and promoted accordingly.
At the end of May 1942, Eisenhower accompanied Lt. Gen. Henry H. Arnold, commanding general of the Army Air Forces, to London to assess the effectiveness of the theater commander in England, Maj. Gen. James E. Chaney. He returned to Washington on June 3 with a pessimistic assessment, stating he had an "uneasy feeling" about Chaney and his staff. On June 23, 1942, he returned to London as Commanding General, European Theater of Operations (ETOUSA), based in London and with a house on Coombe, Kingston upon Thames, and replaced Chaney. He was promoted to lieutenant general on July 7.
In November 1942, he was also appointed Supreme Commander Allied Expeditionary Force of the North African Theater of Operations (NATOUSA) through the new operational Headquarters Allied (Expeditionary) Force Headquarters (A(E)FHQ). The word "expeditionary" was dropped soon after his appointment for security reasons. The campaign in North Africa was designated Operation Torch and was planned underground within the Rock of Gibraltar. Eisenhower was the first non-British person to command Gibraltar in 200 years.
French cooperation was deemed necessary to the campaign, and Eisenhower encountered a "preposterous situation" with the multiple rival factions in France. His primary objective was to move forces successfully into Tunisia, and intending to facilitate that objective, he gave his support to François Darlan as High Commissioner in North Africa, despite Darlan's previous high offices of state in Vichy France and his continued role as commander-in-chief of the French armed forces. The Allied leaders were "thunderstruck" by this from a political standpoint, though none of them had offered Eisenhower guidance with the problem in the course of planning the operation. Eisenhower was severely criticized for the move. Darlan was assassinated on December 24 by Fernand Bonnier de La Chapelle. Eisenhower did not take action to prevent the arrest and extrajudicial execution of Bonnier de La Chapelle by associates of Darlan acting without authority from either Vichy or the Allies, considering it a criminal rather than a military matter. Eisenhower later appointed General Henri Giraud as High Commissioner, who had been installed by the Allies as Darlan's commander-in-chief, and who had refused to postpone the execution.
Operation Torch also served as a valuable training ground for Eisenhower's combat command skills; during the initial phase of Generalfeldmarschall Erwin Rommel's move into the Kasserine Pass, Eisenhower created some confusion in the ranks by some interference with the execution of battle plans by his subordinates. He also was initially indecisive in his removal of Lloyd Fredendall, commanding U.S. II Corps. He became more adroit in such matters in later campaigns. In February 1943, his authority was extended as commander of AFHQ across the Mediterranean basin to include the British Eighth Army, commanded by General Sir Bernard Montgomery. The Eighth Army had advanced across the Western Desert from the east and was ready for the start of the Tunisia Campaign. Eisenhower gained his fourth star and gave up command of ETOUSA to become commander of NATOUSA.
After the capitulation of Axis forces in North Africa, Eisenhower oversaw the highly successful invasion of Sicily. Once Mussolini, the Italian leader, had fallen in Italy, the Allies switched their attention to the mainland with Operation Avalanche. But while Eisenhower argued with President Roosevelt and British Prime Minister Churchill, who both insisted on unconditional terms of surrender in exchange for helping the Italians, the Germans pursued an aggressive buildup of forces in the country – making the job more difficult, by adding 19 divisions and initially outnumbering the Allied forces 2 to 1; nevertheless, the invasion of Italy was highly successful.
In December 1943, President Roosevelt decided that Eisenhower – not Marshall – would be Supreme Allied Commander in Europe. The following month, he resumed command of ETOUSA and the following month was officially designated as the Supreme Allied Commander of the Allied Expeditionary Force (SHAEF), serving in a dual role until the end of hostilities in Europe in May 1945. He was charged in these positions with planning and carrying out the Allied assault on the coast of Normandy in June 1944 under the code name Operation Overlord, the liberation of Western Europe and the invasion of Germany.
Eisenhower, as well as the officers and troops under him, had learned valuable lessons in their previous operations, and their skills had all strengthened in preparation for the next most difficult campaign against the Germans—a beach landing assault. His first struggles, however, were with Allied leaders and officers on matters vital to the success of the Normandy invasion; he argued with Roosevelt over an essential agreement with De Gaulle to use French resistance forces in covert and sabotage operations against the Germans in advance of Overlord. Admiral Ernest J. King fought with Eisenhower over King's refusal to provide additional landing craft from the Pacific. He also insisted that the British give him exclusive command over all strategic air forces to facilitate Overlord, to the point of threatening to resign unless Churchill relented, as he did. Eisenhower then designed a bombing plan in France in advance of Overlord and argued with Churchill over the latter's concern with civilian casualties; de Gaulle interjected that the casualties were justified in shedding the yoke of the Germans, and Eisenhower prevailed. He also had to skillfully manage to retain the services of the often unruly George S. Patton, by severely reprimanding him when Patton earlier had slapped a subordinate, and then when Patton gave a speech in which he made improper comments about postwar policy.
The D-Day Normandy landings on June 6, 1944, were costly but successful. A month later, the invasion of Southern France took place, and control of forces in the southern invasion passed from the AFHQ to the SHAEF. Many prematurely considered that victory in Europe would come by summer's end—however the Germans did not capitulate for almost a year. From then until the end of the war in Europe on May 8, 1945, Eisenhower, through SHAEF, commanded all Allied forces, and through his command of ETOUSA had administrative command of all U.S. forces on the Western Front north of the Alps. He was ever mindful of the inevitable loss of life and suffering that would be experienced on an individual level by the troops under his command and their families. This prompted him to make a point of visiting every division involved in the invasion. Eisenhower's sense of responsibility was underscored by his draft of a statement to be issued if the invasion failed. It has been called one of the great speeches of history:
Once the coastal assault had succeeded, Eisenhower insisted on retaining personal control over the land battle strategy, and was immersed in the command and supply of multiple assaults through France on Germany. Field Marshal Montgomery insisted priority be given to his 21st Army Group's attack being made in the north, while Generals Bradley (12th U.S. Army Group) and Devers (Sixth U.S. Army Group) insisted they be given priority in the center and south of the front (respectively). Eisenhower worked tirelessly to address the demands of the rival commanders to optimize Allied forces, often by giving them tactical, though sometimes ineffective, latitude; many historians conclude this delayed the Allied victory in Europe. However, due to Eisenhower's persistence, the pivotal supply port at Antwerp was successfully, albeit belatedly, opened in late 1944, and victory became a more distinct probability.
In recognition of his senior position in the Allied command, on December 20, 1944, he was promoted to General of the Army, equivalent to the rank of Field Marshal in most European armies. In this and the previous high commands he held, Eisenhower showed his great talents for leadership and diplomacy. Although he had never seen action himself, he won the respect of front-line commanders. He interacted adeptly with allies such as Winston Churchill, Field Marshal Bernard Montgomery and General Charles de Gaulle. He had serious disagreements with Churchill and Montgomery over questions of strategy, but these rarely upset his relationships with them. He dealt with Soviet Marshal Zhukov, his Russian counterpart, and they became good friends.
The Germans launched a surprise counter offensive, in the Battle of the Bulge in December 1944, which the Allies turned back in early 1945 after Eisenhower repositioned his armies and improved weather allowed the Air Force to engage. German defenses continued to deteriorate on both the eastern front with the Soviets and the western front with the Allies. The British wanted Berlin, but Eisenhower decided it would be a military mistake for him to attack Berlin, and said orders to that effect would have to be explicit. The British backed down, but then wanted Eisenhower to move into Czechoslovakia for political reasons. Washington refused to support Churchill's plan to use Eisenhower's army for political maneuvers against Moscow. The actual division of Germany followed the lines that Roosevelt, Churchill and Stalin had previously agreed upon. The Soviet Red Army captured Berlin in a very large-scale bloody battle, and the Germans finally surrendered on May 7, 1945.
Following the German unconditional surrender, Eisenhower was appointed Military Governor of the U.S. Occupation Zone, based at the IG Farben Building in Frankfurt am Main. He had no responsibility for the other three zones, controlled by Britain, France and the Soviet Union, except for the city of Berlin, which was managed by the Four-Power Authorities through the Allied Kommandatura as the governing body. Upon discovery of the Nazi concentration camps, he ordered camera crews to document evidence of the atrocities in them for use in the Nuremberg Trials. He reclassified German prisoners of war (POWs) in U.S. custody as Disarmed Enemy Forces (DEFs), who were no longer subject to the Geneva Convention. Eisenhower followed the orders laid down by the Joint Chiefs of Staff (JCS) in directive JCS 1067, but softened them by bringing in 400,000 tons of food for civilians and allowing more fraternization. In response to the devastation in Germany, including food shortages and an influx of refugees, he arranged distribution of American food and medical equipment. His actions reflected the new American attitudes of the German people as Nazi victims not villains, while aggressively purging the ex-Nazis.
In November 1945, Eisenhower returned to Washington to replace Marshall as Chief of Staff of the Army. His main role was rapid demobilization of millions of soldiers, a slow job that was delayed by lack of shipping. Eisenhower was convinced in 1946 that the Soviet Union did not want war and that friendly relations could be maintained; he strongly supported the new United Nations and favored its involvement in the control of atomic bombs. However, in formulating policies regarding the atomic bomb and relations with the Soviets, Truman was guided by the U.S. State Department and ignored Eisenhower and the Pentagon. Indeed, Eisenhower had opposed the use of the atomic bomb against the Japanese, writing, "First, the Japanese were ready to surrender and it wasn't necessary to hit them with that awful thing. Second, I hated to see our country be the first to use such a weapon." Initially, Eisenhower was characterized by hopes for cooperation with the Soviets. He even visited Warsaw in 1945. Invited by Bolesław Bierut and decorated with the highest military decoration, he was shocked by the scale of destruction in the city. However, by mid-1947, as East–West tensions over economic recovery in Germany and the Greek Civil War escalated, Eisenhower gave up and agreed with a containment policy to stop Soviet expansion.
In June 1943 a visiting politician had suggested to Eisenhower that he might become President of the United States after the war. Believing that a general should not participate in politics, one author later wrote that "figuratively speaking, [Eisenhower] kicked his political-minded visitor out of his office". As others asked him about his political future, Eisenhower told one that he could not imagine wanting to be considered for any political job "from dogcatcher to Grand High Supreme King of the Universe", and another that he could not serve as Army Chief of Staff if others believed he had political ambitions. In 1945 Truman told Eisenhower during the Potsdam Conference that if desired, the president would help the general win the 1948 election, and in 1947 he offered to run as Eisenhower's running mate on the Democratic ticket if MacArthur won the Republican nomination.
As the election approached, other prominent citizens and politicians from both parties urged Eisenhower to run for president. In January 1948, after learning of plans in New Hampshire to elect delegates supporting him for the forthcoming Republican National Convention, Eisenhower stated through the Army that he was "not available for and could not accept nomination to high political office"; "life-long professional soldiers", he wrote, "in the absence of some obvious and overriding reason, [should] abstain from seeking high political office". Eisenhower maintained no political party affiliation during this time. Many believed he was forgoing his only opportunity to be president; Republican Thomas E. Dewey was considered the other probable winner, would presumably serve two terms, and Eisenhower, at age 66 in 1956, would then be too old.
In 1948, Eisenhower became President of Columbia University, an Ivy League university in New York City. The assignment was described as not being a good fit in either direction. During that year Eisenhower's memoir, Crusade in Europe, was published. Critics regarded it as one of the finest U.S. military memoirs, and it was a major financial success as well. Eisenhower's profit on the book was substantially aided by an unprecedented ruling by the U.S. Department of the Treasury that Eisenhower was not a professional writer, but rather, marketing the lifetime asset of his experiences, and thus he only had to pay capital gains tax on his $635,000 advance instead of the much higher personal tax rate. This ruling saved Eisenhower about $400,000.
Eisenhower's stint as the president of Columbia University was punctuated by his activity within the Council on Foreign Relations, a study group he led as president concerning the political and military implications of the Marshall Plan, and The American Assembly, Eisenhower's "vision of a great cultural center where business, professional and governmental leaders could meet from time to time to discuss and reach conclusions concerning problems of a social and political nature". His biographer Blanche Wiesen Cook suggested that this period served as "the political education of General Eisenhower", since he had to prioritize wide-ranging educational, administrative, and financial demands for the university. Through his involvement in the Council on Foreign Relations, he also gained exposure to economic analysis, which would become the bedrock of his understanding in economic policy. "Whatever General Eisenhower knows about economics, he has learned at the study group meetings," one Aid to Europe member claimed.
Within months of beginning his tenure as the president of the university, Eisenhower was requested to advise U.S. Secretary of Defense James Forrestal on the unification of the armed services. About six months after his appointment, he became the informal Chairman of the Joint Chiefs of Staff in Washington. Two months later he fell ill, and he spent over a month in recovery at the Augusta National Golf Club. He returned to his post in New York in mid-May, and in July 1949 took a two-month vacation out-of-state. Because the American Assembly had begun to take shape, he traveled around the country during mid-to-late 1950, building financial support from Columbia Associates, an alumni association.
The contacts gained through university and American Assembly fund-raising activities would later become important supporters in Eisenhower's bid for the Republican party nomination and the presidency. Meanwhile, Columbia University's liberal faculty members became disenchanted with the university president's ties to oilmen and businessmen, including Leonard McCollum, the president of Continental Oil; Frank Abrams, the chairman of Standard Oil of New Jersey; Bob Kleberg, the president of the King Ranch; H. J. Porter, a Texas oil executive; Bob Woodruff, the president of the Coca-Cola Corporation; and Clarence Francis, the chairman of General Foods.
The trustees of Columbia University refused to accept Eisenhower's resignation in December 1950, when he took an extended leave from the university to become the Supreme Commander of the North Atlantic Treaty Organization (NATO), and he was given operational command of NATO forces in Europe. Eisenhower retired from active service as an Army general on May 31, 1952, and he resumed his presidency of Columbia. He held this position until January 20, 1953, when he became the President of the United States.
President Truman, symbolizing a broad-based desire for an Eisenhower candidacy for president, again in 1951 pressed him to run for the office as a Democrat. It was at this time that Eisenhower voiced his disagreements with the Democratic party and declared himself and his family to be Republicans. A "Draft Eisenhower" movement in the Republican Party persuaded him to declare his candidacy in the 1952 presidential election to counter the candidacy of non-interventionist Senator Robert A. Taft. The effort was a long struggle; Eisenhower had to be convinced that political circumstances had created a genuine duty for him to offer himself as a candidate, and that there was a mandate from the populace for him to be their President. Henry Cabot Lodge, who served as his campaign manager, and others succeeded in convincing him, and in June 1952 he resigned his command at NATO to campaign full-time. Eisenhower defeated Taft for the nomination, having won critical delegate votes from Texas. Eisenhower's campaign was noted for the simple but effective slogan, "I Like Ike". It was essential to his success that Eisenhower express opposition to Roosevelt's policy at Yalta and against Truman's policies in Korea and China—matters in which he had once participated. In defeating Taft for the nomination, it became necessary for Eisenhower to appease the right wing Old Guard of the Republican Party; his selection of Richard M. Nixon as the Vice-President on the ticket was designed in part for that purpose. Nixon also provided a strong anti-communist presence as well as some youth to counter Ike's more advanced age.
In the general election, against the advice of his advisors, Eisenhower insisted on campaigning in the South, refusing to surrender the region to the Democratic Party. The campaign strategy, dubbed "K1C2", was to focus on attacking the Truman and Roosevelt administrations on three issues: Korea, Communism and corruption. In an effort to accommodate the right, he stressed that the liberation of Eastern Europe should be by peaceful means only; he also distanced himself from his former boss President Truman.
Two controversies during the campaign tested him and his staff, but did not affect the campaign. One involved a report that Nixon had improperly received funds from a secret trust. Nixon spoke out adroitly to avoid potential damage, but the matter permanently alienated the two candidates. The second issue centered on Eisenhower's relented decision to confront the controversial methods of Joseph McCarthy on his home turf in a Wisconsin appearance. Just two weeks prior to the election, Eisenhower vowed to go to Korea and end the war there. He promised to maintain a strong commitment against Communism while avoiding the topic of NATO; finally, he stressed a corruption-free, frugal administration at home.
Eisenhower was the last president born in the 19th century, and at age 62, was the oldest man elected President since James Buchanan in 1856 (President Truman stood at 64 in 1948 as the incumbent president at the time of his election four years earlier). Eisenhower was the only general to serve as President in the 20th century and the most recent President to have never held elected office prior to the Presidency (The other Presidents who did not have prior elected office were Zachary Taylor, Ulysses S. Grant, William Howard Taft and Herbert Hoover).
Due to a complete estrangement between the two as a result of campaigning, Truman and Eisenhower had minimal discussions about the transition of administrations. After selecting his budget director, Joseph M. Dodge, Eisenhower asked Herbert Brownell and Lucius Clay to make recommendations for his cabinet appointments. He accepted their recommendations without exception; they included John Foster Dulles and George M. Humphrey with whom he developed his closest relationships, and one woman, Oveta Culp Hobby. Eisenhower's cabinet, consisting of several corporate executives and one labor leader, was dubbed by one journalist, "Eight millionaires and a plumber." The cabinet was notable for its lack of personal friends, office seekers, or experienced government administrators. He also upgraded the role of the National Security Council in planning all phases of the Cold War.
Prior to his inauguration, Eisenhower led a meeting of advisors at Pearl Harbor addressing foremost issues; agreed objectives were to balance the budget during his term, to bring the Korean War to an end, to defend vital interests at lower cost through nuclear deterrent, and to end price and wage controls. Eisenhower also conducted the first pre-inaugural cabinet meeting in history in late 1952; he used this meeting to articulate his anti-communist Russia policy. His inaugural address, as well, was exclusively devoted to foreign policy and included this same philosophy, as well as a commitment to foreign trade and the United Nations.
Throughout his presidency, Eisenhower adhered to a political philosophy of dynamic conservatism. A self-described "progressive conservative," he continued all the major New Deal programs still in operation, especially Social Security. He expanded its programs and rolled them into a new cabinet-level agency, the Department of Health, Education and Welfare, while extending benefits to an additional ten million workers. He implemented integration in the Armed Services in two years, which had not been completed under Truman.
As the 1954 congressional elections approached, and it became evident that the Republicans were in danger of losing their thin majority in both houses, Eisenhower was among those blaming the Old Guard for the losses, and took up the charge to stop suspected efforts by the right wing to take control of the GOP. Eisenhower then articulated his position as a moderate, progressive Republican: "I have just one purpose ... and that is to build up a strong progressive Republican Party in this country. If the right wing wants a fight, they are going to get it ... before I end up, either this Republican Party will reflect progressivism or I won't be with them anymore."
Initially Eisenhower planned on serving only one term, but as with other decisions, he maintained a position of maximum flexibility in case leading Republicans wanted him to run again. During his recovery from a heart attack late in 1955, he huddled with his closest advisors to evaluate the GOP's potential candidates; the group, in addition to his doctor, concluded a second term was well advised, and he announced in February 1956 he would run again. Eisenhower was publicly noncommittal about Nixon's repeating as the Vice President on his ticket; the question was an especially important one in light of his heart condition. He personally favored Robert B. Anderson, a Democrat, who rejected his offer; Eisenhower then resolved to leave the matter in the hands of the party. In 1956, Eisenhower faced Adlai Stevenson again and won by an even larger landslide, with 457 of 531 electoral votes and 57.6% of the popular vote. The level of campaigning was curtailed out of health considerations.
Eisenhower's goal to create improved highways was influenced by difficulties encountered during his involvement in the U.S. Army's 1919 Transcontinental Motor Convoy. He was assigned as an observer for the mission, which involved sending a convoy of U.S. Army vehicles coast to coast. His subsequent experience with encountering German autobahn limited-access road systems during the concluding stages of World War II convinced him of the benefits of an Interstate Highway System. Noticing the improved ability to move logistics throughout the country, he thought an Interstate Highway System in the U.S. would not only be beneficial for military operations, but provide a measure of continued economic growth. The legislation initially stalled in the Congress over the issuance of bonds to finance the project, but the legislative effort was renewed and the law was signed by Eisenhower in June 1956.
In 1953, the Republican Party's Old Guard presented Eisenhower with a dilemma by insisting he disavow the Yalta Agreements as beyond the constitutional authority of the Executive Branch; however, the death of Joseph Stalin in March 1953 made the matter a practical moot point. At this time Eisenhower gave his Chance for Peace speech in which he attempted, unsuccessfully, to forestall the nuclear arms race with the Soviet Union by suggesting multiple opportunities presented by peaceful uses of nuclear materials. Biographer Stephen Ambrose opined that this was the best speech of Eisenhower's presidency.
The U.N. speech was well received but the Soviets never acted upon it, due to an overarching concern for the greater stockpiles of nuclear weapons in the U.S. arsenal. Indeed, Eisenhower embarked upon a greater reliance on the use of nuclear weapons, while reducing conventional forces, and with them the overall defense budget, a policy formulated as a result of Project Solarium and expressed in NSC 162/2. This approach became known as the "New Look", and was initiated with defense cuts in late 1953.
In 1955 American nuclear arms policy became one aimed primarily at arms control as opposed to disarmament. The failure of negotiations over arms until 1955 was due mainly to the refusal of the Russians to permit any sort of inspections. In talks located in London that year, they expressed a willingness to discuss inspections; the tables were then turned on Eisenhower, when he responded with an unwillingness on the part of the U.S. to permit inspections. In May of that year the Russians agreed to sign a treaty giving independence to Austria, and paved the way for a Geneva summit with the U.S., U.K. and France. At the Geneva Conference Eisenhower presented a proposal called "Open Skies" to facilitate disarmament, which included plans for Russia and the U.S. to provide mutual access to each other's skies for open surveillance of military infrastructure. Russian leader Nikita Khrushchev dismissed the proposal out of hand.
In 1954, Eisenhower articulated the domino theory in his outlook towards communism in Southeast Asia and also in Central America. He believed that if the communists were allowed to prevail in Vietnam, this would cause a succession of countries to fall to communism, from Laos through Malaysia and Indonesia ultimately to India. Likewise, the fall of Guatemala would end with the fall of neighboring Mexico. That year the loss of North Vietnam to the communists and the rejection of his proposed European Defence Community (EDC) were serious defeats, but he remained optimistic in his opposition to the spread of communism, saying "Long faces don't win wars". As he had threatened the French in their rejection of EDC, he afterwards moved to restore West Germany, as a full NATO partner.
With Eisenhower's leadership and Dulles' direction, CIA activities increased under the pretense of resisting the spread of communism in poorer countries; the CIA in part deposed the leaders of Iran in Operation Ajax, of Guatemala through Operation Pbsuccess, and possibly the newly independent Republic of the Congo (Léopoldville). In 1954 Eisenhower wanted to increase surveillance inside the Soviet Union. With Dulles' recommendation, he authorized the deployment of thirty Lockheed U-2's at a cost of $35 million. The Eisenhower administration also planned the Bay of Pigs Invasion to overthrow Fidel Castro in Cuba, which John F. Kennedy was left to carry out."
Over New York City in 1953, Eastern Airlines Flight 8610, a commercial flight, had a near miss with Air Force Flight 8610, a Lockheed C-121 Constellation known as Columbine II, while the latter was carrying President Eisenhower. This prompted the adoption of the unique call sign Air Force One, to be used whenever the president is on board any US Air Force aircraft. Columbine II is the only presidential aircraft to have ever been sold to the public and is the only remaining presidential aircraft left unrestored and not on public display.
On the whole, Eisenhower's support of the nation's fledgling space program was officially modest until the Soviet launch of Sputnik in 1957, gaining the Cold War enemy enormous prestige around the world. He then launched a national campaign that funded not just space exploration but a major strengthening of science and higher education. His Open Skies Policy attempted to legitimize illegal Lockheed U-2 flyovers and Project Genetrix while paving the way for spy satellite technology to orbit over sovereign territory, created NASA as a civilian space agency, signed a landmark science education law, and fostered improved relations with American scientists.
In late 1952 Eisenhower went to Korea and discovered a military and political stalemate. Once in office, when the Chinese began a buildup in the Kaesong sanctuary, he threatened to use nuclear force if an armistice was not concluded. His earlier military reputation in Europe was effective with the Chinese. The National Security Council, the Joint Chiefs of Staff, and the Strategic Air Command (SAC) devised detailed plans for nuclear war against China. With the death of Stalin in early March 1953, Russian support for a Chinese hard-line weakened and China decided to compromise on the prisoner issue.
In July 1953, an armistice took effect with Korea divided along approximately the same boundary as in 1950. The armistice and boundary remain in effect today, with American soldiers stationed there to guarantee it. The armistice, concluded despite opposition from Secretary Dulles, South Korean President Syngman Rhee, and also within Eisenhower's party, has been described by biographer Ambrose as the greatest achievement of the administration. Eisenhower had the insight to realize that unlimited war in the nuclear age was unthinkable, and limited war unwinnable.
In November 1956, Eisenhower forced an end to the combined British, French and Israeli invasion of Egypt in response to the Suez Crisis, receiving praise from Egyptian president Gamal Abdel Nasser. Simultaneously he condemned the brutal Soviet invasion of Hungary in response to the Hungarian Revolution of 1956. He publicly disavowed his allies at the United Nations, and used financial and diplomatic pressure to make them withdraw from Egypt. Eisenhower explicitly defended his strong position against Britain and France in his memoirs, which were published in 1965.
Early in 1953, the French asked Eisenhower for help in French Indochina against the Communists, supplied from China, who were fighting the First Indochina War. Eisenhower sent Lt. General John W. "Iron Mike" O'Daniel to Vietnam to study and assess the French forces there. Chief of Staff Matthew Ridgway dissuaded the President from intervening by presenting a comprehensive estimate of the massive military deployment that would be necessary. Eisenhower stated prophetically that "this war would absorb our troops by divisions."
Eisenhower did provide France with bombers and non-combat personnel. After a few months with no success by the French, he added other aircraft to drop napalm for clearing purposes. Further requests for assistance from the French were agreed to but only on conditions Eisenhower knew were impossible to meet – allied participation and congressional approval. When the French fortress of Dien Bien Phu fell to the Vietnamese Communists in May 1954, Eisenhower refused to intervene despite urgings from the Chairman of the Joint Chiefs, the Vice President and the head of NCS.
Eisenhower responded to the French defeat with the formation of the SEATO (Southeast Asia Treaty Organization) Alliance with the U.K., France, New Zealand and Australia in defense of Vietnam against communism. At that time the French and Chinese reconvened Geneva peace talks; Eisenhower agreed the U.S. would participate only as an observer. After France and the Communists agreed to a partition of Vietnam, Eisenhower rejected the agreement, offering military and economic aid to southern Vietnam. Ambrose argues that Eisenhower, by not participating in the Geneva agreement, had kept the U.S out of Vietnam; nevertheless, with the formation of SEATO, he had in the end put the U.S. back into the conflict.
In late 1954, Gen. J. Lawton Collins was made ambassador to "Free Vietnam" (the term South Vietnam came into use in 1955), effectively elevating the country to sovereign status. Collins' instructions were to support the leader Ngo Dinh Diem in subverting communism, by helping him to build an army and wage a military campaign. In February 1955, Eisenhower dispatched the first American soldiers to Vietnam as military advisors to Diem's army. After Diem announced the formation of the Republic of Vietnam (RVN, commonly known as South Vietnam) in October, Eisenhower immediately recognized the new state and offered military, economic, and technical assistance.
In the years that followed, Eisenhower increased the number of U.S. military advisors in South Vietnam to 900 men. This was due to North Vietnam's support of "uprisings" in the south and concern the nation would fall. In May 1957 Diem, then President of South Vietnam, made a state visit to the United States for ten days. President Eisenhower pledged his continued support, and a parade was held in Diem's honor in New York City. Although Diem was publicly praised, in private Secretary of State John Foster Dulles conceded that Diem had been selected because there were no better alternatives.
On May 1, 1960, a U.S. one-man U-2 spy plane was reportedly shot down at high altitude over Soviet Union airspace. The flight was made to gain photo intelligence before the scheduled opening of an East–West summit conference, which had been scheduled in Paris, 15 days later. Captain Francis Gary Powers had bailed out of his aircraft and was captured after parachuting down onto Russian soil. Four days after Powers disappeared, the Eisenhower Administration had NASA issue a very detailed press release noting that an aircraft had "gone missing" north of Turkey. It speculated that the pilot might have fallen unconscious while the autopilot was still engaged, and falsely claimed that "the pilot reported over the emergency frequency that he was experiencing oxygen difficulties."
Soviet Premier Nikita Khrushchev announced that a "spy-plane" had been shot down but intentionally made no reference to the pilot. As a result, the Eisenhower Administration, thinking the pilot had died in the crash, authorized the release of a cover story claiming that the plane was a "weather research aircraft" which had unintentionally strayed into Soviet airspace after the pilot had radioed "difficulties with his oxygen equipment" while flying over Turkey. The Soviets put Captain Powers on trial and displayed parts of the U-2, which had been recovered almost fully intact.
The 1960 Four Power Paris Summit between President Dwight Eisenhower, Nikita Khrushchev, Harold Macmillan and Charles de Gaulle collapsed because of the incident. Eisenhower refused to accede to Khrushchev's demands that he apologize. Therefore, Khrushchev would not take part in the summit. Up until this event, Eisenhower felt he had been making progress towards better relations with the Soviet Union. Nuclear arms reduction and Berlin were to have been discussed at the summit. Eisenhower stated it had all been ruined because of that "stupid U-2 business".
While President Truman had begun the process of desegregating the Armed Forces in 1948, actual implementation had been slow. Eisenhower made clear his stance in his first State of the Union address in February 1953, saying "I propose to use whatever authority exists in the office of the President to end segregation in the District of Columbia, including the Federal Government, and any segregation in the Armed Forces". When he encountered opposition from the services, he used government control of military spending to force the change through, stating "Wherever Federal Funds are expended ..., I do not see how any American can justify ... a discrimination in the expenditure of those funds".
Eisenhower told District of Columbia officials to make Washington a model for the rest of the country in integrating black and white public school children. He proposed to Congress the Civil Rights Act of 1957 and of 1960 and signed those acts into law. The 1957 act for the first time established a permanent civil rights office inside the Justice Department and a Civil Rights Commission to hear testimony about abuses of voting rights. Although both acts were much weaker than subsequent civil rights legislation, they constituted the first significant civil rights acts since 1875.
In 1957, the state of Arkansas refused to honor a federal court order to integrate their public school system stemming from the Brown decision. Eisenhower demanded that Arkansas governor Orval Faubus obey the court order. When Faubus balked, the president placed the Arkansas National Guard under federal control and sent in the 101st Airborne Division. They escorted and protected nine black students' entry to Little Rock Central High School, an all-white public school, for the first time since the Reconstruction Era. Martin Luther King Jr. wrote to Eisenhower to thank him for his actions, writing "The overwhelming majority of southerners, Negro and white, stand firmly behind your resolute action to restore law and order in Little Rock".
This prevented Eisenhower from openly condemning Joseph McCarthy's highly criticized methods against communism. To facilitate relations with Congress, Eisenhower decided to ignore McCarthy's controversies and thereby deprive them of more energy from involvement of the White House. This position drew criticism from a number of corners. In late 1953 McCarthy declared on national television that the employment of communists within the government was a menace and would be a pivotal issue in the 1954 Senate elections. Eisenhower was urged to respond directly and specify the various measures he had taken to purge the government of communists. Nevertheless, he refused.
Among Ike's objectives in not directly confronting McCarthy was to prevent McCarthy from dragging the Atomic Energy Commission (AEC) into McCarthy's witch hunt for communists, which would interfere with, and perhaps delay, the AEC's important work on H-bombs. The administration had discovered through its own investigations that one of the leading scientists on the AEC, J. Robert Oppenheimer, had urged that the H-bomb work be delayed. Eisenhower removed him from the agency and revoked his security clearance, though he knew this would create fertile ground for McCarthy.
In May 1955, McCarthy threatened to issue subpoenas to White House personnel. Eisenhower was furious, and issued an order as follows: "It is essential to efficient and effective administration that employees of the Executive Branch be in a position to be completely candid in advising with each other on official matters ... it is not in the public interest that any of their conversations or communications, or any documents or reproductions, concerning such advice be disclosed." This was an unprecedented step by Eisenhower to protect communication beyond the confines of a cabinet meeting, and soon became a tradition known as executive privilege. Ike's denial of McCarthy's access to his staff reduced McCarthy's hearings to rants about trivial matters, and contributed to his ultimate downfall.
The Democrats gained a majority in both houses in the 1954 election. Eisenhower had to work with the Democratic Majority Leader Lyndon B. Johnson (later U.S. president) in the Senate and Speaker Sam Rayburn in the House, both from Texas. Joe Martin, the Republican Speaker from 1947 to 1949 and again from 1953 to 1955, wrote that Eisenhower "never surrounded himself with assistants who could solve political problems with professional skill. There were exceptions, Leonard W. Hall, for example, who as chairman of the Republican National Committee tried to open the administration's eyes to the political facts of life, with occasional success. However, these exceptions were not enough to right the balance."
Speaker Martin concluded that Eisenhower worked too much through subordinates in dealing with Congress, with results, "often the reverse of what he has desired" because Members of Congress, "resent having some young fellow who was picked up by the White House without ever having been elected to office himself coming around and telling them 'The Chief wants this'. The administration never made use of many Republicans of consequence whose services in one form or another would have been available for the asking."
Whittaker was unsuited for the role and soon retired. Stewart and Harlan were conservative Republicans, while Brennan was a Democrat who became a leading voice for liberalism. In selecting a Chief Justice, Eisenhower looked for an experienced jurist who could appeal to liberals in the party as well as law-and-order conservatives, noting privately that Warren "represents the kind of political, economic, and social thinking that I believe we need on the Supreme Court ... He has a national name for integrity, uprightness, and courage that, again, I believe we need on the Court". In the next few years Warren led the Court in a series of liberal decisions that revolutionized the role of the Court.
Eisenhower began smoking cigarettes at West Point, often two or three packs a day. Eisenhower stated that he "gave [himself] an order" to stop cold turkey in March 1949 while at Columbia. He was probably the first president to release information about his health and medical records while in office. On September 24, 1955, while vacationing in Colorado, he had a serious heart attack that required six weeks' hospitalization, during which time Nixon, Dulles, and Sherman Adams assumed administrative duties and provided communication with the President. He was treated by Dr. Paul Dudley White, a cardiologist with a national reputation, who regularly informed the press of the President's progress. Instead of eliminating him as a candidate for a second term as President, his physician recommended a second term as essential to his recovery.
As a consequence of his heart attack, Eisenhower developed a left ventricular aneurysm, which was in turn the cause of a mild stroke on November 25, 1957. This incident occurred during a cabinet meeting when Eisenhower suddenly found himself unable to speak or move his right hand. The stroke had caused an aphasia. The president also suffered from Crohn's disease, chronic inflammatory condition of the intestine, which necessitated surgery for a bowel obstruction on June 9, 1956. To treat the intestinal block, surgeons bypassed about ten inches of his small intestine. His scheduled meeting with Indian Prime Minister Jawaharlal Nehru was postponed so he could recover from surgery at his farm in Gettysburg, Pennsylvania. He was still recovering from this operation during the Suez Crisis. Eisenhower's health issues forced him to give up smoking and make some changes to his dietary habits, but he still indulged in alcohol. During a visit to England he complained of dizziness and had to have his blood pressure checked on August 29, 1959; however, before dinner at Chequers on the next day his doctor General Howard Snyder recalled Eisenhower "drank several gin-and-tonics, and one or two gins on the rocks ... three or four wines with the dinner".
The last three years of Eisenhower's second term in office were ones of relatively good health. Eventually after leaving the White House, he suffered several additional and ultimately crippling heart attacks. A severe heart attack in August 1965 largely ended his participation in public affairs. In August 1966 he began to show symptoms of cholecystitis, for which he underwent surgery on December 12, 1966, when his gallbladder was removed, containing 16 gallstones. After Eisenhower's death in 1969 (see below), an autopsy unexpectedly revealed an adrenal pheochromocytoma, a benign adrenaline-secreting tumor that may have made the President more vulnerable to heart disease. Eisenhower suffered seven heart attacks in total from 1955 until his death.
In the 1960 election to choose his successor, Eisenhower endorsed his own Vice President, Republican Richard Nixon against Democrat John F. Kennedy. He told friends, "I will do almost anything to avoid turning my chair and country over to Kennedy." He actively campaigned for Nixon in the final days, although he may have done Nixon some harm. When asked by reporters at the end of a televised press conference to list one of Nixon's policy ideas he had adopted, Eisenhower joked, "If you give me a week, I might think of one. I don't remember." Kennedy's campaign used the quote in one of its campaign commercials. Nixon narrowly lost to Kennedy. Eisenhower, who was the oldest president in history at that time (then 70), was succeeded by the youngest elected president, as Kennedy was 43.
On January 17, 1961, Eisenhower gave his final televised Address to the Nation from the Oval Office. In his farewell speech, Eisenhower raised the issue of the Cold War and role of the U.S. armed forces. He described the Cold War: "We face a hostile ideology global in scope, atheistic in character, ruthless in purpose and insidious in method ..." and warned about what he saw as unjustified government spending proposals and continued with a warning that "we must guard against the acquisition of unwarranted influence, whether sought or unsought, by the military–industrial complex."
Eisenhower retired to the place where he and Mamie had spent much of their post-war time, a working farm adjacent to the battlefield at Gettysburg, Pennsylvania, only 70 miles from his ancestral home in Elizabethville, Dauphin County, Pennsylvania. In 1967 the Eisenhowers donated the farm to the National Park Service. In retirement, the former president did not completely retreat from political life; he spoke at the 1964 Republican National Convention and appeared with Barry Goldwater in a Republican campaign commercial from Gettysburg. However, his endorsement came somewhat reluctantly because Goldwater had attacked the former president as "a dime-store New Dealer".
On the morning of March 28, 1969, at the age of 78, Eisenhower died in Washington, D.C. of congestive heart failure at Walter Reed Army Medical Center. The following day his body was moved to the Washington National Cathedral's Bethlehem Chapel, where he lay in repose for 28 hours. On March 30, his body was brought by caisson to the United States Capitol, where he lay in state in the Capitol Rotunda. On March 31, Eisenhower's body was returned to the National Cathedral, where he was given an Episcopal Church funeral service.
That evening, Eisenhower's body was placed onto a train en route to Abilene, Kansas, the last time a funeral train has been used as part of funeral proceedings of an American president. His body arrived on April 2, and was interred later that day in a small chapel on the grounds of the Eisenhower Presidential Library. The president's body was buried as a General of the Army. The family used an $80 standard soldier's casket, and dressed Eisenhower's body in his famous short green jacket. His only medals worn were: the Army Distinguished Service Medal with three oak leaf clusters, the Navy Distinguished Service Medal, and the Legion of Merit. Eisenhower is buried alongside his son Doud, who died at age 3 in 1921. His wife Mamie was buried next to him after her death in 1979.
In the immediate years after Eisenhower left office, his reputation declined. He was widely seen by critics as an inactive, uninspiring, golf-playing president compared to his vigorous young successor. Despite his unprecedented use of Army troops to enforce a federal desegregation order at Central High School in Little Rock, Eisenhower was criticized for his reluctance to support the civil rights movement to the degree that activists wanted. Eisenhower also attracted criticism for his handling of the 1960 U-2 incident and the associated international embarrassment, for the Soviet Union's perceived leadership in the nuclear arms race and the Space Race, and for his failure to publicly oppose McCarthyism.
Since the 19th century, many if not all presidents were assisted by a central figure or "gatekeeper", sometimes described as the President's Private Secretary, sometimes with no official title at all. Eisenhower formalized this role, introducing the office of White House Chief of Staff – an idea he borrowed from the United States Army. Every president after Lyndon Johnson has also appointed staff to this position. Initially, Gerald Ford and Jimmy Carter tried to operate without a chief of staff, but each eventually appointed one.
The development of the appreciation medals was initiated by the White House and executed by the Bureau of the Mint through the U.S. Mint in Philadelphia. The medals were struck from September 1958 through October 1960. A total of twenty designs are cataloged with a total mintage of 9,858. Each of the designs incorporates the text "with appreciation" or "with personal and official gratitude" accompanied with Eisenhower's initials "D.D.E." or facsimile signature. The design also incorporates location, date, and/or significant event. Prior to the end of his second term as President, 1,451 medals were turned-in to the Bureau of the Mint and destroyed. The Eisenhower appreciation medals are part of the Presidential Medal of Appreciation Award Medal Series.
The Interstate Highway System is officially known as the 'Dwight D. Eisenhower National System of Interstate and Defense Highways' in his honor. It was inspired in part by Eisenhower's own Army experiences in World War II, where he recognized the advantages of the autobahn systems in Germany, Austria, and Switzerland. Commemorative signs reading "Eisenhower Interstate System" and bearing Eisenhower's permanent 5-star rank insignia were introduced in 1993 and are currently displayed throughout the Interstate System. Several highways are also named for him, including the Eisenhower Expressway (Interstate 290) near Chicago and the Eisenhower Tunnel on Interstate 70 west of Denver.
A loblolly pine, known as the "Eisenhower Pine", was located on Augusta's 17th hole, approximately 210 yards (192 m) from the Masters tee. President Dwight D. Eisenhower, an Augusta National member, hit the tree so many times that, at a 1956 club meeting, he proposed that it be cut down. Not wanting to offend the president, the club's chairman, Clifford Roberts, immediately adjourned the meeting rather than reject the request. The tree was removed in February 2014 after an ice storm caused it significant damage.
Mammals include the largest animals on the planet, the rorquals and other large whales, as well as some of the most intelligent, such as elephants, primates, including humans, and cetaceans. The basic body type is a four-legged land-borne animal, but some mammals are adapted for life at sea, in the air, in trees, or on two legs. The largest group of mammals, the placentals, have a placenta, which enables feeding the fetus during gestation. Mammals range in size from the 30–40 mm (1.2–1.6 in) bumblebee bat to the 33-meter (108 ft) blue whale.
The word "mammal" is modern, from the scientific name Mammalia coined by Carl Linnaeus in 1758, derived from the Latin mamma ("teat, pap"). All female mammals nurse their young with milk, which is secreted from special glands, the mammary glands. According to Mammal Species of the World, 5,416 species were known in 2006. These were grouped in 1,229 genera, 153 families and 29 orders. In 2008 the IUCN completed a five-year, 1,700-scientist Global Mammal Assessment for its IUCN Red List, which counted 5,488 accepted species.
Except for the five species of monotremes (egg-laying mammals), all modern mammals give birth to live young. Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders in numbers, are first Rodentia: mice, rats, porcupines, beavers, capybaras, and other gnawing mammals; then Chiroptera: bats; and then Soricomorpha: shrews, moles and solenodons. The next three orders, depending on the biological classification scheme used, are the Primates including the humans; the Cetartiodactyla including the whales and the even-toed hoofed mammals; and the Carnivora, that is, cats, dogs, weasels, bears, seals, and their relatives.
The early synapsid mammalian ancestors were sphenacodont pelycosaurs, a group that produced the non-mammalian Dimetrodon. At the end of the Carboniferous period, this group diverged from the sauropsid line that led to today's reptiles and birds. The line following the stem group Sphenacodontia split-off several diverse groups of non-mammalian synapsids—sometimes referred to as mammal-like reptiles—before giving rise to the proto-mammals (Therapsida) in the early Mesozoic era. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of the non-avian dinosaurs 66 million years ago.
In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and therian mammals (marsupials and placentals) and all descendants of that ancestor. Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century.
If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. Ambondro is more closely related to monotremes than to therian mammals while Amphilestes and Amphitherium are more closely related to the therians; as fossils of all three genera are dated about 167 million years ago in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group. The earliest known synapsid satisfying Kemp's definitions is Tikitherium, dated 225 Ma, so the appearance of mammals in this broader sense can be given this Late Triassic date. In any case, the temporal range of the group extends to the present day.
George Gaylord Simpson's "Principles of Classification and a Classification of Mammals" (AMNH Bulletin v. 85, 1945) was the original source for the taxonomy listed here. Simpson laid out a systematics of mammal origins and relationships that was universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though field work gradually made Simpson's classification outdated, it remained the closest thing to an official classification of mammals.
In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, Classification of Mammals above the Species Level, is the most comprehensive work to date on the systematics, relationships, and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though recent molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History, New York. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia.
Molecular studies based on DNA analysis have suggested new relationships among mammal families over the last few years. Most of these findings have been independently validated by retrotransposon presence/absence data. Classification systems based on molecular studies reveal three major groups or lineages of placental mammals- Afrotheria, Xenarthra, and Boreoeutheria- which diverged from early common ancestors in the Cretaceous. The relationships between these three lineages is contentious, and all three possible different hypotheses have been proposed with respect to which group is basal with respect to other placentals. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra), and Exafroplacentalia (basal Afrotheria). Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
The first amniotes apparently arose in the Late Carboniferous. They descended from earlier reptiliomorph amphibious tetrapods, which lived on land that was already inhabited by insects and other invertebrates as well as by ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which would later include the common ancestor of the mammals; and the sauropsids, which would eventually come to include turtles, lizards, snakes, crocodilians, dinosaurs and birds. Synapsids have a single hole (temporal fenestra) low on each side of the skull.
Therapsids descended from pelycosaurs in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates. They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger temporal fenestrae and incisors which are equal in size. The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very like their pelycosaur ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:
The Permian–Triassic extinction event, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of the carnivores among the therapsids. In the early Triassic, all the medium to large land carnivore niches were taken over by archosaurs which, over an extended period of time (35 million years), came to include the crocodylomorphs, the pterosaurs, and the dinosaurs. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.
The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike Juramaia sinensis, or "Jurassic mother from China", dated to 160 million years ago in the Late Jurassic. A later eutherian, Eomaia, dated to 125 million years ago in the Early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. In particular:
Recent molecular phylogenetic studies suggest that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene. But paleontologists object that no placental fossils have been found from before the end of the Cretaceous. The earliest undisputed fossils of placentals come from the early Paleocene, after the extinction of the dinosaurs. In particular, scientists have recently identified an early Paleocene animal named Protungulatum donnae as one of the first placental mammals. The earliest known ancestor of primates is Archicebus achilles from around 55 million years ago. This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.
The earliest clear evidence of hair or fur is in fossils of Castorocauda, from 164 million years ago in the Middle Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur; it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard Tupinambis has foramina that are almost identical to those found in the nonmammalian cynodont Thrinaxodon. Popular sources, nevertheless, continue to attribute whiskers to Thrinaxodon.
When endothermy first appeared in the evolution of mammals is uncertain. Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. Some of the evidence found so far suggests that Triassic cynodonts had fairly high metabolic rates, but it is not conclusive. For small animals, an insulative covering like fur is necessary for the maintenance of a high and stable body temperature.
Breathing is largely driven by the muscular diaphragm, which divides the thorax from the abdominal cavity, forming a dome with its convexity towards the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the cavity in which the lung is enclosed. Air enters through the oral and nasal cavities; it flows through the larynx, trachea and bronchi and expands the alveoli. Relaxation of the diaphragm has the opposite effect, passively recoiling during normal breathing. During exercise, the abdominal wall contracts, increasing visceral pressure on the diaphragm, thus forcing the air out more quickly and forcefully. The rib cage itself also is able to expand and contract the thoracic cavity to some degree, through the action of other respiratory and accessory respiratory muscles. As a result, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. This type of lung is known as a bellows lung as it resembles a blacksmith's bellows. Mammals take oxygen into their lungs, and discard carbon dioxide.
The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue. Its job is to store lipids, and to provide cushioning and insulation. The thickness of this layer varies widely from species to species.
Mammalian hair, also known as pelage, can vary in color between populations, organisms within a population, and even on the individual organism. Light-dark color variation is common in the mammalian taxa. Sometimes, this color variation is determined by age variation, however, in other cases, it is determined by other factors. Selective pressures, such as ecological interactions with other populations or environmental conditions, often lead to the variation in mammalian coloration. These selective pressures favor certain colors in order to increase survival. Camouflage is thought to be a major selection pressure shaping coloration in mammals, although there is also evidence that sexual selection, communication, and physiological processes may influence the evolution of coloration as well. Camouflage is the most predominant mechanism for color variation, as it aids in the concealment of the organisms from predators or from their prey. Coat color can also be for intraspecies communication such as warning members of their species about predators, indicating health for reproductive purposes, communicating between mother and young, and intimidating predators. Studies have shown that in some cases, differences in female and male coat color could indicate information nutrition and hormone levels, which are important in the mate selection process. One final mechanism for coat color variation is physiological response purposes, such as temperature regulation in tropical or arctic environments. Although much has been observed about color variation, much of the genetic that link coat color to genes is still unknown. The genetic sites where pigmentation genes are found are known to affect phenotype by: 1) altering the spatial distribution of pigmentation of the hairs, and 2) altering the density and distribution of the hairs. Quantitative trait mapping is being used to better understand the distribution of loci responsible for pigmentation variation. However, although the genetic sites are known, there is still much to learn about how these genes are expressed.
Most mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypuses and the echidnas, lay eggs. The monotremes have a sex determination system different from that of most other mammals. In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal. Like marsupials and most other mammals, monotreme young are larval and fetus-like, as the presence of epipubic bones prevents the expansion of the torso, forcing them to produce small young.
Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. A marsupial has a short gestation period, typically shorter than its estrous cycle, and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. This is the plesyomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy. Even non-placental eutherians probably reproduced this way.
In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.
To maintain a high constant body temperature is energy expensive – mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals – this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants. A herbivorous diet includes subtypes such as fruit-eating and grass-eating. An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract, because the proteins, lipids, and minerals found in meat require little in the way of specialized digestion. Plants, on the other hand, contain complex carbohydrates, such as cellulose. The digestive tract of an herbivore is therefore host to bacteria that ferment these substances, and make them available for digestion. The bacteria are either housed in the multichambered stomach or in a large cecum. The size of an animal is also a factor in determining diet type. Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about 18 oz (500 g) are mostly insectivorous because they cannot tolerate the slow, complex digestive process of a herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (those that prey on larger vertebrates) or a slower digestive process (herbivores). Furthermore, mammals that weigh more than 18 oz (500 g) usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).
The deliberate or accidental hybridising of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown in recent times for economic purposes. The number of successful interspecific mammalian hybrids is relatively small, although it has come to be known that there is a significant number of naturally occurring hybrids between forms or regional varieties of a single species.[citation needed] These may form zones of gradation known as clines. Indeed, the distinction between some hitherto distinct species can become clouded once it can be shown that they may not only breed but produce fertile offspring. Some hybrid animals exhibit greater strength and resilience than either parent. This is known as hybrid vigor. The existence of the mule (donkey sire; horse dam) being used widely as a hardy draught animal throughout ancient and modern history is testament to this. Other well known examples are the lion/tiger hybrid, the liger, which is by far the largest big cat and sometimes used in circuses; and cattle hybrids such as between European and Indian domestic cattle or between domestic cattle and American bison, which are used in the meat industry and marketed as Beefalo. There is some speculation that the donkey itself may be the result of an ancient hybridisation between two wild ass species or sub-species. Hybrid animals are normally infertile partly because their parents usually have slightly different numbers of chromosomes, resulting in unpaired chromosomes in their cells, which prevents division of sex cells and the gonads from operating correctly, particularly in males. There are exceptions to this rule, especially if the speciation process was relatively recent or incomplete as is the case with many cattle and dog species. Normally behavior traits, natural hostility, natural ranges and breeding cycle differences maintain the separateness of closely related species and prevent natural hybridisation. However, the widespread disturbances to natural animal behaviours and range caused by human activity, cities, dumping grounds with food, agriculture, fencing, roads and so on do force animals together which would not normally breed. Clear examples exist between the various sub-species of grey wolf, coyote and domestic dog in North America. As many birds and mammals imprint on their mother and immediate family from infancy, a practice used by animal hybridizers is to foster a planned parent in a hybridization program with the same species as the one with which they are planned to mate.
The Continental Army was created on 14 June 1775 by the Continental Congress as a unified army for the colonies to fight Great Britain, with George Washington appointed as its commander. The army was initially led by men who had served in the British Army or colonial militias and who brought much of British military heritage with them. As the Revolutionary War progressed, French aid, resources, and military thinking influenced the new army. A number of European soldiers came on their own to help, such as Friedrich Wilhelm von Steuben, who taught the army Prussian tactics and organizational skills.
The Vietnam War is often regarded as a low point for the U.S. Army due to the use of drafted personnel, the unpopularity of the war with the American public, and frustrating restrictions placed on the military by American political leaders. While American forces had been stationed in the Republic of Vietnam since 1959, in intelligence & advising/training roles, they did not deploy in large numbers until 1965, after the Gulf of Tonkin Incident. American forces effectively established and maintained control of the "traditional" battlefield, however they struggled to counter the guerrilla hit and run tactics of the communist Viet Cong and the North Vietnamese Army. On a tactical level, American soldiers (and the U.S. military as a whole) did not lose a sizable battle.
By the twentieth century, the U.S. Army had mobilized the U.S. Volunteers on four separate occasions during each of the major wars of the nineteenth century. During World War I, the "National Army" was organized to fight the conflict, replacing the concept of U.S. Volunteers. It was demobilized at the end of World War I, and was replaced by the Regular Army, the Organized Reserve Corps, and the State Militias. In the 1920s and 1930s, the "career" soldiers were known as the "Regular Army" with the "Enlisted Reserve Corps" and "Officer Reserve Corps" augmented to fill vacancies when needed.
The army is led by a civilian Secretary of the Army, who has the statutory authority to conduct all the affairs of the army under the authority, direction and control of the Secretary of Defense. The Chief of Staff of the Army, who is the highest-ranked military officer in the army, serves as the principal military adviser and executive agent for the Secretary of the Army, i.e., its service chief; and as a member of the Joint Chiefs of Staff, a body composed of the service chiefs from each of the four military services belonging to the Department of Defense who advise the President of the United States, the Secretary of Defense, and the National Security Council on operational military matters, under the guidance of the Chairman and Vice Chairman of the Joint Chiefs of Staff. In 1986, the Goldwater–Nichols Act mandated that operational control of the services follows a chain of command from the President to the Secretary of Defense directly to the unified combatant commanders, who have control of all armed forces units in their geographic or function area of responsibility. Thus, the secretaries of the military departments (and their respective service chiefs underneath them) only have the responsibility to organize, train and equip their service components. The army provides trained forces to the combatant commanders for use as directed by the Secretary of Defense.
The United States Army (USA) is the largest branch of the United States Armed Forces and performs land-based military operations. It is one of the seven uniformed services of the United States and is designated as the Army of the United States in the United States Constitution, Article 2, Section 2, Clause 1 and United States Code, Title 10, Subtitle B, Chapter 301, Section 3001. As the largest and senior branch of the U.S. military, the modern U.S. Army has its roots in the Continental Army, which was formed (14 June 1775) to fight the American Revolutionary War (1775–83)—before the U.S. was established as a country. After the Revolutionary War, the Congress of the Confederation created the United States Army on 3 June 1784, to replace the disbanded Continental Army. The United States Army considers itself descended from the Continental Army, and dates its institutional inception from the origin of that armed force in 1775.
The War of 1812, the second and last American war against the United Kingdom, was less successful for the U.S. than the Revolution and Northwest Indian War against natives had been, though it ended on a high note for Americans as well. After the taking control of Lake Erie in 1813, the Americans were able to seize parts of western Upper Canada, burn York and defeat Tecumseh, which caused his Indian Confederacy to collapse. Following ending victories in the province of Upper Canada, which dubbed the U.S. Army "Regulars, by God!", British troops were able to capture and burn Washington. The regular army, however, proved they were professional and capable of defeating the British army during the invasions of Plattsburgh and Baltimore, prompting British agreement on the previously rejected terms of a status quo ante bellum. Two weeks after a treaty was signed (but not ratified), Andrew Jackson defeated the British in the Battle of New Orleans and became a national hero. Per the treaty both sides returned to the status quo with no victor.
After the war, though, the Continental Army was quickly given land certificates and disbanded in a reflection of the republican distrust of standing armies. State militias became the new nation's sole ground army, with the exception of a regiment to guard the Western Frontier and one battery of artillery guarding West Point's arsenal. However, because of continuing conflict with Native Americans, it was soon realized that it was necessary to field a trained standing army. The Regular Army was at first very small, and after General St. Clair's defeat at the Battle of the Wabash, the Regular Army was reorganized as the Legion of the United States, which was established in 1791 and renamed the "United States Army" in 1796.
Collective training at the unit level takes place at the unit's assigned station, but the most intensive training at higher echelons is conducted at the three combat training centers (CTC); the National Training Center (NTC) at Fort Irwin, California, the Joint Readiness Training Center (JRTC) at Fort Polk, Louisiana, and the Joint Multinational Training Center (JMRC) at the Hohenfels Training Area in Hohenfels, Germany. ARFORGEN is the Army Force Generation process approved in 2006 to meet the need to continuously replenish forces for deployment, at unit level, and for other echelons as required by the mission. Individual-level replenishment still requires training at a unit level, which is conducted at the continental US (CONUS) replacement center at Fort Bliss, in New Mexico and Texas, before their individual deployment.
For the first two years Confederate forces did well in set battles but lost control of the border states. The Confederates had the advantage of defending a very large country in an area where disease caused twice as many deaths as combat. The Union pursued a strategy of seizing the coastline, blockading the ports, and taking control of the river systems. By 1863 the Confederacy was being strangled. Its eastern armies fought well, but the western armies were defeated one after another until the Union forces captured New Orleans in 1862 along with the Tennessee River. In the famous Vicksburg Campaign of 1862–63, Ulysses Grant seized the Mississippi River and cut off the Southwest. Grant took command of Union forces in 1864 and after a series of battles with very heavy casualties, he had Lee under siege in Richmond as William T. Sherman captured Atlanta and marched through Georgia and the Carolinas. The Confederate capital was abandoned in April 1865 and Lee subsequently surrendered his army at Appomattox Court House; all other Confederate armies surrendered within a few months.
The end of World War II set the stage for the East–West confrontation known as the Cold War. With the outbreak of the Korean War, concerns over the defense of Western Europe rose. Two corps, V and VII, were reactivated under Seventh United States Army in 1950 and American strength in Europe rose from one division to four. Hundreds of thousands of U.S. troops remained stationed in West Germany, with others in Belgium, the Netherlands and the United Kingdom, until the 1990s in anticipation of a possible Soviet attack.
The United States joined World War II in December 1941 after the Japanese attack on Pearl Harbor. On the European front, U.S. Army troops formed a significant portion of the forces that captured North Africa and Sicily, and later fought in Italy. On D-Day, June 6, 1944, and in the subsequent liberation of Europe and defeat of Nazi Germany, millions of U.S. Army troops played a central role. In the Pacific War, U.S. Army soldiers participated alongside the United States Marine Corps in capturing the Pacific Islands from Japanese control. Following the Axis surrenders in May (Germany) and August (Japan) of 1945, army troops were deployed to Japan and Germany to occupy the two defeated nations. Two years after World War II, the Army Air Forces separated from the army to become the United States Air Force in September 1947 after decades of attempting to separate. Also, in 1948, the army was desegregated by order of President Harry S. Truman.
The Total Force Policy was adopted by Chief of Staff of the Army General Creighton Abrams in the aftermath of the Vietnam War and involves treating the three components of the army – the Regular Army, the Army National Guard and the Army Reserve as a single force. Believing that no U.S. president should be able to take the United States (and more specifically the U.S. Army) to war without the support of the American people, General Abrams intertwined the structure of the three components of the army in such a way as to make extended operations impossible, without the involvement of both the Army National Guard and the Army Reserve.
In response to the September 11 attacks, and as part of the Global War on Terror, U.S. and NATO forces invaded Afghanistan in October 2001, displacing the Taliban government. The U.S. Army also led the combined U.S. and allied invasion of Iraq in 2003. It served as the primary source for ground forces with its ability to sustain short and long-term deployment operations. In the following years the mission changed from conflict between regular militaries to counterinsurgency, resulting in the deaths of more than 4,000 U.S service members (as of March 2008) and injuries to thousands more. 23,813 insurgents were killed in Iraq between 2003–2011.
Currently, the army is divided into the Regular Army, the Army Reserve, and the Army National Guard. The army is also divided into major branches such as Air Defense Artillery, Infantry, Aviation, Signal Corps, Corps of Engineers, and Armor. Before 1903 members of the National Guard were considered state soldiers unless federalized (i.e., activated) by the President. Since the Militia Act of 1903 all National Guard soldiers have held dual status: as National Guardsmen under the authority of the governor of their state or territory and, when activated, as a reserve of the U.S. Army under the authority of the President.
The U.S. Army currently consists of 10 active divisions as well as several independent units. The force is in the process of contracting after several years of growth. In June 2013, the Army announced plans to downsize to 32 active combat brigade teams by 2015 to match a reduction in active duty strength to 490,000 soldiers. Army Chief of Staff Raymond Odierno has projected that by 2018 the Army will eventually shrink to "450,000 in the active component, 335,000 in the National Guard and 195,000 in U.S. Army Reserve."
Training in the U.S. Army is generally divided into two categories – individual and collective. Basic training consists of 10 weeks for most recruits followed by Advanced Individualized Training (AIT) where they receive training for their military occupational specialties (MOS). Some individuals MOSs range anywhere from 14–20 weeks of One Station Unit Training (OSUT), which combines Basic Training and AIT. The length of AIT school varies by the MOS The length of time spent in AIT depends on the MOS of the soldier, and some highly technical MOS training may require many months (e.g., foreign language translators). Depending on the needs of the army, Basic Combat Training for combat arms soldiers is conducted at a number of locations, but two of the longest-running are the Armor School and the Infantry School, both at Fort Benning, Georgia.
Many units are supplemented with a variety of specialized weapons, including the M249 SAW (Squad Automatic Weapon), to provide suppressive fire at the fire-team level. Indirect fire is provided by the M203 grenade launcher. The M1014 Joint Service Combat Shotgun or the Mossberg 590 Shotgun are used for door breaching and close-quarters combat. The M14EBR is used by designated marksmen. Snipers use the M107 Long Range Sniper Rifle, the M2010 Enhanced Sniper Rifle, and the M110 Semi-Automatic Sniper Rifle.
The Pentagon bought 25,000 MRAP vehicles since 2007 in 25 variants through rapid acquisition with no long-term plans for the platforms. The Army plans to divest 7,456 vehicles and retain 8,585. Of the total number of vehicles the Army will keep, 5,036 will be put in storage, 1,073 will be used for training, and the remainder will be spread across the active force. The Oshkosh M-ATV will be kept the most at 5,681 vehicles, as it is smaller and lighter than other MRAPs for off-road mobility. The other most retained vehicle will be the Navistar MaxxPro Dash with 2,633 vehicles, plus 301 Maxxpro ambulances. Thousands of other MRAPs like the Cougar, BAE Caiman, and larger MaxxPros will be disposed of.
The U.S. Army black beret (having been permanently replaced with the patrol cap) is no longer worn with the new ACU for garrison duty. After years of complaints that it wasn't suited well for most work conditions, Army Chief of Staff General Martin Dempsey eliminated it for wear with the ACU in June 2011. Soldiers still wear berets who are currently in a unit in jump status, whether the wearer is parachute-qualified, or not (maroon beret), Members of the 75th Ranger Regiment and the Airborne and Ranger Training Brigade (tan beret), and Special Forces (rifle green beret) and may wear it with the Army Service Uniform for non-ceremonial functions. Unit commanders may still direct the wear of patrol caps in these units in training environments or motor pools.
As a uniformed military service, the Army is part of the Department of the Army, which is one of the three military departments of the Department of Defense. The U.S. Army is headed by a civilian senior appointed civil servant, the Secretary of the Army (SECARMY), and by a chief military officer, the Chief of Staff of the Army (CSA) who is also a member of the Joint Chiefs of Staff. In the fiscal year 2016, the projected end strength for the Regular Army (USA) was 475,000 soldiers; the Army National Guard (ARNG) had 342,000 soldiers, and the United States Army Reserve (USAR) had 198,000 soldiers; the combined-component strength of the U.S. Army was 1,015,000 soldiers. As a branch of the armed forces, the mission of the U.S. Army is "to fight and win our Nation's wars, by providing prompt, sustained, land dominance, across the full range of military operations and the spectrum of conflict, in support of combatant commanders." The service participates in conflicts worldwide and is the major ground-based offensive and defensive force.
The army's major campaign against the Indians was fought in Florida against Seminoles. It took long wars (1818–58) to finally defeat the Seminoles and move them to Oklahoma. The usual strategy in Indian wars was to seize control of the Indians winter food supply, but that was no use in Florida where there was no winter. The second strategy was to form alliances with other Indian tribes, but that too was useless because the Seminoles had destroyed all the other Indians when they entered Florida in the late eighteenth century.
During the Cold War, American troops and their allies fought Communist forces in Korea and Vietnam. The Korean War began in 1950, when the Soviets walked out of a U.N. Security meeting, removing their possible veto. Under a United Nations umbrella, hundreds of thousands of U.S. troops fought to prevent the takeover of South Korea by North Korea, and later, to invade the northern nation. After repeated advances and retreats by both sides, and the PRC People's Volunteer Army's entry into the war, the Korean Armistice Agreement returned the peninsula to the status quo in 1953.
By 1989 Germany was nearing reunification and the Cold War was coming to a close. Army leadership reacted by starting to plan for a reduction in strength. By November 1989 Pentagon briefers were laying out plans to reduce army end strength by 23%, from 750,000 to 580,000. A number of incentives such as early retirement were used. In 1990 Iraq invaded its smaller neighbor, Kuwait, and U.S. land forces, quickly deployed to assure the protection of Saudi Arabia. In January 1991 Operation Desert Storm commenced, a U.S.-led coalition which deployed over 500,000 troops, the bulk of them from U.S. Army formations, to drive out Iraqi forces. The campaign ended in total victory, as Western coalition forces routed the Iraqi Army, organized along Soviet lines, in just one hundred hours.
The task of organizing the U.S. Army commenced in 1775. In the first one hundred years of its existence, the United States Army was maintained as a small peacetime force to man permanent forts and perform other non-wartime duties such as engineering and construction works. During times of war, the U.S. Army was augmented by the much larger United States Volunteers which were raised independently by various state governments. States also maintained full-time militias which could also be called into the service of the army.
The United States Army is made up of three components: the active component, the Regular Army; and two reserve components, the Army National Guard and the Army Reserve. Both reserve components are primarily composed of part-time soldiers who train once a month, known as battle assemblies or unit training assemblies (UTAs), and conduct two to three weeks of annual training each year. Both the Regular Army and the Army Reserve are organized under Title 10 of the United States Code, while the National Guard is organized under Title 32. While the Army National Guard is organized, trained and equipped as a component of the U.S. Army, when it is not in federal service it is under the command of individual state and territorial governors; the District of Columbia National Guard, however, reports to the U.S. President, not the district's mayor, even when not federalized. Any or all of the National Guard can be federalized by presidential order and against the governor's wishes.
Following their basic and advanced training at the individual-level, soldiers may choose to continue their training and apply for an "additional skill identifier" (ASI). The ASI allows the army to take a wide ranging MOS and focus it into a more specific MOS. For example, a combat medic, whose duties are to provide pre-hospital emergency treatment, may receive ASI training to become a cardiovascular specialist, a dialysis specialist, or even a licensed practical nurse. For commissioned officers, ASI training includes pre-commissioning training either at USMA, or via ROTC, or by completing OCS. After commissioning, officers undergo branch specific training at the Basic Officer Leaders Course, (formerly called Officer Basic Course), which varies in time and location according their future assignments. Further career development is available through the Army Correspondence Course Program.
The army has relied heavily on tents to provide the various facilities needed while on deployment. The most common tent uses for the military are as temporary barracks (sleeping quarters), DFAC buildings (dining facilities), forward operating bases (FOBs), after action review (AAR), tactical operations center (TOC), morale, welfare, and recreation (MWR) facilities, and security checkpoints. Furthermore, most of these tents are set up and operated through the support of Natick Soldier Systems Center.
The American Civil War was the costliest war for the U.S. in terms of casualties. After most slave states, located in the southern U.S., formed the Confederate States, C.S. troops led by former U.S. Army officers, mobilized a very large fraction of Southern white manpower. Forces of the United States (the "Union" or "the North") formed the Union Army consisting of a small body of regular army units and a large body of volunteer units raised from every state, north and south, except South Carolina.[citation needed]
Starting in 1910, the army began acquiring fixed-wing aircraft. In 1910, Mexico was having a civil war, peasant rebels fighting government soldiers. The army was deployed to American towns near the border to ensure safety to lives and property. In 1916, Pancho Villa, a major rebel leader, attacked Columbus, New Mexico, prompting a U.S. intervention in Mexico until 7 February 1917. They fought the rebels and the Mexican federal troops until 1918. The United States joined World War I in 1917 on the side of Britain, France, Russia, Italy and other allies. U.S. troops were sent to the Western Front and were involved in the last offensives that ended the war. With the armistice in November 1918, the army once again decreased its forces.
During the 1960s the Department of Defense continued to scrutinize the reserve forces and to question the number of divisions and brigades as well as the redundancy of maintaining two reserve components, the Army National Guard and the Army Reserve. In 1967 Secretary of Defense Robert McNamara decided that 15 combat divisions in the Army National Guard were unnecessary and cut the number to 8 divisions (1 mechanized infantry, 2 armored, and 5 infantry), but increased the number of brigades from 7 to 18 (1 airborne, 1 armored, 2 mechanized infantry, and 14 infantry). The loss of the divisions did not set well with the states. Their objections included the inadequate maneuver element mix for those that remained and the end to the practice of rotating divisional commands among the states that supported them. Under the proposal, the remaining division commanders were to reside in the state of the division base. No reduction, however, in total Army National Guard strength was to take place, which convinced the governors to accept the plan. The states reorganized their forces accordingly between 1 December 1967 and 1 May 1968.
On September 11, 2001, 53 Army civilians (47 employees and six contractors) and 22 soldiers were among the 125 victims killed in the Pentagon in a terrorist attack when American Airlines Flight 77 commandeered by five Al-Qaeda hijackers slammed into the western side of the building, as part of the September 11 attacks. Lieutenant General Timothy Maude was the highest-ranking military official killed at the Pentagon, and the most senior U.S. Army officer killed by foreign action since the death of Lieutenant General Simon B. Buckner, Jr. on June 18, 1945, in the Battle of Okinawa during World War II.
The army is also changing its base unit from divisions to brigades. Division lineage will be retained, but the divisional headquarters will be able to command any brigade, not just brigades that carry their divisional lineage. The central part of this plan is that each brigade will be modular, i.e., all brigades of the same type will be exactly the same, and thus any brigade can be commanded by any division. As specified before the 2013 end-strength re-definitions, the three major types of ground combat brigades are:
The army employs various individual weapons to provide light firepower at short ranges. The most common weapons used by the army are the compact variant of the M16 rifle, the M4 carbine, as well as the 7.62×51mm variant of the FN SCAR for Army Rangers. The primary sidearm in the U.S. Army is the 9 mm M9 pistol; the M11 pistol is also used. Both handguns are to be replaced through the Modular Handgun System program. Soldiers are also equiped with various hand grenades, such as the M67 fragmentation grenade and M18 smoke grenade.
The army's most common vehicle is the High Mobility Multipurpose Wheeled Vehicle (HMMWV), commonly called the Humvee, which is capable of serving as a cargo/troop carrier, weapons platform, and ambulance, among many other roles. While they operate a wide variety of combat support vehicles, one of the most common types centers on the family of HEMTT vehicles. The M1A2 Abrams is the army's main battle tank, while the M2A3 Bradley is the standard infantry fighting vehicle. Other vehicles include the Stryker, and the M113 armored personnel carrier, and multiple types of Mine Resistant Ambush Protected (MRAP) vehicles.
Jefferson's metaphor of a wall of separation has been cited repeatedly by the U.S. Supreme Court. In Reynolds v. United States (1879) the Court wrote that Jefferson's comments "may be accepted almost as an authoritative declaration of the scope and effect of the [First] Amendment." In Everson v. Board of Education (1947), Justice Hugo Black wrote: "In the words of Thomas Jefferson, the clause against establishment of religion by law was intended to erect a wall of separation between church and state."
Many early immigrant groups traveled to America to worship freely, particularly after the English Civil War and religious conflict in France and Germany. They included nonconformists like the Puritans, who were Protestant Christians fleeing religious persecution from the Anglican King of England. Despite a common background, the groups' views on religious toleration were mixed. While some such as Roger Williams of Rhode Island and William Penn of Pennsylvania ensured the protection of religious minorities within their colonies, others like the Plymouth Colony and Massachusetts Bay Colony had established churches. The Dutch colony of New Netherland established the Dutch Reformed Church and outlawed all other worship, though enforcement was sparse. Religious conformity was desired partly for financial reasons: the established Church was responsible for poverty relief, putting dissenting churches at a significant disadvantage.
^Note 2: in 1789 the Georgia Constitution was amended as follows: "Article IV. Section 10. No person within this state shall, upon any pretense, be deprived of the inestimable privilege of worshipping God in any manner agreeable to his own conscience, nor be compelled to attend any place of worship contrary to his own faith and judgment; nor shall he ever be obliged to pay tithes, taxes, or any other rate, for the building or repairing any place of worship, or for the maintenance of any minister or ministry, contrary to what he believes to be right, or hath voluntarily engaged to do. No one religious society shall ever be established in this state, in preference to another; nor shall any person be denied the enjoyment of any civil right merely on account of his religious principles."
^Note 5: The North Carolina Constitution of 1776 disestablished the Anglican church, but until 1835 the NC Constitution allowed only Protestants to hold public office. From 1835-1876 it allowed only Christians (including Catholics) to hold public office. Article VI, Section 8 of the current NC Constitution forbids only atheists from holding public office. Such clauses were held by the United States Supreme Court to be unenforceable in the 1961 case of Torcaso v. Watkins, when the court ruled unanimously that such clauses constituted a religious test incompatible with First and Fourteenth Amendment protections.
The Flushing Remonstrance shows support for separation of church and state as early as the mid-17th century, stating their opposition to religious persecution of any sort: "The law of love, peace and liberty in the states extending to Jews, Turks and Egyptians, as they are considered sons of Adam, which is the glory of the outward state of Holland, so love, peace and liberty, extending to all in Christ Jesus, condemns hatred, war and bondage." The document was signed December 27, 1657 by a group of English citizens in America who were affronted by persecution of Quakers and the religious policies of the Governor of New Netherland, Peter Stuyvesant. Stuyvesant had formally banned all religions other than the Dutch Reformed Church from being practiced in the colony, in accordance with the laws of the Dutch Republic. The signers indicated their "desire therefore in this case not to judge lest we be judged, neither to condemn least we be condemned, but rather let every man stand or fall to his own Master." Stuyvesant fined the petitioners and threw them in prison until they recanted. However, John Bowne allowed the Quakers to meet in his home. Bowne was arrested, jailed, and sent to the Netherlands for trial; the Dutch court exonerated Bowne.
There were also opponents to the support of any established church even at the state level. In 1773, Isaac Backus, a prominent Baptist minister in New England, wrote against a state sanctioned religion, saying: "Now who can hear Christ declare, that his kingdom is, not of this world, and yet believe that this blending of church and state together can be pleasing to him?" He also observed that when "church and state are separate, the effects are happy, and they do not at all interfere with each other: but where they have been confounded together, no tongue nor pen can fully describe the mischiefs that have ensued." Thomas Jefferson's influential Virginia Statute for Religious Freedom was enacted in 1786, five years before the Bill of Rights.
The phrase "[A] hedge or wall of separation between the garden of the church and the wilderness of the world" was first used by Baptist theologian Roger Williams, the founder of the colony of Rhode Island, in his 1644 book The Bloody Tenent of Persecution. The phrase was later used by Thomas Jefferson as a description of the First Amendment and its restriction on the legislative branch of the federal government, in an 1802 letter to the Danbury Baptists (a religious minority concerned about the dominant position of the Congregationalist church in Connecticut):
Jefferson and James Madison's conceptions of separation have long been debated. Jefferson refused to issue Proclamations of Thanksgiving sent to him by Congress during his presidency, though he did issue a Thanksgiving and Prayer proclamation as Governor of Virginia. Madison issued four religious proclamations while President, but vetoed two bills on the grounds they violated the first amendment. On the other hand, both Jefferson and Madison attended religious services at the Capitol. Years before the ratification of the Constitution, Madison contended "Because if Religion be exempt from the authority of the Society at large, still less can it be subject to that of the Legislative Body." After retiring from the presidency, Madison wrote of "total separation of the church from the state." " "Strongly guarded as is the separation between Religion & Govt in the Constitution of the United States," Madison wrote, and he declared, "practical distinction between Religion and Civil Government is essential to the purity of both, and as guaranteed by the Constitution of the United States." In a letter to Edward Livingston Madison further expanded, "We are teaching the world the great truth that Govts. do better without Kings & Nobles than with them. The merit will be doubled by the other lesson that Religion flourishes in greater purity, without than with the aid of Govt." Madison's original draft of the Bill of Rights had included provisions binding the States, as well as the Federal Government, from an establishment of religion, but the House did not pass them.[citation needed]
Jefferson's opponents said his position was the destruction and the governmental rejection of Christianity, but this was a caricature. In setting up the University of Virginia, Jefferson encouraged all the separate sects to have preachers of their own, though there was a constitutional ban on the State supporting a Professorship of Divinity, arising from his own Virginia Statute for Religious Freedom. Some have argued that this arrangement was "fully compatible with Jefferson's views on the separation of church and state;" however, others point to Jefferson's support for a scheme in which students at the University would attend religious worship each morning as evidence that his views were not consistent with strict separation. Still other scholars, such as Mark David Hall, attempt to sidestep the whole issue by arguing that American jurisprudence focuses too narrowly on this one Jeffersonian letter while failing to account for other relevant history
Jefferson's letter entered American jurisprudence in the 1878 Mormon polygamy case Reynolds v. U.S., in which the court cited Jefferson and Madison, seeking a legal definition for the word religion. Writing for the majority, Justice Stephen Johnson Field cited Jefferson's Letter to the Danbury Baptists to state that "Congress was deprived of all legislative power over mere opinion, but was left free to reach actions which were in violation of social duties or subversive of good order." Considering this, the court ruled that outlawing polygamy was constitutional.
Jefferson and Madison's approach was not the only one taken in the eighteenth century. Jefferson's Statute of Religious Freedom was drafted in opposition to a bill, chiefly supported by Patrick Henry, which would permit any Virginian to belong to any denomination, but which would require him to belong to some denomination and pay taxes to support it. Similarly, the Constitution of Massachusetts originally provided that "no subject shall be hurt, molested, or restrained, in his person, liberty, or estate, for worshipping God in the manner and season most agreeable to the dictates of his own conscience... provided he doth not disturb the public peace, or obstruct others in their religious worship," (Article II) but also that:
The Duke of York had required that every community in his new lands of New York and New Jersey support some church, but this was more often Dutch Reformed, Quaker or Presbyterian, than Anglican. Some chose to support more than one church. He also ordained that the tax-payers were free, having paid his local tax, to choose their own church. The terms for the surrender of New Amsterdam had provided that the Dutch would have liberty of conscience, and the Duke, as an openly divine-right Catholic, was no friend of Anglicanism. The first Anglican minister in New Jersey arrived in 1698, though Anglicanism was more popular in New York.
The original charter of the Province of East Jersey had restricted membership in the Assembly to Christians; the Duke of York was fervently Catholic, and the proprietors of Perth Amboy, New Jersey were Scottish Catholic peers. The Province of West Jersey had declared, in 1681, that there should be no religious test for office. An oath had also been imposed on the militia during the French and Indian War requiring them to abjure the pretensions of the Pope, which may or may not have been applied during the Revolution. That law was replaced by 1799.
The first amendment to the US Constitution states "Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof" The two parts, known as the "establishment clause" and the "free exercise clause" respectively, form the textual basis for the Supreme Court's interpretations of the "separation of church and state" doctrine. Three central concepts were derived from the 1st Amendment which became America's doctrine for church-state separation: no coercion in religious matters, no expectation to support a religion against one's will, and religious liberty encompasses all religions. In sum, citizens are free to embrace or reject a faith, any support for religion - financial or physical - must be voluntary, and all religions are equal in the eyes of the law with no special preference or favoritism.
Some legal scholars, such as John Baker of LSU, theorize that Madison's initial proposed language—that Congress should make no law regarding the establishment of a "national religion"—was rejected by the House, in favor of the more general "religion" in an effort to appease the Anti-Federalists. To both the Anti-Federalists and the Federalists, the very word "national" was a cause for alarm because of the experience under the British crown. During the debate over the establishment clause, Rep. Elbridge Gerry of Massachusetts took issue with Madison's language regarding whether the government was a national or federal government (in which the states retained their individual sovereignty), which Baker suggests compelled Madison to withdraw his language from the debate.
Others, such as Rep. Roger Sherman of Connecticut, believed the clause was unnecessary because the original Constitution only gave Congress stated powers, which did not include establishing a national religion. Anti-Federalists such as Rep. Thomas Tucker of South Carolina moved to strike the establishment clause completely because it could preempt the religious clauses in the state constitutions. However, the Anti-Federalists were unsuccessful in persuading the House of Representatives to drop the clause from the first amendment.
The Fourteenth Amendment to the United States Constitution (Amendment XIV) is one of the post-Civil War amendments, intended to secure rights for former slaves. It includes the due process and equal protection clauses among others. The amendment introduces the concept of incorporation of all relevant federal rights against the states. While it has not been fully implemented, the doctrine of incorporation has been used to ensure, through the Due Process Clause and Privileges and Immunities Clause, the application of most of the rights enumerated in the Bill of Rights to the states.
The incorporation of the First Amendment establishment clause in the landmark case of Everson v. Board of Education has impacted the subsequent interpretation of the separation of church and state in regard to the state governments. Although upholding the state law in that case, which provided for public busing to private religious schools, the Supreme Court held that the First Amendment establishment clause was fully applicable to the state governments. A more recent case involving the application of this principle against the states was Board of Education of Kiryas Joel Village School District v. Grumet (1994).
Jefferson's concept of "separation of church and state" first became a part of Establishment Clause jurisprudence in Reynolds v. U.S., 98 U.S. 145 (1878). In that case, the court examined the history of religious liberty in the US, determining that while the constitution guarantees religious freedom, "The word 'religion' is not defined in the Constitution. We must go elsewhere, therefore, to ascertain its meaning, and nowhere more appropriately, we think, than to the history of the times in the midst of which the provision was adopted." The court found that the leaders in advocating and formulating the constitutional guarantee of religious liberty were James Madison and Thomas Jefferson. Quoting the "separation" paragraph from Jefferson's letter to the Danbury Baptists, the court concluded that, "coming as this does from an acknowledged leader of the advocates of the measure, it may be accepted almost as an authoritative declaration of the scope and effect of the amendment thus secured."
The centrality of the "separation" concept to the Religion Clauses of the Constitution was made explicit in Everson v. Board of Education, 330 U.S. 1 (1947), a case dealing with a New Jersey law that allowed government funds to pay for transportation of students to both public and Catholic schools. This was the first case in which the court applied the Establishment Clause to the laws of a state, having interpreted the due process clause of the Fourteenth Amendment as applying the Bill of Rights to the states as well as the federal legislature. Citing Jefferson, the court concluded that "The First Amendment has erected a wall between church and state. That wall must be kept high and impregnable. We could not approve the slightest breach."
While the decision (with four dissents) ultimately upheld the state law allowing the funding of transportation of students to religious schools, the majority opinion (by Justice Hugo Black) and the dissenting opinions (by Justice Wiley Blount Rutledge and Justice Robert H. Jackson) each explicitly stated that the Constitution has erected a "wall between church and state" or a "separation of Church from State": their disagreement was limited to whether this case of state funding of transportation to religious schools breached that wall. Rutledge, on behalf of the four dissenting justices, took the position that the majority had indeed permitted a violation of the wall of separation in this case: "Neither so high nor so impregnable today as yesterday is the wall raised between church and state by Virginia's great statute of religious freedom and the First Amendment, now made applicable to all the states by the Fourteenth." Writing separately, Justice Jackson argued that "[T]here are no good grounds upon which to support the present legislation. In fact, the undertones of the opinion, advocating complete and uncompromising separation of Church from State, seem utterly discordant with its conclusion yielding support to their commingling in educational matters."
In 1962, the Supreme Court addressed the issue of officially-sponsored prayer or religious recitations in public schools. In Engel v. Vitale, 370 U.S. 421 (1962), the Court, by a vote of 6-1, determined it unconstitutional for state officials to compose an official school prayer and require its recitation in public schools, even when the prayer is non-denominational and students may excuse themselves from participation. (The prayer required by the New York State Board of Regents prior to the Court's decision consisted of: "Almighty God, we acknowledge our dependence upon Thee, and we beg Thy blessings upon us, our parents, our teachers, and our country. Amen.") As the Court stated:
The court noted that it "is a matter of history that this very practice of establishing governmentally composed prayers for religious services was one of the reasons which caused many of our early colonists to leave England and seek religious freedom in America." The lone dissenter, Justice Potter Stewart, objected to the court's embrace of the "wall of separation" metaphor: "I think that the Court's task, in this as in all areas of constitutional adjudication, is not responsibly aided by the uncritical invocation of metaphors like the "wall of separation," a phrase nowhere to be found in the Constitution."
In Epperson v. Arkansas, 393 U.S. 97 (1968), the Supreme Court considered an Arkansas law that made it a crime "to teach the theory or doctrine that mankind ascended or descended from a lower order of animals," or "to adopt or use in any such institution a textbook that teaches" this theory in any school or university that received public funds. The court's opinion, written by Justice Abe Fortas, ruled that the Arkansas law violated "the constitutional prohibition of state laws respecting an establishment of religion or prohibiting the free exercise thereof. The overriding fact is that Arkansas' law selects from the body of knowledge a particular segment which it proscribes for the sole reason that it is deemed to conflict with a particular religious doctrine; that is, with a particular interpretation of the Book of Genesis by a particular religious group." The court held that the Establishment Clause prohibits the state from advancing any religion, and that "[T]he state has no legitimate interest in protecting any or all religions from views distasteful to them." 
In Lemon v. Kurtzman, 403 U.S. 602 (1971), the court determined that a Pennsylvania state policy of reimbursing the salaries and related costs of teachers of secular subjects in private religious schools violated the Establishment Clause. The court's decision argued that the separation of church and state could never be absolute: "Our prior holdings do not call for total separation between church and state; total separation is not possible in an absolute sense. Some relationship between government and religious organizations is inevitable," the court wrote. "Judicial caveats against entanglement must recognize that the line of separation, far from being a "wall," is a blurred, indistinct, and variable barrier depending on all the circumstances of a particular relationship."
Subsequent to this decision, the Supreme Court has applied a three-pronged test to determine whether government action comports with the Establishment Clause, known as the "Lemon Test". First, the law or policy must have been adopted with a neutral or non-religious purpose. Second, the principle or primary effect must be one that neither advances nor inhibits religion. Third, the statute or policy must not result in an "excessive entanglement" of government with religion. (The decision in Lemon v. Kurtzman hinged upon the conclusion that the government benefits were flowing disproportionately to Catholic schools, and that Catholic schools were an integral component of the Catholic Church's religious mission, thus the policy involved the state in an "excessive entanglement" with religion.) Failure to meet any of these criteria is a proof that the statute or policy in question violates the Establishment Clause.
In 2002, a three judge panel on the Ninth Circuit Court of Appeals held that classroom recitation of the Pledge of Allegiance in a California public school was unconstitutional, even when students were not compelled to recite it, due to the inclusion of the phrase "under God." In reaction to the case, Elk Grove Unified School District v. Newdow, both houses of Congress passed measures reaffirming their support for the pledge, and condemning the panel's ruling. The case was appealed to the Supreme Court, where the case was ultimately overturned in June 2004, solely on procedural grounds not related to the substantive constitutional issue. Rather, a five-justice majority held that Newdow, a non-custodial parent suing on behalf of his daughter, lacked standing to sue.
On December 20, 2005, the United States Court of Appeals for the Sixth Circuit ruled in the case of ACLU v. Mercer County that the continued display of the Ten Commandments as part of a larger display on American legal traditions in a Kentucky courthouse was allowed, because the purpose of the display (educating the public on American legal traditions) was secular in nature. In ruling on the Mount Soledad cross controversy on May 3, 2006, however, a federal judge ruled that the cross on public property on Mount Soledad must be removed.
In what will be the case is Town of Greece v. Galloway, 12-696, the Supreme Court agreed to hear a case regarding whether prayers at town meetings, which are allowed, must allow various faiths to lead prayer, or whether the prayers can be predominately Christian. On May 5, 2014, the U.S. Supreme Court ruled 5-4 in favor of the Town of Greece by holding that the U.S. Constitution not only allows for prayer at government meetings, but also for sectarian prayers like predominately Christian prayers.
Some scholars and organizations disagree with the notion of "separation of church and state", or the way the Supreme Court has interpreted the constitutional limitation on religious establishment. Such critics generally argue that the phrase misrepresents the textual requirements of the Constitution, while noting that many aspects of church and state were intermingled at the time the Constitution was ratified. These critics argue that the prevalent degree of separation of church and state could not have been intended by the constitutional framers. Some of the intermingling between church and state include religious references in official contexts, and such other founding documents as the United States Declaration of Independence, which references the idea of a "Creator" and "Nature's God", though these references did not ultimately appear in the Constitution nor do they mention any particular religious view of a "Creator" or "Nature's God."
These critics of the modern separation of church and state also note the official establishment of religion in several of the states at the time of ratification, to suggest that the modern incorporation of the Establishment Clause as to state governments goes against the original constitutional intent.[citation needed] The issue is complex, however, as the incorporation ultimately bases on the passage of the 14th Amendment in 1868, at which point the first amendment's application to the state government was recognized. Many of these constitutional debates relate to the competing interpretive theories of originalism versus modern, progressivist theories such as the doctrine of the Living Constitution. Other debates center on the principle of the law of the land in America being defined not just by the Constitution's Supremacy Clause, but also by legal precedence, making an accurate reading of the Constitution subject to the mores and values of a given era, and rendering the concept of historical revisionism irrelevant when discussing the Constitution.
The "religious test" clause has been interpreted to cover both elected officials and appointed ones, career civil servants as well as political appointees. Religious beliefs or the lack of them have therefore not been permissible tests or qualifications with regard to federal employees since the ratification of the Constitution. Seven states, however, have language included in their Bill of Rights, Declaration of Rights, or in the body of their constitutions that require state office-holders to have particular religious beliefs, though some of these have been successfully challenged in court. These states are Texas, Massachusetts, Maryland, North Carolina, Pennsylvania, South Carolina, and Tennessee.
The required beliefs of these clauses include belief in a Supreme Being and belief in a future state of rewards and punishments. (Tennessee Constitution Article IX, Section 2 is one such example.) Some of these same states specify that the oath of office include the words "so help me God." In some cases these beliefs (or oaths) were historically required of jurors and witnesses in court. At one time, such restrictions were allowed under the doctrine of states' rights; today they are deemed to be in violation of the federal First Amendment, as applied to the states via the 14th amendment, and hence unconstitutional and unenforceable.
Relaxed zoning rules and special parking privileges for churches, the tax-free status of church property, the fact that Christmas is a federal holiday, etc., have also been questioned, but have been considered examples of the governmental prerogative in deciding practical and beneficial arrangements for the society. The national motto "In God We Trust" has been challenged as a violation, but the Supreme Court has ruled that ceremonial deism is not religious in nature. A circuit court ruling affirmed Ohio's right to use as its motto a passage from the Bible, "With God, all things are possible", because it displayed no preference for a particular religion.
Jeffries and Ryan (2001) argue that the modern concept of separation of church and state dates from the mid-twentieth century rulings of the Supreme Court. The central point, they argue, was a constitutional ban against aid to religious schools, followed by a later ban on religious observance in public education. Jeffries and Ryan argue that these two propositions—that public aid should not go to religious schools and that public schools should not be religious—make up the separationist position of the modern Establishment Clause.
Jeffries and Ryan argue that no-aid position drew support from a coalition of separationist opinion. Most important was "the pervasive secularism that came to dominate American public life," which sought to confine religion to a private sphere. Further, the ban against government aid to religious schools was supported before 1970 by most Protestants (and most Jews), who opposed aid to religious schools, which were mostly Catholic at the time. After 1980, however, anti-Catholic sentiment has diminished among mainline Protestants, and the crucial coalition of public secularists and Protestant churches has collapsed. While mainline Protestant denominations are more inclined towards strict separation of church and state, much evangelical opinion has now largely deserted that position. As a consequence, strict separationism is opposed today by members of many Protestant faiths, even perhaps eclipsing the opposition of Roman Catholics.[citation needed]
Critics of the modern concept of the "separation of church and state" argue that it is untethered to anything in the text of the constitution and is contrary to the conception of the phrase as the Founding Fathers understood it. Philip Hamburger, Columbia Law school professor and prominent critic of the modern understanding of the concept, maintains that the modern concept, which deviates from the constitutional establishment clause jurisprudence, is rooted in American anti-Catholicism and Nativism.[citation needed] Briefs before the Supreme Court, including by the U.S. government, have argued that some state constitutional amendments relating to the modern conception of separation of church and state (Blaine Amendments) were motivated by and intended to enact anti-Catholicism.
J. Brent Walker, Executive Director of the Baptist Joint Committee, responded to Hamburger's claims noting; "The fact that the separation of church and state has been supported by some who exhibited an anti-Catholic animus or a secularist bent does not impugn the validity of the principle. Champions of religious liberty have argued for the separation of church and state for reasons having nothing to do with anti-Catholicism or desire for a secular culture. Of course, separationists have opposed the Catholic Church when it has sought to tap into the public till to support its parochial schools or to argue for on-campus released time in the public schools. But that principled debate on the issues does not support a charge of religious bigotry"
Steven Waldman notes that; "The evangelicals provided the political muscle for the efforts of Madison and Jefferson, not merely because they wanted to block official churches but because they wanted to keep the spiritual and secular worlds apart." "Religious freedom resulted from an alliance of unlikely partners," writes the historian Frank Lambert in his book The Founding Fathers and the Place of Religion in America. "New Light evangelicals such as Isaac Bachus and John Leland joined forces with Deists and skeptics such as James Madison and Thomas Jefferson to fight for a complete separation of church and state."
Robert N. Bellah has in his writings that although the separation of church and state is grounded firmly in the constitution of the United States, this does not mean that there is no religious dimension in the political society of the United States. He used the term "Civil Religion" to describe the specific relation between politics and religion in the United States. His 1967 article analyzes the inaugural speech of John F. Kennedy: "Considering the separation of church and state, how is a president justified in using the word 'God' at all? The answer is that the separation of church and state has not denied the political realm a religious dimension."
Robert S. Wood has argued that the United States is a model for the world in terms of how a separation of church and state—no state-run or state-established church—is good for both the church and the state, allowing a variety of religions to flourish. Speaking at the Toronto-based Center for New Religions, Wood said that the freedom of conscience and assembly allowed under such a system has led to a "remarkable religiosity" in the United States that isn't present in other industrialized nations. Wood believes that the U.S. operates on "a sort of civic religion," which includes a generally-shared belief in a creator who "expects better of us." Beyond that, individuals are free to decide how they want to believe and fill in their own creeds and express their conscience. He calls this approach the "genius of religious sentiment in the United States."
In the human digestive system, food enters the mouth and mechanical digestion of the food starts by the action of mastication (chewing), a form of mechanical digestion, and the wetting contact of saliva. Saliva, a liquid secreted by the salivary glands, contains salivary amylase, an enzyme which starts the digestion of starch in the food; the saliva also contains mucus, which lubricates the food, and hydrogen carbonate, which provides the ideal conditions of pH (alkaline) for amylase to work. After undergoing mastication and starch digestion, the food will be in the form of a small, round slurry mass called a bolus. It will then travel down the esophagus and into the stomach by the action of peristalsis. Gastric juice in the stomach starts protein digestion. Gastric juice mainly contains hydrochloric acid and pepsin. As these two chemicals may damage the stomach wall, mucus is secreted by the stomach, providing a slimy layer that acts as a shield against the damaging effects of the chemicals. At the same time protein digestion is occurring, mechanical mixing occurs by peristalsis, which is waves of muscular contractions that move along the stomach wall. This allows the mass of food to further mix with the digestive enzymes.
Other animals, such as rabbits and rodents, practise coprophagia behaviours - eating specialised faeces in order to re-digest food, especially in the case of roughage. Capybara, rabbits, hamsters and other related species do not have a complex digestive system as do, for example, ruminants. Instead they extract more nutrition from grass by giving their food a second pass through the gut. Soft faecal pellets of partially digested food are excreted and generally consumed immediately. They also produce normal droppings, which are not eaten.
Digestive systems take many forms. There is a fundamental distinction between internal and external digestion. External digestion developed earlier in evolutionary history, and most fungi still rely on it. In this process, enzymes are secreted into the environment surrounding the organism, where they break down an organic material, and some of the products diffuse back to the organism. Animals have a tube (gastrointestinal tract) in which internal digestion occurs, which is more efficient because more of the broken down products can be captured, and the internal chemical environment can be more efficiently controlled.
The nitrogen fixing Rhizobia are an interesting case, wherein conjugative elements naturally engage in inter-kingdom conjugation. Such elements as the Agrobacterium Ti or Ri plasmids contain elements that can transfer to plant cells. Transferred genes enter the plant cell nucleus and effectively transform the plant cells into factories for the production of opines, which the bacteria use as carbon and energy sources. Infected plant cells form crown gall or root tumors. The Ti and Ri plasmids are thus endosymbionts of the bacteria, which are in turn endosymbionts (or parasites) of the infected plant.
Teeth (singular tooth) are small whitish structures found in the jaws (or mouths) of many vertebrates that are used to tear, scrape, milk and chew food. Teeth are not made of bone, but rather of tissues of varying density and hardness, such as enamel, dentine and cementum. Human teeth have a blood and nerve supply which enables proprioception. This is the ability of sensation when chewing, for example if we were to bite into something too hard for our teeth, such as a chipped plate mixed in food, our teeth send a message to our brain and we realise that it cannot be chewed, so we stop trying.
The abomasum is the fourth and final stomach compartment in ruminants. It is a close equivalent of a monogastric stomach (e.g., those in humans or pigs), and digesta is processed here in much the same way. It serves primarily as a site for acid hydrolysis of microbial and dietary protein, preparing these protein sources for further digestion and absorption in the small intestine. Digesta is finally moved into the small intestine, where the digestion and absorption of nutrients occurs. Microbes produced in the reticulo-rumen are also digested in the small intestine.
An earthworm's digestive system consists of a mouth, pharynx, esophagus, crop, gizzard, and intestine. The mouth is surrounded by strong lips, which act like a hand to grab pieces of dead grass, leaves, and weeds, with bits of soil to help chew. The lips break the food down into smaller pieces. In the pharynx, the food is lubricated by mucus secretions for easier passage. The esophagus adds calcium carbonate to neutralize the acids formed by food matter decay. Temporary storage occurs in the crop where food and calcium carbonate are mixed. The powerful muscles of the gizzard churn and mix the mass of food and dirt. When the churning is complete, the glands in the walls of the gizzard add enzymes to the thick paste, which helps chemically breakdown the organic matter. By peristalsis, the mixture is sent to the intestine where friendly bacteria continue chemical breakdown. This releases carbohydrates, protein, fat, and various vitamins and minerals for absorption into the body.
Digestion of some fats can begin in the mouth where lingual lipase breaks down some short chain lipids into diglycerides. However fats are mainly digested in the small intestine. The presence of fat in the small intestine produces hormones that stimulate the release of pancreatic lipase from the pancreas and bile from the liver which helps in the emulsification of fats for absorption of fatty acids. Complete digestion of one molecule of fat (a triglyceride) results a mixture of fatty acids, mono- and di-glycerides, as well as some undigested triglycerides, but no free glycerol molecules.
Digestion is the breakdown of large insoluble food molecules into small water-soluble food molecules so that they can be absorbed into the watery blood plasma. In certain organisms, these smaller substances are absorbed through the small intestine into the blood stream. Digestion is a form of catabolism that is often divided into two processes based on how food is broken down: mechanical and chemical digestion. The term mechanical digestion refers to the physical breakdown of large pieces of food into smaller pieces which can subsequently be accessed by digestive enzymes. In chemical digestion, enzymes break down food into the small molecules the body can use.
Different phases of digestion take place including: the cephalic phase , gastric phase, and intestinal phase. The cephalic phase occurs at the sight, thought and smell of food, which stimulate the cerebral cortex. Taste and smell stimuli are sent to the hypothalamus and medulla oblongata. After this it is routed through the vagus nerve and release of acetylcholine. Gastric secretion at this phase rises to 40% of maximum rate. Acidity in the stomach is not buffered by food at this point and thus acts to inhibit parietal (secretes acid) and G cell (secretes gastrin) activity via D cell secretion of somatostatin. The gastric phase takes 3 to 4 hours. It is stimulated by distension of the stomach, presence of food in stomach and decrease in pH. Distention activates long and myenteric reflexes. This activates the release of acetylcholine, which stimulates the release of more gastric juices. As protein enters the stomach, it binds to hydrogen ions, which raises the pH of the stomach. Inhibition of gastrin and gastric acid secretion is lifted. This triggers G cells to release gastrin, which in turn stimulates parietal cells to secrete gastric acid. Gastric acid is about 0.5% hydrochloric acid (HCl), which lowers the pH to the desired pH of 1-3. Acid release is also triggered by acetylcholine and histamine. The intestinal phase has two parts, the excitatory and the inhibitory. Partially digested food fills the duodenum. This triggers intestinal gastrin to be released. Enterogastric reflex inhibits vagal nuclei, activating sympathetic fibers causing the pyloric sphincter to tighten to prevent more food from entering, and inhibits local reflexes.
In a channel transupport system, several proteins form a contiguous channel traversing the inner and outer membranes of the bacteria. It is a simple system, which consists of only three protein subunits: the ABC protein, membrane fusion protein (MFP), and outer membrane protein (OMP)[specify]. This secretion system transports various molecules, from ions, drugs, to proteins of various sizes (20 - 900 kDa). The molecules secreted vary in size from the small Escherichia coli peptide colicin V, (10 kDa) to the Pseudomonas fluorescens cell adhesion protein LapA of 900 kDa.
In addition to the use of the multiprotein complexes listed above, Gram-negative bacteria possess another method for release of material: the formation of outer membrane vesicles. Portions of the outer membrane pinch off, forming spherical structures made of a lipid bilayer enclosing periplasmic materials. Vesicles from a number of bacterial species have been found to contain virulence factors, some have immunomodulatory effects, and some can directly adhere to and intoxicate host cells. While release of vesicles has been demonstrated as a general response to stress conditions, the process of loading cargo proteins seems to be selective.
Underlying the process is muscle movement throughout the system through swallowing and peristalsis. Each step in digestion requires energy, and thus imposes an "overhead charge" on the energy made available from absorbed substances. Differences in that overhead cost are important influences on lifestyle, behavior, and even physical structures. Examples may be seen in humans, who differ considerably from other hominids (lack of hair, smaller jaws and musculature, different dentition, length of intestines, cooking, etc.).
Digestion begins in the mouth with the secretion of saliva and its digestive enzymes. Food is formed into a bolus by the mechanical mastication and swallowed into the esophagus from where it enters the stomach through the action of peristalsis. Gastric juice contains hydrochloric acid and pepsin which would damage the walls of the stomach and mucus is secreted for protection. In the stomach further release of enzymes break down the food further and this is combined with the churning action of the stomach. The partially digested food enters the duodenum as a thick semi-liquid chyme. In the small intestine, the larger part of digestion takes place and this is helped by the secretions of bile, pancreatic juice and intestinal juice. The intestinal walls are lined with villi, and their epithelial cells is covered with numerous microvilli to improve the absorption of nutrients by increasing the surface area of the intestine.
Lactase is an enzyme that breaks down the disaccharide lactose to its component parts, glucose and galactose. Glucose and galactose can be absorbed by the small intestine. Approximately 65 percent of the adult population produce only small amounts of lactase and are unable to eat unfermented milk-based foods. This is commonly known as lactose intolerance. Lactose intolerance varies widely by ethnic heritage; more than 90 percent of peoples of east Asian descent are lactose intolerant, in contrast to about 5 percent of people of northern European descent.
After some time (typically 1–2 hours in humans, 4–6 hours in dogs, 3–4 hours in house cats),[citation needed] the resulting thick liquid is called chyme. When the pyloric sphincter valve opens, chyme enters the duodenum where it mixes with digestive enzymes from the pancreas and bile juice from the liver and then passes through the small intestine, in which digestion continues. When the chyme is fully digested, it is absorbed into the blood. 95% of absorption of nutrients occurs in the small intestine. Water and minerals are reabsorbed back into the blood in the colon (large intestine) where the pH is slightly acidic about 5.6 ~ 6.9. Some vitamins, such as biotin and vitamin K (K2MK7) produced by bacteria in the colon are also absorbed into the blood in the colon. Waste material is eliminated from the rectum during defecation.
In mammals, preparation for digestion begins with the cephalic phase in which saliva is produced in the mouth and digestive enzymes are produced in the stomach. Mechanical and chemical digestion begin in the mouth where food is chewed, and mixed with saliva to begin enzymatic processing of starches. The stomach continues to break food down mechanically and chemically through churning and mixing with both acids and enzymes. Absorption occurs in the stomach and gastrointestinal tract, and the process finishes with defecation.
Protein digestion occurs in the stomach and duodenum in which 3 main enzymes, pepsin secreted by the stomach and trypsin and chymotrypsin secreted by the pancreas, break down food proteins into polypeptides that are then broken down by various exopeptidases and dipeptidases into amino acids. The digestive enzymes however are mostly secreted as their inactive precursors, the zymogens. For example, trypsin is secreted by pancreas in the form of trypsinogen, which is activated in the duodenum by enterokinase to form trypsin. Trypsin then cleaves proteins to smaller polypeptides.
Hyderabad (i/ˈhaɪdərəˌbæd/ HY-dər-ə-bad; often /ˈhaɪdrəˌbæd/) is the capital of the southern Indian state of Telangana and de jure capital of Andhra Pradesh.[A] Occupying 650 square kilometres (250 sq mi) along the banks of the Musi River, it has a population of about 6.7 million and a metropolitan population of about 7.75 million, making it the fourth most populous city and sixth most populous urban agglomeration in India. At an average altitude of 542 metres (1,778 ft), much of Hyderabad is situated on hilly terrain around artificial lakes, including Hussain Sagar—predating the city's founding—north of the city centre.
Established in 1591 by Muhammad Quli Qutb Shah, Hyderabad remained under the rule of the Qutb Shahi dynasty for nearly a century before the Mughals captured the region. In 1724, Mughal viceroy Asif Jah I declared his sovereignty and created his own dynasty, known as the Nizams of Hyderabad. The Nizam's dominions became a princely state during the British Raj, and remained so for 150 years, with the city serving as its capital. The Nizami influence can still be seen in the culture of the Hyderabadi Muslims. The city continued as the capital of Hyderabad State after it was brought into the Indian Union in 1948, and became the capital of Andhra Pradesh after the States Reorganisation Act, 1956. Since 1956, Rashtrapati Nilayam in the city has been the winter office of the President of India. In 2014, the newly formed state of Telangana split from Andhra Pradesh and the city became joint capital of the two states, a transitional arrangement scheduled to end by 2025.
Relics of Qutb Shahi and Nizam rule remain visible today, with the Charminar—commissioned by Muhammad Quli Qutb Shah—coming to symbolise Hyderabad. Golconda fort is another major landmark. The influence of Mughlai culture is also evident in the city's distinctive cuisine, which includes Hyderabadi biryani and Hyderabadi haleem. The Qutb Shahis and Nizams established Hyderabad as a cultural hub, attracting men of letters from different parts of the world. Hyderabad emerged as the foremost centre of culture in India with the decline of the Mughal Empire in the mid-19th century, with artists migrating to the city from the rest of the Indian subcontinent. While Hyderabad is losing its cultural pre-eminence, it is today, due to the Telugu film industry, the country's second-largest producer of motion pictures.
Hyderabad was historically known as a pearl and diamond trading centre, and it continues to be known as the City of Pearls. Many of the city's traditional bazaars, including Laad Bazaar, Begum Bazaar and Sultan Bazaar, have remained open for centuries. However, industrialisation throughout the 20th century attracted major Indian manufacturing, research and financial institutions, including Bharat Heavy Electricals Limited, the National Geophysical Research Institute and the Centre for Cellular and Molecular Biology. Special economic zones dedicated to information technology have encouraged companies from across India and around the world to set up operations and the emergence of pharmaceutical and biotechnology industries in the 1990s led to the area's naming as India's "Genome Valley". With an output of US$74 billion, Hyderabad is the fifth-largest contributor to India's overall gross domestic product.
According to John Everett-Heath, the author of Oxford Concise Dictionary of World Place Names, Hyderabad means "Haydar's city" or "lion city", from haydar (lion) and ābād (city). It was named to honour the Caliph Ali Ibn Abi Talib, who was also known as Haydar because of his lion-like valour in battles. Andrew Petersen, a scholar of Islamic architecture, says the city was originally called Baghnagar (city of gardens). One popular theory suggests that Muhammad Quli Qutb Shah, the founder of the city, named it "Bhagyanagar" or "Bhāgnagar" after Bhagmati, a local nautch (dancing) girl with whom he had fallen in love. She converted to Islam and adopted the title Hyder Mahal. The city was renamed Hyderabad in her honour. According to another source, the city was named after Haidar, the son of Quli Qutb Shah.
Archaeologists excavating near the city have unearthed Iron Age sites that may date from 500 BCE. The region comprising modern Hyderabad and its surroundings was known as Golkonda (Golla Konda-"shepherd's hill"), and was ruled by the Chalukya dynasty from 624 CE to 1075 CE. Following the dissolution of the Chalukya empire into four parts in the 11th century, Golkonda came under the control of the Kakatiya dynasty from 1158, whose seat of power was at Warangal, 148 km (92 mi) northeast of modern Hyderabad.
The Kakatiya dynasty was reduced to a vassal of the Khilji dynasty in 1310 after its defeat by Sultan Alauddin Khilji of the Delhi Sultanate. This lasted until 1321, when the Kakatiya dynasty was annexed by Malik Kafur, Allaudin Khilji's general. During this period, Alauddin Khilji took the Koh-i-Noor diamond, which is said to have been mined from the Kollur Mines of Golkonda, to Delhi. Muhammad bin Tughluq succeeded to the Delhi sultanate in 1325, bringing Warangal under the rule of the Tughlaq dynasty until 1347 when Ala-ud-Din Bahman Shah, a governor under bin Tughluq, rebelled against Delhi and established the Bahmani Sultanate in the Deccan Plateau, with Gulbarga, 200 km (124 mi) west of Hyderabad, as its capital. The Bahmani kings ruled the region until 1518 and were the first independent Muslim rulers of the Deccan.
Sultan Quli, a governor of Golkonda, revolted against the Bahmani Sultanate and established the Qutb Shahi dynasty in 1518; he rebuilt the mud-fort of Golconda and named the city "Muhammad nagar". The fifth sultan, Muhammad Quli Qutb Shah, established Hyderabad on the banks of the Musi River in 1591, to avoid the water shortages experienced at Golkonda. During his rule, he had the Charminar and Mecca Masjid built in the city. On 21 September 1687, the Golkonda Sultanate came under the rule of the Mughal emperor Aurangzeb after a year-long siege of the Golkonda fort. The annexed area was renamed Deccan Suba (Deccan province) and the capital was moved from Golkonda to Aurangabad, about 550 km (342 mi) northwest of Hyderabad.
In 1713 Farrukhsiyar, the Mughal emperor, appointed Asif Jah I to be Viceroy of the Deccan, with the title Nizam-ul-Mulk (Administrator of the Realm). In 1724, Asif Jah I defeated Mubariz Khan to establish autonomy over the Deccan Suba, named the region Hyderabad Deccan, and started what came to be known as the Asif Jahi dynasty. Subsequent rulers retained the title Nizam ul-Mulk and were referred to as Asif Jahi Nizams, or Nizams of Hyderabad. The death of Asif Jah I in 1748 resulted in a period of political unrest as his sons, backed by opportunistic neighbouring states and colonial foreign forces, contended for the throne. The accession of Asif Jah II, who reigned from 1762 to 1803, ended the instability. In 1768 he signed the treaty of Masulipatnam, surrendering the coastal region to the East India Company in return for a fixed annual rent.
In 1769 Hyderabad city became the formal capital of the Nizams. In response to regular threats from Hyder Ali (Dalwai of Mysore), Baji Rao I (Peshwa of the Maratha Empire), and Basalath Jung (Asif Jah II's elder brother, who was supported by the Marquis de Bussy-Castelnau), the Nizam signed a subsidiary alliance with the East India Company in 1798, allowing the British Indian Army to occupy Bolarum (modern Secunderabad) to protect the state's borders, for which the Nizams paid an annual maintenance to the British.
After India gained independence, the Nizam declared his intention to remain independent rather than become part of the Indian Union. The Hyderabad State Congress, with the support of the Indian National Congress and the Communist Party of India, began agitating against Nizam VII in 1948. On 17 September that year, the Indian Army took control of Hyderabad State after an invasion codenamed Operation Polo. With the defeat of his forces, Nizam VII capitulated to the Indian Union by signing an Instrument of Accession, which made him the Rajpramukh (Princely Governor) of the state until 31 October 1956. Between 1946 and 1951, the Communist Party of India fomented the Telangana uprising against the feudal lords of the Telangana region. The Constitution of India, which became effective on 26 January 1950, made Hyderabad State one of the part B states of India, with Hyderabad city continuing to be the capital. In his 1955 report Thoughts on Linguistic States, B. R. Ambedkar, then chairman of the Drafting Committee of the Indian Constitution, proposed designating the city of Hyderabad as the second capital of India because of its amenities and strategic central location. Since 1956, the Rashtrapati Nilayam in Hyderabad has been the second official residence and business office of the President of India; the President stays once a year in winter and conducts official business particularly relating to Southern India.
On 1 November 1956 the states of India were reorganised by language. Hyderabad state was split into three parts, which were merged with neighbouring states to form the modern states of Maharashtra, Karnataka and Andhra Pradesh. The nine Telugu- and Urdu-speaking districts of Hyderabad State in the Telangana region were merged with the Telugu-speaking Andhra State to create Andhra Pradesh, with Hyderabad as its capital. Several protests, known collectively as the Telangana movement, attempted to invalidate the merger and demanded the creation of a new Telangana state. Major actions took place in 1969 and 1972, and a third began in 2010. The city suffered several explosions: one at Dilsukhnagar in 2002 claimed two lives; terrorist bombs in May and August 2007 caused communal tension and riots; and two bombs exploded in February 2013. On 30 July 2013 the government of India declared that part of Andhra Pradesh would be split off to form a new Telangana state, and that Hyderabad city would be the capital city and part of Telangana, while the city would also remain the capital of Andhra Pradesh for no more than ten years. On 3 October 2013 the Union Cabinet approved the proposal, and in February 2014 both houses of Parliament passed the Telangana Bill. With the final assent of the President of India in June 2014, Telangana state was formed.
Situated in the southern part of Telangana in southeastern India, Hyderabad is 1,566 kilometres (973 mi) south of Delhi, 699 kilometres (434 mi) southeast of Mumbai, and 570 kilometres (350 mi) north of Bangalore by road. It lies on the banks of the Musi River, in the northern part of the Deccan Plateau. Greater Hyderabad covers 650 km2 (250 sq mi), making it one of the largest metropolitan areas in India. With an average altitude of 542 metres (1,778 ft), Hyderabad lies on predominantly sloping terrain of grey and pink granite, dotted with small hills, the highest being Banjara Hills at 672 metres (2,205 ft). The city has numerous lakes referred to as sagar, meaning "sea". Examples include artificial lakes created by dams on the Musi, such as Hussain Sagar (built in 1562 near the city centre), Osman Sagar and Himayat Sagar. As of 1996, the city had 140 lakes and 834 water tanks (ponds).
Hyderabad has a tropical wet and dry climate (Köppen Aw) bordering on a hot semi-arid climate (Köppen BSh). The annual mean temperature is 26.6 °C (79.9 °F); monthly mean temperatures are 21–33 °C (70–91 °F). Summers (March–June) are hot and humid, with average highs in the mid-to-high 30s Celsius; maximum temperatures often exceed 40 °C (104 °F) between April and June. The coolest temperatures occur in December and January, when the lowest temperature occasionally dips to 10 °C (50 °F). May is the hottest month, when daily temperatures range from 26 to 39 °C (79–102 °F); December, the coldest, has temperatures varying from 14.5 to 28 °C (57–82 °F).
Hyderabad's lakes and the sloping terrain of its low-lying hills provide habitat for an assortment of flora and fauna. The forest region in and around the city encompasses areas of ecological and biological importance, which are preserved in the form of national parks, zoos, mini-zoos and a wildlife sanctuary. Nehru Zoological Park, the city's one large zoo, is the first in India to have a lion and tiger safari park. Hyderabad has three national parks (Mrugavani National Park, Mahavir Harina Vanasthali National Park and Kasu Brahmananda Reddy National Park), and the Manjira Wildlife Sanctuary is about 50 km (31 mi) from the city. Hyderabad's other environmental reserves are: Kotla Vijayabhaskara Reddy Botanical Gardens, Shamirpet Lake, Hussain Sagar, Fox Sagar Lake, Mir Alam Tank and Patancheru Lake, which is home to regional birds and attracts seasonal migratory birds from different parts of the world. Organisations engaged in environmental and wildlife preservation include the Telangana Forest Department, Indian Council of Forestry Research and Education, the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT), the Animal Welfare Board of India, the Blue Cross of Hyderabad and the University of Hyderabad.
The Greater Hyderabad Municipal Corporation (GHMC) oversees the civic infrastructure of the city's 18 "circles", which together encompass 150 municipal wards. Each ward is represented by a corporator, elected by popular vote. The corporators elect the Mayor, who is the titular head of GHMC; executive powers rest with the Municipal Commissioner, appointed by the state government. The GHMC carries out the city's infrastructural work such as building and maintenance of roads and drains, town planning including construction regulation, maintenance of municipal markets and parks, solid waste management, the issuing of birth and death certificates, the issuing of trade licences, collection of property tax, and community welfare services such as mother and child healthcare, and pre-school and non-formal education. The GHMC was formed in April 2007 by merging the Municipal Corporation of Hyderabad (MCH) with 12 municipalities of the Hyderabad, Ranga Reddy and Medak districts covering a total area of 650 km2 (250 sq mi).:3 In the 2016 municipal election, the Telangana Rashtra Samithi formed the majority and the present Mayor is Bonthu Ram Mohan. The Secunderabad Cantonment Board is a civic administration agency overseeing an area of 40.1 km2 (15.5 sq mi),:93 where there are several military camps.:2 The Osmania University campus is administered independently by the university authority.:93
The jurisdictions of the city's administrative agencies are, in ascending order of size: the Hyderabad Police area, Hyderabad district, the GHMC area ("Hyderabad city") and the area under the Hyderabad Metropolitan Development Authority (HMDA). The HMDA is an apolitical urban planning agency that covers the GHMC and its suburbs, extending to 54 mandals in five districts encircling the city. It coordinates the development activities of GHMC and suburban municipalities and manages the administration of bodies such as the Hyderabad Metropolitan Water Supply and Sewerage Board (HMWSSB).
The HMWSSB regulates rainwater harvesting, sewerage services and water supply, which is sourced from several dams located in the suburbs. In 2005, the HMWSSB started operating a 116-kilometre-long (72 mi) water supply pipeline from Nagarjuna Sagar Dam to meet increasing demand. The Telangana Southern Power Distribution Company Limited manages electricity supply. As of October 2014, there were 15 fire stations in the city, operated by the Telangana State Disaster and Fire Response Department. The government-owned India Post has five head post offices and many sub-post offices in Hyderabad, which are complemented by private courier services.
Hyderabad produces around 4,500 tonnes of solid waste daily, which is transported from collection units in Imlibun, Yousufguda and Lower Tank Bund to the dumpsite in Jawaharnagar. Disposal is managed by the Integrated Solid Waste Management project which was started by the GHMC in 2010. Rapid urbanisation and increased economic activity has also led to increased industrial waste, air, noise and water pollution, which is regulated by the Telangana Pollution Control Board (TPCB). The contribution of different sources to air pollution in 2006 was: 20–50% from vehicles, 40–70% from a combination of vehicle discharge and road dust, 10–30% from industrial discharges and 3–10% from the burning of household rubbish. Deaths resulting from atmospheric particulate matter are estimated at 1,700–3,000 each year. Ground water around Hyderabad, which has a hardness of up to 1000 ppm, around three times higher than is desirable, is the main source of drinking water but the increasing population and consequent increase in demand has led to a decline in not only ground water but also river and lake levels. This shortage is further exacerbated by inadequately treated effluent discharged from industrial treatment plants polluting the water sources of the city.
The Commissionerate of Health and Family Welfare is responsible for planning, implementation and monitoring of all facilities related to health and preventive services. As of 2010[update]–11, the city had 50 government hospitals, 300 private and charity hospitals and 194 nursing homes providing around 12,000 hospital beds, fewer than half the required 25,000. For every 10,000 people in the city, there are 17.6 hospital beds, 9 specialist doctors, 14 nurses and 6 physicians. The city also has about 4,000 individual clinics and 500 medical diagnostic centres. Private clinics are preferred by many residents because of the distance to, poor quality of care at and long waiting times in government facilities,:60–61 despite the high proportion of the city's residents being covered by government health insurance: 24% according to a National Family Health Survey in 2005.:41 As of 2012[update], many new private hospitals of various sizes were opened or being built. Hyderabad also has outpatient and inpatient facilities that use Unani, homeopathic and Ayurvedic treatments.
In the 2005 National Family Health Survey, it was reported that the city's total fertility rate is 1.8,:47 which is below the replacement rate. Only 61% of children had been provided with all basic vaccines (BCG, measles and full courses of polio and DPT), fewer than in all other surveyed cities except Meerut.:98 The infant mortality rate was 35 per 1,000 live births, and the mortality rate for children under five was 41 per 1,000 live births.:97 The survey also reported that a third of women and a quarter of men are overweight or obese, 49% of children below 5 years are anaemic, and up to 20% of children are underweight,:44, 55–56 while more than 2% of women and 3% of men suffer from diabetes.:57
When the GHMC was created in 2007, the area occupied by the municipality increased from 175 km2 (68 sq mi) to 650 km2 (250 sq mi). Consequently, the population increased by 87%, from 3,637,483 in the 2001 census to 6,809,970 in the 2011 census, 24% of which are migrants from elsewhere in India,:2 making Hyderabad the nation's fourth most populous city. As of 2011[update], the population density is 18,480/km2 (47,900/sq mi). At the same 2011 census, the Hyderabad Urban Agglomeration had a population of 7,749,334, making it the sixth most populous urban agglomeration in the country. The population of the Hyderabad urban agglomeration has since been estimated by electoral officials to be 9.1 million as of early 2013 but is expected to exceed 10 million by the end of the year. There are 3,500,802 male and 3,309,168 female citizens—a sex ratio of 945 females per 1000 males, higher than the national average of 926 per 1000. Among children aged 0–6 years, 373,794 are boys and 352,022 are girls—a ratio of 942 per 1000. Literacy stands at 82.96% (male 85.96%; female 79.79%), higher than the national average of 74.04%. The socio-economic strata consist of 20% upper class, 50% middle class and 30% working class.
Referred to as "Hyderabadi", the residents of Hyderabad are predominantly Telugu and Urdu speaking people, with minority Bengali, Gujarati (including Memon), Kannada (including Nawayathi), Malayalam, Marathi, Marwari, Odia, Punjabi, Tamil and Uttar Pradeshi communities. Hyderabad is home to a unique dialect of Urdu called Hyderabadi Urdu, which is a type of Dakhini, and is the mother tongue of most Hyderabadi Muslims, a unique community who owe much of their history, language, cuisine, and culture to Hyderabad, and the various dynasties who previously ruled. Hadhrami Arabs, African Arabs, Armenians, Abyssinians, Iranians, Pathans and Turkish people are also present; these communities, of which the Hadhrami are the largest, declined after Hyderabad State became part of the Indian Union, as they lost the patronage of the Nizams.
In the greater metropolitan area, 13% of the population live below the poverty line. According to a 2012 report submitted by GHMC to the World Bank, Hyderabad has 1,476 slums with a total population of 1.7 million, of whom 66% live in 985 slums in the "core" of the city (the part that formed Hyderabad before the April 2007 expansion) and the remaining 34% live in 491 suburban tenements. About 22% of the slum-dwelling households had migrated from different parts of India in the last decade of the 20th century, and 63% claimed to have lived in the slums for more than 10 years.:55 Overall literacy in the slums is 60–80% and female literacy is 52–73%. A third of the slums have basic service connections, and the remainder depend on general public services provided by the government. There are 405 government schools, 267 government aided schools, 175 private schools and 528 community halls in the slum areas.:70 According to a 2008 survey by the Centre for Good Governance, 87.6% of the slum-dwelling households are nuclear families, 18% are very poor, with an income up to ₹20000 (US$300) per annum, 73% live below the poverty line (a standard poverty line recognised by the Andhra Pradesh Government is ₹24000 (US$360) per annum), 27% of the chief wage earners (CWE) are casual labour and 38% of the CWE are illiterate. About 3.72% of the slum children aged 5–14 do not go to school and 3.17% work as child labour, of whom 64% are boys and 36% are girls. The largest employers of child labour are street shops and construction sites. Among the working children, 35% are engaged in hazardous jobs.:59
Many historic and tourist sites lie in south central Hyderabad, such as the Charminar, the Mecca Masjid, the Salar Jung Museum, the Nizam's Museum, the Falaknuma Palace, and the traditional retail corridor comprising the Pearl Market, Laad Bazaar and Madina Circle. North of the river are hospitals, colleges, major railway stations and business areas such as Begum Bazaar, Koti, Abids, Sultan Bazaar and Moazzam Jahi Market, along with administrative and recreational establishments such as the Reserve Bank of India, the Telangana Secretariat, the Hyderabad Mint, the Telangana Legislature, the Public Gardens, the Nizam Club, the Ravindra Bharathi, the State Museum, the Birla Temple and the Birla Planetarium.
North of central Hyderabad lie Hussain Sagar, Tank Bund Road, Rani Gunj and the Secunderabad Railway Station. Most of the city's parks and recreational centres, such as Sanjeevaiah Park, Indira Park, Lumbini Park, NTR Gardens, the Buddha statue and Tankbund Park are located here. In the northwest part of the city there are upscale residential and commercial areas such as Banjara Hills, Jubilee Hills, Begumpet, Khairatabad and Miyapur. The northern end contains industrial areas such as Sanathnagar, Moosapet, Balanagar, Patancheru and Chanda Nagar. The northeast end is dotted with residential areas. In the eastern part of the city lie many defence research centres and Ramoji Film City. The "Cyberabad" area in the southwest and west of the city has grown rapidly since the 1990s. It is home to information technology and bio-pharmaceutical companies and to landmarks such as Hyderabad Airport, Osman Sagar, Himayath Sagar and Kasu Brahmananda Reddy National Park.
Heritage buildings constructed during the Qutb Shahi and Nizam eras showcase Indo-Islamic architecture influenced by Medieval, Mughal and European styles. After the 1908 flooding of the Musi River, the city was expanded and civic monuments constructed, particularly during the rule of Mir Osman Ali Khan (the VIIth Nizam), whose patronage of architecture led to him being referred to as the maker of modern Hyderabad. In 2012, the government of India declared Hyderabad the first "Best heritage city of India".
Qutb Shahi architecture of the 16th and early 17th centuries followed classical Persian architecture featuring domes and colossal arches. The oldest surviving Qutb Shahi structure in Hyderabad is the ruins of Golconda fort built in the 16th century. The Charminar, Mecca Masjid, Charkaman and Qutb Shahi tombs are other existing structures of this period. Among these the Charminar has become an icon of the city; located in the centre of old Hyderabad, it is a square structure with sides 20 m (66 ft) long and four grand arches each facing a road. At each corner stands a 56 m (184 ft)-high minaret. Most of the historical bazaars that still exist were constructed on the street north of Charminar towards Golconda fort. The Charminar, Qutb Shahi tombs and Golconda fort are considered to be monuments of national importance in India; in 2010 the Indian government proposed that the sites be listed for UNESCO World Heritage status.:11–18
Among the oldest surviving examples of Nizam architecture in Hyderabad is the Chowmahalla Palace, which was the seat of royal power. It showcases a diverse array of architectural styles, from the Baroque Harem to its Neoclassical royal court. The other palaces include Falaknuma Palace (inspired by the style of Andrea Palladio), Purani Haveli, King Kothi and Bella Vista Palace all of which were built at the peak of Nizam rule in the 19th century. During Mir Osman Ali Khan's rule, European styles, along with Indo-Islamic, became prominent. These styles are reflected in the Falaknuma Palace and many civic monuments such as the Hyderabad High Court, Osmania Hospital, Osmania University, the State Central Library, City College, the Telangana Legislature, the State Archaeology Museum, Jubilee Hall, and Hyderabad and Kachiguda railway stations. Other landmarks of note are Paigah Palace, Asman Garh Palace, Basheer Bagh Palace, Errum Manzil and the Spanish Mosque, all constructed by the Paigah family.:16–17
Hyderabad is the largest contributor to the gross domestic product (GDP), tax and other revenues, of Telangana, and the sixth largest deposit centre and fourth largest credit centre nationwide, as ranked by the Reserve Bank of India (RBI) in June 2012. Its US$74 billion GDP made it the fifth-largest contributor city to India's overall GDP in 2011–12. Its per capita annual income in 2011 was ₹44300 (US$660). As of 2006[update], the largest employers in the city were the governments of Andhra Pradesh (113,098 employees) and India (85,155). According to a 2005 survey, 77% of males and 19% of females in the city were employed. The service industry remains dominant in the city, and 90% of the employed workforce is engaged in this sector.
Hyderabad's role in the pearl trade has given it the name "City of Pearls" and up until the 18th century, the city was also the only global trading centre for large diamonds. Industrialisation began under the Nizams in the late 19th century, helped by railway expansion that connected the city with major ports. From the 1950s to the 1970s, Indian enterprises, such as Bharat Heavy Electricals Limited (BHEL), Nuclear Fuel Complex (NFC), National Mineral Development Corporation (NMDC), Bharat Electronics (BEL), Electronics Corporation of India Limited (ECIL), Defence Research and Development Organisation (DRDO), Hindustan Aeronautics Limited (HAL), Centre for Cellular and Molecular Biology (CCMB), Centre for DNA Fingerprinting and Diagnostics (CDFD), State Bank of Hyderabad (SBH) and Andhra Bank (AB) were established in the city. The city is home to Hyderabad Securities formerly known as Hyderabad Stock Exchange (HSE), and houses the regional office of the Securities and Exchange Board of India (SEBI). In 2013, the Bombay Stock Exchange (BSE) facility in Hyderabad was forecast to provide operations and transactions services to BSE-Mumbai by the end of 2014. The growth of the financial services sector has helped Hyderabad evolve from a traditional manufacturing city to a cosmopolitan industrial service centre. Since the 1990s, the growth of information technology (IT), IT-enabled services (ITES), insurance and financial institutions has expanded the service sector, and these primary economic activities have boosted the ancillary sectors of trade and commerce, transport, storage, communication, real estate and retail.
The establishment of Indian Drugs and Pharmaceuticals Limited (IDPL), a public sector undertaking, in 1961 was followed over the decades by many national and global companies opening manufacturing and research facilities in the city. As of 2010[update], the city manufactured one third of India's bulk drugs and 16% of biotechnology products, contributing to its reputation as "India's pharmaceutical capital" and the "Genome Valley of India". Hyderabad is a global centre of information technology, for which it is known as Cyberabad (Cyber City). As of 2013[update], it contributed 15% of India's and 98% of Andhra Pradesh's exports in IT and ITES sectors and 22% of NASSCOM's total membership is from the city. The development of HITEC City, a township with extensive technological infrastructure, prompted multinational companies to establish facilities in Hyderabad. The city is home to more than 1300 IT and ITES firms, including global conglomerates such as Microsoft (operating its largest R&D campus outside the US), Google, IBM, Yahoo!, Dell, Facebook,:3 and major Indian firms including Tech Mahindra, Infosys, Tata Consultancy Services (TCS), Polaris and Wipro.:3 In 2009 the World Bank Group ranked the city as the second best Indian city for doing business. The city and its suburbs contain the highest number of special economic zones of any Indian city.
Like the rest of India, Hyderabad has a large informal economy that employs 30% of the labour force.:71 According to a survey published in 2007, it had 40–50,000 street vendors, and their numbers were increasing.:9 Among the street vendors, 84% are male and 16% female,:12 and four fifths are "stationary vendors" operating from a fixed pitch, often with their own stall.:15–16 Most are financed through personal savings; only 8% borrow from moneylenders.:19 Vendor earnings vary from ₹50 (74¢ US) to ₹800 (US$12) per day.:25 Other unorganised economic sectors include dairy, poultry farming, brick manufacturing, casual labour and domestic help. Those involved in the informal economy constitute a major portion of urban poor.:71
Hyderabad emerged as the foremost centre of culture in India with the decline of the Mughal Empire. After the fall of Delhi in 1857, the migration of performing artists to the city particularly from the north and west of the Indian sub continent, under the patronage of the Nizam, enriched the cultural milieu. This migration resulted in a mingling of North and South Indian languages, cultures and religions, which has since led to a co-existence of Hindu and Muslim traditions, for which the city has become noted.:viii A further consequence of this north–south mix is that both Telugu and Urdu are official languages of Telangana. The mixing of religions has also resulted in many festivals being celebrated in Hyderabad such as Ganesh Chaturthi, Diwali and Bonalu of Hindu tradition and Eid ul-Fitr and Eid al-Adha by Muslims.
In the past, Qutb Shahi rulers and Nizams attracted artists, architects and men of letters from different parts of the world through patronage. The resulting ethnic mix popularised cultural events such as mushairas (poetic symposia). The Qutb Shahi dynasty particularly encouraged the growth of Deccani Urdu literature leading to works such as the Deccani Masnavi and Diwan poetry, which are among the earliest available manuscripts in Urdu. Lazzat Un Nisa, a book compiled in the 15th century at Qutb Shahi courts, contains erotic paintings with diagrams for secret medicines and stimulants in the eastern form of ancient sexual arts. The reign of the Nizams saw many literary reforms and the introduction of Urdu as a language of court, administration and education. In 1824, a collection of Urdu Ghazal poetry, named Gulzar-e-Mahlaqa, authored by Mah Laqa Bai—the first female Urdu poet to produce a Diwan—was published in Hyderabad.
Hyderabad has continued with these traditions in its annual Hyderabad Literary Festival, held since 2010, showcasing the city's literary and cultural creativity. Organisations engaged in the advancement of literature include the Sahitya Akademi, the Urdu Academy, the Telugu Academy, the National Council for Promotion of Urdu Language, the Comparative Literature Association of India, and the Andhra Saraswata Parishad. Literary development is further aided by state institutions such as the State Central Library, the largest public library in the state which was established in 1891, and other major libraries including the Sri Krishna Devaraya Andhra Bhasha Nilayam, the British Library and the Sundarayya Vignana Kendram.
South Indian music and dances such as the Kuchipudi and Bharatanatyam styles are popular in the Deccan region. As a result of their culture policies, North Indian music and dance gained popularity during the rule of the Mughals and Nizams, and it was also during their reign that it became a tradition among the nobility to associate themselves with tawaif (courtesans). These courtesans were revered as the epitome of etiquette and culture, and were appointed to teach singing, poetry and classical dance to many children of the aristocracy. This gave rise to certain styles of court music, dance and poetry. Besides western and Indian popular music genres such as filmi music, the residents of Hyderabad play city-based marfa music, dholak ke geet (household songs based on local Folklore), and qawwali, especially at weddings, festivals and other celebratory events. The state government organises the Golconda Music and Dance Festival, the Taramati Music Festival and the Premavathi Dance Festival to further encourage the development of music.
Although the city is not particularly noted for theatre and drama, the state government promotes theatre with multiple programmes and festivals in such venues as the Ravindra Bharati, Shilpakala Vedika and Lalithakala Thoranam. Although not a purely music oriented event, Numaish, a popular annual exhibition of local and national consumer products, does feature some musical performances. The city is home to the Telugu film industry, popularly known as Tollywood and as of 2012[update], produces the second largest number of films in India behind Bollywood. Films in the local Hyderabadi dialect are also produced and have been gaining popularity since 2005. The city has also hosted international film festivals such as the International Children's Film Festival and the Hyderabad International Film Festival. In 2005, Guinness World Records declared Ramoji Film City to be the world's largest film studio.
The region is well known for its Golconda and Hyderabad painting styles which are branches of Deccani painting. Developed during the 16th century, the Golconda style is a native style blending foreign techniques and bears some similarity to the Vijayanagara paintings of neighbouring Mysore. A significant use of luminous gold and white colours is generally found in the Golconda style. The Hyderabad style originated in the 17th century under the Nizams. Highly influenced by Mughal painting, this style makes use of bright colours and mostly depicts regional landscape, culture, costumes and jewellery.
Although not a centre for handicrafts itself, the patronage of the arts by the Mughals and Nizams attracted artisans from the region to Hyderabad. Such crafts include: Bidriware, a metalwork handicraft from neighbouring Karnataka, which was popularised during the 18th century and has since been granted a Geographical Indication (GI) tag under the auspices of the WTO act; and Zari and Zardozi, embroidery works on textile that involve making elaborate designs using gold, silver and other metal threads. Another example of a handicraft drawn to Hyderabad is Kalamkari, a hand-painted or block-printed cotton textile that comes from cities in Andhra Pradesh. This craft is distinguished in having both a Hindu style, known as Srikalahasti and entirely done by hand, and an Islamic style, known as Machilipatnam that uses both hand and block techniques. Examples of Hyderabad's arts and crafts are housed in various museums including the Salar Jung Museum (housing "one of the largest one-man-collections in the world"), the AP State Archaeology Museum, the Nizam Museum, the City Museum and the Birla Science Museum.
Hyderabadi cuisine comprises a broad repertoire of rice, wheat and meat dishes and the skilled use of various spices. Hyderabadi biryani and Hyderabadi haleem, with their blend of Mughlai and Arab cuisines, have become iconic dishes of India. Hyderabadi cuisine is highly influenced by Mughlai and to some extent by French, Arabic, Turkish, Iranian and native Telugu and Marathwada cuisines. Other popular native dishes include nihari, chakna, baghara baingan and the desserts qubani ka meetha, double ka meetha and kaddu ki kheer (a sweet porridge made with sweet gourd).
One of Hyderabad's earliest newspapers, The Deccan Times, was established in the 1780s. In modern times, the major Telugu dailies published in Hyderabad are Eenadu, Andhra Jyothy, Sakshi and Namaste Telangana, while the major English papers are The Times of India, The Hindu and The Deccan Chronicle. The major Urdu papers include The Siasat Daily, The Munsif Daily and Etemaad. Many coffee table magazines, professional magazines and research journals are also regularly published. The Secunderabad Cantonment Board established the first radio station in Hyderabad State around 1919. Deccan Radio was the first radio public broadcast station in the city starting on 3 February 1935, with FM broadcasting beginning in 2000. The available channels in Hyderabad include All India Radio, Radio Mirchi, Radio City, Red FM and Big FM.
Television broadcasting in Hyderabad began in 1974 with the launch of Doordarshan, the Government of India's public service broadcaster, which transmits two free-to-air terrestrial television channels and one satellite channel. Private satellite channels started in July 1992 with the launch of Star TV. Satellite TV channels are accessible via cable subscription, direct-broadcast satellite services or internet-based television. Hyderabad's first dial-up internet access became available in the early 1990s and was limited to software development companies. The first public internet access service began in 1995, with the first private sector internet service provider (ISP) starting operations in 1998. In 2015, high-speed public WiFi was introduced in parts of the city.
Public and private schools in Hyderabad are governed by the Central Board of Secondary Education and follow a "10+2+3" plan. About two-thirds of pupils attend privately run institutions. Languages of instruction include English, Hindi, Telugu and Urdu. Depending on the institution, students are required to sit the Secondary School Certificate or the Indian Certificate of Secondary Education. After completing secondary education, students enroll in schools or junior colleges with a higher secondary facility. Admission to professional graduation colleges in Hyderabad, many of which are affiliated with either Jawaharlal Nehru Technological University Hyderabad (JNTUH) or Osmania University (OU), is through the Engineering Agricultural and Medical Common Entrance Test (EAM-CET).
There are 13 universities in Hyderabad: two private universities, two deemed universities, six state universities and three central universities. The central universities are the University of Hyderabad, Maulana Azad National Urdu University and the English and Foreign Languages University. Osmania University, established in 1918, was the first university in Hyderabad and as of 2012[update] is India's second most popular institution for international students. The Dr. B. R. Ambedkar Open University, established in 1982, is the first distance learning open university in India.
Hyderabad is also home to a number of centres specialising in particular fields such as biomedical sciences, biotechnology and pharmaceuticals, such as the National Institute of Pharmaceutical Education and Research (NIPER) and National Institute of Nutrition (NIN). Hyderabad has five major medical schools—Osmania Medical College, Gandhi Medical College, Nizam's Institute of Medical Sciences, Deccan College of Medical Sciences and Shadan Institute of Medical Sciences—and many affiliated teaching hospitals. The Government Nizamia Tibbi College is a college of Unani medicine. Hyderabad is also the headquarters of the Indian Heart Association, a non-profit foundation for cardiovascular education.
Institutes in Hyderabad include the National Institute of Rural Development, the Indian School of Business, the Institute of Public Enterprise, the Administrative Staff College of India and the Sardar Vallabhbhai Patel National Police Academy. Technical and engineering schools include the International Institute of Information Technology, Hyderabad (IIITH), Birla Institute of Technology and Science, Pilani – Hyderabad (BITS Hyderabad) and Indian Institute of Technology, Hyderabad (IIT-H) as well as agricultural engineering institutes such as the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and the Acharya N. G. Ranga Agricultural University. Hyderabad also has schools of fashion design including Raffles Millennium International, NIFT Hyderabad and Wigan and Leigh College. The National Institute of Design, Hyderabad (NID-H), will offer undergraduate and postgraduate courses from 2015.
The most popular sports played in Hyderabad are cricket and association football. At the professional level, the city has hosted national and international sports events such as the 2002 National Games of India, the 2003 Afro-Asian Games, the 2004 AP Tourism Hyderabad Open women's tennis tournament, the 2007 Military World Games, the 2009 World Badminton Championships and the 2009 IBSF World Snooker Championship. The city hosts a number of venues suitable for professional competition such as the Swarnandhra Pradesh Sports Complex for field hockey, the G. M. C. Balayogi Stadium in Gachibowli for athletics and football, and for cricket, the Lal Bahadur Shastri Stadium and Rajiv Gandhi International Cricket Stadium, home ground of the Hyderabad Cricket Association. Hyderabad has hosted many international cricket matches, including matches in the 1987 and the 1996 ICC Cricket World Cups. The Hyderabad cricket team represents the city in the Ranji Trophy—a first-class cricket tournament among India's states and cities. Hyderabad is also home to the Indian Premier League franchise Sunrisers Hyderabad. A previous franchise was the Deccan Chargers, which won the 2009 Indian Premier League held in South Africa.
During British rule, Secunderabad became a well-known sporting centre and many race courses, parade grounds and polo fields were built.:18 Many elite clubs formed by the Nizams and the British such as the Secunderabad Club, the Nizam Club and the Hyderabad Race Club, which is known for its horse racing especially the annual Deccan derby, still exist. In more recent times, motorsports has become popular with the Andhra Pradesh Motor Sports Club organising popular events such as the Deccan ¼ Mile Drag, TSD Rallies and 4x4 off-road rallying.
International-level sportspeople from Hyderabad include: cricketers Ghulam Ahmed, M. L. Jaisimha, Mohammed Azharuddin, V. V. S. Laxman, Venkatapathy Raju, Shivlal Yadav, Arshad Ayub, Syed Abid Ali and Noel David; football players Syed Abdul Rahim, Syed Nayeemuddin and Shabbir Ali; tennis player Sania Mirza; badminton players S. M. Arif, Pullela Gopichand, Saina Nehwal, P. V. Sindhu, Jwala Gutta and Chetan Anand; hockey players Syed Mohammad Hadi and Mukesh Kumar; rifle shooters Gagan Narang and Asher Noria and bodybuilder Mir Mohtesham Ali Khan.
The most commonly used forms of medium distance transport in Hyderabad include government owned services such as light railways and buses, as well as privately operated taxis and auto rickshaws. Bus services operate from the Mahatma Gandhi Bus Station in the city centre and carry over 130 million passengers daily across the entire network.:76 Hyderabad's light rail transportation system, the Multi-Modal Transport System (MMTS), is a three line suburban rail service used by over 160,000 passengers daily. Complementing these government services are minibus routes operated by Setwin (Society for Employment Promotion & Training in Twin Cities). Intercity rail services also operate from Hyderabad; the main, and largest, station is Secunderabad Railway Station, which serves as Indian Railways' South Central Railway zone headquarters and a hub for both buses and MMTS light rail services connecting Secunderabad and Hyderabad. Other major railway stations in Hyderabad are Hyderabad Deccan Station, Kachiguda Railway Station, Begumpet Railway Station, Malkajgiri Railway Station and Lingampally Railway Station. The Hyderabad Metro, a new rapid transit system, is to be added to the existing public transport infrastructure and is scheduled to operate three lines by 2015.
As of 2012[update], there are over 3.5 million vehicles operating in the city, of which 74% are two-wheelers, 15% cars and 3% three-wheelers. The remaining 8% include buses, goods vehicles and taxis. The large number of vehicles coupled with relatively low road coverage—roads occupy only 9.5% of the total city area:79—has led to widespread traffic congestion especially since 80% of passengers and 60% of freight are transported by road.:3 The Inner Ring Road, the Outer Ring Road, the Hyderabad Elevated Expressway, the longest flyover in India, and various interchanges, overpasses and underpasses were built to ease the congestion. Maximum speed limits within the city are 50 km/h (31 mph) for two-wheelers and cars, 35 km/h (22 mph) for auto rickshaws and 40 km/h (25 mph) for light commercial vehicles and buses.
Hyderabad sits at the junction of three National Highways linking it to six other states: NH-7 runs 2,369 km (1,472 mi) from Varanasi, Uttar Pradesh, in the north to Kanyakumari, Tamil Nadu, in the south; NH-9, runs 841 km (523 mi) east-west between Machilipatnam, Andhra Pradesh, and Pune, Maharashtra; and the 280 km (174 mi) NH-163 links Hyderabad to Bhopalpatnam, Chhattisgarh NH-765 links Hyderabad to Srisailam. Five state highways, SH-1, SH-2, SH-4, SH-5 and SH-6, either start from, or pass through, Hyderabad.:58
The region, as part of Lorraine, was part of the Holy Roman Empire, and then was gradually annexed by France in the 17th century, and formalized as one of the provinces of France. The Calvinist manufacturing republic of Mulhouse, known as Stadtrepublik Mülhausen, became a part of Alsace after a vote by its citizens on 4 January 1798. Alsace is frequently mentioned with and as part of Lorraine and the former duchy of Lorraine, since it was a vital part of the duchy, and later because German possession as the imperial province (Alsace-Lorraine, 1871–1918) was contested in the 19th and 20th centuries; France and Germany exchanged control of parts of Lorraine (including Alsace) four times in 75 years.
With the decline of the Roman Empire, Alsace became the territory of the Germanic Alemanni. The Alemanni were agricultural people, and their Germanic language formed the basis of modern-day dialects spoken along the Upper Rhine (Alsatian, Alemannian, Swabian, Swiss). Clovis and the Franks defeated the Alemanni during the 5th century AD, culminating with the Battle of Tolbiac, and Alsace became part of the Kingdom of Austrasia. Under Clovis' Merovingian successors the inhabitants were Christianized. Alsace remained under Frankish control until the Frankish realm, following the Oaths of Strasbourg of 842, was formally dissolved in 843 at the Treaty of Verdun; the grandsons of Charlemagne divided the realm into three parts. Alsace formed part of the Middle Francia, which was ruled by the youngest grandson Lothar I. Lothar died early in 855 and his realm was divided into three parts. The part known as Lotharingia, or Lorraine, was given to Lothar's son. The rest was shared between Lothar's brothers Charles the Bald (ruler of the West Frankish realm) and Louis the German (ruler of the East Frankish realm). The Kingdom of Lotharingia was short-lived, however, becoming the stem duchy of Lorraine in Eastern Francia after the Treaty of Ribemont in 880. Alsace was united with the other Alemanni east of the Rhine into the stem duchy of Swabia.
At about this time the surrounding areas experienced recurring fragmentation and reincorporations among a number of feudal secular and ecclesiastical lordships, a common process in the Holy Roman Empire. Alsace experienced great prosperity during the 12th and 13th centuries under Hohenstaufen emperors. Frederick I set up Alsace as a province (a procuratio, not a provincia) to be ruled by ministeriales, a non-noble class of civil servants. The idea was that such men would be more tractable and less likely to alienate the fief from the crown out of their own greed. The province had a single provincial court (Landgericht) and a central administration with its seat at Hagenau. Frederick II designated the Bishop of Strasbourg to administer Alsace, but the authority of the bishop was challenged by Count Rudolf of Habsburg, who received his rights from Frederick II's son Conrad IV. Strasbourg began to grow to become the most populous and commercially important town in the region. In 1262, after a long struggle with the ruling bishops, its citizens gained the status of free imperial city. A stop on the Paris-Vienna-Orient trade route, as well as a port on the Rhine route linking southern Germany and Switzerland to the Netherlands, England and Scandinavia, it became the political and economic center of the region. Cities such as Colmar and Hagenau also began to grow in economic importance and gained a kind of autonomy within the "Decapole" or "Dekapolis", a federation of ten free towns.
As in much of Europe, the prosperity of Alsace came to an end in the 14th century by a series of harsh winters, bad harvests, and the Black Death. These hardships were blamed on Jews, leading to the pogroms of 1336 and 1339. In 1349, Jews of Alsace were accused of poisoning the wells with plague, leading to the massacre of thousands of Jews during the Strasbourg pogrom. Jews were subsequently forbidden to settle in the town. An additional natural disaster was the Rhine rift earthquake of 1356, one of Europe's worst which made ruins of Basel. Prosperity returned to Alsace under Habsburg administration during the Renaissance.
Holy Roman Empire central power had begun to decline following years of imperial adventures in Italian lands, often ceding hegemony in Western Europe to France, which had long since centralized power. France began an aggressive policy of expanding eastward, first to the rivers Rhône and Meuse, and when those borders were reached, aiming for the Rhine. In 1299, the French proposed a marriage alliance between Philip IV of France's sister Blanche and Albert I of Germany's son Rudolf, with Alsace to be the dowry; however, the deal never came off. In 1307, the town of Belfort was first chartered by the Counts of Montbéliard. During the next century, France was to be militarily shattered by the Hundred Years' War, which prevented for a time any further tendencies in this direction. After the conclusion of the war, France was again free to pursue its desire to reach the Rhine and in 1444 a French army appeared in Lorraine and Alsace. It took up winter quarters, demanded the submission of Metz and Strasbourg and launched an attack on Basel.
In 1469, following the Treaty of St. Omer, Upper Alsace was sold by Archduke Sigismund of Austria to Charles the Bold, Duke of Burgundy. Although Charles was the nominal landlord, taxes were paid to Frederick III, Holy Roman Emperor. The latter was able to use this tax and a dynastic marriage to his advantage to gain back full control of Upper Alsace (apart from the free towns, but including Belfort) in 1477 when it became part of the demesne of the Habsburg family, who were also rulers of the empire. The town of Mulhouse joined the Swiss Confederation in 1515, where it was to remain until 1798.
By the time of the Protestant Reformation in the 16th century, Strasbourg was a prosperous community, and its inhabitants accepted Protestantism in 1523. Martin Bucer was a prominent Protestant reformer in the region. His efforts were countered by the Roman Catholic Habsburgs who tried to eradicate heresy in Upper Alsace. As a result, Alsace was transformed into a mosaic of Catholic and Protestant territories. On the other hand, Mömpelgard (Montbéliard) to the southwest of Alsace, belonging to the Counts of Württemberg since 1397, remained a Protestant enclave in France until 1793.
This situation prevailed until 1639, when most of Alsace was conquered by France so as to keep it out of the hands of the Spanish Habsburgs, who wanted a clear road to their valuable and rebellious possessions in the Spanish Netherlands. Beset by enemies and seeking to gain a free hand in Hungary, the Habsburgs sold their Sundgau territory (mostly in Upper Alsace) to France in 1646, which had occupied it, for the sum of 1.2 million Thalers. When hostilities were concluded in 1648 with the Treaty of Westphalia, most of Alsace was recognized as part of France, although some towns remained independent. The treaty stipulations regarding Alsace were complex; although the French king gained sovereignty, existing rights and customs of the inhabitants were largely preserved. France continued to maintain its customs border along the Vosges mountains where it had been, leaving Alsace more economically oriented to neighbouring German-speaking lands. The German language remained in use in local administration, in schools, and at the (Lutheran) University of Strasbourg, which continued to draw students from other German-speaking lands. The 1685 Edict of Fontainebleau, by which the French king ordered the suppression of French Protestantism, was not applied in Alsace. France did endeavour to promote Catholicism; Strasbourg Cathedral, for example, which had been Lutheran from 1524 to 1681, was returned to the Catholic Church. However, compared to the rest of France, Alsace enjoyed a climate of religious tolerance.
The year 1789 brought the French Revolution and with it the first division of Alsace into the départements of Haut- and Bas-Rhin. Alsatians played an active role in the French Revolution. On 21 July 1789, after receiving news of the Storming of the Bastille in Paris, a crowd of people stormed the Strasbourg city hall, forcing the city administrators to flee and putting symbolically an end to the feudal system in Alsace. In 1792, Rouget de Lisle composed in Strasbourg the Revolutionary marching song "La Marseillaise" (as Marching song for the Army of the Rhine), which later became the anthem of France. "La Marseillaise" was played for the first time in April of that year in front of the mayor of Strasbourg Philippe-Frédéric de Dietrich. Some of the most famous generals of the French Revolution also came from Alsace, notably Kellermann, the victor of Valmy, Kléber, who led the armies of the French Republic in Vendée and Westermann, who also fought in the Vendée.
At the same time, some Alsatians were in opposition to the Jacobins and sympathetic to the invading forces of Austria and Prussia who sought to crush the nascent revolutionary republic. Many of the residents of the Sundgau made "pilgrimages" to places like Mariastein Abbey, near Basel, in Switzerland, for baptisms and weddings. When the French Revolutionary Army of the Rhine was victorious, tens of thousands fled east before it. When they were later permitted to return (in some cases not until 1799), it was often to find that their lands and homes had been confiscated. These conditions led to emigration by hundreds of families to newly vacant lands in the Russian Empire in 1803–4 and again in 1808. A poignant retelling of this event based on what Goethe had personally witnessed can be found in his long poem Hermann and Dorothea.
The population grew rapidly, from 800,000 in 1814 to 914,000 in 1830 and 1,067,000 in 1846. The combination of economic and demographic factors led to hunger, housing shortages and a lack of work for young people. Thus, it is not surprising that people left Alsace, not only for Paris – where the Alsatian community grew in numbers, with famous members such as Baron Haussmann – but also for more distant places like Russia and the Austrian Empire, to take advantage of the new opportunities offered there: Austria had conquered lands in Eastern Europe from the Ottoman Empire and offered generous terms to colonists as a way of consolidating its hold on the new territories. Many Alsatians also began to sail to the United States, settling in many areas from 1820 to 1850. In 1843 and 1844, sailing ships bringing immigrant families from Alsace arrived at the port of New York. Some settled in Illinois, many to farm or to seek success in commercial ventures: for example, the sailing ships Sully (in May 1843) and Iowa (in June 1844) brought families who set up homes in northern Illinois and northern Indiana. Some Alsatian immigrants were noted for their roles in 19th-century American economic development. Others ventured to Canada to settle in southwestern Ontario, notably Waterloo County.
By 1790, the Jewish population of Alsace was approximately 22,500, about 3% of the provincial population. They were highly segregated and subject to long-standing anti-Jewish regulations. They maintained their own customs, Yiddish language, and historic traditions within the tightly-knit ghettos; they adhered to Talmudic law enforced by their rabbis. Jews were barred from most cities and instead lived in villages. They concentrated in trade, services, and especially in money lending. They financed about a third of the mortgages in Alsace. Official tolerance grew during the French Revolution, with full emancipation in 1791. However, local antisemitism also increased and Napoleon turned hostile in 1806, imposing a one-year moratorium on all debts owed to Jews.[citation needed] In the 1830-1870 era most Jews moved to the cities, where they integrated and acculturated, as antisemitism sharply declined. By 1831, the state began paying salaries to official rabbis, and in 1846 a special legal oath for Jews was discontinued. Antisemitic local riots occasionally occurred, especially during the Revolution of 1848. Merger of Alsace into Germany in 1871-1918 lessened antisemitic violence.
France started the Franco-Prussian War (1870–71), and was defeated by the Kingdom of Prussia and other German states. The end of the war led to the unification of Germany. Otto von Bismarck annexed Alsace and northern Lorraine to the new German Empire in 1871; unlike other members states of the German federation, which had governments of their own, the new Imperial territory of Alsace-Lorraine was under the sole authority of the Kaiser, administered directly by the imperial government in Berlin. Between 100,000 and 130,000 Alsatians (of a total population of about a million and a half) chose to remain French citizens and leave Reichsland Elsaß-Lothringen, many of them resettling in French Algeria as Pieds-Noirs. Only in 1911 was Alsace-Lorraine granted some measure of autonomy, which was manifested also in a flag and an anthem (Elsässisches Fahnenlied). In 1913, however, the Saverne Affair (French: Incident de Saverne) showed the limits of this new tolerance of the Alsatian identity.
During the First World War, to avoid ground fights between brothers, many Alsatians served as sailors in the Kaiserliche Marine and took part in the Naval mutinies that led to the abdication of the Kaiser in November 1918, which left Alsace-Lorraine without a nominal head of state. The sailors returned home and tried to found a republic. While Jacques Peirotes, at this time deputy at the Landrat Elsass-Lothringen and just elected mayor of Strasbourg, proclaimed the forfeiture of the German Empire and the advent of the French Republic, a self-proclaimed government of Alsace-Lorraine declared independence as the "Republic of Alsace-Lorraine". French troops entered Alsace less than two weeks later to quash the worker strikes and remove the newly established Soviets and revolutionaries from power. At the arrival of the French soldiers, many Alsatians and local Prussian/German administrators and bureaucrats cheered the re-establishment of order (which can be seen and is described in detail in the reference video below). Although U.S. President Woodrow Wilson had insisted that the région was self-ruling by legal status, as its constitution had stated it was bound to the sole authority of the Kaiser and not to the German state, France tolerated no plebiscite, as granted by the League of Nations to some eastern German territories at this time, because Alsatians were considered by the French public as fellow Frenchmen liberated from German rule. Germany ceded the region to France under the Treaty of Versailles.
Alsace-Lorraine was occupied by Germany in 1940 during the Second World War. Although Germany never formally annexed Alsace-Lorraine, it was incorporated into the Greater German Reich, which had been restructured into Reichsgaue. Alsace was merged with Baden, and Lorraine with the Saarland, to become part of a planned Westmark. During the war, 130,000 young men from Alsace and Lorraine were inducted into the German army against their will (malgré-nous) and in some cases, the Waffen SS. Some of the latter were involved in war crimes such as the Oradour-sur-Glane massacre. Most of them perished on the eastern front. The few that could escape fled to Switzerland or joined the resistance. In July 1944, 1500 malgré-nous were released from Soviet captivity and sent to Algiers, where they joined the Free French Forces.
Alsace is one of the most conservative régions of France. It is one of just two régions in metropolitan France where the conservative right won the 2004 région elections and thus controls the Alsace Regional Council. Conservative leader Nicolas Sarkozy got his best score in Alsace (over 65%) in the second round of the French presidential elections of 2007. The president of the Regional Council is Philippe Richert, a member of the Union for a Popular Movement, elected in the 2010 regional election. The frequently changing status of the région throughout history has left its mark on modern day politics in terms of a particular interest in national identity issues. Alsace is also one of the most pro-EU regions of France. It was one of the few French regions that voted 'yes' to the European Constitution in 2005.
Most of the Alsatian population is Roman Catholic, but, largely because of the region's German heritage, a significant Protestant community also exists: today, the EPCAAL (a Lutheran church) is France's second largest Protestant church, also forming an administrative union (UEPAL) with the much smaller Calvinist EPRAL. Unlike the rest of France, the Local law in Alsace-Moselle still provides for to the Napoleonic Concordat of 1801 and the organic articles, which provides public subsidies to the Roman Catholic, Lutheran, and Calvinist churches, as well as to Jewish synagogues; religion classes in one of these faiths is compulsory in public schools. This divergence in policy from the French majority is due to the region having been part of Imperial Germany when the 1905 law separating the French church and state was instituted (for a more comprehensive history, see: Alsace-Lorraine). Controversy erupts periodically on the appropriateness of this legal disposition, as well as on the exclusion of other religions from this arrangement.
Following the Protestant Reformation, promoted by local reformer Martin Bucer, the principle of cuius regio, eius religio led to a certain amount of religious diversity in the highlands of northern Alsace. Landowners, who as "local lords" had the right to decide which religion was allowed on their land, were eager to entice populations from the more attractive lowlands to settle and develop their property. Many accepted without discrimination Catholics, Lutherans, Calvinists, Jews and Anabaptists. Multiconfessional villages appeared, particularly in the region of Alsace bossue. Alsace became one of the French regions boasting a thriving Jewish community, and the only region with a noticeable Anabaptist population. The schism of the Amish under the lead of Jacob Amman from the Mennonites occurred in 1693 in Sainte-Marie-aux-Mines. The strongly Catholic Louis XIV tried in vain to drive them from Alsace. When Napoleon imposed military conscription without religious exception, most emigrated to the American continent.
There is controversy around the recognition of the Alsacian flag. The authentic historical flag is the Rot-un-Wiss ; Red and White are commonly found on the coat of arms of Alsacian cities (Strasbourg, Mulhouse, Sélestat...) and of many Swiss cites, especially in Basel's region. The German region Hesse uses a flag similar to the Rot-un-Wiss. As it underlines the Germanic roots of the region, it was replaced in 1949 by a new "Union jack-like" flag representing the union of the two déprtements. It has, however, no real historical relevance. It has been since replaced again by a slightly different one, also representing the two départements. With the purpose of "Frenchizing" the region, the Rot-un-Wiss has not been recognized by Paris. Some overzealous statesmen have called it a Nazi invention - while its origins date back to the XIth century and the Red and White banner of Gérard de Lorraine (aka. d'Alsace). The Rot-un-Wiss flag is still known as the real historical emblem of the region by most of the population and the departments' parliaments and has been widely used during protests against the creation of a new "super-region" gathering Champagne-Ardennes, Lorraine and Alsace, namely on Colmar's statue of liberty.
From the annexation of Alsace by France in the 17th century and the language policy of the French Revolution up to 1870, knowledge of French in Alsace increased considerably. With the education reforms of the 19th century, the middle classes began to speak and write French well. The French language never really managed, however, to win over the masses, the vast majority of whom continued to speak their German dialects and write in German (which we would now call "standard German").[citation needed]
During a reannexation by Germany (1940–1945), High German was reinstated as the language of education. The population was forced to speak German and 'French' family names were Germanized. Following the Second World War, the 1927 regulation was not reinstated and the teaching of German in primary schools was suspended by a provisional rectorial decree, which was supposed to enable French to regain lost ground. The teaching of German became a major issue, however, as early as 1946. Following World War II, the French government pursued, in line with its traditional language policy, a campaign to suppress the use of German as part of a wider Francization campaign.
It was not until 9 June 1982, with the Circulaire sur la langue et la culture régionales en Alsace (Memorandum on regional language and culture in Alsace) issued by the Vice-Chancellor of the Académie Pierre Deyon, that the teaching of German in primary schools in Alsace really began to be given more official status. The Ministerial Memorandum of 21 June 1982, known as the Circulaire Savary, introduced financial support, over three years, for the teaching of regional languages in schools and universities. This memorandum was, however, implemented in a fairly lax manner.
Both Alsatian and Standard German were for a time banned from public life (including street and city names, official administration, and educational system). Though the ban has long been lifted and street signs today are often bilingual, Alsace-Lorraine is today very French in language and culture. Few young people speak Alsatian today, although there do still exist one or two enclaves in the Sundgau region where some older inhabitants cannot speak French, and where Alsatian is still used as the mother tongue. A related Alemannic German survives on the opposite bank of the Rhine, in Baden, and especially in Switzerland. However, while French is the major language of the region, the Alsatian dialect of French is heavily influenced by German and other languages such a Yiddish in phonology and vocabulary.
The constitution of the Fifth Republic states that French alone is the official language of the Republic. However, Alsatian, along with other regional languages, are recognized by the French government in the official list of languages of France. A 1999 INSEE survey counted 548,000 adult speakers of Alsatian in France, making it the second most-spoken regional language in the country (after Occitan). Like all regional languages in France, however, the transmission of Alsatian is on the decline. While 39% of the adult population of Alsace speaks Alsatian, only one in four children speaks it, and only one in ten children uses it regularly.
The gastronomic symbol of the région is undoubtedly the Choucroute, a local variety of Sauerkraut. The word Sauerkraut in Alsatian has the form sûrkrût, same as in other southwestern German dialects, and means "sour cabbage" as its Standard German equivalent. This word was included into the French language as choucroute. To make it, the cabbage is finely shredded, layered with salt and juniper and left to ferment in wooden barrels. Sauerkraut can be served with poultry, pork, sausage or even fish. Traditionally it is served with Strasbourg sausage or frankfurters, bacon, smoked pork or smoked Morteau or Montbéliard sausages, or a selection of other pork products. Served alongside are often roasted or steamed potatoes or dumplings.
"Alsatia", the Latin form of Alsace's name, has long ago entered the English language with the specialized meaning of "a lawless place" or "a place under no jurisdiction" - since Alsace was conceived by English people to be such. It was used into the 20th century as a term for a ramshackle marketplace, "protected by ancient custom and the independence of their patrons". As of 2007, the word is still in use among the English and Australian judiciaries with the meaning of a place where the law cannot reach: "In setting up the Serious Organised Crime Agency, the state has set out to create an Alsatia - a region of executive action free of judicial oversight," Lord Justice Sedley in UMBS v SOCA 2007.
At present, plans are being considered for building a new dual carriageway west of Strasbourg, which would reduce the buildup of traffic in that area by picking up north and southbound vehicles and getting rid of the buildup outside Strasbourg. The line plans to link up the interchange of Hœrdt to the north of Strasbourg, with Innenheim in the southwest. The opening is envisaged at the end of 2011, with an average usage of 41,000 vehicles a day. Estimates of the French Works Commissioner however, raised some doubts over the interest of such a project, since it would pick up only about 10% of the traffic of the A35 at Strasbourg. Paradoxically, this reversed the situation of the 1950s. At that time, the French trunk road left of the Rhine not been built, so that traffic would cross into Germany to use the Karlsruhe-Basel Autobahn.
The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b]
The story focuses on series protagonist Link, who tries to prevent Hyrule from being engulfed by a corrupted parallel dimension known as the Twilight Realm. To do so, he takes the form of both a Hylian and a wolf, and is assisted by a mysterious creature named Midna. The game takes place hundreds of years after Ocarina of Time and Majora's Mask, in an alternate timeline from The Wind Waker.
At the time of its release, Twilight Princess was considered the greatest entry in the Zelda series by many critics, including writers for 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, IGN, and The Washington Post. It received several Game of the Year awards, and was the most critically acclaimed game of 2006. In 2011, the Wii version was rereleased under the Nintendo Selects label. A high-definition port for the Wii U, The Legend of Zelda: Twilight Princess HD, will be released in March 2016.
The Legend of Zelda: Twilight Princess is an action-adventure game focused on combat, exploration, and item collection. It uses the basic control scheme introduced in Ocarina of Time, including context-sensitive action buttons and L-targeting (Z-targeting on the Wii), a system that allows the player to keep Link's view focused on an enemy or important object while moving and attacking. Link can walk, run, and attack, and will automatically jump when running off of or reaching for a ledge.[c] Link uses a sword and shield in combat, complemented with secondary weapons and items, including a bow and arrows, a boomerang, bombs, and the Clawshot (similar to the Hookshot introduced earlier in the The Legend of Zelda series).[d] While L-targeting, projectile-based weapons can be fired at a target without the need for manual aiming.[c]
The context-sensitive button mechanic allows one button to serve a variety of functions, such as talking, opening doors, and pushing, pulling, and throwing objects.[e] The on-screen display shows what action, if any, the button will trigger, determined by the situation. For example, if Link is holding a rock, the context-sensitive button will cause Link to throw the rock if he is moving or targeting an object or enemy, or place the rock on the ground if he is standing still.[f]
The GameCube and Wii versions feature several minor differences in their controls. The Wii version of the game makes use of the motion sensors and built-in speaker of the Wii Remote. The speaker emits the sounds of a bowstring when shooting an arrow, Midna's laugh when she gives advice to Link, and the series' trademark "chime" when discovering secrets. The player controls Link's sword by swinging the Wii Remote. Other attacks are triggered using similar gestures with the Nunchuk. Unique to the GameCube version is the ability for the player to control the camera freely, without entering a special "lookaround" mode required by the Wii; however, in the GameCube version, only two of Link's secondary weapons can be equipped at a time, as opposed to four in the Wii version.[g]
The game features nine dungeons—large, contained areas where Link battles enemies, collects items, and solves puzzles. Link navigates these dungeons and fights a boss at the end in order to obtain an item or otherwise advance the plot. The dungeons are connected by a large overworld, across which Link can travel on foot; on his horse, Epona; or by teleporting.
When Link enters the Twilight Realm, the void that corrupts parts of Hyrule, he transforms into a wolf.[h] He is eventually able to transform between his Hylian and wolf forms at will. As a wolf, Link loses the ability to use his sword, shield, or any secondary items; he instead attacks by biting, and defends primarily by dodging attacks. However, "Wolf Link" gains several key advantages in return—he moves faster than he does as a human (though riding Epona is still faster) and digs holes to create new passages and uncover buried items, and has improved senses, including the ability to follow scent trails.[i] He also carries Midna, a small imp-like creature who gives him hints, uses an energy field to attack enemies, helps him jump long distances, and eventually allows Link to "warp" to any of several preset locations throughout the overworld.[j] Using Link's wolf senses, the player can see and listen to the wandering spirits of those affected by the Twilight, as well as hunt for enemy ghosts named Poes.[k]
The artificial intelligence (AI) of enemies in Twilight Princess is more advanced than that of enemies in The Wind Waker. Enemies react to defeated companions and to arrows or slingshot pellets that pass by, and can detect Link from a greater distance than was possible in previous games.
There is very little voice acting in the game, as is the case in most Zelda titles to date. Link remains silent in conversation, but grunts when attacking or injured and gasps when surprised. His emotions and responses are largely indicated visually by nods and facial expressions. Other characters have similar language-independent verbalizations, including laughter, surprised or fearful exclamations, and screams. The character of Midna has the most voice acting—her on-screen dialog is often accompanied by a babble of pseudo-speech, which was produced by scrambling the phonemes of English phrases[better source needed] sampled by Japanese voice actress Akiko Kōmoto.
Twilight Princess takes place several centuries after Ocarina of Time and Majora's Mask, and begins with a youth named Link who is working as a ranch hand in Ordon Village. One day, the village is attacked by Bulblins, who carry off the village's children with Link in pursuit before he encounters a wall of Twilight. A Shadow Beast pulls him beyond the wall into the Realm of Twilight, where he is transformed into a wolf and imprisoned. Link is soon freed by an imp-like Twilight being named Midna, who dislikes Link but agrees to help him if he obeys her unconditionally. She guides him to Princess Zelda. Zelda explains that Zant, the King of the Twilight, has stolen the light from three of the four Light Spirits and conquered Hyrule. In order to save Hyrule, Link must first restore the Light Spirits by entering the Twilight-covered areas and, as a wolf, recover the Spirits' lost light. He must do this by collecting the multiple "Tears of Light"; once all the Tears of Light are collected for one area, he restores that area's Light Spirit. As he restores them, the Light Spirits return Link to his Hylian form.
During this time, Link also helps Midna find the Fused Shadows, fragments of a relic containing powerful dark magic. In return, she helps Link find Ordon Village's children while helping the monkeys of Faron, the Gorons of Eldin, and the Zoras of Lanayru. Once Link has restored the Light Spirits and Midna has all the Fused Shadows, they are ambushed by Zant. After he relieves Midna of the Fused Shadow fragments, she ridicules him for abusing his tribe's magic, but Zant reveals that his power comes from another source as he uses it to turn Link back into a wolf, and then leaves Midna in Hyrule to die from the world's light. Bringing a dying Midna to Zelda, Link learns he needs the Master Sword to return to human form. Zelda sacrifices herself to heal Midna with her power before vanishing mysteriously. Midna is moved by Zelda's sacrifice, and begins to care more about Link and the fate of the light world.
After gaining the Master Sword, Link is cleansed of the magic that kept him in wolf form, obtaining the Shadow Crystal. Now able to use it to switch between both forms at will, Link is led by Midna to the Mirror of Twilight located deep within the Gerudo Desert, the only known gateway between the Twilight Realm and Hyrule. However, they discover that the mirror is broken. The Sages there explain that Zant tried to destroy it, but he was only able to shatter it into fragments; only the true ruler of the Twili can completely destroy the Mirror of Twilight. They also reveal that they used it a century ago to banish Ganondorf, the Gerudo leader who attempted to steal the Triforce, to the Twilight Realm when executing him failed. Assisted by an underground resistance group they meet in Castle Town, Link and Midna set out to retrieve the missing shards of the Mirror, defeating those they infected. Once the portal has been restored, Midna is revealed to be the true ruler of the Twilight Realm, usurped by Zant when he cursed her into her current form. Confronting Zant, Link and Midna learn that Zant's coup was made possible when he forged a pact with Ganondorf, who asked for Zant's assistance in conquering Hyrule. After Link defeats Zant, Midna recovers the Fused Shadows, but destroys Zant after learning that only Ganondorf's death can release her from her curse. Returning to Hyrule, Link and Midna find Ganondorf in Hyrule Castle, with a lifeless Zelda suspended above his head. Ganondorf fights Link by possessing Zelda's body and eventually by transforming into a beast, but Link defeats him and Midna is able to resurrect Zelda.
Ganondorf then revives, and Midna teleports Link and Zelda outside the castle so she can hold him off with the Fused Shadows. However, as Hyrule Castle collapses, it is revealed that Ganondorf was victorious as he crushes Midna's helmet. Ganondorf engages Link on horseback, and, assisted by Zelda and the Light Spirits, Link eventually knocks Ganondorf off his horse and they duel on foot before Link strikes down Ganondorf and plunges the Master Sword into his chest. With Ganondorf dead, the Light Spirits not only bring Midna back to life, but restore her to her true form. After bidding farewell to Link and Zelda, Midna returns home before destroying the Mirror of Twilight with a tear to maintain balance between Hyrule and the Twilight Realm. Near the end, as Hyrule Castle is rebuilt, Link is shown leaving Ordon Village heading to parts unknown.
In 2003, Nintendo announced that a new The Legend of Zelda game was in the works for the GameCube by the same team that had created the cel-shaded The Wind Waker. At the following year's Game Developers Conference, director Eiji Aonuma unintentionally revealed that the game's sequel was in development under the working title The Wind Waker 2; it was set to use a similar graphical style to that of its predecessor. Nintendo of America told Aonuma that North American sales of The Wind Waker were sluggish because its cartoon appearance created the impression that the game was designed for a young audience. Concerned that the sequel would have the same problem, Aonuma expressed to producer Shigeru Miyamoto that he wanted to create a realistic Zelda game that would appeal to the North American market. Miyamoto, hesitant about solely changing the game's presentation, suggested the team's focus should instead be on coming up with gameplay innovations. He advised that Aonuma should start by doing what could not be done in Ocarina of Time, particularly horseback combat.[l]
In four months, Aonuma's team managed to present realistic horseback riding,[l] which Nintendo later revealed to the public with a trailer at Electronic Entertainment Expo 2004. The game was scheduled to be released the next year, and was no longer a follow-up to The Wind Waker; a true sequel to it was released for the Nintendo DS in 2007, in the form of Phantom Hourglass. Miyamoto explained in interviews that the graphical style was chosen to satisfy demand, and that it better fit the theme of an older incarnation of Link. The game runs on a modified The Wind Waker engine.
Prior Zelda games have employed a theme of two separate, yet connected, worlds. In A Link to the Past, Link travels between a "Light World" and a "Dark World"; in Ocarina of Time, as well as in Oracle of Ages, Link travels between two different time periods. The Zelda team sought to reuse this motif in the series' latest installment. It was suggested that Link transform into a wolf, much like he metamorphoses into a rabbit in the Dark World of A Link to the Past.[m] The story of the game was created by Aonuma, and later underwent several changes by scenario writers Mitsuhiro Takano and Aya Kyogoku. Takano created the script for the story scenes, while Kyogoku and Takayuki Ikkaku handled the actual in-game script. Aonuma left his team working on the new idea while he directed The Minish Cap for the Game Boy Advance. When he returned, he found the Twilight Princess team struggling. Emphasis on the parallel worlds and the wolf transformation had made Link's character unbelievable. Aonuma also felt the gameplay lacked the caliber of innovation found in Phantom Hourglass, which was being developed with touch controls for the Nintendo DS. At the same time, the Wii was under development with the code name "Revolution". Miyamoto thought that the Revolution's pointing device, the Wii Remote, was well suited for aiming arrows in Zelda, and suggested that Aonuma consider using it.[n]
Aonuma had anticipated creating a Zelda game for what would later be called the Wii, but had assumed that he would need to complete Twilight Princess first. His team began work developing a pointing-based interface for the bow and arrow, and Aonuma found that aiming directly at the screen gave the game a new feel, just like the DS control scheme for Phantom Hourglass. Aonuma felt confident this was the only way to proceed, but worried about consumers who had been anticipating a GameCube release. Developing two versions would mean delaying the previously announced 2005 release, still disappointing the consumer. Satoru Iwata felt that having both versions would satisfy users in the end, even though they would have to wait for the finished product. Aonuma then started working on both versions in parallel.[o]
Transferring GameCube development to the Wii was relatively simple, since the Wii was being created to be compatible with the GameCube.[o] At E3 2005, Nintendo released a small number of Nintendo DS game cards containing a preview trailer for Twilight Princess. They also announced that Zelda would appear on the Wii (then codenamed "Revolution"), but it was not clear to the media if this meant Twilight Princess or a different game.
The team worked on a Wii control scheme, adapting camera control and the fighting mechanics to the new interface. A prototype was created that used a swinging gesture to control the sword from a first-person viewpoint, but was unable to show the variety of Link's movements. When the third-person view was restored, Aonuma thought it felt strange to swing the Wii Remote with the right hand to control the sword in Link's left hand, so the entire Wii version map was mirrored.[p] Details about Wii controls began to surface in December 2005 when British publication NGC Magazine claimed that when a GameCube copy of Twilight Princess was played on the Revolution, it would give the player the option of using the Revolution controller. Miyamoto confirmed the Revolution controller-functionality in an interview with Nintendo of Europe and Time reported this soon after. However, support for the Wii controller did not make it into the GameCube release. At E3 2006, Nintendo announced that both versions would be available at the Wii launch, and had a playable version of Twilight Princess for the Wii.[p] Later, the GameCube release was pushed back to a month after the launch of the Wii.
Nintendo staff members reported that demo users complained about the difficulty of the control scheme. Aonuma realized that his team had implemented Wii controls under the mindset of "forcing" users to adapt, instead of making the system intuitive and easy to use. He began rethinking the controls with Miyamoto to focus on comfort and ease.[q] The camera movement was reworked and item controls were changed to avoid accidental button presses.[r] In addition, the new item system required use of the button that had previously been used for the sword. To solve this, sword controls were transferred back to gestures—something E3 attendees had commented they would like to see. This reintroduced the problem of using a right-handed swing to control a left-handed sword attack. The team did not have enough time before release to rework Link's character model, so they instead flipped the entire game—everything was made a mirror image.[s] Link was now right-handed, and references to "east" and "west" were switched around. The GameCube version, however, was left with the original orientation. The Twilight Princess player's guide focuses on the Wii version, but has a section in the back with mirror-image maps for GameCube users.[t]
The game's score was composed by Toru Minegishi and Asuka Ohta, with series regular Koji Kondo serving as the sound supervisor. Minegishi took charge of composition and sound design in Twilight Princess, providing all field and dungeon music under the supervision of Kondo. For the trailers, three pieces were written by different composers, two of which were created by Mahito Yokota and Kondo. Michiru Ōshima created orchestral arrangements for the three compositions, later to be performed by an ensemble conducted by Yasuzo Takemoto. Kondo's piece was later chosen as music for the E3 2005 trailer and for the demo movie after the game's title screen.
Media requests at the trade show prompted Kondo to consider using orchestral music for the other tracks in the game as well, a notion reinforced by his preference for live instruments. He originally envisioned a full 50-person orchestra for action sequences and a string quartet for more "lyrical moments", though the final product used sequenced music instead. Kondo later cited the lack of interactivity that comes with orchestral music as one of the main reasons for the decision. Both six- and seven-track versions of the game's soundtrack were released on November 19, 2006, as part of a Nintendo Power promotion and bundled with replicas of the Master Sword and the Hylian Shield.
Following the discovery of a buffer overflow vulnerability in the Wii version of Twilight Princess, an exploit known as the "Twilight Hack" was developed, allowing the execution of custom code from a Secure Digital (SD) card on the console. A properly designed save file would cause the game to load unsigned code, which could include Executable and Linkable Format (ELF) programs and homebrew Wii applications. Versions 3.3 and 3.4 of the Wii Menu prevented copying exploited save files onto the console until circumvention methods were discovered, and version 4.0 of the Wii Menu patched the vulnerability.
A high-definition remaster of the game, The Legend of Zelda: Twilight Princess HD, is being developed by Tantalus Media for the Wii U. Officially announced during a Nintendo Direct presentation on November 12, 2015, it features enhanced graphics and Amiibo functionality. The game will be released in North America and Europe on March 4, 2016; in Australia on March 5, 2016; and in Japan on March 10, 2016.
Special bundles of the game contain a Wolf Link Amiibo figurine, which unlocks a Wii U-exclusive dungeon called the "Cave of Shadows" and can carry data over to the upcoming 2016 Zelda game. Other Zelda-related Amiibo figurines have distinct functions: Link and Toon Link replenish arrows, Zelda and Sheik restore Link's health, and Ganondorf causes Link to take twice as much damage.
A CD containing 20 musical selections from the game was available as a GameStop preorder bonus in the United States; it is included in all bundles in Japan, Europe, and Australia.[citation needed]
Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.
On release, Twilight Princess was considered to be the greatest Zelda game ever made by many critics including writers for 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, IGN and The Washington Post. Game Informer called it "so creative that it rivals the best that Hollywood has to offer". GamesRadar praised Twilight Princess as "a game that deserves nothing but the absolute highest recommendation". Cubed3 hailed Twilight Princess as "the single greatest videogame experience". Twilight Princess's graphics were praised for the art style and animation, although the game was designed for the GameCube, which is technically lacking compared to the next generation consoles. Both IGN and GameSpy pointed out the existence of blurry textures and low-resolution characters. Despite these complaints, Computer and Video Games felt the game's atmosphere was superior to that of any previous Zelda game, and regarded Twilight Princess's Hyrule as the best version ever created. PALGN praised the game's cinematics, noting that "the cutscenes are the best ever in Zelda games". Regarding the Wii version, GameSpot's Jeff Gerstmann said the Wii controls felt "tacked-on", although 1UP.com said the remote-swinging sword attacks were "the most impressive in the entire series". Gaming Nexus considered Twilight Princess's soundtrack to be the best of this generation, though IGN criticized its MIDI-formatted songs for lacking "the punch and crispness" of their orchestrated counterparts. Hyper's Javier Glickman commended the game for its "very long quests, superb Wii controls and being able to save anytime". However, he criticised it for "no voice acting, no orchestral score and slightly outdated graphics".
Twilight Princess received the awards for Best Artistic Design, Best Original Score, and Best Use of Sound from IGN for its GameCube version. Both IGN and Nintendo Power gave Twilight Princess the awards for Best Graphics and Best Story. Twilight Princess received Game of the Year awards from GameTrailers, 1UP.com, Electronic Gaming Monthly, Game Informer, Games Radar, GameSpy, Spacey Awards, X-Play and Nintendo Power. It was also given awards for Best Adventure Game from the Game Critics Awards, X-Play, IGN, GameTrailers, 1UP.com, and Nintendo Power. The game was considered the Best Console Game by the Game Critics Awards and GameSpy. The game placed 16th in Official Nintendo Magazine's list of the 100 Greatest Nintendo Games of All Time. IGN ranked the game as the 4th-best Wii game. Nintendo Power ranked the game as the third-best game to be released on a Nintendo system in the 2000s decade.
In the PAL region, which covers most of Africa, Asia, Europe, and Oceania, Twilight Princess is the best-selling entry in the Zelda series. During its first week, the game was sold with three of every four Wii purchases. The game had sold 5.82 million copies on the Wii as of March 31, 2011[update], and 1.32 million on the GameCube as of March 31, 2007[update].
A Japan-exclusive manga series based on Twilight Princess, penned and illustrated by Akira Himekawa, was first released on February 8, 2016. The series is available solely via publisher Shogakukan's MangaOne mobile application. While the manga adaptation began almost ten years after the initial release of the game on which it is based, it launched only a month before the release of the high-definition remake.
An airport is an aerodrome with facilities for flights to take off and land. Airports often have facilities to store and maintain aircraft, and a control tower. An airport consists of a landing area, which comprises an aerially accessible open space including at least one operationally active surface such as a runway for a plane to take off or a helipad, and often includes adjacent utility buildings such as control towers, hangars  and terminals. Larger airports may have fixed base operator services, airport aprons, air traffic control centres, passenger facilities such as restaurants and lounges, and emergency services.
The majority of the world's airports are non-towered, with no air traffic control presence. Busy airports have air traffic control (ATC) system. All airports use a traffic pattern to assure smooth traffic flow between departing and arriving aircraft. There are a number of aids available to pilots, though not all airports are equipped with them. Many airports have lighting that help guide planes using the runways and taxiways at night or in rain, snow, or fog. In the U.S. and Canada, the vast majority of airports, large and small, will either have some form of automated airport weather station, a human observer or a combination of the two. Air safety is an important concern in the operation of an airport, and airports often have their own safety services.
Most of the world's airports are owned by local, regional, or national government bodies who then lease the airport to private corporations who oversee the airport's operation. For example, in the United Kingdom the state-owned British Airports Authority originally operated eight of the nation's major commercial airports - it was subsequently privatized in the late 1980s, and following its takeover by the Spanish Ferrovial consortium in 2006, has been further divested and downsized to operating just five. Germany's Frankfurt Airport is managed by the quasi-private firm Fraport. While in India GMR Group operates, through joint ventures, Indira Gandhi International Airport and Rajiv Gandhi International Airport. Bengaluru International Airport and Chhatrapati Shivaji International Airport are controlled by GVK Group. The rest of India's airports are managed by the Airports Authority of India.
Airports are divided into landside and airside areas. Landside areas include parking lots, public transportation train stations and access roads. Airside areas include all areas accessible to aircraft, including runways, taxiways and aprons. Access from landside areas to airside areas is tightly controlled at most airports. Passengers on commercial flights access airside areas through terminals, where they can purchase tickets, clear security check, or claim luggage and board aircraft through gates. The waiting areas which provide passenger access to aircraft are typically called concourses, although this term is often used interchangeably with terminal.
Most major airports provide commercial outlets for products and services. Most of these companies, many of which are internationally known brands, are located within the departure areas. These include clothing boutiques and restaurants. Prices charged for items sold at these outlets are generally higher than those outside the airport. However, some airports now regulate costs to keep them comparable to "street prices". This term is misleading as prices often match the manufacturers' suggested retail price (MSRP) but are almost never discounted.[citation needed]
Airports may also contain premium and VIP services. The premium and VIP services may include express check-in and dedicated check-in counters. These services are usually reserved for First and Business class passengers, premium frequent flyers, and members of the airline's clubs. Premium services may sometimes be open to passengers who are members of a different airline's frequent flyer program. This can sometimes be part of a reciprocal deal, as when multiple airlines are part of the same alliance, or as a ploy to attract premium customers away from rival airlines.
Many large airports are located near railway trunk routes for seamless connection of multimodal transport, for instance Frankfurt Airport, Amsterdam Airport Schiphol, London Heathrow Airport, London Gatwick Airport and London Stansted Airport. It is also common to connect an airport and a city with rapid transit, light rail lines or other non-road public transport systems. Some examples of this would include the AirTrain JFK at John F. Kennedy International Airport in New York, Link Light Rail that runs from the heart of downtown Seattle to Seattle–Tacoma International Airport, and the Silver Line T at Boston's Logan International Airport by the Massachusetts Bay Transportation Authority (MBTA). Such a connection lowers risk of missed flights due to traffic congestion. Large airports usually have access also through controlled-access highways ('freeways' or 'motorways') from which motor vehicles enter either the departure loop or the arrival loop.
The distances passengers need to move within a large airport can be substantial. It is common for airports to provide moving walkways and buses. The Hartsfield–Jackson Atlanta International Airport has a tram that takes people through the concourses and baggage claim. Major airports with more than one terminal offer inter-terminal transportation, such as Mexico City International Airport, where the domestic building of Terminal 1 is connected by Aerotrén to Terminal 2, on the other side of the airport.
The title of "world's oldest airport" is disputed, but College Park Airport in Maryland, US, established in 1909 by Wilbur Wright, is generally agreed to be the world's oldest continually operating airfield, although it serves only general aviation traffic. Bisbee-Douglas International Airport in Arizona was declared "the first international airport of the Americas" by US president Franklin D. Roosevelt in 1943. Pearson Field Airport in Vancouver, Washington had a dirigible land in 1905 and planes in 1911 and is still in use. Bremen Airport opened in 1913 and remains in use, although it served as an American military field between 1945 and 1949. Amsterdam Airport Schiphol opened on September 16, 1916 as a military airfield, but only accepted civil aircraft from December 17, 1920, allowing Sydney Airport in Sydney, Australia—which started operations in January 1920—to claim to be one of the world's oldest continually operating commercial airports. Minneapolis-Saint Paul International Airport in Minneapolis-Saint Paul, Minnesota, opened in 1920 and has been in continuous commercial service since. It serves about 35,000,000 passengers each year and continues to expand, recently opening a new 11,000 foot (3,355 meter) runway. Of the airports constructed during this early period in aviation, it is one of the largest and busiest that is still currently operating. Rome Ciampino Airport, opened 1916, is also a contender, as well as the Don Mueang International Airport near Bangkok,Thailand, which opened in 1914. Increased aircraft traffic during World War I led to the construction of landing fields. Aircraft had to approach these from certain directions and this led to the development of aids for directing the approach and landing slope.
Following the war, some of these military airfields added civil facilities for handling passenger traffic. One of the earliest such fields was Paris – Le Bourget Airport at Le Bourget, near Paris. The first airport to operate scheduled international commercial services was Hounslow Heath Aerodrome in August 1919, but it was closed and supplanted by Croydon Airport in March 1920. In 1922, the first permanent airport and commercial terminal solely for commercial aviation was opened at Flughafen Devau near what was then Königsberg, East Prussia. The airports of this era used a paved "apron", which permitted night flying as well as landing heavier aircraft.
The first lighting used on an airport was during the latter part of the 1920s; in the 1930s approach lighting came into use. These indicated the proper direction and angle of descent. The colours and flash intervals of these lights became standardized under the International Civil Aviation Organization (ICAO). In the 1940s, the slope-line approach system was introduced. This consisted of two rows of lights that formed a funnel indicating an aircraft's position on the glideslope. Additional lights indicated incorrect altitude and direction.
Airport construction boomed during the 1960s with the increase in jet aircraft traffic. Runways were extended out to 3,000 m (9,800 ft). The fields were constructed out of reinforced concrete using a slip-form machine that produces a continual slab with no disruptions along the length. The early 1960s also saw the introduction of jet bridge systems to modern airport terminals, an innovation which eliminated outdoor passenger boarding. These systems became commonplace in the United States by the 1970s.
The majority of the world's airports are non-towered, with no air traffic control presence. However, at particularly busy airports, or airports with other special requirements, there is an air traffic control (ATC) system whereby controllers (usually ground-based) direct aircraft movements via radio or other communications links. This coordinated oversight facilitates safety and speed in complex operations where traffic moves in all three dimensions. Air traffic control responsibilities at airports are usually divided into at least two main areas: ground and tower, though a single controller may work both stations. The busiest airports also have clearance delivery, apron control, and other specialized ATC stations.
Ground Control is responsible for directing all ground traffic in designated "movement areas", except the traffic on runways. This includes planes, baggage trains, snowplows, grass cutters, fuel trucks, stair trucks, airline food trucks, conveyor belt vehicles and other vehicles. Ground Control will instruct these vehicles on which taxiways to use, which runway they will use (in the case of planes), where they will park, and when it is safe to cross runways. When a plane is ready to takeoff it will stop short of the runway, at which point it will be turned over to Tower Control. After a plane has landed, it will depart the runway and be returned to Ground Control.
Tower Control controls aircraft on the runway and in the controlled airspace immediately surrounding the airport. Tower controllers may use radar to locate an aircraft's position in three-dimensional space, or they may rely on pilot position reports and visual observation. They coordinate the sequencing of aircraft in the traffic pattern and direct aircraft on how to safely join and leave the circuit. Aircraft which are only passing through the airspace must also contact Tower Control in order to be sure that they remain clear of other traffic.
At all airports the use of a traffic pattern (often called a traffic circuit outside the U.S.) is possible. They may help to assure smooth traffic flow between departing and arriving aircraft. There is no technical need within modern aviation for performing this pattern, provided there is no queue. And due to the so-called SLOT-times, the overall traffic planning tend to assure landing queues are avoided. If for instance an aircraft approaches runway 17 (which has a heading of approx. 170 degrees) from the north (coming from 360/0 degrees heading towards 180 degrees), the aircraft will land as fast as possible by just turning 10 degrees and follow the glidepath, without orbit the runway for visual reasons, whenever this is possible. For smaller piston engined airplanes at smaller airfields without ILS equipment, things are very differently though.
Generally, this pattern is a circuit consisting of five "legs" that form a rectangle (two legs and the runway form one side, with the remaining legs forming three more sides). Each leg is named (see diagram), and ATC directs pilots on how to join and leave the circuit. Traffic patterns are flown at one specific altitude, usually 800 or 1,000 ft (244 or 305 m) above ground level (AGL). Standard traffic patterns are left-handed, meaning all turns are made to the left. One of the main reason for this is that pilots sit on the left side of the airplane, and a Left-hand patterns improves their visibility of the airport and pattern. Right-handed patterns do exist, usually because of obstacles such as a mountain, or to reduce noise for local residents. The predetermined circuit helps traffic flow smoothly because all pilots know what to expect, and helps reduce the chance of a mid-air collision.
At extremely large airports, a circuit is in place but not usually used. Rather, aircraft (usually only commercial with long routes) request approach clearance while they are still hours away from the airport, often before they even take off from their departure point. Large airports have a frequency called Clearance Delivery which is used by departing aircraft specifically for this purpose. This then allows aircraft to take the most direct approach path to the runway and land without worrying about interference from other aircraft. While this system keeps the airspace free and is simpler for pilots, it requires detailed knowledge of how aircraft are planning to use the airport ahead of time and is therefore only possible with large commercial airliners on pre-scheduled flights. The system has recently become so advanced that controllers can predict whether an aircraft will be delayed on landing before it even takes off; that aircraft can then be delayed on the ground, rather than wasting expensive fuel waiting in the air.
There are a number of aids available to pilots, though not all airports are equipped with them. A visual approach slope indicator (VASI) helps pilots fly the approach for landing. Some airports are equipped with a VHF omnidirectional range (VOR) to help pilots find the direction to the airport. VORs are often accompanied by a distance measuring equipment (DME) to determine the distance to the VOR. VORs are also located off airports, where they serve to provide airways for aircraft to navigate upon. In poor weather, pilots will use an instrument landing system (ILS) to find the runway and fly the correct approach, even if they cannot see the ground. The number of instrument approaches based on the use of the Global Positioning System (GPS) is rapidly increasing and may eventually be the primary means for instrument landings.
On runways, green lights indicate the beginning of the runway for landing, while red lights indicate the end of the runway. Runway edge lighting consists of white lights spaced out on both sides of the runway, indicating the edge. Some airports have more complicated lighting on the runways including lights that run down the centerline of the runway and lights that help indicate the approach (an approach lighting system, or ALS). Low-traffic airports may use pilot controlled lighting to save electricity and staffing costs.
Hazards to aircraft include debris, nesting birds, and reduced friction levels due to environmental conditions such as ice, snow, or rain. Part of runway maintenance is airfield rubber removal which helps maintain friction levels. The fields must be kept clear of debris using cleaning equipment so that loose material does not become a projectile and enter an engine duct (see foreign object damage). In adverse weather conditions, ice and snow clearing equipment can be used to improve traction on the landing strip. For waiting aircraft, equipment is used to spray special deicing fluids on the wings.
Many ground crew at the airport work at the aircraft. A tow tractor pulls the aircraft to one of the airbridges, The ground power unit is plugged in. It keeps the electricity running in the plane when it stands at the terminal. The engines are not working, therefore they do not generate the electricity, as they do in flight. The passengers disembark using the airbridge. Mobile stairs can give the ground crew more access to the aircraft's cabin. There is a cleaning service to clean the aircraft after the aircraft lands. Flight catering provides the food and drinks on flights. A toilet waste truck removes the human waste from the tank which holds the waste from the toilets in the aircraft. A water truck fills the water tanks of the aircraft. A fuel transfer vehicle transfers aviation fuel from fuel tanks underground, to the aircraft tanks. A tractor and its dollies bring in luggage from the terminal to the aircraft. They also carry luggage to the terminal if the aircraft has landed, and is being unloaded. Hi-loaders lift the heavy luggage containers to the gate of the cargo hold. The ground crew push the luggage containers into the hold. If it has landed, they rise, the ground crew push the luggage container on the hi-loader, which carries it down. The luggage container is then pushed on one of the tractors dollies. The conveyor, which is a conveyor belt on a truck, brings in the awkwardly shaped, or late luggage. The airbridge is used again by the new passengers to embark the aircraft. The tow tractor pushes the aircraft away from the terminal to a taxi area. The aircraft should be off of the airport and in the air in 90 minutes. The airport charges the airline for the time the aircraft spends at the airport.
An airbase, sometimes referred to as an air station or airfield, provides basing and support of military aircraft. Some airbases, known as military airports, provide facilities similar to their civilian counterparts. For example, RAF Brize Norton in the UK has a terminal which caters to passengers for the Royal Air Force's scheduled TriStar flights to the Falkland Islands. Some airbases are co-located with civilian airports, sharing the same ATC facilities, runways, taxiways and emergency services, but with separate terminals, parking areas and hangars. Bardufoss Airport , Bardufoss Air Station in Norway and Pune Airport in India are examples of this.
Airports have played major roles in films and television programs due to their very nature as a transport and international hub, and sometimes because of distinctive architectural features of particular airports. One such example of this is The Terminal, a film about a man who becomes permanently grounded in an airport terminal and must survive only on the food and shelter provided by the airport. They are also one of the major elements in movies such as The V.I.P.s, Airplane!, Airport (1970), Die Hard 2, Soul Plane, Jackie Brown, Get Shorty, Home Alone, Liar Liar, Passenger 57, Final Destination (2000), Unaccompanied Minors, Catch Me If You Can, Rendition and The Langoliers. They have also played important parts in television series like Lost, The Amazing Race, America's Next Top Model, Cycle 10 which have significant parts of their story set within airports. In other programmes and films, airports are merely indicative of journeys, e.g. Good Will Hunting.
Most airports welcome filming on site, although it must be agreed in advance and may be subject to a fee. Landside, filming can take place in all public areas. However airside, filming is heavily restricted, the only airside locations where filming is permitted are the Departure Lounge and some outside areas. To film in an airside location, all visitors must go through security, the same as passengers, and be accompanied by a full airside pass holder and have their passport with them at all times. Filming can not be undertaken in Security, at Immigration/Customs, or in Baggage Reclaim.
Physically, clothing serves many purposes: it can serve as protection from the elements, and can enhance safety during hazardous activities such as hiking and cooking. It protects the wearer from rough surfaces, rash-causing plants, insect bites, splinters, thorns and prickles by providing a barrier between the skin and the environment. Clothes can insulate against cold or hot conditions. Further, they can provide a hygienic barrier, keeping infectious and toxic materials away from the body. Clothing also provides protection from harmful UV radiation.
There is no easy way to determine when clothing was first developed, but some information has been inferred by studying lice. The body louse specifically lives in clothing, and diverge from head lice about 107,000 years ago, suggesting that clothing existed at that time. Another theory is that modern humans are the only survivors of several species of primates who may have worn clothes and that clothing may have been used as long ago as 650 thousand years ago. Other louse-based estimates put the introduction of clothing at around 42,000–72,000 BP.
The most obvious function of clothing is to improve the comfort of the wearer, by protecting the wearer from the elements. In hot climates, clothing provides protection from sunburn or wind damage, while in cold climates its thermal insulation properties are generally more important. Shelter usually reduces the functional need for clothing. For example, coats, hats, gloves, and other superficial layers are normally removed when entering a warm home, particularly if one is residing or sleeping there. Similarly, clothing has seasonal and regional aspects, so that thinner materials and fewer layers of clothing are generally worn in warmer seasons and regions than in colder ones.
Clothing can and has in history been made from a very wide variety of materials. Materials have ranged from leather and furs, to woven materials, to elaborate and exotic natural and synthetic fabrics. Not all body coverings are regarded as clothing. Articles carried rather than worn (such as purses), worn on a single part of the body and easily removed (scarves), worn purely for adornment (jewelry), or those that serve a function other than protection (eyeglasses), are normally considered accessories rather than clothing, as are footwear and hats.
Clothing protects against many things that might injure the uncovered human body. Clothes protect people from the elements, including rain, snow, wind, and other weather, as well as from the sun. However, clothing that is too sheer, thin, small, tight, etc., offers less protection. Clothes also reduce risk during activities such as work or sport. Some clothing protects from specific environmental hazards, such as insects, noxious chemicals, weather, weapons, and contact with abrasive substances. Conversely, clothing may protect the environment from the clothing wearer, as with doctors wearing medical scrubs.
Humans have shown extreme inventiveness in devising clothing solutions to environmental hazards. Examples include: space suits, air conditioned clothing, armor, diving suits, swimsuits, bee-keeper gear, motorcycle leathers, high-visibility clothing, and other pieces of protective clothing. Meanwhile, the distinction between clothing and protective equipment is not always clear-cut—since clothes designed to be fashionable often have protective value and clothes designed for function often consider fashion in their design. Wearing clothes also has social implications. They cover parts of the body that social norms require to be covered, act as a form of adornment, and serve other social purposes.
Although dissertations on clothing and its function appear from the 19th century as colonising countries dealt with new environments, concerted scientific research into psycho-social, physiological and other functions of clothing (e.g. protective, cartage) occurred in the first half of the 20th century, with publications such as J. C. Flügel's Psychology of Clothes in 1930, and Newburgh's seminal Physiology of Heat Regulation and The Science of Clothing in 1949. By 1968, the field of environmental physiology had advanced and expanded significantly, but the science of clothing in relation to environmental physiology had changed little. While considerable research has since occurred and the knowledge-base has grown significantly, the main concepts remain unchanged, and indeed Newburgh's book is still cited by contemporary authors, including those attempting to develop thermoregulatory models of clothing development.
In Western societies, skirts, dresses and high-heeled shoes are usually seen as women's clothing, while neckties are usually seen as men's clothing. Trousers were once seen as exclusively male clothing, but are nowadays worn by both genders. Male clothes are often more practical (that is, they can function well under a wide variety of situations), but a wider range of clothing styles are available for females. Males are typically allowed to bare their chests in a greater variety of public places. It is generally acceptable for a woman to wear traditionally male clothing, while the converse is unusual.
In some societies, clothing may be used to indicate rank or status. In ancient Rome, for example, only senators could wear garments dyed with Tyrian purple. In traditional Hawaiian society, only high-ranking chiefs could wear feather cloaks and palaoa, or carved whale teeth. Under the Travancore Kingdom of Kerala, (India), lower caste women had to pay a tax for the right to cover their upper body. In China, before establishment of the republic, only the emperor could wear yellow. History provides many examples of elaborate sumptuary laws that regulated what people could wear. In societies without such laws, which includes most modern societies, social status is instead signaled by the purchase of rare or luxury items that are limited by cost to those with wealth or status. In addition, peer pressure influences clothing choice.
According to archaeologists and anthropologists, the earliest clothing likely consisted of fur, leather, leaves, or grass that were draped, wrapped, or tied around the body. Knowledge of such clothing remains inferential, since clothing materials deteriorate quickly compared to stone, bone, shell and metal artifacts. Archeologists have identified very early sewing needles of bone and ivory from about 30,000 BC, found near Kostenki, Russia in 1988. Dyed flax fibers that could have been used in clothing have been found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP.
Scientists are still debating when people started wearing clothes. Ralf Kittler, Manfred Kayser and Mark Stoneking, anthropologists at the Max Planck Institute for Evolutionary Anthropology, have conducted a genetic analysis of human body lice that suggests clothing originated quite recently, around 170,000 years ago. Body lice is an indicator of clothes-wearing, since most humans have sparse body hair, and lice thus require human clothing to survive. Their research suggests the invention of clothing may have coincided with the northward migration of modern Homo sapiens away from the warm climate of Africa, thought to have begun between 50,000 and 100,000 years ago. However, a second group of researchers using similar genetic methods estimate that clothing originated around 540,000 years ago (Reed et al. 2004. PLoS Biology 2(11): e340). For now, the date of the origin of clothing remains unresolved.[citation needed]
Different cultures have evolved various ways of creating clothes out of cloth. One approach simply involves draping the cloth. Many people wore, and still wear, garments consisting of rectangles of cloth wrapped to fit – for example, the dhoti for men and the sari for women in the Indian subcontinent, the Scottish kilt or the Javanese sarong. The clothes may simply be tied up, as is the case of the first two garments; or pins or belts hold the garments in place, as in the case of the latter two. The precious cloth remains uncut, and people of various sizes or the same person at different sizes can wear the garment.
By the early years of the 21st century, western clothing styles had, to some extent, become international styles. This process began hundreds of years earlier, during the periods of European colonialism. The process of cultural dissemination has perpetuated over the centuries as Western media corporations have penetrated markets throughout the world, spreading Western culture and styles. Fast fashion clothing has also become a global phenomenon. These garments are less expensive, mass-produced Western clothing. Donated used clothing from Western countries are also delivered to people in poor countries by charity organizations.
Most sports and physical activities are practiced wearing special clothing, for practical, comfort or safety reasons. Common sportswear garments include shorts, T-shirts, tennis shirts, leotards, tracksuits, and trainers. Specialized garments include wet suits (for swimming, diving or surfing), salopettes (for skiing) and leotards (for gymnastics). Also, spandex materials are often used as base layers to soak up sweat. Spandex is also preferable for active sports that require form fitting garments, such as volleyball, wrestling, track & field, dance, gymnastics and swimming.
The world of clothing is always changing, as new cultural influences meet technological innovations. Researchers in scientific labs have been developing prototypes for fabrics that can serve functional purposes well beyond their traditional roles, for example, clothes that can automatically adjust their temperature, repel bullets, project images, and generate electricity. Some practical advances already available to consumers are bullet-resistant garments made with kevlar and stain-resistant fabrics that are coated with chemical mixtures that reduce the absorption of liquids.
Though mechanization transformed most aspects of human industry by the mid-20th century, garment workers have continued to labor under challenging conditions that demand repetitive manual labor. Mass-produced clothing is often made in what are considered by some to be sweatshops, typified by long work hours, lack of benefits, and lack of worker representation. While most examples of such conditions are found in developing countries, clothes made in industrialized nations may also be manufactured similarly, often staffed by undocumented immigrants.[citation needed]
Outsourcing production to low wage countries like Bangladesh, China, India and Sri Lanka became possible when the Multi Fibre Agreement (MFA) was abolished. The MFA, which placed quotas on textiles imports, was deemed a protectionist measure.[citation needed] Globalization is often quoted as the single most contributing factor to the poor working conditions of garment workers. Although many countries recognize treaties like the International Labor Organization, which attempt to set standards for worker safety and rights, many countries have made exceptions to certain parts of the treaties or failed to thoroughly enforce them. India for example has not ratified sections 87 and 92 of the treaty.[citation needed]
The use of animal fur in clothing dates to prehistoric times. It is currently associated in developed countries with expensive, designer clothing, although fur is still used by indigenous people in arctic zones and higher elevations for its warmth and protection. Once uncontroversial, it has recently been the focus of campaigns on the grounds that campaigners consider it cruel and unnecessary. PETA, along with other animal rights and animal liberation groups have called attention to fur farming and other practices they consider cruel.
Many kinds of clothing are designed to be ironed before they are worn to remove wrinkles. Most modern formal and semi-formal clothing is in this category (for example, dress shirts and suits). Ironed clothes are believed to look clean, fresh, and neat. Much contemporary casual clothing is made of knit materials that do not readily wrinkle, and do not require ironing. Some clothing is permanent press, having been treated with a coating (such as polytetrafluoroethylene) that suppresses wrinkles and creates a smooth appearance without ironing.
A resin used for making non-wrinkle shirts releases formaldehyde, which could cause contact dermatitis for some people; no disclosure requirements exist, and in 2008 the U.S. Government Accountability Office tested formaldehyde in clothing and found that generally the highest levels were in non-wrinkle shirts and pants. In 1999, a study of the effect of washing on the formaldehyde levels found that after 6 months after washing, 7 of 27 shirts had levels in excess of 75 ppm, which is a safe limit for direct skin exposure.
In past times, mending was an art. A meticulous tailor or seamstress could mend rips with thread raveled from hems and seam edges so skillfully that the tear was practically invisible. When the raw material – cloth – was worth more than labor, it made sense to expend labor in saving it. Today clothing is considered a consumable item. Mass-manufactured clothing is less expensive than the labor required to repair it. Many people buy a new piece of clothing rather than spend time mending. The thrifty still replace zippers and buttons and sew up ripped hems.
Oklahoma City is the capital and largest city of the state of Oklahoma. The county seat of Oklahoma County, the city ranks 27th among United States cities in population. The population grew following the 2010 Census, with the population estimated to have increased to 620,602 as of July 2014. As of 2014, the Oklahoma City metropolitan area had a population of 1,322,429, and the Oklahoma City-Shawnee Combined Statistical Area had a population of 1,459,758 (Chamber of Commerce) residents, making it Oklahoma's largest metropolitan area. Oklahoma City's city limits extend into Canadian, Cleveland, and Pottawatomie counties, though much of those areas outside of the core Oklahoma County area are suburban or rural (watershed). The city ranks as the eighth-largest city in the United States by land area (including consolidated city-counties; it is the largest city in the United States by land area whose government is not consolidated with that of a county or borough).
Oklahoma City, lying in the Great Plains region, features one of the largest livestock markets in the world. Oil, natural gas, petroleum products and related industries are the largest sector of the local economy. The city is situated in the middle of an active oil field and oil derricks dot the capitol grounds. The federal government employs large numbers of workers at Tinker Air Force Base and the United States Department of Transportation's Mike Monroney Aeronautical Center (these two sites house several offices of the Federal Aviation Administration and the Transportation Department's Enterprise Service Center, respectively).
Oklahoma City is on the I-35 Corridor and is one of the primary travel corridors into neighboring Texas and Mexico. Located in the Frontier Country region of the state, the city's northeast section lies in an ecological region known as the Cross Timbers. The city was founded during the Land Run of 1889, and grew to a population of over 10,000 within hours of its founding. The city was the scene of the April 19, 1995 bombing of the Alfred P. Murrah Federal Building, in which 168 people died. It was the deadliest terror attack in the history of the United States until the attacks of September 11, 2001, and remains the deadliest act of domestic terrorism in U.S. history.
Oklahoma City was settled on April 22, 1889, when the area known as the "Unassigned Lands" was opened for settlement in an event known as "The Land Run". Some 10,000 homesteaders settled the area that would become the capital of Oklahoma. The town grew quickly; the population doubled between 1890 and 1900. Early leaders of the development of the city included Anton Classen, John Shartel, Henry Overholser and James W. Maney.
By the time Oklahoma was admitted to the Union in 1907, Oklahoma City had surpassed Guthrie, the territorial capital, as the population center and commercial hub of the new state. Soon after, the capital was moved from Guthrie to Oklahoma City. Oklahoma City was a major stop on Route 66 during the early part of the 20th century; it was prominently mentioned in Bobby Troup's 1946 jazz classic, "(Get Your Kicks on) Route 66", later made famous by artist Nat King Cole.
Before World War II, Oklahoma City developed major stockyards, attracting jobs and revenue formerly in Chicago and Omaha, Nebraska. With the 1928 discovery of oil within the city limits (including under the State Capitol), Oklahoma City became a major center of oil production. Post-war growth accompanied the construction of the Interstate Highway System, which made Oklahoma City a major interchange as the convergence of I-35, I-40 and I-44. It was also aided by federal development of Tinker Air Force Base.
Patience Latting was elected Mayor of Oklahoma City in 1971, becoming the city's first female mayor. Latting was also the first woman to serve as mayor of a U.S. city with over 350,000 residents.
In 1993, the city passed a massive redevelopment package known as the Metropolitan Area Projects (MAPS), intended to rebuild the city's core with civic projects to establish more activities and life to downtown. The city added a new baseball park; central library; renovations to the civic center, convention center and fairgrounds; and a water canal in the Bricktown entertainment district. Water taxis transport passengers within the district, adding color and activity along the canal. MAPS has become one of the most successful public-private partnerships undertaken in the U.S., exceeding $3 billion in private investment as of 2010. As a result of MAPS, the population living in downtown housing has exponentially increased, together with demand for additional residential and retail amenities, such as grocery, services, and shops.
Since the MAPS projects' completion, the downtown area has seen continued development. Several downtown buildings are undergoing renovation/restoration. Notable among these was the restoration of the Skirvin Hotel in 2007. The famed First National Center is being renovated.
Residents of Oklahoma City suffered substantial losses on April 19, 1995 when Timothy McVeigh detonated a bomb in front of the Murrah building. The building was destroyed (the remnants of which had to be imploded in a controlled demolition later that year), more than 100 nearby buildings suffered severe damage, and 168 people were killed. The site has been commemorated as the Oklahoma City National Memorial and Museum. Since its opening in 2000, over three million people have visited. Every year on April 19, survivors, families and friends return to the memorial to read the names of each person lost.
The "Core-to-Shore" project was created to relocate I-40 one mile (1.6 km) south and replace it with a boulevard to create a landscaped entrance to the city. This also allows the central portion of the city to expand south and connect with the shore of the Oklahoma River. Several elements of "Core to Shore" were included in the MAPS 3 proposal approved by voters in late 2009.
According to the United States Census Bureau, the city has a total area of 620.34 square miles (1,606.7 km2), of which, 601.11 square miles (1,556.9 km2) of it is land and 19.23 square miles (49.8 km2) of it is water. The total area is 3.09 percent water.
Oklahoma City lies in the Sandstone Hills region of Oklahoma, known for hills of 250 to 400 feet (120 m) and two species of oak: blackjack oak (Quercus marilandica) and post oak (Q. stellata). The northeastern part of the city and its eastern suburbs fall into an ecological region known as the Cross Timbers.
The city is roughly bisected by the North Canadian River (recently renamed the Oklahoma River inside city limits). The North Canadian once had sufficient flow to flood every year, wreaking destruction on surrounding areas, including the central business district and the original Oklahoma City Zoo. In the 1940s, a dam was built on the river to manage the flood control and reduced its level. In the 1990s, as part of the citywide revitalization project known as MAPS, the city built a series of low-water dams, returning water to the portion of the river flowing near downtown. The city has three large lakes: Lake Hefner and Lake Overholser, in the northwestern quarter of the city; and the largest, Lake Stanley Draper, in the sparsely populated far southeast portion of the city.
The population density normally reported for Oklahoma City using the area of its city limits can be a bit misleading. Its urbanized zone covers roughly 244 sq mi (630 km2) resulting in a density of 2,500 per square mile (2013 est), compared with larger rural watershed areas incorporated by the city, which cover the remaining 377 sq mi (980 km2) of the city limits.
The city is bisected geographically and culturally by the North Canadian River, which basically divides North Oklahoma City and South Oklahoma City. The two halves of the city were actually founded and plotted as separate cities, but soon grew together. The north side is characterized by very diverse and fashionable urban neighborhoods near the city center and sprawling suburbs further north. South Oklahoma City is generally more blue collar working class and significantly more industrial, having grown up around the Stockyards and meat packing plants at the turn of the century, and is currently the center of the city's rapidly growing Latino community.
Downtown Oklahoma City, which has 7,600 residents, is currently seeing an influx of new private investment and large scale public works projects, which have helped to resuscitate a central business district left almost deserted by the Oil Bust of the early 1980s. The centerpiece of downtown is the newly renovated Crystal Bridge and Myriad Botanical Gardens, one of the few elements of the Pei Plan to be completed. In the next few years a massive new central park will link the gardens near the CBD and the new convention center to be built just south of it to the North Canadian River, as part of a massive works project known as Core to Shore; the new park is part of MAPS3, a collection of civic projects funded by a 1-cent temporary (seven-year) sales tax increase.
Oklahoma City has a humid subtropical climate (Köppen: Cfa), with frequent variations in weather daily and seasonally, except during the consistently hot and humid summer months. Prolonged and severe droughts (sometimes leading to wildfires in the vicinity) as well as very heavy rainfall leading to flash flooding and flooding occur with some regularity. Consistent winds, usually from the south or south-southeast during the summer, help temper the hotter weather. Consistent northerly winds during the winter can intensify cold periods. Severe ice storms and snowstorms happen sporadically during the winter.
The average temperature is 61.4 °F (16.3 °C), with the monthly daily average ranging from 39.2 °F (4.0 °C) in January to 83.0 °F (28.3 °C) in July. Extremes range from −17 °F (−27 °C) on February 12, 1899 to 113 °F (45 °C) on August 11, 1936 and August 3, 2012; the last sub-zero (°F) reading was −5 °F (−21 °C) on February 10, 2011. Temperatures reach 100 °F (38 °C) on 10.4 days of the year, 90 °F (32 °C) on nearly 70 days, and fail to rise above freezing on 8.3 days. The city receives about 35.9 inches (91.2 cm) of precipitation annually, of which 8.6 inches (21.8 cm) is snow.
Oklahoma City has a very active severe weather season from March through June, especially during April and May. Being in the center of what is colloquially referred to as Tornado Alley, it is prone to especially frequent and severe tornadoes, as well as very severe hailstorms and occasional derechoes. Tornadoes have occurred in every month of the year and a secondary smaller peak also occurs during autumn, especially October. The Oklahoma City metropolitan area is one of the most tornado-prone major cities in the world, with about 150 tornadoes striking within the city limits since 1890. Since the time weather records have been kept, Oklahoma City has been struck by thirteen violent tornadoes, eleven F/EF4s and two F/EF5. On May 3, 1999 parts of southern Oklahoma City and nearby suburban communities suffered from one of the most powerful tornadoes on record, an F5 on the Fujita scale, with wind speeds estimated by radar at 318 mph (510 km/h). On May 20, 2013, far southwest Oklahoma City, along with Newcastle and Moore, was hit again by a EF5 tornado; it was 0.5 to 1.3 miles (0.80 to 2.09 km) wide and killed 23 people. Less than two weeks later, on May 31, another outbreak affected the Oklahoma City area, including an EF1 and an EF0 within the city and a tornado several miles west of the city that was 2.6 miles (4.2 km) in width, the widest tornado ever recorded.
With 19.48 inches of rainfall, May 2015 was by far Oklahoma City's record-wettest month since record keeping began in 1890. Across Oklahoma and Texas generally, there was record flooding in the latter part of the month 
As of the 2010 census, there were 579,999 people, 230,233 households, and 144,120 families residing in the city. The population density was 956.4 inhabitants per square mile (321.9/km²). There were 256,930 housing units at an average density of 375.9 per square mile (145.1/km²).
There were 230,233 households, 29.4% of which had children under the age of 18 living with them, 43.4% were married couples living together, 13.9% had a female householder with no husband present, and 37.4% were non-families. One person households account for 30.5% of all households and 8.7% of all households had someone living alone who is 65 years of age or older. The average household size was 2.47 and the average family size was 3.11.
In the 2000 Census Oklahoma City's age composition was 25.5% under the age of 18, 10.7% from 18 to 24, 30.8% from 25 to 44, 21.5% from 45 to 64, and 11.5% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 92.7 males.
Oklahoma City has experienced significant population increases since the late 1990s. In May 2014, the U.S. Census announced Oklahoma City had an estimated population of 620,602 in 2014 and that it had grown 5.3 percent between April 2010 and June 2013. Since the official Census in 2000, Oklahoma City had grown 21 percent (a 114,470 raw increase) according to the Bureau estimates. The 2014 estimate of 620,602 is the largest population Oklahoma City has ever recorded. It is the first city in the state to record a population greater than 600,000 residents and the largest municipal population of the Great Plains region (OK, KS, NE, SD, ND).
Oklahoma City is the principal city of the eight-county Oklahoma City Metropolitan Statistical Area in Central Oklahoma and is the state's largest urbanized area. Based on population rank, the metropolitan area was the 42nd largest in the nation as of 2012.
With regards to Mexican drug cartels, Oklahoma City has traditionally been the territory of the notorious Juárez Cartel, but the Sinaloa Cartel has been reported as trying to establish a foothold in Oklahoma City. There are many rival gangs in Oklahoma City, one whose headquarters has been established in the city, the Southside Locos, traditionally known as Sureños.
Oklahoma City also has its share of very brutal crimes, particularly in the 1970s. The worst of which occurred in 1978, when six employees of a Sirloin Stockade restaurant on the city's south side were murdered execution-style in the restaurant's freezer. An intensive investigation followed, and the three individuals involved, who also killed three others in Purcell, Oklahoma, were identified. One, Harold Stafford, died in a motorcycle accident in Tulsa not long after the restaurant murders. Another, Verna Stafford, was sentenced to life without parole after being granted a new trial after she had previously been sentenced to death. Roger Dale Stafford, considered the mastermind of the murder spree, was executed by lethal injection at the Oklahoma State Penitentiary in 1995.
The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.
On April 19, 1995, the Alfred P. Murrah Federal Building was destroyed by a fertilizer bomb manufactured and detonated by Timothy McVeigh. The blast and catastrophic collapse killed 168 people and injured over 680. The blast shockwave destroyed or damaged 324 buildings within a 340-meter radius, destroyed or burned 86 cars, and shattered glass in 258 nearby buildings, causing at least an estimated $652 million worth of damage. The main suspect- Timothy McVeigh, was executed by lethal injection on June 11, 2001. It was the deadliest single domestic terrorist attack in US history, prior to 9/11.
While not in Oklahoma City proper, other large employers within the MSA region include: Tinker Air Force Base (27,000); University of Oklahoma (11,900); University of Central Oklahoma (2,900); and Norman Regional Hospital (2,800).
According to the Oklahoma City Chamber of Commerce, the metropolitan area's economic output grew by 33 percent between 2001 and 2005 due chiefly to economic diversification. Its gross metropolitan product was $43.1 billion in 2005 and grew to $61.1 billion in 2009.
In 2008, Forbes magazine named Oklahoma City the most "recession proof city in America". The magazine reported that the city had falling unemployment, one of the strongest housing markets in the country and solid growth in energy, agriculture and manufacturing. However, during the early 1980s, Oklahoma City had one of the worst job and housing markets due to the bankruptcy of Penn Square Bank in 1982 and then the post-1985 crash in oil prices.[citation needed]
Other theaters include Lyric Theatre, Jewel Box Theatre, Kirkpatrick Auditorium, the Poteet Theatre, the Oklahoma City Community College Bruce Owen Theater and the 488-seat Petree Recital Hall, at the Oklahoma City University campus. The university also opened the Wanda L Bass School of Music and auditorium in April 2006.
The Science Museum Oklahoma (formerly Kirkpatrick Science and Air Space Museum at Omniplex) houses exhibits on science, aviation, and an IMAX theater. The museum formerly housed the International Photography Hall of Fame (IPHF) that exhibits photographs and artifacts from a large collection of cameras and other artifacts preserving the history of photography. IPHF honors those who have made significant contributions to the art and/or science of photography and relocated to St. Louis, Missouri in 2013.
The Museum of Osteology houses more than 300 real animal skeletons. Focusing on the form and function of the skeletal system, this 7,000 sq ft (650 m2) museum displays hundreds of skulls and skeletons from all corners of the world. Exhibits include adaptation, locomotion, classification and diversity of the vertebrate kingdom. The Museum of Osteology is the only one of its kind in America.
The National Cowboy & Western Heritage Museum has galleries of western art and is home to the Hall of Great Western Performers. In contrast, the city will also be home to The American Indian Cultural Center and Museum that began construction in 2009 (although completion of the facility has been held up due to insufficient funding), on the south side of Interstate 40, southeast from Bricktown.
The Oklahoma City National Memorial in the northern part of Oklahoma City's downtown was created as the inscription on its eastern gate of the Memorial reads, "to honor the victims, survivors, rescuers, and all who were changed forever on April 19, 1995"; the memorial was built on the land formerly occupied by the Alfred P. Murrah Federal Building complex prior to its 1995 bombing. The outdoor Symbolic Memorial can be visited 24 hours a day for free, and the adjacent Memorial Museum, located in the former Journal Record building damaged by the bombing, can be entered for a small fee. The site is also home to the National Memorial Institute for the Prevention of Terrorism, a non-partisan, nonprofit think tank devoted to the prevention of terrorism.
The American Banjo Museum located in the Bricktown Entertainment district is dedicated to preserving and promoting the music and heritage of America's native musical instrument – the banjo. With a collection valued at $3.5 million it is truly a national treasure. An interpretive exhibits tells the evolution of the banjo from its humble roots in American slavery, to bluegrass, to folk and world music.
The Oklahoma History Center is the history museum of the state of Oklahoma. Located across the street from the governor's mansion at 800 Nazih Zuhdi Drive in northeast Oklahoma City, the museum opened in 2005 and is operated by the Oklahoma Historical Society. It preserves the history of Oklahoma from the prehistoric to the present day.
Oklahoma City is home to several professional sports teams, including the Oklahoma City Thunder of the National Basketball Association. The Thunder is the city's second "permanent" major professional sports franchise after the now-defunct AFL Oklahoma Wranglers and is the third major-league team to call the city home when considering the temporary hosting of the New Orleans/Oklahoma City Hornets for the 2005–06 and 2006–07 NBA seasons.
Other professional sports clubs in Oklahoma City include the Oklahoma City Dodgers, the Triple-A affiliate of the Los Angeles Dodgers, the Oklahoma City Energy FC of the United Soccer League, and the Crusaders of Oklahoma Rugby Football Club USA Rugby.
Chesapeake Energy Arena in downtown is the principal multipurpose arena in the city which hosts concerts, NHL exhibition games, and many of the city's pro sports teams. In 2008, the Oklahoma City Thunder became the major tenant. Located nearby in Bricktown, the Chickasaw Bricktown Ballpark is the home to the city's baseball team, the Dodgers. "The Brick", as it is locally known, is considered one of the finest minor league parks in the nation.[citation needed]
Oklahoma City is the annual host of the Big 12 Baseball Tournament, the World Cup of Softball, and the annual NCAA Women's College World Series. The city has held the 2005 NCAA Men's Basketball First and Second round and hosted the Big 12 Men's and Women's Basketball Tournaments in 2007 and 2009. The major universities in the area – University of Oklahoma, Oklahoma City University, and Oklahoma State University – often schedule major basketball games and other sporting events at Chesapeake Energy Arena and Chickasaw Bricktown Ballpark, although most home games are played at their campus stadiums.
Other major sporting events include Thoroughbred and Quarter horse racing circuits at Remington Park and numerous horse shows and equine events that take place at the state fairgrounds each year. There are numerous golf courses and country clubs spread around the city.
The state of Oklahoma hosts a highly competitive high school football culture, with many teams in the Oklahoma City metropolitan area. The Oklahoma Secondary School Activities Association (OSSAA) organizes high school football into eight distinct classes based on the size of school enrollment. Beginning with the largest, the classes are: 6A, 5A, 4A, 3A, 2A, A, B, and C. Class 6A is broken into two divisions. Oklahoma City area schools in this division include: Edmond North, Mustang, Moore, Yukon, Edmond Memorial, Edmond Santa Fe, Norman North, Westmoore, Southmoore, Putnam City North, Norman, Putnam City, Putnam City West, U.S. Grant, Midwest City.
The Oklahoma City Thunder of the National Basketball Association (NBA) has called Oklahoma City home since the 2008–09 season, when owner Clayton Bennett relocated the franchise from Seattle, Washington. The Thunder plays home games at the Chesapeake Energy Arena in downtown Oklahoma City, known affectionately in the national media as 'the Peake' and 'Loud City'. The Thunder is known by several nicknames, including "OKC Thunder" and simply "OKC", and its mascot is Rumble the Bison.
After a lackluster arrival to Oklahoma City for the 2008–09 season, the Oklahoma City Thunder secured a berth (8th) in the 2010 NBA Playoffs the next year after boasting its first 50-win season, winning two games in the first round against the Los Angeles Lakers. In 2012, Oklahoma City made it to the NBA Finals, but lost to the Miami Heat in five games. In 2013 the Thunder reached the Western Conference semifinals without All-Star guard Russell Westbrook, who was injured in their first round series against the Houston Rockets, only to lose to the Memphis Grizzlies. In 2014 Oklahoma City again reached the NBA's Western Conference Finals but eventually lost to the San Antonio Spurs in six games.
The Oklahoma City Thunder has been regarded by sports analysts as one of the elite franchises of the NBA's Western Conference and that of a media darling as the future of the league. Oklahoma City has earned Northwest Division titles every year since 2009 and has consistently improved its win record to 59-wins in 2014. The Thunder is led by first year head coach Billy Donovan and is anchored by several NBA superstars, including perennial All-Star point guard Russell Westbrook, 2014 MVP and four-time NBA scoring champion Kevin Durant, and Defensive Player of the Year nominee and shot-blocker Serge Ibaka.
In the aftermath of Hurricane Katrina, the NBA's New Orleans Hornets (now the New Orleans Pelicans) temporarily relocated to the Ford Center, playing the majority of its home games there during the 2005–06 and 2006–07 seasons. The team became the first NBA franchise to play regular-season games in the state of Oklahoma.[citation needed] The team was known as the New Orleans/Oklahoma City Hornets while playing in Oklahoma City. The team ultimately returned to New Orleans full-time for the 2007–08 season. The Hornets played their final home game in Oklahoma City during the exhibition season on October 9, 2007 against the Houston Rockets.
One of the more prominent landmarks downtown is the Crystal Bridge at the Myriad Botanical Gardens, a large downtown urban park. Designed by I. M. Pei, the Crystal Bridge is a tropical conservatory in the area. The park has an amphitheater, known as the Water Stage. In 2007, following a renovation of the stage, Oklahoma Shakespeare in the Park relocated to the Myriad Gardens. The Myriad Gardens will undergo a massive renovation in conjunction with the recently built Devon Tower directly north of it.
The Oklahoma City Zoo and Botanical Garden is home to numerous natural habitats, WPA era architecture and landscaping, and hosts major touring concerts during the summer at its amphitheater. Oklahoma City also has two amusement parks, Frontier City theme park and White Water Bay water park. Frontier City is an 'Old West'-themed amusement park. The park also features a recreation of a western gunfight at the 'OK Corral' and many shops that line the "Western" town's main street. Frontier City also hosts a national concert circuit at its amphitheater during the summer. Oklahoma City also has a combination racetrack and casino open year-round, Remington Park, which hosts both Quarter horse (March – June) and Thoroughbred (August – December) seasons.
Walking trails line Lake Hefner and Lake Overholser in the northwest part of the city and downtown at the canal and the Oklahoma River. The majority of the east shore area is taken up by parks and trails, including a new leashless dog park and the postwar-era Stars and Stripes Park. Lake Stanley Draper is the city's largest and most remote lake.
Oklahoma City has a major park in each quadrant of the city, going back to the first parks masterplan. Will Rogers Park, Lincoln Park, Trosper Park, and Woodson Park were once connected by the Grand Boulevard loop, some sections of which no longer exist. Martin Park Nature Center is a natural habitat in far northwest Oklahoma City. Will Rogers Park is home to the Lycan Conservatory, the Rose Garden, and Butterfly Garden, all built in the WPA era. Oklahoma City is home to the American Banjo Museum, which houses a large collection of highly decorated banjos from the early 20th century and exhibits on the history of the banjo and its place in American history. Concerts and lectures are also held there.
In April 2005, the Oklahoma City Skate Park at Wiley Post Park was renamed the Mat Hoffman Action Sports Park to recognize Mat Hoffman, an Oklahoma City area resident and businessman that was instrumental in the design of the skate park and is a 10-time BMX World Vert champion. In March 2009, the Mat Hoffman Action Sports Park was named by the National Geographic Society Travel Guide as one of the "Ten Best."
The City of Oklahoma City has operated under a council-manager form of city government since 1927. Mick Cornett serves as Mayor, having first been elected in 2004, and re-elected in 2006, 2010, and 2014. Eight councilpersons represent each of the eight wards of Oklahoma City. City Manager Jim Couch was appointed in late 2000. Couch previously served as assistant city manager, Metropolitan Area Projects Plan (MAPS) director and utilities director prior to his service as city manager.
The city is home to several colleges and universities. Oklahoma City University, formerly known as Epworth University, was founded by the United Methodist Church on September 1, 1904 and is renowned for its performing arts, science, mass communications, business, law, and athletic programs. OCU has its main campus in the north-central section of the city, near the city's chinatown area. OCU Law is located in the Midtown district near downtown, in the old Central High School building.
The University of Oklahoma has several institutions of higher learning in the city and metropolitan area, with OU Medicine and the University of Oklahoma Health Sciences Center campuses located east of downtown in the Oklahoma Health Center district, and the main campus located to the south in the suburb of Norman. The OU Medicine hosting the state's only Level-One trauma center. OU Health Sciences Center is one of the nation's largest independent medical centers, employing more than 12,000 people. OU is one of only four major universities in the nation to operate six medical schools.[clarification needed]
The third-largest university in the state, the University of Central Oklahoma, is located just north of the city in the suburb of Edmond. Oklahoma Christian University, one of the state's private liberal arts institutions, is located just south of the Edmond border, inside the Oklahoma City limits.
Oklahoma City Community College in south Oklahoma City is the second-largest community college in the state. Rose State College is located east of Oklahoma City in suburban Midwest City. Oklahoma State University–Oklahoma City is located in the "Furniture District" on the Westside. Northeast of the city is Langston University, the state's historically black college (HBCU). Langston also has an urban campus in the eastside section of the city. Southern Nazarene University, which was founded by the Church of the Nazarene, is a university located in suburban Bethany, which is surrounded by the Oklahoma City city limits.
Although technically not a university, the FAA's Mike Monroney Aeronautical Center has many aspects of an institution of higher learning. Its FAA Academy is accredited by the North Central Association of Colleges and Schools. Its Civil Aerospace Medical Institute (CAMI) has a medical education division responsible for aeromedical education in general as well as the education of aviation medical examiners in the U.S. and 93 other countries. In addition, The National Academy of Science offers Research Associateship Programs for fellowship and other grants for CAMI research.
Oklahoma City is home to the state's largest school district, Oklahoma City Public Schools. The district's Classen School of Advanced Studies and Harding Charter Preparatory High School rank high among public schools nationally according to a formula that looks at the number of Advanced Placement, International Baccalaureate and/or Cambridge tests taken by the school's students divided by the number of graduating seniors. In addition, OKCPS's Belle Isle Enterprise Middle School was named the top middle school in the state according to the Academic Performance Index, and recently received the Blue Ribbon School Award, in 2004 and again in 2011. KIPP Reach College Preparatory School in Oklahoma City received the 2012 National Blue Ribbon along with its school leader, Tracy McDaniel Sr., being awarded the Terrel H. Bell Award for Outstanding Leadership.
The Oklahoma School of Science and Mathematics, a school for some of the state's most gifted math and science pupils, is also located in Oklahoma City.
Oklahoma City has several public career and technology education schools associated with the Oklahoma Department of Career and Technology Education, the largest of which are Metro Technology Center and Francis Tuttle Technology Center.
Private career and technology education schools in Oklahoma City include Oklahoma Technology Institute, Platt College, Vatterott College, and Heritage College. The Dale Rogers Training Center in Oklahoma City is a nonprofit vocational training center for individuals with disabilities.
The Oklahoman is Oklahoma City's major daily newspaper and is the most widely circulated in the state. NewsOK.com is the Oklahoman's online presence. Oklahoma Gazette is Oklahoma City's independent newsweekly, featuring such staples as local commentary, feature stories, restaurant reviews and movie listings and music and entertainment. The Journal Record is the city's daily business newspaper and okcBIZ is a monthly publication that covers business news affecting those who live and work in Central Oklahoma.
There are numerous community and international newspapers locally that cater to the city's ethnic mosaic; such as The Black Chronicle, headquartered in the Eastside, the OK VIETIMES and Oklahoma Chinese Times, located in Asia District, and various Hispanic community publications. The Campus is the student newspaper at Oklahoma City University. Gay publications include The Gayly Oklahoman.
An upscale lifestyle publication called Slice Magazine is circulated throughout the metropolitan area. In addition, there is a magazine published by Back40 Design Group called The Edmond Outlook. It contains local commentary and human interest pieces direct-mailed to over 50,000 Edmond residents.
Oklahoma City was home to several pioneers in radio and television broadcasting. Oklahoma City's WKY Radio was the first radio station transmitting west of the Mississippi River and the third radio station in the United States. WKY received its federal license in 1921 and has continually broadcast under the same call letters since 1922. In 1928, WKY was purchased by E.K. Gaylord's Oklahoma Publishing Company and affiliated with the NBC Red Network; in 1949, WKY-TV (channel 4) went on the air and later became the first independently owned television station in the U.S. to broadcast in color. In mid-2002, WKY radio was purchased outright by Citadel Broadcasting, who was bought out by Cumulus Broadcasting in 2011. The Gaylord family earlier sold WKY-TV in 1976, which has gone through a succession of owners (what is now KFOR-TV is currently owned by Tribune Broadcasting as of December 2013).
The major U.S. broadcast television networks have affiliates in the Oklahoma City market (ranked 41st for television by Nielsen and 48th for radio by Arbitron, covering a 34-county area serving the central, northern-central and west-central sections Oklahoma); including NBC affiliate KFOR-TV (channel 4), ABC affiliate KOCO-TV (channel 5), CBS affiliate KWTV-DT (channel 9, the flagship of locally based Griffin Communications), PBS station KETA-TV (channel 13, the flagship of the state-run OETA member network), Fox affiliate KOKH-TV (channel 25), CW affiliate KOCB (channel 34), independent station KAUT-TV (channel 43), MyNetworkTV affiliate KSBI-TV (channel 52), and Ion Television owned-and-operated station KOPX-TV (channel 62). The market is also home to several religious stations including TBN owned-and-operated station KTBO-TV (channel 14) and Norman-based Daystar owned-and-operated station KOCM (channel 46).
Oklahoma City is protected by the Oklahoma City Fire Department (OKCFD), which employs 1015 paid, professional firefighters. The current Chief of Department is G. Keith Bryant, the department is also commanded by three Deputy Chiefs, who – along with the department chief – oversee the Operational Services, Prevention Services, and Support Services bureaus. The OKCFD currently operates out of 37 fire stations, located throughout the city in six battalions. The OKCFD also operates a fire apparatus fleet of 36 engines (including 30 paramedic engines), 13 ladders, 16 brush patrol units, six water tankers, two hazardous materials units, one Technical Rescue Unit, one Air Supply Unit, six Arson Investigation Units, and one Rehabilitation Unit. Each engine is staffed with a driver, an officer, and one to two firefighters, while each ladder company is staffed with a driver, an officer, and one firefighter. Minimum staffing per shift is 213 personnel. The Oklahoma City Fire Department responds to over 70,000 emergency calls annually.
Oklahoma City is an integral point on the United States Interstate Network, with three major interstate highways – Interstate 35, Interstate 40, and Interstate 44 – bisecting the city. Interstate 240 connects Interstate 40 and Interstate 44 in south Oklahoma City, while Interstate 235 spurs from Interstate 44 in north-central Oklahoma City into downtown.
Major state expressways through the city include Lake Hefner Parkway (SH-74), the Kilpatrick Turnpike, Airport Road (SH-152), and Broadway Extension (US-77) which continues from I-235 connecting Central Oklahoma City to Edmond. Lake Hefner Parkway runs through northwest Oklahoma City, while Airport Road runs through southwest Oklahoma City and leads to Will Rogers World Airport. The Kilpatrick Turnpike loops around north and west Oklahoma City.
Oklahoma City also has several major national and state highways within its city limits. Shields Boulevard (US-77) continues from E.K. Gaylord Boulevard in downtown Oklahoma City and runs south eventually connecting to I-35 near the suburb of Moore. Northwest Expressway (Oklahoma State Highway 3) runs from North Classen Boulevard in north-central Oklahoma City to the northwestern suburbs.
Oklahoma City is served by two primary airports, Will Rogers World Airport and the much smaller Wiley Post Airport (incidentally, the two honorees died in the same plane crash in Alaska) Will Rogers World Airport is the state's busiest commercial airport, with over 3.6 million passengers annually. Tinker Air Force Base, in southeast Oklahoma City, is the largest military air depot in the nation; a major maintenance and deployment facility for the Navy and the Air Force, and the second largest military institution in the state (after Fort Sill in Lawton).
METRO Transit is the city's public transit company. The main transfer terminal is located downtown at NW 5th Street and Hudson Avenue. METRO Transit maintains limited coverage of the city's main street grid using a hub-and-spoke system from the main terminal, making many journeys impractical due to the rather small number of bus routes offered and that most trips require a transfer downtown. The city has recognized that transit as a major issue for the rapidly growing and urbanizing city and has initiated several studies in recent times to improve upon the existing bus system starting with a plan known as the Fixed Guideway Study. This study identified several potential commuter transit routes from the suburbs into downtown OKC as well as feeder-line bus and/or rail routes throughout the city.
On December 2009, Oklahoma City voters passed MAPS 3, the $777 million (7-year 1-cent tax) initiative, which will include funding (appx $130M) for an estimated 5-to-6-mile (8.0 to 9.7 km) modern streetcar in downtown Oklahoma City and the establishment of a transit hub. It is believed the streetcar would begin construction in 2014 and be in operation around 2017.
Oklahoma City and the surrounding metropolitan area are home to a number of health care facilities and specialty hospitals. In Oklahoma City's MidTown district near downtown resides the state's oldest and largest single site hospital, St. Anthony Hospital and Physicians Medical Center.
OU Medicine, an academic medical institution located on the campus of The University of Oklahoma Health Sciences Center, is home to OU Medical Center. OU Medicine operates Oklahoma's only level-one trauma center at the OU Medical Center and the state's only level-one trauma center for children at Children's Hospital at OU Medicine, both of which are located in the Oklahoma Health Center district. Other medical facilities operated by OU Medicine include OU Physicians and OU Children's Physicians, the OU College of Medicine, the Oklahoma Cancer Center and OU Medical Center Edmond, the latter being located in the northern suburb of Edmond.
INTEGRIS Health owns several hospitals, including INTEGRIS Baptist Medical Center, the INTEGRIS Cancer Institute of Oklahoma, and the INTEGRIS Southwest Medical Center. INTEGRIS Health operates hospitals, rehabilitation centers, physician clinics, mental health facilities, independent living centers and home health agencies located throughout much of Oklahoma. INTEGRIS Baptist Medical Center was named in U.S. News & World Report's 2012 list of Best Hospitals. INTEGRIS Baptist Medical Center ranks high-performing in the following categories: Cardiology and Heart Surgery; Diabetes and Endocrinology; Ear, Nose and Throat; Gastroenterology; Geriatrics; Nephrology; Orthopedics; Pulmonology and Urology.
The Midwest Regional Medical Center located in the suburb of Midwest City; other major hospitals in the city include the Oklahoma Heart Hospital and the Mercy Health Center. There are 347 physicians for every 100,000 people in the city.
In the American College of Sports Medicine's annual ranking of the United States' 50 most populous metropolitan areas on the basis of community health, Oklahoma City took last place in 2010, falling five places from its 2009 rank of 45. The ACSM's report, published as part of its American Fitness Index program, cited, among other things, the poor diet of residents, low levels of physical fitness, higher incidences of obesity, diabetes, and cardiovascular disease than the national average, low access to recreational facilities like swimming pools and baseball diamonds, the paucity of parks and low investment by the city in their development, the high percentage of households below the poverty level, and the lack of state-mandated physical education curriculum as contributing factors.
Post-punk is a heterogeneous type of rock music that emerged in the wake of the punk movement of the 1970s. Drawing inspiration from elements of punk rock while departing from its musical conventions and wider cultural affiliations, post-punk music was marked by varied, experimentalist sensibilities and its "conceptual assault" on rock tradition. Artists embraced electronic music, black dance styles and the avant-garde, as well as novel recording technology and production techniques. The movement also saw the frequent intersection of music with art and politics, as artists liberally drew on sources such as critical theory, cinema, performance art and modernist literature. Accompanying these musical developments were subcultures that produced visual art, multimedia performances, independent record labels and fanzines in conjunction with the music.
The term "post-punk" was first used by journalists in the late 1970s to describe groups moving beyond punk's sonic template into disparate areas. Many of these artists, initially inspired by punk's DIY ethic and energy, ultimately became disillusioned with the style and movement, feeling that it had fallen into commercial formula, rock convention and self-parody. They repudiated its populist claims to accessibility and raw simplicity, instead seeing an opportunity to break with musical tradition, subvert commonplaces and challenge audiences. Artists moved beyonds punk's focus on the concerns of a largely white, male, working class population and abandoned its continued reliance on established rock and roll tropes, such as three-chord progressions and Chuck Berry-based guitar riffs. These artists instead defined punk as "an imperative to constant change", believing that "radical content demands radical form".
Though the music varied widely between regions and artists, the post-punk movement has been characterized by its "conceptual assault" on rock conventions and rejection of aesthetics perceived of as traditionalist, hegemonic or rockist in favor of experimentation with production techniques and non-rock musical styles such as dub, electronic music, disco, noise, jazz, krautrock, world music and the avant-garde. While post-punk musicians often avoided or intentionally obscured conventional influences, previous musical styles did serve as touchstones for the movement, including particular brands of glam, art rock and "[the] dark undercurrent of '60s music".[nb 1] According to Reynolds, artists once again approached the studio as an instrument, using new recording methods and pursuing novel sonic territories. Author Matthew Bannister wrote that post-punk artists rejected the high cultural references of 1960s rock artists like the Beatles and Bob Dylan as well as paradigms that defined "rock as progressive, as art, as 'sterile' studio perfectionism ... by adopting an avant-garde aesthetic".
Nicholas Lezard described post-punk as "a fusion of art and music". The era saw the robust appropriation of ideas from literature, art, cinema, philosophy, politics and critical theory into musical and pop cultural contexts. Artists sought to refuse the common distinction between high and low culture and returned to the art school tradition found in the work of artists such as Captain Beefheart and David Bowie. Among major influences on a variety of post-punk artists were writers such as William S. Burroughs and J.G. Ballard, avant-garde political scenes such as Situationism and Dada, and intellectual movements such as postmodernism. Many artists viewed their work in explicitly political terms. Additionally, in some locations, the creation of post-punk music was closely linked to the development of efficacious subcultures, which played important roles in the production of art, multimedia performances, fanzines and independent labels related to the music. Many post-punk artists maintained an anti-corporatist approach to recording and instead seized on alternate means of producing and releasing music. Journalists also became an important element of the culture, and popular music magazines and critics became immersed in the movement.
The scope of the term "post-punk" has been subject to controversy. While some critics, such as AllMusic's Stephen Thomas Erlewine, have employed the term "post-punk" to denote "a more adventurous and arty form of punk", others have suggested it pertains to a set of artistic sensibilities and approaches rather than any unifying style. Music journalist and post-punk scholar Simon Reynolds has advocated that post-punk be conceived as "less a genre of music than a space of possibility", suggesting that "what unites all this activity is a set of open-ended imperatives: innovation; willful oddness; the willful jettisoning of all things precedented or 'rock'n'roll'". Nicholas Lezard, problematizing the categorization of post-punk as a genre, described the movement as "so multifarious that only the broadest use of the term is possible".
Generally, post-punk music is defined as music that emerged from the cultural milieu of punk rock in the late 1970s, although many groups now categorized as post-punk were initially subsumed under the broad umbrella of punk or new wave music, only becoming differentiated as the terms came to signify more narrow styles. Additionally, the accuracy of the term's chronological prefix "post" has been disputed, as various groups commonly labeled post-punk in fact predate the punk rock movement. Reynolds defined the post-punk era as occurring loosely between 1978 and 1984.
During the initial punk era, a variety of entrepreneurs interested in local punk-influenced music scenes began founding independent record labels, including Rough Trade (founded by record shop owner Geoff Travis) and Factory (founded by Manchester-based television personality Tony Wilson). By 1977, groups began pointedly pursuing methods of releasing music independently , an idea disseminated in particular by the Buzzcocks' release of their Spiral Scratch EP on their own label as well as the self-released 1977 singles of Desperate Bicycles. These DIY imperatives would help form the production and distribution infrastructure of post-punk and the indie music scene that later blossomed in the mid-1980s.
In late 1977, music writers for Sounds first used the terms "New Musick" and "post punk" to describe British acts such as Siouxsie and the Banshees and Wire, who began experimenting with sounds, lyrics and aesthetics that differed significantly from their punk contemporaries. Writer Jon Savage described some of these early developments as exploring "harsh urban scrapings [,] controlled white noise" and "massively accented drumming". In January 1978, singer John Lydon (then known as Johnny Rotten) announced the break-up of his pioneering punk band the Sex Pistols, citing his disillusionment with punk's musical predictability and cooption by commercial interests, as well as his desire to explore more diverse interests.
As the initial punk movement dwindled, vibrant new scenes began to coalesce out of a variety of bands pursuing experimental sounds and wider conceptual territory in their work. Many of these artists drew on backgrounds in art and viewed their music as invested in particular political or aesthetic agendas. British music publications such as the NME and Sounds developed an influential part in this nascent post-punk culture, with writers like Jon Savage, Paul Morley and Ian Penman developing a dense (and often playful) style of criticism that drew on critical theory, radical politics and an eclectic variety of other sources.
Weeks after ending the Sex Pistols, Lydon formed the experimental group Public Image Ltd and declared the project to be "anti music of any kind". Public Image and other acts such as the Pop Group and the Slits had begun experimenting with dance music, dub production techniques and the avant-garde, while punk-indebted Manchester acts such as Joy Division, The Fall and A Certain Ratio developed unique styles which drew on a similarly disparate range of influences across music and modernist literature. Bands such as Scritti Politti, Gang of Four and This Heat incorporated Leftist political philosophy and their own art school studies in their work.
The innovative production techniques devised by post-punk producers such as Martin Hannett and Dennis Bovell during this period would become an important element of the emerging music, with studio experimentation taking a central role. A variety of groups that predated punk, such as Cabaret Voltaire and Throbbing Gristle, experimented with crude production techniques and electronic instruments in tandem with performance art methods and influence from transgressive literature, ultimately helping to pioneer industrial music. Throbbing Gristle's independent label Industrial Records would become a hub for this scene and provide it with its namesake.
In the mid 1970s, various American groups (some with ties to Downtown Manhattan's punk scene, including Television and Suicide) had begun expanding on the vocabulary of punk music. Midwestern groups such as Pere Ubu and Devo drew inspiration from the region's derelict industrial environments, employing conceptual art techniques, musique concrète and unconventional verbal styles that would presage the post-punk movement by several years. A variety of subsequent groups, including New York-based Talking Heads and Boston-based Mission of Burma, combined elements of punk with art school sensibilities. In 1978, the former band began a series of collaborations with British ambient pioneer and ex-Roxy Music member Brian Eno, experimenting with Dada-influenced lyrical techniques, dance music, and African polyrhythms. San Francisco's vibrant post-punk scene was centered around such groups as Chrome, the Residents and Tuxedomoon, who incorporated multimedia experimentation, film and ideas from Antonin Artaud's Theater of Cruelty.
Also emerging during this period was New York's no wave movement, a short-lived art and music scene that began in part as a reaction against punk's recycling of traditionalist rock tropes and often reflected an abrasive, confrontational and nihilistic worldview. No wave musicians such as the Contortions, Teenage Jesus and the Jerks, Mars, DNA, Theoretical Girls and Rhys Chatham instead experimented with noise, dissonance and atonality in addition to non-rock styles. The former four groups were included on the Eno-produced No New York compilation, often considered the quintessential testament to the scene. The no wave-affiliated label ZE Records was founded in 1978, and would also produce acclaimed and influential compilations in subsequent years.
British post-punk entered the 1980s with support from members of the critical community—American critic Greil Marcus characterised "Britain's postpunk pop avant-garde" in a 1980 Rolling Stone article as "sparked by a tension, humour and sense of paradox plainly unique in present day pop music"—as well as media figures such as BBC DJ John Peel, while several groups, such as PiL and Joy Division, achieved some success in the popular charts. The network of supportive record labels that included Industrial, Fast, E.G., Mute, Axis/4AD and Glass continued to facilitate a large output of music, by artists such as the Raincoats, Essential Logic, Killing Joke, the Teardrop Explodes, and the Psychedelic Furs.
However, during this period, major figures and artists in the scene began leaning away from underground aesthetics. In the music press, the increasingly esoteric writing of post-punk publications soon began to alienate their readerships; it is estimated that within several years, NME suffered the loss of half its circulation. Writers like Paul Morley began advocating "overground brightness" instead of the experimental sensibilities promoted in early years. Morley's own musical collaboration with engineer Gary Langan and programmer J. J. Jeczalik, the Art of Noise, would attempt to bring sampled and electronic sounds to the pop mainstream. A variety of more pop-oriented groups, including ABC, the Associates, Adam and the Ants and Bow Wow Wow (the latter two managed by former Sex Pistols manager Malcolm McLaren) emerged in tandem with the development of the New Romantic subcultural scene. Emphasizing glamour, fashion, and escapism in distinction to the experimental seriousness of earlier post-punk groups, the club-oriented scene drew some suspicion from denizens of the movement.
Artists such as Gary Numan, the Human League, Soft Cell, John Foxx and Visage helped pioneer a new synthpop style that drew more heavily from electronic and synthesizer music and benefited from the rise of MTV. Post-punk artists such as Scritti Politti's Green Gartside and Josef K's Paul Haig, previously engaged in avant-garde practices, turned away from these approaches and pursued mainstream styles and commercial success. These new developments, in which post-punk artists attempted to bring subversive ideas into the pop mainstream, began to be categorized under the marketing term new pop.
In the early 1980s, Downtown Manhattan's no wave scene transitioned from its abrasive origins into a more dance-oriented sound, with compilations such as ZE's Mutant Disco (1981) highlighting a newly playful sensibility borne out of the city's clash of hip hop, disco and punk styles, as well as dub reggae and world music influences. Artists such as Liquid Liquid, the B-52s, Cristina, Arthur Russell, James White and the Blacks and Lizzy Mercier Descloux pursued a formula described by Luc Sante as "anything at all + disco bottom". The decadent parties and art installations of venues such as Club 57 and the Mudd Club became cultural hubs for musicians and visual artists alike, with figures such as Jean-Michel Basquiat, Keith Haring and Michael Holman frequenting the scene. Other no wave-indebted groups such as Swans, Glenn Branca, the Lounge Lizards, Bush Tetras and Sonic Youth instead continued exploring the early scene's forays into noise and more abrasive territory.
In Germany, groups such as Einstürzende Neubauten developed a unique style of industrial music, utilizing avant-garde noise, homemade instruments and found objects. Members of that group would later go on to collaborate with members of the Birthday Party. In Brazil, the post-punk scene grew after the generation of Brasilia rock with bands such as Legião Urbana, Capital Inicial and Plebe Rude and then the opening of the music club Madame Satã in São Paulo, with acts like Cabine C, Titãs, Patife Band, Fellini and Mercenárias, as documented on compilations like The Sexual Life of the Savages and the Não Wave/Não São Paulo series, released in the UK, Germany and Brazil, respectively.[citation needed]
The original post-punk movement ended as the bands associated with the movement turned away from its aesthetics, often in favor of more commercial sounds. Many of these groups would continue recording as part of the new pop movement, with entryism becoming a popular concept. In the United States, driven by MTV and modern rock radio stations, a number of post-punk acts had an influence on or became part of the Second British Invasion of "New Music" there. Some shifted to a more commercial new wave sound (such as Gang of Four), while others were fixtures on American college radio and became early examples of alternative rock. Perhaps the most successful band to emerge from post-punk was U2, who combined elements of religious imagery together with political commentary into their often anthemic music.
Until recently, in most critical writing the post-punk era was "often dismissed as an awkward period in which punk's gleeful ructions petered out into the vacuity of the Eighties". Contemporary scholars have argued to the contrary, asserting that the period produced significant innovations and music on its own. Simon Reynolds described the period as "a fair match for the sixties in terms of the sheer amount of great music created, the spirit of adventure and idealism that infused it, and the way that the music seemed inextricably connected to the political and social turbulence of its era". Nicholas Lezard wrote that the music of the period "was avant-garde, open to any musical possibilities that suggested themselves, united only in the sense that it was very often cerebral, concocted by brainy young men and women interested as much in disturbing the audience, or making them think, as in making a pop song".
Post-punk was an eclectic genre which resulted in a wide variety of musical innovations and helped merge white and black musical styles. Out of the post-punk milieu came the beginnings of various subsequent genres, including new wave, dance-rock, New Pop, industrial music, synthpop, post-hardcore, neo-psychedelia alternative rock and house music. Bands such as Joy Division, Siouxsie and the Banshees, Bauhaus and the Cure played in a darker, more morose style of post-punk that lead to the development of the gothic rock genre.
At the turn of the 21st century, a post-punk revival developed in British and American alternative and indie rock, which soon started appearing in other countries, as well. The earliest sign of a revival was the emergence of various underground bands in the mid-'90s. However, the first commercially successful bands – the Strokes, Franz Ferdinand, Interpol, Neils Children and Editors – surfaced in the late 1990s to early 2000s, as did several dance-oriented bands such as the Rapture, Radio 4 and LCD Soundsystem. Additionally, some darker post-punk bands began to appear in the indie music scene in the 2010s, including Cold Cave, She Wants Revenge, Eagulls, the Soft Moon, She Past Away and Light Asylum, who were also affiliated with the darkwave revival, as well as A Place to Bury Strangers, who combined early post-punk and shoegaze. These bands tend to draw a fanbase who are a combination of the indie music subculture, older post-punk fans and the current goth subculture. In the 2010s, Savages played a music reminiscent of early British post-punk bands of the late '70s.
The original Latin word "universitas" refers in general to "a number of persons associated into one body, a society, company, community, guild, corporation, etc." At the time of the emergence of urban town life and medieval guilds, specialised "associations of students and teachers with collective legal rights usually guaranteed by charters issued by princes, prelates, or the towns in which they were located" came to be denominated by this general term. Like other guilds, they were self-regulating and determined the qualifications of their members.
An important idea in the definition of a university is the notion of academic freedom. The first documentary evidence of this comes from early in the life of the first university. The University of Bologna adopted an academic charter, the Constitutio Habita, in 1158 or 1155, which guaranteed the right of a traveling scholar to unhindered passage in the interests of education. Today this is claimed as the origin of "academic freedom". This is now widely recognised internationally - on 18 September 1988, 430 university rectors signed the Magna Charta Universitatum, marking the 900th anniversary of Bologna's foundation. The number of universities signing the Magna Charta Universitatum continues to grow, drawing from all parts of the world.
European higher education took place for hundreds of years in Christian cathedral schools or monastic schools (scholae monasticae), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century. The earliest universities were developed under the aegis of the Latin Church by papal bull as studia generalia and perhaps from cathedral schools. It is possible, however, that the development of cathedral schools into universities was quite rare, with the University of Paris being an exception. Later they were also founded by Kings (University of Naples Federico II, Charles University in Prague, Jagiellonian University in Kraków) or municipal administrations (University of Cologne, University of Erfurt). In the early medieval period, most new universities were founded from pre-existing schools, usually when these schools were deemed to have become primarily sites of higher education. Many historians state that universities and cathedral schools were a continuation of the interest in learning promoted by monasteries.
All over Europe rulers and city governments began to create universities to satisfy a European thirst for knowledge, and the belief that society would benefit from the scholarly expertise generated from these institutions. Princes and leaders of city governments perceived the potential benefits of having a scholarly expertise develop with the ability to address difficult problems and achieve desired ends. The emergence of humanism was essential to this understanding of the possible utility of universities as well as the revival of interest in knowledge gained from ancient Greek texts.
The rediscovery of Aristotle's works–more than 3000 pages of it would eventually be translated –fuelled a spirit of inquiry into natural processes that had already begun to emerge in the 12th century. Some scholars believe that these works represented one of the most important document discoveries in Western intellectual history. Richard Dales, for instance, calls the discovery of Aristotle's works "a turning point in the history of Western thought." After Aristotle re-emerged, a community of scholars, primarily communicating in Latin, accelerated the process and practice of attempting to reconcile the thoughts of Greek antiquity, and especially ideas related to understanding the natural world, with those of the church. The efforts of this "scholasticism" were focused on applying Aristotelian logic and thoughts about natural processes to biblical passages and attempting to prove the viability of those passages through reason. This became the primary mission of lecturers, and the expectation of students.
The university culture developed differently in northern Europe than it did in the south, although the northern (primarily Germany, France and Great Britain) and southern universities (primarily Italy) did have many elements in common. Latin was the language of the university, used for all texts, lectures, disputations and examinations. Professors lectured on the books of Aristotle for logic, natural philosophy, and metaphysics; while Hippocrates, Galen, and Avicenna were used for medicine. Outside of these commonalities, great differences separated north and south, primarily in subject matter. Italian universities focused on law and medicine, while the northern universities focused on the arts and theology. There were distinct differences in the quality of instruction in these areas which were congruent with their focus, so scholars would travel north or south based on their interests and means. There was also a difference in the types of degrees awarded at these universities. English, French and German universities usually awarded bachelor's degrees, with the exception of degrees in theology, for which the doctorate was more common. Italian universities awarded primarily doctorates. The distinction can be attributed to the intent of the degree holder after graduation – in the north the focus tended to be on acquiring teaching positions, while in the south students often went on to professional positions. The structure of northern universities tended to be modeled after the system of faculty governance developed at the University of Paris. Southern universities tended to be patterned after the student-controlled model begun at the University of Bologna. Among the southern universities, a further distinction has been noted between those of northern Italy, which followed the pattern of Bologna as a "self-regulating, independent corporation of scholars" and those of southern Italy and Iberia, which were "founded by royal and imperial charter to serve the needs of government."
Their endowment by a prince or monarch and their role in training government officials made these Mediterranean universities similar to Islamic madrasas, although madrasas were generally smaller and individual teachers, rather than the madrasa itself, granted the license or degree. Scholars like Arnold H. Green and Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madrasahs became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Other scholars regard the university as uniquely European in origin and characteristics.
Many scholars (including Makdisi) have argued that early medieval universities were influenced by the religious madrasahs in Al-Andalus, the Emirate of Sicily, and the Middle East (during the Crusades). Other scholars see this argument as overstated. Lowe and Yasuhara have recently drawn on the well-documented influences of scholarship from the Islamic world on the universities of Western Europe to call for a reconsideration of the development of higher education, turning away from a concern with local institutional structures to a broader consideration within a global context.
During the Early Modern period (approximately late 15th century to 1800), the universities of Europe would see a tremendous amount of growth, productivity and innovative research. At the end of the Middle Ages, about 400 years after the first university was founded, there were twenty-nine universities spread throughout Europe. In the 15th century, twenty-eight new ones were created, with another eighteen added between 1500 and 1625. This pace continued until by the end of the 18th century there were approximately 143 universities in Europe and Eastern Europe, with the highest concentrations in the German Empire (34), Italian countries (26), France (25), and Spain (23) – this was close to a 500% increase over the number of universities toward the end of the Middle Ages. This number does not include the numerous universities that disappeared, or institutions that merged with other universities during this time. It should be noted that the identification of a university was not necessarily obvious during the Early Modern period, as the term is applied to a burgeoning number of institutions. In fact, the term "university" was not always used to designate a higher education institution. In Mediterranean countries, the term studium generale was still often used, while "Academy" was common in Northern European countries.
The propagation of universities was not necessarily a steady progression, as the 17th century was rife with events that adversely affected university expansion. Many wars, and especially the Thirty Years' War, disrupted the university landscape throughout Europe at different times. War, plague, famine, regicide, and changes in religious power and structure often adversely affected the societies that provided support for universities. Internal strife within the universities themselves, such as student brawling and absentee professors, acted to destabilize these institutions as well. Universities were also reluctant to give up older curricula, and the continued reliance on the works of Aristotle defied contemporary advancements in science and the arts. This era was also affected by the rise of the nation-state. As universities increasingly came under state control, or formed under the auspices of the state, the faculty governance model (begun by the University of Paris) became more and more prominent. Although the older student-controlled universities still existed, they slowly started to move toward this structural organization. Control of universities still tended to be independent, although university leadership was increasingly appointed by the state.
Although the structural model provided by the University of Paris, where student members are controlled by faculty "masters," provided a standard for universities, the application of this model took at least three different forms. There were universities that had a system of faculties whose teaching addressed a very specific curriculum; this model tended to train specialists. There was a collegiate or tutorial model based on the system at University of Oxford where teaching and organization was decentralized and knowledge was more of a generalist nature. There were also universities that combined these models, using the collegiate model but having a centralized organization.
Early Modern universities initially continued the curriculum and research of the Middle Ages: natural philosophy, logic, medicine, theology, mathematics, astronomy (and astrology), law, grammar and rhetoric. Aristotle was prevalent throughout the curriculum, while medicine also depended on Galen and Arabic scholarship. The importance of humanism for changing this state-of-affairs cannot be underestimated. Once humanist professors joined the university faculty, they began to transform the study of grammar and rhetoric through the studia humanitatis. Humanist professors focused on the ability of students to write and speak with distinction, to translate and interpret classical texts, and to live honorable lives. Other scholars within the university were affected by the humanist approaches to learning and their linguistic expertise in relation to ancient texts, as well as the ideology that advocated the ultimate importance of those texts. Professors of medicine such as Niccolò Leoniceno, Thomas Linacre and William Cop were often trained in and taught from a humanist perspective as well as translated important ancient medical texts. The critical mindset imparted by humanism was imperative for changes in universities and scholarship. For instance, Andreas Vesalius was educated in a humanist fashion before producing a translation of Galen, whose ideas he verified through his own dissections. In law, Andreas Alciatus infused the Corpus Juris with a humanist perspective, while Jacques Cujas humanist writings were paramount to his reputation as a jurist. Philipp Melanchthon cited the works of Erasmus as a highly influential guide for connecting theology back to original texts, which was important for the reform at Protestant universities. Galileo Galilei, who taught at the Universities of Pisa and Padua, and Martin Luther, who taught at the University of Wittenberg (as did Melanchthon), also had humanist training. The task of the humanists was to slowly permeate the university; to increase the humanist presence in professorships and chairs, syllabi and textbooks so that published works would demonstrate the humanistic ideal of science and scholarship.
Although the initial focus of the humanist scholars in the university was the discovery, exposition and insertion of ancient texts and languages into the university, and the ideas of those texts into society generally, their influence was ultimately quite progressive. The emergence of classical texts brought new ideas and led to a more creative university climate (as the notable list of scholars above attests to). A focus on knowledge coming from self, from the human, has a direct implication for new forms of scholarship and instruction, and was the foundation for what is commonly known as the humanities. This disposition toward knowledge manifested in not simply the translation and propagation of ancient texts, but also their adaptation and expansion. For instance, Vesalius was imperative for advocating the use of Galen, but he also invigorated this text with experimentation, disagreements and further research. The propagation of these texts, especially within the universities, was greatly aided by the emergence of the printing press and the beginning of the use of the vernacular, which allowed for the printing of relatively large texts at reasonable prices.
There are several major exceptions on tuition fees. In many European countries, it is possible to study without tuition fees. Public universities in Nordic countries were entirely without tuition fees until around 2005. Denmark, Sweden and Finland then moved to put in place tuition fees for foreign students. Citizens of EU and EEA member states and citizens from Switzerland remain exempted from tuition fees, and the amounts of public grants granted to promising foreign students were increased to offset some of the impact.
Colloquially, the term university may be used to describe a phase in one's life: "When I was at university..." (in the United States and Ireland, college is often used instead: "When I was in college..."). In Australia, Canada, New Zealand, the United Kingdom, Nigeria, the Netherlands, Spain and the German-speaking countries university is often contracted to uni. In Ghana, New Zealand and in South Africa it is sometimes called "varsity" (although this has become uncommon in New Zealand in recent years). "Varsity" was also common usage in the UK in the 19th century.[citation needed] "Varsity" is still in common usage in Scotland.
In Canada, "college" generally refers to a two-year, non-degree-granting institution, while "university" connotes a four-year, degree-granting institution. Universities may be sub-classified (as in the Macleans rankings) into large research universities with many PhD granting programs and medical schools (for example, McGill University); "comprehensive" universities that have some PhDs but aren't geared toward research (such as Waterloo); and smaller, primarily undergraduate universities (such as St. Francis Xavier).
Although each institution is organized differently, nearly all universities have a board of trustees; a president, chancellor, or rector; at least one vice president, vice-chancellor, or vice-rector; and deans of various divisions. Universities are generally divided into a number of academic departments, schools or faculties. Public university systems are ruled over by government-run higher education boards. They review financial requests and budget proposals and then allocate funds for each university in the system. They also approve new programs of instruction and cancel or make changes in existing programs. In addition, they plan for the further coordinated growth and development of the various institutions of higher education in the state or country. However, many public universities in the world have a considerable degree of financial, research and pedagogical autonomy. Private universities are privately funded and generally have broader independence from state policies. However, they may have less independence from business corporations depending on the source of their finances.
The funding and organization of universities varies widely between different countries around the world. In some countries universities are predominantly funded by the state, while in others funding may come from donors or from fees which students attending the university must pay. In some countries the vast majority of students attend university in their local town, while in other countries universities attract students from all over the world, and may provide university accommodation for their students.
Universities created by bilateral or multilateral treaties between states are intergovernmental. An example is the Academy of European Law, which offers training in European law to lawyers, judges, barristers, solicitors, in-house counsel and academics. EUCLID (Pôle Universitaire Euclide, Euclid University) is chartered as a university and umbrella organisation dedicated to sustainable development in signatory countries, and the United Nations University engages in efforts to resolve the pressing global problems that are of concern to the United Nations, its peoples and member states. The European University Institute, a post-graduate university specialised in the social sciences, is officially an intergovernmental organisation, set up by the member states of the European Union.
A national university is generally a university created or run by a national state but at the same time represents a state autonomic institution which functions as a completely independent body inside of the same state. Some national universities are closely associated with national cultural or political aspirations, for instance the National University of Ireland in the early days of Irish independence collected a large amount of information on the Irish language and Irish culture. Reforms in Argentina were the result of the University Revolution of 1918 and its posterior reforms by incorporating values that sought for a more equal and laic higher education system.
In 1963, the Robbins Report on universities in the United Kingdom concluded that such institutions should have four main "objectives essential to any properly balanced system: instruction in skills; the promotion of the general powers of the mind so as to produce not mere specialists but rather cultivated men and women; to maintain research in balance with teaching, since teaching should not be separated from the advancement of learning and the search for truth; and to transmit a common culture and common standards of citizenship."
Until the 19th century, religion played a significant role in university curriculum; however, the role of religion in research universities decreased in the 19th century, and by the end of the 19th century, the German university model had spread around the world. Universities concentrated on science in the 19th and 20th centuries and became increasingly accessible to the masses. In Britain, the move from Industrial Revolution to modernity saw the arrival of new civic universities with an emphasis on science and engineering, a movement initiated in 1960 by Sir Keith Murray (chairman of the University Grants Committee) and Sir Samuel Curran, with the formation of the University of Strathclyde. The British also established universities worldwide, and higher education became available to the masses not only in Europe.
By the end of the early modern period, the structure and orientation of higher education had changed in ways that are eminently recognizable for the modern context. Aristotle was no longer a force providing the epistemological and methodological focus for universities and a more mechanistic orientation was emerging. The hierarchical place of theological knowledge had for the most part been displaced and the humanities had become a fixture, and a new openness was beginning to take hold in the construction and dissemination of knowledge that were to become imperative for the formation of the modern state.
The epistemological tensions between scientists and universities were also heightened by the economic realities of research during this time, as individual scientists, associations and universities were vying for limited resources. There was also competition from the formation of new colleges funded by private benefactors and designed to provide free education to the public, or established by local governments to provide a knowledge hungry populace with an alternative to traditional universities. Even when universities supported new scientific endeavors, and the university provided foundational training and authority for the research and conclusions, they could not compete with the resources available through private benefactors.
Other historians find incongruity in the proposition that the very place where the vast number of the scholars that influenced the scientific revolution received their education should also be the place that inhibits their research and the advancement of science. In fact, more than 80% of the European scientists between 1450–1650 included in the Dictionary of Scientific Biography were university trained, of which approximately 45% held university posts. It was the case that the academic foundations remaining from the Middle Ages were stable, and they did provide for an environment that fostered considerable growth and development. There was considerable reluctance on the part of universities to relinquish the symmetry and comprehensiveness provided by the Aristotelian system, which was effective as a coherent system for understanding and interpreting the world. However, university professors still utilized some autonomy, at least in the sciences, to choose epistemological foundations and methods. For instance, Melanchthon and his disciples at University of Wittenberg were instrumental for integrating Copernican mathematical constructs into astronomical debate and instruction. Another example was the short-lived but fairly rapid adoption of Cartesian epistemology and methodology in European universities, and the debates surrounding that adoption, which led to more mechanistic approaches to scientific problems as well as demonstrated an openness to change. There are many examples which belie the commonly perceived intransigence of universities. Although universities may have been slow to accept new sciences and methodologies as they emerged, when they did accept new ideas it helped to convey legitimacy and respectability, and supported the scientific changes through providing a stable environment for instruction and material resources.
Regardless of the way the tension between universities, individual scientists, and the scientific revolution itself is perceived, there was a discernible impact on the way that university education was constructed. Aristotelian epistemology provided a coherent framework not simply for knowledge and knowledge construction, but also for the training of scholars within the higher education setting. The creation of new scientific constructs during the scientific revolution, and the epistemological challenges that were inherent within this creation, initiated the idea of both the autonomy of science and the hierarchy of the disciplines. Instead of entering higher education to become a "general scholar" immersed in becoming proficient in the entire curriculum, there emerged a type of scholar that put science first and viewed it as a vocation in itself. The divergence between those focused on science and those still entrenched in the idea of a general scholar exacerbated the epistemological tensions that were already beginning to emerge.
Examining the influence of humanism on scholars in medicine, mathematics, astronomy and physics may suggest that humanism and universities were a strong impetus for the scientific revolution. Although the connection between humanism and the scientific discovery may very well have begun within the confines of the university, the connection has been commonly perceived as having been severed by the changing nature of science during the scientific revolution. Historians such as Richard S. Westfall have argued that the overt traditionalism of universities inhibited attempts to re-conceptualize nature and knowledge and caused an indelible tension between universities and scientists. This resistance to changes in science may have been a significant factor in driving many scientists away from the university and toward private benefactors, usually in princely courts, and associations with newly forming scientific societies.
In 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692, and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Brunswick-Lüneburg, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its electors would later become monarchs of Great Britain (and from 1801, of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who ruled in Hanover was William IV. Semi-Salic law, which required succession by the male line if possible, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were concurrently also Electoral Princes of Hanover.
As an important railroad and road junction and production center, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory M.N.H. Maschinenfabrik Niedersachsen (Badenstedt). Forced labourers were sometimes used from the Hannover-Misburg subcamp of the Neuengamme concentration camp. Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city center was destroyed in a total of 88 bombing raids. After the war, the Aegidienkirche was not rebuilt and its ruins were left as a war memorial.
The Hanover Zoo is one of the most spectacular and best zoos in Europe. The zoo received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany. The zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In 2010 the Hanover Zoo had over 1.6 million visitors.
Hanover's leading cabaret-stage is the GOP Variety theatre which is located in the Georgs Palace. Some other famous cabaret-stages are the Variety Marlene, the Uhu-Theatre. the theatre Die Hinterbühne, the Rampenlich Variety and the revue-stage TAK. The most important Cabaret-Event is the Kleines Fest im Großen Garten (Little Festival in the Great Garden) which is the most successful Cabaret Festival in Germany. It features artists from around the world. Some other important events are the Calenberger Cabaret Weeks, the Hanover Cabaret Festival and the Wintervariety.
"Hanover" is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and the local government uses the German spelling on English websites. The English pronunciation /ˈhænəvər/, with stress on the first syllable and a reduced second syllable, is applied to both the German and English spellings, which is different from German pronunciation [haˈnoːfɐ], with stress on the second syllable and a long second vowel. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover.
After 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). A large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family. However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Gryszpan's son Herschel Grynszpan was in Paris at the time. When he learned of what was happening, he drove to the German embassy in Paris and shot the German diplomat Eduard Ernst vom Rath, who died shortly afterwards.
The Great Garden is an important European baroque garden. The palace itself, however, was largely destroyed by Allied bombing but is currently under reconstruction.[citation needed] Some points of interest are the Grotto (the interior was designed by the French artist Niki de Saint-Phalle), the Gallery Building, the Orangerie and the two pavilions by Remy de la Fosse. The Great Garden consists of several parts. The most popular ones are the Great Ground and the Nouveau Jardin. At the centre of the Nouveau Jardin is Europe's highest garden fountain. The historic Garden Theatre inter alia hosted the musicals of the German rock musician Heinz Rudolf Kunze.[citation needed]
Some other popular sights are the Waterloo Column, the Laves House, the Wangenheim Palace, the Lower Saxony State Archives, the Hanover Playhouse, the Kröpcke Clock, the Anzeiger Tower Block, the Administration Building of the NORD/LB, the Cupola Hall of the Congress Centre, the Lower Saxony Stock, the Ministry of Finance, the Garten Church, the Luther Church, the Gehry Tower (designed by the American architect Frank O. Gehry), the specially designed Bus Stops, the Opera House, the Central Station, the Maschsee lake and the city forest Eilenriede, which is one of the largest of its kind in Europe. With around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities.
But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen. The Schützenfest Hannover is the largest Marksmen's Fun Fair in the world and takes place once a year (late June to early July) (2014 - July 4th to the 13th). It consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the 12 kilometres (7 mi) long Parade of the Marksmen with more than 12.000 participants from all over the world, among them around 5.000 marksmen, 128 bands and more than 70 wagons, carriages and big festival vehicles. It is the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this Fun Fair is the biggest transportable Ferris Wheel in the world (60 m or 197 ft high). The origins of this fun fair is located in the year 1529.
The Schnellweg (en: expressway) system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at Seelhorster Kreuz, then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing Westschnellweg, then becomes B65 again at Seelhorster Kreuz).
In 1837, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). Hanover could be inherited only by male heirs. Thus, Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government.
In September 1941, through the "Action Lauterbacher" plan, a ghettoisation of the remaining Hanoverian Jewish families began. Even before the Wannsee Conference, on 15 December 1941, the first Jews from Hanover were deported to Riga. A total of 2,400 people were deported, and very few survived. During the war seven concentration camps were constructed in Hanover, in which many Jews were confined. Of the approximately 4,800 Jews who had lived in Hannover in 1938, fewer than 100 were still in the city when troops of the United States Army arrived on 10 April 1945 to occupy Hanover at the end of the war.[citation needed] Today, a memorial at the Opera Square is a reminder of the persecution of the Jews in Hanover. After the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover.
Hanover was founded in medieval times on the east bank of the River Leine. Its original name Honovere may mean "high (river)bank", though this is debated (cf. das Hohe Ufer). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century due to its position at a natural crossroads. As overland travel was relatively difficult, its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine, and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.
After Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 30,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was the only German army to fight against France throughout the entire Napoleonic wars. The Legion later played an important role in the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably.
The Berggarten is an important European botanical garden.[citation needed] Some points of interest are the Tropical House, the Cactus House, the Canary House and the Orchid House, which hosts one of the world's biggest collection of orchids, and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic Library Pavillon. The Mausoleum of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the Paradies and the Prairie Garden. There is also the Sea Life Centre Hanover, which is the first tropical aquarium in Germany.[citation needed]
Various industrial businesses are located in Hannover. The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a huge plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Jointly with a factory of German tire and automobile parts manufacturer Continental AG, they have a coal-burning power plant. Continental AG, founded in Hanover in 1871, is one of the city's major companies, as is Sennheiser. Since 2008 a take-over is in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of the stock but were required due to the financial crisis to deposit the options as securities at banks. TUI AG has its HQ in Hanover. Hanover is home to many insurance companies, many of which operate only in Germany. One major global reinsurance company is Hannover Re, whose headquarters are east of the city centre.
Around 40 theatres are located in Hanover. The Opera House, the Schauspielhaus (Play House), the Ballhofeins, the Ballhofzwei and the Cumberlandsche Galerie belong to the Lower Saxony State Theatre. The Theater am Aegi is Hanover's big theatre for musicals, shows and guest performances. The Neues Theater (New Theatre) is the Boulevard Theatre of Hanover. The Theater für Niedersachsen is another big theatre in Hanover, which also has an own Musical-Company. Some of the most important Musical-Productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the Garden-Theatre in the Great Garden.
Hannover 96 (nickname Die Roten or 'The Reds') is the top local football team that plays in the Bundesliga top division. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team Hannover 96 II plays in the fourth league. Their home games were played in the traditional Eilenriedestadium till they moved to the HDI Arena due to DFL directives. Arminia Hannover is another very traditional soccer team in Hanover that has played in the first league for years and plays now in the Niedersachsen-West Liga (Lower Saxony League West). Home matches are played in the Rudolf-Kalweit-Stadium.
With a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city. Hanover also hosts annual commercial trade fairs such as the Hanover Fair and the CeBIT. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest Oktoberfest in the world (beside Oktoberfest of Blumenau). In 2000, Hanover hosted the world fair Expo 2000. The Hanover fairground, due to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover is of national importance because of its universities and medical school, its international airport and its large zoo. The city is also a major crossing point of railway lines and highways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area) and north-south (Hamburg–Munich, etc.) directions.
Another point of interest is the Old Town. In the centre are the large Marktkirche (Church St. Georgii et Jacobi, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the Old Town Hall. Nearby are the Leibniz House, the Nolte House, and the Beguine Tower. A very nice quarter of the Old Town is the Kreuz-Church-Quarter around the Kreuz Church with many nice little lanes. Nearby is the old royal sports hall, now called the Ballhof theatre. On the edge of the Old Town are the Market Hall, the Leine Palace, and the ruin of the Aegidien Church which is now a monument to the victims of war and violence. Through the Marstall Gate you arrive at the bank of the river Leine, where the world-famous Nanas of Niki de Saint-Phalle are located. They are part of the Mile of Sculptures, which starts from Trammplatz, leads along the river bank, crosses Königsworther Square, and ends at the entrance of the Georgengarten. Near the Old Town is the district of Calenberger Neustadt where the Catholic Basilica Minor of St. Clemens, the Reformed Church and the Lutheran Neustädter Hof- und Stadtkirche St. Johannis are located.
A cabinet of coins is the Münzkabinett der TUI-AG. The Polizeigeschichtliche Sammlung Niedersachsen is the largest police museum in Germany. Textiles from all over the world can be visited in the Museum for textile art. The EXPOseeum is the museum of the world-exhibition "EXPO 2000 Hannover". Carpets and objects from the orient can be visited in the Oriental Carpet Museum. The Blind Man Museum is a rarity in Germany, another one is only in Berlin. The Museum of veterinary medicine is unique in Germany. The Museum for Energy History describes the 150 years old history of the application of energy. The Home Museum Ahlem shows the history of the district of Ahlem. The Mahn- und Gedenkstätte Ahlem describes the history of the Jewish people in Hanover and the Stiftung Ahlers Pro Arte / Kestner Pro Arte shows modern art. Modern art is also the main topic of the Kunsthalle Faust, the Nord/LB Art Gellery and of the Foro Artistico / Eisfabrik.
Lighting or illumination is the deliberate use of light to achieve a practical or aesthetic effect. Lighting includes the use of both artificial light sources like lamps and light fixtures, as well as natural illumination by capturing daylight. Daylighting (using windows, skylights, or light shelves) is sometimes used as the main source of light during daytime in buildings. This can save energy in place of using artificial lighting, which represents a major component of energy consumption in buildings. Proper lighting can enhance task performance, improve the appearance of an area, or have positive psychological effects on occupants.
Indoor lighting is usually accomplished using light fixtures, and is a key part of interior design. Lighting can also be an intrinsic component of landscape projects.
Forms of lighting include alcove lighting, which like most other uplighting is indirect. This is often done with fluorescent lighting (first available at the 1939 World's Fair) or rope light, occasionally with neon lighting, and recently with LED strip lighting. It is a form of backlighting.
Recessed lighting (often called "pot lights" in Canada, "can lights" or 'high hats" in the US) is popular, with fixtures mounted into the ceiling structure so as to appear flush with it. These downlights can use narrow beam spotlights, or wider-angle floodlights, both of which are bulbs having their own reflectors. There are also downlights with internal reflectors designed to accept common 'A' lamps (light bulbs) which are generally less costly than reflector lamps. Downlights can be incandescent, fluorescent, HID (high intensity discharge) or LED.
With the discovery of fire, the earliest form of artificial lighting used to illuminate an area were campfires or torches. As early as 400,000 BCE, fire was kindled in the caves of Peking Man. Prehistoric people used primitive oil lamps to illuminate surroundings. These lamps were made from naturally occurring materials such as rocks, shells, horns and stones, were filled with grease, and had a fiber wick. Lamps typically used animal or vegetable fats as fuel. Hundreds of these lamps (hollow worked stones) have been found in the Lascaux caves in modern-day France, dating to about 15,000 years ago. Oily animals (birds and fish) were also used as lamps after being threaded with a wick. Fireflies have been used as lighting sources. Candles and glass and pottery lamps were also invented. Chandeliers were an early form of "light fixture".
Major reductions in the cost of lighting occurred with the discovery of whale oil and kerosene. Gas lighting was economical enough to power street lights in major cities starting in the early 1800s, and was also used in some commercial buildings and in the homes of wealthy people. The gas mantle boosted the luminosity of utility lighting and of kerosene lanterns. The next major drop in price came about with the incandescent light bulb powered by electricity.
Over time, electric lighting became ubiquitous in developed countries. Segmented sleep patterns disappeared, improved nighttime lighting made more activities possible at night, and more street lights reduced urban crime.
Lighting fixtures come in a wide variety of styles for various functions. The most important functions are as a holder for the light source, to provide directed light and to avoid visual glare. Some are very plain and functional, while some are pieces of art in themselves. Nearly any material can be used, so long as it can tolerate the excess heat and is in keeping with safety codes.
An important property of light fixtures is the luminous efficacy or wall-plug efficiency, meaning the amount of usable light emanating from the fixture per used energy, usually measured in lumen per watt. A fixture using replaceable light sources can also have its efficiency quoted as the percentage of light passed from the "bulb" to the surroundings. The more transparent the lighting fixture is, the higher efficacy. Shading the light will normally decrease efficacy but increase the directionality and the visual comfort probability.
Color temperature for white light sources also affects their use for certain applications. The color temperature of a white light source is the temperature in Kelvin of a theoretical black body emitter that most closely matches the spectral characteristics of the lamp. An incandescent bulb has a color temperature around 2800 to 3000 Kelvin; daylight is around 6400 Kelvin. Lower color temperature lamps have relatively more energy in the yellow and red part of the visible spectrum, while high color temperatures correspond to lamps with more of a blue-white appearance. For critical inspection or color matching tasks, or for retail displays of food and clothing, the color temperature of the lamps will be selected for the best overall lighting effect.
Lighting is classified by intended use as general, accent, or task lighting, depending largely on the distribution of the light produced by the fixture.
Track lighting, invented by Lightolier, was popular at one period of time because it was much easier to install than recessed lighting, and individual fixtures are decorative and can be easily aimed at a wall. It has regained some popularity recently in low-voltage tracks, which often look nothing like their predecessors because they do not have the safety issues that line-voltage systems have, and are therefore less bulky and more ornamental in themselves. A master transformer feeds all of the fixtures on the track or rod with 12 or 24 volts, instead of each light fixture having its own line-to-low voltage transformer. There are traditional spots and floods, as well as other small hanging fixtures. A modified version of this is cable lighting, where lights are hung from or clipped to bare metal cables under tension.
A sconce is a wall-mounted fixture, particularly one that shines up and sometimes down as well. A torchiere is an uplight intended for ambient lighting. It is typically a floor lamp but may be wall-mounted like a sconce.
The portable or table lamp is probably the most common fixture, found in many homes and offices. The standard lamp and shade that sits on a table is general lighting, while the desk lamp is considered task lighting. Magnifier lamps are also task lighting.
The illuminated ceiling was once popular in the 1960s and 1970s but fell out of favor after the 1980s. This uses diffuser panels hung like a suspended ceiling below fluorescent lights, and is considered general lighting. Other forms include neon, which is not usually intended to illuminate anything else, but to actually be an artwork in itself. This would probably fall under accent lighting, though in a dark nightclub it could be considered general lighting.
In a movie theater, steps in the aisles are usually marked with a row of small lights for convenience and safety, when the film has started and the other lights are off. Traditionally made up of small low wattage, low voltage lamps in a track or translucent tube, these are rapidly being replaced with LED based versions.
Street Lights are used to light roadways and walkways at night. Some manufacturers are designing LED and photovoltaic luminaires to provide an energy-efficient alternative to traditional street light fixtures.
Floodlights can be used to illuminate outdoor playing fields or work zones during nighttime hours. The most common type of floodlights are metal halide and high pressure sodium lights.
Sometimes security lighting can be used along roadways in urban areas, or behind homes or commercial facilities. These are extremely bright lights used to deter crime. Security lights may include floodlights.
Entry lights can be used outside to illuminate and signal the entrance to a property. These lights are installed for safety, security, and for decoration.
Vehicles typically include headlamps and tail lights. Headlamps are white or selective yellow lights placed in the front of the vehicle, designed to illuminate the upcoming road and to make the vehicle more visible. Many manufactures are turning to LED headlights as an energy-efficient alternative to traditional headlamps. Tail and brake lights are red and emit light to the rear so as to reveal the vehicle's direction of travel to following drivers. White rear-facing reversing lamps indicate that the vehicle's transmission has been placed in the reverse gear, warning anyone behind the vehicle that it is moving backwards, or about to do so. Flashing turn signals on the front, side, and rear of the vehicle indicate an intended change of position or direction. In the late 1950s, some automakers began to use electroluminescent technology to backlight their cars' speedometers and other gauges or to draw attention to logos or other decorative elements.
Commonly called 'light bulbs', lamps are the removable and replaceable part of a light fixture, which converts electrical energy into electromagnetic radiation. While lamps have traditionally been rated and marketed primarily in terms of their power consumption, expressed in watts, proliferation of lighting technology beyond the incandescent light bulb has eliminated the correspondence of wattage to the amount of light produced. For example, a 60 W incandescent light bulb produces about the same amount of light as a 13 W compact fluorescent lamp. Each of these technologies has a different efficacy in converting electrical energy to visible light. Visible light output is typically measured in lumens. This unit only quantifies the visible radiation, and excludes invisible infrared and ultraviolet light. A wax candle produces on the close order of 13 lumens, a 60 watt incandescent lamp makes around 700 lumens, and a 15-watt compact fluorescent lamp produces about 800 lumens, but actual output varies by specific design. Rating and marketing emphasis is shifting away from wattage and towards lumen output, to give the purchaser a directly applicable basis upon which to select a lamp.
Lighting design as it applies to the built environment is known as 'architectural lighting design'. Lighting of structures considers aesthetic elements as well as practical considerations of quantity of light required, occupants of the structure, energy efficiency and cost. Artificial lighting takes into account the amount of daylight received in an internal space by using Daylight factor calculation. For simple installations, hand-calculations based on tabular data are used to provide an acceptable lighting design. More critical or optimized designs now routinely use mathematical modeling on a computer using software such as Radiance which can allow an Architect to quickly undertake complex calculations to review the benefit of a particular design.
In some design instances, materials used on walls and furniture play a key role in the lighting effect< for example dark paint tends to absorb light, making the room appear smaller and more dim than it is, whereas light paint does the opposite. In addition to paint, reflective surfaces also have an effect on lighting design.
Photometric studies (also sometimes referred to as "layouts" or "point by points") are often used to simulate lighting designs for projects before they are built or renovated. This enables architects, lighting designers, and engineers to determine whether a proposed lighting setup will deliver the amount of light intended. They will also be able to determine the contrast ratio between light and dark areas. In many cases these studies are referenced against IESNA or CIBSE recommended lighting practices for the type of application. Depending on the type of area, different design aspects may be emphasized for safety or practicality (i.e. such as maintaining uniform light levels, avoiding glare or highlighting certain areas). Specialized software is often used to create these, which typically combine the use of two-dimensional digital CAD drawings and lighting calculation software (i.e. AGi32or Dialux).
Lighting illuminates the performers and artists in a live theatre, dance, or musical performance, and is selected and arranged to create dramatic effects. Stage lighting uses general illumination technology in devices configured for easy adjustment of their output characteristics.[citation needed] The setup of stage lighting is tailored for each scene of each production. Dimmers, colored filters, reflectors, lenses, motorized or manually aimed lamps, and different kinds of flood and spot lights are among the tools used by a stage lighting designer to produce the desired effects. A set of lighting cues are prepared so that the lighting operator can control the lights in step with the performance; complex theatre lighting systems use computer control of lighting instruments.
Motion picture and television production use many of the same tools and methods of stage lighting. Especially in the early days of these industries, very high light levels were required and heat produced by lighting equipment presented substantial challenges. Modern cameras require less light, and modern light sources emit less heat.
Measurement of light or photometry is generally concerned with the amount of useful light falling on a surface and the amount of light emerging from a lamp or other source, along with the colors that can be rendered by this light. The human eye responds differently to light from different parts of the visible spectrum, therefore photometric measurements must take the luminosity function into account when measuring the amount of useful light. The basic SI unit of measurement is the candela (cd), which describes the luminous intensity, all other photometric units are derived from the candela. Luminance for instance is a measure of the density of luminous intensity in a given direction. It describes the amount of light that passes through or is emitted from a particular area, and falls within a given solid angle. The SI unit for luminance is candela per square metre (cd/m2). The CGS unit of luminance is the stilb, which is equal to one candela per square centimetre or 10 kcd/m2. The amount of useful light emitted from a source or the luminous flux is measured in lumen (lm).
The SI unit of illuminance and luminous emittance, being the luminous power per area, is measured in Lux. It is used in photometry as a measure of the intensity, as perceived by the human eye, of light that hits or passes through a surface. It is analogous to the radiometric unit watts per square metre, but with the power at each wavelength weighted according to the luminosity function, a standardized model of human visual brightness perception. In English, "lux" is used in both singular and plural.
Several measurement methods have been developed to control glare resulting from indoor lighting design. The Unified Glare Rating (UGR), the Visual Comfort Probability, and the Daylight Glare Index are some of the most well-known methods of measurement. In addition to these new methods, four main factors influence the degree of discomfort glare; the luminance of the glare source, the solid angle of the glare source, the background luminance, and the position of the glare source in the field of view must all be taken into account.
To define light source color properties, the lighting industry predominantly relies on two metrics, correlated color temperature (CCT), commonly used as an indication of the apparent "warmth" or "coolness" of the light emitted by a source, and color rendering index (CRI), an indication of the light source’s ability to make objects appear natural.
For example, in order to meet the expectations for good color rendering in retail applications, research suggests using the well-established CRI along with another metric called gamut area index (GAI). GAI represents the relative separation of object colors illuminated by a light source; the greater the GAI, the greater the apparent saturation or vividness of the object colors. As a result, light sources which balance both CRI and GAI are generally preferred over ones that have only high CRI or only high GAI.
Typical measurements of light have used a Dosimeter. Dosimeters measure an individual's or an object's exposure to something in the environment, such as light dosimeters and ultraviolet dosimeters.
In order to specifically measure the amount of light entering the eye, personal circadian light meter called the Daysimeter has been developed. This is the first device created to accurately measure and characterize light (intensity, spectrum, timing, and duration) entering the eye that affects the human body's clock.
The small, head-mounted device measures an individual's daily rest and activity patterns, as well as exposure to short-wavelength light that stimulates the circadian system. The device measures activity and light together at regular time intervals and electronically stores and logs its operating temperature. The Daysimeter can gather data for up to 30 days for analysis.
Specification of illumination requirements is the basic concept of deciding how much illumination is required for a given task. Clearly, much less light is required to illuminate a hallway compared to that needed for a word processing work station. Generally speaking, the energy expended is proportional to the design illumination level. For example, a lighting level of 400 lux might be chosen for a work environment involving meeting rooms and conferences, whereas a level of 80 lux could be selected for building hallways. If the hallway standard simply emulates the conference room needs, then much more energy will be consumed than is needed. Unfortunately, most of the lighting standards even today have been specified by industrial groups who manufacture and sell lighting, so that a historical commercial bias exists in designing most building lighting, especially for office and industrial settings.
Lighting control systems reduce energy usage and cost by helping to provide light only when and where it is needed. Lighting control systems typically incorporate the use of time schedules, occupancy control, and photocell control (i.e.daylight harvesting). Some systems also support demand response and will automatically dim or turn off lights to take advantage of utility incentives. Lighting control systems are sometimes incorporated into larger building automation systems.
Many newer control systems are using wireless mesh open standards (such as ZigBee), which provides benefits including easier installation (no need to run control wires) and interoperability with other standards-based building control systems (e.g. security).
Occupancy sensors to allow operation for whenever someone is within the area being scanned can control lighting. When motion can no longer be detected, the lights shut off. Passive infrared sensors react to changes in heat, such as the pattern created by a moving person. The control must have an unobstructed view of the building area being scanned. Doors, partitions, stairways, etc. will block motion detection and reduce its effectiveness. The best applications for passive infrared occupancy sensors are open spaces with a clear view of the area being scanned. Ultrasonic sensors transmit sound above the range of human hearing and monitor the time it takes for the sound waves to return. A break in the pattern caused by any motion in the area triggers the control. Ultrasonic sensors can see around obstructions and are best for areas with cabinets and shelving, restrooms, and open areas requiring 360-degree coverage. Some occupancy sensors utilize both passive infrared and ultrasonic technology, but are usually more expensive. They can be used to control one lamp, one fixture or many fixtures.
Daylighting is the oldest method of interior lighting. Daylighting is simply designing a space to use as much natural light as possible. This decreases energy consumption and costs, and requires less heating and cooling from the building. Daylighting has also been proven to have positive effects on patients in hospitals as well as work and school performance. Due to a lack of information that indicate the likely energy savings, daylighting schemes are not yet popular among most buildings.
In recent years light emitting diodes (LEDs) are becoming increasingly efficient leading to an extraordinary increase in the use of solid state lighting. In many situations, controlling the light emission of LEDs may be done most effectively by using the principles of nonimaging optics.
Beyond the energy factors being considered, it is important not to over-design illumination, lest adverse health effects such as headache frequency, stress, and increased blood pressure be induced by the higher lighting levels. In addition, glare or excess light can decrease worker efficiency.
Analysis of lighting quality particularly emphasizes use of natural lighting, but also considers spectral content if artificial light is to be used. Not only will greater reliance on natural light reduce energy consumption, but will favorably impact human health and performance. New studies have shown that the performance of students is influenced by the time and duration of daylight in their regular schedules. Designing school facilities to incorporate the right types of light at the right time of day for the right duration may improve student performance and well-being. Similarly, designing lighting systems that maximize the right amount of light at the appropriate time of day for the elderly may help relieve symptoms of Alzheimer's Disease. The human circadian system is entrained to a 24-hour light-dark pattern that mimics the earth’s natural light/dark pattern. When those patterns are disrupted, they disrupt the natural circadian cycle. Circadian disruption may lead to numerous health problems including breast cancer, seasonal affective disorder, delayed sleep phase syndrome, and other ailments.
A study conducted in 1972 and 1981, documented by Robert Ulrich, surveyed 23 surgical patients assigned to rooms looking out on a natural scene. The study concluded that patients assigned to rooms with windows allowing lots of natural light had shorter postoperative hospital stays, received fewer negative evaluative comments in nurses’ notes, and took fewer potent analegesics than 23 matched patients in similar rooms with windows facing a brick wall. This study suggests that due to the nature of the scenery and daylight exposure was indeed healthier for patients as opposed to those exposed to little light from the brick wall. In addition to increased work performance, proper usage of windows and daylighting crosses the boundaries between pure aesthetics and overall health.
Alison Jing Xu, assistant professor of management at the University of Toronto Scarborough and Aparna Labroo of Northwestern University conducted a series of studies analyzing the correlation between lighting and human emotion. The researchers asked participants to rate a number of things such as: the spiciness of chicken-wing sauce, the aggressiveness of a fictional character, how attractive someone was, their feelings about specific words, and the taste of two juices–all under different lighting conditions. In their study, they found that both positive and negative human emotions are felt more intensely in bright light. Professor Xu stated, "we found that on sunny days depression-prone people actually become more depressed." They also found that dim light makes people make more rational decisions and settle negotiations easier. In the dark, emotions are slightly suppressed. However, emotions are intensified in the bright light.
In 1849, Dr. Abraham Gesner, a Canadian geologist, devised a method where kerosene could be distilled from petroleum. Earlier coal-gas methods had been used for lighting since the 1820s, but they were expensive. Gesner's kerosene was cheap, easy to produce, could be burned in existing lamps, and did not produce an offensive odor as did most whale oil. It could be stored indefinitely, unlike whale oil, which would eventually spoil. The American petroleum boom began in the 1850s. By the end of the decade there were 30 kerosene plants operating in the United States. The cheaper, more efficient fuel began to drive whale oil out of the market. John D. Rockefeller was most responsible for the commercial success of kerosene. He set up a network of kerosene distilleries which would later become Standard Oil, thus completely abolishing the need for whale-oil lamps. These types of lamps may catch fire or emit carbon-monoxide and sometimes are odorous making them problematic for asthmatic people.
Compact fluorescent lamps (aka 'CFLs') use less power to supply the same amount of light as an incandescent lamp, however they contain mercury which is a dispose hazard. Due to the ability to reduce electric consumption, many organizations have undertaken measures to encourage the adoption of CFLs. Some electric utilities and local governments have subsidized CFLs or provided them free to customers as a means of reducing electric demand. For a given light output, CFLs use between one fifth and one quarter of the power of an equivalent incandescent lamp. One of the simplest and quickest ways for a household or business to become more energy efficient is to adopt CFLs as the main lamp source, as suggested by the Alliance for Climate Protection. Unlike incandescent lamps CFL's need a little time to 'warm up' and reach full brightness. Care should be taken when selecting CFL's because not all of them are suitable for dimming.
LED lamps have been advocated as the newest and best environmental lighting method. According to the Energy Saving Trust, LED lamps use only 10% power compared to a standard incandescent bulb, where compact fluorescent lamps use 20% and energy saving halogen lamps 70%. The lifetime is also much longer — up to 50,000 hours. A downside is still the initial cost, which is higher than that of compact fluorescent lamps.
Light pollution is a growing problem in reaction to excess light being given off by numerous signs, houses, and buildings. Polluting light is often wasted light involving unnecessary energy costs and carbon dioxide emissions. Light pollution is described as artificial light that is excessive or intrudes where it is not wanted. Well-designed lighting sends light only where it is needed without scattering it elsewhere. Poorly designed lighting can also compromise safety. For example, glare creates safety issues around buildings by causing very sharp shadows, temporarily blinding passersby making them vulnerable to would-be assailants.
From a military standpoint, lighting is a critical part of the battlefield conditions. Shadows are good places to hide, while bright areas are more exposed. It is often beneficial to fight with the Sun or other light source behind you, giving your enemy disturbing visual glare and partially hiding your own movements in backlight. If natural light is not present searchlights and flares can be used. However the use of light may disclose your own hidden position and modern warfare have seen increased use of night vision through the use of infrared cameras and image intensifiers.
Flares can also be used by the military to mark positions, usually for targeting, but laser-guided and GPS weapons have eliminated this need for the most part.
The International Commission on Illumination (CIE) is an international authority and standard defining organization on color and lighting. Publishing widely used standard metrics such as various CIE color spaces and the color rendering index.
The Illuminating Engineering Society of North America (IESNA), in conjunction with organizations like ANSI and ASHRAE, publishes guidelines, standards, and handbooks that allow categorization of the illumination needs of different built environments. Manufacturers of lighting equipment publish photometric data for their products, which defines the distribution of light released by a specific luminaire. This data is typically expressed in standardized form defined by the IESNA.
The International Association of Lighting Designers (IALD) is an organization which focuses on the advancement of lighting design education and the recognition of independent professional lighting designers. Those fully independent designers who meet the requirements for professional membership in the association typically append the abbreviation IALD to their name.
The Professional Lighting Designers Association (PLDA), formerly known as ELDA is an organisation focusing on the promotion of the profession of Architectural Lighting Design. They publish a monthly newsletter and organise different events throughout the world.
The National Council on Qualifications for the Lighting Professions (NCQLP) offers the Lighting Certification Examination which tests rudimentary lighting design principles. Individuals who pass this exam become ‘Lighting Certified’ and may append the abbreviation LC to their name. This certification process is one of three national (U.S.) examinations (the others are CLEP and CLMC) in the lighting industry and is open not only to designers, but to lighting equipment manufacturers, electric utility employees, etc.
The Professional Lighting And Sound Association (PLASA) is a UK-based trade organisation representing the 500+ individual and corporate members drawn from the technical services sector. Its members include manufacturers and distributors of stage and entertainment lighting, sound, rigging and similar products and services, and affiliated professionals in the area. They lobby for and represent the interests of the industry at various levels, interacting with government and regulating bodies and presenting the case for the entertainment industry. Example subjects of this representation include the ongoing review of radio frequencies (which may or may not affect the radio bands in which wireless microphones and other devices use) and engaging with the issues surrounding the introduction of the RoHS (Restriction of Hazardous Substances Directive) regulations.
Saint Helena (/ˌseɪnt həˈliːnə/ SAYNT-hə-LEE-nə) is a volcanic tropical island in the South Atlantic Ocean, 4,000 kilometres (2,500 mi) east of Rio de Janeiro and 1,950 kilometres (1,210 mi) west of the Cunene River, which marks the border between Namibia and Angola in southwestern Africa. It is part of the British Overseas Territory of Saint Helena, Ascension and Tristan da Cunha. Saint Helena measures about 16 by 8 kilometres (10 by 5 mi) and has a population of 4,255 (2008 census). It was named after Saint Helena of Constantinople.
The island was uninhabited when discovered by the Portuguese in 1502. One of the most remote islands in the world, it was for centuries an important stopover for ships sailing to Europe from Asia and South Africa. Napoleon was imprisoned there in exile by the British, as were Dinuzulu kaCetshwayo (for leading a Zulu army against British rule) and more than 5,000 Boers taken prisoner during the Second Boer War.
Between 1791 and 1833, Saint Helena became the site of a series of experiments in conservation, reforestation and attempts to boost rainfall artificially. This environmental intervention was closely linked to the conceptualisation of the processes of environmental change and helped establish the roots of environmentalism.
Most historical accounts state that the island was discovered on 21 May 1502 by the Galician navigator João da Nova sailing at the service of Portugal, and that he named it "Santa Helena" after Helena of Constantinople. Another theory holds that the island found by da Nova was actually Tristan da Cunha, 2,430 kilometres (1,510 mi) to the south, and that Saint Helena was discovered by some of the ships attached to the squadron of Estêvão da Gama expedition on 30 July 1503 (as reported in the account of clerk Thomé Lopes). However, a paper published in 2015 reviewed the discovery date and dismissed the 18 August as too late for da Nova to make a discovery and then return to Lisbon by 11 September 1502, whether he sailed from St Helena or Tristan da Cunha. It demonstrates the 21 May is probably a Protestant rather than Catholic or Orthodox feast-day, first quoted in 1596 by Jan Huyghen van Linschoten, who was probably mistaken because the island was discovered several decades before the Reformation and start of Protestantism. The alternative discovery date of 3 May, the Catholic feast-day for the finding of the True Cross by Saint Helena in Jerusalem, quoted by Odoardo Duarte Lopes and Sir Thomas Herbert is suggested as being historically more credible.
The Portuguese found the island uninhabited, with an abundance of trees and fresh water. They imported livestock, fruit trees and vegetables, and built a chapel and one or two houses. Though they formed no permanent settlement, the island was an important rendezvous point and source of food for ships travelling from Asia to Europe, and frequently sick mariners were left on the island to recover, before taking passage on the next ship to call on the island.
Englishman Sir Francis Drake probably located the island on the final leg of his circumnavigation of the world (1577–1580). Further visits by other English explorers followed, and, once Saint Helena’s location was more widely known, English ships of war began to lie in wait in the area to attack Portuguese India carracks on their way home. In developing their Far East trade, the Dutch also began to frequent the island. The Portuguese and Spanish soon gave up regularly calling at the island, partly because they used ports along the West African coast, but also because of attacks on their shipping, the desecration of their chapel and religious icons, destruction of their livestock and destruction of plantations by Dutch and English sailors.
The Dutch Republic formally made claim to Saint Helena in 1633, although there is no evidence that they ever occupied, colonised or fortified it. By 1651, the Dutch had mainly abandoned the island in favour of their colony at the Cape of Good Hope.
In 1657, Oliver Cromwell granted the English East India Company a charter to govern Saint Helena and the following year the company decided to fortify the island and colonise it with planters. The first governor, Captain John Dutton, arrived in 1659, making Saint Helena one of Britain's oldest colonies outside North America and the Caribbean. A fort and houses were built. After the Restoration of the English monarchy in 1660, the East India Company received a royal charter giving it the sole right to fortify and colonise the island. The fort was renamed James Fort and the town Jamestown, in honour of the Duke of York, later James II of England.
Between January and May 1673, the Dutch East India Company forcibly took the island, before English reinforcements restored English East India Company control. The company experienced difficulty attracting new immigrants, and sentiments of unrest and rebellion fomented among the inhabitants. Ecological problems, including deforestation, soil erosion, vermin and drought, led Governor Isaac Pyke to suggest in 1715 that the population be moved to Mauritius, but this was not acted upon and the company continued to subsidise the community because of the island's strategic location. A census in 1723 recorded 1,110 people, including 610 slaves.
18th century governors tried to tackle the island's problems by implementing tree plantation, improving fortifications, eliminating corruption, building a hospital, tackling the neglect of crops and livestock, controlling the consumption of alcohol and introducing legal reforms. From about 1770, the island enjoyed a lengthy period of prosperity. Captain James Cook visited the island in 1775 on the final leg of his second circumnavigation of the world. St. James' Church was erected in Jamestown in 1774 and in 1791–92 Plantation House was built, and has since been the official residence of the Governor.
On leaving the University of Oxford, in 1676, Edmond Halley visited Saint Helena and set up an astronomical observatory with a 7.3-metre-long (24 ft) aerial telescope with the intention of studying stars from the Southern Hemisphere. The site of this telescope is near Saint Mathew's Church in Hutt's Gate, in the Longwood district. The 680-metre (2,230 ft) high hill there is named for him and is called Halley's Mount.
Throughout this period, Saint Helena was an important port of call of the East India Company. East Indiamen would stop there on the return leg of their voyages to British India and China. At Saint Helena ships could replenish supplies of water and provisions, and during war time, form convoys that would sail under the protection of vessels of the Royal Navy. Captain James Cook's vessel HMS Endeavour anchored and resupplied off the coast of St Helena in May 1771, on her return from the European discovery of the east coast of Australia and rediscovery of New Zealand.
The importation of slaves was made illegal in 1792. Governor Robert Patton (1802–1807) recommended that the company import Chinese labour to supplement the rural workforce. The coolie labourers arrived in 1810, and their numbers reached 600 by 1818. Many were allowed to stay, and their descendents became integrated into the population. An 1814 census recorded 3,507 people on the island.
In 1815, the British government selected Saint Helena as the place of detention of Napoleon Bonaparte. He was taken to the island in October 1815. Napoleon stayed at the Briars pavilion on the grounds of the Balcombe family's home until his permanent residence, Longwood House, was completed in December 1815. Napoleon died there on 5 May 1821.
After Napoleon's death, the thousands of temporary visitors were soon withdrawn and the East India Company resumed full control of Saint Helena. Between 1815 and 1830, the EIC made available to the government of the island the packet schooner St Helena, which made multiple trips per year between the island and the Cape carrying passengers both ways, and supplies of wine and provisions back to the island.
Owing to Napoleon's praise of Saint Helena’s coffee during his exile on the island, the product enjoyed a brief popularity in Paris in the years after his death.
Although the importation of slaves to St Helena had been banned in 1792, the phased emancipation of over 800 resident slaves did not take place until 1827, which was still some six years before the British Parliament passed legislation to ban slavery in the colonies.
Under the provisions of the 1833 India Act, control of Saint Helena was passed from the East India Company to the British Crown, becoming a crown colony. Subsequent administrative cost-cutting triggered the start of a long-term population decline whereby those who could afford to do so tended to leave the island for better opportunities elsewhere. The latter half of the 19th century saw the advent of steam ships not reliant on trade winds, as well as the diversion of Far East trade away from the traditional South Atlantic shipping lanes to a route via the Red Sea (which, prior to the building of the Suez Canal, involved a short overland section). These factors contributed to a decline in the number of ships calling at the island from 1,100 in 1855 to only 288 in 1889.
In 1840, a British naval station established to suppress the African slave trade was based on the island, and between 1840 and 1849 over 15,000 freed slaves, known as "Liberated Africans", were landed there.
In 1858, the French emperor Napoleon III successfully gained the possession, in the name of the French government, of Longwood House and the lands around it, last residence of Napoleon I (who died there in 1821). It is still French property, administered by a French representative and under the authority of the French Ministry of Foreign Affairs.
On 11 April 1898 American Joshua Slocum, on his famous and epic solo round the world voyage arrived at Jamestown. He departed on 20 April 1898 for the final leg of his circumnavigation having been extended hospitality from the governor, his Excellency Sir R A Standale, presented two lectures on his voyage and been invited to Longwood by the French Consular agent.
A local industry manufacturing fibre from New Zealand flax was successfully reestablished in 1907 and generated considerable income during the First World War. Ascension Island was made a dependency of Saint Helena in 1922, and Tristan da Cunha followed in 1938. During the Second World War, the United States built Wideawake airport on Ascension in 1942, but no military use was made of Saint Helena.
During this period, the island enjoyed increased revenues through the sale of flax, with prices peaking in 1951. However, the industry declined because of transportation costs and competition from synthetic fibres. The decision by the British Post Office to use synthetic fibres for its mailbags was a further blow, contributing to the closure of the island's flax mills in 1965.
From 1958, the Union Castle shipping line gradually reduced its service calls to the island. Curnow Shipping, based in Avonmouth, replaced the Union-Castle Line mailship service in 1977, using the RMS (Royal Mail Ship) St Helena.
The British Nationality Act 1981 reclassified Saint Helena and the other Crown colonies as British Dependent Territories. The islanders lost their right of abode in Britain. For the next 20 years, many could find only low-paid work with the island government, and the only available employment outside Saint Helena was on the Falkland Islands and Ascension Island. The Development and Economic Planning Department, which still operates, was formed in 1988 to contribute to raising the living standards of the people of Saint Helena.
In 1989, Prince Andrew launched the replacement RMS St Helena to serve the island; the vessel was specially built for the Cardiff–Cape Town route and features a mixed cargo/passenger layout.
The Saint Helena Constitution took effect in 1989 and provided that the island would be governed by a Governor and Commander-in-Chief, and an elected Executive and Legislative Council. In 2002, the British Overseas Territories Act 2002 granted full British citizenship to the islanders, and renamed the Dependent Territories (including Saint Helena) the British Overseas Territories. In 2009, Saint Helena and its two territories received equal status under a new constitution, and the British Overseas Territory was renamed Saint Helena, Ascension and Tristan da Cunha.
The UK government has spent £250 million in the construction of the island's airport. Expected to be fully operational early 2016, it is expected to help the island towards self-sufficiency and encourage economic development, reducing dependence on British government aid. The airport is also expected to kick start the tourism industry, with up to 30,000 visitors expected annually. As of August, 2015 ticketing was postponed until an airline could be firmly designated.
Located in the South Atlantic Ocean on the Mid-Atlantic Ridge, more than 2,000 kilometres (1,200 mi) from the nearest major landmass, Saint Helena is one of the most remote places in the world. The nearest port on the continent is Namibe in southern Angola, and the nearest international airport the Quatro de Fevereiro Airport of Angola's capital Luanda; connections to Cape Town in South Africa are used for most shipping needs, such as the mail boat that serves the island, the RMS St Helena. The island is associated with two other isolated islands in the southern Atlantic, also British territories: Ascension Island about 1,300 kilometres (810 mi) due northwest in more equatorial waters and Tristan da Cunha, which is well outside the tropics 2,430 kilometres (1,510 mi) to the south. The island is situated in the Western Hemisphere and has the same longitude as Cornwall in the United Kingdom. Despite its remote location, it is classified as being in West Africa by the United Nations.
The island of Saint Helena has a total area of 122 km2 (47 sq mi), and is composed largely of rugged terrain of volcanic origin (the last volcanic eruptions occurred about 7 million years ago). Coastal areas are covered in volcanic rock and warmer and drier than the centre. The highest point of the island is Diana's Peak at 818 m (2,684 ft). In 1996 it became the island's first national park. Much of the island is covered by New Zealand flax, a legacy of former industry, but there are some original trees augmented by plantations, including those of the Millennium Forest project which was established in 2002 to replant part of the lost Great Wood and is now managed by the Saint Helena National Trust. When the island was discovered, it was covered with unique indigenous vegetation, including a remarkable cabbage tree species. The island's hinterland must have been a dense tropical forest but the coastal areas were probably also quite green. The modern landscape is very different, with widespread bare rock in the lower areas, although inland it is green, mainly due to introduced vegetation. There are no native land mammals, but cattle, cats, dogs, donkeys, goats, mice, rabbits, rats and sheep have been introduced, and native species have been adversely affected as a result. The dramatic change in landscape must be attributed to these introductions. As a result, the string tree (Acalypha rubrinervis) and the St Helena olive (Nesiota elliptica) are now extinct, and many of the other endemic plants are threatened with extinction.
There are several rocks and islets off the coast, including: Castle Rock, Speery Island, the Needle, Lower Black Rock, Upper Black Rock (South), Bird Island (Southwest), Black Rock, Thompson's Valley Island, Peaked Island, Egg Island, Lady's Chair, Lighter Rock (West), Long Ledge (Northwest), Shore Island, George Island, Rough Rock Island, Flat Rock (East), the Buoys, Sandy Bay Island, the Chimney, White Bird Island and Frightus Rock (Southeast), all of which are within one kilometre (0.62 miles) of the shore.
The national bird of Saint Helena is the Saint Helena plover, known locally as the wirebird. It appears on the coat of arms of Saint Helena and on the flag.
The climate of Saint Helena is tropical, marine and mild, tempered by the Benguela Current and trade winds that blow almost continuously. The climate varies noticeably across the island. Temperatures in Jamestown, on the north leeward shore, range between 21–28 °C (70–82 °F) in the summer (January to April) and 17–24 °C (63–75 °F) during the remainder of the year. The temperatures in the central areas are, on average, 5–6 °C (9.0–10.8 °F) lower. Jamestown also has a very low annual rainfall, while 750–1,000 mm (30–39 in) falls per year on the higher ground and the south coast, where it is also noticeably cloudier. There are weather recording stations in the Longwood and Blue Hill districts.
Saint Helena is divided into eight districts, each with a community centre. The districts also serve as statistical subdivisions. The island is a single electoral area and elects twelve representatives to the Legislative Council of fifteen.
Saint Helena was first settled by the English in 1659, and the island has a population of about 4,250 inhabitants, mainly descended from people from Britain – settlers ("planters") and soldiers – and slaves who were brought there from the beginning of settlement – initially from Africa (the Cape Verde Islands, Gold Coast and west coast of Africa are mentioned in early records), then India and Madagascar. Eventually the planters felt there were too many slaves and no more were imported after 1792.
In 1840, St Helena became a provisioning station for the British West Africa Squadron, preventing slavery to Brazil (mainly), and many thousands of slaves were freed on the island. These were all African, and about 500 stayed while the rest were sent on to the West Indies and Cape Town, and eventually to Sierra Leone.
Imported Chinese labourers arrived in 1810, reaching a peak of 618 in 1818, after which numbers were reduced. Only a few older men remained after the British Crown took over the government of the island from the East India Company in 1834. The majority were sent back to China, although records in the Cape suggest that they never got any farther than Cape Town. There were also a very few Indian lascars who worked under the harbour master.
The citizens of Saint Helena hold British Overseas Territories citizenship. On 21 May 2002, full British citizenship was restored by the British Overseas Territories Act 2002. See also British nationality law.
During periods of unemployment, there has been a long pattern of emigration from the island since the post-Napoleonic period. The majority of "Saints" emigrated to the UK, South Africa and in the early years, Australia. The population has steadily declined since the late 1980s and has dropped from 5,157 at the 1998 census to 4,255 in 2008. In the past emigration was characterised by young unaccompanied persons leaving to work on long-term contracts on Ascension and the Falkland Islands, but since "Saints" were re-awarded UK citizenship in 2002, emigration to the UK by a wider range of wage-earners has accelerated due to the prospect of higher wages and better progression prospects.
Most residents belong to the Anglican Communion and are members of the Diocese of St Helena, which has its own bishop and includes Ascension Island. The 150th anniversary of the diocese was celebrated in June 2009.
Other Christian denominations on the island include: Roman Catholic (since 1852), Salvation Army (since 1884), Baptist (since 1845) and, in more recent times, Seventh-day Adventist (since 1949), New Apostolic and Jehovah's Witnesses (of which one in 35 residents is a member, the highest ratio of any country). The Catholics are pastorally served by the Mission sui iuris of Saint Helena, Ascension and Tristan da Cunha, whose office of ecclesiastical superior is vested in the Apostolic Prefecture of the Falkland Islands.
Executive authority in Saint Helena is vested in Queen Elizabeth II and is exercised on her behalf by the Governor of Saint Helena. The Governor is appointed by the Queen on the advice of the British government. Defence and Foreign Affairs remain the responsibility of the United Kingdom.
There are fifteen seats in the Legislative Council of Saint Helena, a unicameral legislature, in addition to a Speaker and a Deputy Speaker. Twelve of the fifteen members are elected in elections held every four years. The three ex officio members are the Chief Secretary, Financial Secretary and Attorney General. The Executive Council is presided over by the Governor, and consists of three ex officio officers and five elected members of the Legislative Council appointed by the Governor. There is no elected Chief Minister, and the Governor acts as the head of government. In January 2013 it was proposed that the Executive Council would be led by a "Chief Councillor" who would be elected by the members of the Legislative Council and would nominate the other members of the Executive Council. These proposals were put to a referendum on 23 March 2013 where they were defeated by 158 votes to 42 on a 10% turnout.
One commentator has observed that, notwithstanding the high unemployment resulting from the loss of full passports during 1981–2002, the level of loyalty to the British monarchy by the St Helena population is probably not exceeded in any other part of the world. King George VI is the only reigning monarch to have visited the island. This was in 1947 when the King, accompanied by Queen Elizabeth (later the Queen Mother), Princess Elizabeth (later Queen Elizabeth II) and Princess Margaret were travelling to South Africa. Prince Philip arrived at St Helena in 1957 and then his son Prince Andrew visited as a member of the armed forces in 1984 and his sister the Princess Royal arrived in 2002.
In 2012, the government of St. Helena funded the creation of the St. Helena Human Rights Action Plan 2012-2015. Work is being done under this action plan, including publishing awareness-raising articles in local newspapers, providing support for members of the public with human rights queries, and extending several UN Conventions on human rights to St. Helena.
In recent years[when?], there have been reports of child abuse in St Helena. Britain’s Foreign and Commonwealth Office (FCO) has been accused of lying to the United Nations about child abuse in St Helena to cover up allegations, including cases of a police officer having raped a four-year-old girl and of a police officer having mutilated a two-year-old.
St Helena has long been known for its high proportion of endemic birds and vascular plants. The highland areas contain most of the 400 endemic species recognised to date. Much of the island has been identified by BirdLife International as being important for bird conservation, especially the endemic Saint Helena plover or wirebird, and for seabirds breeding on the offshore islets and stacks, in the north-east and the south-west Important Bird Areas. On the basis of these endemics and an exceptional range of habitats, Saint Helena is on the United Kingdom's tentative list for future UNESCO World Heritage Sites.
St Helena's biodiversity, however, also includes marine vertebrates, invertebrates (freshwater, terrestrial and marine), fungi (including lichen-forming species), non-vascular plants, seaweeds and other biological groups. To date, very little is known about these, although more than 200 lichen-forming fungi have been recorded, including 9 endemics, suggesting that many significant discoveries remain to be made.
The island had a monocrop economy until 1966, based on the cultivation and processing of New Zealand flax for rope and string. St Helena's economy is now weak, and is almost entirely sustained by aid from the British government. The public sector dominates the economy, accounting for about 50% of gross domestic product. Inflation was running at 4% in 2005. There have been increases in the cost of fuel, power and all imported goods.
The tourist industry is heavily based on the promotion of Napoleon's imprisonment. A golf course also exists and the possibility for sportfishing tourism is great. Three hotels operate on the island but the arrival of tourists is directly linked to the arrival and departure schedule of the RMS St Helena. Some 3,200 short-term visitors arrived on the island in 2013.
Saint Helena produces what is said to be the most expensive coffee in the world. It also produces and exports Tungi Spirit, made from the fruit of the prickly or cactus pears, Opuntia ficus-indica ("Tungi" is the local St Helenian name for the plant). Ascension Island, Tristan da Cunha and Saint Helena all issue their own postage stamps which provide a significant income.
Quoted at constant 2002 prices, GDP fell from £12 million in 1999-2000 to £11 million in 2005-06. Imports are mainly from the UK and South Africa and amounted to £6.4 million in 2004-05 (quoted on an FOB basis). Exports are much smaller, amounting to £0.2 million in 2004-05. Exports are mainly fish and coffee; Philatelic sales were £0.06 million in 2004-05. The limited number of visiting tourists spent about £0.4 million in 2004-05, representing a contribution to GDP of 3%.
Public expenditure rose from £10 million in 2001-02 to £12 million in 2005-06 to £28m in 2012-13. The contribution of UK budgetary aid to total SHG government expenditure rose from £4.6 million in to £6.4 million to £12.1 million over the same period. Wages and salaries represent about 38% of recurrent expenditure.
Unemployment levels are low (31 individuals in 2013, compared to 50 in 2004 and 342 in 1998). Employment is dominated by the public sector, the number of government positions has fallen from 1,142 in 2006 to just over 800 in 2013. St Helena’s private sector employs approximately 45% of the employed labour force and is largely dominated by small and micro businesses with 218 private businesses employing 886 in 2004.
Household survey results suggest the percentage of households spending less than £20 per week on a per capita basis fell from 27% to 8% between 2000 and 2004, implying a decline in income poverty. Nevertheless, 22% of the population claimed social security benefit in 2006/7, most of them aged over 60, a sector that represents 20% of the population.
In 1821, Saul Solomon issued a 70,560 copper tokens worth a halfpenny each Payable at St Helena by Solomon, Dickson and Taylor – presumably London partners – that circulated alongside the East India Company's local coinage until the Crown took over the island in 1836. The coin remains readily available to collectors.
Today Saint Helena has its own currency, the Saint Helena pound, which is at parity with the pound sterling. The government of Saint Helena produces its own coinage and banknotes. The Bank of Saint Helena was established on Saint Helena and Ascension Island in 2004. It has branches in Jamestown on Saint Helena, and Georgetown, Ascension Island and it took over the business of the St. Helena government savings bank and Ascension Island Savings Bank.
Saint Helena is one of the most remote islands in the world, has one commercial airport under construction, and travel to the island is by ship only. A large military airfield is located on Ascension Island, with two Friday flights to RAF Brize Norton, England (as from September 2010). These RAF flights offer a limited number of seats to civilians.
The ship RMS Saint Helena runs between St Helena and Cape Town on a 5-day voyage, also visiting Ascension Island and Walvis Bay, and occasionally voyaging north to Tenerife and Portland, UK. It berths in James Bay, St Helena approximately thirty times per year. The RMS Saint Helena was due for decommissioning in 2010. However, its service life has been extended indefinitely until the airport is completed.
After a long period of rumour and consultation, the British government announced plans to construct an airport in Saint Helena in March 2005. The airport was expected to be completed by 2010. However an approved bidder, the Italian firm Impregilo, was not chosen until 2008, and then the project was put on hold in November 2008, allegedly due to new financial pressures brought on by the Financial crisis of 2007–2010. By January 2009, construction had not commenced and no final contracts had been signed. Governor Andrew Gurr departed for London in an attempt to speed up the process and solve the problems.
On 22 July 2010, the British government agreed to help pay for the new airport using taxpayer money. In November 2011 a new deal between the British government and South African civil engineering company Basil Read was signed and the airport was scheduled to open in February 2016, with flights to and from South Africa and the UK. In March 2015 South African airline Comair became the preferred bidder to provide weekly air service between the island and Johannesburg, starting from 2016.
The first aircraft, a South African Beechcraft King Air 200, landed at the new airport on 15 September 2015, prior to conducting a series of flights to calibrate the airport's radio navigation equipment.
The first helicopter landing at the new airfield was conducted by the Wildcat HMA.2 ZZ377 from 825 Squadron 201 Flight, embarked on visiting HMS Lancaster on 23 October 2015.
A minibus offers a basic service to carry people around Saint Helena, with most services designed to take people into Jamestown for a few hours on weekdays to conduct their business. Car hire is available for visitors.
Radio St Helena, which started operations on Christmas Day 1967, provided a local radio service that had a range of about 100 km (62 mi) from the island, and also broadcast internationally on shortwave radio (11092.5 kHz) on one day a year. The station presented news, features and music in collaboration with its sister newspaper, the St Helena Herald. It closed on 25 December 2012 to make way for a new three-channel FM service, also funded by St. Helena Government and run by the South Atlantic Media Services (formerly St. Helena Broadcasting (Guarantee) Corporation).
Saint FM provided a local radio service for the island which was also available on internet radio and relayed in Ascension Island. The station was not government funded. It was launched in January 2005 and closed on 21 December 2012. It broadcast news, features and music in collaboration with its sister newspaper, the St Helena Independent (which continues).
Saint FM Community Radio took over the radio channels vacated by Saint FM and launched on 10 March 2013. The station operates as a limited-by-guarantee company owned by its members and is registered as a fund-raising Association. Membership is open to everyone, and grants access to a live audio stream.
St Helena Online is a not-for-profit internet news service run from the UK by a former print and BBC journalist, working in partnership with Saint FM and the St Helena Independent.
Sure South Atlantic Ltd ("Sure") offers television for the island via 17 analogue terrestrial UHF channels, offering a mix of British, US, and South African programming. The channels are from DSTV and include Mnet, SuperSport and BBC channels. The feed signal, from MultiChoice DStv in South Africa, is received by a satellite dish at Bryant's Beacon from Intelsat 7 in the Ku band.
SURE provide the telecommunications service in the territory through a digital copper-based telephone network including ADSL-broadband service. In August 2011 the first fibre-optic link has been installed on the island, which connects the television receive antennas at Bryant's Beacon to the Cable & Wireless Technical Centre in the Briars.
A satellite ground station with a 7.6-metre (25 ft) satellite dish installed in 1989 at The Briars is the only international connection providing satellite links through Intelsat 707 to Ascension island and the United Kingdom. Since all international telephone and internet communications are relying on this single satellite link both internet and telephone service are subject to sun outages.
Saint Helena has the international calling code +290 which, since 2006, Tristan da Cunha shares. Saint Helena telephone numbers changed from 4 to 5 digits on 1 October 2013 by being prefixed with the digit "2", i.e. 2xxxx, with the range 5xxxx being reserved for mobile numbering, and 8xxx being used for Tristan da Cunha numbers (these still shown as 4 digits).
Saint Helena has a 10/3.6 Mbit/s internet link via Intelsat 707 provided by SURE. Serving a population of more than 4,000, this single satellite link is considered inadequate in terms of bandwidth.
ADSL-broadband service is provided with maximum speeds of up to 1536 KBit/s downstream and 512 KBit/s upstream offered on contract levels from lite £16 per month to gold+ at £190 per month. There are a few public WiFi hotspots in Jamestown, which are also being operated by SURE (formerly Cable & Wireless).
The South Atlantic Express, a 10,000 km (6,214 mi) submarine communications cable connecting Africa to South America, run by the undersea fibre optic provider eFive, will pass St Helena relatively closely. There were no plans to land the cable and install a landing station ashore, which could supply St Helena's population with sufficient bandwidth to fully leverage the benefits of today's Information Society. In January 2012, a group of supporters petitioned the UK government to meet the cost of landing the cable at St Helena. On 6 October 2012, eFive agreed to reroute the cable through St. Helena after a successful lobbying campaign by A Human Right, a San Francisco-based NGA working on initiatives to ensure all people are connected to the Internet. Islanders have sought the assistance of the UK Department for International Development and Foreign and Commonwealth Office in funding the £10m required to bridge the connection from a local junction box on the cable to the island. The UK Government have announced that a review of the island's economy would be required before such funding would be agreed to.
The island has two local newspapers, both of which are available on the Internet. The St Helena Independent has been published since November 2005. The Sentinel newspaper was introduced in 2012.
Education is free and compulsory between the ages of 5 and 16  The island has three primary schools for students of age 4 to 11: Harford, Pilling, and St Paul’s. Prince Andrew School provides secondary education for students aged 11 to 18. At the beginning of the academic year 2009-10, 230 students were enrolled in primary school and 286 in secondary school.
The Education and Employment Directorate also offers programmes for students with special needs, vocational training, adult education, evening classes, and distance learning. The island has a public library (the oldest in the Southern Hemisphere) and a mobile library service which operates weekly rural areas.
The UK national curriculum is adapted for local use. A range of qualifications are offered – from GCSE, A/S and A2, to Level 3 Diplomas and VRQ qualifications:
Sports played on the island include football, cricket, volleyball, tennis, golf, motocross, shooting sports and yachting. Saint Helena has sent teams to a number of Commonwealth Games. Saint Helena is a member of the International Island Games Association. The Saint Helena cricket team made its debut in international cricket in Division Three of the African region of the World Cricket League in 2011.
The Governor's Cup is a yacht race between Cape Town and Saint Helena island, held every two years in December/January; the most recent event was in December 2010. In Jamestown a timed run takes place up Jacob's Ladder every year, with people coming from all over the world to take part.
There are scouting and guiding groups on Saint Helena and Ascension Island. Scouting was established on Saint Helena island in 1912. Lord and Lady Baden-Powell visited the Scouts on Saint Helena on the return from their 1937 tour of Africa. The visit is described in Lord Baden-Powell's book entitled African Adventures.
The Chicago Cubs are an American professional baseball team located on the North Side of Chicago, Illinois. The Cubs compete in Major League Baseball (MLB) as a members of the National League (NL) Central division; the team plays its home baseball games at Wrigley Field. The Cubs are also one of two active major league teams based in Chicago; the other is the Chicago White Sox, who are a member of the American League (AL) Central division. The team is currently owned by Thomas S. Ricketts, son of TD Ameritrade founder Joe Ricketts.
The team played its first games in 1876 as a founding member of the National League (NL), eventually becoming known officially as the Chicago Cubs for the 1903 season. Officially, the Cubs are tied for the distinction of being the oldest currently active U.S. professional sports club, along with the Atlanta Braves, which also began play in the NL in 1876 as the Boston Red Stockings (Major League Baseball does not officially recognize the National Association of Professional Base Ball Players as a major league.)
In 1906, the franchise recorded a Major League record 116 wins (tied by the 2001 Seattle Mariners) and posted a modern-era record winning percentage of .763, which still stands today. They appeared in their first World Series the same year, falling to their crosstown rivals, the Chicago White Sox, four games to two. The Cubs won back-to-back World Series championships in 1907 and 1908, becoming the first Major League team to play in three consecutive Fall Classics, and the first to win it twice. The team has appeared in seven World Series following their 1908 title, most recently in 1945. The Cubs have not won the World Series in 107 years, the longest championship drought of any major North American professional sports team, and are often referred to as the "Lovable Losers" because of this distinction. They are also known as "The North Siders" because Wrigley Field, their home park since 1916, is located in Chicago's North Side Lake View community at 1060 West Addison Street. The Cubs have a major rivalry with the St. Louis Cardinals.
The Cubs began play as the Chicago White Stockings, joining the National League (NL) as a charter member. Owner William Hulbert signed multiple star players, such as pitcher Albert Spalding and infielders Ross Barnes, Deacon White, and Adrian "Cap" Anson, to join the team prior to the N.L.'s first season. The White Stockings played their home games at West Side Grounds,against the bloods and quickly established themselves as one of the new league's top teams. Spalding won forty-seven games and Barnes led the league in hitting at .429 as Chicago won the first ever National League pennant, which at the time was the game's top prize.
After back-to-back pennants in 1880 and 1881, Hulbert died, and Spalding, who had retired to start Spalding sporting goods, assumed ownership of the club. The White Stockings, with Anson acting as player/manager, captured their third consecutive pennant in 1882, and Anson established himself as the game's first true superstar. In 1885 and '86, after winning N.L. pennants, the White Stockings met the short-lived American Association champion in that era's version of a World Series. Both seasons resulted in match ups with the St. Louis Brown Stockings, with the clubs tying in 1885 and with St. Louis winning in 1886. This was the genesis of what would eventually become one of the greatest rivalries in sports. In all, the Anson-led Chicago Base Ball Club won six National League pennants between 1876 and 1886. As a result, Chicago's club nickname transitioned, and by 1890 they had become known as the Chicago Colts, or sometimes "Anson's Colts", referring to Cap's influence within the club. Anson was the first player in history credited with collecting 3,000 career hits. After a disappointing record of 59-73 and a 9th-place finish in 1897, Anson was released by the Cubs as both a player and manager. Due to Anson's absence from the club after 22 years, local newspaper reporters started to refer to the Cubs as the "Orphans".
In 1902, Spalding, who by this time had revamped the roster to boast what would soon be one of the best teams of the early century, sold the club to Jim Hart. The franchise was nicknamed the Cubs by the Chicago Daily News in 1902, although not officially becoming the Chicago Cubs until the 1907 season. During this period, which has become known as baseball's dead-ball era, Cub infielders Joe Tinker, Johnny Evers, and Frank Chance were made famous as a double-play combination by Franklin P. Adams' poem Baseball's Sad Lexicon. The poem first appeared in the July 18, 1910 edition of the New York Evening Mail. Mordecai "Three-Finger" Brown, Jack Taylor, Ed Reulbach, Jack Pfiester, and Orval Overall were several key pitchers for the Cubs during this time period. With Chance acting as player-manager from 1905 to 1912, the Cubs won four pennants and two World Series titles over a five-year span. Although they fell to the "Hitless Wonders" White Sox in the 1906 World Series, the Cubs recorded a record 116 victories and the best winning percentage (.763) in Major League history. With mostly the same roster, Chicago won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play three times in the Fall Classic and the first to win it twice. However, the Cubs have not won a World Series since; this remains the longest championship drought in North American professional sports.
In 1914, advertising executive Albert Lasker obtained a large block of the club's shares and before the 1916 season assumed majority ownership of the franchise. Lasker brought in a wealthy partner, Charles Weeghman, the proprietor of a popular chain of lunch counters who had previously owned the Chicago Whales of the short-lived Federal League. As principal owners, the pair moved the club from the West Side Grounds to the much newer Weeghman Park, which had been constructed for the Whales only two years earlier, where they remain to this day. The Cubs responded by winning a pennant in the war-shortened season of 1918, where they played a part in another team's curse: the Boston Red Sox defeated Grover Cleveland Alexander's Cubs four games to two in the 1918 World Series, Boston's last Series championship until 2004.
Near the end of the first decade of the double-Bills' guidance, the Cubs won the NL pennant in 1929 and then achieved the unusual feat of winning a pennant every three years, following up the 1929 flag with league titles in 1932, 1935, and 1938. Unfortunately, their success did not extend to the Fall Classic, as they fell to their AL rivals each time. The '32 series against the Yankees featured Babe Ruth's "called shot" at Wrigley Field in Game 3. There were some historic moments for the Cubs as well; In 1930, Hack Wilson, one of the top home run hitters in the game, had one of the most impressive seasons in MLB history, hitting 56 home runs and establishing the current runs-batted-in record of 191. That 1930 club, which boasted six eventual Hall of Famers (Wilson, Gabby Hartnett, Rogers Hornsby, George "High Pockets" Kelly, Kiki Cuyler and manager Joe McCarthy) established the current team batting average record of .309. In 1935 the Cubs claimed the pennant in thrilling fashion, winning a record 21 games in a row in September. The '38 club saw Dizzy Dean lead the team's pitching staff and provided a historic moment when they won a crucial late-season game at Wrigley Field over the Pittsburgh Pirates with a walk-off home run by Gabby Hartnett, which became known in baseball lore as "The Homer in the Gloamin'".
The Cubs enjoyed one more pennant at the close of World War II, finishing 98–56. Due to the wartime travel restrictions, the first three games of the 1945 World Series were played in Detroit, where the Cubs won two games, including a one-hitter by Claude Passeau, and the final four were played at Wrigley. In Game 4 of the Series, the Curse of the Billy Goat was allegedly laid upon the Cubs when P.K. Wrigley ejected Billy Sianis, who had come to Game 4 with two box seat tickets, one for him and one for his goat. They paraded around for a few innings, but Wrigley demanded the goat leave the park due to its unpleasant odor. Upon his ejection, Mr. Sianis uttered, "The Cubs, they ain't gonna win no more." The Cubs lost Game 4, lost the Series, and have not been back since. It has also been said by many that Sianis put a "curse" on the Cubs, apparently preventing the team from playing in the World Series. After losing the 1945 World Series to the Detroit Tigers, the Cubs finished with winning seasons the next two years, but those teams did not enter post-season play.
In the following two decades after Sianis' ill will, the Cubs played mostly forgettable baseball, finishing among the worst teams in the National League on an almost annual basis. Longtime infielder/manager Phil Cavarretta, who had been a key player during the '45 season, was fired during spring training in 1954 after admitting the team was unlikely to finish above fifth place. Although shortstop Ernie Banks would become one of the star players in the league during the next decade, finding help for him proved a difficult task, as quality players such as Hank Sauer were few and far between. This, combined with poor ownership decisions such as the College of Coaches, and the ill-fated trade of future Hall of Famer Lou Brock to the Cardinals for pitcher Ernie Broglio (who won only 7 games over the next three seasons), hampered on-field performance.
In 1969 the Cubs, managed by Leo Durocher, built a substantial lead in the newly created National League Eastern Division by mid-August. Ken Holtzman pitched a no-hitter on August 19, and the division lead grew to 8 1⁄2 games over the St. Louis Cardinals and by 9 1⁄2 games over the New York Mets. After the game of September 2, the Cubs record was 84-52 with the Mets in second place at 77-55. But then a losing streak began just as a Mets winning streak was beginning. The Cubs lost the final game of a series at Cincinnati, then came home to play the resurgent Pittsburgh Pirates (who would finish in third place). After losing the first two games by scores of 9-2 and 13-4, the Cubs led going into the ninth inning. A win would be a positive springboard since the Cubs were to play a crucial series with the Mets the very next day. But Willie Stargell drilled a 2-out, 2-strike pitch from the Cubs' ace reliever, Phil Regan, onto Sheffield Avenue to tie the score in the top of the ninth. The Cubs would lose 7-5 in extra innings. Burdened by a four-game losing streak, the Cubs traveled to Shea Stadium for a short two-game set. The Mets won both games, and the Cubs left New York with a record of 84-58 just 1⁄2 game in front. Disaster followed in Philadelphia, as a 99 loss Phillies team nonetheless defeated the Cubs twice, to extend Chicago's losing streak to eight games. In a key play in the second game, on September 11, Cubs starter Dick Selma threw a surprise pickoff attempt to third baseman Ron Santo, who was nowhere near the bag or the ball. Selma's throwing error opened the gates to a Phillies rally. After that second Philly loss, the Cubs were 84-60 and the Mets had pulled ahead at 85-57. The Mets would not look back. The Cubs' eight-game losing streak finally ended the next day in St. Louis, but the Mets were in the midst of a ten-game winning streak, and the Cubs, wilting from team fatigue, generally deteriorated in all phases of the game. The Mets (who had lost a record 120 games 7 years earlier), would go on to win the World Series. The Cubs, despite a respectable 92-70 record, would be remembered for having lost a remarkable 17½ games in the standings to the Mets in the last quarter of the season.
Following the '69 season, the club posted winning records for the next few seasons, but no playoff action. After the core players of those teams started to move on, the 70s got worse for the team, and they became known as "The Loveable Losers." In 1977, the team found some life, but ultimately experienced one of its biggest collapses. The Cubs hit a high-water mark on June 28 at 47–22, boasting an 8 1⁄2 game NL East lead, as they were led by Bobby Murcer (27 Hr/89 RBI), and Rick Reuschel (20–10). However, the Philadelphia Phillies cut the lead to two by the All-star break, as the Cubs sat 19 games over .500, but they swooned late in the season, going 20–40 after July 31. The Cubs finished in 4th place at 81–81, while Philadelphia surged, finishing with 101 wins. The following two seasons also saw the Cubs get off to a fast start, as the team rallied to over 10 games above .500 well into both seasons, only to again wear down and play poorly later on, and ultimately settling back to mediocrity. This trait became known as the "June Swoon." Again, the Cubs' unusually high number of day games is often pointed to as one reason for the team's inconsistent late season play.
After over a dozen more subpar seasons, in 1981 the Cubs hired GM Dallas Green from Philadelphia to turn around the franchise. Green had managed the 1980 Phillies to the World Series title. One of his early GM moves brought in a young Phillies minor-league 3rd baseman named Ryne Sandberg, along with Larry Bowa for Iván DeJesús. The 1983 Cubs had finished 71–91 under Lee Elia, who was fired before the season ended by Green. Green continued the culture of change and overhauled the Cubs roster, front-office and coaching staff prior to 1984. Jim Frey was hired to manage the 1984 Cubs, with Don Zimmer coaching 3rd base and Billy Connors serving as pitching coach.
Green shored up the 1984 roster with a series of transactions. In December, 1983 Scott Sanderson was acquired from Montreal in a three-team deal with San Diego for Carmelo Martínez. Pinch hitter Richie Hebner (.333 BA in 1984) was signed as a free-agent. In spring training, moves continued: LF Gary Matthews and CF Bobby Dernier came from Philadelphia on March 26, for Bill Campbell and a minor leaguer. Reliever Tim Stoddard (10–6 3.82, 7 saves) was acquired the same day for a minor leaguer; veteran pitcher Ferguson Jenkins was released.
The team's commitment to contend was complete when Green made a midseason deal on June 15 to shore up the starting rotation due to injuries to Rick Reuschel (5–5) and Sanderson. The deal brought 1979 NL Rookie of the Year pitcher Rick Sutcliffe from the Cleveland Indians. Joe Carter (who was with the Triple-A Iowa Cubs at the time) and center fielder Mel Hall were sent to Cleveland for Sutcliffe and back-up catcher Ron Hassey (.333 with Cubs in 1984). Sutcliffe (5–5 with the Indians) immediately joined Sanderson (8–5 3.14), Eckersley (10–8 3.03), Steve Trout (13–7 3.41) and Dick Ruthven (6–10 5.04) in the starting rotation. Sutcliffe proceeded to go 16–1 for Cubs and capture the Cy Young Award.
The shift in the Cubs' fortunes was characterized June 23 on the "NBC Saturday Game of the Week" contest against the St. Louis Cardinals. it has since been dubbed simply "The Sandberg Game." With the nation watching and Wrigley Field packed, Sandberg emerged as a superstar with not one, but two game-tying home runs against Cardinals closer Bruce Sutter. With his shots in the 9th and 10th innings Wrigley Field erupted and Sandberg set the stage for a comeback win that cemented the Cubs as the team to beat in the East. No one would catch them, except the Padres in the playoffs.
In 1984, each league had two divisions, East and West. The divisional winners met in a best-of-5 series to advance to the World Series, in a "2–3" format, first two games were played at the home of the team who did not have home field advantage. Then the last three games were played at the home of the team, with home field advantage. Thus the first two games were played at Wrigley Field and the next three at the home of their opponents, San Diego. A common and unfounded myth is that since Wrigley Field did not have lights at that time the National League decided to give the home field advantage to the winner of the NL West. In fact, home field advantage had rotated between the winners of the East and West since 1969 when the league expanded. In even numbered years, the NL West had home field advantage. In odd numbered years, the NL East had home field advantage. Since the NL East winners had had home field advantage in 1983, the NL West winners were entitled to it.
The confusion may stem from the fact that Major League Baseball did decide that, should the Cubs make it to the World Series, the American League winner would have home field advantage unless the Cubs hosted home games at an alternate site since the Cubs home field of Wrigley Field did not yet have lights. Rumor was the Cubs could hold home games across town at Comiskey Park, home of the American League's Chicago White Sox. Rather than hold any games in the cross town rival Sox Park, the Cubs made arrangements with the August A. Busch, owner of the St. Louis Cardinals, to use Busch Stadium in St. Louis as the Cubs "home field" for the World Series. This was approved by Major League Baseball and would have enabled the Cubs to host games 1 and 2, along with games 6 and 7 if necessary. At the time home field advantage was rotated between each league. Odd numbered years the AL had home field advantage. Even numbered years the NL had home field advantage. In the 1982 World Series the St. Louis Cardinals of the NL had home field advantage. In the 1983 World Series the Baltimore Orioles of the AL had home field advantage.
In the NLCS, the Cubs easily won the first two games at Wrigley Field against the San Diego Padres. The Padres were the winners of the Western Division with Steve Garvey, Tony Gwynn, Eric Show, Goose Gossage and Alan Wiggins. With wins of 13–0 and 4–2, the Cubs needed to win only one game of the next three in San Diego to make it to the World Series. After being beaten in Game 3 7–1, the Cubs lost Game 4 when Smith, with the game tied 5–5, allowed a game-winning home run to Garvey in the bottom of the ninth inning. In Game 5 the Cubs took a 3–0 lead into the 6th inning, and a 3–2 lead into the seventh with Sutcliffe (who won the Cy Young Award that year) still on the mound. Then, Leon Durham had a sharp grounder go under his glove. This critical error helped the Padres win the game 6–3, with a 4-run 7th inning and keep Chicago out of the 1984 World Series against the Detroit Tigers. The loss ended a spectacular season for the Cubs, one that brought alive a slumbering franchise and made the Cubs relevant for a whole new generation of Cubs fans.
In 1989, the first full season with night baseball at Wrigley Field, Don Zimmer's Cubs were led by a core group of veterans in Ryne Sandberg, Rick Sutcliffe and Andre Dawson, who were boosted by a crop of youngsters such as Mark Grace, Shawon Dunston, Greg Maddux, Rookie of the Year Jerome Walton, and Rookie of the Year Runner-Up Dwight Smith. The Cubs won the NL East once again that season winning 93 games. This time the Cubs met the San Francisco Giants in the NLCS. After splitting the first two games at home, the Cubs headed to the Bay Area, where despite holding a lead at some point in each of the next three games, bullpen meltdowns and managerial blunders ultimately led to three straight losses. The Cubs couldn't overcome the efforts of Will Clark, whose home run off Maddux, just after a managerial visit to the mound, led Maddux to think Clark knew what pitch was coming. Afterward, Maddux would speak into his glove during any mound conversation, beginning what is a norm today. Mark Grace was 11–17 in the series with 8 RBI. Eventually, the Giants lost to the "Bash Brothers" and the Oakland A's in the famous "Earthquake Series."
The '98 season would begin on a somber note with the death of legendary broadcaster Harry Caray. After the retirement of Sandberg and the trade of Dunston, the Cubs had holes to fill and the signing of Henry Rodríguez, known affectionately as "H-Rod" to bat cleanup provided protection for Sammy Sosa in the lineup, as Rodriguez slugged 31 round-trippers in his first season in Chicago. Kevin Tapani led the club with a career high 19 wins, Rod Beck anchored a strong bullpen and Mark Grace turned in one of his best seasons. The Cubs were swamped by media attention in 1998, and the team's two biggest headliners were Sosa and rookie flamethrower Kerry Wood. Wood's signature performance was one-hitting the Houston Astros, a game in which he tied the major league record of 20 strikeouts in nine innings. His torrid strikeout numbers earned Wood the nickname "Kid K," and ultimately earned him the 1998 NL Rookie of the Year award. Sosa caught fire in June, hitting a major league record 20 home runs in the month, and his home run race with Cardinals slugger Mark McGwire transformed the pair into international superstars in a matter of weeks. McGwire finished the season with a new major league record of 70 home runs, but Sosa's .308 average and 66 homers earned him the National League MVP Award. After a down-to-the-wire Wild Card chase with the San Francisco Giants, Chicago and San Francisco ended the regular season tied, and thus squared off in a one-game playoff at Wrigley Field in which third baseman Gary Gaetti hit the eventual game winning homer. The win propelled the Cubs into the postseason once again with a 90–73 regular season tally. Unfortunately, the bats went cold in October, as manager Jim Riggleman's club batted .183 and scored only four runs en route to being swept by Atlanta. On a positive note, the home run chase between Sosa, McGwire and Ken Griffey, Jr. helped professional baseball to bring in a new crop of fans as well as bringing back some fans who had been disillusioned by the 1994 strike. The Cubs retained many players who experienced career years in '98, and after a fast start in 1999, they collapsed again (starting with being swept at the hands of the cross-town White Sox in mid-June) and finished in the bottom of the division for the next two seasons.
Despite losing fan favorite Grace to free agency, and the lack of production from newcomer Todd Hundley, skipper Don Baylor's Cubs put together a good season in 2001. The season started with Mack Newton being brought in to preach "positive thinking." One of the biggest stories of the season transpired as the club made a midseason deal for Fred McGriff, which was drawn out for nearly a month as McGriff debated waiving his no-trade clause, as the Cubs led the wild card race by 2.5 games in early September. That run died when Preston Wilson hit a three run walk off homer off of closer Tom "Flash" Gordon, which halted the team's momentum. The team was unable to make another serious charge, and finished at 88–74, five games behind both Houston and St. Louis, who tied for first. Sosa had perhaps his finest season and Jon Lieber led the staff with a 20 win season.
The Cubs had high expectations in 2002, but the squad played poorly. On July 5, 2002 the Cubs promoted assistant general manager and player personnel director Jim Hendry to the General Manager position. The club responded by hiring Dusty Baker and by making some major moves in '03. Most notably, they traded with the Pittsburgh Pirates for outfielder Kenny Lofton and third baseman Aramis Ramírez, and rode dominant pitching, led by Kerry Wood and Mark Prior, as the Cubs led the division down the stretch.
After losing an extra-inning game in Game 1, the Cubs rallied and took a 3 games to 1 lead over the Wild Card Florida Marlins in the NLCS. Florida shut the Cubs out in Game 5, but young pitcher Mark Prior led the Cubs in Game 6 as they took a 3–0 lead into the 8th inning and it was at this point when a now-infamous incident took place. Several spectators attempted to catch a foul ball off the bat of Luis Castillo. A Chicago Cubs fan by the name of Steve Bartman, of Northbrook, Illinois, reached for the ball and deflected it away from the glove of Moisés Alou for the second out of the 8th inning. Alou reacted angrily toward the stands, and after the game stated that he would have caught the ball. Alou at one point recanted, saying he would not have been able to make the play, but later said this was just an attempt to make Bartman feel better and believing the whole incident should be forgotten. Interference was not called on the play, as the ball was ruled to be on the spectator side of the wall. Castillo was eventually walked by Prior. Two batters later, and to the chagrin of the packed stadium, Cubs shortstop Alex Gonzalez misplayed an inning ending double play, loading the bases and leading to eight Florida runs and a Marlin victory. Despite sending Kerry Wood to the mound and holding a lead twice, the Cubs ultimately dropped Game 7, and failed to reach the World Series.
In 2004, the Cubs were a consensus pick by most media outlets to win the World Series. The offseason acquisition of Derek Lee (who was acquired in a trade with Florida for Hee-seop Choi) and the return of Greg Maddux only bolstered these expectation. Despite a mid-season deal for Nomar Garciaparra, misfortune struck the Cubs again. They led the Wild Card by 1.5 games over San Francisco and Houston on September 25, and both of those teams lost that day, giving the Cubs a chance at increasing the lead to a commanding 2.5 games with only eight games remaining in the season, but reliever LaTroy Hawkins blew a save to the Mets, and the Cubs lost the game in extra innings, a defeat that seemingly deflated the team, as they proceeded to drop 6 of their last 8 games as the Astros won the Wild Card.
Despite the fact that the Cubs had won 89 games, this fallout was decidedly unlovable, as the Cubs traded superstar Sammy Sosa after he had left the season's final game early and then lied about it publicly. Already a controversial figure in the clubhouse after his corked-bat incident, Sammy's actions alienated much of his once strong fan base as well as the few teammates still on good terms with him, (many teammates grew tired of Sosa playing loud salsa music in the locker room) and possibly tarnished his place in Cubs' lore for years to come. The disappointing season also saw fans start to become frustrated with the constant injuries to ace pitchers Mark Prior and Kerry Wood. Additionally, the '04 season led to the departure of popular commentator Steve Stone, who had become increasingly critical of management during broadcasts and was verbally attacked by reliever Kent Mercker. Things were no better in 2005, despite a career year from first baseman Derrek Lee and the emergence of closer Ryan Dempster. The club struggled and suffered more key injuries, only managing to win 79 games after being picked by many to be a serious contender for the N.L. pennant. In 2006, bottom fell out as the Cubs finished 66–96, last in the NL Central.
After finishing last in the NL Central with 66 wins in 2006, the Cubs re-tooled and went from "worst to first" in 2007. In the offseason they signed Alfonso Soriano to a contract at 8 years for $136 million, and replaced manager Dusty Baker with fiery veteran manager Lou Piniella. After a rough start, which included a brawl between Michael Barrett and Carlos Zambrano, the Cubs overcame the Milwaukee Brewers, who had led the division for most of the season, with winning streaks in June and July, coupled with a pair of dramatic, late-inning wins against the Reds, and ultimately clinched the NL Central with a record of 85–77. The Cubs traded Barrett to the Padres, and later acquired Jason Kendall from Oakland. Kendall was highly successful with his management of the pitching rotation and helped at the plate as well. By September, Geovany Soto became the full-time starter behind the plate, replacing the veteran Kendall. They met Arizona in the NLDS, but controversy followed as Piniella, in a move that has since come under scrutiny, pulled Carlos Zambrano after the sixth inning of a pitcher's duel with D-Backs ace Brandon Webb, to "....save Zambrano for (a potential) Game 4." The Cubs, however, were unable to come through, losing the first game and eventually stranding over 30 baserunners in a 3-game Arizona sweep.
The Cubs successfully defended their National League Central title in 2008, going to the postseason in consecutive years for the first time since 1906–08. The offseason was dominated by three months of unsuccessful trade talks with the Orioles involving 2B Brian Roberts, as well as the signing of Chunichi Dragons star Kosuke Fukudome. The team recorded their 10,000th win in April, while establishing an early division lead. Reed Johnson and Jim Edmonds were added early on and Rich Harden was acquired from the Oakland Athletics in early July. The Cubs headed into the All-Star break with the N.L.'s best record, and tied the league record with eight representatives to the All-Star game, including catcher Geovany Soto, who was named Rookie of the Year. The Cubs took control of the division by sweeping a four-game series in Milwaukee. On September 14, in a game moved to Miller Park due to Hurricane Ike, Zambrano pitched a no-hitter against the Astros, and six days later the team clinched by beating St. Louis at Wrigley. The club ended the season with a 97–64 record and met Los Angeles in the NLDS. The heavily favored Cubs took an early lead in Game 1, but James Loney's grand slam off Ryan Dempster changed the series' momentum. Chicago committed numerous critical errors and were outscored 20–6 in a Dodger sweep, which provided yet another sudden ending.
The Ricketts family acquired a majority interest in the Cubs in 2009, ending the Tribune years. Apparently handcuffed by the Tribune's bankruptcy and the sale of the club to the Ricketts family, the Cubs' quest for a NL Central 3-peat started with notice that there would be less invested into contracts than in previous years. Chicago engaged St. Louis in a see-saw battle for first place into August 2009, but the Cardinals played to a torrid 20–6 pace that month, designating their rivals to battle in the Wild Card race, from which they were eliminated in the season's final week. The Cubs were plagued by injuries in 2009, and were only able to field their Opening Day starting lineup three times the entire season. Third baseman Aramis Ramírez injured his throwing shoulder in an early May game against the Milwaukee Brewers, sidelining him until early July and forcing journeyman players like Mike Fontenot and Aaron Miles into more prominent roles. Additionally, key players like Derrek Lee (who still managed to hit .306 with 35 HR and 111 RBI that season), Alfonso Soriano and Geovany Soto also nursed nagging injuries. The Cubs posted a winning record (83–78) for the third consecutive season, the first time the club had done so since 1972, and a new era of ownership under the Ricketts' family was approved by MLB owners in early October.
Rookie Starlin Castro debuted in early May (2010) as the starting shortstop. However, the club played poorly in the early season, finding themselves 10 games under .500 at the end of June. In addition, long-time ace Carlos Zambrano was pulled from a game against the White Sox on June 25 after a tirade and shoving match with Derrek Lee, and was suspended indefinitely by Jim Hendry, who called the conduct "unacceptable." On August 22, Lou Piniella, who had already announced his retirement at the end of the season, announced that he would leave the Cubs prematurely to take care of his sick mother. Mike Quade took over as the interim manager for the final 37 games of the year. Despite being well out of playoff contention the Cubs went 24–13 under Quade, the best record in baseball during that 37 game stretch, earning Quade to have the interim tag removed on October 19.
Despite trading for pitcher Matt Garza and signing free-agent slugger Carlos Peña, the Cubs finished the 2011 season 20 games under .500 with a record of 71-91. Weeks after the season came to an end, the club was rejuvenated in the form of a new philosophy, as new owner Tom Ricketts signed Theo Epstein away from the Boston Red Sox, naming him club President and giving him a five-year contract worth over $18 million, and subsequently discharged manager Mike Quade. Epstein, a proponent of sabremetrics and one of the architects of two world series titles in Boston brought along Jed Hoyer to fill the role of GM and hired Dale Sveum as manager. Although the team had a dismal 2012 season, losing 101 games (the worst record since 1966) it was largely expected. The youth movement ushered in by Epstein and Hoyer began as longtime fan favorite Kerry Wood retired in May, followed by Ryan Dempster and Geovany Soto being traded to Texas at the All-Star break for a group of minor league prospects headlined by Christian Villanueva. The development of Castro, Anthony Rizzo, Darwin Barney, Brett Jackson and pitcher Jeff Samardzija as well as the replenishing of the minor-league system with prospects such as Javier Baez, Albert Almora, and Jorge Soler became the primary focus of the season, a philosophy which the new management said would carry over at least through the 2013 season.
The 2013 season resulted in much as the same the year before. Shortly before the trade deadline, the Cubs traded Matt Garza to the Texas Rangers for Mike Olt, C. J. Edwards, Neil Ramirez, and Justin Grimm. Three days later, the Cubs sent Alfonso Soriano to the New York Yankees for minor leaguer Corey Black. The mid season fire sale led to another last place finish in the NL Central, finishing with a record of 66-96. Although there was a five-game improvement in the record from the year before, Anthony Rizzo and Starlin Castro seemed to take steps backward in their development. On September 30, 2013, Theo Epstein made the decision to fire manager Dale Sveum after just two seasons at the helm of the Cubs. The regression of several young players was thought to be the main focus point, as the front office said Dale would not be judged based on wins and losses. In two seasons as skipper, Sveum finished with a record of 127-197.
On November 2, 2014, the Cubs announced that Joe Maddon had signed a five-year contract to be the 54th manager in team history. On December 10, 2014, Maddon announced that the team had signed free agent Jon Lester to a 6-year, $155 million contract. Many other trades and acquisitions occurred during the off season. The opening day lineup for the Cubs contained five new players including rookie right fielder Jorge Soler. Rookies Kris Bryant and Addison Russell were in the starting lineup by mid-April, and rookie Kyle Schwarber was added in mid-June. The Cubs finished the 2015 season with a record of 97–65, third best in the majors. On October 7, in the 2015 National League Wild Card Game, Jake Arrieta pitched a complete game shutout and the Cubs defeated the Pittsburgh Pirates 4–0.
On September 23, 1908, the Cubs and New York Giants were involved in a tight pennant race. The two clubs were tied in the bottom of the ninth inning at the Polo Grounds, and N.Y. had runners on first and third and two outs when Al Bridwell singled, scoring Moose McCormick from third with the Giants' apparent winning run, but the runner on first base, rookie Fred Merkle, left the field without touching second base. As fans swarmed the field, Cub infielder Johnny Evers retrieved the ball and touched second. Since there were two outs, a forceout was called at second base, ending the inning and the game. Because of the tie the Giants and Cubs ended up tied for first place. The Giants lost the ensuing one-game playoff and the Cubs went on to the World Series.
On October 1, 1932, in game three of the World Series between the Cubs and the New York Yankees, Babe Ruth allegedly stepped to the plate, pointed his finger to Wrigley Field's center field bleachers and hit a long home run to center. There is speculation as to whether the "facts" surrounding the story are true or not, but nevertheless Ruth did help the Yankees secure a World Series win that year and the home run accounted for his 15th and last home run in the post season before he retired in 1935.
Hack Wilson set a record of 56 home-runs and 190 runs-batted-in in 1930, breaking Lou Gehrig's MLB record of 176 RBI. (In 1999, a long-lost extra RBI mistakenly credited to Charlie Grimm had been found by Cooperstown researcher Cliff Kachline and verified by historian Jerome Holtzman, increasing the record number to 191.) As of 2014 the record still stands, with no serious threats coming since Gehrig (184) and Hank Greenberg (183) in the same era. The closest anyone has come to the mark in the last 75 years was Manny Ramirez's 165 RBI in 1999. In addition to the RBI record, Wilson 56 home-runs stood as the National League record until 1998, when Sammy Sosa and Mark McGwire hit 66 and 70, respectively. Wilson was named "Most Useful" player that year by the Baseball Writers' Association of America, as the official N.L. Most Valuable Player Award was not awarded until the next season.
On April 25, 1976, at Dodger Stadium, father-and-son protestors ran into the outfield and tried to set fire to a U.S. flag. When Cubs outfielder Rick Monday noticed the flag on the ground and the man and boy fumbling with matches and lighter fluid, he dashed over and snatched the flag to thunderous applause. When he came up to bat in the next half-inning, he got a standing ovation from the crowd and the stadium titantron flashed the message, "RICK MONDAY... YOU MADE A GREAT PLAY..." Monday later said, "If you're going to burn the flag, don't do it around me. I've been to too many veterans' hospitals and seen too many broken bodies of guys who tried to protect it."
In June, 1998 Sammy Sosa exploded into the pursuit of Roger Maris' home run record. Sosa had 13 home runs entering the month, representing less than half of Mark McGwire's total. Sosa had his first of four multi-home run games that month on June 1, and went on to break Rudy York's record with 20 home runs in the month, a record that still stands. By the end of his historic month, the outfielder's 33 home runs tied him with Ken Griffey, Jr. and left him only four behind McGwire's 37. Sosa finished with 66 and won the NL MVP Award.
On April 23, 2008, against the Colorado Rockies, the Cubs recorded the 10,000th regular-season win in their franchise's history dating back to the beginning of the National League in 1876. The Cubs reached the milestone with an overall National League record of 10,000-9,465. Chicago was only the second club in Major League Baseball history to attain this milestone, the first having been the San Francisco Giants in mid-season 2005. The Cubs, however, hold the mark for victories for a team in a single city. The Chicago club's 77–77 record in the National Association (1871, 1874–1875) is not included in MLB record keeping. Post-season series are also not included in the totals. To honor the milestone, the Cubs flew an extra white flag displaying "10,000" in blue, along with the customary "W" flag.
In only his third career start, Kerry Wood struck out 20 batters against Houston on May 6, 1998. This is the franchise record and tied for the Major League record for the most strikeouts in one game by one pitcher (the only other pitcher to strike out 20 batters in a nine-inning game was Roger Clemens, who achieved it twice). The game is often considered the most dominant pitching performance of all time. Interestingly, Wood's first pitch struck home plate umpire Jerry Meals in the facemask. Wood then struck out the first five batters he faced. Wood hit one batter, Craig Biggio, and allowed one hit, a scratch single by Ricky Gutiérrez off third baseman Kevin Orie's glove. The play was nearly scored an error, which would have given Wood a no-hitter.
The Chicago Cubs have not won a World Series championship since 1908, and have not appeared in the Fall Classic since 1945, although between their postseason appearance in 1984 and their most recent in 2015, they have made the postseason seven times. 107 seasons is the longest championship drought in all four of the major North American professional sports leagues, which also includes the National Football League (NFL), the National Basketball Association (NBA), and the National Hockey League (NHL). In fact, the Cubs' last World Series title occurred before those other three leagues even existed, and even the Cubs' last World Series appearance predates the founding of the NBA. The much publicized drought was concurrent to championship droughts by the Boston Red Sox and the Chicago White Sox, who both had over 80 years between championships. It is this unfortunate distinction that has led to the club often being known as "The Lovable Losers." The team was one win away from breaking what is often called the "Curse of the Billy Goat" in 1984 and 2003 (Steve Bartman incident), but was unable get the victory that would send it to the World Series.
On May 11, 2000, Glenallen Hill, facing Brewers starter Steve Woodard, became the first, and thus far only player, to hit a pitched ball onto the roof of a five-story residential building across Waveland Ave, beyond Wrigley Field's left field wall. The shot was estimated at well over 500 feet (150 m), but the Cubs fell to Milwaukee 12–8. No batted ball has ever hit the center field scoreboard, although the original "Slammin' Sammy", golfer Sam Snead, hit it with a golf ball in an exhibition in the 1950s. In 1948, Bill Nicholson barely missed the scoreboard when he launched a home run ball onto Sheffield Avenue and in 1959, Roberto Clemente came even closer with a home run ball hit onto Waveland Avenue. In 2001, a Sammy Sosa shot landed across Waveland and bounced a block down Kenmore Avenue. Dave Kingman hit a shot in 1979 that hit the third porch roof on the east side of Kenmore, estimated at 555 feet (169 m), and is regarded as the longest home run in Wrigley Field history. On May 26, 2015, the Cubs rookie third baseman, Kris Bryant, hit a homerun that traveled an estimated 477 feet (145 m) off the park's new videoboard in left field. Later the same year, he hit a homer that traveled 495 feet (151 m) that also ricocheted off of the videoboard On October 13, 2015, Kyle Schwarber's 438-foot home run landed on the equally new right field videoboard.
Before signing a developmental agreement with the Kane County Cougars in 2012, the Cubs had a Class A minor league affiliation on two occasions with the Peoria Chiefs (1985–1995 and 2004–2012). Ryne Sandberg managed the Chiefs from 2006 to 2010. In the period between those associations with the Chiefs the club had affiliations with the Dayton Dragons and Lansing Lugnuts. The Lugnuts were often affectionately referred to by Chip Caray as "Steve Stone's favorite team." The 2007 developmental contract with the Tennessee Smokies was preceded by Double A affiliations with the Orlando Cubs and West Tenn Diamond Jaxx. On September 16, 2014 the Cubs announced a move of their top Class A affiliate from Daytona in the Florida State League to Myrtle Beach in the Carolina League for the 2015 season. Two days later, on the 18th, the Cubs signed a 4-year player development contract with the South Bend Silver Hawks of the Midwest League, ending their brief relationship with the Kane County Cougars and shortly thereafter renaming the Silver Hawks the South Bend Cubs.
The Chicago White Stockings, (today's Chicago Cubs), began spring training in Hot Springs, Arkansas in 1886. President Albert Spalding (founder of Spalding Sporting Goods) and player/manager Cap Anson brought their players to Hot Springs and played at the Hot Springs Baseball Grounds. The concept was for the players to have training and fitness before the start of the regular season. After the White Stockings had a successful season in 1886, winning the National League Pennant, other teams began bringing their players to "spring training".  The Chicago Cubs, St. Louis Browns, New York Yankees, St. Louis Cardinals, Cleveland Spiders, Detroit Tigers, Pittsburgh Pirates, Cincinnati Reds, New York Highlanders, Brooklyn Dodgers and Boston Red Sox were among the early squads to arrive. Whittington Park (1894) and later Majestic Park (1909) and Fogel Field (1912) were all built in Hot Springs specifically to host Major League teams. 
The Cubs' current spring training facility is located in Sloan Park in |Mesa, Arizona, where they play in the Cactus League. The park seats 15,000, making it Major League baseball's largest spring training facility by capacity. The Cubs annually sell out most of their games both at home and on the road. Before Sloan Park opened in 2014, the team played games at HoHoKam Park - Dwight Patterson Field from 1979. "HoHoKam" is literally translated from Native American as "those who vanished." The North Siders have called Mesa their spring home for most seasons since 1952.
In addition to Mesa, the club has held spring training in Hot Springs, Arkansas (1886, 1896–1900), (1909–1910) New Orleans (1870, 1907, 1911–1912); Champaign, Illinois (1901–02, 1906); Los Angeles (1903–04, 1948–1949), Santa Monica, California (1905); French Lick, Indiana (1908, 1943–1945); Tampa, Florida (1913–1916); Pasadena, California (1917–1921); Santa Catalina Island, California (1922–1942, 1946–1947, 1950–1951); Rendezvous Park in Mesa (1952–1965); Blair Field in Long Beach, California (1966); and Scottsdale, Arizona (1967–1978).
The curious location on Catalina Island stemmed from Cubs owner William Wrigley Jr.'s then-majority interest in the island in 1919. Wrigley constructed a ballpark on the island to house the Cubs in spring training: it was built to the same dimensions as Wrigley Field. (The ballpark is long gone, but a clubhouse built by Wrigley to house the Cubs exists as the Catalina County Club.) However, by 1951 the team chose to leave Catalina Island and spring training was shifted to Mesa, Arizona. The Cubs' 30-year association with Catalina is chronicled in the book, The Cubs on Catalina, by Jim Vitti . . . which was named International 'Book of the Year' by The Sporting News.
The former location in Mesa is actually the second HoHoKam Park; the first was built in 1976 as the spring-training home of the Oakland Athletics who left the park in 1979. Apart from HoHoKam Park and Sloan Park the Cubs also have another Mesa training facility called Fitch Park, this complex provides 25,000 square feet (2,300 m2) of team facilities, including major league clubhouse, four practice fields, one practice infield, enclosed batting tunnels, batting cages, a maintenance facility, and administrative offices for the Cubs.
Jack Brickhouse manned the Cubs radio and especially the TV booth for parts of five decades, the 34-season span from 1948 to 1981. He covered the games with a level of enthusiasm that often seemed unjustified by the team's poor performance on the field for many of those years. His trademark call "Hey Hey!" always followed a home run. That expression is spelled out in large letters vertically on both foul pole screens at Wrigley Field. "Whoo-boy!" and "Wheeee!" and "Oh, brother!" were among his other pet expressions. When he approached retirement age, he personally recommended his successor.
Harry Caray's stamp on the team is perhaps even deeper than that of Brickhouse, although his 17-year tenure, from 1982 to 1997, was half as long. First, Caray had already become a well-known Chicago figure by broadcasting White Sox games for a decade, after having been a St Louis Cardinals icon for 25 years. Caray also had the benefit of being in the booth during the NL East title run in 1984, which was widely seen due to WGN's status as a cable-TV superstation. His trademark call of "Holy Cow!" and his enthusiastic singing of "Take me out to the ballgame" during the 7th inning stretch (as he had done with the White Sox) made Caray a fan favorite both locally and nationally.
Caray had lively discussions with commentator Steve Stone, who was hand-picked by Harry himself, and producer Arne Harris. Caray often playfully quarreled with Stone over Stone's cigar and why Stone was single, while Stone would counter with poking fun at Harry being "under the influence." Stone disclosed in his book "Where's Harry" that most of this "arguing" was staged, and usually a ploy developed by Harry himself to add flavor to the broadcast. The Cubs still have a "guest conductor", usually a celebrity, lead the crowd in singing "Take me out to the ballgame" during the 7th inning stretch to honor Caray's memory.
In 1981, after 6 decades under the Wrigley family, the Cubs were purchased by Tribune Company for $20,500,000. Tribune, owners of the Chicago Tribune, Los Angeles Times, WGN Television, WGN Radio and many other media outlets, controlled the club until December 2007, when Sam Zell completed his purchase of the entire Tribune organization and announced his intention to sell the baseball team. After a nearly two-year process which involved potential buyers such as Mark Cuban and a group led by Hank Aaron, a family trust of TD Ameritrade founder Joe Ricketts won the bidding process as the 2009 season came to a close. Ultimately, the sale was unanimously approved by MLB owners and the Ricketts family took control on October 27, 2009.
"Baseball's Sad Lexicon," also known as "Tinker to Evers to Chance" after its refrain, is a 1910 baseball poem by Franklin Pierce Adams. The poem is presented as a single, rueful stanza from the point of view of a New York Giants fan seeing the talented Chicago Cubs infield of shortstop Joe Tinker, second baseman Johnny Evers, and first baseman Frank Chance complete a double play. The trio began playing together with the Cubs in 1902, and formed a double play combination that lasted through April 1912. The Cubs won the pennant four times between 1906 and 1910, often defeating the Giants en route to the World Series.
The official Cubs team mascot is a young bear cub, named Clark, described by the team's press release as a young and friendly Cub. Clark made his debut at Advocate Health Care on January 13, 2014, the same day as the press release announcing his installation as the club's first ever official physical mascot. The bear cub itself was used in the clubs since the early 1900s and was the inspiration of the Chicago Staleys changing their team's name to the Chicago Bears, due to the Cubs allowing the football team to play at Wrigley Field in the 1930s.
The Cubs had no official physical mascot prior to Clark, though a man in a 'polar bear' looking outfit, called "The Bear-man" (or Beeman), which was mildly popular with the fans, paraded the stands briefly in the early 1990s. There is no record of whether or not he was just a fan in a costume or employed by the club. Through the 2013 season, there were "Cubbie-bear" mascots outside of Wrigley on game day, but none are employed by the team. They pose for pictures with fans for tips. The most notable of these was "Billy Cub" who worked outside of the stadium until for over 6 years until July 2013, when the club asked him to stop. Billy Cub, who is played by fan John Paul Weier, had unsuccessfully petitioned the team to become the official mascot.
Another unofficial but much more well-known mascot is Ronnie "Woo Woo" Wickers who is a longtime fan and local celebrity in the Chicago area. He is known to Wrigley Field visitors for his idiosyncratic cheers at baseball games, generally punctuated with an exclamatory "Woo!" (e.g., "Cubs, woo! Cubs, woo! Big-Z, woo! Zambrano, woo! Cubs, woo!") Longtime Cubs announcer Harry Caray dubbed Wickers "Leather Lungs" for his ability to shout for hours at a time. He is not employed by the team, although the club has on two separate occasions allowed him into the broadcast booth and allow him some degree of freedom once he purchases or is given a ticket by fans to get into the games. He is largely allowed to roam the park and interact with fans by Wrigley Field security.
Located in Chicago's Lake View neighborhood, Wrigley Field sits on an irregular block bounded by Clark and Addison Streets and Waveland and Sheffield Avenues. The area surrounding the ballpark is typically referred to as Wrigleyville. There is a dense collection of sports bars and restaurants in the area, most with baseball inspired themes, including Sluggers, Murphy's Bleachers and The Cubby Bear. Many of the apartment buildings surrounding Wrigley Field on Waveland and Sheffield Avenues have built bleachers on their rooftops for fans to view games and other sell space for advertisement. One building on Sheffield Avenue has a sign atop its roof which says "Eamus Catuli!" which is Latin for "Let's Go Cubs!" and another chronicles the time since the last Division title, pennant, and World Series championship. The 02 denotes two years since the 2008 NL Central title, 65 years since the 1945 pennant and 102 years since the 1908 World Series championship. On game days, many residents rent out their yards and driveways to people looking for parking spots. The uniqueness of the neighborhood itself has ingrained itself into the culture of the Chicago Cubs as well as the Wrigleyville neighborhood, and has led to being used for concerts and other sporting events, such as the 2010 NHL Winter Classic between the Chicago Blackhawks and Detroit Red Wings, as well as a 2010 NCAA men's football game between the Northwestern Wildcats and Illinois Fighting Illini.
In 2013, Tom Ricketts and team president Crane Kenney unveiled plans for a five-year, $575 million privately funded renovation of Wrigley Field. Called the 1060 Project, the proposed plans included vast improvements to the stadium's facade, infrastructure, restrooms, concourses, suites, press box, bullpens, and clubhouses, as well as a 6,000-square foot jumbotron to be added in the left field bleachers, batting tunnels, a 3,000-square-foot video board in right field, and, eventually, an adjacent hotel, plaza, and office-retail complex. In previously years mostly all efforts to conduct any large-scale renovations to the field had been opposed by the city, former mayor Richard M. Daley (a staunch White Sox fan), and especially the rooftop owners.
The "Bleacher Bums" is a name given to fans, many of whom spend much of the day heckling, who sit in the bleacher section at Wrigley Field. Initially, the group was called "bums" because it referred to a group of fans who were at most games, and since those games were all day games, it was assumed they did not work. Many of those fans were, and are still, students at Chicago area colleges, such as DePaul University, Loyola, Northwestern University, and Illinois-Chicago. A Broadway play, starring Joe Mantegna, Dennis Farina, Dennis Franz, and James Belushi ran for years and was based on a group of Cub fans who frequented the club's games. The group was started in 1967 by dedicated fans Ron Grousl, Tom Nall and "mad bugler" Mike Murphy, who was a sports radio host during mid days on Chicago-based WSCR AM 670 "The Score". Murphy alleges that Grousl started the Wrigley tradition of throwing back opposing teams' home run balls. The current group is headed by Derek Schaul (Derek the Five Dollar Kid). Prior to the 2006 season, they were updated, with new shops and private bar (The Batter's Eye) being added, and Bud Light bought naming rights to the bleacher section, dubbing them the Bud Light Bleachers. Bleachers at Wrigley are general admission, except during the playoffs. The bleachers have been referred to as the "World's Largest Beer Garden." A popular T-shirt (sold inside the park and licensed by the club) which says "Wrigley Bleachers" on the front and the phrase "Shut Up and Drink Your Beer" on the reverse fuels this stereotype.
In 1975, a group of Chicago Cubs fans based in Washington, D.C. formed the Emil Verban Society. The society is a select club of high profile Cub fans, currently headed by Illinois Senator Dick Durbin which is named for Emil Verban, who in three seasons with the Cubs in the 1940s batted .280 with 39 runs batted in and one home run. Verban was picked as the epitome of a Cub player, explains columnist George Will, because "He exemplified mediocrity under pressure, he was competent but obscure and typifying of the work ethics." Verban initially believed he was being ridiculed, but his ill feeling disappeared several years later when he was flown to Washington to meet President Ronald Reagan, also a society member, at the White House. Hillary Clinton, Jim Belushi, Joe Mantegna, Rahm Emanuel, Dick Cheney and many others have been included among its membership.
During the summer of 1969, a Chicago studio group produced a single record called "Hey Hey! Holy Mackerel! (The Cubs Song)" whose title and lyrics incorporated the catch-phrases of the respective TV and radio announcers for the Cubs, Jack Brickhouse and Vince Lloyd. Several members of the Cubs recorded an album called Cub Power which contained a cover of the song. The song received a good deal of local airplay that summer, associating it very strongly with that bittersweet season. It was played much less frequently thereafter, although it remained an unofficial Cubs theme song for some years after.
An album entitled Take Me Out to a Cubs Game was released in 2008. It is a collection of 17 songs and other recordings related to the team, including Harry Caray's final performance of "Take Me Out to the Ball Game" on September 21, 1997, the Steve Goodman song mentioned above, and a newly recorded rendition of "Talkin' Baseball" (subtitled "Baseball and the Cubs") by Terry Cashman. The album was produced in celebration of the 100th anniversary of the Cubs' 1908 World Series victory and contains sounds and songs of the Cubs and Wrigley Field.
The 1989 film Back to the Future Part II depicts the Chicago Cubs defeating a baseball team from Miami in the 2015 World Series, ending the longest championship drought in all four of the major North American professional sports leagues. In 2015, the Miami Marlins failed to make the playoffs and were able to make it to the 2015 National League Wild Card round and move on to the 2015 National League Championship Series by October 21, 2015, the date where protagonist Marty McFly traveled to the future in the film. However, it was on October 21 that the Cubs were swept by the New York Mets in the NLCS.
Everton were founder members of the Premier League in 1992, but struggled to find the right manager. Howard Kendall had returned in 1990 but could not repeat his previous success, while his successor, Mike Walker, was statistically the least successful Everton manager to date. When former Everton player Joe Royle took over in 1994 the club's form started to improve; his first game in charge was a 2–0 victory over derby rivals Liverpool. Royle dragged Everton clear of relegation, leading the club to the FA Cup for the fifth time in its history, defeating Manchester United 1–0 in the final.
The Tower has been inextricably linked with the Everton area since its construction in 1787. It was originally used as a bridewell to incarcerate mainly drunks and minor criminals, and it still stands today on Everton Brow in Netherfield Road. The tower was accompanied by two laurel wreaths on either side and, according to the College of Arms in London, Kelly chose to include the laurels as they were the sign of winners. The crest was accompanied by the club motto, "Nil Satis Nisi Optimum", meaning "Nothing but the best is good enough".
On matchdays, in a tradition going back to 1962, players walk out to the theme tune to Z-Cars, named "Johnny Todd", a traditional Liverpool children's song collected in 1890 by Frank Kidson which tells the story of a sailor betrayed by his lover while away at sea, although on two separate occasions in the 1994, they ran out to different songs. In August 1994, the club played 2 Unlimited's song "Get Ready For This", and a month later, a reworking of the Creedence Clearwater Revival classic "Bad Moon Rising". Both were met with complete disapproval by Everton fans.
Everton hold the record for the most seasons in England's top tier (Division One/Premier League), at 111 seasons out of 114 as of 2014–15 (the club played in Division 2 in 1930–31 and from 1951–54). They are one of seven teams to have played all 22 seasons of the Premier League since its inception in August 1992 – the others being Arsenal, Aston Villa, Chelsea, Liverpool, Manchester United, and Tottenham Hotspur. Everton against Aston Villa is the most played fixture in England's top flight, as of the 2012–13 season the two founder members of the Football League have played a record 196 league games.
Formed in 1878, Everton were founding members of The Football League in 1888 and won their first league championship two seasons later. Following four league titles and two FA Cup wins, Everton experienced a lull in the immediate post World War Two period until a revival in the 1960s which saw the club win two league championships and an FA Cup. The mid-1980s represented their most recent period of sustained success, with two League Championship successes, an FA Cup, and the 1985 European Cup Winners' Cup. The club's most recent major trophy was the 1995 FA Cup. The club's supporters are known as Evertonians.
Everton were relegated to the Second Division two years later during internal turmoil at the club. However, the club was promoted at the first attempt scoring a record number of goals in the second division. On return to the top flight in 1931–32, Everton wasted no time in reaffirming their status and won a fourth League title at the first opportunity. Everton also won their second FA Cup in 1933 with a 3–0 win against Manchester City in the final. The era ended in 1938–39 with a fifth League title.
The Everton board finally ran out of patience with Smith and he was sacked in March 2002 after an FA Cup exit at Middlesbrough, with Everton in real danger of relegation. David Moyes, was his replacement and guided Everton to a safe finish in fifteenth place. In 2002–03 Everton finished seventh, their highest finish since 1996. A fourth-place finish in 2004–05, ensured Everton qualified for the Champions League qualifying round. The team failed to make it through to the Champions League group stage and were then eliminated from the UEFA Cup. Everton qualified for the 2007–08 and 2008–09 UEFA Cup competitions and they were runners-up in the 2009 FA Cup Final.
In May 2013, the club launched a new crest to improve the reproducibility of the design in print and broadcast media, particularly on a small scale. Critics[who?] suggested that it was external pressure from sports manufacturers Nike, Inc. that evoked the redesign as the number of colours has been reduced and the radial effect have been removed, making the kit more cost efficient to reproduce.[citation needed] The redesign was poorly received by supporters, with a poll on an Everton fan site registering a 91% negative response to the crest. A protest petition reached over 22,000 signatures before the club offered an apology and announced a new crest would be created for the 2014–15 season with an emphasis on fan consultation. Shortly afterwards, the Head of Marketing left the club.
Everton originally played in the southeast corner of Stanley Park, which was the site for the new Liverpool F.C. stadium, with the first official match taking place in 1879. In 1882, a man named J. Cruitt donated land at Priory Road which became the club's home before they moved to Anfield, which was Everton's home until 1892. At this time, a dispute of how the club was to be owned and run emerged with Anfield's owner and Everton's chairman, John Houlding. A dispute between Houlding and the club's committee over how the club should be run, led to Houlding attempting to gain full control of the club by registering the company, "Everton F.C. and Athletic Grounds Ltd". In response, Everton left Anfield for a new ground, Goodison Park, where the club have played ever since. Houlding attempted to take over Everton's name, colours, fixtures and league position, but was denied by The Football Association. Instead, Houlding formed a new club, Liverpool F.C.
There have been indications since 1996 that Everton will move to a new stadium. The original plan was for a new 60,000-seat stadium to be built, but in 2000 a proposal was submitted to build a 55,000 seat stadium as part of the King's Dock regeneration. This was unsuccessful as Everton failed to generate the £30 million needed for a half stake in the stadium project, with the city council rejecting the proposal in 2003. Late in 2004, driven by Liverpool Council and the Northwest Development Corporation, the club entered talks with Liverpool F.C. about sharing a proposed stadium on Stanley Park. Negotiations broke down as Everton failed to raise 50% of the costs. On 11 January 2005, Liverpool announced that ground-sharing was not a possibility, proceeding to plan their own Stanley Park Stadium.
Everton have a large fanbase, with the eighth highest average attendance in the Premier League in the 2008–09 season. The majority of Everton's matchday support comes from the North West of England, primarily Merseyside, Cheshire, West Lancashire and parts of Western Greater Manchester along with many fans who travel from North Wales and Ireland. Within the city of Liverpool support for Everton and city rivals Liverpool is not determined by geographical basis with supporters mixed across the city. However Everton's support heartland is traditionally based in the North West of the city and in the southern parts of Sefton. Everton also have many supporters' clubs worldwide, in places such as North America, Singapore, Indonesia, Lebanon, Malaysia, Thailand, and Australia. The official supporters club is FOREVERTON, and there are also several fanzines including When Skies are Grey and Speke from the Harbour, which are sold around Goodison Park on match days.
Current manager, Roberto Martínez, is the fourteenth permanent holder of the position since it was established in 1939. There have also been four caretaker managers, and before 1939 the team was selected by either the club secretary or by committee. The club's longest-serving manager has been Harry Catterick, who was in charge of the team from 1961–73, taking in 594 first team matches. The Everton manager to win most domestic and international trophies is Howard Kendall, who won two Division One championships, the 1984 FA Cup, the 1984 UEFA Cup Winners' Cup, and three Charity Shields.
Everton's second successful era started when Harry Catterick was made manager in 1961. In 1962–63, his second season in charge, Everton won the League title and in 1966 the FA Cup followed with a 3–2 win over Sheffield Wednesday. Everton again reached the final in 1968, but this time were unable to overcome West Bromwich Albion at Wembley. Two seasons later in 1969–70, Everton won the League championship, nine points clear of nearest rivals Leeds United. During this period, Everton were the first English club to achieve five consecutive years in European competitions—seasons 1961–62 to 1966–67.
On 16 June 2006, it was announced that Everton had entered into talks with Knowsley Council and Tesco over the possibility of building a new 55,000 seat stadium, expandable to over 60,000, in Kirkby. The club took the unusual move of giving its supporters a say in the club's future by holding a ballot on the proposal, finding a split of 59% to 41% in favour. Opponents to the plan included other local councils concerned by the effect of a large Tesco store being built as part of the development, and a group of fans demanding that Everton should remain within the city boundaries of Liverpool.
Everton regularly take large numbers away from home both domestically and in European fixtures. The club implements a loyalty points scheme offering the first opportunity to purchase away tickets to season ticket holders who have attended the most away matches. Everton often sell out the full allocation in away grounds and tickets sell particularly well for North West England away matches. In October 2009, Everton took 7,000 travelling fans to Benfica, their largest ever away crowd in Europe since the 1985 European Cup Winners' Cup Final.
Everton F.C. is a limited company with the board of directors holding a majority of the shares. The club's most recent accounts, from May 2014, show a net total debt of £28.1 million, with a turnover of £120.5 million and a profit of £28.2 million. The club's overdraft with Barclays Bank is secured against the Premier League's "Basic Award Fund", a guaranteed sum given to clubs for competing in the Premier League. Everton agreed a long-term loan of £30 million with Bear Stearns and Prudential plc in 2002 over the duration of 25 years; a consolidation of debts at the time as well as a source of capital for new player acquisitions. Goodison Park is secured as collateral.
Everton's biggest rivalry is with neighbours Liverpool, against whom they contest the Merseyside derby. The Merseyside derby is usually a sellout fixture, and has been known as the "friendly derby" because both sets of fans can often be seen side by side red and blue inside the stadium both at Anfield and Goodison Park. Recently on the field, matches tend to be extremely stormy affairs; the derby has had more red cards than any other fixture in Premiership history. The rivalry stems from an internal dispute between Everton officials and the owners of Anfield, which was then Everton's home ground, resulting in Everton moving to Goodison Park, and the subsequent formation of Liverpool F.C., in 1892.
Neville Southall holds the record for the most Everton appearances, having played 751 first-team matches between 1981 and 1997, and previously held the record for the most league clean sheets during a season (15). During the 2008–09 season, this record was beaten by American goalkeeper Tim Howard (17). The late centre half and former captain Brian Labone comes second, having played 534 times. The longest serving player is Goalkeeper Ted Sagar who played for 23 years between 1929 and 1953, both sides of the Second World War, making a total of 495 appearances. The club's top goalscorer, with 383 goals in all competitions, is Dixie Dean; the second-highest goalscorer is Graeme Sharp with 159. Dean still holds the English national record of most goals in a season, with 60.
The record attendance for an Everton home match is 78,299 against Liverpool on 18 September 1948. Amazingly, there was only 1 injury at this game-Tom Fleetwood was hit on the head by a coin thrown from the crowd whilst he marched around the perimeter with St Edward's Orphanage Band, playing the cornet. Goodison Park, like all major English football grounds since the recommendations of the Taylor Report were implemented, is now an all-seater and only holds just under 40,000, meaning it is unlikely that this attendance record will ever be broken at Goodison. Everton's record transfer paid was to Chelsea for Belgian forward Romelu Lukaku for a sum of £28m. Everton bought the player after he played the previous year with the team on loan.
The club also owned and operated a professional basketball team, by the name of Everton Tigers, who compete in the elite British Basketball League. The team was launched in the summer of 2007 as part of the clubs' Community programme, and play their home games at the Greenbank Sports Academy. The team was an amalgam of the Toxteth Tigers community youth programme which started in 1968. The team quickly became one of the most successful in the league winning the BBL Cup in 2009 and the play-offs in 2010. However Everton withdrew funding before the 2010–11 season and the team was re launched as the Mersey Tigers.
Everton also have links with Chilean team Everton de Viña del Mar who were named after the English club. On 4 August 2010, the two Evertons played each other in a friendly named the Copa Hermandad at Goodison Park to mark the centenary of the Chilean team, an occasion organised by The Ruleteros Society, a society founded to promote connections between the two clubs. Other Evertons exist in Rosario in Colonia Department, Uruguay, La Plata, and Río Cuarto in Argentina, Elk Grove, California in the United States, and in Cork, Ireland.
The club have entered the UK pop charts on four occasions under different titles during the 1980s and 1990s when many clubs released a song to mark their reaching the FA Cup Final. "The Boys in Blue", released in 1984, peaked at number 82. The following year the club scored their biggest hit when "Here We Go" peaked at 14. In 1986 the club released "Everybody's Cheering The Blues" which reached number 83. "All Together Now", a reworking of a song by Merseyside band The Farm, was released for the 1995 FA Cup Final and reached number 27. When the club next reached the 2009 FA Cup Final, the tradition had passed into history and no song was released.
The cup triumph was also Everton's passport to the Cup Winners' Cup—their first European campaign in the post-Heysel era. Progress under Joe Royle continued in 1995–96 as they climbed to sixth place in the Premiership. A fifteenth-place finish the following season saw Royle resign towards the end of the campaign, to be temporarily replaced by club captain, Dave Watson. Howard Kendall was appointed Everton manager for the third time in 1997, but the appointment proved unsuccessful as Everton finished seventeenth in the Premiership; only avoiding relegation due to their superior goal difference over Bolton Wanderers. Former Rangers manager Walter Smith then took over from Kendall in the summer of 1998 but only managed three successive finishes in the bottom half of the table.
Everton have had many other nicknames over the years. When the black kit was worn Everton were nicknamed "The Black Watch", after the famous army regiment. Since going blue in 1901, Everton have been given the simple nickname "The Blues". Everton's attractive style of play led to Steve Bloomer calling the team "scientific" in 1928, which is thought to have inspired the nickname "The School of Science". The battling 1995 FA Cup winning side were known as "The Dogs of War". When David Moyes arrived as manager he proclaimed Everton as "The People's Club", which has been adopted as a semi-official club nickname.
The Qing dynasty (Chinese: 清朝; pinyin: Qīng Cháo; Wade–Giles: Ch'ing Ch'ao; IPA: [tɕʰíŋ tʂʰɑ̌ʊ̯]), officially the Great Qing (Chinese: 大清; pinyin: Dà Qīng), also called the Empire of the Great Qing, or the Manchu dynasty, was the last imperial dynasty of China, ruling from 1644 to 1912 with a brief, abortive restoration in 1917. It was preceded by the Ming dynasty and succeeded by the Republic of China. The Qing multi-cultural empire lasted almost three centuries and formed the territorial base for the modern Chinese state.
The dynasty was founded by the Jurchen Aisin Gioro clan in Manchuria. In the late sixteenth century, Nurhaci, originally a Ming vassal, began organizing Jurchen clans into "Banners", military-social units. Nurhaci formed these clans into a unified entity, the subjects of which became known collectively as the Manchu people. By 1636, his son Hong Taiji began driving Ming forces out of Liaodong and declared a new dynasty, the Qing. In 1644, peasant rebels led by Li Zicheng conquered the Ming capital Beijing. Rather than serve them, Ming general Wu Sangui made an alliance with the Manchus and opened the Shanhai Pass to the Banner Armies led by Prince Dorgon, who defeated the rebels and seized Beijing. The conquest of China proper was not completed until 1683 under the Kangxi Emperor (r. 1661–1722). The Ten Great Campaigns of the Qianlong Emperor from the 1750s to the 1790s extended Qing control into Central Asia. While the early rulers maintained their Manchu ways, and while their official title was Emperor they were known as khans to the Mongols and patronized Tibetan Buddhism, they governed using Confucian styles and institutions of bureaucratic government. They retained the imperial examinations to recruit Han Chinese to work under or in parallel with Manchus. They also adapted the ideals of the tributary system in international relations, and in places such as Taiwan, the Qing so-called internal foreign policy closely resembled colonial policy and control.
The reign of the Qianlong Emperor (1735–1796) saw the apogee and initial decline in prosperity and imperial control. The population rose to some 400 million, but taxes and government revenues were fixed at a low rate, virtually guaranteeing eventual fiscal crisis. Corruption set in, rebels tested government legitimacy, and ruling elites did not change their mindsets in the face of changes in the world system. Following the Opium War, European powers imposed unequal treaties, free trade, extraterritoriality and treaty ports under foreign control. The Taiping Rebellion (1850–64) and Dungan Revolt (1862–77) in Central Asia led to the deaths of some 20 million people. In spite of these disasters, in the Tongzhi Restoration of the 1860s, Han Chinese elites rallied to the defense of the Confucian order and the Qing rulers. The initial gains in the Self-Strengthening Movement were destroyed in the First Sino-Japanese War of 1895, in which the Qing lost its influence over Korea and the possession of Taiwan. New Armies were organized, but the ambitious Hundred Days' Reform of 1898 was turned back by Empress Dowager Cixi, a ruthless but capable leader. When, in response to the violently anti-foreign Yihetuan ("Boxers"), foreign powers invaded China, the Empress Dowager declared war on them, leading to defeat and the flight of the Imperial Court to Xi'an.
After agreeing to sign the Boxer Protocol the government then initiated unprecedented fiscal and administrative reforms, including elections, a new legal code, and abolition of the examination system. Sun Yat-sen and other revolutionaries competed with reformers such as Liang Qichao and monarchists such as Kang Youwei to transform the Qing empire into a modern nation. After the death of Empress Dowager Cixi and the Guangxu Emperor in 1908, the hardline Manchu court alienated reformers and local elites alike. Local uprisings starting on October 11, 1911 led to the Xinhai Revolution. Puyi, the last emperor, abdicated on February 12, 1912.
Nurhaci declared himself the "Bright Khan" of the Later Jin (lit. "gold") state in honor both of the 12–13th century Jurchen Jin dynasty and of his Aisin Gioro clan (Aisin being Manchu for the Chinese 金 (jīn, "gold")).  His son Hong Taiji renamed the dynasty Great Qing in 1636. There are competing explanations on the meaning of Qīng (lit. "clear" or "pure"). The name may have been selected in reaction to the name of the Ming dynasty (明), which consists of the Chinese characters for "sun" (日) and "moon" (月), both associated with the fire element of the Chinese zodiacal system. The character Qīng (清) is composed of "water" (氵) and "azure" (青), both associated with the water element. This association would justify the Qing conquest as defeat of fire by water. The water imagery of the new name may also have had Buddhist overtones of perspicacity and enlightenment and connections with the Bodhisattva Manjusri. The Manchu name daicing, which sounds like a phonetic rendering of Dà Qīng or Dai Ching, may in fact have been derived from a Mongolian word that means "warrior". Daicing gurun may therefore have meant "warrior state", a pun that was only intelligible to Manchu and Mongol people. In the later part of the dynasty, however, even the Manchus themselves had forgotten this possible meaning.
After conquering "China proper", the Manchus identified their state as "China" (中國, Zhōngguó; "Middle Kingdom"), and referred to it as Dulimbai Gurun in Manchu (Dulimbai means "central" or "middle," gurun means "nation" or "state"). The emperors equated the lands of the Qing state (including present day Northeast China, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, and rejecting the idea that "China" only meant Han areas. The Qing emperors proclaimed that both Han and non-Han peoples were part of "China." They used both "China" and "Qing" to refer to their state in official documents, international treaties (as the Qing was known internationally as "China" or the "Chinese Empire") and foreign affairs, and "Chinese language" (Dulimbai gurun i bithe) included Chinese, Manchu, and Mongol languages, and "Chinese people" (中國之人 Zhōngguó zhī rén; Manchu: Dulimbai gurun i niyalma) referred to all subjects of the empire. In the Chinese-language versions of its treaties and its maps of the world, the Qing government used "Qing" and "China" interchangeably.
The Qing dynasty was founded not by Han Chinese, who constitute the majority of the Chinese population, but by a sedentary farming people known as the Jurchen, a Tungusic people who lived around the region now comprising the Chinese provinces of Jilin and Heilongjiang. The Manchus are sometimes mistaken for a nomadic people, which they were not. What was to become the Manchu state was founded by Nurhaci, the chieftain of a minor Jurchen tribe – the Aisin Gioro – in Jianzhou in the early 17th century. Originally a vassal of the Ming emperors, Nurhachi embarked on an intertribal feud in 1582 that escalated into a campaign to unify the nearby tribes. By 1616, he had sufficiently consolidated Jianzhou so as to be able to proclaim himself Khan of the Great Jin in reference to the previous Jurchen dynasty.
Relocating his court from Jianzhou to Liaodong provided Nurhachi access to more resources; it also brought him in close contact with the Khorchin Mongol domains on the plains of Mongolia. Although by this time the once-united Mongol nation had long since fragmented into individual and hostile tribes, these tribes still presented a serious security threat to the Ming borders. Nurhachi's policy towards the Khorchins was to seek their friendship and cooperation against the Ming, securing his western border from a powerful potential enemy.
There were too few ethnic Manchus to conquer China, so they gained strength by defeating and absorbing Mongols, but more importantly, adding Han Chinese to the Eight Banners. The Manchus had to create an entire "Jiu Han jun" (Old Han Army) due to the massive amount of Han Chinese soldiers which were absorbed into the Eight Banners by both capture and defection, Ming artillery was responsible for many victories against the Manchus, so the Manchus established an artillery corps made out of Han Chinese soldiers in 1641 and the swelling of Han Chinese numbers in the Eight Banners led in 1642 of all Eight Han Banners being created. It was defected Ming Han Chinese armies which conquered southern China for the Qing.
This was followed by the creation of the first two Han Banners in 1637 (increasing to eight in 1642). Together these military reforms enabled Hong Taiji to resoundingly defeat Ming forces in a series of battles from 1640 to 1642 for the territories of Songshan and Jinzhou. This final victory resulted in the surrender of many of the Ming dynasty's most battle-hardened troops, the death of Yuan Chonghuan at the hands of the Chongzhen Emperor (who thought Yuan had betrayed him), and the complete and permanent withdrawal of the remaining Ming forces north of the Great Wall.
Hong Taiji's bureaucracy was staffed with many Han Chinese, including many newly surrendered Ming officials. The Manchus' continued dominance was ensured by an ethnic quota for top bureaucratic appointments. Hong Taiji's reign also saw a fundamental change of policy towards his Han Chinese subjects. Nurhaci had treated Han in Liaodong differently according to how much grain they had, those with less than 5 to 7 sin were treated like chattel while those with more than that amount were rewarded with property. Due to a revolt by Han in Liaodong in 1623, Nurhachi, who previously gave concessions to conquered Han subjects in Liaodong, turned against them and ordered that they no longer be trusted; He enacted discriminatory policies and killings against them, while ordering that Han who assimilated to the Jurchen (in Jilin) before 1619 be treated equally as Jurchens were and not like the conquered Han in Liaodong. Hong Taiji instead incorporated them into the Jurchen "nation" as full (if not first-class) citizens, obligated to provide military service. By 1648, less than one-sixth of the bannermen were of Manchu ancestry. This change of policy not only increased Hong Taiji's manpower and reduced his military dependence on banners not under his personal control, it also greatly encouraged other Han Chinese subjects of the Ming dynasty to surrender and accept Jurchen rule when they were defeated militarily. Through these and other measures Hong Taiji was able to centralize power unto the office of the Khan, which in the long run prevented the Jurchen federation from fragmenting after his death.
Hong Taiji died suddenly in September 1643 without a designated heir. As the Jurchens had traditionally "elected" their leader through a council of nobles, the Qing state did not have in place a clear succession system until the reign of the Kangxi Emperor. The leading contenders for power at this time were Hong Taiji's oldest son Hooge and Hong Taiji' half brother Dorgon. A compromise candidate in the person of Hong Taiji's five-year-old son, Fulin, was installed as the Shunzhi Emperor, with Dorgon as regent and de facto leader of the Manchu nation.
Ming government officials fought against each other, against fiscal collapse, and against a series of peasant rebellions. They were unable to capitalise on the Manchu succession dispute and installation of a minor as emperor. In April 1644, the capital at Beijing was sacked by a coalition of rebel forces led by Li Zicheng, a former minor Ming official, who established a short-lived Shun dynasty. The last Ming ruler, the Chongzhen Emperor, committed suicide when the city fell, marking the official end of the dynasty.
Li Zicheng then led a coalition of rebel forces numbering 200,000[a] to confront Wu Sangui, the general commanding the Ming garrison at Shanhai Pass. Shanhai Pass is a pivotal pass of the Great Wall, located fifty miles northeast of Beijing, and for years its defenses kept the Manchus from directly raiding the Ming capital. Wu Sangui, caught between a rebel army twice his size and a foreign enemy he had fought for years, decided to cast his lot with the Manchus, with whom he was familiar. Wu Sangui may have been influenced by Li Zicheng's mistreatment of his family and other wealthy and cultured officials; it was said that Li also took Wu's concubine Chen Yuanyuan for himself. Wu and Dorgon allied in the name of avenging the death of the Chongzhen Emperor. Together, the two former enemies met and defeated Li Zicheng's rebel forces in battle on May 27, 1644.
The newly allied armies captured Beijing on June 6. The Shunzhi Emperor was invested as the "Son of Heaven" on October 30. The Manchus, who had positioned themselves as political heir to the Ming emperor by defeating the rebel Li Zicheng, completed the symbolic transition by holding a formal funeral for the Chongzhen Emperor. However the process of conquering the rest of China took another seventeen years of battling Ming loyalists, pretenders and rebels. The last Ming pretender, Prince Gui, sought refuge with the King of Burma, but was turned over to a Qing expeditionary army commanded by Wu Sangui, who had him brought back to Yunnan province and executed in early 1662.
Han Chinese Banners were made up of Han Chinese who defected to the Qing up to 1644 and joined the Eight Banners, giving them social and legal privileges in addition to being acculturated to Manchu culture. So many Han defected to the Qing and swelled the ranks of the Eight Banners that ethnic Manchus became a minority, making up only 16% in 1648, with Han Bannermen dominating at 75% and Mongol Bannermen making up the rest. This multi-ethnic force in which Manchus were only a minority conquered China for the Qing.
The Qing showed that the Manchus valued military skills in propaganda targeted towards the Ming military to get them to defect to the Qing, since the Ming civilian political system discriminated against the military. The three Liaodong Han Bannermen officers who played a massive role in the conquest of southern China from the Ming were Shang Kexi, Geng Zhongming, and Kong Youde and they governed southern China autonomously as viceroys for the Qing after their conquests. Normally the Manchu Bannermen acted only as reserve forces or in the rear and were used predominantly for quick strikes with maximum impact, so as to minimize ethnic Manchu losses; instead, the Qing used defected Han Chinese troops to fight as the vanguard during the entire conquest of China.
First, the Manchus had entered "China proper" because Dorgon responded decisively to Wu Sangui's appeal. Then, after capturing Beijing, instead of sacking the city as the rebels had done, Dorgon insisted, over the protests of other Manchu princes, on making it the dynastic capital and reappointing most Ming officials. Choosing Beijing as the capital had not been a straightforward decision, since no major Chinese dynasty had directly taken over its immediate predecessor's capital. Keeping the Ming capital and bureaucracy intact helped quickly stabilize the regime and sped up the conquest of the rest of the country. However, not all of Dorgon's policies were equally popular nor easily implemented.
Dorgon's controversial July 1645 edict (the "haircutting order") forced adult Han Chinese men to shave the front of their heads and comb the remaining hair into the queue hairstyle which was worn by Manchu men, on pain of death. The popular description of the order was: "To keep the hair, you lose the head; To keep your head, you cut the hair." To the Manchus, this policy was a test of loyalty and an aid in distinguishing friend from foe. For the Han Chinese, however, it was a humiliating reminder of Qing authority that challenged traditional Confucian values. The Classic of Filial Piety (Xiaojing) held that "a person's body and hair, being gifts from one's parents, are not to be damaged." Under the Ming dynasty, adult men did not cut their hair but instead wore it in the form of a top-knot. The order triggered strong resistance to Qing rule in Jiangnan and massive killing of ethnic Han Chinese. It was Han Chinese defectors who carried out massacres against people refusing to wear the queue.. Li Chengdong, a Han Chinese general who had served the Ming but surrendered to the Qing, ordered his Han troops to carry out three separate massacres in the city of Jiading within a month, resulting in tens of thousands of deaths. At the end of the third massacre, there was hardly any living person left in this city. Jiangyin also held out against about 10,000 Han Chinese Qing troops for 83 days. When the city wall was finally breached on 9 October 1645, the Han Chinese Qing army led by the Han Chinese Ming defector Liu Liangzuo (劉良佐), who had been ordered to "fill the city with corpses before you sheathe your swords," massacred the entire population, killing between 74,000 and 100,000 people. The queue was the only aspect of Manchu culture which the Qing forced on the common Han population. The Qing required people serving as officials to wear Manchu clothing, but allowed non-official Han civilians to continue wearing Hanfu (Han clothing).
Although his support had been essential to Shunzhi's ascent, Dorgon had through the years centralised so much power in his hands as to become a direct threat to the throne. So much so that upon his death he was extraordinarily bestowed the posthumous title of Emperor Yi (Chinese: 義皇帝), the only instance in Qing history in which a Manchu "prince of the blood" (Chinese: 親王) was so honored. Two months into Shunzhi's personal rule, Dorgon was not only stripped of his titles, but his corpse was disinterred and mutilated.[b] to atone for multiple "crimes", one of which was persecuting to death Shunzhi’s agnate eldest brother, Hooge. More importantly, Dorgon's symbolic fall from grace also signaled a political purge of his family and associates at court, thus reverting power back to the person of the emperor. After a promising start, Shunzhi's reign was cut short by his early death in 1661 at the age of twenty-four from smallpox. He was succeeded by his third son Xuanye, who reigned as the Kangxi Emperor.
The Manchus sent Han Bannermen to fight against Koxinga's Ming loyalists in Fujian. The Qing carried out a massive depopulation policy and seaban forcing people to evacuated the coast in order to deprive Koxinga's Ming loyalists of resources, this has led to a myth that it was because Manchus were "afraid of water". In Fujian, it was Han Bannermen who were the ones carrying out the fighting and killing for the Qing and this disproved the entirely irrelevant claim that alleged fear of the water on part of the Manchus had to do with the coastal evacuation and seaban. Even though a poem refers to the soldiers carrying out massacres in Fujian as "barbarian", both Han Green Standard Army and Han Bannermen were involved in the fighting for the Qing side and carried out the worst slaughter. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.
The sixty-one year reign of the Kangxi Emperor was the longest of any Chinese emperor. Kangxi's reign is also celebrated as the beginning of an era known as the "High Qing", during which the dynasty reached the zenith of its social, economic and military power. Kangxi's long reign started when he was eight years old upon the untimely demise of his father. To prevent a repeat of Dorgon's dictatorial monopolizing of power during the regency, the Shunzhi Emperor, on his deathbed, hastily appointed four senior cabinet ministers to govern on behalf of his young son. The four ministers — Sonin, Ebilun, Suksaha, and Oboi — were chosen for their long service, but also to counteract each other's influences. Most important, the four were not closely related to the imperial family and laid no claim to the throne. However, as time passed, through chance and machination, Oboi, the most junior of the four, achieved such political dominance as to be a potential threat. Even though Oboi's loyalty was never an issue, his personal arrogance and political conservatism led him into an escalating conflict with the young emperor. In 1669 Kangxi, through trickery, disarmed and imprisoned Oboi — a significant victory for a fifteen-year-old emperor over a wily politician and experienced commander.
The early Manchu rulers also established two foundations of legitimacy which help to explain the stability of their dynasty. The first was the bureaucratic institutions and the neo-Confucian culture which they adopted from earlier dynasties. Manchu rulers and Han Chinese scholar-official elites gradually came to terms with each other. The examination system offered a path for ethnic Han to become officials. Imperial patronage of Kangxi Dictionary demonstrated respect for Confucian learning, while the Sacred Edict of 1670 effectively extolled Confucian family values. The second major source of stability was the Central Asian aspect of their Manchu identity which allowed them to appeal to Mongol, Tibetan and Uighur constituents. The Qing used the title of Emperor (Huangdi) in Chinese while among Mongols the Qing monarch was referred to as Bogda khan (wise Khan), and referred to as Gong Ma in Tibet. Qianlong propagated the image of himself as Buddhist sage rulers, patrons of Tibetan Buddhism. In the Manchu language, the Qing monarch was alternately referred to as either Huwangdi (Emperor) or Khan with no special distinction between the two usages. The Kangxi Emperor also welcomed to his court Jesuit missionaries, who had first come to China under the Ming. Missionaries including Tomás Pereira, Martino Martini, Johann Adam Schall von Bell, Ferdinand Verbiest and Antoine Thomas held significant positions as military weapons experts, mathematicians, cartographers, astronomers and advisers to the emperor. The relationship of trust was however lost in the later Chinese Rites controversy.
Yet controlling the "Mandate of Heaven" was a daunting task. The vastness of China's territory meant that there were only enough banner troops to garrison key cities forming the backbone of a defense network that relied heavily on surrendered Ming soldiers. In addition, three surrendered Ming generals were singled out for their contributions to the establishment of the Qing dynasty, ennobled as feudal princes (藩王), and given governorships over vast territories in Southern China. The chief of these was Wu Sangui, who was given the provinces of Yunnan and Guizhou, while generals Shang Kexi and Geng Jingzhong were given Guangdong and Fujian provinces respectively.
As the years went by, the three feudal lords and their extensive territories became increasingly autonomous. Finally, in 1673, Shang Kexi petitioned Kangxi for permission to retire to his hometown in Liaodong province and nominated his son as his successor. The young emperor granted his retirement, but denied the heredity of his fief. In reaction, the two other generals decided to petition for their own retirements to test Kangxi's resolve, thinking that he would not risk offending them. The move backfired as the young emperor called their bluff by accepting their requests and ordering that all three fiefdoms to be reverted to the crown.
Faced with the stripping of their powers, Wu Sangui, later joined by Geng Zhongming and by Shang Kexi's son Shang Zhixin, felt they had no choice but to revolt. The ensuing Revolt of the Three Feudatories lasted for eight years. Wu attempted, ultimately in vain, to fire the embers of south China Ming loyalty by restoring Ming customs, ordering that the resented queues be cut, and declaring himself emperor of a new dynasty. At the peak of the rebels' fortunes, they extended their control as far north as the Yangtze River, nearly establishing a divided China. Wu then hesitated to go further north, not being able to coordinate strategy with his allies, and Kangxi was able to unify his forces for a counterattack led by a new generation of Manchu generals. By 1681, the Qing government had established control over a ravaged southern China which took several decades to recover. Manchu Generals and Bannermen were initially put to shame by the better performance of the Han Chinese Green Standard Army, who fought better than them against the rebels and this was noted by Kangxi, leading him to task Generals Sun Sike, Wang Jinbao, and Zhao Liangdong to lead Green Standard Soldiers to crush the rebels. The Qing thought that Han Chinese were superior at battling other Han people and so used the Green Standard Army as the dominant and majority army in crushing the rebels instead of Bannermen. Similarly, in northwestern China against Wang Fuchen, the Qing used Han Chinese Green Standard Army soldiers and Han Chinese Generals such as Zhang Liangdong, Wang Jinbao, and Zhang Yong as the primary military forces. This choice was due to the rocky terrain, which favoured infantry troops over cavalry, to the desire to keep Bannermen in the reserves, and, again, to the belief that Han troops were better at fighting other Han people. These Han generals achieved victory over the rebels. Also due to the mountainous terrain, Sichuan and southern Shaanxi were also retaken by the Han Chinese Green Standard Army under Wang Jinbao and Zhao Liangdong in 1680, with Manchus only participating in dealing with logistics and provisions. 400,000 Green Standard Army soldiers and 150,000 Bannermen served on the Qing side during the war. 213 Han Chinese Banner companies, and 527 companies of Mongol and Manchu Banners were mobilized by the Qing during the revolt. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.
The Qing forces were crushed by Wu from 1673-1674. The Qing had the support of the majority of Han Chinese soldiers and Han elite against the Three Feudatories, since they refused to join Wu Sangui in the revolt, while the Eight Banners and Manchu officers fared poorly against Wu Sangui, so the Qing responded with using a massive army of more than 900,000 Han Chinese (non-Banner) instead of the Eight Banners, to fight and crush the Three Feudatories. Wu Sangui's forces were crushed by the Green Standard Army, made out of defected Ming soldiers.
To extend and consolidate the dynasty's control in Central Asia, the Kangxi Emperor personally led a series of military campaigns against the Dzungars in Outer Mongolia. The Kangxi Emperor was able to successfully expel Galdan's invading forces from these regions, which were then incorporated into the empire. Galdan was eventually killed in the Dzungar–Qing War. In 1683, Qing forces received the surrender of Taiwan from Zheng Keshuang, grandson of Koxinga, who had conquered Taiwan from the Dutch colonists as a base against the Qing. Zheng Keshuang was awarded the title "Duke Haicheng" (海澄公) and was inducted into the Han Chinese Plain Red Banner of the Eight Banners when he moved to Beijing. Several Ming princes had accompanied Koxinga to Taiwan in 1661-1662, including the Prince of Ningjing Zhu Shugui and Prince Zhu Honghuan (朱弘桓), son of Zhu Yihai, where they lived in the Kingdom of Tungning. The Qing sent the 17 Ming princes still living on Taiwan in 1683 back to mainland China where they spent the rest of their lives in exile since their lives were spared from execution. Winning Taiwan freed Kangxi's forces for series of battles over Albazin, the far eastern outpost of the Tsardom of Russia. Zheng's former soldiers on Taiwan like the rattan shield troops were also inducted into the Eight Banners and used by the Qing against Russian Cossacks at Albazin. The 1689 Treaty of Nerchinsk was China's first formal treaty with a European power and kept the border peaceful for the better part of two centuries. After Galdan's death, his followers, as adherents to Tibetan Buddhism, attempted to control the choice of the next Dalai Lama. Kangxi dispatched two armies to Lhasa, the capital of Tibet, and installed a Dalai Lama sympathetic to the Qing.
After the Kangxi Emperor's death in the winter of 1722, his fourth son, Prince Yong (雍親王), became the Yongzheng Emperor. In the later years of Kangxi's reign, Yongzheng and his brothers had fought, and there were rumours that he had usurped the throne(most of the rumours believe that Yongzheng's brother Yingzhen (Kangxi's 14th son) is the real successor of Kangxi Emperor, the reason why Yingzhen failed to sit on the throne is because Yongzheng and his confidant Keduo Long tampered the content of Kangxi's testament at the night when Kangxi passed away), a charge for which there is little evidence. In fact, his father had trusted him with delicate political issues and discussed state policy with him. When Yongzheng came to power at the age of 45, he felt a sense of urgency about the problems which had accumulated in his father's later years and did not need instruction in how to exercise power. In the words of one recent historian, he was "severe, suspicious, and jealous, but extremely capable and resourceful," and in the words of another, turned out to be an "early modern state-maker of the first order."
He moved rapidly. First, he promoted Confucian orthodoxy and reversed what he saw as his father's laxness by cracking down on unorthodox sects and by decapitating an anti-Manchu writer his father had pardoned. In 1723 he outlawed Christianity and expelled Christian missionaries, though some were allowed to remain in the capital. Next, he moved to control the government. He expanded his father's system of Palace Memorials which brought frank and detailed reports on local conditions directly to the throne without being intercepted by the bureaucracy, and created a small Grand Council of personal advisors which eventually grew into the emperor's de facto cabinet for the rest of the dynasty. He shrewdly filled key positions with Manchu and Han Chinese officials who depended on his patronage. When he began to realize that the financial crisis was even greater than he had thought, Yongzheng rejected his father's lenient approach to local landowning elites and mounted a campaign to enforce collection of the land tax. The increased revenues were to be used for "money to nourish honesty" among local officials and for local irrigation, schools, roads, and charity. Although these reforms were effective in the north, in the south and lower Yangzi valley, where Kangxi had wooed the elites, there were long established networks of officials and landowners. Yongzheng dispatched experienced Manchu commissioners to penetrate the thickets of falsified land registers and coded account books, but they were met with tricks, passivity, and even violence. The fiscal crisis persisted.
In 1725 Yongzheng bestowed the hereditary title of Marquis on a descendant of the Ming dynasty Imperial family, Zhu Zhiliang, who received a salary from the Qing government and whose duty was to perform rituals at the Ming tombs, and was also inducted the Chinese Plain White Banner in the Eight Banners. Later the Qianlong Emperor bestowed the title Marquis of Extended Grace posthumously on Zhu Zhuliang in 1750, and the title passed on through twelve generations of Ming descendants until the end of the Qing dynasty.
Yongzheng also inherited diplomatic and strategic problems. A team made up entirely of Manchus drew up the Treaty of Kyakhta (1727) to solidify the diplomatic understanding with Russia. In exchange for territory and trading rights, the Qing would have a free hand dealing with the situation in Mongolia. Yongzheng then turned to that situation, where the Zunghars threatened to re-emerge, and to the southwest, where local Miao chieftains resisted Qing expansion. These campaigns drained the treasury but established the emperor's control of the military and military finance.
Qianlong's reign saw the launch of several ambitious cultural projects, including the compilation of the Siku Quanshu, or Complete Repository of the Four Branches of Literature. With a total of over 3,400 books, 79,000 chapters, and 36,304 volumes, the Siku Quanshu is the largest collection of books in Chinese history. Nevertheless, Qianlong used Literary Inquisition to silence opposition. The accusation of individuals began with the emperor's own interpretation of the true meaning of the corresponding words. If the emperor decided these were derogatory or cynical towards the dynasty, persecution would begin. Literary inquisition began with isolated cases at the time of Shunzhi and Kangxi, but became a pattern under Qianlong's rule, during which there were 53 cases of literary persecution.
China also began suffering from mounting overpopulation during this period. Population growth was stagnant for the first half of the 17th century due to civil wars and epidemics, but prosperity and internal stability gradually reversed this trend. The introduction of new crops from the Americas such as the potato and peanut allowed an improved food supply as well, so that the total population of China during the 18th century ballooned from 100 million to 300 million people. Soon all available farmland was used up, forcing peasants to work ever-smaller and more intensely worked plots. The Qianlong Emperor once bemoaned the country's situation by remarking "The population continues to grow, but the land does not." The only remaining part of the empire that had arable farmland was Manchuria, where the provinces of Jilin and Heilongjiang had been walled off as a Manchu homeland. The emperor decreed for the first time that Han Chinese civilians were forbidden to settle. Mongols were forbidden by the Qing from crossing the borders of their banners, even into other Mongol Banners and from crossing into neidi (the Han Chinese 18 provinces) and were given serious punishments if they did in order to keep the Mongols divided against each other to benefit the Qing.
However Qing rule saw an massively increasing amount of Han Chinese both illegally and legally streaming into Manchuria and settling down to cultivate land as Manchu landlords desired Han Chinese peasants to rent on their land and grow grain, most Han Chinese migrants were not evicted as they went over the Great Wall and Willow Palisade, during the eighteenth century Han Chinese farmed 500,000 hectares of privately owned land in Manchuria and 203,583 hectares of lands which were part of coutrier stations, noble estates, and Banner lands, in garrisons and towns in Manchuria Han Chinese made up 80% of the population.
Han Chinese farmers were resettled from north China by the Qing to the area along the Liao River in order to restore the land to cultivation. Wasteland was reclaimed by Han Chinese squatters in addition to other Han who rented land from Manchu landlords. Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia so that Han Chinese farmed 500,000 hectares in Manchuria and tens of thousands of hectares in Inner Mongolia by the 1780s. Qianlong allowed Han Chinese peasants suffering from drought to move into Manchuria despite him issuing edicts in favor of banning them from 1740–1776. Chinese tenant farmers rented or even claimed title to land from the "imperial estates" and Manchu Bannerlands in the area. Besides moving into the Liao area in southern Manchuria, the path linking Jinzhou, Fengtian, Tieling, Changchun, Hulun, and Ningguta was settled by Han Chinese during the Qianlong Emperor's rule, and Han Chinese were the majority in urban areas of Manchuria by 1800. To increase the Imperial Treasury's revenue, the Qing sold formerly Manchu only lands along the Sungari to Han Chinese at the beginning of the Daoguang Emperor's reign, and Han Chinese filled up most of Manchuria's towns by the 1840s according to Abbe Huc.
However, the 18th century saw the European empires gradually expand across the world, as European states developed economies built on maritime trade. The dynasty was confronted with newly developing concepts of the international system and state to state relations. European trading posts expanded into territorial control in nearby India and on the islands that are now Indonesia. The Qing response, successful for a time, was in 1756 to establish the Canton System, which restricted maritime trade to that city and gave monopoly trading rights to private Chinese merchants. The British East India Company and the Dutch East India Company had long before been granted similar monopoly rights by their governments.
Demand in Europe for Chinese goods such as silk, tea, and ceramics could only be met if European companies funneled their limited supplies of silver into China. In the late 1700s, the governments of Britain and France were deeply concerned about the imbalance of trade and the drain of silver. To meet the growing Chinese demand for opium, the British East India Company greatly expanded its production in Bengal. Since China's economy was essentially self-sufficient, the country had little need to import goods or raw materials from the Europeans, so the usual way of payment was through silver. The Daoguang Emperor, concerned both over the outflow of silver and the damage that opium smoking was causing to his subjects, ordered Lin Zexu to end the opium trade. Lin confiscated the stocks of opium without compensation in 1839, leading Britain to send a military expedition the following year.
The First Opium War revealed the outdated state of the Chinese military. The Qing navy, composed entirely of wooden sailing junks, was severely outclassed by the modern tactics and firepower of the British Royal Navy. British soldiers, using advanced muskets and artillery, easily outmaneuvered and outgunned Qing forces in ground battles. The Qing surrender in 1842 marked a decisive, humiliating blow to China. The Treaty of Nanjing, the first of the unequal treaties, demanded war reparations, forced China to open up the five ports of Canton, Amoy, Fuchow, Ningpo and Shanghai to western trade and missionaries, and to cede Hong Kong Island to Britain. It revealed many inadequacies in the Qing government and provoked widespread rebellions against the already hugely unpopular regime.
The Taiping Rebellion in the mid-19th century was the first major instance of anti-Manchu sentiment threatening the stability of the dynasty. Hong Xiuquan, a failed civil service candidate, led the Taiping Rebellion, amid widespread social unrest and worsening famine. In 1851 Hong Xiuquan and others launched an uprising in Guizhou province, established the Taiping Heavenly Kingdom with Hong himself as king, claiming he often had visions of God and that he was the brother of Jesus Christ. Slavery, concubinage, arranged marriage, opium smoking, footbinding, judicial torture, and the worship of idols were all banned. However, success and subsequent authority and power led to internal feuds, defections and corruption. In addition, British and French troops, equipped with modern weapons, had come to the assistance of the Qing imperial army. It was not until 1864 that Qing armies under Zeng Guofan succeeded in crushing the revolt. The rebellion not only posed the most serious threat towards Qing rulers; it was also "bloodiest civil war of all time." Between 20 and 30 million people died during its fourteen-year course from 1850 to 1864. After the outbreak of this rebellion, there were also revolts by the Muslims and Miao people of China against the Qing dynasty, most notably in the Dungan Revolt (1862–77) in the northwest and the Panthay Rebellion (1856–1873) in Yunnan.
The Western powers, largely unsatisfied with the Treaty of Nanjing, gave grudging support to the Qing government during the Taiping and Nian Rebellions. China's income fell sharply during the wars as vast areas of farmland were destroyed, millions of lives were lost, and countless armies were raised and equipped to fight the rebels. In 1854, Britain tried to re-negotiate the Treaty of Nanjing, inserting clauses allowing British commercial access to Chinese rivers and the creation of a permanent British embassy at Beijing.
Ratification of the treaty the following year led to resumption of hostilities and in 1860, with Anglo-French forces marching on Beijing, the emperor and his court fled the capital for the imperial hunting lodge at Rehe. Once in Beijing, the Anglo-French forces looted the Old Summer Palace, and in an act of revenge for the arrest of several Englishmen, burnt it to the ground. Prince Gong, a younger half-brother of the emperor, who had been left as his brother's proxy in the capital, was forced to sign the Convention of Beijing. Meanwhile, the humiliated emperor died the following year at Rehe.
Chinese generals and officials such as Zuo Zongtang led the suppression of rebellions and stood behind the Manchus. When the Tongzhi Emperor came to the throne at the age of five in 1861, these officials rallied around him in what was called the Tongzhi Restoration. Their aim was to adopt western military technology in order to preserve Confucian values. Zeng Guofan, in alliance with Prince Gong, sponsored the rise of younger officials such as Li Hongzhang, who put the dynasty back on its feet financially and instituted the Self-Strengthening Movement. The reformers then proceeded with institutional reforms, including China's first unified ministry of foreign affairs, the Zongli Yamen; allowing foreign diplomats to reside in the capital; establishment of the Imperial Maritime Customs Service; the formation of modernized armies, such as the Beiyang Army, as well as a navy; and the purchase from Europeans of armament factories. 
The dynasty lost control of peripheral territories bit by bit. In return for promises of support against the British and the French, the Russian Empire took large chunks of territory in the Northeast in 1860. The period of cooperation between the reformers and the European powers ended with the Tientsin Massacre of 1870, which was incited by the murder of French nuns set off by the belligerence of local French diplomats. Starting with the Cochinchina Campaign in 1858, France expanded control of Indochina. By 1883, France was in full control of the region and had reached the Chinese border. The Sino-French War began with a surprise attack by the French on the Chinese southern fleet at Fuzhou. After that the Chinese declared war on the French. A French invasion of Taiwan was halted and the French were defeated on land in Tonkin at the Battle of Bang Bo. However Japan threatened to enter the war against China due to the Gapsin Coup and China chose to end the war with negotiations. The war ended in 1885 with the Treaty of Tientsin (1885) and the Chinese recognition of the French protectorate in Vietnam.
Historians have judged the Qing dynasty's vulnerability and weakness to foreign imperialism in the 19th century to be based mainly on its maritime naval weakness while it achieved military success against westerners on land, the historian Edward L. Dreyer said that "China’s nineteenth-century humiliations were strongly related to her weakness and failure at sea. At the start of the Opium War, China had no unified navy and no sense of how vulnerable she was to attack from the sea; British forces sailed and steamed wherever they wanted to go......In the Arrow War (1856–60), the Chinese had no way to prevent the Anglo-French expedition of 1860 from sailing into the Gulf of Zhili and landing as near as possible to Beijing. Meanwhile, new but not exactly modern Chinese armies suppressed the midcentury rebellions, bluffed Russia into a peaceful settlement of disputed frontiers in Central Asia, and defeated the French forces on land in the Sino-French War (1884–85). But the defeat of the fleet, and the resulting threat to steamship traffic to Taiwan, forced China to conclude peace on unfavorable terms."
In 1884, pro-Japanese Koreans in Seoul led the Gapsin Coup. Tensions between China and Japan rose after China intervened to suppress the uprising. Japanese Prime Minister Itō Hirobumi and Li Hongzhang signed the Convention of Tientsin, an agreement to withdraw troops simultaneously, but the First Sino-Japanese War of 1895 was a military humiliation. The Treaty of Shimonoseki recognized Korean independence and ceded Taiwan and the Pescadores to Japan. The terms might have been harsher, but when Japanese citizen attacked and wounded Li Hongzhang, an international outcry shamed the Japanese into revising them. The original agreement stipulated the cession of Liaodong Peninsula to Japan, but Russia, with its own designs on the territory, along with Germany and France, in what was known as the Triple Intervention, successfully put pressure on the Japanese to abandon the peninsula.
These years saw an evolution in the participation of Empress Dowager Cixi (Wade–Giles: Tz'u-Hsi) in state affairs. She entered the imperial palace in the 1850s as a concubine to the Xianfeng Emperor (r. 1850–1861) and came to power in 1861 after her five-year-old son, the Tongzhi Emperor ascended the throne. She, the Empress Dowager Ci'an (who had been Xianfeng's empress), and Prince Gong (a son of the Daoguang Emperor), staged a coup that ousted several regents for the boy emperor. Between 1861 and 1873, she and Ci'an served as regents, choosing the reign title "Tongzhi" (ruling together). Following the emperor's death in 1875, Cixi's nephew, the Guangxu Emperor, took the throne, in violation of the dynastic custom that the new emperor be of the next generation, and another regency began. In the spring of 1881, Ci'an suddenly died, aged only forty-three, leaving Cixi as sole regent. 
From 1889, when Guangxu began to rule in his own right, to 1898, the Empress Dowager lived in semi-retirement, spending the majority of the year at the Summer Palace. On November 1, 1897, two German Roman Catholic missionaries were murdered in the southern part of Shandong Province (the Juye Incident). In response, Germany used the murders as a pretext for a naval occupation of Jiaozhou Bay. The occupation prompted a "scramble for concessions" in 1898, which included the German lease of Jiazhou Bay, the Russian acquisition of Liaodong, and the British lease of the New Territories of Hong Kong.
In the wake of these external defeats, the Guangxu Emperor initiated the Hundred Days' Reform of 1898. Newer, more radical advisers such as Kang Youwei were given positions of influence. The emperor issued a series of edicts and plans were made to reorganize the bureaucracy, restructure the school system, and appoint new officials. Opposition from the bureaucracy was immediate and intense. Although she had been involved in the initial reforms, the empress dowager stepped in to call them off, arrested and executed several reformers, and took over day-to-day control of policy. Yet many of the plans stayed in place, and the goals of reform were implanted.
Widespread drought in North China, combined with the imperialist designs of European powers and the instability of the Qing government, created conditions that led to the emergence of the Righteous and Harmonious Fists, or "Boxers." In 1900, local groups of Boxers proclaiming support for the Qing dynasty murdered foreign missionaries and large numbers of Chinese Christians, then converged on Beijing to besiege the Foreign Legation Quarter. A coalition of European, Japanese, and Russian armies (the Eight-Nation Alliance) then entered China without diplomatic notice, much less permission. Cixi declared war on all of these nations, only to lose control of Beijing after a short, but hard-fought campaign. She fled to Xi'an. The victorious allies drew up scores of demands on the Qing government, including compensation for their expenses in invading China and execution of complicit officials.
By the early 20th century, mass civil disorder had begun in China, and it was growing continuously. To overcome such problems, Empress Dowager Cixi issued an imperial edict in 1901 calling for reform proposals from the governors-general and governors and initiated the era of the dynasty's "New Policies", also known as the "Late Qing Reform". The edict paved the way for the most far-reaching reforms in terms of their social consequences, including the creation of a national education system and the abolition of the imperial examinations in 1905.
The Guangxu Emperor died on November 14, 1908, and on November 15, 1908, Cixi also died. Rumors held that she or Yuan Shikai ordered trusted eunuchs to poison the Guangxu Emperor, and an autopsy conducted nearly a century later confirmed lethal levels of arsenic in his corpse. Puyi, the oldest son of Zaifeng, Prince Chun, and nephew to the childless Guangxu Emperor, was appointed successor at the age of two, leaving Zaifeng with the regency. This was followed by the dismissal of General Yuan Shikai from his former positions of power. In April 1911 Zaifeng created a cabinet in which there were two vice-premiers. Nonetheless, this cabinet was also known by contemporaries as "The Royal Cabinet" because among the thirteen cabinet members, five were members of the imperial family or Aisin Gioro relatives. This brought a wide range of negative opinions from senior officials like Zhang Zhidong. The Wuchang Uprising of October 10, 1911, led to the creation of a new central government, the Republic of China, in Nanjing with Sun Yat-sen as its provisional head. Many provinces soon began "separating" from Qing control. Seeing a desperate situation unfold, the Qing government brought Yuan Shikai back to military power. He took control of his Beiyang Army to crush the revolution in Wuhan at the Battle of Yangxia. After taking the position of Prime Minister and creating his own cabinet, Yuan Shikai went as far as to ask for the removal of Zaifeng from the regency. This removal later proceeded with directions from Empress Dowager Longyu.
With Zaifeng gone, Yuan Shikai and his Beiyang commanders effectively dominated Qing politics. He reasoned that going to war would be unreasonable and costly, especially when noting that the Qing government had a goal for constitutional monarchy. Similarly, Sun Yat-sen's government wanted a republican constitutional reform, both aiming for the benefit of China's economy and populace. With permission from Empress Dowager Longyu, Yuan Shikai began negotiating with Sun Yat-sen, who decided that his goal had been achieved in forming a republic, and that therefore he could allow Yuan to step into the position of President of the Republic of China.
On 12 February 1912, after rounds of negotiations, Longyu issued an imperial edict bringing about the abdication of the child emperor Puyi. This brought an end to over 2,000 years of Imperial China and began an extended period of instability of warlord factionalism. The unorganized political and economic systems combined with a widespread criticism of Chinese culture led to questioning and doubt about the future. In the 1930s, the Empire of Japan invaded Northeast China and founded Manchukuo in 1932, with Puyi, as the emperor. After the invasion by the Soviet Union, Manchukuo collapsed in 1945.
The early Qing emperors adopted the bureaucratic structures and institutions from the preceding Ming dynasty but split rule between Han Chinese and Manchus, with some positions also given to Mongols. Like previous dynasties, the Qing recruited officials via the imperial examination system, until the system was abolished in 1905. The Qing divided the positions into civil and military positions, each having nine grades or ranks, each subdivided into a and b categories. Civil appointments ranged from attendant to the emperor or a Grand Secretary in the Forbidden City (highest) to being a prefectural tax collector, deputy jail warden, deputy police commissioner or tax examiner. Military appointments ranged from being a field marshal or chamberlain of the imperial bodyguard to a third class sergeant, corporal or a first or second class private.
The formal structure of the Qing government centered on the Emperor as the absolute ruler, who presided over six Boards (Ministries[c]), each headed by two presidents[d] and assisted by four vice presidents.[e] In contrast to the Ming system, however, Qing ethnic policy dictated that appointments were split between Manchu noblemen and Han officials who had passed the highest levels of the state examinations. The Grand Secretariat,[f] which had been an important policy-making body under the Ming, lost its importance during the Qing and evolved into an imperial chancery. The institutions which had been inherited from the Ming formed the core of the Qing "Outer Court," which handled routine matters and was located in the southern part of the Forbidden City.
In order not to let the routine administration take over the running of the empire, the Qing emperors made sure that all important matters were decided in the "Inner Court," which was dominated by the imperial family and Manchu nobility and which was located in the northern part of the Forbidden City. The core institution of the inner court was the Grand Council.[g] It emerged in the 1720s under the reign of the Yongzheng Emperor as a body charged with handling Qing military campaigns against the Mongols, but it soon took over other military and administrative duties and served to centralize authority under the crown. The Grand Councillors[h] served as a sort of privy council to the emperor.
From the early Qing, the central government was characterized by a system of dual appointments by which each position in the central government had a Manchu and a Han Chinese assigned to it. The Han Chinese appointee was required to do the substantive work and the Manchu to ensure Han loyalty to Qing rule. The distinction between Han Chinese and Manchus extended to their court costumes. During the Qianlong Emperor's reign, for example, members of his family were distinguished by garments with a small circular emblem on the back, whereas Han officials wore clothing with a square emblem.
In addition to the six boards, there was a Lifan Yuan unique to the Qing government. This institution was established to supervise the administration of Tibet and the Mongol lands. As the empire expanded, it took over administrative responsibility of all minority ethnic groups living in and around the empire, including early contacts with Russia — then seen as a tribute nation. The office had the status of a full ministry and was headed by officials of equal rank. However, appointees were at first restricted only to candidates of Manchu and Mongol ethnicity, until later open to Han Chinese as well.
Even though the Board of Rites and Lifan Yuan performed some duties of a foreign office, they fell short of developing into a professional foreign service. It was not until 1861 — a year after losing the Second Opium War to the Anglo-French coalition — that the Qing government bowed to foreign pressure and created a proper foreign affairs office known as the Zongli Yamen. The office was originally intended to be temporary and was staffed by officials seconded from the Grand Council. However, as dealings with foreigners became increasingly complicated and frequent, the office grew in size and importance, aided by revenue from customs duties which came under its direct jurisdiction.
There was also another government institution called Imperial Household Department which was unique to the Qing dynasty. It was established before the fall of the Ming, but it became mature only after 1661, following the death of the Shunzhi Emperor and the accession of his son, the Kangxi Emperor. The department's original purpose was to manage the internal affairs of the imperial family and the activities of the inner palace (in which tasks it largely replaced eunuchs), but it also played an important role in Qing relations with Tibet and Mongolia, engaged in trading activities (jade, ginseng, salt, furs, etc.), managed textile factories in the Jiangnan region, and even published books. Relations with the Salt Superintendents and salt merchants, such as those at Yangzhou, were particularly lucrative, especially since they were direct, and did not go through absorptive layers of bureaucracy. The department was manned by booi,[o] or "bondservants," from the Upper Three Banners. By the 19th century, it managed the activities of at least 56 subagencies.
Qing China reached its largest extent during the 18th century, when it ruled China proper (eighteen provinces) as well as the areas of present-day Northeast China, Inner Mongolia, Outer Mongolia, Xinjiang and Tibet, at approximately 13 million km2 in size. There were originally 18 provinces, all of which in China proper, but later this number was increased to 22, with Manchuria and Xinjiang being divided or turned into provinces. Taiwan, originally part of Fujian province, became a province of its own in the late 19th century, but was ceded to the Empire of Japan in 1895 following the First Sino-Japanese War. In addition, many surrounding countries, such as Korea (Joseon dynasty), Vietnam frequently paid tribute to China during much of this period. Khanate of Kokand were forced to submit as protectorate and pay tribute to the Qing dynasty in China between 1774 and 1798.
The Qing organization of provinces was based on the fifteen administrative units set up by the Ming dynasty, later made into eighteen provinces by splitting for example, Huguang into Hubei and Hunan provinces. The provincial bureaucracy continued the Yuan and Ming practice of three parallel lines, civil, military, and censorate, or surveillance. Each province was administered by a governor (巡撫, xunfu) and a provincial military commander (提督, tidu). Below the province were prefectures (府, fu) operating under a prefect (知府, zhīfǔ), followed by subprefectures under a subprefect. The lowest unit was the county, overseen by a county magistrate. The eighteen provinces are also known as "China proper". The position of viceroy or governor-general (總督, zongdu) was the highest rank in the provincial administration. There were eight regional viceroys in China proper, each usually took charge of two or three provinces. The Viceroy of Zhili, who was responsible for the area surrounding the capital Beijing, is usually considered as the most honorable and powerful viceroy among the eight.
By the mid-18th century, the Qing had successfully put outer regions such as Inner and Outer Mongolia, Tibet and Xinjiang under its control. Imperial commissioners and garrisons were sent to Mongolia and Tibet to oversee their affairs. These territories were also under supervision of a central government institution called Lifan Yuan. Qinghai was also put under direct control of the Qing court. Xinjiang, also known as Chinese Turkestan, was subdivided into the regions north and south of the Tian Shan mountains, also known today as Dzungaria and Tarim Basin respectively, but the post of Ili General was established in 1762 to exercise unified military and administrative jurisdiction over both regions. Dzungaria was fully opened to Han migration by the Qianlong Emperor from the beginning. Han migrants were at first forbidden from permanently settling in the Tarim Basin but were the ban was lifted after the invasion by Jahangir Khoja in the 1820s. Likewise, Manchuria was also governed by military generals until its division into provinces, though some areas of Xinjiang and Northeast China were lost to the Russian Empire in the mid-19th century. Manchuria was originally separated from China proper by the Inner Willow Palisade, a ditch and embankment planted with willows intended to restrict the movement of the Han Chinese, as the area was off-limits to civilian Han Chinese until the government started colonizing the area, especially since the 1860s.
With respect to these outer regions, the Qing maintained imperial control, with the emperor acting as Mongol khan, patron of Tibetan Buddhism and protector of Muslims. However, Qing policy changed with the establishment of Xinjiang province in 1884. During The Great Game era, taking advantage of the Dungan revolt in northwest China, Yaqub Beg invaded Xinjiang from Central Asia with support from the British Empire, and made himself the ruler of the kingdom of Kashgaria. The Qing court sent forces to defeat Yaqub Beg and Xinjiang was reconquered, and then the political system of China proper was formally applied onto Xinjiang. The Kumul Khanate, which was incorporated into the Qing empire as a vassal after helping Qing defeat the Zunghars in 1757, maintained its status after Xinjiang turned into a province through the end of the dynasty in the Xinhai Revolution up until 1930. In early 20th century, Britain sent an expedition force to Tibet and forced Tibetans to sign a treaty. The Qing court responded by asserting Chinese sovereignty over Tibet, resulting in the 1906 Anglo-Chinese Convention signed between Britain and China. The British agreed not to annex Tibetan territory or to interfere in the administration of Tibet, while China engaged not to permit any other foreign state to interfere with the territory or internal administration of Tibet. Furthermore, similar to Xinjiang which was converted into a province earlier, the Qing government also turned Manchuria into three provinces in the early 20th century, officially known as the "Three Northeast Provinces", and established the post of Viceroy of the Three Northeast Provinces to oversee these provinces, making the total number of regional viceroys to nine.
The early Qing military was rooted in the Eight Banners first developed by Nurhaci to organize Jurchen society beyond petty clan affiliations. There were eight banners in all, differentiated by color. The yellow, bordered yellow, and white banners were known as the "Upper Three Banners" and were under the direct command of the emperor. Only Manchus belonging to the Upper Three Banners, and selected Han Chinese who had passed the highest level of martial exams could serve as the emperor's personal bodyguards. The remaining Banners were known as the "Lower Five Banners." They were commanded by hereditary Manchu princes descended from Nurhachi's immediate family, known informally as the "Iron cap princes". Together they formed the ruling council of the Manchu nation as well as high command of the army. Nurhachi's son Hong Taiji expanded the system to include mirrored Mongol and Han Banners. After capturing Beijing in 1644, the relatively small Banner armies were further augmented by the Green Standard Army, made up of those Ming troops who had surrendered to the Qing, which eventually outnumbered Banner troops three to one. They maintained their Ming era organization and were led by a mix of Banner and Green Standard officers.[citation needed]
Banner Armies were organized along ethnic lines, namely Manchu and Mongol, but included non-Manchu bondservants registered under the household of their Manchu masters. The years leading up to the conquest increased the number of Han Chinese under Manchu rule, leading Hong Taiji to create the Eight Han Banners (zh), and around the time of the Qing takeover of Beijing, their numbers rapidly swelled. Han Bannermen held high status and power in the early Qing period, especially immediately after the conquest during Shunzhi and Kangxi's reign where they dominated Governor-Generalships and Governorships across China at the expense of both Manchu Bannermen and Han civilians. Han also numerically dominated the Banners up until the mid 18th century. European visitors in Beijing called them "Tartarized Chinese" or "Tartarified Chinese". It was in Qianlong's reign that the Qianlong Emperor, concerned about maintaining Manchu identity, re-emphasized Manchu ethnicity, ancestry, language, and culture in the Eight Banners and started a mass discharge of Han Bannermen from the Eight Banners, either asking them to voluntarily resign from the Banner rolls or striking their names off. This led to a change from Han majority to a Manchu majority within the Banner system, and previous Han Bannermen garrisons in southern China such as at Fuzhou, Zhenjiang, Guangzhou, were replaced by Manchu Bannermen in the purge, which started in 1754. The turnover by Qianlong most heavily impacted Han banner garrisons stationed in the provinces while it less impacted Han Bannermen in Beijing, leaving a larger proportion of remaining Han Bannermen in Beijing than the provinces. Han Bannermen's status was decreased from that point on with Manchu Banners gaining higher status. Han Bannermen numbered 75% in 1648 Shunzhi's reign, 72% in 1723 Yongzheng's reign, but decreased to 43% in 1796 during the first year of Jiaqing's reign, which was after Qianlong's purge. The mass discharge was known as the Disbandment of the Han Banners (zh). Qianlong directed most of his ire at those Han Bannermen descended from defectors who joined the Qing after the Qing passed through the Great Wall at Shanhai Pass in 1644, deeming their ancestors as traitors to the Ming and therefore untrustworthy, while retaining Han Bannermen who were descended from defectors who joined the Qing before 1644 in Liaodong and marched through Shanhai pass, also known as those who "followed the Dragon through the pass" (從龍入關; cong long ru guan).
Early during the Taiping Rebellion, Qing forces suffered a series of disastrous defeats culminating in the loss of the regional capital city of Nanjing in 1853. Shortly thereafter, a Taiping expeditionary force penetrated as far north as the suburbs of Tianjin, the imperial heartlands. In desperation the Qing court ordered a Chinese official, Zeng Guofan, to organize regional and village militias into an emergency army called tuanlian. Zeng Guofan's strategy was to rely on local gentry to raise a new type of military organization from those provinces that the Taiping rebels directly threatened. This new force became known as the Xiang Army, named after the Hunan region where it was raised. The Xiang Army was a hybrid of local militia and a standing army. It was given professional training, but was paid for out of regional coffers and funds its commanders — mostly members of the Chinese gentry — could muster. The Xiang Army and its successor, the Huai Army, created by Zeng Guofan's colleague and mentee Li Hongzhang, were collectively called the "Yong Ying" (Brave Camp).
Zeng Guofan had no prior military experience. Being a classically educated official, he took his blueprint for the Xiang Army from the Ming general Qi Jiguang, who, because of the weakness of regular Ming troops, had decided to form his own "private" army to repel raiding Japanese pirates in the mid-16th century. Qi Jiguang's doctrine was based on Neo-Confucian ideas of binding troops' loyalty to their immediate superiors and also to the regions in which they were raised. Zeng Guofan's original intention for the Xiang Army was simply to eradicate the Taiping rebels. However, the success of the Yongying system led to its becoming a permanent regional force within the Qing military, which in the long run created problems for the beleaguered central government.
First, the Yongying system signaled the end of Manchu dominance in Qing military establishment. Although the Banners and Green Standard armies lingered on as a drain on resources, henceforth the Yongying corps became the Qing government's de facto first-line troops. Second, the Yongying corps were financed through provincial coffers and were led by regional commanders, weakening central government's grip on the whole country. Finally, the nature of Yongying command structure fostered nepotism and cronyism amongst its commanders, who laid the seeds of regional warlordism in the first half of the 20th century.
By the late 19th century, the most conservative elements within the Qing court could no longer ignore China's military weakness. In 1860, during the Second Opium War, the capital Beijing was captured and the Summer Palace sacked by a relatively small Anglo-French coalition force numbering 25,000. The advent of modern weaponry resulting from the European Industrial Revolution had rendered China's traditionally trained and equipped army and navy obsolete. The government attempts to modernize during the Self-Strengthening Movement were initially successful, but yielded few lasting results because of the central government's lack of funds, lack of political will, and unwillingness to depart from tradition.
Losing the First Sino-Japanese War of 1894–1895 was a watershed. Japan, a country long regarded by the Chinese as little more than an upstart nation of pirates, annihilated the Qing government's modernized Beiyang Fleet, then deemed to be the strongest naval force in Asia. The Japanese victory occurred a mere three decades after the Meiji Restoration set a feudal Japan on course to emulate the Western nations in their economic and technological achievements. Finally, in December 1894, the Qing government took concrete steps to reform military institutions and to re-train selected units in westernized drills, tactics and weaponry. These units were collectively called the New Army. The most successful of these was the Beiyang Army under the overall supervision and control of a former Huai Army commander, General Yuan Shikai, who used his position to build networks of loyal officers and eventually become President of the Republic of China.
The most significant fact of early and mid-Qing social history was population growth. The population doubled during the 18th century. People in this period were also remarkably on the move. There is evidence suggesting that the empire's rapidly expanding population was geographically mobile on a scale, which, in term of its volume and its protracted and routinized nature, was unprecedented in Chinese history. Indeed, the Qing government did far more to encourage mobility than to discourage it. Migration took several different forms, though might be divided in two varieties: permanent migration for resettlement, and relocation conceived by the party (in theory at least) as a temporary sojourn. Parties to the latter would include the empire's increasingly large and mobile manual workforce, as well as its densely overlapping internal diaspora of local-origin-based merchant groups. It would also included the patterned movement of Qing subjects overseas, largely to Southeastern Asia, in search of trade and other economic opportunities.
According to statute, Qing society was divided into relatively closed estates, of which in most general terms there were five. Apart from the estates of the officials, the comparatively minuscule aristocracy, and the degree-holding literati, there also existed a major division among ordinary Chinese between commoners and people with inferior status. They were divided into two categories: one of them, the good "commoner" people, the other "mean" people. The majority of the population belonged to the first category and were described as liangmin, a legal term meaning good people, as opposed to jianmin meaning the mean (or ignoble) people. Qing law explicitly stated that the traditional four occupational groups of scholars, farmers, artisans and merchants were "good", or having a status of commoners. On the other hand, slaves or bondservants, entertainers (including prostitutes and actors), and those low-level employees of government officials were the "mean people". Mean people were considered legally inferior to commoners and suffered unequal treatments, forbidden to take the imperial examination.
By the end of the 17th century, the Chinese economy had recovered from the devastation caused by the wars in which the Ming dynasty were overthrown, and the resulting breakdown of order. In the following century, markets continued to expand as in the late Ming period, but with more trade between regions, a greater dependence on overseas markets and a greatly increased population. After the re-opening of the southeast coast, which had been closed in the late 17th century, foreign trade was quickly re-established, and was expanding at 4% per annum throughout the latter part of the 18th century. China continued to export tea, silk and manufactures, creating a large, favorable trade balance with the West. The resulting inflow of silver expanded the money supply, facilitating the growth of competitive and stable markets.
The government broadened land ownership by returning land that had been sold to large landowners in the late Ming period by families unable to pay the land tax. To give people more incentives to participate in the market, they reduced the tax burden in comparison with the late Ming, and replaced the corvée system with a head tax used to hire laborers. The administration of the Grand Canal was made more efficient, and transport opened to private merchants. A system of monitoring grain prices eliminated severe shortages, and enabled the price of rice to rise slowly and smoothly through the 18th century. Wary of the power of wealthy merchants, Qing rulers limited their trading licenses and usually refused them permission to open new mines, except in poor areas. These restrictions on domestic resource exploration, as well as on foreign trade, are held by some scholars as a cause of the Great Divergence, by which the Western world overtook China economically.
By the end of the 18th century the population had risen to 300 million from approximately 150 million during the late Ming dynasty. The dramatic rise in population was due to several reasons, including the long period of peace and stability in the 18th century and the import of new crops China received from the Americas, including peanuts, sweet potatoes and maize. New species of rice from Southeast Asia led to a huge increase in production. Merchant guilds proliferated in all of the growing Chinese cities and often acquired great social and even political influence. Rich merchants with official connections built up huge fortunes and patronized literature, theater and the arts. Textile and handicraft production boomed.
The Qing emperors were generally adept at poetry and often skilled in painting, and offered their patronage to Confucian culture. The Kangxi and Qianlong Emperors, for instance, embraced Chinese traditions both to control them and to proclaim their own legitimacy. The Kangxi Emperor sponsored the Peiwen Yunfu, a rhyme dictionary published in 1711, and the Kangxi Dictionary published in 1716, which remains to this day an authoritative reference. The Qianlong Emperor sponsored the largest collection of writings in Chinese history, the Siku Quanshu, completed in 1782. Court painters made new versions of the Song masterpiece, Zhang Zeduan's Along the River During the Qingming Festival whose depiction of a prosperous and happy realm demonstrated the beneficence of the emperor. The emperors undertook tours of the south and commissioned monumental scrolls to depict the grandeur of the occasion. Imperial patronage also encouraged the industrial production of ceramics and Chinese export porcelain.
Yet the most impressive aesthetic works were done among the scholars and urban elite. Calligraphy and painting remained a central interest to both court painters and scholar-gentry who considered the Four Arts part of their cultural identity and social standing. The painting of the early years of the dynasty included such painters as the orthodox Four Wangs and the individualists Bada Shanren (1626–1705) and Shitao (1641–1707). The nineteenth century saw such innovations as the Shanghai School and the Lingnan School which used the technical skills of tradition to set the stage for modern painting.
Literature grew to new heights in the Qing period. Poetry continued as a mark of the cultivated gentleman, but women wrote in larger and larger numbers and poets came from all walks of life. The poetry of the Qing dynasty is a lively field of research, being studied (along with the poetry of the Ming dynasty) for its association with Chinese opera, developmental trends of Classical Chinese poetry, the transition to a greater role for vernacular language, and for poetry by women in Chinese culture. The Qing dynasty was a period of much literary collection and criticism, and many of the modern popular versions of Classical Chinese poems were transmitted through Qing dynasty anthologies, such as the Quantangshi and the Three Hundred Tang Poems. Pu Songling brought the short story form to a new level in his Strange Stories from a Chinese Studio, published in the mid-18th century, and Shen Fu demonstrated the charm of the informal memoir in Six Chapters of a Floating Life, written in the early 19th century but published only in 1877. The art of the novel reached a pinnacle in Cao Xueqin's Dream of the Red Chamber, but its combination of social commentary and psychological insight were echoed in highly skilled novels such as Wu Jingzi's The Scholars (1750) and Li Ruzhen's Flowers in the Mirror (1827).
Cuisine aroused a cultural pride in the accumulated richness of a long and varied past. The gentleman gourmet, such as Yuan Mei, applied aesthetic standards to the art of cooking, eating, and appreciation of tea at a time when New World crops and products entered everyday life. The Suiyuan Shidan written by him, detailed the culinary esthetics and theory, along with a wide range of recipes from the ruling period of Qianlong during Qing Dynasty. The Manchu Han Imperial Feast originated at the court. Although this banquet was probably never common, it reflected an appreciation by Han Chinese for Manchu culinary customs. Nevertheless, culinary traditionalists such as Yuan Mei lambasted the opulent culinary rituals of the Manchu Han Imperial Feast, saying that it is cause in part by "...the vulgar habits of bad chefs" and that "Display this trite are useful only for welcoming new relations through one’s gates or when the boss comes to visit." (皆惡廚陋習。只可用之於新親上門，上司入境)
Josip Broz Tito (Cyrillic: Јосип Броз Тито, pronounced [jǒsip brôːz tîto]; born Josip Broz; 7 May 1892[nb 1] – 4 May 1980) was a Yugoslav revolutionary and statesman, serving in various roles from 1943 until his death in 1980. During World War II he was the leader of the Partisans, often regarded as the most effective resistance movement in occupied Europe. While his presidency has been criticized as authoritarian, and concerns about the repression of political opponents have been raised, Tito was "seen by most as a benevolent dictator" due to his economic and diplomatic policies. He was a popular public figure both in Yugoslavia and abroad. Viewed as a unifying symbol, his internal policies maintained the peaceful coexistence of the nations of the Yugoslav federation. He gained further international attention as the chief leader of the Non-Aligned Movement, working with Jawaharlal Nehru of India, Gamal Abdel Nasser of Egypt and Sukarno of Indonesia.
He was General Secretary (later Chairman of the Presidium) of the League of Communists of Yugoslavia (1939–80), and went on to lead the World War II Yugoslav guerrilla movement, the Partisans (1941–45). After the war, he was the Prime Minister (1944–63), President (later President for Life) (1953–80) of the Socialist Federal Republic of Yugoslavia (SFRY). From 1943 to his death in 1980, he held the rank of Marshal of Yugoslavia, serving as the supreme commander of the Yugoslav military, the Yugoslav People's Army (JNA). With a highly favourable reputation abroad in both Cold War blocs, Josip Broz Tito received some 98 foreign decorations, including the Legion of Honour and the Order of the Bath.
Josip Broz was born to a Croat father and Slovene mother in the village of Kumrovec, Croatia. Drafted into military service, he distinguished himself, becoming the youngest Sergeant Major in the Austro-Hungarian Army of that time. After being seriously wounded and captured by the Imperial Russians during World War I, Josip was sent to a work camp in the Ural Mountains. He participated in the October Revolution, and later joined a Red Guard unit in Omsk. Upon his return home, Broz found himself in the newly established Kingdom of Yugoslavia, where he joined the Communist Party of Yugoslavia (KPJ).
Tito was the chief architect of the second Yugoslavia, a socialist federation that lasted from 1943 to 1991–92. Despite being one of the founders of Cominform, soon he became the first Cominform member to defy Soviet hegemony and the only one to manage to leave Cominform and begin with its own socialist program. Tito was a backer of independent roads to socialism (sometimes referred to as "national communism"). In 1951 he implemented a self-management system that differentiated Yugoslavia from other socialist countries. A turn towards a model of market socialism brought economic expansion in the 1950s and 1960s and a decline during the 1970s. His internal policies included the suppression of nationalist sentiment and the promotion of the "brotherhood and unity" of the six Yugoslav nations. After Tito's death in 1980, tensions between the Yugoslav republics emerged and in 1991 the country disintegrated and went into a series of wars and unrest that lasted the rest of the decade, and which continue to impact most of the former Yugoslav republics. He remains a very controversial figure in the Balkans.
Josip Broz was born on 7 May 1892 in Kumrovec, in the northern Croatian region of Hrvatsko Zagorje in Austria-Hungary.[nb 1] He was the seventh child of Franjo and Marija Broz. His father, Franjo Broz (26 November 1860 – 16 December 1936), was a Croat, while his mother Marija (25 March 1864 – 14 January 1918), was a Slovene. His parents were married on 21 January 1891. After spending part of his childhood years with his maternal grandfather Martin Javeršek in the Slovenian village of Podsreda, he entered primary school in 1900 at Kumrovec, he failed the 2nd grade and graduated in 1905. In 1907 he moved out of the rural environment and started working as a machinist's apprentice in Sisak. There, he became aware of the labour movement and celebrated 1 May – Labour Day for the first time. In 1910, he joined the union of metallurgy workers and at the same time the Social-Democratic Party of Croatia and Slavonia. Between 1911 and 1913, Broz worked for shorter periods in Kamnik (1911–1912, factory "Titan"), Cenkov, Munich and Mannheim, where he worked for the Benz car factory; then he went to Wiener Neustadt, Austria, and worked as a test driver for Daimler.
In the autumn of 1913, he was conscripted into the Austro-Hungarian Army. He was sent to a school for non-commissioned officers and became a sergeant, serving in the 25th Croatian Regiment based in Zagreb. In May 1914, Broz won a silver medal at an army fencing competition in Budapest. At the outbreak of World War I in 1914, he was sent to Ruma, where he was arrested for anti-war propaganda and imprisoned in the Petrovaradin fortress. In January 1915, he was sent to the Eastern Front in Galicia to fight against Russia. He distinguished himself as a capable soldier, becoming the youngest Sergeant Major in the Austro-Hungarian Army. For his bravery in the face of the enemy, he was recommended for the Silver Bravery Medal but was taken prisoner of war before it could be formally presented. On 25 March 1915, while in Bukovina, he was seriously wounded and captured by the Russians.
After 13 months at the hospital, Broz was sent to a work camp in the Ural Mountains where prisoners selected him for their camp leader. In February 1917, revolting workers broke into the prison and freed the prisoners. Broz subsequently joined a Bolshevik group. In April 1917, he was arrested again but managed to escape and participate in the July Days demonstrations in Petrograd (St. Petersburg) on 16–17 July 1917. On his way to Finland, Broz was caught and imprisoned in the Peter and Paul Fortress for three weeks. He was again sent to Kungur, but escaped from the train. He hid with a Russian family in Omsk, Siberia where he met his future wife Pelagija Belousova. After the October Revolution, he joined a Red Guard unit in Omsk. Following a White counteroffensive, he fled to Kirgiziya and subsequently returned to Omsk, where he married Belousova. In the spring of 1918, he joined the Yugoslav section of the Russian Communist Party. By June of the same year, Broz left Omsk to find work and support his family, and was employed as a mechanic near Omsk for a year.
In January 1920, Tito and his wife made a long and difficult journey home to Yugoslavia where he arrived in September. Upon his return, Broz joined the Communist Party of Yugoslavia. The CPY's influence on the political life of the Kingdom of Yugoslavia was growing rapidly. In the 1920 elections the Communists won 59 seats in the parliament and became the third strongest party. Winning numerous local elections, they gained a stronghold in the second largest city of Zagreb, electing Svetozar Delić for mayor. After the assassination of Milorad Drašković, the Yugoslav Minister of the Interior, by a young communist on 2 August 1921, the CPY was declared illegal under the Yugoslav State Security Act of 1921. During 1920 and 1921 all Communist-won mandates were nullified. Broz continued his work underground despite pressure on Communists from the government. As 1921 began he moved to Veliko Trojstvo near Bjelovar and found work as a machinist. In 1925, Broz moved to Kraljevica where he started working at a shipyard. He was elected as a union leader and a year later he led a shipyard strike. He was fired and moved to Belgrade, where he worked in a train coach factory in Smederevska Palanka. He was elected as Workers' Commissary but was fired as soon as his CPY membership was revealed. Broz then moved to Zagreb, where he was appointed secretary of Metal Workers' Union of Croatia. In 1928, he became the Zagreb Branch Secretary of the CPY. In the same year he was arrested, tried in court for his illegal communist activities, and sent to jail. During his five years at Lepoglava prison he met Moša Pijade, who became his ideological mentor. After his release, he lived incognito and assumed numerous noms de guerre, among them "Walter" and "Tito".
In 1934 the Zagreb Provincial Committee sent Tito to Vienna where all the Central Committee of the Communist Party of Yugoslavia had sought refuge. He was appointed to the Committee and started to appoint allies to him, among them Edvard Kardelj, Milovan Đilas, Aleksandar Ranković and Boris Kidrič. In 1935, Tito travelled to the Soviet Union, working for a year in the Balkans section of Comintern. He was a member of the Soviet Communist Party and the Soviet secret police (NKVD). Tito was also involved in recruiting for the Dimitrov Battalion, a group of volunteers serving in the Spanish Civil War. In 1936, the Comintern sent "Comrade Walter" (i.e. Tito) back to Yugoslavia to purge the Communist Party there. In 1937, Stalin had the Secretary-General of the CPY, Milan Gorkić, murdered in Moscow. Subsequently Tito was appointed Secretary-General of the still-outlawed CPY.
On 6 April 1941, German forces, with Hungarian and Italian assistance, launched an invasion of Yugoslavia. On 10 April 1941, Slavko Kvaternik proclaimed the Independent State of Croatia, and Tito responded by forming a Military Committee within the Central Committee of the Yugoslav Communist Party. Attacked from all sides, the armed forces of the Kingdom of Yugoslavia quickly crumbled. On 17 April 1941, after King Peter II and other members of the government fled the country, the remaining representatives of the government and military met with the German officials in Belgrade. They quickly agreed to end military resistance. On 1 May 1941, Tito issued a pamphlet calling on the people to unite in a battle against the occupation. On 27 June 1941, the Central Committee of the Communist Party of Yugoslavia appointed Tito Commander in Chief of all project national liberation military forces. On 1 July 1941, the Comintern sent precise instructions calling for immediate action.
Despite conflicts with the rival monarchic Chetnik movement, Tito's Partisans succeeded in liberating territory, notably the "Republic of Užice". During this period, Tito held talks with Chetnik leader Draža Mihailović on 19 September and 27 October 1941. It is said that Tito ordered his forces to assist escaping Jews, and that more than 2,000 Jews fought directly for Tito.
On 21 December 1941, the Partisans created the First Proletarian Brigade (commanded by Koča Popović) and on 1 March 1942, Tito created the Second Proletarian Brigade. In liberated territories, the Partisans organised People's Committees to act as civilian government. The Anti-Fascist Council of National Liberation of Yugoslavia (AVNOJ) convened in Bihać on 26–27 November 1942 and in Jajce on 29 November 1943. In the two sessions, the resistance representatives established the basis for post-war organisation of the country, deciding on a federation of the Yugoslav nations. In Jajce, a 67-member "presidency" was elected and established a nine-member National Committee of Liberation (five communist members) as a de facto provisional government. Tito was named President of the National Committee of Liberation.
With the growing possibility of an Allied invasion in the Balkans, the Axis began to divert more resources to the destruction of the Partisans main force and its high command. This meant, among other things, a concerted German effort to capture Josip Broz Tito personally. On 25 May 1944, he managed to evade the Germans after the Raid on Drvar (Operation Rösselsprung), an airborne assault outside his Drvar headquarters in Bosnia.
After the Partisans managed to endure and avoid these intense Axis attacks between January and June 1943, and the extent of Chetnik collaboration became evident, Allied leaders switched their support from Draža Mihailović to Tito. King Peter II, American President Franklin Roosevelt and British Prime Minister Winston Churchill joined Soviet Premier Joseph Stalin in officially recognising Tito and the Partisans at the Tehran Conference. This resulted in Allied aid being parachuted behind Axis lines to assist the Partisans. On 17 June 1944 on the Dalmatian island of Vis, the Treaty of Vis (Viški sporazum) was signed in an attempt to merge Tito's government (the AVNOJ) with the government in exile of King Peter II. The Balkan Air Force was formed in June 1944 to control operations that were mainly aimed at aiding his forces.
In the first post war years Tito was widely considered a communist leader very loyal to Moscow, indeed, he was often viewed as second only to Stalin in the Eastern Bloc. In fact, Stalin and Tito had an uneasy alliance from the start, with Stalin considering Tito too independent.
On 12 September 1944, King Peter II called on all Yugoslavs to come together under Tito's leadership and stated that those who did not were "traitors", by which time Tito was recognized by all Allied authorities (including the government-in-exile) as the Prime Minister of Yugoslavia, in addition to commander-in-chief of the Yugoslav forces. On 28 September 1944, the Telegraph Agency of the Soviet Union (TASS) reported that Tito signed an agreement with the Soviet Union allowing "temporary entry" of Soviet troops into Yugoslav territory which allowed the Red Army to assist in operations in the northeastern areas of Yugoslavia. With their strategic right flank secured by the Allied advance, the Partisans prepared and executed a massive general offensive which succeeded in breaking through German lines and forcing a retreat beyond Yugoslav borders. After the Partisan victory and the end of hostilities in Europe, all external forces were ordered off Yugoslav territory.
In the final days of World War II in Yugoslavia, units of the Partisans were responsible for atrocities after the repatriations of Bleiburg, and accusations of culpability were later raised at the Yugoslav leadership under Tito. At the time, Josip Broz Tito repeatedly issued calls for surrender to the retreating column, offering amnesty and attempting to avoid a disorderly surrender. On 14 May he dispatched a telegram to the supreme headquarters Slovene Partisan Army prohibiting "in the sternest language" the execution of prisoners of war and commanding the transfer of the possible suspects to a military court.
On 7 March 1945, the provisional government of the Democratic Federal Yugoslavia (Demokratska Federativna Jugoslavija, DFY) was assembled in Belgrade by Josip Broz Tito, while the provisional name allowed for either a republic or monarchy. This government was headed by Tito as provisional Yugoslav Prime Minister and included representatives from the royalist government-in-exile, among others Ivan Šubašić. In accordance with the agreement between resistance leaders and the government-in-exile, post-war elections were held to determine the form of government. In November 1945, Tito's pro-republican People's Front, led by the Communist Party of Yugoslavia, won the elections with an overwhelming majority, the vote having been boycotted by monarchists. During the period, Tito evidently enjoyed massive popular support due to being generally viewed by the populace as the liberator of Yugoslavia. The Yugoslav administration in the immediate post-war period managed to unite a country that had been severely affected by ultra-nationalist upheavals and war devastation, while successfully suppressing the nationalist sentiments of the various nations in favor of tolerance, and the common Yugoslav goal. After the overwhelming electoral victory, Tito was confirmed as the Prime Minister and the Minister of Foreign Affairs of the DFY. The country was soon renamed the Federal People's Republic of Yugoslavia (FPRY) (later finally renamed into Socialist Federal Republic of Yugoslavia, SFRY). On 29 November 1945, King Peter II was formally deposed by the Yugoslav Constituent Assembly. The Assembly drafted a new republican constitution soon afterwards.
Yugoslavia organized the Yugoslav People's Army (Jugoslavenska narodna armija, or JNA) from the Partisan movement and became the fourth strongest army in Europe at the time. The State Security Administration (Uprava državne bezbednosti/sigurnosti/varnosti, UDBA) was also formed as the new secret police, along with a security agency, the Department of People's Security (Organ Zaštite Naroda (Armije), OZNA). Yugoslav intelligence was charged with imprisoning and bringing to trial large numbers of Nazi collaborators; controversially, this included Catholic clergymen due to the widespread involvement of Croatian Catholic clergy with the Ustaša regime. Draža Mihailović was found guilty of collaboration, high treason and war crimes and was subsequently executed by firing squad in July 1946.
Prime Minister Josip Broz Tito met with the president of the Bishops' Conference of Yugoslavia, Aloysius Stepinac on 4 June 1945, two days after his release from imprisonment. The two could not reach an agreement on the state of the Catholic Church. Under Stepinac's leadership, the bishops' conference released a letter condemning alleged Partisan war crimes in September, 1945. The following year Stepinac was arrested and put on trial. In October 1946, in its first special session for 75 years, the Vatican excommunicated Tito and the Yugoslav government for sentencing Stepinac to 16 years in prison on charges of assisting Ustaše terror and of supporting forced conversions of Serbs to Catholicism. Stepinac received preferential treatment in recognition of his status and the sentence was soon shortened and reduced to house-arrest, with the option of emigration open to the archbishop. At the conclusion of the "Informbiro period", reforms rendered Yugoslavia considerably more religiously liberal than the Eastern Bloc states.
Unlike other new communist states in east-central Europe, Yugoslavia liberated itself from Axis domination with limited direct support from the Red Army. Tito's leading role in liberating Yugoslavia not only greatly strengthened his position in his party and among the Yugoslav people, but also caused him to be more insistent that Yugoslavia had more room to follow its own interests than other Bloc leaders who had more reasons (and pressures) to recognize Soviet efforts in helping them liberate their own countries from Axis control. Although Tito was formally an ally of Stalin after World War II, the Soviets had set up a spy ring in the Yugoslav party as early as 1945, giving way to an uneasy alliance.[citation needed]
In the immediate aftermath of World War II, there occurred several armed incidents between Yugoslavia and the Western Allies. Following the war, Yugoslavia acquired the Italian territory of Istria as well as the cities of Zadar and Rijeka. Yugoslav leadership was looking to incorporate Trieste into the country as well, which was opposed by the Western Allies. This led to several armed incidents, notably attacks by Yugoslav fighter planes on US transport aircraft, causing bitter criticism from the west. From 1945 to 1948, at least four US aircraft were shot down.[better source needed] Stalin was opposed to these provocations, as he felt the USSR unready to face the West in open war so soon after the losses of World War II and at the time when US had operational nuclear weapons whereas USSR had yet to conduct its first test. In addition, Tito was openly supportive of the Communist side in the Greek Civil War, while Stalin kept his distance, having agreed with Churchill not to pursue Soviet interests there, although he did support the Greek communist struggle politically, as demonstrated in several assemblies of the UN Security Council. In 1948, motivated by the desire to create a strong independent economy, Tito modeled his economic development plan independently from Moscow, which resulted in a diplomatic escalation followed by a bitter exchange of letters in which Tito affirmed that
The Soviet answer on 4 May admonished Tito and the Communist Party of Yugoslavia (CPY) for failing to admit and correct its mistakes, and went on to accuse them of being too proud of their successes against the Germans, maintaining that the Red Army had saved them from destruction. Tito's response on 17 May suggested that the matter be settled at the meeting of the Cominform to be held that June. However, Tito did not attend the second meeting of the Cominform, fearing that Yugoslavia was to be openly attacked. In 1949 the crisis nearly escalated into an armed conflict, as Hungarian and Soviet forces were massing on the northern Yugoslav frontier. On 28 June, the other member countries expelled Yugoslavia, citing "nationalist elements" that had "managed in the course of the past five or six months to reach a dominant position in the leadership" of the CPY. The assumption in Moscow was that once it was known that he had lost Soviet approval, Tito would collapse; 'I will shake my little finger and there will be no more Tito,' Stalin remarked. The expulsion effectively banished Yugoslavia from the international association of socialist states, while other socialist states of Eastern Europe subsequently underwent purges of alleged "Titoists". Stalin took the matter personally and arranged several assassination attempts on Tito, none of which succeeded. In a correspondence between the two leaders, Tito openly wrote:
One significant consequence of the tension arising between Yugoslavia and Soviet Union, was that Tito fought Yugoslav Stalinists with Stalin's methods. In other words, Aleksandar Ranković and the State Security Service (UBDA) employed the same inhumane methods against their opponents as Stalin did in the Soviet Union against his. Not every person accused of a political crime was convicted and nobody was sentenced to death for his or her pro-Soviet feelings. However this repression, which lasted until 1956, was marked by significant violations of human rights.
Tito's estrangement from the USSR enabled Yugoslavia to obtain US aid via the Economic Cooperation Administration (ECA), the same US aid institution which administered the Marshall Plan. Still, he did not agree to align with the West, which was a common consequence of accepting American aid at the time. After Stalin's death in 1953, relations with the USSR were relaxed and he began to receive aid as well from the COMECON. In this way, Tito played East-West antagonism to his advantage. Instead of choosing sides, he was instrumental in kick-starting the Non-Aligned Movement, which would function as a 'third way' for countries interested in staying outside of the East-West divide.
The event was significant not only for Yugoslavia and Tito, but also for the global development of socialism, since it was the first major split between Communist states, casting doubt on Comintern's claims for socialism to be a unified force that would eventually control the whole world, as Tito became the first (and the only successful) socialist leader to defy Stalin's leadership in the COMINFORM. This rift with the Soviet Union brought Tito much international recognition, but also triggered a period of instability often referred to as the Informbiro period. Tito's form of communism was labeled "Titoism" by Moscow, which encouraged purges against suspected "Titoites'" throughout the Eastern bloc.
On 26 June 1950, the National Assembly supported a crucial bill written by Milovan Đilas and Tito about "self-management" (samoupravljanje): a type of cooperative independent socialist experiment that introduced profit sharing and workplace democracy in previously state-run enterprises which then became the direct social ownership of the employees. On 13 January 1953, they established that the law on self-management was the basis of the entire social order in Yugoslavia. Tito also succeeded Ivan Ribar as the President of Yugoslavia on 14 January 1953. After Stalin's death Tito rejected the USSR's invitation for a visit to discuss normalization of relations between two nations. Nikita Khrushchev and Nikolai Bulganin visited Tito in Belgrade in 1955 and apologized for wrongdoings by Stalin's administration. Tito visited the USSR in 1956, which signaled to the world that animosity between Yugoslavia and USSR was easing. However, the relationship between the USSR and Yugoslavia would reach another low in the late 1960s. Commenting on the crisis, Tito concluded that:
The Tito-Stalin split had large ramifications for countries outside the USSR and Yugoslavia. It has, for example, been given as one of the reasons for the Slánský trial in Czechoslovakia, in which 14 high-level Communist officials were purged, with 11 of them being executed. Stalin put pressure on Czechoslovakia to conduct purges in order to discourage the spread of the idea of a "national path to socialism," which Tito espoused.
Under Tito's leadership, Yugoslavia became a founding member of the Non-Aligned Movement. In 1961, Tito co-founded the movement with Egypt's Gamal Abdel Nasser, India's Jawaharlal Nehru, Indonesia's Sukarno and Ghana's Kwame Nkrumah, in an action called The Initiative of Five (Tito, Nehru, Nasser, Sukarno, Nkrumah), thus establishing strong ties with third world countries. This move did much to improve Yugoslavia's diplomatic position. On 1 September 1961, Josip Broz Tito became the first Secretary-General of the Non-Aligned Movement.
Tito's foreign policy led to relationships with a variety of governments, such as exchanging visits (1954 and 1956) with Emperor Haile Selassie of Ethiopia, where a street was named in his honor.
Tito was notable for pursuing a foreign policy of neutrality during the Cold War and for establishing close ties with developing countries. Tito's strong belief in self-determination caused early rift with Stalin and consequently, the Eastern Bloc. His public speeches often reiterated that policy of neutrality and cooperation with all countries would be natural as long as these countries did not use their influence to pressure Yugoslavia to take sides. Relations with the United States and Western European nations were generally cordial.
Yugoslavia had a liberal travel policy permitting foreigners to freely travel through the country and its citizens to travel worldwide, whereas it was limited by most Communist countries. A number[quantify] of Yugoslav citizens worked throughout Western Europe. Tito met many world leaders during his rule, such as Soviet rulers Joseph Stalin, Nikita Khrushchev and Leonid Brezhnev; Egypt's Gamal Abdel Nasser, Indian politicians Jawaharlal Nehru and Indira Gandhi; British Prime Ministers Winston Churchill, James Callaghan and Margaret Thatcher; U.S. Presidents Dwight D. Eisenhower, John F. Kennedy, Richard Nixon, Gerald Ford and Jimmy Carter; other political leaders, dignitaries and heads of state that Tito met at least once in his lifetime included Che Guevara, Fidel Castro, Yasser Arafat, Willy Brandt, Helmut Schmidt, Georges Pompidou, Queen Elizabeth II, Hua Guofeng, Kim Il Sung, Sukarno, Sheikh Mujibur Rahman, Suharto, Idi Amin, Haile Selassie, Kenneth Kaunda, Gaddafi, Erich Honecker, Nicolae Ceaușescu, János Kádár and Urho Kekkonen. He also met numerous celebrities.
Tito visited India from December 22, 1954 through January 8, 1955. After his return, he removed many restrictions on churches and spiritual institutions in Yugoslavia.
Tito also developed warm relations with Burma under U Nu, travelling to the country in 1955 and again in 1959, though he didn't receive the same treatment in 1959 from the new leader, Ne Win.
Because of its neutrality, Yugoslavia would often be rare among Communist countries to have diplomatic relations with right-wing, anti-Communist governments. For example, Yugoslavia was the only communist country allowed to have an embassy in Alfredo Stroessner's Paraguay. One notable exception to Yugoslavia's neutral stance toward anti-communist countries was Chile under Pinochet; Yugoslavia was one of many countries which severed diplomatic relations with Chile after Salvador Allende was overthrown. Yugoslavia also provided military aid and arms supplies to staunchly anti-Communist regimes such as that of Guatemala under Kjell Eugenio Laugerud García.
On 7 April 1963, the country changed its official name to the Socialist Federal Republic of Yugoslavia. Reforms encouraged private enterprise and greatly relaxed restrictions on freedom of speech and religious expression. Tito subsequently went on a tour of the Americas. In Chile, two government ministers resigned over his visit to that country. In the autumn of 1960 Tito met President Dwight D. Eisenhower at the United Nations General Assembly meeting. Tito and Eisenhower discussed a range of issues from arms control to economic development. When Eisenhower remarked that Yugoslavia's neutralism was "neutral on his side", Tito replied that neutralism did not imply passivity but meant "not taking sides".
In 1966 an agreement with the Vatican, fostered in part by the death in 1960 of anti-communist archbishop of Zagreb Aloysius Stepinac and shifts in the church's approach to resisting communism originating in the Second Vatican Council, accorded new freedom to the Yugoslav Roman Catholic Church, particularly to catechize and open seminaries. The agreement also eased tensions, which had prevented the naming of new bishops in Yugoslavia since 1945. Tito's new socialism met opposition from traditional communists culminating in conspiracy headed by Aleksandar Ranković. In the same year Tito declared that Communists must henceforth chart Yugoslavia's course by the force of their arguments (implying an abandonment of Leninist orthodoxy and development of liberal Communism). The State Security Administration (UDBA) saw its power scaled back and its staff reduced to 5000.
On 1 January 1967, Yugoslavia was the first communist country to open its borders to all foreign visitors and abolish visa requirements. In the same year Tito became active in promoting a peaceful resolution of the Arab–Israeli conflict. His plan called for Arabs to recognize the state of Israel in exchange for territories Israel gained.
In 1968, Tito offered Czechoslovak leader Alexander Dubček to fly to Prague on three hours notice if Dubček needed help in facing down the Soviets. In April 1969, Tito removed generals Ivan Gošnjak and Rade Hamović in the aftermath of the invasion of Czechoslovakia due to the unpreparedness of the Yugoslav army to respond to a similar invasion of Yugoslavia.
In 1971, Tito was re-elected as President of Yugoslavia by the Federal Assembly for the sixth time. In his speech before the Federal Assembly he introduced 20 sweeping constitutional amendments that would provide an updated framework on which the country would be based. The amendments provided for a collective presidency, a 22-member body consisting of elected representatives from six republics and two autonomous provinces. The body would have a single chairman of the presidency and chairmanship would rotate among six republics. When the Federal Assembly fails to agree on legislation, the collective presidency would have the power to rule by decree. Amendments also provided for stronger cabinet with considerable power to initiate and pursue legislature independently from the Communist Party. Džemal Bijedić was chosen as the Premier. The new amendments aimed to decentralize the country by granting greater autonomy to republics and provinces. The federal government would retain authority only over foreign affairs, defense, internal security, monetary affairs, free trade within Yugoslavia, and development loans to poorer regions. Control of education, healthcare, and housing would be exercised entirely by the governments of the republics and the autonomous provinces.
Tito's greatest strength, in the eyes of the western communists, had been in suppressing nationalist insurrections and maintaining unity throughout the country. It was Tito's call for unity, and related methods, that held together the people of Yugoslavia. This ability was put to a test several times during his reign, notably during the Croatian Spring (also referred as the Masovni pokret, maspok, meaning "Mass Movement") when the government suppressed both public demonstrations and dissenting opinions within the Communist Party. Despite this suppression, much of maspok's demands were later realized with the new constitution, heavily backed by Tito himself against opposition from the Serbian branch of the party.[citation needed] On 16 May 1974, the new Constitution was passed, and the aging Tito was named president for life, a status which he would enjoy for five years.
Tito's visits to the United States avoided most of the Northeast due to large minorities of Yugoslav emigrants bitter about communism in Yugoslavia. Security for the state visits was usually high to keep him away from protesters, who would frequently burn the Yugoslav flag. During a visit to the United Nations in the late 1970s emigrants shouted "Tito murderer" outside his New York hotel, for which he protested to United States authorities.
After the constitutional changes of 1974, Tito began reducing his role in the day-to-day running of the state. He continued to travel abroad and receive foreign visitors, going to Beijing in 1977 and reconciling with a Chinese leadership that had once branded him a revisionist. In turn, Chairman Hua Guofeng visited Yugoslavia in 1979. In 1978, Tito traveled to the U.S. During the visit strict security was imposed in Washington, D.C. owing to protests by anti-communist Croat, Serb and Albanian groups.
Tito became increasingly ill over the course of 1979. During this time Vila Srna was built for his use near Morović in the event of his recovery. On 7 January and again on 11 January 1980, Tito was admitted to the Medical Centre in Ljubljana, the capital city of the SR Slovenia, with circulation problems in his legs. His left leg was amputated soon afterward due to arterial blockages and he died of gangrene at the Medical Centre Ljubljana on 4 May 1980 at 15:05, three days short of his 88th birthday. His funeral drew many world statesmen. Based on the number of attending politicians and state delegations, at the time it was the largest state funeral in history; this concentration of dignitaries would be unmatched until the funeral of Pope John Paul II in 2005 and the memorial service of Nelson Mandela in 2013. Those who attended included four kings, 31 presidents, six princes, 22 prime ministers and 47 ministers of foreign affairs. They came from both sides of the Cold War, from 128 different countries out of 154 UN members at the time.
Tito was interred in a mausoleum in Belgrade, which forms part of a memorial complex in the grounds of the Museum of Yugoslav History (formerly called "Museum 25 May" and "Museum of the Revolution"). The actual mausoleum is called House of Flowers (Kuća Cveća) and numerous people visit the place as a shrine to "better times". The museum keeps the gifts Tito received during his presidency. The collection also includes original prints of Los Caprichos by Francisco Goya, and many others. The Government of Serbia has planned to merge it into the Museum of the History of Serbia. At the time of his death, speculation began about whether his successors could continue to hold Yugoslavia together. Ethnic divisions and conflict grew and eventually erupted in a series of Yugoslav wars a decade after his death.
During his life and especially in the first year after his death, several places were named after Tito. Several of these places have since returned to their original names, such as Podgorica, formerly Titograd (though Podgorica's international airport is still identified by the code TGD), and Užice, formerly Titovo Užice, which reverted to its original name in 1992. Streets in Belgrade, the capital, have all reverted to their original pre–World War II and pre-communist names as well. In 2004, Antun Augustinčić's statue of Broz in his birthplace of Kumrovec was decapitated in an explosion. It was subsequently repaired. Twice in 2008, protests took place in Zagreb's Marshal Tito Square, organized by a group called Circle for the Square (Krug za Trg), with an aim to force the city government to rename it to its previous name, while a counter-protest by Citizens' Initiative Against Ustašism (Građanska inicijativa protiv ustaštva) accused the "Circle for the Square" of historical revisionism and neo-fascism. Croatian president Stjepan Mesić criticized the demonstration to change the name. In the Croatian coastal city of Opatija the main street (also its longest street) still bears the name of Marshal Tito, as do streets in numerous towns in Serbia, mostly in the country's north. One of the main streets in downtown Sarajevo is called Marshal Tito Street, and Tito's statue in a park in front of the university campus (ex. JNA barrack "Maršal Tito") in Marijin Dvor is a place where Bosnians and Sarajevans still today commemorate and pay tribute to Tito (image on the right). The largest Tito monument in the world, about 10 m (33 ft) high, is located at Tito Square (Slovene: Titov trg), the central square in Velenje, Slovenia. One of the main bridges in Slovenia's second largest city of Maribor is Tito Bridge (Titov most). The central square in Koper, the largest Slovenian port city, is as well named Tito Square.
Every year a "Brotherhood and Unity" relay race is organized in Montenegro, Macedonia and Serbia which ends at the "House of Flowers" in Belgrade on May 25 – the final resting place of Tito. At the same time, runners in Slovenia, Croatia and Bosnia and Herzegovina set off for Kumrovec, Tito's birthplace in northern Croatia. The relay is a left-over from Yugoslav times, when young people made a similar yearly trek on foot through Yugoslavia that ended in Belgrade with a massive celebration.
In the years following the dissolution of Yugoslavia, some historians stated that human rights were suppressed in Yugoslavia under Tito, particularly in the first decade up until the Tito-Stalin split. On 4 October 2011, the Slovenian Constitutional Court found a 2009 naming of a street in Ljubljana after Tito to be unconstitutional. While several public areas in Slovenia (named during the Yugoslav period) do already bear Tito's name, on the issue of renaming an additional street the court ruled that:
The court, however, explicitly made it clear that the purpose of the review was "not a verdict on Tito as a figure or on his concrete actions, as well as not a historical weighing of facts and circumstances". Slovenia has several streets and squares named after Tito, notably Tito Square in Velenje, incorporating a 10-meter statue.
Tito has also been named as responsible for systematic eradication of the ethnic German (Danube Swabian) population in Vojvodina by expulsions and mass executions following the collapse of the German occupation of Yugoslavia at the end of World War II, in contrast to his inclusive attitude towards other Yugoslav nationalities.
Tito carried on numerous affairs and was married several times. In 1918 he was brought to Omsk, Russia, as a prisoner of war. There he met Pelagija Belousova who was then thirteen; he married her a year later, and she moved with him to Yugoslavia. Pelagija bore him five children but only their son Žarko Leon (born 4 February, 1924) survived. When Tito was jailed in 1928, she returned to Russia. After the divorce in 1936 she later remarried.
In 1936, when Tito stayed at the Hotel Lux in Moscow, he met the Austrian comrade Lucia Bauer. They married in October 1936, but the records of this marriage were later erased.
His next relationship was with Herta Haas, whom he married in 1940. Broz left for Belgrade after the April War, leaving Haas pregnant. In May 1941, she gave birth to their son, Aleksandar "Mišo" Broz. All throughout his relationship with Haas, Tito had maintained a promiscuous life and had a parallel relationship with Davorjanka Paunović, who, under the codename "Zdenka", served as a courier in the resistance and subsequently became his personal secretary. Haas and Tito suddenly parted company in 1943 in Jajce during the second meeting of AVNOJ after she reportedly walked in on him and Davorjanka. The last time Haas saw Broz was in 1946. Davorjanka died of tuberculosis in 1946 and Tito insisted that she be buried in the backyard of the Beli Dvor, his Belgrade residence.
His best known wife was Jovanka Broz. Tito was just shy of his 59th birthday, while she was 27, when they finally married in April 1952, with state security chief Aleksandar Ranković as the best man. Their eventual marriage came about somewhat unexpectedly since Tito actually rejected her some years earlier when his confidante Ivan Krajacic brought her in originally. At that time, she was in her early 20s and Tito, objecting to her energetic personality, opted for the more mature opera singer Zinka Kunc instead. Not one to be discouraged easily, Jovanka continued working at Beli Dvor, where she managed the staff and eventually got another chance after Tito's strange relationship with Zinka failed. Since Jovanka was the only female companion he married while in power, she also went down in history as Yugoslavia's first lady. Their relationship was not a happy one, however. It had gone through many, often public, ups and downs with episodes of infidelities and even allegations of preparation for a coup d'état by the latter pair. Certain unofficial reports suggest Tito and Jovanka even formally divorced in the late 1970s, shortly before his death. However, during Tito's funeral she was officially present as his wife, and later claimed rights for inheritance. The couple did not have any children.
Tito's notable grandchildren include Aleksandra Broz, a prominent theatre director in Croatia; Svetlana Broz, a cardiologist and writer in Bosnia-Herzegovina; and Josip "Joška" Broz, Edvard Broz and Natali Klasevski, an artisan of Bosnia-Herzegovina.
As the President, Tito had access to extensive (state-owned) property associated with the office, and maintained a lavish lifestyle. In Belgrade he resided in the official residence, the Beli dvor, and maintained a separate private home. The Brijuni islands were the site of the State Summer Residence from 1949 on. The pavilion was designed by Jože Plečnik, and included a zoo. Close to 100 foreign heads of state were to visit Tito at the island residence, along with film stars such as Elizabeth Taylor, Richard Burton, Sophia Loren, Carlo Ponti, and Gina Lollobrigida.
Another residence was maintained at Lake Bled, while the grounds at Karađorđevo were the site of "diplomatic hunts". By 1974 the Yugoslav President had at his disposal 32 official residences, larger and small, the yacht Galeb ("seagull"), a Boeing 727 as the presidential airplane, and the Blue Train. After Tito's death the presidential Boeing 727 was sold to Aviogenex, the Galeb remained docked in Montenegro, while the Blue Train was stored in a Serbian train shed for over two decades. While Tito was the person who held the office of president for by far the longest period, the associated property was not private and much of it continues to be in use by Yugoslav successor states, as public property, or maintained at the disposal of high-ranking officials.
As regards knowledge of languages, Tito replied that he spoke Serbo-Croatian, German, Russian, and some English. A biographer also stated that he spoke "Serbo-Croatian ... Russian, Czech, Slovenian ... German (with a Viennese accent) ... understands and reads French and Italian ... [and] also speaks Kirghiz."
In his youth Tito attended Catholic Sunday school, and was later an altar boy. After an incident where he was slapped and shouted at by a priest when he had difficulty assisting the priest to remove his vestments, Tito would not enter a church again. As an adult, he frequently declared that he was an atheist.
Every federal unit had a town or city with historic significance from the World War II period renamed to have Tito's name included. The largest of these was Titograd, now Podgorica, the capital city of Montenegro. With the exception of Titograd, the cities were renamed simply by the addition of the adjective "Tito's" ("Titov"). The cities were:
In the years after Tito's death up to nowadays, some people have disputed his identity. Tito's personal doctor, Aleksandar Matunović, wrote a book about Tito in which he also questioned his true origin, noting that Tito's habits and lifestyle could only mean that he was from an aristocratic family. Serbian journalist Vladan Dinić (born 1949), in Tito nije tito, includes several possible alternate identities of Tito.
In 2013 a lot of media coverage was given to unclassified NSA's study in Cryptologic Spectrum that concluded that Tito did not speak the language as a native, and had features of other Slavic languages (Russian and Polish). The hypothesis that "a non-Yugoslav, perhaps a Russian or a Pole" assumed Tito's identity was included. The report also notes Draža Mihailović's impressions of Tito's Russian origins.
However, the NSA's report was completely disproved by Croatian experts. The report failed to recognize that Tito was a native speaker of the very distinctive local Kajkavian dialect of Zagorje. The acute accent, present only in Croatian dialects, which Tito is perfectly pronouncing, is the strongest proof of Tito's belonging to Kajkavian dialect.
As the Communist Party was outlawed in Yugoslavia starting on 30 December 1920, Josip Broz took on many assumed names during his activity within the Party, including "Rudi", "Walter", and "Tito." Broz himself explains:
Josip Broz Tito received a total of 119 awards and decorations from 60 countries around the world (59 countries and Yugoslavia). 21 decorations were from Yugoslavia itself, 18 having been awarded once, and the Order of the National Hero on three occasions. Of the 98 international awards and decorations, 92 were received once, and three on two occasions (Order of the White Lion, Polonia Restituta, and Karl Marx). The most notable awards included the French Legion of Honour and National Order of Merit, the British Order of the Bath, the Soviet Order of Lenin, the Japanese Order of the Chrysanthemum, the German Federal Cross of Merit, and the Order of Merit of Italy.
The decorations were seldom displayed, however. After the Tito–Stalin split of 1948 and his inauguration as president in 1953, Tito rarely wore his uniform except when present in a military function, and then (with rare exception) only wore his Yugoslav ribbons for obvious practical reasons. The awards were displayed in full number only at his funeral in 1980. Tito's reputation as one of the Allied leaders of World War II, along with his diplomatic position as the founder of the Non-Aligned Movement, was primarily the cause of the favorable international recognition.
Some of the other foreign awards and decorations of Josip Broz Tito include Order of Merit, Order of Manuel Amador Guerrero, Order of Prince Henry, Order of Independence, Order of Merit, Order of the Nile, Order of the Condor of the Andes, Order of the Star of Romania, Order of the Gold Lion of the House of Nassau, Croix de Guerre, Order of the Cross of Grunwald, Czechoslovak War Cross, Decoration of Honour for Services to the Republic of Austria, Military Order of the White Lion, Nishan-e-Pakistan, Order of Al Rafidain, Order of Carol I, Order of Georgi Dimitrov, Order of Karl Marx, Order of Manuel Amador Guerrero, Order of Michael the Brave, Order of Pahlavi, Order of Sukhbaatar, Order of Suvorov, Order of the Liberator, Order of the October Revolution, Order of the Queen of Sheba, Order of the White Rose of Finland, Partisan Cross, Royal Order of Cambodia and Star of People's Friendship and Thiri Thudhamma Thingaha.[citation needed]
The term "Great Plains", for the region west of about the 96th or 98th meridian and east of the Rocky Mountains, was not generally used before the early 20th century. Nevin Fenneman's 1916 study, Physiographic Subdivision of the United States, brought the term Great Plains into more widespread usage. Before that the region was almost invariably called the High Plains, in contrast to the lower Prairie Plains of the Midwestern states. Today the term "High Plains" is used for a subregion of the Great Plains.
Much of the Great Plains became open range, or rangeland where cattle roamed free, hosting ranching operations where anyone was theoretically free to run cattle. In the spring and fall, ranchers held roundups where their cowboys branded new calves, treated animals and sorted the cattle for sale. Such ranching began in Texas and gradually moved northward. In 1866-95, cowboys herded 10 million cattle north to rail heads such as Dodge City, Kansas and Ogallala, Nebraska; from there, cattle were shipped eastward.
With the arrival of Francisco Vázquez de Coronado, a Spanish conquistador, the first recorded history of encounter between Europeans and Native Americans in the Great Plains occurred in Texas, Kansas and Nebraska from 1540-1542. In that same time period, Hernando de Soto crossed a west-northwest direction in what is now Oklahoma and Texas. Today this is known as the De Soto Trail. The Spanish thought the Great Plains were the location of the mythological Quivira and Cíbola, a place said to be rich in gold.
The 100th meridian roughly corresponds with the line that divides the Great Plains into an area that receive 20 inches (510 millimetres) or more of rainfall per year and an area that receives less than 20 in (510 mm). In this context, the High Plains, as well as Southern Alberta, south-western Saskatchewan and Eastern Montana are mainly semi hot steppe land and are generally characterised by rangeland or marginal farmland. The region (especially the High Plains) is periodically subjected to extended periods of drought; high winds in the region may then generate devastating dust storms. The eastern Great Plains near the eastern boundary falls in the humid subtropical climate zone in the southern areas, and the northern and central areas fall in the humid continental climate.
After 1870, the new railroads across the Plains brought hunters who killed off almost all the bison for their hides. The railroads offered attractive packages of land and transportation to European farmers, who rushed to settle the land. They (and Americans as well) also took advantage of the homestead laws to obtain free farms. Land speculators and local boosters identified many potential towns, and those reached by the railroad had a chance, while the others became ghost towns. In Kansas, for example, nearly 5000 towns were mapped out, but by 1970 only 617 were actually operating. In the mid-20th century, closeness to an interstate exchange determined whether a town would flourish or struggle for business.
The rural Plains have lost a third of their population since 1920. Several hundred thousand square miles (several hundred thousand square kilometers) of the Great Plains have fewer than 6 inhabitants per square mile (2.3 inhabitants per square kilometer)—the density standard Frederick Jackson Turner used to declare the American frontier "closed" in 1893. Many have fewer than 2 inhabitants per square mile (0.77 inhabitants per square kilometer). There are more than 6,000 ghost towns in the state of Kansas alone, according to Kansas historian Daniel Fitzgerald. This problem is often exacerbated by the consolidation of farms and the difficulty of attracting modern industry to the region. In addition, the smaller school-age population has forced the consolidation of school districts and the closure of high schools in some communities. The continuing population loss has led some to suggest that the current use of the drier parts of the Great Plains is not sustainable, and there has been a proposal - the "Buffalo Commons" - to return approximately 139,000 square miles (360,000 km2) of these drier parts to native prairie land.
Although the eastern image of farm life in the prairies emphasized the isolation of the lonely farmer and wife, plains residents created busy social lives for themselves. They often sponsored activities that combined work, food and entertainment such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities and school functions. Women organized shared meals and potluck events, as well as extended visits between families. The Grange was a nationwide farmers' organization, they reserved high offices for women, and gave them a voice in public affairs.
From the 1950s on, many areas of the Great Plains have become productive crop-growing areas because of extensive irrigation on large landholdings. The United States is a major exporter of agricultural products. The southern portion of the Great Plains lies over the Ogallala Aquifer, a huge underground layer of water-bearing strata dating from the last ice age. Center pivot irrigation is used extensively in drier sections of the Great Plains, resulting in aquifer depletion at a rate that is greater than the ground's ability to recharge.
The Great Plains is the broad expanse of flat land (a plain), much of it covered in prairie, steppe and grassland, that lies west of the Mississippi River tallgrass prairie states and east of the Rocky Mountains in the United States and Canada. This area covers parts, but not all, of the states of Colorado, Kansas, Montana, Nebraska, New Mexico, North Dakota, Oklahoma, South Dakota, Texas, and Wyoming, and the Canadian provinces of Alberta, Manitoba and Saskatchewan. The region is known for supporting extensive cattle ranching and dry farming.
The North American Environmental Atlas, produced by the Commission for Environmental Cooperation, a NAFTA agency composed of the geographical agencies of the Mexican, American, and Canadian governments uses the "Great Plains" as an ecoregion synonymous with predominant prairies and grasslands rather than as physiographic region defined by topography. The Great Plains ecoregion includes five sub-regions: Temperate Prairies, West-Central Semi-Arid Prairies, South-Central Semi-Arid Prairies, Texas Louisiana Coastal Plains, and Tamaulipus-Texas Semi-Arid Plain, which overlap or expand upon other Great Plains designations.
The railroads opened up the Great Plains for settlement, for now it was possible to ship wheat and other crops at low cost to the urban markets in the East, and Europe. Homestead land was free for American settlers. Railroads sold their land at cheap rates to immigrants in expectation they would generate traffic as soon as farms were established. Immigrants poured in, especially from Germany and Scandinavia. On the plains, very few single men attempted to operate a farm or ranch by themselves; they clearly understood the need for a hard-working wife, and numerous children, to handle the many chores, including child-rearing, feeding and clothing the family, managing the housework, feeding the hired hands, and, especially after the 1930s, handling paperwork and financial details. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After approximately one generation, women increasingly left the fields, thus redefining their roles within the family. New technology including sewing and washing machines encouraged women to turn to domestic roles. The scientific housekeeping movement, promoted across the land by the media and government extension agents, as well as county fairs which featured achievements in home cookery and canning, advice columns for women regarding farm bookkeeping, and home economics courses in the schools.
During the Cenozoic era, specifically about 25 million years ago during the Miocene and Pliocene epochs, the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. The grasslands provided a new niche for mammals, including many ungulates and glires, that switched from browsing diets to grazing diets. Traditionally, the spread of grasslands and the development of grazers have been strongly linked. However, an examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes in mammals, giving rise to the "grit, not grass" hypothesis.
To allow for agricultural development of the Great Plains and house a growing population, the US passed the Homestead Acts of 1862: it allowed a settler to claim up to 160 acres (65 ha) of land, provided that he lived on it for a period of five years and cultivated it. The provisions were expanded under the Kinkaid Act of 1904 to include a homestead of an entire section. Hundreds of thousands of people claimed such homesteads, sometimes building sod houses out of the very turf of their land. Many of them were not skilled dryland farmers and failures were frequent. Much of the Plains were settled during relatively wet years. Government experts did not understand how farmers should cultivate the prairies and gave advice counter to what would have worked[citation needed]. Germans from Russia who had previously farmed, under similar circumstances, in what is now Ukraine were marginally more successful than other homesteaders. The Dominion Lands Act of 1871 served a similar function for establishing homesteads on the prairies in Canada.
In physics, energy is a property of objects which can be transferred to other objects or converted into different forms. The "ability of a system to perform work" is a common description, but it is difficult to give one single comprehensive definition of energy because of its many forms. For instance, in SI units, energy is measured in joules, and one joule is defined "mechanically", being the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton.[note 1] However, there are many other definitions of energy, depending on the context, such as thermal energy, radiant energy, electromagnetic, nuclear, etc., where definitions are derived that are the most convenient.
Common energy forms include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature. All of the many forms of energy are convertible to other kinds of energy. In Newtonian physics, there is a universal law of conservation of energy which says that energy can be neither created nor be destroyed; however, it can change from one form to another.
For "closed systems" with no external source or sink of energy, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by mechanical work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.
Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.
The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy.
Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them.[need quotation to verify].The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy.
In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.
In 1807, Thomas Young was possibly the first to use the term "energy" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.
These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.
Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).
Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.
In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor e−E/kT – that is the probability of molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.
In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy.
Sunlight is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.
Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants, chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria
It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings").[note 3] Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.
Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.
In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.
In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.
In quantum mechanics, energy is defined in terms of the energy operator as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation:  (where  is Planck's constant and  the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.
For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction.
There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.
Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being "released" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.
Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula E = mc², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).
Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since  is extremely large relative to ordinary human scales, the conversion of ordinary amount of matter (for example, 1 kg) to other forms of energy (such as heat, light, and other radiation) can liberate tremendous amounts of energy (~ joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.
Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).
As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.
According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.
This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.
In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.
Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat.[note 4] Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy,[note 5] and the conductive transfer of thermal energy.
The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics.
Circadian rhythms allow organisms to anticipate and prepare for precise and regular environmental changes. They thus enable organisms to best capitalize on environmental resources (e.g. light and food) compared to those that cannot predict such availability. It has therefore been suggested that circadian rhythms put organisms at a selective advantage in evolutionary terms. However, rhythmicity appears to be as important in regulating and coordinating internal metabolic processes, as in coordinating with the environment. This is suggested by the maintenance (heritability) of circadian rhythms in fruit flies after several hundred generations in constant laboratory conditions, as well as in creatures in constant darkness in the wild, and by the experimental elimination of behavioral, but not physiological, circadian rhythms in quail.
Norwegian researchers at the University of Tromsø have shown that some Arctic animals (ptarmigan, reindeer) show circadian rhythms only in the parts of the year that have daily sunrises and sunsets. In one study of reindeer, animals at 70 degrees North showed circadian rhythms in the autumn, winter and spring, but not in the summer. Reindeer on Svalbard at 78 degrees North showed such rhythms only in autumn and spring. The researchers suspect that other Arctic animals as well may not show circadian rhythms in the constant light of summer and the constant dark of winter.
The central oscillator generates a self-sustaining rhythm and is driven by two interacting feedback loops that are active at different times of day. The morning loop consists of CCA1 (Circadian and Clock-Associated 1) and LHY (Late Elongated Hypocotyl), which encode closely related MYB transcription factors that regulate circadian rhythms in Arabidopsis, as well as PRR 7 and 9 (Pseudo-Response Regulators.) The evening loop consists of GI (Gigantea) and ELF4, both involved in regulation of flowering time genes. When CCA1 and LHY are overexpressed (under constant light or dark conditions), plants become arrhythmic, and mRNA signals reduce, contributing to a negative feedback loop. Gene expression of CCA1 and LHY oscillates and peaks in the early morning, whereas TOC1 gene expression oscillates and peaks in the early evening. While it was previously hypothesised that these three genes model a negative feedback loop in which over-expressed CCA1 and LHY repress TOC1 and over-expressed TOC1 is a positive regulator of CCA1 and LHY, it was shown in 2012 by Andrew Millar and others that TOC1 in fact serves as a repressor not only of CCA1, LHY, and PRR7 and 9 in the morning loop but also of GI and ELF4 in the evening loop. This finding and further computational modeling of TOC1 gene functions and interactions suggest a reframing of the plant circadian clock as a triple negative-component repressilator model rather than the positive/negative-element feedback loop characterizing the clock in mammals.
A defect in the human homologue of the Drosophila "period" gene was identified as a cause of the sleep disorder FASPS (Familial advanced sleep phase syndrome), underscoring the conserved nature of the molecular circadian clock through evolution. Many more genetic components of the biological clock are now known. Their interactions result in an interlocked feedback loop of gene products resulting in periodic fluctuations that the cells of the body interpret as a specific time of the day.[citation needed]
Due to the work nature of airline pilots, who often cross several timezones and regions of sunlight and darkness in one day, and spend many hours awake both day and night, they are often unable to maintain sleep patterns that correspond to the natural human circadian rhythm; this situation can easily lead to fatigue. The NTSB cites this as contributing to many accidents[unreliable medical source?]  and has conducted several research studies in order to find methods of combating fatigue in pilots.
The earliest recorded account of a circadian process dates from the 4th century B.C.E., when Androsthenes, a ship captain serving under Alexander the Great, described diurnal leaf movements of the tamarind tree. The observation of a circadian or diurnal process in humans is mentioned in Chinese medical texts dated to around the 13th century, including the Noon and Midnight Manual and the Mnemonic Rhyme to Aid in the Selection of Acu-points According to the Diurnal Cycle, the Day of the Month and the Season of the Year.
Plant circadian rhythms tell the plant what season it is and when to flower for the best chance of attracting pollinators. Behaviors showing rhythms include leaf movement, growth, germination, stomatal/gas exchange, enzyme activity, photosynthetic activity, and fragrance emission, among others. Circadian rhythms occur as a plant entrains to synchronize with the light cycle of its surrounding environment. These rhythms are endogenously generated and self-sustaining and are relatively constant over a range of ambient temperatures. Important features include two interacting transcription-translation feedback loops: proteins containing PAS domains, which facilitate protein-protein interactions; and several photoreceptors that fine-tune the clock to different light conditions. Anticipation of changes in the environment allows appropriate changes in a plant's physiological state, conferring an adaptive advantage. A better understanding of plant circadian rhythms has applications in agriculture, such as helping farmers stagger crop harvests to extend crop availability and securing against massive losses due to weather.
The simplest known circadian clock is that of the prokaryotic cyanobacteria. Recent research has demonstrated that the circadian clock of Synechococcus elongatus can be reconstituted in vitro with just the three proteins (KaiA, KaiB, KaiC) of their central oscillator. This clock has been shown to sustain a 22-hour rhythm over several days upon the addition of ATP. Previous explanations of the prokaryotic circadian timekeeper were dependent upon a DNA transcription/translation feedback mechanism.[citation needed]
The rhythm is linked to the light–dark cycle. Animals, including humans, kept in total darkness for extended periods eventually function with a free-running rhythm. Their sleep cycle is pushed back or forward each "day", depending on whether their "day", their endogenous period, is shorter or longer than 24 hours. The environmental cues that reset the rhythms each day are called zeitgebers (from the German, "time-givers"). Totally blind subterranean mammals (e.g., blind mole rat Spalax sp.) are able to maintain their endogenous clocks in the apparent absence of external stimuli. Although they lack image-forming eyes, their photoreceptors (which detect light) are still functional; they do surface periodically as well.[page needed]
Melatonin is absent from the system or undetectably low during daytime. Its onset in dim light, dim-light melatonin onset (DLMO), at roughly 21:00 (9 p.m.) can be measured in the blood or the saliva. Its major metabolite can also be measured in morning urine. Both DLMO and the midpoint (in time) of the presence of the hormone in the blood or saliva have been used as circadian markers. However, newer research indicates that the melatonin offset may be the more reliable marker. Benloucif et al. found that melatonin phase markers were more stable and more highly correlated with the timing of sleep than the core temperature minimum. They found that both sleep offset and melatonin offset are more strongly correlated with phase markers than the onset of sleep. In addition, the declining phase of the melatonin levels is more reliable and stable than the termination of melatonin synthesis.
For temperature studies, subjects must remain awake but calm and semi-reclined in near darkness while their rectal temperatures are taken continuously. Though variation is great among normal chronotypes, the average human adult's temperature reaches its minimum at about 05:00 (5 a.m.), about two hours before habitual wake time. Baehr et al. found that, in young adults, the daily body temperature minimum occurred at about 04:00 (4 a.m.) for morning types but at about 06:00 (6 a.m.) for evening types. This minimum occurred at approximately the middle of the eight hour sleep period for morning types, but closer to waking in evening types.
In 1896, Patrick and Gilbert observed that during a prolonged period of sleep deprivation, sleepiness increases and decreases with a period of approximately 24 hours. In 1918, J.S. Szymanski showed that animals are capable of maintaining 24-hour activity patterns in the absence of external cues such as light and changes in temperature. In the early 20th century, circadian rhythms were noticed in the rhythmic feeding times of bees. Extensive experiments were done by Auguste Forel, Ingeborg Beling, and Oskar Wahl to see whether this rhythm was due to an endogenous clock.[citation needed] Ron Konopka and Seymour Benzer isolated the first clock mutant in Drosophila in the early 1970s and mapped the "period" gene, the first discovered genetic determinant of behavioral rhythmicity. Joseph Takahashi discovered the first mammalian circadian clock mutation (clockΔ19) using mice in 1994. However, recent studies show that deletion of clock does not lead to a behavioral phenotype (the animals still have normal circadian rhythms), which questions its importance in rhythm generation.
It is now known that the molecular circadian clock can function within a single cell; i.e., it is cell-autonomous. This was shown by Gene Block in isolated mollusk BRNs.[clarification needed] At the same time, different cells may communicate with each other resulting in a synchronised output of electrical signaling. These may interface with endocrine glands of the brain to result in periodic release of hormones. The receptors for these hormones may be located far across the body and synchronise the peripheral clocks of various organs. Thus, the information of the time of the day as relayed by the eyes travels to the clock in the brain, and, through that, clocks in the rest of the body may be synchronised. This is how the timing of, for example, sleep/wake, body temperature, thirst, and appetite are coordinately controlled by the biological clock.[citation needed]
Light is the signal by which plants synchronize their internal clocks to their environment and is sensed by a wide variety of photoreceptors. Red and blue light are absorbed through several phytochromes and cryptochromes. One phytochrome, phyA, is the main phytochrome in seedlings grown in the dark but rapidly degrades in light to produce Cry1. Phytochromes B–E are more stable with phyB, the main phytochrome in seedlings grown in the light. The cryptochrome (cry) gene is also a light-sensitive component of the circadian clock and is thought to be involved both as a photoreceptor and as part of the clock's endogenous pacemaker mechanism. Cryptochromes 1–2 (involved in blue–UVA) help to maintain the period length in the clock through a whole range of light conditions.
Studies by Nathaniel Kleitman in 1938 and by Derk-Jan Dijk and Charles Czeisler in the 1990s put human subjects on enforced 28-hour sleep–wake cycles, in constant dim light and with other time cues suppressed, for over a month. Because normal people cannot entrain to a 28-hour day in dim light if at all,[citation needed] this is referred to as a forced desynchrony protocol. Sleep and wake episodes are uncoupled from the endogenous circadian period of about 24.18 hours and researchers are allowed to assess the effects of circadian phase on aspects of sleep and wakefulness including sleep latency and other functions.[page needed]
Shift-work or chronic jet-lag have profound consequences on circadian and metabolic events in the body. Animals that are forced to eat during their resting period show increased body mass and altered expression of clock and metabolic genes.[medical citation needed] In humans, shift-work that favors irregular eating times is associated with altered insulin sensitivity and higher body mass. Shift-work also leads to increased metabolic risks for cardio-metabolic syndrome, hypertension, inflammation.
Studies conducted on both animals and humans show major bidirectional relationships between the circadian system and abusive drugs. It is indicated that these abusive drugs affect the central circadian pacemaker. Individuals suffering from substance abuse display disrupted rhythms. These disrupted rhythms can increase the risk for substance abuse and relapse. It is possible that genetic and/or environmental disturbances to the normal sleep and wake cycle can increase the susceptibility to addiction.
What drove circadian rhythms to evolve has been an enigmatic question. Previous hypotheses emphasized that photosensitive proteins and circadian rhythms may have originated together in the earliest cells, with the purpose of protecting replicating DNA from high levels of damaging ultraviolet radiation during the daytime. As a result, replication was relegated to the dark. However, evidence for this is lacking, since the simplest organisms with a circadian rhythm, the cyanobacteria, do the opposite of this - they divide more in the daytime. Recent studies instead highlight the importance of co-evolution of redox proteins with circadian oscillators in all three kingdoms of life following the Great Oxidation Event approximately 2.3 billion years ago. The current view is that circadian changes in environmental oxygen levels and the production of reactive oxygen species (ROS) in the presence of daylight are likely to have driven a need to evolve circadian rhythms to preempt, and therefore counteract, damaging redox reactions on a daily basis.
Mutations or deletions of clock gene in mice have demonstrated the importance of body clocks to ensure the proper timing of cellular/metabolic events; clock-mutant mice are hyperphagic and obese, and have altered glucose metabolism. In mice, deletion of the Rev-ErbA alpha clock gene facilitates diet-induced obesity and changes the balance between glucose and lipid utilization predisposing to diabetes. However, it is not clear whether there is a strong association between clock gene polymorphisms in humans and the susceptibility to develop the metabolic syndrome.
The primary circadian "clock" in mammals is located in the suprachiasmatic nucleus (or nuclei) (SCN), a pair of distinct groups of cells located in the hypothalamus. Destruction of the SCN results in the complete absence of a regular sleep–wake rhythm. The SCN receives information about illumination through the eyes. The retina of the eye contains "classical" photoreceptors ("rods" and "cones"), which are used for conventional vision. But the retina also contains specialized ganglion cells that are directly photosensitive, and project directly to the SCN, where they help in the entrainment (synchronization) of this master circadian clock.
Early research into circadian rhythms suggested that most people preferred a day closer to 25 hours when isolated from external stimuli like daylight and timekeeping. However, this research was faulty because it failed to shield the participants from artificial light. Although subjects were shielded from time cues (like clocks) and daylight, the researchers were not aware of the phase-delaying effects of indoor electric lights.[dubious – discuss] The subjects were allowed to turn on light when they were awake and to turn it off when they wanted to sleep. Electric light in the evening delayed their circadian phase.[citation needed] A more stringent study conducted in 1999 by Harvard University estimated the natural human rhythm to be closer to 24 hours, 11 minutes: much closer to the solar day but still not perfectly in sync.
More-or-less independent circadian rhythms are found in many organs and cells in the body outside the suprachiasmatic nuclei (SCN), the "master clock". These clocks, called peripheral oscillators, are found in the adrenal gland,[citation needed] oesophagus, lungs, liver, pancreas, spleen, thymus, and skin.[citation needed] Though oscillators in the skin respond to light, a systemic influence has not been proven. There is also some evidence that the olfactory bulb and prostate may experience oscillations when cultured, suggesting that these structures may also be weak oscillators.[citation needed]
Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.
As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.
The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.
The College of Engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the College of Science since the 1870s. Today the college, housed in the Fitzpatrick, Cushing, and Stinson-Remick Halls of Engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight B.S. degrees offered. Additionally, the college offers five-year dual degree programs with the Colleges of Arts and Letters and of Business awarding additional B.A. and Master of Business Administration (MBA) degrees, respectively.
All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.
The university first offered graduate degrees, in the form of a Master of Arts (MA), in the 1854–1855 academic year. The program expanded to include Master of Laws (LL.M.) and Master of Civil Engineering in its early stages of growth, before a formal graduate school education was developed with a thesis not required to receive the degrees. This changed in 1924 with formal requirements developed for graduate degrees, including offering Doctorate (PhD) degrees. Today each of the five colleges offer graduate education. Most of the departments from the College of Arts and Letters offer PhD programs, while a professional Master of Divinity (M.Div.) program also exists. All of the departments in the College of Science offer PhD programs, except for the Department of Pre-Professional Studies. The School of Architecture offers a Master of Architecture, while each of the departments of the College of Engineering offer PhD programs. The College of Business offers multiple professional programs including MBA and Master of Science in Accountancy programs. It also operates facilities in Chicago and Cincinnati for its executive MBA program. Additionally, the Alliance for Catholic Education program offers a Master of Education program where students study at the university during the summer and teach in Catholic elementary schools, middle schools, and high schools across the Southern United States for two school years.
The Joan B. Kroc Institute for International Peace Studies at the University of Notre Dame is dedicated to research, education and outreach on the causes of violent conflict and the conditions for sustainable peace. It offers PhD, Master's, and undergraduate degrees in peace studies. It was founded in 1986 through the donations of Joan B. Kroc, the widow of McDonald's owner Ray Kroc. The institute was inspired by the vision of the Rev. Theodore M. Hesburgh CSC, President Emeritus of the University of Notre Dame. The institute has contributed to international policy discussions about peace building practices.
The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as "Touchdown Jesus" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown.
Notre Dame is known for its competitive admissions, with the incoming class enrolling in fall 2015 admitting 3,577 from a pool of 18,156 (19.7%). The academic profile of the enrolled class continues to rate among the top 10 to 15 in the nation for national research universities. The university practices a non-restrictive early action policy that allows admitted students to consider admission to Notre Dame as well as any other colleges to which they were accepted. 1,400 of the 3,577 (39.1%) were admitted under the early action plan. Admitted students came from 1,311 high schools and the average student traveled more than 750 miles to Notre Dame, making it arguably the most representative university in the United States. While all entering students begin in the College of the First Year of Studies, 25% have indicated they plan to study in the liberal arts or social sciences, 24% in engineering, 24% in business, 24% in science, and 3% in architecture.
In 2015-2016, Notre Dame ranked 18th overall among "national universities" in the United States in U.S. News & World Report's Best Colleges 2016. In 2014, USA Today ranked Notre Dame 10th overall for American universities based on data from College Factual. Forbes.com's America's Best Colleges ranks Notre Dame 13th among colleges in the United States in 2015, 8th among Research Universities, and 1st in the Midwest. U.S. News & World Report also lists Notre Dame Law School as 22nd overall. BusinessWeek ranks Mendoza College of Business undergraduate school as 1st overall. It ranks the MBA program as 20th overall. The Philosophical Gourmet Report ranks Notre Dame's graduate philosophy program as 15th nationally, while ARCHITECT Magazine ranked the undergraduate architecture program as 12th nationally. Additionally, the study abroad program ranks sixth in highest participation percentage in the nation, with 57.6% of students choosing to study abroad in 17 countries. According to payscale.com, undergraduate alumni of University of Notre Dame have a mid-career median salary $110,000, making it the 24th highest among colleges and universities in the United States. The median starting salary of $55,300 ranked 58th in the same peer group.
Father Joseph Carrier, C.S.C. was Director of the Science Museum and the Library and Professor of Chemistry and Physics until 1874. Carrier taught that scientific research and its promise for progress were not antagonistic to the ideals of intellectual and moral culture endorsed by the Church. One of Carrier's students was Father John Augustine Zahm (1851–1921) who was made Professor and Co-Director of the Science Department at age 23 and by 1900 was a nationally prominent scientist and naturalist. Zahm was active in the Catholic Summer School movement, which introduced Catholic laity to contemporary intellectual issues. His book Evolution and Dogma (1896) defended certain aspects of evolutionary theory as true, and argued, moreover, that even the great Church teachers Thomas Aquinas and Augustine taught something like it. The intervention of Irish American Catholics in Rome prevented Zahm's censure by the Vatican. In 1913, Zahm and former President Theodore Roosevelt embarked on a major expedition through the Amazon.
In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models. Around 1899, Professor Jerome Green became the first American to send a wireless message. In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene. Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nuclear Astrophysics.
The Lobund Institute grew out of pioneering research in germ-free-life which began in 1928. This area of research originated in a question posed by Pasteur as to whether animal life was possible without bacteria. Though others had taken up this idea, their research was short lived and inconclusive. Lobund was the first research organization to answer definitively, that such life is possible and that it can be prolonged through generations. But the objective was not merely to answer Pasteur's question but also to produce the germ free animal as a new tool for biological and medical research. This objective was reached and for years Lobund was a unique center for the study and production of germ free animals and for their use in biological and medical investigations. Today the work has spread to other universities. In the beginning it was under the Department of Biology and a program leading to the master's degree accompanied the research program. In the 1940s Lobund achieved independent status as a purely research organization and in 1950 was raised to the status of an Institute. In 1958 it was brought back into the Department of Biology as integral part of that department, but with its own program leading to the degree of PhD in Gnotobiotics.
The Review of Politics was founded in 1939 by Gurian, modeled after German Catholic journals. It quickly emerged as part of an international Catholic intellectual revival, offering an alternative vision to positivist philosophy. For 44 years, the Review was edited by Gurian, Matthew Fitzsimons, Frederick Crosson, and Thomas Stritch. Intellectual leaders included Gurian, Jacques Maritain, Frank O'Malley, Leo Richard Ward, F. A. Hermens, and John U. Nef. It became a major forum for political ideas and modern political concerns, especially from a Catholic and scholastic tradition.
As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become "one of the pre–eminent research institutions in the world" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.
In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. Around 21–24% of students are children of alumni, and although 37% of students come from the Midwestern United States, the student body represents all 50 states and 100 countries. As of March 2007[update] The Princeton Review ranked the school as the fifth highest 'dream school' for parents to send their children. As of March 2015[update] The Princeton Review ranked Notre Dame as the ninth highest. The school has been previously criticized for its lack of diversity, and The Princeton Review ranks the university highly among schools at which "Alternative Lifestyles [are] Not an Alternative." It has also been commended by some diversity oriented publications; Hispanic Magazine in 2004 ranked the university ninth on its list of the top–25 colleges for Latinos, and The Journal of Blacks in Higher Education recognized the university in 2006 for raising enrollment of African-American students. With 6,000 participants, the university's intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where "Everyone Plays Intramural Sports." The annual Bookstore Basketball tournament is the largest outdoor five-on-five tournament in the world with over 700 teams participating each year, while the Notre Dame Men's Boxing Club hosts the annual Bengal Bouts tournament that raises money for the Holy Cross Missions in Bangladesh.
About 80% of undergraduates and 20% of graduate students live on campus. The majority of the graduate students on campus live in one of four graduate housing complexes on campus, while all on-campus undergraduates live in one of the 29 residence halls. Because of the religious affiliation of the university, all residence halls are single-sex, with 15 male dorms and 14 female dorms. The university maintains a visiting policy (known as parietal hours) for those students who live in dormitories, specifying times when members of the opposite sex are allowed to visit other students' dorm rooms; however, all residence halls have 24-hour social spaces for students regardless of gender. Many residence halls have at least one nun and/or priest as a resident. There are no traditional social fraternities or sororities at the university, but a majority of students live in the same residence hall for all four years. Some intramural sports are based on residence hall teams, where the university offers the only non-military academy program of full-contact intramural American football. At the end of the intramural season, the championship game is played on the field in Notre Dame Stadium.
The university is affiliated with the Congregation of Holy Cross (Latin: Congregatio a Sancta Cruce, abbreviated postnominals: "CSC"). While religious affiliation is not a criterion for admission, more than 93% of students identify as Christian, with over 80% of the total being Catholic. Collectively, Catholic Mass is celebrated over 100 times per week on campus, and a large campus ministry program provides for the faith needs of the community. There are multitudes of religious statues and artwork around campus, most prominent of which are the statue of Mary on the Main Building, the Notre Dame Grotto, and the Word of Life mural on Hesburgh Library depicting Christ as a teacher. Additionally, every classroom displays a crucifix. There are many religious clubs (catholic and non-Catholic) at the school, including Council #1477 of the Knights of Columbus (KOC), Baptist Collegiate Ministry (BCM), Jewish Club, Muslim Student Association, Orthodox Christian Fellowship, The Mormon Club, and many more. The Notre Dame KofC are known for being the first collegiate council of KofC, operating a charitable concession stand during every home football game and owning their own building on campus which can be used as a cigar lounge. Fifty-seven chapels are located throughout the campus.
This Main Building, and the library collection, was entirely destroyed by a fire in April 1879, and the school closed immediately and students were sent home. The university founder, Fr. Sorin and the president at the time, the Rev. William Corby, immediately planned for the rebuilding of the structure that had housed virtually the entire University. Construction was started on the 17th of May and by the incredible zeal of administrator and workers the building was completed before the fall semester of 1879. The library collection was also rebuilt and stayed housed in the new Main Building for years afterwards. Around the time of the fire, a music hall was opened. Eventually becoming known as Washington Hall, it hosted plays and musical acts put on by the school. By 1880, a science program was established at the university, and a Science Hall (today LaFortune Student Center) was built in 1883. The hall housed multiple classrooms and science labs needed for early research at the university.
In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president.
One of the main driving forces in the growth of the University was its football team, the Notre Dame Fighting Irish. Knute Rockne became head coach in 1918. Under Rockne, the Irish would post a record of 105 wins, 12 losses, and five ties. During his 13 years the Irish won three national championships, had five undefeated seasons, won the Rose Bowl in 1925, and produced players such as George Gipp and the "Four Horsemen". Knute Rockne has the highest winning percentage (.881) in NCAA Division I/FBS football history. Rockne's offenses employed the Notre Dame Box and his defenses ran a 7–2–2 scheme. The last game Rockne coached was on December 14, 1930 when he led a group of Notre Dame all-stars against the New York Giants in New York City.
The success of its football team made Notre Dame a household name. The success of Note Dame reflected rising status of Irish Americans and Catholics in the 1920s. Catholics rallied up around the team and listen to the games on the radio, especially when it knocked off the schools that symbolized the Protestant establishment in America — Harvard, Yale, Princeton, and Army. Yet this role as high-profile flagship institution of Catholicism made it an easy target of anti-Catholicism. The most remarkable episode of violence was the clash between Notre Dame students and the Ku Klux Klan in 1924. Nativism and anti-Catholicism, especially when directed towards immigrants, were cornerstones of the KKK's rhetoric, and Notre Dame was seen as a symbol of the threat posed by the Catholic Church. The Klan decided to have a week-long Klavern in South Bend. Clashes with the student body started on March 17, when students, aware of the anti-Catholic animosity, blocked the Klansmen from descending from their trains in the South Bend station and ripped the KKK clothes and regalia. On May 19 thousands of students massed downtown protesting the Klavern, and only the arrival of college president Fr. Matthew Walsh prevented any further clashes. The next day, football coach Knute Rockne spoke at a campus rally and implored the students to obey the college president and refrain from further violence. A few days later the Klavern broke up, but the hostility shown by the students was an omen and a contribution to the downfall of the KKK in Indiana.
Holy Cross Father John Francis O'Hara was elected vice-president in 1933 and president of Notre Dame in 1934. During his tenure at Notre Dame, he brought numerous refugee intellectuals to campus; he selected Frank H. Spearman, Jeremiah D. M. Ford, Irvin Abell, and Josephine Brownson for the Laetare Medal, instituted in 1883. O'Hara strongly believed that the Fighting Irish football team could be an effective means to "acquaint the public with the ideals that dominate" Notre Dame. He wrote, "Notre Dame football is a spiritual service because it is played for the honor and glory of God and of his Blessed Mother. When St. Paul said: 'Whether you eat or drink, or whatsoever else you do, do all for the glory of God,' he included football."
The Rev. John J. Cavanaugh, C.S.C. served as president from 1946 to 1952. Cavanaugh's legacy at Notre Dame in the post-war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when Notre Dame quadrupled in student census, undergraduate enrollment increased by more than half, and graduate student enrollment grew fivefold. Cavanaugh also established the Lobund Institute for Animal Studies and Notre Dame's Medieval Institute. Cavanaugh also presided over the construction of the Nieuwland Science Hall, Fisher Hall, and the Morris Inn, as well as the Hall of Liberal Arts (now O'Shaughnessy Hall), made possible by a donation from I.A. O'Shaughnessy, at the time the largest ever made to an American Catholic university. Cavanaugh also established a system of advisory councils at the university, which continue today and are vital to the university's governance and development
The Rev. Theodore Hesburgh, C.S.C., (1917–2015) served as president for 35 years (1952–87) of dramatic transformations. In that time the annual operating budget rose by a factor of 18 from $9.7 million to $176.6 million, and the endowment by a factor of 40 from $9 million to $350 million, and research funding by a factor of 20 from $735,000 to $15 million. Enrollment nearly doubled from 4,979 to 9,600, faculty more than doubled 389 to 950, and degrees awarded annually doubled from 1,212 to 2,500.
Hesburgh is also credited with transforming the face of Notre Dame by making it a coeducational institution. In the mid-1960s Notre Dame and Saint Mary's College developed a co-exchange program whereby several hundred students took classes not offered at their home institution, an arrangement that added undergraduate women to a campus that already had a few women in the graduate schools. After extensive debate, merging with St. Mary's was rejected, primarily because of the differential in faculty qualifications and pay scales. "In American college education," explained the Rev. Charles E. Sheedy, C.S.C., Notre Dame's Dean of Arts and Letters, "certain features formerly considered advantageous and enviable are now seen as anachronistic and out of place.... In this environment of diversity, the integration of the sexes is a normal and expected aspect, replacing separatism." Thomas Blantz, C.S.C., Notre Dame's Vice President of Student Affairs, added that coeducation "opened up a whole other pool of very bright students." Two of the male residence halls were converted for the newly admitted female students that first year, while two others were converted for the next school year. In 1971 Mary Ann Proctor became the first female undergraduate; she transferred from St. Mary's College. In 1972 the first woman to graduate was Angela Sienko, who earned a bachelor's degree in marketing.
In the 18 years under the presidency of Edward Malloy, C.S.C., (1987–2005), there was a rapid growth in the school's reputation, faculty, and resources. He increased the faculty by more than 500 professors; the academic quality of the student body has improved dramatically, with the average SAT score rising from 1240 to 1360; the number of minority students more than doubled; the endowment grew from $350 million to more than $3 billion; the annual operating budget rose from $177 million to more than $650 million; and annual research funding improved from $15 million to more than $70 million. Notre Dame's most recent[when?] capital campaign raised $1.1 billion, far exceeding its goal of $767 million, and is the largest in the history of Catholic higher education.
Since 2005, Notre Dame has been led by John I. Jenkins, C.S.C., the 17th president of the university. Jenkins took over the position from Malloy on July 1, 2005. In his inaugural address, Jenkins described his goals of making the university a leader in research that recognizes ethics and building the connection between faith and studies. During his tenure, Notre Dame has increased its endowment, enlarged its student body, and undergone many construction projects on campus, including Compton Family Ice Arena, a new architecture hall, additional residence halls, and the Campus Crossroads, a $400m enhancement and expansion of Notre Dame Stadium.
Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the spot of Fr. Sorin's original church, which became too small for the growing college. It is built in French Revival style and it is decorated by stained glass windows imported directly from France. The interior was painted by Luigi Gregori, an Italian painter invited by Fr. Sorin to be artist in residence. The Basilica also features a bell tower with a carillon. Inside the church there are also sculptures by Ivan Mestrovic. The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the most beloved spots on campus.
A Science Hall was built in 1883 under the direction of Fr. Zahm, but in 1950 it was converted to a student union building and named LaFortune Center, after Joseph LaFortune, an oil executive from Tulsa, Oklahoma. Commonly known as "LaFortune" or "LaFun," it is a 4-story building of 83,000 square feet that provides the Notre Dame community with a meeting place for social, recreational, cultural, and educational activities. LaFortune employs 35 part-time student staff and 29 full-time non-student staff and has an annual budget of $1.2 million. Many businesses, services, and divisions of The Office of Student Affairs are found within. The building also houses restaurants from national restaurant chains.
Since the construction of its oldest buildings, the university's physical plant has grown substantially. Over the years 29 residence halls have been built to accommodate students and each has been constructed with its own chapel. Many academic building were added together with a system of libraries, the most prominent of which is the Theodore Hesburgh Library, built in 1963 and today containing almost 4 million books. Since 2004, several buildings have been added, including the DeBartolo Performing Arts Center, the Guglielmino Complex, and the Jordan Hall of Science. Additionally, a new residence for men, Duncan Hall, was begun on March 8, 2007, and began accepting residents for the Fall 2008 semester. Ryan Hall was completed and began housing undergraduate women in the fall of 2009. A new engineering building, Stinson-Remick Hall, a new combination Center for Social Concerns/Institute for Church Life building, Geddes Hall, and a law school addition have recently been completed as well. Additionally the new hockey arena opened in the fall of 2011. The Stayer Center for Executive Education, which houses the Mendoza College of Business Executive Education Department opened in March 2013 just South of the Mendoza College of Business building. Because of its long athletic tradition, the university features also many building dedicated to sport. The most famous is Notre Dame Stadium, home of the Fighting Irish football team; it has been renovated several times and today it can hold more than 80 thousand people. Prominent venues include also the Edmund P. Joyce Center, with indoor basketball and volleyball courts, and the Compton Family Ice Arena, a two-rink facility dedicated to hockey. Also, there are many outdoor fields, as the Frank Eck Stadium for baseball.
The University of Notre Dame has made being a sustainability leader an integral part of its mission, creating the Office of Sustainability in 2008 to achieve a number of goals in the areas of power generation, design and construction, waste reduction, procurement, food services, transportation, and water.As of 2012[update] four building construction projects were pursuing LEED-Certified status and three were pursuing LEED Silver. Notre Dame's dining services sources 40% of its food locally and offers sustainably caught seafood as well as many organic, fair-trade, and vegan options. On the Sustainable Endowments Institute's College Sustainability Report Card 2010, University of Notre Dame received a "B" grade. The university also houses the Kroc Institute for International Peace Studies. Father Gustavo Gutierrez, the founder of Liberation Theology is a current faculty member.
The university owns several centers around the world used for international studies and research, conferences abroad, and alumni support. The university has had a presence in London, England, since 1968. Since 1998, its London center has been based in the former United University Club at 1 Suffolk Street in Trafalgar Square. The center enables the Colleges of Arts & Letters, Business Administration, Science, Engineering and the Law School to develop their own programs in London, as well as hosting conferences and symposia. Other Global Gateways are located in Beijing, Chicago, Dublin, Jerusalem and Rome.
The College of Arts and Letters was established as the university's first college in 1842 with the first degrees given in 1849. The university's first academic curriculum was modeled after the Jesuit Ratio Studiorum from Saint Louis University. Today the college, housed in O'Shaughnessy Hall, includes 20 departments in the areas of fine arts, humanities, and social sciences, and awards Bachelor of Arts (B.A.) degrees in 33 majors, making it the largest of the university's colleges. There are around 2,500 undergraduates and 750 graduates enrolled in the college.
The College of Science was established at the university in 1865 by president Father Patrick Dillon. Dillon's scientific courses were six years of work, including higher-level mathematics courses. Today the college, housed in the newly built Jordan Hall of Science, includes over 1,200 undergraduates in six departments of study – biology, chemistry, mathematics, physics, pre-professional studies, and applied and computational mathematics and statistics (ACMS) – each awarding Bachelor of Science (B.S.) degrees. According to university statistics, its science pre-professional program has one of the highest acceptance rates to medical school of any university in the United States.
The School of Architecture was established in 1899, although degrees in architecture were first awarded by the university in 1898. Today the school, housed in Bond Hall, offers a five-year undergraduate program leading to the Bachelor of Architecture degree. All undergraduate students study the third year of the program in Rome. The university is globally recognized for its Notre Dame School of Architecture, a faculty that teaches (pre-modernist) traditional and classical architecture and urban planning (e.g. following the principles of New Urbanism and New Classical Architecture). It also awards the renowned annual Driehaus Architecture Prize.
The library system also includes branch libraries for Architecture, Chemistry & Physics, Engineering, Law, and Mathematics as well as information centers in the Mendoza College of Business, the Kellogg Institute for International Studies, the Joan B. Kroc Institute for International Peace Studies, and a slide library in O'Shaughnessy Hall. A theology library was also opened in fall of 2015. Located on the first floor of Stanford Hall, it is the first branch of the library system to be housed in a dorm room. The library system holds over three million volumes, was the single largest university library in the world upon its completion, and remains one of the 100 largest libraries in the country.
The rise of Hitler and other dictators in the 1930s forced numerous Catholic intellectuals to flee Europe; president John O'Hara brought many to Notre Dame. From Germany came Anton-Hermann Chroust (1907–1982) in classics and law, and Waldemar Gurian a German Catholic intellectual of Jewish descent. Positivism dominated American intellectual life in the 1920s onward but in marked contrast, Gurian received a German Catholic education and wrote his doctoral dissertation under Max Scheler. Ivan Meštrović (1883–1962), a renowned sculptor, brought Croatian culture to campus, 1955–62. Yves Simon (1903–61), brought to ND in the 1940s the insights of French studies in the Aristotelian-Thomistic tradition of philosophy; his own teacher Jacques Maritain (1882–73) was a frequent visitor to campus.
The University of Notre Dame du Lac (or simply Notre Dame /ˌnoʊtərˈdeɪm/ NOH-tər-DAYM) is a Catholic research university located adjacent to South Bend, Indiana, in the United States. In French, Notre Dame du Lac means "Our Lady of the Lake" and refers to the university's patron saint, the Virgin Mary. The main campus covers 1,250 acres in a suburban setting and it contains a number of recognizable landmarks, such as the Golden Dome, the "Word of Life" mural (commonly known as Touchdown Jesus), and the Basilica.
Notre Dame rose to national prominence in the early 1900s for its Fighting Irish football team, especially under the guidance of the legendary coach Knute Rockne. The university's athletic teams are members of the NCAA Division I and are known collectively as the Fighting Irish. The football team, an Independent, has accumulated eleven consensus national championships, seven Heisman Trophy winners, 62 members in the College Football Hall of Fame and 13 members in the Pro Football Hall of Fame and is considered one of the most famed and successful college football teams in history. Other ND teams, chiefly in the Atlantic Coast Conference, have accumulated 16 national championships. The Notre Dame Victory March is often regarded as the most famous and recognizable collegiate fight song.
Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university. The undergraduate component of the university is organized into four colleges (Arts and Letters, Science, Engineering, Business) and the Architecture School. The latter is known for teaching New Classical Architecture and for awarding the globally renowned annual Driehaus Architecture Prize. Notre Dame's graduate program has more than 50 master's, doctoral and professional degree programs offered by the five schools, with the addition of the Notre Dame Law School and a MD-PhD program offered in combination with IU medical School. It maintains a system of libraries, cultural venues, artistic and scientific museums, including Hesburgh Library and the Snite Museum of Art. Over 80% of the university's 8,000 undergraduates live on campus in one of 29 single-sex residence halls, each with its own traditions, legacies, events and intramural sports teams. The university counts approximately 120,000 alumni, considered among the strongest alumni networks among U.S. colleges.
In 1842, the Bishop of Vincennes, Célestine Guynemer de la Hailandière, offered land to Father Edward Sorin of the Congregation of the Holy Cross, on the condition that he build a college in two years. Fr. Sorin arrived on the site with eight Holy Cross brothers from France and Ireland on November 26, 1842, and began the school using Father Stephen Badin's old log chapel. He soon erected additional buildings, including Old College, the first church, and the first main building. They immediately acquired two students and set about building additions to the campus.
The first degrees from the college were awarded in 1849. The university was expanded with new buildings to accommodate more students and faculty. With each new president, new academic programs were offered and new buildings built to accommodate them. The original Main Building built by Sorin just after he arrived was replaced by a larger "Main Building" in 1865, which housed the university's administration, classrooms, and dormitories. Beginning in 1873, a library collection was started by Father Lemonnier. By 1879 it had grown to ten thousand volumes that were housed in the Main Building.
The television station, NDtv, grew from one show in 2002 to a full 24-hour channel with original programming by September 2006. WSND-FM serves the student body and larger South Bend community at 88.9 FM, offering students a chance to become involved in bringing classical music, fine arts and educational programming, and alternative rock to the airwaves. Another radio station, WVFI, began as a partner of WSND-FM. More recently, however, WVFI has been airing independently and is streamed on the Internet.
The first phase of Eddy Street Commons, a $215 million development located adjacent to the University of Notre Dame campus and funded by the university, broke ground on June 3, 2008. The Eddy Street Commons drew union protests when workers hired by the City of South Bend to construct the public parking garage picketed the private work site after a contractor hired non-union workers. The developer, Kite Realty out of Indianapolis, has made agreements with major national chains rather than local businesses, a move that has led to criticism from alumni and students.
Notre Dame teams are known as the Fighting Irish. They compete as a member of the National Collegiate Athletic Association (NCAA) Division I, primarily competing in the Atlantic Coast Conference (ACC) for all sports since the 2013–14 school year. The Fighting Irish previously competed in the Horizon League from 1982-83 to 1985-86, and again from 1987-88 to 1994-95, and then in the Big East Conference through 2012–13. Men's sports include baseball, basketball, crew, cross country, fencing, football, golf, ice hockey, lacrosse, soccer, swimming & diving, tennis and track & field; while women's sports include basketball, cross country, fencing, golf, lacrosse, rowing, soccer, softball, swimming & diving, tennis, track & field and volleyball. The football team competes as an Football Bowl Subdivision (FBS) Independent since its inception in 1887. Both fencing teams compete in the Midwest Fencing Conference, and the men's ice hockey team competes in Hockey East.
Notre Dame's conference affiliations for all of its sports except football and fencing changed in July 2013 as a result of major conference realignment, and its fencing affiliation will change in July 2014. The Irish left the Big East for the ACC during a prolonged period of instability in the Big East; while they maintain their football independence, they have committed to play five games per season against ACC opponents. In ice hockey, the Irish were forced to find a new conference home after the Big Ten Conference's decision to add the sport in 2013–14 led to a cascade of conference moves that culminated in the dissolution of the school's former hockey home, the Central Collegiate Hockey Association, after the 2012–13 season. Notre Dame moved its hockey team to Hockey East. After Notre Dame joined the ACC, the conference announced it would add fencing as a sponsored sport beginning in the 2014–15 school year. There are many theories behind the adoption of the athletics moniker but it is known that the Fighting Irish name was used in the early 1920s with respect to the football team and was popularized by alumnus Francis Wallace in his New York Daily News columns. The official colors of Notre Dame are Navy Blue and Gold Rush which are worn in competition by its athletic teams. In addition, the color green is often worn because of the Fighting Irish nickname. The Notre Dame Leprechaun is the mascot of the athletic teams. Created by Theodore W. Drake in 1964, the leprechaun was first used on the football pocket schedule and later on the football program covers. The leprechaun was featured on the cover of Time in November 1964 and gained national exposure.
On July 1, 2014, the University of Notre Dame and Under Armour reached an agreement in which Under Armour will provide uniforms, apparel,equipment, and monetary compensation to Notre Dame for 10 years. This contract, worth almost $100 million, is the most lucrative in the history of the NCAA. The university marching band plays at home games for most of the sports. The band, which began in 1846 and has a claim as the oldest university band in continuous existence in the United States, was honored by the National Music Council as a "Landmark of American Music" during the United States Bicentennial. The band regularly plays the school's fight song the Notre Dame Victory March, which was named as the most played and most famous fight song by Northern Illinois Professor William Studwell. According to College Fight Songs: An Annotated Anthology published in 1998, the "Notre Dame Victory March" ranks as the greatest fight song of all time.
The Notre Dame football team has a long history, first beginning when the Michigan Wolverines football team brought football to Notre Dame in 1887 and played against a group of students. In the long history since then, 13 Fighting Irish teams have won consensus national championships (although the university only claims 11), along with another nine teams being named national champion by at least one source. Additionally, the program has the most members in the College Football Hall of Fame, is tied with Ohio State University with the most Heisman Trophies won, and have the highest winning percentage in NCAA history. With the long history, Notre Dame has accumulated many rivals, and its annual game against USC for the Jeweled Shillelagh has been named by some as one of the most important in college football and is often called the greatest intersectional rivalry in college football in the country.
George Gipp was the school's legendary football player during 1916–20. He played semiprofessional baseball and smoked, drank, and gambled when not playing sports. He was also humble, generous to the needy, and a man of integrity. It was in 1928 that famed coach Knute Rockne used his final conversation with the dying Gipp to inspire the Notre Dame team to beat the Army team and "win one for the Gipper." The 1940 film, Knute Rockne, All American, starred Pat O'Brien as Knute Rockne and Ronald Reagan as Gipp. Today the team competes in Notre Dame Stadium, an 80,795-seat stadium on campus. The current head coach is Brian Kelly, hired from the University of Cincinnati on December 11, 2009. Kelly's record in midway through his sixth season at Notre Dame is 52–21. In 2012, Kelly's Fighting Irish squad went undefeated and played in the BCS National Championship Game. Kelly succeeded Charlie Weis, who was fired in November 2009 after five seasons. Although Weis led his team to two Bowl Championship Series bowl games, his overall record was 35–27, mediocre by Notre Dame standards, and the 2007 team had the most losses in school history. The football team generates enough revenue to operate independently while $22.1 million is retained from the team's profits for academic use. Forbes named the team as the most valuable in college football, worth a total of $101 million in 2007.
Football gameday traditions During home games, activities occur all around campus and different dorms decorate their halls with a traditional item (e.g. Zahm House's two-story banner). Traditional activities begin at the stroke of midnight with the Drummers' Circle. This tradition involves the drum line of the Band of the Fighting Irish and ushers in the rest of the festivities that will continue the rest of the gameday Saturday. Later that day, the trumpet section will play the Notre Dame Victory March and the Notre Dame Alma Mater under the dome. The band entire will play a concert at the steps of Bond Hall, from where they will march into Notre Dame Stadium, leading fans and students alike across campus to the game.
The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.
The "Notre Dame Victory March" is the fight song for the University of Notre Dame. It was written by two brothers who were Notre Dame graduates. The Rev. Michael J. Shea, a 1904 graduate, wrote the music, and his brother, John F. Shea, who earned degrees in 1906 and 1908, wrote the original lyrics. The lyrics were revised in the 1920s; it first appeared under the copyright of the University of Notre Dame in 1928. The chorus is, "Cheer cheer for old Notre Dame, wake up the echos cheering her name. Send a volley cheer on high, shake down the thunder from the sky! What though the odds be great or small, old Notre Dame will win over all. While her loyal sons are marching, onward to victory!"
In the film Knute Rockne, All American, Knute Rockne (played by Pat O'Brien) delivers the famous "Win one for the Gipper" speech, at which point the background music swells with the "Notre Dame Victory March". George Gipp was played by Ronald Reagan, whose nickname "The Gipper" was derived from this role. This scene was parodied in the movie Airplane! with the same background music, only this time honoring George Zipp, one of Ted Striker's former comrades. The song also was prominent in the movie Rudy, with Sean Astin as Daniel "Rudy" Ruettiger, who harbored dreams of playing football at the University of Notre Dame despite significant obstacles.
Notre Dame alumni work in various fields. Alumni working in political fields include state governors, members of the United States Congress, and former United States Secretary of State Condoleezza Rice. A notable alumnus of the College of Science is Medicine Nobel Prize winner Eric F. Wieschaus. A number of university heads are alumni, including Notre Dame's current president, the Rev. John Jenkins. Additionally, many alumni are in the media, including talk show hosts Regis Philbin and Phil Donahue, and television and radio personalities such as Mike Golic and Hannah Storm. With the university having high profile sports teams itself, a number of alumni went on to become involved in athletics outside the university, including professional baseball, basketball, football, and ice hockey players, such as Joe Theismann, Joe Montana, Tim Brown, Ross Browner, Rocket Ismail, Ruth Riley, Jeff Samardzija, Jerome Bettis, Brett Lebda, Olympic gold medalist Mariel Zagunis, professional boxer Mike Lee, former football coaches such as Charlie Weis, Frank Leahy and Knute Rockne, and Basketball Hall of Famers Austin Carr and Adrian Dantley. Other notable alumni include prominent businessman Edward J. DeBartolo, Jr. and astronaut Jim Wetherbee.
Compact Disc (CD) is a digital optical disc data storage format. The format was originally developed to store and play only sound recordings but was later adapted for storage of data (CD-ROM). Several other formats were further derived from these, including write-once audio and data storage (CD-R), rewritable media (CD-RW), Video Compact Disc (VCD), Super Video Compact Disc (SVCD), Photo CD, PictureCD, CD-i, and Enhanced Music CD. Audio CDs and audio CD players have been commercially available since October 1982.
In 2004, worldwide sales of audio CDs, CD-ROMs and CD-Rs reached about 30 billion discs. By 2007, 200 billion CDs had been sold worldwide. CDs are increasingly being replaced by other forms of digital storage and distribution, with the result that audio CD sales rates in the U.S. have dropped about 50% from their peak; however, they remain one of the primary distribution methods for the music industry. In 2014, revenues from digital music services matched those from physical format sales for the first time.
The Compact Disc is an evolution of LaserDisc technology, where a focused laser beam is used that enables the high information density required for high-quality digital audio signals. Prototypes were developed by Philips and Sony independently in the late 1970s. In 1979, Sony and Philips set up a joint task force of engineers to design a new digital audio disc. After a year of experimentation and discussion, the Red Book CD-DA standard was published in 1980. After their commercial release in 1982, compact discs and their players were extremely popular. Despite costing up to $1,000, over 400,000 CD players were sold in the United States between 1983 and 1984. The success of the compact disc has been credited to the cooperation between Philips and Sony, who came together to agree upon and develop compatible hardware. The unified design of the compact disc allowed consumers to purchase any disc or player from any company, and allowed the CD to dominate the at-home music market unchallenged.
In 1974, L. Ottens, director of the audio division of Philips, started a small group with the aim to develop an analog optical audio disc with a diameter of 20 cm and a sound quality superior to that of the vinyl record. However, due to the unsatisfactory performance of the analog format, two Philips research engineers recommended a digital format in March 1974. In 1977, Philips then established a laboratory with the mission of creating a digital audio disc. The diameter of Philips's prototype compact disc was set at 11.5 cm, the diagonal of an audio cassette.
Heitaro Nakajima, who developed an early digital audio recorder within Japan's national public broadcasting organization NHK in 1970, became general manager of Sony's audio department in 1971. His team developed a digital PCM adaptor audio tape recorder using a Betamax video recorder in 1973. After this, in 1974 the leap to storing digital audio on an optical disc was easily made. Sony first publicly demonstrated an optical digital audio disc in September 1976. A year later, in September 1977, Sony showed the press a 30 cm disc that could play 60 minutes of digital audio (44,100 Hz sampling rate and 16-bit resolution) using MFM modulation. In September 1978, the company demonstrated an optical digital audio disc with a 150-minute playing time, 44,056 Hz sampling rate, 16-bit linear resolution, and cross-interleaved error correction code—specifications similar to those later settled upon for the standard Compact Disc format in 1980. Technical details of Sony's digital audio disc were presented during the 62nd AES Convention, held on 13–16 March 1979, in Brussels. Sony's AES technical paper was published on 1 March 1979. A week later, on 8 March, Philips publicly demonstrated a prototype of an optical digital audio disc at a press conference called "Philips Introduce Compact Disc" in Eindhoven, Netherlands.
As a result, in 1979, Sony and Philips set up a joint task force of engineers to design a new digital audio disc. Led by engineers Kees Schouhamer Immink and Toshitada Doi, the research pushed forward laser and optical disc technology. After a year of experimentation and discussion, the task force produced the Red Book CD-DA standard. First published in 1980, the standard was formally adopted by the IEC as an international standard in 1987, with various amendments becoming part of the standard in 1996.
The Japanese launch was followed in March 1983 by the introduction of CD players and discs to Europe and North America (where CBS Records released sixteen titles). This event is often seen as the "Big Bang" of the digital audio revolution. The new audio disc was enthusiastically received, especially in the early-adopting classical music and audiophile communities, and its handling quality received particular praise. As the price of players gradually came down, and with the introduction of the portable Walkman the CD began to gain popularity in the larger popular and rock music markets. The first artist to sell a million copies on CD was Dire Straits, with their 1985 album Brothers in Arms. The first major artist to have his entire catalogue converted to CD was David Bowie, whose 15 studio albums were made available by RCA Records in February 1985, along with four greatest hits albums. In 1988, 400 million CDs were manufactured by 50 pressing plants around the world.
The CD was planned to be the successor of the gramophone record for playing music, rather than primarily as a data storage medium. From its origins as a musical format, CDs have grown to encompass other applications. In 1983, following the CD's introduction, Immink and Braat presented the first experiments with erasable compact discs during the 73rd AES Convention. In June 1985, the computer-readable CD-ROM (read-only memory) and, in 1990, CD-Recordable were introduced, also developed by both Sony and Philips. Recordable CDs were a new alternative to tape for recording music and copying music albums without defects introduced in compression used in other digital recording methods. Other newer video formats such as DVD and Blu-ray use the same physical geometry as CD, and most DVD and Blu-ray players are backward compatible with audio CD.
Meanwhile, with the advent and popularity of Internet-based distribution of files in lossily-compressed audio formats such as MP3, sales of CDs began to decline in the 2000s. For example, between 2000 - 2008, despite overall growth in music sales and one anomalous year of increase, major-label CD sales declined overall by 20%, although independent and DIY music sales may be tracking better according to figures released 30 March 2009, and CDs still continue to sell greatly. As of 2012, CDs and DVDs made up only 34 percent of music sales in the United States. In Japan, however, over 80 percent of music was bought on CDs and other physical formats as of 2015.
Replicated CDs are mass-produced initially using a hydraulic press. Small granules of heated raw polycarbonate plastic are fed into the press. A screw forces the liquefied plastic into the mold cavity. The mold closes with a metal stamper in contact with the disc surface. The plastic is allowed to cool and harden. Once opened, the disc substrate is removed from the mold by a robotic arm, and a 15 mm diameter center hole (called a stacking ring) is created. The time it takes to "stamp" one CD is usually two to three seconds.
This method produces the clear plastic blank part of the disc. After a metallic reflecting layer (usually aluminium, but sometimes gold or other metal) is applied to the clear blank substrate, the disc goes under a UV light for curing and it is ready to go to press. To prepare to press a CD, a glass master is made, using a high-powered laser on a device similar to a CD writer. The glass master is a positive image of the desired CD surface (with the desired microscopic pits and lands). After testing, it is used to make a die by pressing it against a metal disc.
The die is a negative image of the glass master: typically, several are made, depending on the number of pressing mills that are to make the CD. The die then goes into a press, and the physical image is transferred to the blank CD, leaving a final positive image on the disc. A small amount of lacquer is applied as a ring around the center of the disc, and rapid spinning spreads it evenly over the surface. Edge protection lacquer is applied before the disc is finished. The disc can then be printed and packed.
The most expensive part of a CD is the jewel case. In 1995, material costs were 30 cents for the jewel case and 10 to 15 cents for the CD. Wholesale cost of CDs was $0.75 to $1.15, which retailed for $16.98. On average, the store received 35 percent of the retail price, the record company 27 percent, the artist 16 percent, the manufacturer 13 percent, and the distributor 9 percent. When 8-track tapes, cassette tapes, and CDs were introduced, each was marketed at a higher price than the format they succeeded, even though the cost to produce the media was reduced. This was done because the apparent value increased. This continued from vinyl to CDs but was broken when Apple marketed MP3s for $0.99, and albums for $9.99. The incremental cost, though, to produce an MP3 is very small.
CD-R recordings are designed to be permanent. Over time, the dye's physical characteristics may change causing read errors and data loss until the reading device cannot recover with error correction methods. The design life is from 20 to 100 years, depending on the quality of the discs, the quality of the writing drive, and storage conditions. However, testing has demonstrated such degradation of some discs in as little as 18 months under normal storage conditions. This failure is known as disc rot, for which there are several, mostly environmental, reasons.
The ReWritable Audio CD is designed to be used in a consumer audio CD recorder, which will not (without modification) accept standard CD-RW discs. These consumer audio CD recorders use the Serial Copy Management System (SCMS), an early form of digital rights management (DRM), to conform to the United States' Audio Home Recording Act (AHRA). The ReWritable Audio CD is typically somewhat more expensive than CD-RW due to (a) lower volume and (b) a 3% AHRA royalty used to compensate the music industry for the making of a copy.
Due to technical limitations, the original ReWritable CD could be written no faster than 4x speed. High Speed ReWritable CD has a different design, which permits writing at speeds ranging from 4x to 12x. Original CD-RW drives can only write to original ReWritable CDs. High Speed CD-RW drives can typically write to both original ReWritable CDs and High Speed ReWritable CDs. Both types of CD-RW discs can be read in most CD drives. Higher speed CD-RW discs, Ultra Speed (16x to 24x write speed) and Ultra Speed+ (32x write speed) are now available.
A CD is read by focusing a 780 nm wavelength (near infrared) semiconductor laser housed within the CD player, through the bottom of the polycarbonate layer. The change in height between pits and lands results in a difference in the way the light is reflected. By measuring the intensity change with a photodiode, the data can be read from the disc. In order to accommodate the spiral pattern of data, the semiconductor laser is placed on a swing arm within the disc tray of any CD player. This swing arm allows the laser to read information from the centre to the edge of a disc, without having to interrupt the spinning of the disc itself.
The pits and lands themselves do not directly represent the zeros and ones of binary data. Instead, non-return-to-zero, inverted encoding is used: a change from pit to land or land to pit indicates a one, while no change indicates a series of zeros. There must be at least two and no more than ten zeros between each one, which is defined by the length of the pit. This in turn is decoded by reversing the eight-to-fourteen modulation used in mastering the disc, and then reversing the cross-interleaved Reed–Solomon coding, finally revealing the raw data stored on the disc. These encoding techniques (defined in the Red Book) were originally designed for CD Digital Audio, but they later became a standard for almost all CD formats (such as CD-ROM).
CDs are susceptible to damage during handling and from environmental exposure. Pits are much closer to the label side of a disc, enabling defects and contaminants on the clear side to be out of focus during playback. Consequently, CDs are more likely to suffer damage on the label side of the disc. Scratches on the clear side can be repaired by refilling them with similar refractive plastic or by careful polishing. The edges of CDs are sometimes incompletely sealed, allowing gases and liquids to corrode the metal reflective layer and to interfere with the focus of the laser on the pits. The fungus Geotrichum candidum, found in Belize, has been found to consume the polycarbonate plastic and aluminium found in CDs.
The digital data on a CD begins at the center of the disc and proceeds toward the edge, which allows adaptation to the different size formats available. Standard CDs are available in two sizes. By far, the most common is 120 millimetres (4.7 in) in diameter, with a 74- or 80-minute audio capacity and a 650 or 700 MiB (737,280,000-byte) data capacity. This capacity was reportedly specified by Sony executive Norio Ohga in May 1980 so as to be able to contain the entirety of the London Philharmonic Orchestra's recording of Beethoven's Ninth Symphony on one disc. This is a myth according to Kees Immink, as the code format had not yet been decided in May 1980. The adoption of EFM one month later would have allowed a playing time of 97 minutes for 120 mm diameter or 74 minutes for a disc as small as 100 mm. The 120 mm diameter has been adopted by subsequent formats, including Super Audio CD, DVD, HD DVD, and Blu-ray Disc. Eighty-millimeter discs ("Mini CDs") were originally designed for CD singles and can hold up to 24 minutes of music or 210 MiB of data but never became popular.[citation needed] Today, nearly every single is released on a 120 mm CD, called a Maxi single.[citation needed]
The logical format of an audio CD (officially Compact Disc Digital Audio or CD-DA) is described in a document produced in 1980 by the format's joint creators, Sony and Philips. The document is known colloquially as the Red Book CD-DA after the colour of its cover. The format is a two-channel 16-bit PCM encoding at a 44.1 kHz sampling rate per channel. Four-channel sound was to be an allowable option within the Red Book format, but has never been implemented. Monaural audio has no existing standard on a Red Book CD; thus, mono source material is usually presented as two identical channels in a standard Red Book stereo track (i.e., mirrored mono); an MP3 CD, however, can have audio file formats with mono sound.
Compact Disc + Graphics is a special audio compact disc that contains graphics data in addition to the audio data on the disc. The disc can be played on a regular audio CD player, but when played on a special CD+G player, it can output a graphics signal (typically, the CD+G player is hooked up to a television set or a computer monitor); these graphics are almost exclusively used to display lyrics on a television set for karaoke performers to sing along with. The CD+G format takes advantage of the channels R through W. These six bits store the graphics information.
SVCD has two-thirds the resolution of DVD, and over 2.7 times the resolution of VCD. One CD-R disc can hold up to 60 minutes of standard quality SVCD-format video. While no specific limit on SVCD video length is mandated by the specification, one must lower the video bit rate, and therefore quality, to accommodate very long videos. It is usually difficult to fit much more than 100 minutes of video onto one SVCD without incurring significant quality loss, and many hardware players are unable to play video with an instantaneous bit rate lower than 300 to 600 kilobits per second.
Photo CD is a system designed by Kodak for digitizing and storing photos on a CD. Launched in 1992, the discs were designed to hold nearly 100 high-quality images, scanned prints and slides using special proprietary encoding. Photo CDs are defined in the Beige Book and conform to the CD-ROM XA and CD-i Bridge specifications as well. They are intended to play on CD-i players, Photo CD players and any computer with the suitable software irrespective of the operating system. The images can also be printed out on photographic paper with a special Kodak machine. This format is not to be confused with Kodak Picture CD, which is a consumer product in CD-ROM format.
The Red Book audio specification, except for a simple "anti-copy" statement in the subcode, does not include any copy protection mechanism. Known at least as early as 2001, attempts were made by record companies to market "copy-protected" non-standard compact discs, which cannot be ripped, or copied, to hard drives or easily converted to MP3s. One major drawback to these copy-protected discs is that most will not play on either computer CD-ROM drives or some standalone CD players that use CD-ROM mechanisms. Philips has stated that such discs are not permitted to bear the trademarked Compact Disc Digital Audio logo because they violate the Red Book specifications. Numerous copy-protection systems have been countered by readily available, often free, software.
Earth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as the result of a Mars-sized object with about 10% of the Earth's mass impacting the planet in a glancing blow. Some of this object's mass merged with the Earth, significantly altering its internal composition, and a portion was ejected into space. Some of the material survived to form an orbiting moon. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.
Earth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as the result of a Mars-sized object with about 10% of the Earth's mass impacting the planet in a glancing blow. Some of this object's mass merged with the Earth, significantly altering its internal composition, and a portion was ejected into space. Some of the material survived to form an orbiting moon. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.
The Earth of the early Archean (4,000 to 2,500 million years ago) may have had a different tectonic style. During this time, the Earth's crust cooled enough that rocks and continental plates began to form. Some scientists think because the Earth was hotter, that plate tectonic activity was more vigorous than it is today, resulting in a much greater rate of recycling of crustal material. This may have prevented cratonisation and continent formation until the mantle cooled and convection slowed down. Others argue that the subcontinental lithospheric mantle is too buoyant to subduct and that the lack of Archean rocks is a function of erosion and subsequent tectonic events.
In contrast to the Proterozoic, Archean rocks are often heavily metamorphized deep-water sediments, such as graywackes, mudstones, volcanic sediments and banded iron formations. Greenstone belts are typical Archean formations, consisting of alternating high- and low-grade metamorphic rocks. The high-grade rocks were derived from volcanic island arcs, while the low-grade metamorphic rocks represent deep-sea sediments eroded from the neighboring island frogs and deposited in a forearc basin. In short, greenstone belts represent sutured protocontinents.
The geologic record of the Proterozoic (2,500 to 541 million years ago) is more complete than that for the preceding Archean. In contrast to the deep-water deposits of the Archean, the Proterozoic features many strata that were laid down in extensive shallow epicontinental seas; furthermore, many of these rocks are less metamorphosed than Archean-age ones, and plenty are unaltered. Study of these rocks show that the eon featured massive, rapid continental accretion (unique to the Proterozoic), supercontinent cycles, and wholly modern orogenic activity. Roughly 750 million years ago, the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–540 Ma.
The Paleozoic spanned from roughly 541 to 252 million years ago (Ma) and is subdivided into six geologic periods; from oldest to youngest they are the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian. Geologically, the Paleozoic starts shortly after the breakup of a supercontinent called Pannotia and at the end of a global ice age. Throughout the early Paleozoic, the Earth's landmass was broken up into a substantial number of relatively small continents. Toward the end of the era the continents gathered together into a supercontinent called Pangaea, which included most of the Earth's land area.
The Cambrian is a major division of the geologic timescale that begins about 541.0 ± 1.0 Ma. Cambrian continents are thought to have resulted from the breakup of a Neoproterozoic supercontinent called Pannotia. The waters of the Cambrian period appear to have been widespread and shallow. Continental drift rates may have been anomalously high. Laurentia, Baltica and Siberia remained independent continents following the break-up of the supercontinent of Pannotia. Gondwana started to drift toward the South Pole. Panthalassa covered most of the southern hemisphere, and minor oceans included the Proto-Tethys Ocean, Iapetus Ocean and Khanty Ocean.
The Ordovician Period started at a major extinction event called the Cambrian-Ordovician extinction events some time about 485.4 ± 1.9 Ma. During the Ordovician the southern continents were collected into a single continent called Gondwana. Gondwana started the period in the equatorial latitudes and, as the period progressed, drifted toward the South Pole. Early in the Ordovician the continents Laurentia, Siberia and Baltica were still independent continents (since the break-up of the supercontinent Pannotia earlier), but Baltica began to move toward Laurentia later in the period, causing the Iapetus Ocean to shrink between them. Also, Avalonia broke free from Gondwana and began to head north toward Laurentia. The Rheic Ocean was formed as a result of this. By the end of the period, Gondwana had neared or approached the pole and was largely glaciated.
The most-commonly accepted theory is that these events were triggered by the onset of an ice age, in the Hirnantian faunal stage that ended the long, stable greenhouse conditions typical of the Ordovician. The ice age was probably not as long-lasting as once thought; study of oxygen isotopes in fossil brachiopods shows that it was probably no longer than 0.5 to 1.5 million years. The event was preceded by a fall in atmospheric carbon dioxide (from 7000ppm to 4400ppm) which selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it. Evidence of these ice caps have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.
The Silurian is a major division of the geologic timescale that started about 443.8 ± 1.5 Ma. During the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian ice caps were less extensive than those of the late Ordovician glaciation. The melting of ice caps and glaciers contributed to a rise in sea levels, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. Other cratons and continent fragments drifted together near the equator, starting the formation of a second supercontinent known as Euramerica. The vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include Proto-Tethys, Paleo-Tethys, Rheic Ocean, a seaway of Iapetus Ocean (now in between Avalonia and Laurentia), and newly formed Ural Ocean.
The Devonian spanned roughly from 419 to 359 Ma. The period was a time of great tectonic activity, as Laurasia and Gondwana drew closer together. The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions. Near the equator Pangaea began to consolidate from the plates containing North America and Europe, further raising the northern Appalachian Mountains and forming the Caledonian Mountains in Great Britain and Scandinavia. The southern continents remained tied together in the supercontinent of Gondwana. The remainder of modern Eurasia lay in the Northern Hemisphere. Sea levels were high worldwide, and much of the land lay submerged under shallow seas. The deep, enormous Panthalassa (the "universal ocean") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean and Ural Ocean (which was closed during the collision with Siberia and Baltica).
A global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread epicontinental seas and carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwana was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush coal swamps flourished within 30 degrees of the northernmost glaciers. A mid-Carboniferous drop in sea-level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea-level drop and the associated unconformity in North America separate the Mississippian Period from the Pennsylvanian period.
The Carboniferous was a time of active mountain building, as the supercontinent Pangea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America-Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural mountains. There were two major oceans in the Carboniferous the Panthalassa and Paleo-Tethys. Other minor oceans were shrinking and eventually closed the Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica, and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean.
During the Permian all the Earth's major land masses, except portions of East Asia, were collected into a single supercontinent known as Pangaea. Pangaea straddled the equator and extended toward the poles, with a corresponding effect on ocean currents in the single great ocean (Panthalassa, the universal sea), and the Paleo-Tethys Ocean, a large ocean that was between Asia and Gondwana. The Cimmeria continent rifted away from Gondwana and drifted north to Laurasia, causing the Paleo-Tethys to shrink. A new ocean was growing on its southern end, the Tethys Ocean, an ocean that would dominate much of the Mesozoic Era. Large continental landmasses create climates with extreme variations of heat and cold ("continental climate") and monsoon conditions with highly seasonal rainfall patterns. Deserts seem to have been widespread on Pangaea.
The remainder was the world-ocean known as Panthalassa ("all the sea"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean. The supercontinent Pangaea was rifting during the Triassic—especially late in the period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangea—which separated New Jersey from Morocco—are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Supergroup. Because of the limited shoreline of one super-continental mass, Triassic marine deposits are globally relatively rare; despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms living in lagoons and hypersaline environments, such as Estheria crustaceans and terrestrial vertebrates.
The Jurassic Period extends from about 201.3 ± 0.2 to 145.0 Ma. During the early Jurassic, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous Period, when Gondwana itself rifted apart. The Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed. The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site and the renowned late Jurassic lagerstätten of Holzmaden and Solnhofen. In contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation. The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.
During the Cretaceous, the late Paleozoic-early Mesozoic supercontinent of Pangaea completed its breakup into present day continents, although their positions were substantially different at the time. As the Atlantic Ocean widened, the convergent-margin orogenies that had begun during the Jurassic continued in the North American Cordillera, as the Nevadan orogeny was followed by the Sevier and Laramide orogenies. Though Gondwana was still intact in the beginning of the Cretaceous, Gondwana itself broke up as South America, Antarctica and Australia rifted away from Africa (though India and Madagascar remained attached to each other); thus, the South Atlantic and Indian Oceans were newly formed. Such active rifting lifted great undersea mountain chains along the welts, raising eustatic sea levels worldwide.
To the north of Africa the Tethys Sea continued to narrow. Broad shallow seas advanced across central North America (the Western Interior Seaway) and Europe, then receded late in the period, leaving thick marine deposits sandwiched between coal beds. At the peak of the Cretaceous transgression, one-third of Earth's present land area was submerged. The Cretaceous is justly famous for its chalk; indeed, more chalk formed in the Cretaceous than in any other period in the Phanerozoic. Mid-ocean ridge activity—or rather, the circulation of seawater through the enlarged ridges—enriched the oceans in calcium; this made the oceans more saturated, as well as increased the bioavailability of the element for calcareous nanoplankton. These widespread carbonates and other sedimentary deposits make the Cretaceous rock record especially fine. Famous formations from North America include the rich marine fossils of Kansas's Smoky Hill Chalk Member and the terrestrial fauna of the late Cretaceous Hell Creek Formation. Other important Cretaceous exposures occur in Europe and China. In the area that is now India, massive lava beds called the Deccan Traps were laid down in the very late Cretaceous and early Paleocene.
The Cenozoic Era covers the 66 million years since the Cretaceous–Paleogene extinction event up to and including the present day. By the end of the Mesozoic era, the continents had rifted into nearly their present form. Laurasia became North America and Eurasia, while Gondwana split into South America, Africa, Australia, Antarctica and the Indian subcontinent, which collided with the Asian plate. This impact gave rise to the Himalayas. The Tethys Sea, which had separated the northern continents from Africa and India, began to close up, forming the Mediterranean sea.
In many ways, the Paleocene continued processes that had begun during the late Cretaceous Period. During the Paleocene, the continents continued to drift toward their present positions. Supercontinent Laurasia had not yet separated into three continents. Europe and Greenland were still connected. North America and Asia were still intermittently joined by a land bridge, while Greenland and North America were beginning to separate. The Laramide orogeny of the late Cretaceous continued to uplift the Rocky Mountains in the American west, which ended in the succeeding epoch. South and North America remained separated by equatorial seas (they joined during the Neogene); the components of the former southern supercontinent Gondwana continued to split apart, with Africa, South America, Antarctica and Australia pulling away from each other. Africa was heading north toward Europe, slowly closing the Tethys Ocean, and India began its migration to Asia that would lead to a tectonic collision and the formation of the Himalayas.
During the Eocene (56 million years ago - 33.9 million years ago), the continents continued to drift toward their present positions. At the beginning of the period, Australia and Antarctica remained connected, and warm equatorial currents mixed with colder Antarctic waters, distributing the heat around the world and keeping global temperatures high. But when Australia split from the southern continent around 45 Ma, the warm equatorial currents were deflected away from Antarctica, and an isolated cold water channel developed between the two continents. The Antarctic region cooled down, and the ocean surrounding Antarctica began to freeze, sending cold water and ice floes north, reinforcing the cooling. The present pattern of ice ages began about 40 million years ago.[citation needed]
The northern supercontinent of Laurasia began to break up, as Europe, Greenland and North America drifted apart. In western North America, mountain building started in the Eocene, and huge lakes formed in the high flat basins among uplifts. In Europe, the Tethys Sea finally vanished, while the uplift of the Alps isolated its final remnant, the Mediterranean, and created another shallow sea with island archipelagos to the north. Though the North Atlantic was opening, a land connection appears to have remained between North America and Europe since the faunas of the two regions are very similar. India continued its journey away from Africa and began its collision with Asia, creating the Himalayan orogeny.
Antarctica continued to become more isolated and finally developed a permanent ice cap. Mountain building in western North America continued, and the Alps started to rise in Europe as the African plate continued to push north into the Eurasian plate, isolating the remnants of Tethys Sea. A brief marine incursion marks the early Oligocene in Europe. There appears to have been a land bridge in the early Oligocene between North America and Europe since the faunas of the two regions are very similar. During the Oligocene, South America was finally detached from Antarctica and drifted north toward North America. It also allowed the Antarctic Circumpolar Current to flow, rapidly cooling the continent.
During the Miocene continents continued to drift toward their present positions. Of the modern geologic features, only the land bridge between South America and North America was absent, the subduction zone along the Pacific Ocean margin of South America caused the rise of the Andes and the southward extension of the Meso-American peninsula. India continued to collide with Asia. The Tethys Seaway continued to shrink and then disappeared as Africa collided with Eurasia in the Turkish-Arabian region between 19 and 12 Ma (ICS 2004). Subsequent uplift of mountains in the western Mediterranean region and a global fall in sea levels combined to cause a temporary drying up of the Mediterranean Sea resulting in the Messinian salinity crisis near the end of the Miocene.
South America became linked to North America through the Isthmus of Panama during the Pliocene, bringing a nearly complete end to South America's distinctive marsupial faunas. The formation of the Isthmus had major consequences on global temperatures, since warm equatorial ocean currents were cut off and an Atlantic cooling cycle began, with cold Arctic and Antarctic waters dropping temperatures in the now-isolated Atlantic Ocean. Africa's collision with Europe formed the Mediterranean Sea, cutting off the remnants of the Tethys Ocean. Sea level changes exposed the land-bridge between Alaska and Asia. Near the end of the Pliocene, about 2.58 million years ago (the start of the Quaternary Period), the current ice age began. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years.
The last glacial period of the current ice age ended about 10,000 years ago. Ice melt caused world sea levels to rise about 35 metres (115 ft) in the early part of the Holocene. In addition, many areas above about 40 degrees north latitude had been depressed by the weight of the Pleistocene glaciers and rose as much as 180 metres (591 ft) over the late Pleistocene and Holocene, and are still rising today. The sea level rise and temporary land depression allowed temporary marine incursions into areas that are now far from the sea. Holocene marine fossils are known from Vermont, Quebec, Ontario and Michigan. Other than higher latitude temporary marine incursions associated with glacial depression, Holocene fossils are found primarily in lakebed, floodplain and cave deposits. Holocene marine deposits along low-latitude coastlines are rare because the rise in sea levels during the period exceeds any likely upthrusting of non-glacial origin. Post-glacial rebound in Scandinavia resulted in the emergence of coastal areas around the Baltic Sea, including much of Finland. The region continues to rise, still causing weak earthquakes across Northern Europe. The equivalent event in North America was the rebound of Hudson Bay, as it shrank from its larger, immediate post-glacial Tyrrell Sea phase, to near its present boundaries.
An Internet service provider (ISP) is an organization that provides services for accessing, using, the Internet. Internet service providers may be organized in various forms, such as commercial, community-owned, non-profit, or otherwise privately owned.
Internet services typically provided by ISPs include Internet access, Internet transit, domain name registration, web hosting, Usenet service, and colocation.
The Internet was developed as a network between government research laboratories and participating departments of universities. By the late 1980s, a process was set in place towards public, commercial use of the Internet. The remaining restrictions were removed by 1995, 4 years after the introduction of the World Wide Web.
In 1989, the first ISPs were established in Australia and the United States. In Brookline, Massachusetts, The World became the first commercial ISP in the US. Its first customer was served in November 1989.
On 23 April 2014, the U.S. Federal Communications Commission (FCC) was reported to be considering a new rule that will permit ISPs to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On 15 May 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On 10 November 2014, President Barack Obama recommended that the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On 16 January 2015, Republicans presented legislation, in the form of a U.S. Congress H.R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers. On 31 January 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on 26 February 2015. Adoption of this notion would reclassify internet service from one of information to one of the telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to the New York Times.
On 26 February 2015, the FCC ruled in favor of net neutrality by adopting Title II (common carrier) of the Communications Act of 1934 and Section 706 in the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."
On 12 March 2015, the FCC released the specific details of the net neutrality rules. On 13 April 2015, the FCC published the final rule on its new "Net Neutrality" regulations.
 ISPs provide Internet access, employing a range of technologies to connect users to their network. Available technologies have ranged from computer modems with acoustic couplers to telephone lines, to television cable (CATV), wireless Ethernet (wi-fi), and fiber optics.
For users and small businesses, traditional options include copper wires to provide dial-up, DSL, typically asymmetric digital subscriber line (ADSL), cable modem or Integrated Services Digital Network (ISDN) (typically basic rate interface). Using fiber-optics to end users is called Fiber To The Home or similar names.
For customers with more demanding requirements (such as medium-to-large businesses, or other ISPs) can use higher-speed DSL (such as single-pair high-speed digital subscriber line), Ethernet, metropolitan Ethernet, gigabit Ethernet, Frame Relay, ISDN Primary Rate Interface, ATM (Asynchronous Transfer Mode) and synchronous optical networking (SONET).
A mailbox provider is an organization that provides services for hosting electronic mail domains with access to storage for mail boxes. It provides email servers to send, receive, accept, and store email for end users or other organizations.
Many mailbox providers are also access providers, while others are not (e.g., Yahoo! Mail, Outlook.com, Gmail, AOL Mail, Po box). The definition given in RFC 6650 covers email hosting services, as well as the relevant department of companies, universities, organizations, groups, and individuals that manage their mail servers themselves. The task is typically accomplished by implementing Simple Mail Transfer Protocol (SMTP) and possibly providing access to messages through Internet Message Access Protocol (IMAP), the Post Office Protocol, Webmail, or a proprietary protocol.
Internet hosting services provide email, web-hosting, or online storage services. Other services include virtual server, cloud services, or physical server operation.
Just as their customers pay them for Internet access, ISPs themselves pay upstream ISPs for Internet access. An upstream ISP usually has a larger network than the contracting ISP or is able to provide the contracting ISP with access to parts of the Internet the contracting ISP by itself has no access to.
In the simplest case, a single connection is established to an upstream ISP and is used to transmit data to or from areas of the Internet beyond the home network; this mode of interconnection is often cascaded multiple times until reaching a tier 1 carrier. In reality, the situation is often more complex. ISPs with more than one point of presence (PoP) may have separate connections to an upstream ISP at multiple PoPs, or they may be customers of multiple upstream ISPs and may have connections to each one of them at one or more point of presence. Transit ISPs provide large amounts of bandwidth for connecting hosting ISPs and access ISPs.
A virtual ISP (VISP) is an operation that purchases services from another ISP, sometimes called a wholesale ISP in this context, which allow the VISP's customers to access the Internet using services and infrastructure owned and operated by the wholesale ISP. VISPs resemble mobile virtual network operators and competitive local exchange carriers for voice communications.
Free ISPs are Internet service providers that provide service free of charge. Many free ISPs display advertisements while the user is connected; like commercial television, in a sense they are selling the user's attention to the advertiser. Other free ISPs, sometimes called freenets, are run on a nonprofit basis, usually with volunteer staff.[citation needed]
A wireless Internet service provider (WISP) is an Internet service provider with a network based on wireless networking. Technology may include commonplace Wi-Fi wireless mesh networking, or proprietary equipment designed to operate over open 900 MHz, 2.4 GHz, 4.9, 5.2, 5.4, 5.7, and 5.8 GHz bands or licensed frequencies such as 2.5 GHz (EBS/BRS), 3.65 GHz (NN) and in the UHF band (including the MMDS frequency band) and LMDS.[citation needed]
ISPs may engage in peering, where multiple ISPs interconnect at peering points or Internet exchange points (IXs), allowing routing of data between each network, without charging one another for the data transmitted—data that would otherwise have passed through a third upstream ISP, incurring charges from the upstream ISP.
Network hardware, software and specifications, as well as the expertise of network management personnel are important in ensuring that data follows the most efficient route, and upstream connections work reliably. A tradeoff between cost and efficiency is possible.[citation needed]
Internet service providers in many countries are legally required (e.g., via Communications Assistance for Law Enforcement Act (CALEA) in the U.S.) to allow law enforcement agencies to monitor some or all of the information transmitted by the ISP. Furthermore, in some countries ISPs are subject to monitoring by intelligence agencies. In the U.S., a controversial National Security Agency program known as PRISM provides for broad monitoring of Internet users traffic and has raised concerns about potential violation of the privacy protections in the Fourth Amendment to the United States Constitution. Modern ISPs integrate a wide array of surveillance and packet sniffing equipment into their networks, which then feeds the data to law-enforcement/intelligence networks (such as DCSNet in the United States, or SORM in Russia) allowing monitoring of Internet traffic in real time.
Asphalt/bitumen also occurs in unconsolidated sandstones known as "oil sands" in Alberta, Canada, and the similar "tar sands" in Utah, US. The Canadian province of Alberta has most of the world's reserves of natural bitumen, in three huge deposits covering 142,000 square kilometres (55,000 sq mi), an area larger than England or New York state. These bituminous sands contain 166 billion barrels (26.4×10^9 m3) of commercially established oil reserves, giving Canada the third largest oil reserves in the world. and produce over 2.3 million barrels per day (370×10^3 m3/d) of heavy crude oil and synthetic crude oil. Although historically it was used without refining to pave roads, nearly all of the bitumen is now used as raw material for oil refineries in Canada and the United States.
The first use of asphalt/bitumen in the New World was by indigenous peoples. On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring asphalt/bitumen that seeped to the surface above underlying petroleum deposits. All three used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphaltum to provide decorations. It was used as a sealant on baskets to make them watertight for carrying water. Asphaltum was used also to seal the planks on ocean-going canoes.
When maintenance is performed on asphalt pavements, such as milling to remove a worn or damaged surface, the removed material can be returned to a facility for processing into new pavement mixtures. The asphalt/bitumen in the removed material can be reactivated and put back to use in new pavement mixes. With some 95% of paved roads being constructed of or surfaced with asphalt, a substantial amount of asphalt pavement material is reclaimed each year. According to industry surveys conducted annually by the Federal Highway Administration and the National Asphalt Pavement Association, more than 99% of the asphalt removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments.
In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.
Asphalt/bitumen is typically stored and transported at temperatures around 150 °C (302 °F). Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called "bitumen feedstock", or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt/bitumen, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is no longer used as a release agent due to environmental concerns.
The Albanian bitumen extraction has a long history and was practiced in an organized way by the Romans. After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen. In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa. Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.
The word asphalt is derived from the late Middle English, in turn from French asphalte, based on Late Latin asphalton, asphaltum, which is the latinisation of the Greek ἄσφαλτος (ásphaltos, ásphalton), a word meaning "asphalt/bitumen/pitch", which perhaps derives from ἀ-, "without" and σφάλλω (sfallō), "make fall". Note that in French, the term asphalte is used for naturally occurring bitumen-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads. It is a significant fact that the first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. Specifically Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall. From the Greek, the word passed into late Latin, and thence into French (asphalte) and English ("asphaltum" and "asphalt").
The terms asphalt and bitumen are often used interchangeably to mean both natural and manufactured forms of the substance. In American English, asphalt (or asphalt cement) is the carefully refined residue from the distillation process of selected crude oils. Outside the United States, the product is often called bitumen. Geologists often prefer the term bitumen. Common usage often refers to various forms of asphalt/bitumen as "tar", such as at the La Brea Tar Pits. Another archaic term for asphalt/bitumen is "pitch".
The great majority of asphalt used commercially is obtained from petroleum. Nonetheless, large amounts of asphalt occur in concentrated form in nature. Naturally occurring deposits of asphalt/bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things. These remains were deposited in the mud on the bottom of the ocean or lake where the organisms lived. Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as asphalt/bitumen, kerogen, or petroleum.
The world's largest deposit of natural bitumen, known as the Athabasca oil sands is located in the McMurray Formation of Northern Alberta. This formation is from the early Cretaceous, and is composed of numerous lenses of oil-bearing sand with up to 20% oil. Isotopic studies attribute the oil deposits to be about 110 million years old. Two smaller but still very large formations occur in the Peace River oil sands and the Cold Lake oil sands, to the west and southeast of the Athabasca oil sands, respectively. Of the Alberta bitumen deposits, only parts of the Athabasca oil sands are shallow enough to be suitable for surface mining. The other 80% has to be produced by oil wells using enhanced oil recovery techniques like steam-assisted gravity drainage.
Bitumen was used in early photographic technology. In 1826 or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature. The bitumen was thinly coated onto a pewter plate which was then exposed in a camera. Exposure to light hardened the bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained. Many hours of exposure in the camera were required, making bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.[not in citation given]
The first British patent for the use of asphalt/bitumen was 'Cassell's patent asphalte or bitumen' in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson writes that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)". Indeed, mastic pavements had been previously employed at Vauxhall by a competitor of Claridge, but without success.
Roads in the US have been paved with materials that include asphalt/bitumen since at least 1870, when a street in front of the Newark, NJ City Hall was paved. In many cases, these early pavings were made from naturally occurring "bituminous rock", such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873. In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington, DC, in time for the celebration of the national centennial. Asphalt/bitumen was also used for flooring, paving and waterproofing of baths and swimming pools during the early 20th century, following similar trends in Europe.
In 1838, there was a flurry of entrepreneurial activity involving asphalt/bitumen, which had uses beyond paving. For example, asphalt could also used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, with these latter themselves proliferating in the 19th century. On the London stockmarket, there were various claims as to the exclusivity of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, "Claridge's was the type most used in the 1840s and 50s"
The value of the deposit was obvious from the start, but the means of extracting the bitumen were not. The nearest town, Fort McMurray, Alberta was a small fur trading post, other markets were far away, and transportation costs were too high to ship the raw bituminous sand for paving. In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the bitumen to pave 600 feet of road in Edmonton, Alberta. Other roads in Alberta were paved with oil sands, but it was generally not economic. During the 1920s Dr. Karl A. Clark of the Alberta Research Council patented a hot water oil separation process and entrepreneur Robert C. Fitzsimmons built the Bitumount oil separation plant, which between 1925 and 1958 produced up to 300 barrels (50 m3) per day of bitumen using Dr. Clark's method. Most of the bitumen was used for waterproofing roofs, but other uses included fuels, lubrication oils, printers ink, medicines, rust and acid-proof paints, fireproof roofing, street paving, patent leather, and fence post preservatives. Eventually Fitzsimmons ran out of money and the plant was taken over by the Alberta government. Today the Bitumount plant is a Provincial Historic Site.
Canadian bitumen does not differ substantially from oils such as Venezuelan extra-heavy and Mexican heavy oil in chemical composition, and the real difficulty is moving the extremely viscous bitumen through oil pipelines to the refinery. Many modern oil refineries are extremely sophisticated and can process non-upgraded bitumen directly into products such as gasoline, diesel fuel, and refined asphalt without any preprocessing. This is particularly common in areas such as the US Gulf coast, where refineries were designed to process Venezuelan and Mexican oil, and in areas such as the US Midwest where refineries were rebuilt to process heavy oil as domestic light oil production declined. Given the choice, such heavy oil refineries usually prefer to buy bitumen rather than synthetic oil because the cost is lower, and in some cases because they prefer to produce more diesel fuel and less gasoline. By 2015 Canadian production and exports of non-upgraded bitumen exceeded that of synthetic crude oil at over 1.3 million barrels (210×10^3 m3) per day, of which about 65% was exported to the United States.
A number of technologies allow asphalt/bitumen to be mixed at much lower temperatures. These involve mixing with petroleum solvents to form "cutbacks" with reduced melting point, or mixtures with water to turn the asphalt/bitumen into an emulsion. Asphalt emulsions contain up to 70% asphalt/bitumen and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock, gravel or crushed slag. Slurry seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold-mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth and asphalt emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements.
Naturally occurring crude asphalt/bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from "Oil sands", currently under development in Alberta, Canada. Canada has most of the world's supply of natural asphalt/bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world. The Athabasca oil sands is the largest asphalt/bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by in situ methods. Because of oil price increases after 2003, producing bitumen became highly profitable, but as a result of the decline after 2014 it became uneconomic to build new plants again. By 2014, Canadian crude asphalt/bitumen production averaged about 2.3 million barrels (370,000 m3) per day and was projected to rise to 4.4 million barrels (700,000 m3) per day by 2020. The total amount of crude asphalt/bitumen in Alberta which could be extracted is estimated to be about 310 billion barrels (50×10^9 m3), which at a rate of 4,400,000 barrels per day (700,000 m3/d) would last about 200 years.
Roofing shingles account for most of the remaining asphalt/bitumen consumption. Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics. Asphalt/bitumen is used to make Japan black, a lacquer known especially for its use on iron and steel, and it is also used in paint and marker inks by some graffiti supply companies to increase the weather resistance and permanence of the paint or ink, and to make the color much darker.[citation needed] Asphalt/bitumen is also used to seal some alkaline batteries during the manufacturing process.
The expression "bitumen" originated in the Sanskrit, where we find the words jatu, meaning "pitch," and jatu-krit, meaning "pitch creating", "pitch producing" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally gwitu-men (pertaining to pitch), and by others, pixtumens (exuding or bubbling pitch), which was subsequently shortened to bitumen, thence passing via French into English. From the same root is derived the Anglo Saxon word cwidu (mastix), the German word Kitt (cement or mastic) and the old Norse word kvada.
Asphalt/bitumen can sometimes be confused with "coal tar", which is a visually similar black, thermoplastic material produced by the destructive distillation of coal. During the early and mid-20th century when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates. The addition of tar to macadam roads led to the word tarmac, which is now used in common parlance to refer to road-making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt/bitumen has completely overtaken the use of coal tar in these applications. Other examples of this confusion include the La Brea Tar Pits and the Canadian oil sands, both of which actually contain natural bitumen rather than tar. Pitch is another term sometimes used at times to refer to asphalt/bitumen, as in Pitch Lake.
One hundred years after the fall of Constantinople in 1453, Pierre Belon described in his work Observations in 1553 that pissasphalto, a mixture of pitch and bitumen, was used in Dubrovnik for tarring of ships from where it was exported to a market place in Venice where it could be bought by anyone. An 1838 edition of Mechanics Magazine cites an early use of asphalt in France. A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways – "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth", which at that time made the water unusable. "He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation". But it was generally neglected in France until the revolution of 1830. Then, in the 1830s, there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes". Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (BasRhin), the Parc (l'Ain) and the Puy-de-la-Poix (Puy-de-Dome)", although it could also be made artificially. One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.
In 1838, Claridge obtained patents in Scotland on 27 March, and Ireland on 23 April, and in 1851 extensions were sought for all three patents, by the trustees of a company previously formed by Claridge. This was Claridge's Patent Asphalte Company, formed in 1838 for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France", and "laid one of the first asphalt pavements in Whitehall". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park". "The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry". "By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order".
Canada has the world's largest deposit of natural bitumen in the Athabasca oil sands and Canadian First Nations along the Athabasca River had long used it to waterproof their canoes. In 1719, a Cree Indian named Wa-Pa-Su brought a sample for trade to Henry Kelsey of the Hudson’s Bay Company, who was the first recorded European to see it. However, it wasn't until 1787 that fur trader and explorer Alexander MacKenzie saw the Athabasca oil sands and said, "At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance."
Mastic asphalt is a type of asphalt which differs from dense graded asphalt (asphalt concrete) in that it has a higher asphalt/bitumen (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% added asphalt/bitumen. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of 210 °C (410 °F) and is spread in layers to form an impervious barrier about 20 millimeters (0.79 inches) thick.
Because of the difficulty of moving crude bitumen through pipelines, non-upgraded bitumen is usually diluted with natural-gas condensate in a form called dilbit or with synthetic crude oil, called synbit. However, to meet international competition, much non-upgraded bitumen is now sold as a blend of multiple grades of bitumen, conventional crude oil, synthetic crude oil, and condensate in a standardized benchmark product such as Western Canadian Select. This sour, heavy crude oil blend is designed to have uniform refining characteristics to compete with internationally marketed heavy oils such as Mexican Mayan or Arabian Dubai Crude.
Asphalt/bitumen is similar to the organic matter in carbonaceous meteorites. However, detailed studies have shown these materials to be distinct. The vast Alberta bitumen resources are believed to have started out as living material from marine plants and animals, mainly algae, that died millions of years ago when an ancient ocean covered Alberta. They were covered by mud, buried deeply over the eons, and gently cooked into oil by geothermal heat at a temperature of 50 to 150 °C (120 to 300 °F). Due to pressure from the rising of the Rocky Mountains in southwestern Alberta, 80 to 55 million years ago, the oil was driven northeast hundreds of kilometres into underground sand deposits left behind by ancient river beds and ocean beaches, thus forming the oil sands.
Selenizza is mainly used as an additive in the road construction sector. It is mixed with traditional bitumen to improve both the viscoelastic properties and the resistance to ageing. It may be blended with the hot bitumen in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants. Other typical applications include the production of mastic asphalts for sidewalks, bridges, car-parks and urban roads as well as drilling fluid additives for the oil and gas industry. Selenizza is available in powder or in granular material of various particle sizes and is packaged in big bags or in thermal fusible polyethylene bags.
In British English, the word 'asphalt' is used to refer to a mixture of mineral aggregate and asphalt/bitumen (also called tarmac in common parlance). When bitumen is mixed with clay it is usually called asphaltum. The earlier word 'asphaltum' is now archaic and not commonly used.[citation needed] In American English, 'asphalt' is equivalent to the British 'bitumen'. However, 'asphalt' is also commonly used as a shortened form of 'asphalt concrete' (therefore equivalent to the British 'asphalt' or 'tarmac'). In Australian English, bitumen is often used as the generic term for road surfaces. In Canadian English, the word bitumen is used to refer to the vast Canadian deposits of extremely heavy crude oil, while asphalt is used for the oil refinery product used to pave roads and manufacture roof shingles and various waterproofing products. Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as dilbit in the Canadian petroleum industry, while bitumen "upgraded" to synthetic crude oil is known as syncrude and syncrude blended with bitumen as synbit. Bitumen is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum. Bituminous rock is a form of sandstone impregnated with bitumen. The tar sands of Alberta, Canada are a similar material.
Bitumen was the nemesis of many artists during the 19th century. Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common diluents, such as linseed oil, varnish and turpentine. Unless thoroughly diluted, bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact. The use of bitumen as a glaze to set in shadow or mixed with other colors to render a darker tone resulted in the eventual deterioration of many paintings, for instance those of Delacroix. Perhaps the most famous example of the destructiveness of bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.
In 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd. Two products resulted, namely Clarmac, and Clarphalte, with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although Clarmac was more widely used.[note 1] However, the First World War impacted financially on the Clarmac Company, which entered into liquidation in 1915. The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset, and in a subsequent attempt to save the Clarmac Company.
The largest use of asphalt/bitumen is for making asphalt concrete for road surfaces and accounts for approximately 85% of the asphalt consumed in the United States. Asphalt concrete pavement mixes are typically composed of 5% asphalt/bitumen cement and 95% aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt/bitumen cement must be heated so it can be mixed with the aggregates at the asphalt mixing facility. The temperature required varies depending upon characteristics of the asphalt/bitumen and the aggregates, but warm-mix asphalt technologies allow producers to reduce the temperature required. There are about 4,000 asphalt concrete mixing plants in the U.S., and a similar number in Europe.
Synthetic crude oil, also known as syncrude, is the output from a bitumen upgrader facility used in connection with oil sand production in Canada. Bituminous sands are mined using enormous (100 ton capacity) power shovels and loaded into even larger (400 ton capacity) dump trucks for movement to an upgrading facility. The process used to extract the bitumen from the sand is a hot water process originally developed by Dr. Karl Clark of the University of Alberta during the 1920s. After extraction from the sand, the bitumen is fed into a bitumen upgrader which converts it into a light crude oil equivalent. This synthetic substance is fluid enough to be transferred through conventional oil pipelines and can be fed into conventional oil refineries without any further treatment. By 2015 Canadian bitumen upgraders were producing over 1 million barrels (160×10^3 m3) per day of synthetic crude oil, of which 75% was exported to oil refineries in the United States.
About 40,000,000 tons were produced in 1984[needs update]. It is obtained as the "heavy" (i.e., difficult to distill) fraction. Material with a boiling point greater than around 500 °C is considered asphalt. Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel). The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications. In a de-asphalting unit, the crude asphalt is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated. Further processing is possible by "blowing" the product: namely reacting it with oxygen. This step makes the product harder and more viscous.
Selenizza is a naturally occurring solid hydrocarbon bitumen found in the native asphalt deposit of Selenice, in Albania, the only European asphalt mine still in use. The rock asphalt is found in the form of veins, filling cracks in a more or less horizontal direction. The bitumen content varies from 83% to 92% (soluble in carbon disulphide), with a penetration value near to zero and a softening point (ring & ball) around 120 °C. The insoluble matter, consisting mainly of silica ore, ranges from 8% to 17%.
People can be exposed to asphalt in the workplace by breathing in fumes or skin absorption. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 5 mg/m3 over a 15-minute period. Asphalt is basically an inert material that must be heated or diluted to a point where it becomes workable for the production of materials for paving, roofing, and other applications. In examining the potential health hazards associated with asphalt, the International Agency for Research on Cancer (IARC) determined that it is the application parameters, predominantly temperature, that effect occupational exposure and the potential bioavailable carcinogenic hazard/risk of the asphalt emissions. In particular, temperatures greater than 199 °C (390 °F), were shown to produce a greater exposure risk than when asphalt was heated to lower temperatures, such as those typically used in asphalt pavement mix production and placement.
Bird migration is the regular seasonal movement, often north and south along a flyway, between breeding and wintering grounds. Many species of bird migrate. Migration carries high costs in predation and mortality, including from hunting by humans, and is driven primarily by availability of food. It occurs mainly in the northern hemisphere, where birds are funnelled on to specific routes by natural barriers such as the Mediterranean Sea or the Caribbean Sea.
Historically, migration has been recorded as much as 3,000 years ago by Ancient Greek authors including Homer and Aristotle, and in the Book of Job, for species such as storks, turtle doves, and swallows. More recently, Johannes Leche began recording dates of arrivals of spring migrants in Finland in 1749, and scientific studies have used techniques including bird ringing and satellite tracking. Threats to migratory birds have grown with habitat destruction especially of stopover and wintering sites, as well as structures such as power lines and wind farms.
The Arctic tern holds the long-distance migration record for birds, travelling between Arctic breeding grounds and the Antarctic each year. Some species of tubenoses (Procellariiformes) such as albatrosses circle the earth, flying over the southern oceans, while others such as Manx shearwaters migrate 14,000 km (8,700 mi) between their northern breeding grounds and the southern ocean. Shorter migrations are common, including altitudinal migrations on mountains such as the Andes and Himalayas.
The timing of migration seems to be controlled primarily by changes in day length. Migrating birds navigate using celestial cues from the sun and stars, the earth's magnetic field, and probably also mental maps.
Records of bird migration were made as much as 3,000 years ago by the Ancient Greek writers Hesiod, Homer, Herodotus and Aristotle. The Bible also notes migrations, as in the Book of Job (39:26), where the inquiry is made: "Is it by your insight that the hawk hovers, spreads its wings southward?" The author of Jeremiah (8:7) wrote: "Even the stork in the heavens knows its seasons, and the turtle dove, the swift and the crane keep the time of their arrival."
Aristotle noted that cranes traveled from the steppes of Scythia to marshes at the headwaters of the Nile. Pliny the Elder, in his Historia Naturalis, repeats Aristotle's observations.
Aristotle however suggested that swallows and other birds hibernated. This belief persisted as late as 1878, when Elliott Coues listed the titles of no less than 182 papers dealing with the hibernation of swallows. Even the "highly observant" Gilbert White, in his posthumously published 1789 The Natural History of Selborne, quoted a man's story about swallows being found in a chalk cliff collapse "while he was a schoolboy at Brighthelmstone", though the man denied being an eyewitness. However, he also writes that "as to swallows being found in a torpid state during the winter in the Isle of Wight or any part of this country, I never heard any such account worth attending to", and that if early swallows "happen to find frost and snow they immediately withdraw for a time—a circumstance this much more in favour of hiding than migration", since he doubts they would "return for a week or two to warmer latitudes".
It was not until the end of the eighteenth century that migration as an explanation for the winter disappearance of birds from northern climes was accepted. Thomas Bewick's A History of British Birds (Volume 1, 1797) mentions a report from "a very intelligent master of a vessel" who, "between the islands of Minorca and Majorca, saw great numbers of Swallows flying northward", and states the situation in Britain as follows:
Bewick then describes an experiment which succeeded in keeping swallows alive in Britain for several years, where they remained warm and dry through the winters. He concludes:
Migration is the regular seasonal movement, often north and south, undertaken by many species of birds. Bird movements include those made in response to changes in food availability, habitat, or weather. Sometimes, journeys are not termed "true migration" because they are irregular (nomadism, invasions, irruptions) or in only one direction (dispersal, movement of young away from natal area). Migration is marked by its annual seasonality. Non-migratory birds are said to be resident or sedentary. Approximately 1800 of the world's 10,000 bird species are long-distance migrants.
Many bird populations migrate long distances along a flyway. The most common pattern involves flying north in the spring to breed in the temperate or Arctic summer and returning in the autumn to wintering grounds in warmer regions to the south. Of course, in the southern hemisphere the directions are reversed, but there is less land area in the far south to support long-distance migration.
The primary motivation for migration appears to be food; for example, some hummingbirds choose not to migrate if fed through the winter. Also, the longer days of the northern summer provide extended time for breeding birds to feed their young. This helps diurnal birds to produce larger clutches than related non-migratory species that remain in the tropics. As the days shorten in autumn, the birds return to warmer regions where the available food supply varies little with the season.
These advantages offset the high stress, physical exertion costs, and other risks of the migration. Predation can be heightened during migration: Eleonora's falcon Falco eleonorae, which breeds on Mediterranean islands, has a very late breeding season, coordinated with the autumn passage of southbound passerine migrants, which it feeds to its young. A similar strategy is adopted by the greater noctule bat, which preys on nocturnal passerine migrants. The higher concentrations of migrating birds at stopover sites make them prone to parasites and pathogens, which require a heightened immune response.
Within a species not all populations may be migratory; this is known as "partial migration". Partial migration is very common in the southern continents; in Australia, 44% of non-passerine birds and 32% of passerine species are partially migratory. In some species, the population at higher latitudes tends to be migratory and will often winter at lower latitude. The migrating birds bypass the latitudes where other populations may be sedentary, where suitable wintering habitats may already be occupied. This is an example of leap-frog migration. Many fully migratory species show leap-frog migration (birds that nest at higher latitudes spend the winter at lower latitudes), and many show the alternative, chain migration, where populations 'slide' more evenly north and south without reversing order.
Within a population, it is common for different ages and/or sexes to have different patterns of timing and distance. Female chaffinches Fringilla coelebs in Eastern Fennoscandia migrate earlier in the autumn than males do.
Most migrations begin with the birds starting off in a broad front. Often, this front narrows into one or more preferred routes termed flyways. These routes typically follow mountain ranges or coastlines, sometimes rivers, and may take advantage of updrafts and other wind patterns or avoid geographical barriers such as large stretches of open water. The specific routes may be genetically programmed or learned to varying degrees. The routes taken on forward and return migration are often different. A common pattern in North America is clockwise migration, where birds flying North tend to be further West, and flying South tend to shift Eastwards.
Many, if not most, birds migrate in flocks. For larger birds, flying in flocks reduces the energy cost. Geese in a V-formation may conserve 12–20% of the energy they would need to fly alone. Red knots Calidris canutus and dunlins Calidris alpina were found in radar studies to fly 5 km/h (3.1 mph) faster in flocks than when they were flying alone.
Birds fly at varying altitudes during migration. An expedition to Mt. Everest found skeletons of northern pintail Anas acuta and black-tailed godwit Limosa limosa at 5,000 m (16,000 ft) on the Khumbu Glacier. Bar-headed geese Anser indicus have been recorded by GPS flying at up to 6,540 metres (21,460 ft) while crossing the Himalayas, at the same time engaging in the highest rates of climb to altitude for any bird. Anecdotal reports of them flying much higher have yet to be corroborated with any direct evidence. Seabirds fly low over water but gain altitude when crossing land, and the reverse pattern is seen in landbirds. However most bird migration is in the range of 150 to 600 m (490 to 1,970 ft). Bird strike aviation records from the United States show most collisions occur below 600 m (2,000 ft) and almost none above 1,800 m (5,900 ft).
Bird migration is not limited to birds that can fly. Most species of penguin (Spheniscidae) migrate by swimming. These routes can cover over 1,000 km (620 mi). Dusky grouse Dendragapus obscurus perform altitudinal migration mostly by walking. Emus Dromaius novaehollandiae in Australia have been observed to undertake long-distance movements on foot during droughts.
The typical image of migration is of northern landbirds, such as swallows (Hirundinidae) and birds of prey, making long flights to the tropics. However, many Holarctic wildfowl and finch (Fringillidae) species winter in the North Temperate Zone, in regions with milder winters than their summer breeding grounds. For example, the pink-footed goose Anser brachyrhynchus migrates from Iceland to Britain and neighbouring countries, whilst the dark-eyed junco Junco hyemalis migrates from subarctic and arctic climates to the contiguous United States and the American goldfinch from taiga to wintering grounds extending from the American South northwestward to Western Oregon. Migratory routes and wintering grounds are traditional and learned by young during their first migration with their parents. Some ducks, such as the garganey Anas querquedula, move completely or partially into the tropics. The European pied flycatcher Ficedula hypoleuca also follows this migratory trend, breeding in Asia and Europe and wintering in Africa.
Often, the migration route of a long-distance migrator bird doesn't follow a straight line between breeding and wintering grounds. Rather, it could follow an hooked or arched line, with detours around geographical barriers. For most land-birds, such barriers could consist in seas, large water bodies or high mountain ranges, because of the lack of stopover or feeding sites, or the lack of thermal columns for broad-winged birds.
The same considerations about barriers and detours that apply to long-distance land-bird migration apply to water birds, but in reverse: a large area of land without bodies of water that offer feeding sites may also be a barrier to a bird that feeds in coastal waters. Detours avoiding such barriers are observed: for example, brent geese Branta bernicla migrating from the Taymyr Peninsula to the Wadden Sea travel via the White Sea coast and the Baltic Sea rather than directly across the Arctic Ocean and northern Scandinavia.
A similar situation occurs with waders (called shorebirds in North America). Many species, such as dunlin Calidris alpina and western sandpiper Calidris mauri, undertake long movements from their Arctic breeding grounds to warmer locations in the same hemisphere, but others such as semipalmated sandpiper C. pusilla travel longer distances to the tropics in the Southern Hemisphere.
For some species of waders, migration success depends on the availability of certain key food resources at stopover points along the migration route. This gives the migrants an opportunity to refuel for the next leg of the voyage. Some examples of important stopover locations are the Bay of Fundy and Delaware Bay.
Some bar-tailed godwits Limosa lapponica have the longest known non-stop flight of any migrant, flying 11,000 km from Alaska to their New Zealand non-breeding areas. Prior to migration, 55 percent of their bodyweight is stored as fat to fuel this uninterrupted journey.
Seabird migration is similar in pattern to those of the waders and waterfowl. Some, such as the black guillemot Cepphus grylle and some gulls, are quite sedentary; others, such as most terns and auks breeding in the temperate northern hemisphere, move varying distances south in the northern winter. The Arctic tern Sterna paradisaea has the longest-distance migration of any bird, and sees more daylight than any other, moving from its Arctic breeding grounds to the Antarctic non-breeding areas. One Arctic tern, ringed (banded) as a chick on the Farne Islands off the British east coast, reached Melbourne, Australia in just three months from fledging, a sea journey of over 22,000 km (14,000 mi). Many tubenosed birds breed in the southern hemisphere and migrate north in the southern winter.
The most pelagic species, mainly in the 'tubenose' order Procellariiformes, are great wanderers, and the albatrosses of the southern oceans may circle the globe as they ride the "roaring forties" outside the breeding season. The tubenoses spread widely over large areas of open ocean, but congregate when food becomes available. Many are also among the longest-distance migrants; sooty shearwaters Puffinus griseus nesting on the Falkland Islands migrate 14,000 km (8,700 mi) between the breeding colony and the North Atlantic Ocean off Norway. Some Manx shearwaters Puffinus puffinus do this same journey in reverse. As they are long-lived birds, they may cover enormous distances during their lives; one record-breaking Manx shearwater is calculated to have flown 8 million km (5 million miles) during its over-50 year lifespan.
Some large broad-winged birds rely on thermal columns of rising hot air to enable them to soar. These include many birds of prey such as vultures, eagles, and buzzards, but also storks. These birds migrate in the daytime. Migratory species in these groups have great difficulty crossing large bodies of water, since thermals only form over land, and these birds cannot maintain active flight for long distances. Mediterranean and other seas present a major obstacle to soaring birds, which must cross at the narrowest points. Massive numbers of large raptors and storks pass through areas such as the Strait of Messina, Gibraltar, Falsterbo, and the Bosphorus at migration times. More common species, such as the European honey buzzard Pernis apivorus, can be counted in hundreds of thousands in autumn. Other barriers, such as mountain ranges, can also cause funnelling, particularly of large diurnal migrants. This is a notable factor in the Central American migratory bottleneck. Batumi bottleneck in the Caucasus is one of the heaviest migratory funnels on earth. Avoiding flying over the Black Sea surface and across high mountains, hundreds of thousands of soaring birds funnel through an area around the city of Batumi, Georgia. Birds of prey such as honey buzzards which migrate using thermals lose only 10 to 20% of their weight during migration, which may explain why they forage less during migration than do smaller birds of prey with more active flight such as falcons, hawks and harriers.
Many of the smaller insectivorous birds including the warblers, hummingbirds and flycatchers migrate large distances, usually at night. They land in the morning and may feed for a few days before resuming their migration. The birds are referred to as passage migrants in the regions where they occur for short durations between the origin and destination.
Nocturnal migrants minimize predation, avoid overheating, and can feed during the day. One cost of nocturnal migration is the loss of sleep. Migrants may be able to alter their quality of sleep to compensate for the loss.
Many long-distance migrants appear to be genetically programmed to respond to changing day length. Species that move short distances, however, may not need such a timing mechanism, instead moving in response to local weather conditions. Thus mountain and moorland breeders, such as wallcreeper Tichodroma muraria and white-throated dipper Cinclus cinclus, may move only altitudinally to escape the cold higher ground. Other species such as merlin Falco columbarius and Eurasian skylark Alauda arvensis move further, to the coast or towards the south. Species like the chaffinch are much less migratory in Britain than those of continental Europe, mostly not moving more than 5 km in their lives.
Short-distance passerine migrants have two evolutionary origins. Those that have long-distance migrants in the same family, such as the common chiffchaff Phylloscopus collybita, are species of southern hemisphere origins that have progressively shortened their return migration to stay in the northern hemisphere.
Species that have no long-distance migratory relatives, such as the waxwings Bombycilla, are effectively moving in response to winter weather and the loss of their usual winter food, rather than enhanced breeding opportunities.
In the tropics there is little variation in the length of day throughout the year, and it is always warm enough for a food supply, but altitudinal migration occurs in some tropical birds. There is evidence that this enables the migrants to obtain more of their preferred foods such as fruits.
Sometimes circumstances such as a good breeding season followed by a food source failure the following year lead to irruptions in which large numbers of a species move far beyond the normal range. Bohemian waxwings Bombycilla garrulus well show this unpredictable variation in annual numbers, with five major arrivals in Britain during the nineteenth century, but 18 between the years 1937 and 2000. Red crossbills Loxia curvirostra too are irruptive, with widespread invasions across England noted in 1251, 1593, 1757, and 1791.
Bird migration is primarily, but not entirely, a Northern Hemisphere phenomenon. This is because land birds in high northern latitudes, where food becomes scarce in winter, leave for areas further south (including the Southern Hemisphere) to overwinter, and because the continental landmass is much larger in the Northern Hemisphere. In contrast, among (pelagic) seabirds, species of the Southern Hemisphere are more likely to migrate. This is because there is a large area of ocean in the Southern Hemisphere, and more islands suitable for seabirds to nest.
The control of migration, its timing and response are genetically controlled and appear to be a primitive trait that is present even in non-migratory species of birds. The ability to navigate and orient themselves during migration is a much more complex phenomenon that may include both endogenous programs as well as learning.
The primary physiological cue for migration are the changes in the day length. These changes are also related to hormonal changes in the birds. In the period before migration, many birds display higher activity or Zugunruhe (German: migratory restlessness), first described by Johann Friedrich Naumann in 1795, as well as physiological changes such as increased fat deposition. The occurrence of Zugunruhe even in cage-raised birds with no environmental cues (e.g. shortening of day and falling temperature) has pointed to the role of circannual endogenous programs in controlling bird migrations. Caged birds display a preferential flight direction that corresponds with the migratory direction they would take in nature, changing their preferential direction at roughly the same time their wild conspecifics change course.
In polygynous species with considerable sexual dimorphism, males tend to return earlier to the breeding sites than their females. This is termed protandry.
Navigation is based on a variety of senses. Many birds have been shown to use a sun compass. Using the sun for direction involves the need for making compensation based on the time. Navigation has also been shown to be based on a combination of other abilities including the ability to detect magnetic fields (magnetoception), use visual landmarks as well as olfactory cues.
Long distance migrants are believed to disperse as young birds and form attachments to potential breeding sites and to favourite wintering sites. Once the site attachment is made they show high site-fidelity, visiting the same wintering sites year after year.
The ability of birds to navigate during migrations cannot be fully explained by endogenous programming, even with the help of responses to environmental cues. The ability to successfully perform long-distance migrations can probably only be fully explained with an accounting for the cognitive ability of the birds to recognize habitats and form mental maps. Satellite tracking of day migrating raptors such as ospreys and honey buzzards has shown that older individuals are better at making corrections for wind drift.
Migratory birds may use two electromagnetic tools to find their destinations: one that is entirely innate and another that relies on experience. A young bird on its first migration flies in the correct direction according to the Earth's magnetic field, but does not know how far the journey will be. It does this through a radical pair mechanism whereby chemical reactions in special photo pigments sensitive to long wavelengths are affected by the field. Although this only works during daylight hours, it does not use the position of the sun in any way. At this stage the bird is in the position of a boy scout with a compass but no map, until it grows accustomed to the journey and can put its other capabilities to use. With experience it learns various landmarks and this "mapping" is done by magnetites in the trigeminal system, which tell the bird how strong the field is. Because birds migrate between northern and southern regions, the magnetic field strengths at different latitudes let it interpret the radical pair mechanism more accurately and let it know when it has reached its destination. There is a neural connection between the eye and "Cluster N", the part of the forebrain that is active during migrational orientation, suggesting that birds may actually be able to see the magnetic field of the earth.
Migrating birds can lose their way and appear outside their normal ranges. This can be due to flying past their destinations as in the "spring overshoot" in which birds returning to their breeding areas overshoot and end up further north than intended. Certain areas, because of their location, have become famous as watchpoints for such birds. Examples are the Point Pelee National Park in Canada, and Spurn in England.
Reverse migration, where the genetic programming of young birds fails to work properly, can lead to rarities turning up as vagrants thousands of kilometres out of range.
A related phenomenon called "abmigration" involves birds from one region joining similar birds from a different breeding region in the common winter grounds and then migrating back along with the new population. This is especially common in some waterfowl, which shift from one flyway to another.
It has been possible to teach a migration route to a flock of birds, for example in re-introduction schemes. After a trial with Canada geese Branta canadensis, microlight aircraft were used in the US to teach safe migration routes to reintroduced whooping cranes Grus americana.
Birds need to alter their metabolism in order to meet the demands of migration. The storage of energy through the accumulation of fat and the control of sleep in nocturnal migrants require special physiological adaptations. In addition, the feathers of a bird suffer from wear-and-tear and require to be molted. The timing of this molt - usually once a year but sometimes twice - varies with some species molting prior to moving to their winter grounds and others molting prior to returning to their breeding grounds. Apart from physiological adaptations, migration sometimes requires behavioural changes such as flying in flocks to reduce the energy used in migration or the risk of predation.
Migration in birds is highly labile and is believed to have developed independently in many avian lineages. While it is agreed that the behavioral and physiological adaptations necessary for migration are under genetic control, some authors have argued that no genetic change is necessary for migratory behavior to develop in a sedentary species because the genetic framework for migratory behavior exists in nearly all avian lineages. This explains the rapid appearance of migratory behavior after the most recent glacial maximum.
Theoretical analyses show that detours that increase flight distance by up to 20% will often be adaptive on aerodynamic grounds - a bird that loads itself with food to cross a long barrier flies less efficiently. However some species show circuitous migratory routes that reflect historical range expansions and are far from optimal in ecological terms. An example is the migration of continental populations of Swainson's thrush Catharus ustulatus, which fly far east across North America before turning south via Florida to reach northern South America; this route is believed to be the consequence of a range expansion that occurred about 10,000 years ago. Detours may also be caused by differential wind conditions, predation risk, or other factors.
Large scale climatic changes, as have been experienced in the past, are expected to have an effect on the timing of migration. Studies have shown a variety of effects including timing changes in migration, breeding as well as population variations.
The migration of birds also aids the movement of other species, including those of ectoparasites such as ticks and lice, which in turn may carry micro-organisms including those of concern to human health. Due to the global spread of avian influenza, bird migration has been studied as a possible mechanism of disease transmission, but it has been found not to present a special risk; import of pet and domestic birds is a greater threat. Some viruses that are maintained in birds without lethal effects, such as the West Nile Virus may however be spread by migrating birds. Birds may also have a role in the dispersal of propagules of plants and plankton.
Some predators take advantage of the concentration of birds during migration. Greater noctule bats feed on nocturnal migrating passerines. Some birds of prey specialize on migrating waders.
Bird migration routes have been studied by a variety of techniques including the oldest, marking. Swans have been marked with a nick on the beak since about 1560 in England. Scientific ringing was pioneered by Hans Christian Cornelius Mortensen in 1899. Other techniques include radar and satellite tracking.
Orientation behaviour studies have been traditionally carried out using variants of a setup known as the Emlen funnel, which consists of a circular cage with the top covered by glass or wire-screen so that either the sky is visible or the setup is placed in a planetarium or with other controls on environmental cues. The orientation behaviour of the bird inside the cage is studied quantitatively using the distribution of marks that the bird leaves on the walls of the cage. Other approaches used in pigeon homing studies make use of the direction in which the bird vanishes on the horizon.
Hunting along migration routes threatens some bird species. The populations of Siberian cranes (Leucogeranus leucogeranus) that wintered in India declined due to hunting along the route, particularly in Afghanistan and Central Asia. Birds were last seen in their favourite wintering grounds in Keoladeo National Park in 2002. Structures such as power lines, wind farms and offshore oil-rigs have also been known to affect migratory birds. Other migration hazards include pollution, storms, wildfires, and habitat destruction along migration routes, denying migrants food at stopover points. For example, in the East Asian–Australasian Flyway, up to 65% of key intertidal habitat at the Yellow Sea migration bottleneck has been destroyed since the 1950s.
Eton is one of ten English HMC schools, commonly referred to as "public schools", regulated by the Public Schools Act of 1868. Following the public school tradition, Eton is a full boarding school, which means all pupils live at the school, and it is one of four such remaining single-sex boys' public schools in the United Kingdom (the others being Harrow, Radley, and Winchester) to continue this practice. Eton has educated 19 British prime ministers and generations of the aristocracy and has been referred to as the chief nurse of England's statesmen. Charging up to £11,478 per term (there are three terms per academic year) in 2014/15, Eton is the sixth most expensive HMC boarding school in the UK.
Eton has a long list of distinguished former pupils. David Cameron is the 19th British prime minister to have attended the school, and has recommended that Eton set up a school in the state sector to help drive up standards. Eton now co-sponsors a state sixth-form college in Newham, a deprived area of East London, called the London Academy of Excellence, opened in 2012, which is free of charge and aims to get all its students into higher education. In September 2014, Eton opened, and became the sole educational sponsor for, a new purpose-built co-educational state boarding and day school for around 500 pupils, Holyport College, in Maidenhead in Berkshire, with construction costing around £15 million, in which a fifth of places for day pupils will be set aside for children from poor homes, 21 boarding places will go to youngsters on the verge of being taken into care, and a further 28 boarders will be funded or part-funded through bursaries.
About 20% of pupils at Eton receive financial support, through a range of bursaries and scholarships. The recent Head Master, Tony Little, said that Eton is developing plans to allow any boy to attend the school whatever his parents' income and, in 2011, said that around 250 boys received "significant" financial help from the school. In early 2014, this figure had risen to 263 pupils receiving the equivalent of around 60% of school fee assistance, whilst a further 63 received their education free of charge. Little said that, in the short term, he wanted to ensure that around 320 pupils per year receive bursaries, and that 70 were educated free of charge, with the intention that the number of pupils receiving financial assistance would continue to increase. These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August). Launched in 1982, the Universities Summer School is an intensive residential course open to boys and girls throughout the UK who attend state schools, are at the end of their first year in the Sixth Form, and are about to begin their final year of schooling. The Brent-Eton Summer School, started in 1994, offers 40-50 young people from the London Borough of Brent, an area of inner-city deprivation, an intensive one-week residential course, free of charge, designed to help bridge the gap between GCSE and A-level. In 2008, Eton helped found the Eton, Slough, Windsor and Hounslow Independent and State School Partnership (ISSP), with six local state schools. The ISSP's aims are "to raise pupil achievement, improve pupil self-esteem, raise pupil aspirations and improve professional practice across the schools". Eton also runs a number of choral and English language courses during the summer months.
In the run-up to the London 2012 Summer Olympic Games and London 2012 Summer Paralympic Games, Eton's purpose-built Dorney Lake, a permanent, eight-lane, 2,200 metre course (about 1.4 miles) in a 400-acre park, officially known throughout the Games as Eton Dorney, provided training facilities for Olympic and Paralympic competitors, and during the Games, hosted the Olympic and Paralympic Rowing competitions as well as the Olympic Canoe Sprint event, attracting over 400,000 visitors during the Games period (around 30,000 per day), and voted the best 2012 Olympic venue by spectators. Access to the 400-acre parkland around the Lake is provided to members of the public, free of charge, almost all the year round.
Construction of the chapel, originally intended to be slightly over twice as long, with eighteen - or possibly seventeen - bays (there are eight today) was stopped when Henry VI was deposed. Only the Quire of the intended building was completed. Eton's first Headmaster, William Waynflete, founder of Magdalen College, Oxford and previously Head Master of Winchester College, built the ante-chapel that finishes the Chapel today. The important wall paintings in the Chapel and the brick north range of the present School Yard also date from the 1480s; the lower storeys of the cloister, including College Hall, had been built between 1441 and 1460.
As the school suffered reduced income while still under construction, the completion and further development of the school has since depended to some extent on wealthy benefactors. Building resumed when Roger Lupton was Provost, around 1517. His name is borne by the big gate-house in the west range of the cloisters, fronting School Yard, perhaps the most famous image of the school. This range includes the important interiors of the Parlour, Election Hall, and Election Chamber, where most of the 18th century "leaving portraits" are kept.
The Duke of Wellington is often incorrectly quoted as saying that "The Battle of Waterloo was won on the playing-fields of Eton". Wellington was at Eton from 1781 to 1784 and was to send his sons there. According to Nevill (citing the historian Sir Edward Creasy), what Wellington said, while passing an Eton cricket match many decades later, was, "There grows the stuff that won Waterloo", a remark Nevill construes as a reference to "the manly character induced by games and sport" amongst English youth generally, not a comment about Eton specifically. In 1889, Sir William Fraser conflated this uncorroborated remark with the one attributed to him by Count Charles de Montalembert's "C'est ici qu'a été gagné la bataille de Waterloo" ("It is here that the Battle of Waterloo was won.")
As with other public schools, a scheme was devised towards the end of the 19th century to familiarize privileged schoolboys with social conditions in deprived areas. The project of establishing an 'Eton Mission' in the crowded district of Hackney Wick in east London was started at the beginning of 1880, and lasted until 1971 when it was decided that a more local project (at Dorney) would be more realistic. However over the years much money was raised for the Eton Mission, a fine church by G. F. Bodley was erected, many Etonians visited, and stimulated among other things the Eton Manor Boys' Club, a notable rowing club which has survived the Mission itself, and the 59 Club for motorcyclists.
The very large and ornate School Hall and School Library (by L K. Hall) were erected in 1906-8 across the road from Upper School as the school's memorial to the Etonians who had died in the Boer War. Many tablets in the cloisters and chapel commemorate the large number of dead Etonians of the Great War. A bomb destroyed part of Upper School in World War Two and blew out many windows in the Chapel. The college commissioned replacements by Evie Hone (1949–52) and by John Piper and Patrick Reyntiens (1959 onwards).
In the past, people at Eton have occasionally been guilty of antisemitism. For a time, new admissions were called 'Jews' by their fellow Collegers. In 1945, the school introduced a nationality statute conditioning entry on the applicant's father being British by birth. The statute was removed after the intervention of Prime Minister Harold Macmillan in the 1960s after it came to the attention of Oxford's Wykeham Professor of Logic, A. J. Ayer, himself Jewish and an Old Etonian, who "suspected a whiff of anti-semitism".
One boarding house, College, is reserved for seventy King's Scholars, who attend Eton on scholarships provided by the original foundation and awarded by examination each year; King's Scholars pay up to 90% of full fees, depending on their means. Of the other pupils, up to a third receive some kind of bursary or scholarship. The name "King's Scholars" is because the school was founded by King Henry VI in 1440. The original School consisted of the seventy Scholars (together with some Commensals) and the Scholars were educated and boarded at the foundation's expense.
As the School grew, more students were allowed to attend provided that they paid their own fees and lived in the town, outside the College's original buildings. These students became known as Oppidans, from the Latin word oppidum, meaning town. The Houses developed over time as a means of providing residence for the Oppidans in a more congenial manner, and during the 18th and 19th centuries were mostly run by women known as "dames". They typically contain about fifty boys. Although classes are organised on a School basis, most boys spend a large proportion of their time in their House. Each House has a formal name, mainly used for post and people outside the Eton community. It is generally known by the boys by the initials or surname of the House Master, the teacher who lives in the house and manages the pupils in it.
Not all boys who pass the College election examination choose to become King's Scholars. If they choose instead to belong to one of the 24 Oppidan Houses, they are known as Oppidan Scholars. Oppidan scholarships may also be awarded for consistently performing with distinction in School and external examinations. To gain an Oppidan Scholarship, a boy must have either three distinctions in a row or four throughout his career. Within the school, an Oppidan Scholar is entitled to use the letters OS after his name.
The Oppidan Houses are named Godolphin House, Jourdelay's, (both built as such c. 1720), Hawtrey House, Durnford House, (the first two built as such by the Provost and Fellows, 1845, when the school was increasing in numbers and needed more centralised control), The Hopgarden, South Lawn, Waynflete, Evans's, Keate House, Warre House, Villiers House, Common Lane House, Penn House, Walpole House, Cotton Hall, Wotton House, Holland House, Mustians, Angelo's, Manor House, Farrer House, Baldwin's Bec, The Timbralls, and Westbury.
For much of Eton's history, junior boys had to act as "fags", or servants, to older boys. Their duties included cleaning, cooking, and running errands. A Library member was entitled to yell at any time and without notice, "Boy, Up!" or "Boy, Queue!", and all first-year boys had to come running. The last boy to arrive was given the task. These practices, known as fagging, were partially phased out of most houses in the 1970s. Captains of House and Games still sometimes give tasks to first-year boys, such as collecting the mail from School Office.[citation needed]
The long-standing claim that the present uniform was first worn as mourning for the death of George III is unfounded. "Eton dress" has undergone significant changes since its standardisation in the 19th century. Originally (along with a top-hat and walking-cane), Etonian dress was reserved for formal occasions, but boys wear it today for classes, which are referred to as "divisions", or "divs". As stated above, King's Scholars wear a black gown over the top of their tailcoats, and occasionally a surplice in Chapel. Members of the teaching staff (known as Beaks) are required to wear a form of school dress when teaching.
Later the emphasis was on classical studies, dominated by Latin and Ancient History, and, for boys with sufficient ability, Classical Greek. From the latter part of the 19th century this curriculum has changed and broadened: for example, there are now more than 100 students of Chinese, which is a non-curriculum course. In the 1970s, there was just one school computer, in a small room attached to the science buildings. It used paper tape to store programs. Today, all boys must have laptop computers, and the school fibre-optic network connects all classrooms and all boys' bedrooms to the internet.
The primary responsibility for a boy's studies lies with his House Master, but he is assisted by an additional director of studies, known as a tutor. Classes, colloquially known as "divs" (divisions), are organised on a School basis; the classrooms are separate from the houses. New school buildings have appeared for teaching purposes every decade or so since New Schools, designed by Henry Woodyer and built 1861-3. Despite the introduction of modern technology, the external appearance and locations of many of the classrooms have remained unchanged for a long time.
Societies tend to come and go, of course, depending on the special enthusiasms of the masters and boys in the school at the time, but some have been in existence many years. Those in existence at present include: Aeronautical, African, Alexander Cozens (Art), Amnesty, Archeological, Architectural, Astronomy, Banks (conservation), Caledonian, Cheese, Classical, Comedy, Cosmopolitan, Debating, Design, Entrepreneurship, Geographical, Henry Fielding, Hispanic, History, Keynes (economics), Law, Literary, Mathematical, Medical, Middle Eastern, Model United Nations, Modern Languages, Oriental, Orwell (left-wing), Simeon (Christian), Parry (music), Photographic, Political, Praed (poetry), Rock (music), Rous (equestrian), Salisbury (diplomatic), Savile (Rare Books and Manuscripts), Shelley, Scientific, Sports, Tech Club, Theatre, Wellington (military), Wine and Wotton’s (philosophy).
Prizes are awarded on the results of trials (internal exams), GCSE and AS-levels. In addition, many subjects and activities have specially endowed prizes, several of which are awarded by visiting experts. The most prestigious is the Newcastle Scholarship, awarded on the strength of an examination, consisting of two papers in philosophical theology, moral theory and applied ethics. Also of note are the Gladstone Memorial Prize and the Coutts Prize, awarded on the results of trials and AS-level examinations in C; and the Huxley Prize, awarded for a project on a scientific subject. Other specialist prizes include the Newcastle Classical Prize; the Rosebery Exhibition for History; the Queen’s Prizes for French and German; the Duke of Newcastle’s Russian Prize; the Beddington Spanish Prize; the Strafford and Bowman Shakespeare Prizes; the Tomline and Russell Prizes in Mathematics; the Sotheby Prize for History of Art; the Waddington Prize for Theology and Philosophy; the Birley Prize for History; The Lower Boy Rosebery Prize and the Wilder Prize for Theology. Prizes are awarded too for excellence in such activities as painting, sculpture, ceramics, playing musical instruments, musical composition, declamation, silverwork, and design.
Various benefactions make it possible to give grants each year to boys who wish, for educational or cultural reasons, to work or travel abroad. These include the Busk Fund, which supports individual ventures that show particular initiative; the C.M. Wells Memorial Trust Fund, for the promotion of visits to classical lands; the Sadler Fund, which supports, amongst others, those intending to enter the Foreign Service; and the Marsden Fund, for travel in countries where the principal language is not English.
If any boy produces an outstanding piece of work, it may be "Sent Up For Good", storing the effort in the College Archives for posterity. This award has been around since the 18th century. As Sending Up For Good is fairly infrequent, the process is rather mysterious to many of Eton's boys. First, the master wishing to Send Up For Good must gain the permission of the relevant Head of Department. Upon receiving his or her approval, the piece of work will be marked with Sent Up For Good and the student will receive a card to be signed by House Master, tutor and division master.
The opposite of a Show Up is a "Rip". This is for sub-standard work, which is sometimes torn at the top of the page/sheet and must be submitted to the boy's housemaster for signature. Boys who accumulate rips are liable to be given a "White Ticket", which must be signed by all his teachers and may be accompanied by other punishments, usually involving doing domestic chores or writing lines. In recent times,[when?] a milder form of the rip, 'sign for information', colloquially known as an "info", has been introduced, which must also be signed by the boy's housemaster and tutor.
A boy who is late for any division or other appointment may be required to sign "Tardy Book", a register kept in the School Office, between 7.35am and 7.45am, every morning for the duration of his sentence (typically three days). Tardy Book may also be issued for late work. For more serious misdeeds, a boy is summoned from his lessons to the Head Master, or Lower Master if the boy is in the lower two years, to talk personally about his misdeeds. This is known as the "Bill". The most serious misdeeds may result in expulsion, or rustication (suspension). Conversely, should a master be more than 15 minutes late for a class, traditionally the pupils might claim it as a "run" and absent themselves for the rest of its duration.
John Keate, Head Master from 1809 to 1834, took over at a time when discipline was poor. Anthony Chenevix-Trench, Head Master from 1964 to 1970, abolished the birch and replaced it with caning, also applied to the bare posterior, which he administered privately in his office. Chenevix-Trench also abolished corporal punishment administered by senior boys. Previously, House Captains were permitted to cane miscreants over the seat of the trousers. This was a routine occurrence, carried out privately with the boy bending over with his head under the edge of a table. Less common but more severe were the canings administered by Pop (see Eton Society below) in the form of a "Pop-Tanning", in which a large number of hard strokes were inflicted by the President of Pop in the presence of all Pop members (or, in earlier times, each member of Pop took it in turns to inflict a stroke). The culprit was summoned to appear in a pair of old trousers, as the caning would cut the cloth to shreds. This was the most severe form of physical punishment at Eton.
The current "Precentor" (Head of Music) is Tim Johnson, and the School boasts eight organs and an entire building for music (performance spaces include the School Hall, the Farrer Theatre and two halls dedicated to music, the Parry Hall and the Concert Hall). Many instruments are taught, including obscure ones such as the didgeridoo. The School participates in many national competitions; many pupils are part of the National Youth Orchestra, and the School gives scholarships for dedicated and talented musicians. A former Precentor of the college, Ralph Allwood set up and organised Eton Choral Courses, which run at the School every summer.
Numerous plays are put on every year at Eton College; there is one main theatre, called the Farrer (seating 400) and 2 Studio theatres, called the Caccia Studio and Empty Space (seating 90 and 80 respectively). There are about 8 or 9 house productions each year, around 3 or 4 "independent" plays (not confined solely to one house, produced, directed and funded by Etonians) and three school plays, one specifically for boys in the first two years, and two open to all years. The School Plays have such good reputations that they are normally fully booked every night. Productions also take place in varying locations around the School, varying from the sports fields to more historic buildings such as Upper School and College Chapel.
In recent years, the School has put on a musical version of The Bacchae (October 2009) as well as productions of A Funny Thing Happened on the Way to the Forum (May 2010), The Cherry Orchard (February 2011), Joseph K (October 2011), Cyrano de Bergerac (May 2012), Macbeth (October 2012), London Assurance (May 2013) and Jerusalem (October 2013). Upcoming in May 2014 was a production of A Midsummer Night's Dream . Often girls from surrounding schools, such as St George's, Ascot, St Mary's School Ascot, Windsor Girls' School and Heathfield St Mary's School, are cast in female roles. Boys from the School are also responsible for the lighting, sound and stage management of all the productions, under the guidance of several professional full-time theatre staff.
Eton's best-known holiday takes place on the so-called "Fourth of June", a celebration of the birthday of King George III, Eton's greatest patron. This day is celebrated with the Procession of Boats, in which the top rowing crews from the top four years row past in vintage wooden rowing boats. Similar to the Queen's Official Birthday, the "Fourth of June" is no longer celebrated on 4 June, but on the Wednesday before the first weekend of June. Eton also observes St. Andrew's Day, on which the Eton wall game is played.[citation needed]
Until 18 December 2010, Eton College was an exempt charity under English law (Charities Act 1993, Schedule 2). Under the provisions of the Charities Act 2006, it is now an excepted charity, and fully registered with the Charities Commission, and is now one of the 100 largest charities in the UK. As a charity, it benefits from substantial tax breaks. It was calculated by the late David Jewell, former Master of Haileybury, that in 1992 such tax breaks saved the School about £1,945 per pupil per year, although he had no direct connection with the School. This subsidy has declined since the 2001 abolition by the Labour Government of state-funded scholarships (formerly known as "assisted places") to independent schools. However, no child attended Eton on this scheme, meaning that the actual level of state assistance to the School has always been lower. Eton's retiring Head Master, Tony Little, has claimed that the benefits that Eton provides to the local community free of charge (use of its facilities, etc.) have a higher value than the tax breaks it receives as a result of its charitable status. The fee for the academic year 2010–2011 was £29,862 (approximately US$48,600 or €35,100 as of March 2011), although the sum is considerably lower for those pupils on bursaries and scholarships.
In 1995 the National Lottery granted money for a £4.6m sports complex, to add to Eton's existing facilities of two swimming pools, 30 cricket squares, 24 football, rugby and hockey pitches and a gym. The College paid £200,000 and contributed 4.5 hectares of land in return for exclusive use of the facilities during the daytime only. The UK Sports Council defended the deal on the grounds that the whole community would benefit, while the bursar claimed that Windsor, Slough and Eton Athletic Club was "deprived" because local people (who were not pupils at the College) did not have a world-class running track and facilities to train with. Steve Osborn, director of the Safe Neighbourhoods Unit, described the decision as "staggering" given the background of a substantial reduction in youth services by councils across the country, a matter over which, however, neither the College nor the UK Sports Council, had any control. The facility, which became the Thames Valley Athletics Centre, opened in April 1999.
In October 2004, Sarah Forsyth claimed that she had been dismissed unfairly by Eton College and had been bullied by senior staff. She also claimed she was instructed to do some of Prince Harry's coursework to enable him to pass AS Art. As evidence, Forsyth provided secretly recorded conversations with both Prince Harry and her Head of Department, Ian Burke. An employment tribunal in July 2005 found that she had been unfairly dismissed and criticised Burke for bullying her and for repeatedly changing his story. It also criticised the school for failing to produce its capability procedures and criticised the Head Master for not reviewing the case independently.
It criticised Forsyth's decision to record a conversation with Harry as an abuse of teacher–student confidentiality and said "It is clear whichever version of the evidence is accepted that Mr Burke did ask the claimant to assist Prince Harry with text for his expressive art project ... It is not part of this tribunal's function to determine whether or not it was legitimate." In response to the tribunal's ruling concerning the allegations about Prince Harry, the School issued a statement, saying Forsyth's claims "were dismissed for what they always have been - unfounded and irrelevant." A spokesperson from Clarence House said, "We are delighted that Harry has been totally cleared of cheating."
In 2005, the Office of Fair Trading found fifty independent schools, including Eton, to have breached the Competition Act by "regularly and systematically" exchanging information about planned increases in school fees, which was collated and distributed among the schools by the bursar at Sevenoaks School. Following the investigation by the OFT, each school was required to pay around £70,000, totalling around £3.5 million, significantly less than the maximum possible fine. In addition, the schools together agreed to contribute another £3m to a new charitable educational fund. The incident raised concerns over whether the charitable status of independent schools such as Eton should be reconsidered, and perhaps revoked. However, Jean Scott, the head of the Independent Schools Council, said that independent schools had always been exempt from anti-cartel rules applied to business, were following a long-established procedure in sharing the information with each other, and that they were unaware of the change to the law (on which they had not been consulted). She wrote to John Vickers, the OFT director-general, saying, "They are not a group of businessmen meeting behind closed doors to fix the price of their products to the disadvantage of the consumer. They are schools that have quite openly continued to follow a long-established practice because they were unaware that the law had changed."
A Freedom of Information request in 2005 revealed that Eton had received £2,652 in farming subsidies in 2004 under the Common Agricultural Policy. Asked to explain under what grounds it was eligible to receive farming subsidies, Eton admitted that it was 'a bit of a mystery'. The TaxPayers' Alliance also stated that Eton had received a total of £5,300 in CAP subsidies between 2002 and 2007. Panorama revealed in March 2012 that farming subsidies were granted to Eton for 'environmental improvements', in effect 'being paid without having to do any farming at all'.
Figures obtained by The Daily Telegraph had revealed that, in 2010, 37 applicants from Eton were accepted by Oxford whilst state schools had difficulty obtaining entry even for pupils with the country's most impressive exam results. According to The Economist, Oxford and Cambridge admit more Etonians each year than applicants from the whole country who qualify for free school meals. In April 2011 the Labour MP David Lammy described as unfair and 'indefensible' the fact that Oxford University had organised nine 'outreach events' at Eton in 2010, although he admitted that it had, in fact, held fewer such events for Eton than for another independent school, Wellington College.
In July 2015, Eton accidentally sent emails to 400 prospective students, offering them conditional entrance to the school in September 2017. The email was intended for nine students, but an IT glitch caused the email to be sent to 400 additional families, who didn't necessarily have a place. In response, the school issued the following statement: "This error was discovered within minutes and each family was immediately contacted to notify them that it should be disregarded and to apologise. We take this type of incident very seriously indeed and so a thorough investigation, overseen by the headmaster Tony Little and led by the tutor for admissions, is being carried out to find out exactly what went wrong and ensure it cannot happen again. Eton College offers its sincere apologies to those boys concerned and their families. We deeply regret the confusion and upset this must have caused."
In January 2016, the Eton College beagling club was accused by the League Against Cruel Sports of undertaking an illegal hare hunt. The allegations were accompanied by a video of the Eton Beagles chasing a hare, as 'the hunt staff urge the beagles on and make no efforts to call the dogs off.' A spokesman representing Eton College released the following statement: "Eton College takes its legal responsibilities extremely seriously and expects all school activities to comply with the law. We are investigating this allegation as a matter of urgency and will be co-operating fully with the relevant authorities." 
Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.
Besides Prince William and Prince Harry, members of the extended British Royal Family who have attended Eton include Prince Richard, Duke of Gloucester and his son Alexander Windsor, Earl of Ulster; Prince Edward, Duke of Kent, his eldest son George Windsor, Earl of St Andrews and grandson Edward Windsor, Lord Downpatrick and his youngest son Lord Nicholas Windsor; Prince Michael of Kent and his son Lord Frederick Windsor; James Ogilvy, son of Princess Alexandra and the Right Honourable Angus Ogilvy, himself an Eton alumnus. Prince William of Gloucester (1942-1972) also attended Eton, as did George Lascelles, 7th Earl of Harewood, son of Princess Mary, Princess Royal.
Other notable Old Etonians include scientists Robert Boyle, John Maynard Smith, J. B. S. Haldane, Stephen Wolfram and the 2012 Nobel Prize in Physiology or Medicine winner, John Gurdon; Beau Brummell; economists John Maynard Keynes and Richard Layard; Antarctic explorer Lawrence Oates; politician Alan Clark; entrepreneur, charity organiser and partner of Adele, Simon Konecki; cricket commentator Henry Blofeld; explorer Sir Ranulph Fiennes; adventurer Bear Grylls; composers Thomas Arne, George Butterworth, Roger Quilter, Frederick Septimus Kelly, Donald Tovey, Thomas Dunhill, Lord Berners, Victor Hely-Hutchinson, and Peter Warlock (Philip Heseltine); Hubert Parry, who wrote the song Jerusalem and the coronation anthem I was glad; and musicians Frank Turner and Humphrey Lyttelton.
Notable Old Etonians in the media include the former Political Editor of both ITN and The Times, Julian Haviland; the current BBC Deputy Political Editor, James Landale, and the BBC Science Editor, David Shukman; the current President of Conde Nast International and Managing Director of Conde Nast UK, Nicholas Coleridge; the former ITN newscaster and BBC Panorama presenter, Ludovic Kennedy; current BBC World News and BBC Rough Justice current affairs presenter David Jessel; former chief ITV and Channel 4 racing commentator John Oaksey; 1950s BBC newsreader and 1960s ITN newscaster Timothy Brinton; 1960s BBC newsreader Corbet Woodall; the former Editor of The Daily Telegraph, Charles Moore; the former Editor of The Spectator, Ferdinand Mount; and the current Editor of The Mail on Sunday, Geordie Greig.
Actor Dominic West has been unenthusiastic about the career benefits of being an Old Etonian, saying it "is a stigma that is slightly above 'paedophile' in the media in a gallery of infamy", but asked whether he would consider sending his own children there, said "Yes, I would. It’s an extraordinary place... It has the facilities and the excellence of teaching and it will find what you’re good at and nurture it", while the actor Tom Hiddleston says there are widespread misconceptions about Eton, and that "People think it's just full of braying toffs... It isn’t true... It's actually one of the most broadminded places I’ve ever been. The reason it’s a good school is that it encourages people to find the thing they love and to go for it. They champion the talent of the individual and that’s what’s special about it".
The Republic of the Congo (French: République du Congo), also known as Congo, Congo Republic, West Congo[citation needed], or Congo-Brazzaville, is a country located in Central Africa. It is bordered by five countries: Gabon to the west; Cameroon to the northwest; the Central African Republic to the northeast; the Democratic Republic of the Congo to the east and south; and the Angolan exclave of Cabinda to the southwest.
The region was dominated by Bantu-speaking tribes, who built trade links leading into the Congo River basin. Congo-Brazzaville was formerly part of the French colony of Equatorial Africa. Upon independence in 1960, the former colony of French Congo became the Republic of the Congo. The People's Republic of the Congo was a Marxist–Leninist one-party state from 1970 to 1991. Multi-party elections have been held since 1992, although a democratically elected government was ousted in the 1997 Republic of the Congo Civil War and President Denis Sassou Nguesso has ruled for 26 of the past 36 years.
The political stability and development of hydrocarbon production made Republic of Congo the fourth largest oil producer in the Gulf of Guinea and provided the country with a relative prosperity despite the poor state of its infrastructure and public services and an unequal distribution of oil revenues.
Bantu-speaking peoples who founded tribes during the Bantu expansions largely displaced and absorbed the earliest inhabitants of the region, the Pygmy people, about 1500 BC. The Bakongo, a Bantu ethnic group that also occupied parts of present-day Angola, Gabon, and Democratic Republic of the Congo, formed the basis for ethnic affinities and rivalries among those countries. Several Bantu kingdoms—notably those of the Kongo, the Loango, and the Teke—built trade links leading into the Congo River basin.
The Portuguese explorer Diogo Cão reached the mouth of the Congo in 1484. Commercial relationships quickly grew between the inland Bantu kingdoms and European merchants who traded various commodities, manufactured goods, and people captured from the hinterlands. After centuries as a major hub for transatlantic trade, direct European colonization of the Congo river delta began in the late 19th century, subsequently eroding the power of the Bantu societies in the region.
The area north of the Congo River came under French sovereignty in 1880 as a result of Pierre de Brazza's treaty with Makoko of the Bateke. This Congo Colony became known first as French Congo, then as Middle Congo in 1903. In 1908, France organized French Equatorial Africa (AEF), comprising Middle Congo, Gabon, Chad, and Oubangui-Chari (the modern Central African Republic). The French designated Brazzaville as the federal capital. Economic development during the first 50 years of colonial rule in Congo centered on natural-resource extraction. The methods were often brutal: construction of the Congo–Ocean Railroad following World War I has been estimated to have cost at least 14,000 lives.
During the Nazi occupation of France during World War II, Brazzaville functioned as the symbolic capital of Free France between 1940 and 1943. The Brazzaville Conference of 1944 heralded a period of major reform in French colonial policy. Congo benefited from the postwar expansion of colonial administrative and infrastructure spending as a result of its central geographic location within AEF and the federal capital at Brazzaville. It also received a local legislature after the adoption of the 1946 constitution that established the Fourth Republic.
Following the revision of the French constitution that established the Fifth Republic in 1958, the AEF dissolved into its constituent parts, each of which became an autonomous colony within the French Community. During these reforms, Middle Congo became known as the Republic of the Congo in 1958 and published its first constitution in 1959. Antagonism between the pro-Opangault Mbochis and the pro-Youlou Balalis resulted in a series of riots in Brazzaville in February 1959, which the French Army subdued.
The Republic of the Congo received full independence from France on August 15, 1960. Fulbert Youlou ruled as the country's first president until labour elements and rival political parties instigated a three-day uprising that ousted him. The Congolese military took charge of the country briefly and installed a civilian provisional government headed by Alphonse Massamba-Débat.
Under the 1963 constitution, Massamba-Débat was elected President for a five-year term. During Massamba-Débat's term in office the regime adopted "scientific socialism" as the country's constitutional ideology. In 1965, Congo established relations with the Soviet Union, the People's Republic of China, North Korea and North Vietnam. Massamba-Débat's regime also invited several hundred Cuban army troops into the country to train his party's militia units and these troops helped his government survive a coup in 1966 led by paratroopers loyal to future President Marien Ngouabi. Nevertheless, Massamba-Débat was unable to reconcile various institutional, tribal and ideological factions within the country and his regime ended abruptly with a bloodless coup d'état in September 1968.
Marien Ngouabi, who had participated in the coup, assumed the presidency on December 31, 1968. One year later, President Ngouabi proclaimed Congo Africa's first "people's republic", the People's Republic of the Congo, and announced the decision of the National Revolutionary Movement to change its name to the Congolese Labour Party (PCT). Ngouabi survived an attempted coup in 1972 but was assassinated on March 16, 1977. An 11-member Military Committee of the Party (CMP) was then named to head an interim government with Joachim Yhombi-Opango to serve as President of the Republic. Two years later, Yhombi-Opango was forced from power and Denis Sassou Nguesso become the new president.
Sassou Nguesso aligned the country with the Eastern Bloc and signed a twenty-year friendship pact with the Soviet Union. Over the years, Sassou had to rely more on political repression and less on patronage to maintain his dictatorship.
Pascal Lissouba, who became Congo's first elected president (1992–1997) during the period of multi-party democracy, attempted to implement economic reforms with IMF backing to liberalise the economy. In June 1996 the IMF approved a three-year SDR69.5m (US$100m) enhanced structural adjustment facility (ESAF) and was on the verge of announcing a renewed annual agreement when civil war broke out in Congo in mid-1997.
Congo's democratic progress was derailed in 1997 when Lissouba and Sassou started to fight for power in the civil war. As presidential elections scheduled for July 1997 approached, tensions between the Lissouba and Sassou camps mounted. On June 5, President Lissouba's government forces surrounded Sassou's compound in Brazzaville and Sassou ordered members of his private militia (known as "Cobras") to resist. Thus began a four-month conflict that destroyed or damaged much of Brazzaville and caused tens of thousands of civilian deaths. In early October, the Angolan socialist régime began an invasion of Congo to install Sassou in power. In mid-October, the Lissouba government fell. Soon thereafter, Sassou declared himself president.
In the controversial elections in 2002, Sassou won with almost 90% of the vote cast. His two main rivals, Lissouba and Bernard Kolelas, were prevented from competing and the only remaining credible rival, Andre Milongo, advised his supporters to boycott the elections and then withdrew from the race. A new constitution, agreed upon by referendum in January 2002, granted the president new powers, extended his term to seven years, and introduced a new bicameral assembly. International observers took issue with the organization of the presidential election and the constitutional referendum, both of which were reminiscent in their organization of Congo's era of the one-party state. Following the presidential elections, fighting restarted in the Pool region between government forces and rebels led by Pastor Ntumi; a peace treaty to end the conflict was signed in April 2003.
Sassou also won the following presidential election in July 2009. According to the Congolese Observatory of Human Rights, a non-governmental organization, the election was marked by "very low" turnout and "fraud and irregularities".
Congo-Brazzaville has had a multi-party political system since the early 1990s, although the system is heavily dominated by President Denis Sassou Nguesso; he has lacked serious competition in the presidential elections held under his rule. Sassou Nguesso is backed by his own Congolese Labour Party (French: Parti Congolais du Travail) as well as a range of smaller parties.
Internationally, Sassou's regime has been hit by corruption revelations despite attempts to censor them. One French investigation found over 110 bank accounts and dozens of lavish properties in France; Sassou denounced embezzlement investigations as "racist" and "colonial".
On March 27, 2015 Sassou Nguesso announced that his government would hold a referendum to change the country's 2002 constitution and allow him to run for a third consecutive term in office. On October 25 the government held a referendum to allow Sassou Nguesso to run in the next election. The government claimed that the proposal as approved by 92 percent of voters with 72 percent of eligible voters participating. The opposition, who had boycotted the referendum claimed that the government's statistics were false and that the vote was a sham.
In 2008, the main media were owned by the government, but many more privately run forms of media were being created. There is one government-owned television station and around 10 small private television channels.
Many Pygmies belong from birth to Bantus in a relationship many refer to as slavery. The Congolese Human Rights Observatory says that the Pygmies are treated as property the same way "pets" are. On December 30, 2010, the Congolese parliament adopted a law for the promotion and protection of the rights of indigenous peoples. This law is the first of its kind in Africa, and its adoption is a historic development for indigenous peoples on the continent.
Congo is located in the central-western part of sub-Saharan Africa, along the Equator, lying between latitudes 4°N and 5°S, and longitudes 11° and 19°E. To the south and east of it is the Democratic Republic of Congo. It is also bounded by Gabon to the west, Cameroon and the Central African Republic to the north, and Cabinda (Angola) to the southwest. It has a short coast on the Atlantic Ocean.
The capital, Brazzaville, is located on the Congo River, in the south of the country, immediately across from Kinshasa, the capital of the Democratic Republic of the Congo.
The southwest of the country is a coastal plain for which the primary drainage is the Kouilou-Niari River; the interior of the country consists of a central plateau between two basins to the south and north. Forests are under increasing exploitation pressure.
Since the country is located on the Equator, the climate is consistent year-round, with the average day temperature being a humid 24 °C (75 °F) and nights generally between 16 °C (61 °F) and 21 °C (70 °F). The average yearly rainfall ranges from 1,100 millimetres (43 in) in south in the Niari Valley to over 2,000 millimetres (79 in) in central parts of the country. The dry season is from June to August while in the majority of the country the wet season has two rainfall maxima: one in March–May and another in September–November.
In 2006–07, researchers from the Wildlife Conservation Society studied gorillas in heavily forested regions centered on the Ouesso district of the Sangha Region. They suggest a population on the order of 125,000 Western Lowland Gorillas, whose isolation from humans has been largely preserved by inhospitable swamps.
The economy is a mixture of village agriculture and handicrafts, an industrial sector based largely on petroleum, support services, and a government characterized by budget problems and overstaffing. Petroleum extraction has supplanted forestry as the mainstay of the economy. In 2008, oil sector accounted for 65% of the GDP, 85% of government revenue, and 92% of exports. The country also has large untapped mineral wealth.
In the early 1980s, rapidly rising oil revenues enabled the government to finance large-scale development projects with GDP growth averaging 5% annually, one of the highest rates in Africa. The government has mortgaged a substantial portion of its petroleum earnings, contributing to a shortage of revenues. January 12, 1994 devaluation of Franc Zone currencies by 50% resulted in inflation of 46% in 1994, but inflation has subsided since.
Economic reform efforts continued with the support of international organizations, notably the World Bank and the International Monetary Fund. The reform program came to a halt in June 1997 when civil war erupted. When Sassou Nguesso returned to power at the end of the war in October 1997, he publicly expressed interest in moving forward on economic reforms and privatization and in renewing cooperation with international financial institutions. However, economic progress was badly hurt by slumping oil prices and the resumption of armed conflict in December 1998, which worsened the republic's budget deficit.
The current administration presides over an uneasy internal peace and faces difficult economic problems of stimulating recovery and reducing poverty, despite record-high oil prices since 2003. Natural gas and diamonds are also recent major Congolese exports, although Congo was excluded from the Kimberley Process in 2004 amid allegations that most of its diamond exports were in fact being smuggled out of the neighboring Democratic Republic of Congo; it was re-admitted to the group in 2007.
The Republic of the Congo also has large untapped base metal, gold, iron and phosphate deposits. The country is a member of the Organization for the Harmonization of Business Law in Africa (OHADA). The Congolese government signed an agreement in 2009 to lease 200,000 hectares of land to South African farmers to reduce its dependence on imports.
Transport in the Republic of the Congo includes land, air and water transportation. The country's rail system was built by forced laborers during the 1930s and largely remains in operation. There are also over 1000 km of paved roads and two major international airports (Maya-Maya Airport and Pointe Noire Airport) which have flights to Paris and many African cities. The country also has a large port on the Atlantic Ocean at Pointe-Noire and others along the Congo River at Brazzaville and Impfondo.
The Republic of the Congo's sparse population is concentrated in the southwestern portion of the country, leaving the vast areas of tropical jungle in the north virtually uninhabited. Thus, Congo is one of the most urbanized countries in Africa, with 70% of its total population living in a few urban areas, namely in Brazzaville, Pointe-Noire or one of the small cities or villages lining the 534-kilometre (332 mi) railway which connects the two cities. In rural areas, industrial and commercial activity has declined rapidly in recent years, leaving rural economies dependent on the government for support and subsistence.
Ethnically and linguistically the population of the Republic of the Congo is diverse—Ethnologue recognises 62 spoken languages in the country—but can be grouped into three categories. The Kongo are the largest ethnic group and form roughly half of the population. The most significant subgroups of the Kongo are Laari in Brazzaville and Pool regions and Vili around Pointe-Noire and along the Atlantic coast. The second largest group are the Teke who live to the north of Brazzaville with 17% of the population. Boulangui (M’Boshi) live in the northwest and in Brazzaville and form 12% of the population. Pygmies make up 2% of Congo's population.
Before the 1997 war, about 9,000 Europeans and other non-Africans lived in Congo, most of whom were French; only a fraction of this number remains. Around 300 American expatriates reside in the Congo.
According to CIA World Factbook, the people of Republic of the Congo are largely a mix of Catholics (33.1%), Awakening Lutherans (22.3%) and other Protestants (19.9%). Followers of Islam make up 1.6%, and this is primarily due to an influx of foreign workers into the urban centers.
Public expenditure health was at 8.9% of the GDP in 2004, whereas private expenditure was at 1.3%. As of 2012, the HIV/AIDS prevalence was at 2.8% among 15- to 49-year-olds. Health expenditure was at US$30 per capita in 2004. A large proportion of the population is undernourished, with malnutrition being a problem in Congo-Brazzaville. There were 20 physicians per 100,000 persons in the early 2000s (decade).
As of 2010, the maternal mortality rate was 560 deaths/100,000 live births, and the infant mortality rate was 59.34 deaths/1,000 live births. Female genital mutilation (FGM) is rare in the country, being confined to limited geographic areas of the country.
Public expenditure of the GDP was less in 2002–05 than in 1991. Public education is theoretically free and mandatory for under-16-year-olds, but in practice, expenses exist. Net primary enrollment rate was 44% in 2005, much less than the 79% in 1991. The country has universities. Education between ages six and sixteen is compulsory. Pupils who complete six years of primary school and seven years of secondary school obtain a baccalaureate. At the university, students can obtain a bachelor's degree in three years and a master's after four. Marien Ngouabi University—which offers courses in medicine, law and several other fields—is the country's only public university. Instruction at all levels is in French, and the educational system as a whole models the French system. The educational infrastructure has been seriously degraded as a result of political and economic crises. There are no seats in most classrooms, forcing children to sit on the floor. Enterprising individuals have set up private schools, but they often lack the technical knowledge and familiarity with the national curriculum to teach effectively. Families frequently enroll their children in private schools only to find they cannot make the payments.
The boroughs of Liverpool, Knowsley, St Helens and Sefton were included in Merseyside. In Greater Manchester the successor boroughs were Bury, Bolton, Manchester, Oldham (part), Rochdale, Salford, Tameside (part), Trafford (part) and Wigan. Warrington and Widnes, south of the new Merseyside/Greater Manchester border were added to the new non-metropolitan county of Cheshire. The urban districts of Barnoldswick and Earby, Bowland Rural District and the parishes of Bracewell and Brogden and Salterforth from Skipton Rural District in the West Riding of Yorkshire became part of the new Lancashire. One parish, Simonswood, was transferred from the borough of Knowsley in Merseyside to the district of West Lancashire in 1994. In 1998 Blackpool and Blackburn with Darwen became independent unitary authorities.
The Duchy of Lancaster is one of two royal duchies in England. It has landholdings throughout the region and elsewhere, operating as a property company, but also exercising the right of the Crown in the County Palatine of Lancaster. While the administrative boundaries changed in the 1970s, the county palatine boundaries remain the same as the historic boundaries. As a result, the High Sheriffs for Lancashire, Greater Manchester and Merseyside are appointed "within the Duchy and County Palatine of Lancaster".
Lancashire has a mostly comprehensive system with four state grammar schools. Not including sixth form colleges, there are 77 state schools (not including Burnley's new schools) and 24 independent schools. The Clitheroe area has secondary modern schools. Sixth form provision is limited at most schools in most districts, with only Fylde and Lancaster districts having mostly sixth forms at schools. The rest depend on FE colleges and sixth form colleges, where they exist. South Ribble has the largest school population and Fylde the smallest (only three schools). Burnley's schools have had a new broom and have essentially been knocked down and started again in 2006. There are many Church of England and Catholic faith schools in Lancashire.
Lancashire produced well known teams in super league such as St Helens, Wigan, and Warrington. The county was once the focal point for many of the sport's professional competitions including the Lancashire League competition which ran from 1895 to 1970, and the Lancashire County Cup which was abandoned in 1993. Rugby League has also seen a representative fixture between Lancashire and Yorkshire contested 89 times since its inception in 1895. Currently there are several rugby league teams that are based within the ceremonial county which include Blackpool Panthers, East Lancashire Lions, Blackpool Sea Eagles, Bamber Bridge, Leyland Warriors, Chorley Panthers, Blackpool Stanley, Blackpool Scorpions and Adlington Rangers.
Lancashire had a lively culture of choral and classical music, with very large numbers of local church choirs from the 17th century, leading to the foundation of local choral societies from the mid-18th century, often particularly focused on performances of the music of Handel and his contemporaries. It also played a major part in the development of brass bands which emerged in the county, particularly in the textile and coalfield areas, in the 19th century. The first open competition for brass bands was held at Manchester in 1853, and continued annually until the 1980s. The vibrant brass band culture of the area made an important contribution to the foundation and staffing of the Hallé Orchestra from 1857, the oldest extant professional orchestra in the United Kingdom. The same local musical tradition produced eminent figures such as Sir William Walton (1902–88), son of an Oldham choirmaster and music teacher, Sir Thomas Beecham (1879–1961), born in St. Helens, who began his career by conducting local orchestras and Alan Rawsthorne (1905–71) born in Haslingden. The conductor David Atherton, co-founder of the London Sinfonietta, was born in Blackpool in 1944. Lancashire also produced more populist figures, such as early musical theatre composer Leslie Stuart (1863–1928), born in Southport, who began his musical career as organist of Salford Cathedral.
Lancashire emerged as a major commercial and industrial region during the Industrial Revolution. Manchester and Liverpool grew into its largest cities, dominating global trade and the birth of modern capitalism. The county contained several mill towns and the collieries of the Lancashire Coalfield. By the 1830s, approximately 85% of all cotton manufactured worldwide was processed in Lancashire. Accrington, Blackburn, Bolton, Burnley, Bury, Chorley, Colne, Darwen, Nelson, Oldham, Preston, Rochdale and Wigan were major cotton mill towns during this time. Blackpool was a centre for tourism for the inhabitants of Lancashire's mill towns, particularly during wakes week.
The county was subject to significant boundary reform in 1974 that removed Liverpool and Manchester and most of their surrounding conurbations to form the metropolitan counties of Merseyside and Greater Manchester. The detached northern part of Lancashire in the Lake District, including the Furness Peninsula and Cartmel, was merged with Cumberland and Westmorland to form Cumbria. Lancashire lost 709 square miles of land to other counties, about two fifths of its original area, although it did gain some land from the West Riding of Yorkshire. Today the county borders Cumbria to the north, Greater Manchester and Merseyside to the south and North and West Yorkshire to the east; with a coastline on the Irish Sea to the west. The county palatine boundaries remain the same[clarification needed] with the Duke of Lancaster exercising sovereignty rights, including the appointment of lords lieutenant in Greater Manchester and Merseyside.
Lancashire is smaller than its historical extent following a major reform of local government. In 1889, the administrative county of Lancashire was created, covering the historical county except for the county boroughs such as Blackburn, Burnley, Barrow-in-Furness, Preston, Wigan, Liverpool and Manchester. The area served by the Lord-Lieutenant (termed now a ceremonial county) covered the entirety of the administrative county and the county boroughs, and was expanded whenever boroughs annexed areas in neighbouring counties such as Wythenshawe in Manchester south of the River Mersey and historically in Cheshire, and southern Warrington. It did not cover the western part of Todmorden, where the ancient border between Lancashire and Yorkshire passes through the middle of the town.
During the 20th century, the county became increasingly urbanised, particularly the southern part. To the existing county boroughs of Barrow-in-Furness, Blackburn, Bolton, Bootle, Burnley, Bury, Liverpool, Manchester, Oldham, Preston, Rochdale, Salford, St Helens and Wigan were added Blackpool (1904), Southport (1905), and Warrington (1900). The county boroughs also had many boundary extensions. The borders around the Manchester area were particularly complicated, with narrow protrusions of the administrative county between the county boroughs – Lees urban district formed a detached part of the administrative county, between Oldham county borough and the West Riding of Yorkshire.
To the east of the county are upland areas leading to the Pennines. North of the Ribble is Beacon Fell Country Park and the Forest of Bowland, another AONB. Much of the lowland in this area is devoted to dairy farming and cheesemaking, whereas the higher ground is more suitable for sheep, and the highest ground is uncultivated moorland. The valleys of the River Ribble and its tributary the Calder form a large gap to the west of the Pennines, overlooked by Pendle Hill. Most of the larger Lancashire towns are in these valleys South of the Ribble are the West Pennine Moors and the Forest of Rossendale where former cotton mill towns are in deep valleys. The Lancashire Coalfield, largely in modern-day Greater Manchester, extended into Merseyside and to Ormskirk, Chorley, Burnley and Colne in Lancashire.
The Duchy administers bona vacantia within the County Palatine, receiving the property of persons who die intestate and where the legal ownership cannot be ascertained. There is no separate Duke of Lancaster, the title merged into the Crown many centuries ago – but the Duchy is administered by the Queen in Right of the Duchy of Lancaster. A separate court system for the county palatine was abolished by Courts Act 1971. A particular form of The Loyal Toast, 'The Queen, Duke of Lancaster' is in regular use in the county palatine. Lancaster serves as the county town of the county palatine.
The Lancashire economy relies strongly on the M6 motorway which runs from north to south, past Lancaster and Preston. The M55 connects Preston to Blackpool and is 11.5 miles (18.3 km) long. The M65 motorway from Colne, connects Burnley, Accrington, Blackburn to Preston. The M61 from Preston via Chorley and the M66 starting 500 metres (0.3 mi) inside the county boundary near Edenfield, provide links between Lancashire and Manchester] and the trans-Pennine M62. The M58 crosses the southernmost part of the county from the M6 near Wigan to Liverpool via Skelmersdale.
The major settlements in the ceremonial county are concentrated on the Fylde coast (the Blackpool Urban Area), and a belt of towns running west-east along the M65: Preston, Blackburn, Accrington, Burnley, Nelson and Colne. South of Preston are the towns of Leyland and Chorley; the three formed part of the Central Lancashire New Town designated in 1970. The north of the county is predominantly rural and sparsely populated, except for the towns of Lancaster and Morecambe which form a large conurbation of almost 100,000 people. Lancashire is home to a significant Asian population, numbering over 70,000 and 6% of the county's population, and concentrated largely in the former cotton mill towns in the south east.
Liverpool produced a number of nationally and internationally successful popular singers in the 1950s, including traditional pop stars Frankie Vaughan and Lita Roza, and one of the most successful British rock and roll stars in Billy Fury. Many Lancashire towns had vibrant skiffle scenes in the late 1950s, out of which by the early 1960s a flourishing culture of beat groups began to emerge, particularly around Liverpool and Manchester. It has been estimated that there were around 350 bands active in and around Liverpool in this era, often playing ballrooms, concert halls and clubs, among them the Beatles. After their national success from 1962, a number of Liverpool performers were able to follow them into the charts, including Gerry & the Pacemakers, the Searchers and Cilla Black. The first act to break through in the UK who were not from Liverpool, or managed by Brian Epstein, were Freddie and the Dreamers, who were based in Manchester, as were Herman's Hermits and the Hollies. Led by the Beatles, beat groups from the region spearheaded the British Invasion of the US, which made a major contribution to the development of rock music. After the decline of beat groups in the late 1960s the centre of rock culture shifted to London and there were relatively few local bands who achieved national prominence until the growth of a disco funk scene and the punk rock revolution in the mid and late 1970s.
Lancashire has a long and highly productive tradition of music making. In the early modern era the county shared in the national tradition of balladry, including perhaps the finest border ballad, "The Ballad of Chevy Chase", thought to have been composed by the Lancashire-born minstrel Richard Sheale. The county was also a common location for folk songs, including "The Lancashire Miller", "Warrington Ale" and "The soldier's farewell to Manchester", while Liverpool, as a major seaport, was the subject of many sea shanties, including "The Leaving of Liverpool" and "Maggie May", beside several local Wassailing songs. In the Industrial Revolution changing social and economic patterns helped create new traditions and styles of folk song, often linked to migration and patterns of work. These included processional dances, often associated with rushbearing or the Wakes Week festivities, and types of step dance, most famously clog dancing.
The county was established in 1182, later than many other counties. During Roman times the area was part of the Brigantes tribal area in the military zone of Roman Britain. The towns of Manchester, Lancaster, Ribchester, Burrow, Elslack and Castleshaw grew around Roman forts. In the centuries after the Roman withdrawal in 410AD the northern parts of the county probably formed part of the Brythonic kingdom of Rheged, a successor entity to the Brigantes tribe. During the mid-8th century, the area was incorporated into the Anglo-Saxon Kingdom of Northumbria, which became a part of England in the 10th century.
By the census of 1971, the population of Lancashire and its county boroughs had reached 5,129,416, making it the most populous geographic county in the UK. The administrative county was also the most populous of its type outside London, with a population of 2,280,359 in 1961. On 1 April 1974, under the Local Government Act 1972, the administrative county was abolished, as were the county boroughs. The urbanised southern part largely became part of two metropolitan counties, Merseyside and Greater Manchester. The new county of Cumbria incorporates the Furness exclave.
A local pioneer of folk song collection in the first half of the 19th century was Shakespearean scholar James Orchard Halliwell, but it was not until the second folk revival in the 20th century that the full range of song from the county, including industrial folk song, began to gain attention. The county produced one of the major figures of the revival in Ewan MacColl, but also a local champion in Harry Boardman, who from 1965 onwards probably did more than anyone to popularise and record the folk song of the county. Perhaps the most influential folk artists to emerge from the region in the late 20th century were Liverpool folk group The Spinners, and from Manchester folk troubadour Roy Harper and musician, comedian and broadcaster Mike Harding. The region is home to numerous folk clubs, many of them catering to Irish and Scottish folk music. Regular folk festivals include the Fylde Folk Festival at Fleetwood.
The Red Rose of Lancaster is the county flower found on the county's heraldic badge and flag. The rose was a symbol of the House of Lancaster, immortalised in the verse "In the battle for England's head/York was white, Lancaster red" (referring to the 15th-century Wars of the Roses). The traditional Lancashire flag, a red rose on a white field, was not officially registered. When an attempt was made to register it with the Flag Institute it was found that it was officially registered by Montrose in Scotland, several hundred years earlier with the Lyon Office. Lancashire's official flag is registered as a red rose on a gold field.
More recent Lancashire-born composers include Hugh Wood (1932- Parbold), Sir Peter Maxwell Davies (1934-, Salford), Sir Harrison Birtwistle (1934-, Accrington), Gordon Crosse (1937-, Bury),John McCabe (1939-2015, Huyton), Roger Smalley (1943-2015, Swinton), Nigel Osborne (1948-, Manchester), Steve Martland (1954-2013, Liverpool), Simon Holt (1958-, Bolton) and Philip Cashian (1963-, Manchester). The Royal Manchester College of Music was founded in 1893 to provide a northern counterpart to the London musical colleges. It merged with the Northern College of Music (formed in 1920) to form the Royal Northern College of Music in 1972.
Computer security, also known as cybersecurity or IT security, is the protection of information systems from theft or damage to the hardware, the software, and to the information on them, as well as from disruption or misdirection of the services they provide. It includes controlling physical access to the hardware, as well as protecting against harm that may come via network access, data and code injection, and due to malpractice by operators, whether intentional, accidental, or due to them being tricked into deviating from secure procedures.
Denial of service attacks are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of Distributed denial of service (DDoS) attacks are possible, where the attack comes from a large number of points – and defending is much more difficult. Such attacks can originate from the zombie computers of a botnet, but a range of other techniques are possible including reflection and amplification attacks, where innocent systems are fooled into sending traffic to the victim.
If access is gained to a car's internal controller area network, it is possible to disable the brakes and turn the steering wheel. Computerized engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver assistance systems make these disruptions possible, and self-driving cars go even further. Connected cars may use wifi and bluetooth to communicate with onboard consumer devices, and the cell phone network to contact concierge and emergency assistance services or get navigational or entertainment information; each of these networks is a potential entry point for malware or an attacker. Researchers in 2011 were even able to use a malicious compact disc in a car's stereo system as a successful attack vector, and cars with built-in voice recognition or remote assistance features have onboard microphones which could be used for eavesdropping.
However, relatively few organisations maintain computer systems with effective detection systems, and fewer still have organised response mechanisms in place. As result, as Reuters points out: "Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets". The primary obstacle to effective eradication of cyber crime could be traced to excessive reliance on firewalls and other automated "detection" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.
One use of the term "computer security" refers to technology that is used to implement secure operating systems. In the 1980s the United States Department of Defense (DoD) used the "Orange Book" standards, but the current international standard ISO/IEC 15408, "Common Criteria" defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being "Methodically Designed, Tested and Reviewed", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 ("Semiformally Verified Design and Tested") system is Integrity-178B, which is used in the Airbus A380 and several military jets.
China's network security and information technology leadership team was established February 27, 2014. The leadership team is tasked with national security and long-term development and co-ordination of major issues related to network security and information technology. Economic, political, cultural, social and military fields as related to network security and information technology strategy, planning and major macroeconomic policy are being researched. The promotion of national network security and information technology law are constantly under study for enhanced national security capabilities.
Eavesdropping is the act of surreptitiously listening to a private conversation, typically between hosts on a network. For instance, programs such as Carnivore and NarusInsight have been used by the FBI and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon via monitoring the faint electro-magnetic transmissions generated by the hardware; TEMPEST is a specification by the NSA referring to these attacks.
Desktop computers and laptops are commonly infected with malware either to gather passwords or financial account information, or to construct a botnet to attack another target. Smart phones, tablet computers, smart watches, and other mobile devices such as Quantified Self devices like activity trackers have also become targets and many of these have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. Wifi, Bluetooth, and cell phone network on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.
Within computer systems, two of many security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.[citation needed]
In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.
In July of 2015, a hacker group known as "The Impact Team" successfully breached the extramarital relationship website Ashley Madison. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. With this initial data release, the group stated “Avid Life Media has been instructed to take Ashley Madison and Established Men offline permanently in all forms, or we will release all customer records, including profiles with all the customers' secret sexual fantasies and matching credit card transactions, real names and addresses, and employee documents and emails. The other websites may stay online.”  When Avid Life Media, the parent company that created the Ashley Madison website, did not take the site offline, The Impact Group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned, but the website remained functional.
The question of whether the government should intervene or not in the regulation of the cyberspace is a very polemical one. Indeed, for as long as it has existed and by definition, the cyberspace is a virtual space free of any government intervention. Where everyone agree that an improvement on cybersecurity is more than vital, is the government the best actor to solve this issue? Many government officials and experts think that the government should step in and that there is a crucial need for regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the "industry only responds when you threaten regulation. If industry doesn't respond (to the threat), you have to follow through." On the other hand, executives from the private sector agree that improvements are necessary, but think that the government intervention would affect their ability to innovate efficiently.
On October 3, 2010, Public Safety Canada unveiled Canada’s Cyber Security Strategy, following a Speech from the Throne commitment to boost the security of Canadian cyberspace. The aim of the strategy is to strengthen Canada’s "cyber systems and critical infrastructure sectors, support economic growth and protect Canadians as they connect to each other and to the world." Three main pillars define the strategy: securing government systems, partnering to secure vital cyber systems outside the federal government, and helping Canadians to be secure online. The strategy involves multiple departments and agencies across the Government of Canada. The Cyber Incident Management Framework for Canada outlines these responsibilities, and provides a plan for coordinated response between government and other partners in the event of a cyber incident. The Action Plan 2010–2015 for Canada's Cyber Security Strategy outlines the ongoing implementation of the strategy.
Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable to physical damage caused by malicious commands sent to industrial equipment (in that case uranium enrichment centrifuges) which are infected via removable media. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.
Today, computer security comprises mainly "preventive" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real time filtering and blocking. Another implementation is a so-called physical firewall which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.
Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. "Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal."
While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.
Public Safety Canada’s Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada’s critical infrastructure and cyber systems. The CCIRC provides support to mitigate cyber threats, technical support to respond and recover from targeted cyber attacks, and provides online tools for members of Canada’s critical infrastructure sectors. The CCIRC posts regular cyber security bulletins on the Public Safety Canada website. The CCIRC also operates an online reporting tool where individuals and organizations can report a cyber incident. Canada's Cyber Security Strategy is part of a larger, integrated approach to critical infrastructure protection, and functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure.
This has led to new terms such as cyberwarfare and cyberterrorism. More and more critical infrastructure is being controlled via computer programs that, while increasing efficiency, exposes new vulnerabilities. The test will be to see if governments and corporations that control critical systems such as energy, communications and other information will be able to prevent attacks before they occur. As Jay Cross, the chief scientist of the Internet Time Group, remarked, "Connectedness begets vulnerability."
On September 27, 2010, Public Safety Canada partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations dedicated to informing the general public on how to protect themselves online. On February 4, 2014, the Government of Canada launched the Cyber Security Cooperation Program. The program is a $1.5 million five-year initiative aimed at improving Canada’s cyber systems through grants and contributions to projects in support of this objective. Public Safety Canada aims to begin an evaluation of Canada's Cyber Security Strategy in early 2015. Public Safety Canada administers and routinely updates the GetCyberSafe portal for Canadian citizens, and carries out Cyber Security Awareness Month during October.
An unauthorized user gaining physical access to a computer is most likely able to directly download data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, or covert listening devices. Even when the system is protected by standard security measures, these may be able to be by passed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.
Clickjacking, also known as "UI redress attack or User Interface redress attack", is a malicious technique in which an attacker tricks a user into clicking on a button or link on another webpage while the user intended to click on the top level page. This is done using multiple transparent or opaque layers. The attacker is basically "hijacking" the clicks meant for the top level page and routing them to some other irrelevant page, most likely owned by someone else. A similar technique can be used to hijack keystrokes. Carefully drafting a combination of stylesheets, iframes, buttons and text boxes, a user can be led into believing that they are typing the password or other information on some authentic webpage while it is being channeled into an invisible frame controlled by the attacker.
In 1988, only 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On November 2, 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet "computer worm". The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris, Jr. who said 'he wanted to count how many machines were connected to the Internet'.
In 2013 and 2014, a Russian/Ukrainian hacking ring known as "Rescator" broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. "The malware utilized is absolutely unsophisticated and uninteresting," says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.
Berlin starts National Cyber Defense Initiative: On June 16, 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt für Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt für den Militärischen Abschirmdienst and other national organisations in Germany taking care of national security aspects. According to the Minister the primary task of the new organisation founded on February 23, 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet.
Popper is known for his rejection of the classical inductivist views on the scientific method, in favour of empirical falsification: A theory in the empirical sciences can never be proven, but it can be falsified, meaning that it can and should be scrutinized by decisive experiments. He used the black swan fallacy to discuss falsification. If the outcome of an experiment contradicts the theory, one should refrain from ad hoc manoeuvres that evade the contradiction merely by making it less falsifiable. Popper is also known for his opposition to the classical justificationist account of knowledge, which he replaced with critical rationalism, "the first non-justificational philosophy of criticism in the history of philosophy."
Karl Popper was born in Vienna (then in Austria-Hungary) in 1902, to upper middle-class parents. All of Karl Popper's grandparents were Jewish, but the Popper family converted to Lutheranism before Karl was born, and so he received Lutheran baptism. They understood this as part of their cultural assimilation, not as an expression of devout belief. Karl's father Simon Siegmund Carl Popper was a lawyer from Bohemia and a doctor of law at the Vienna University, and mother Jenny Schiff was of Silesian and Hungarian descent. After establishing themselves in Vienna, the Poppers made a rapid social climb in Viennese society: Simon Siegmund Carl became a partner in the law firm of Vienna's liberal Burgomaster Herr Grübl and, after Grübl's death in 1898, Simon took over the business. (Malachi Hacohen records that Herr Grübl's first name was Raimund, after which Karl received his middle name. Popper himself, in his autobiography, erroneously recalls that Herr Grübl's first name was Carl.) His father was a bibliophile who had 12,000–14,000 volumes in his personal library. Popper inherited both the library and the disposition from him.
Popper left school at the age of 16 and attended lectures in mathematics, physics, philosophy, psychology and the history of music as a guest student at the University of Vienna. In 1919, Popper became attracted by Marxism and subsequently joined the Association of Socialist School Students. He also became a member of the Social Democratic Workers' Party of Austria, which was at that time a party that fully adopted the Marxist ideology. After the street battle in the Hörlgasse on 15 June 1919, when police shot eight of his unarmed party comrades, he became disillusioned by what he saw to be the "pseudo-scientific" historical materialism of Marx, abandoned the ideology, and remained a supporter of social liberalism throughout his life.
He worked in street construction for a short amount of time, but was unable to cope with the heavy labour. Continuing to attend university as a guest student, he started an apprenticeship as cabinetmaker, which he completed as a journeyman. He was dreaming at that time of starting a daycare facility for children, for which he assumed the ability to make furniture might be useful. After that he did voluntary service in one of psychoanalyst Alfred Adler's clinics for children. In 1922, he did his matura by way of a second chance education and finally joined the University as an ordinary student. He completed his examination as an elementary teacher in 1924 and started working at an after-school care club for socially endangered children. In 1925, he went to the newly founded Pädagogisches Institut and continued studying philosophy and psychology. Around that time he started courting Josefine Anna Henninger, who later became his wife.
In 1928, he earned a doctorate in psychology, under the supervision of Karl Bühler. His dissertation was entitled "Die Methodenfrage der Denkpsychologie" (The question of method in cognitive psychology). In 1929, he obtained the authorisation to teach mathematics and physics in secondary school, which he started doing. He married his colleague Josefine Anna Henninger (1906–1985) in 1930. Fearing the rise of Nazism and the threat of the Anschluss, he started to use the evenings and the nights to write his first book Die beiden Grundprobleme der Erkenntnistheorie (The Two Fundamental Problems of the Theory of Knowledge). He needed to publish one to get some academic position in a country that was safe for people of Jewish descent. However, he ended up not publishing the two-volume work, but a condensed version of it with some new material, Logik der Forschung (The Logic of Scientific Discovery), in 1934. Here, he criticised psychologism, naturalism, inductionism, and logical positivism, and put forth his theory of potential falsifiability as the criterion demarcating science from non-science. In 1935 and 1936, he took unpaid leave to go to the United Kingdom for a study visit.
In 1937, Popper finally managed to get a position that allowed him to emigrate to New Zealand, where he became lecturer in philosophy at Canterbury University College of the University of New Zealand in Christchurch. It was here that he wrote his influential work The Open Society and its Enemies. In Dunedin he met the Professor of Physiology John Carew Eccles and formed a lifelong friendship with him. In 1946, after the Second World War, he moved to the United Kingdom to become reader in logic and scientific method at the London School of Economics. Three years later, in 1949, he was appointed professor of logic and scientific method at the University of London. Popper was president of the Aristotelian Society from 1958 to 1959. He retired from academic life in 1969, though he remained intellectually active for the rest of his life. In 1985, he returned to Austria so that his wife could have her relatives around her during the last months of her life; she died in November that year. After the Ludwig Boltzmann Gesellschaft failed to establish him as the director of a newly founded branch researching the philosophy of science, he went back again to the United Kingdom in 1986, settling in Kenley, Surrey.
Popper died of "complications of cancer, pneumonia and kidney failure" in Kenley at the age of 92 on 17 September 1994. He had been working continuously on his philosophy until two weeks before, when he suddenly fell terminally ill. After cremation, his ashes were taken to Vienna and buried at Lainzer cemetery adjacent to the ORF Centre, where his wife Josefine Anna Popper (called ‘Hennie’) had already been buried. Popper's estate is managed by his secretary and personal assistant Melitta Mew and her husband Raymond. Popper's manuscripts went to the Hoover Institution at Stanford University, partly during his lifetime and partly as supplementary material after his death. Klagenfurt University possesses Popper's library, including his precious bibliophilia, as well as hard copies of the original Hoover material and microfilms of the supplementary material. The remaining parts of the estate were mostly transferred to The Karl Popper Charitable Trust. In October 2008 Klagenfurt University acquired the copyrights from the estate.
Popper won many awards and honours in his field, including the Lippincott Award of the American Political Science Association, the Sonning Prize, the Otto Hahn Peace Medal of the United Nations Association of Germany in Berlin and fellowships in the Royal Society, British Academy, London School of Economics, King's College London, Darwin College, Cambridge, and Charles University, Prague. Austria awarded him the Grand Decoration of Honour in Gold for Services to the Republic of Austria in 1986, and the Federal Republic of Germany its Grand Cross with Star and Sash of the Order of Merit, and the peace class of the Order Pour le Mérite. He received the Humanist Laureate Award from the International Academy of Humanism. He was knighted by Queen Elizabeth II in 1965, and was elected a Fellow of the Royal Society in 1976. He was invested with the Insignia of a Companion of Honour in 1982.
Other awards and recognition for Popper included the City of Vienna Prize for the Humanities (1965), Karl Renner Prize (1978), Austrian Decoration for Science and Art (1980), Dr. Leopold Lucas Prize (1981), Ring of Honour of the City of Vienna (1983) and the Premio Internazionale of the Italian Federico Nietzsche Society (1988). In 1992, he was awarded the Kyoto Prize in Arts and Philosophy for "symbolising the open spirit of the 20th century" and for his "enormous influence on the formation of the modern intellectual climate".
Karl Popper's rejection of Marxism during his teenage years left a profound mark on his thought. He had at one point joined a socialist association, and for a few months in 1919 considered himself a communist. During this time he became familiar with the Marxist view of economics, class-war, and history. Although he quickly became disillusioned with the views expounded by Marxism, his flirtation with the ideology led him to distance himself from those who believed that spilling blood for the sake of a revolution was necessary. He came to realise that when it came to sacrificing human lives, one was to think and act with extreme prudence.
The failure of democratic parties to prevent fascism from taking over Austrian politics in the 1920s and 1930s traumatised Popper. He suffered from the direct consequences of this failure, since events after the Anschluss, the annexation of Austria by the German Reich in 1938, forced him into permanent exile. His most important works in the field of social science—The Poverty of Historicism (1944) and The Open Society and Its Enemies (1945)—were inspired by his reflection on the events of his time and represented, in a sense, a reaction to the prevalent totalitarian ideologies that then dominated Central European politics. His books defended democratic liberalism as a social and political philosophy. They also represented extensive critiques of the philosophical presuppositions underpinning all forms of totalitarianism.
Popper puzzled over the stark contrast between the non-scientific character of Freud and Adler's theories in the field of psychology and the revolution set off by Einstein's theory of relativity in physics in the early 20th century. Popper thought that Einstein's theory, as a theory properly grounded in scientific thought and method, was highly "risky", in the sense that it was possible to deduce consequences from it which were, in the light of the then-dominant Newtonian physics, highly improbable (e.g., that light is deflected towards solid bodies—confirmed by Eddington's experiments in 1919), and which would, if they turned out to be false, falsify the whole theory. In contrast, nothing could, even in principle, falsify psychoanalytic theories. He thus came to the conclusion that psychoanalytic theories had more in common with primitive myths than with genuine science.
This led Popper to conclude that what were regarded[by whom?] as the remarkable strengths of psychoanalytical theories were actually their weaknesses. Psychoanalytical theories were crafted in a way that made them able to refute any criticism and to give an explanation for every possible form of human behaviour. The nature of such theories made it impossible for any criticism or experiment - even in principle - to show them to be false. This realisation had an important consequence when Popper later tackled the problem of demarcation in the philosophy of science, as it led him to posit that the strength of a scientific theory lies in its both being susceptible to falsification, and not actually being falsified by criticism made of it. He considered that if a theory cannot, in principle, be falsified by criticism, it is not a scientific theory.
Popper coined the term "critical rationalism" to describe his philosophy. Concerning the method of science, the term indicates his rejection of classical empiricism, and the classical observationalist-inductivist account of science that had grown out of it. Popper argued strongly against the latter, holding that scientific theories are abstract in nature, and can be tested only indirectly, by reference to their implications. He also held that scientific theory, and human knowledge generally, is irreducibly conjectural or hypothetical, and is generated by the creative imagination to solve problems that have arisen in specific historico-cultural settings.
Logically, no number of positive outcomes at the level of experimental testing can confirm a scientific theory, but a single counterexample is logically decisive: it shows the theory, from which the implication is derived, to be false. To say that a given statement (e.g., the statement of a law of some scientific theory) -- [call it "T"] -- is "falsifiable" does not mean that "T" is false. Rather, it means that, if "T" is false, then (in principle), "T" could be shown to be false, by observation or by experiment. Popper's account of the logical asymmetry between verification and falsifiability lies at the heart of his philosophy of science. It also inspired him to take falsifiability as his criterion of demarcation between what is, and is not, genuinely scientific: a theory should be considered scientific if, and only if, it is falsifiable. This led him to attack the claims of both psychoanalysis and contemporary Marxism to scientific status, on the basis that their theories are not falsifiable.
In All Life is Problem Solving, Popper sought to explain the apparent progress of scientific knowledge – that is, how it is that our understanding of the universe seems to improve over time. This problem arises from his position that the truth content of our theories, even the best of them, cannot be verified by scientific testing, but can only be falsified. Again, in this context the word "falsified" does not refer to something being "fake"; rather, that something can be (i.e., is capable of being) shown to be false by observation or experiment. Some things simply do not lend themselves to being shown to be false, and therefore, are not falsifiable. If so, then how is it that the growth of science appears to result in a growth in knowledge? In Popper's view, the advance of scientific knowledge is an evolutionary process characterised by his formula:
In response to a given problem situation (), a number of competing conjectures, or tentative theories (), are systematically subjected to the most rigorous attempts at falsification possible. This process, error elimination (), performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more "fit"—in other words, more applicable to the problem situation at hand (). Consequently, just as a species' biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may, in Popper's view, reflect a certain type of progress: toward more and more interesting problems (). For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances toward greater and greater problems; in a process very much akin to the interplay between genetic variation and natural selection.
Among his contributions to philosophy is his claim to have solved the philosophical problem of induction. He states that while there is no way to prove that the sun will rise, it is possible to formulate the theory that every day the sun will rise; if it does not rise on some particular day, the theory will be falsified and will have to be replaced by a different one. Until that day, there is no need to reject the assumption that the theory is true. Nor is it rational according to Popper to make instead the more complex assumption that the sun will rise until a given day, but will stop doing so the day after, or similar statements with additional conditions.
Popper held that rationality is not restricted to the realm of empirical or scientific theories, but that it is merely a special case of the general method of criticism, the method of finding and eliminating contradictions in knowledge without ad-hoc-measures. According to this view, rational discussion about metaphysical ideas, about moral values and even about purposes is possible. Popper's student W.W. Bartley III tried to radicalise this idea and made the controversial claim that not only can criticism go beyond empirical knowledge, but that everything can be rationally criticised.
To Popper, who was an anti-justificationist, traditional philosophy is misled by the false principle of sufficient reason. He thinks that no assumption can ever be or needs ever to be justified, so a lack of justification is not a justification for doubt. Instead, theories should be tested and scrutinised. It is not the goal to bless theories with claims of certainty or justification, but to eliminate errors in them. He writes, "there are no such things as good positive reasons; nor do we need such things [...] But [philosophers] obviously cannot quite bring [themselves] to believe that this is my opinion, let alone that it is right" (The Philosophy of Karl Popper, p. 1043)
In The Open Society and Its Enemies and The Poverty of Historicism, Popper developed a critique of historicism and a defence of the "Open Society". Popper considered historicism to be the theory that history develops inexorably and necessarily according to knowable general laws towards a determinate end. He argued that this view is the principal theoretical presupposition underpinning most forms of authoritarianism and totalitarianism. He argued that historicism is founded upon mistaken assumptions regarding the nature of scientific law and prediction. Since the growth of human knowledge is a causal factor in the evolution of human history, and since "no society can predict, scientifically, its own future states of knowledge", it follows, he argued, that there can be no predictive science of human history. For Popper, metaphysical and historical indeterminism go hand in hand.
As early as 1934, Popper wrote of the search for truth as "one of the strongest motives for scientific discovery." Still, he describes in Objective Knowledge (1972) early concerns about the much-criticised notion of truth as correspondence. Then came the semantic theory of truth formulated by the logician Alfred Tarski and published in 1933. Popper writes of learning in 1935 of the consequences of Tarski's theory, to his intense joy. The theory met critical objections to truth as correspondence and thereby rehabilitated it. The theory also seemed, in Popper's eyes, to support metaphysical realism and the regulative idea of a search for truth.
According to this theory, the conditions for the truth of a sentence as well as the sentences themselves are part of a metalanguage. So, for example, the sentence "Snow is white" is true if and only if snow is white. Although many philosophers have interpreted, and continue to interpret, Tarski's theory as a deflationary theory, Popper refers to it as a theory in which "is true" is replaced with "corresponds to the facts". He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. He identifies Tarski's formulation of the truth conditions of sentences as the introduction of a "metalinguistic predicate" and distinguishes the following cases:
Upon this basis, along with that of the logical content of assertions (where logical content is inversely proportional to probability), Popper went on to develop his important notion of verisimilitude or "truthlikeness". The intuitive idea behind verisimilitude is that the assertions or hypotheses of scientific theories can be objectively measured with respect to the amount of truth and falsity that they imply. And, in this way, one theory can be evaluated as more or less true than another on a quantitative basis which, Popper emphasises forcefully, has nothing to do with "subjective probabilities" or other merely "epistemic" considerations.
Knowledge, for Popper, was objective, both in the sense that it is objectively true (or truthlike), and also in the sense that knowledge has an ontological status (i.e., knowledge as object) independent of the knowing subject (Objective Knowledge: An Evolutionary Approach, 1972). He proposed three worlds: World One, being the physical world, or physical states; World Two, being the world of mind, or mental states, ideas, and perceptions; and World Three, being the body of human knowledge expressed in its manifold forms, or the products of the second world made manifest in the materials of the first world (i.e., books, papers, paintings, symphonies, and all the products of the human mind). World Three, he argued, was the product of individual human beings in exactly the same sense that an animal path is the product of individual animals, and that, as such, has an existence and evolution independent of any individual knowing subjects. The influence of World Three, in his view, on the individual human mind (World Two) is at least as strong as the influence of World One. In other words, the knowledge held by a given individual mind owes at least as much to the total accumulated wealth of human knowledge, made manifest, as to the world of direct experience. As such, the growth of human knowledge could be said to be a function of the independent evolution of World Three. Many contemporary philosophers, such as Daniel Dennett, have not embraced Popper's Three World conjecture, due mostly, it seems, to its resemblance to mind-body dualism.
The creation–evolution controversy in the United States raises the issue of whether creationistic ideas may be legitimately called science and whether evolution itself may be legitimately called science. In the debate, both sides and even courts in their decisions have frequently invoked Popper's criterion of falsifiability (see Daubert standard). In this context, passages written by Popper are frequently quoted in which he speaks about such issues himself. For example, he famously stated "Darwinism is not a testable scientific theory, but a metaphysical research program—a possible framework for testable scientific theories." He continued:
Popper had his own sophisticated views on evolution that go much beyond what the frequently-quoted passages say. In effect, Popper agreed with some of the points of both creationists and naturalists, but also disagreed with both views on crucial aspects. Popper understood the universe as a creative entity that invents new things, including life, but without the necessity of something like a god, especially not one who is pulling strings from behind the curtain. He said that evolution must, as the creationists say, work in a goal-directed way but disagreed with their view that it must necessarily be the hand of god that imposes these goals onto the stage of life.
Instead, he formulated the spearhead model of evolution, a version of genetic pluralism. According to this model, living organisms themselves have goals, and act according to these goals, each guided by a central control. In its most sophisticated form, this is the brain of humans, but controls also exist in much less sophisticated ways for species of lower complexity, such as the amoeba. This control organ plays a special role in evolution—it is the "spearhead of evolution". The goals bring the purpose into the world. Mutations in the genes that determine the structure of the control may then cause drastic changes in behaviour, preferences and goals, without having an impact on the organism's phenotype. Popper postulates that such purely behavioural changes are less likely to be lethal for the organism compared to drastic changes of the phenotype.
Popper contrasts his views with the notion of the "hopeful monster" that has large phenotype mutations and calls it the "hopeful behavioural monster". After behaviour has changed radically, small but quick changes of the phenotype follow to make the organism fitter to its changed goals. This way it looks as if the phenotype were changing guided by some invisible hand, while it is merely natural selection working in combination with the new behaviour. For example, according to this hypothesis, the eating habits of the giraffe must have changed before its elongated neck evolved. Popper contrasted this view as "evolution from within" or "active Darwinism" (the organism actively trying to discover new ways of life and being on a quest for conquering new ecological niches), with the naturalistic "evolution from without" (which has the picture of a hostile environment only trying to kill the mostly passive organism, or perhaps segregate some of its groups).
About the creation-evolution controversy, Popper wrote that he considered it "a somewhat sensational clash between a brilliant scientific hypothesis concerning the history of the various species of animals and plants on earth, and an older metaphysical theory which, incidentally, happened to be part of an established religious belief" with a footnote to the effect that "[he] agree[s] with Professor C.E. Raven when, in his Science, Religion, and the Future, 1943, he calls this conflict "a storm in a Victorian tea-cup"; though the force of this remark is perhaps a little impaired by the attention he pays to the vapours still emerging from the cup—to the Great Systems of Evolutionist Philosophy, produced by Bergson, Whitehead, Smuts, and others."
In an interview that Popper gave in 1969 with the condition that it shall be kept secret until after his death, he summarised his position on God as follows: "I don't know whether God exists or not. ... Some forms of atheism are arrogant and ignorant and should be rejected, but agnosticism—to admit that we don't know and to search—is all right. ... When I look at what I call the gift of life, I feel a gratitude which is in tune with some religious ideas of God. However, the moment I even speak of it, I am embarrassed that I may do something wrong to God in talking about God." He objected to organised religion, saying "it tends to use the name of God in vain", noting the danger of fanaticism because of religious conflicts: "The whole thing goes back to myths which, though they may have a kernel of truth, are untrue. Why then should the Jewish myth be true and the Indian and Egyptian myths not be true?" In a letter unrelated to the interview, he stressed his tolerant attitude: "Although I am not for religion, I do think that we should show respect for anybody who believes honestly."
Popper played a vital role in establishing the philosophy of science as a vigorous, autonomous discipline within philosophy, through his own prolific and influential works, and also through his influence on his own contemporaries and students. Popper founded in 1946 the Department of Philosophy, Logic and Scientific Method at the London School of Economics and there lectured and influenced both Imre Lakatos and Paul Feyerabend, two of the foremost philosophers of science in the next generation of philosophy of science. (Lakatos significantly modified Popper's position,:1 and Feyerabend repudiated it entirely, but the work of both is deeply influenced by Popper and engaged with many of the problems that Popper set.)
While there is some dispute as to the matter of influence, Popper had a long-standing and close friendship with economist Friedrich Hayek, who was also brought to the London School of Economics from Vienna. Each found support and similarities in the other's work, citing each other often, though not without qualification. In a letter to Hayek in 1944, Popper stated, "I think I have learnt more from you than from any other living thinker, except perhaps Alfred Tarski." Popper dedicated his Conjectures and Refutations to Hayek. For his part, Hayek dedicated a collection of papers, Studies in Philosophy, Politics, and Economics, to Popper, and in 1982 said, "...ever since his Logik der Forschung first came out in 1934, I have been a complete adherent to his general theory of methodology."
He does not argue that any such conclusions are therefore true, or that this describes the actual methods of any particular scientist.[citation needed] Rather, it is recommended as an essential principle of methodology that, if enacted by a system or community, will lead to slow but steady progress of a sort (relative to how well the system or community enacts the method). It has been suggested that Popper's ideas are often mistaken for a hard logical account of truth because of the historical co-incidence of their appearing at the same time as logical positivism, the followers of which mistook his aims for their own.
The Quine-Duhem thesis argues that it's impossible to test a single hypothesis on its own, since each one comes as part of an environment of theories. Thus we can only say that the whole package of relevant theories has been collectively falsified, but cannot conclusively say which element of the package must be replaced. An example of this is given by the discovery of the planet Neptune: when the motion of Uranus was found not to match the predictions of Newton's laws, the theory "There are seven planets in the solar system" was rejected, and not Newton's laws themselves. Popper discussed this critique of naïve falsificationism in Chapters 3 and 4 of The Logic of Scientific Discovery. For Popper, theories are accepted or rejected via a sort of selection process. Theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value. Thus Newton's laws, with their wide general application, are to be preferred over the much more specific "the solar system has seven planets".[dubious – discuss]
Popper claimed to have recognised already in the 1934 version of his Logic of Discovery a fact later stressed by Kuhn, "that scientists necessarily develop their ideas within a definite theoretical framework", and to that extent to have anticipated Kuhn's central point about "normal science". (But Popper criticised what he saw as Kuhn's relativism.) Also, in his collection Conjectures and Refutations: The Growth of Scientific Knowledge (Harper & Row, 1963), Popper writes, "Science must begin with myths, and with the criticism of myths; neither with the collection of observations, nor with the invention of experiments, but with the critical discussion of myths, and of magical techniques and practices. The scientific tradition is distinguished from the pre-scientific tradition in having two layers. Like the latter, it passes on its theories; but it also passes on a critical attitude towards them. The theories are passed on, not as dogmas, but rather with the challenge to discuss them and improve upon them."
Another objection is that it is not always possible to demonstrate falsehood definitively, especially if one is using statistical criteria to evaluate a null hypothesis. More generally it is not always clear, if evidence contradicts a hypothesis, that this is a sign of flaws in the hypothesis rather than of flaws in the evidence. However, this is a misunderstanding of what Popper's philosophy of science sets out to do. Rather than offering a set of instructions that merely need to be followed diligently to achieve science, Popper makes it clear in The Logic of Scientific Discovery that his belief is that the resolution of conflicts between hypotheses and observations can only be a matter of the collective judgment of scientists, in each individual case.
In a book called Science Versus Crime, Houck writes that Popper's falsificationism can be questioned logically: it is not clear how Popper would deal with a statement like "for every metal, there is a temperature at which it will melt." The hypothesis cannot be falsified by any possible observation, for there will always be a higher temperature than tested at which the metal may in fact melt, yet it seems to be a valid scientific hypothesis. These examples were pointed out by Carl Gustav Hempel. Hempel came to acknowledge that Logical Positivism's verificationism was untenable, but argued that falsificationism was equally untenable on logical grounds alone. The simplest response to this is that, because Popper describes how theories attain, maintain and lose scientific status, individual consequences of currently accepted scientific theories are scientific in the sense of being part of tentative scientific knowledge, and both of Hempel's examples fall under this category. For instance, atomic theory implies that all metals melt at some temperature.
In 2004, philosopher and psychologist Michel ter Hark (Groningen, The Netherlands) published a book, called Popper, Otto Selz and the rise of evolutionary epistemology, in which he claimed that Popper took some of his ideas from his tutor, the German psychologist Otto Selz. Selz never published his ideas, partly because of the rise of Nazism, which forced him to quit his work in 1933, and the prohibition of referring to Selz' work. Popper, the historian of ideas and his scholarship, is criticised in some academic quarters for his rejection of Plato, Hegel and Marx.
According to John N. Gray, Popper held that "a theory is scientific only in so far as it is falsifiable, and should be given up as soon as it is falsified." By applying Popper's account of scientific method, Gray's Straw Dogs states that this would have "killed the theories of Darwin and Einstein at birth." When they were first advanced, Gray claims, each of them was "at odds with some available evidence; only later did evidence become available that gave them crucial support." Against this, Gray seeks to establish the irrationalist thesis that "the progress of science comes from acting against reason."
Gray does not, however, give any indication of what available evidence these theories were at odds with, and his appeal to "crucial support" illustrates the very inductivist approach to science that Popper sought to show was logically illegitimate. For, according to Popper, Einstein's theory was at least equally as well corroborated as Newton's upon its initial conception; they both equally well accounted for all the hitherto available evidence. Moreover, since Einstein also explained the empirical refutations of Newton's theory, general relativity was immediately deemed suitable for tentative acceptance on the Popperian account. Indeed, Popper wrote, several decades before Gray's criticism, in reply to a critical essay by Imre Lakatos:
Such a theory would be true with higher probability, because it cannot be attacked so easily: to falsify the first one, it is sufficient to find that the sun has stopped rising; to falsify the second one, one additionally needs the assumption that the given day has not yet been reached. Popper held that it is the least likely, or most easily falsifiable, or simplest theory (attributes which he identified as all the same thing) that explains known facts that one should rationally prefer. His opposition to positivism, which held that it is the theory most likely to be true that one should prefer, here becomes very apparent. It is impossible, Popper argues, to ensure a theory to be true; it is more important that its falsity can be detected as easily as possible.
In his early years Popper was impressed by Marxism, whether of Communists or socialists. An event that happened in 1919 had a profound effect on him: During a riot, caused by the Communists, the police shot several unarmed people, including some of Popper's friends, when they tried to free party comrades from prison. The riot had, in fact, been part of a plan by which leaders of the Communist party with connections to Béla Kun tried to take power by a coup; Popper did not know about this at that time. However, he knew that the riot instigators were swayed by the Marxist doctrine that class struggle would produce vastly more dead men than the inevitable revolution brought about as quickly as possible, and so had no scruples to put the life of the rioters at risk to achieve their selfish goal of becoming the future leaders of the working class. This was the start of his later criticism of historicism. Popper began to reject Marxist historicism, which he associated with questionable means, and later socialism, which he associated with placing equality before freedom (to the possible disadvantage of equality).
Palermo (Italian: [paˈlɛrmo] ( listen), Sicilian: Palermu, Latin: Panormus, from Greek: Πάνορμος, Panormos, Arabic: بَلَرْم‎, Balarm; Phoenician: זִיז, Ziz) is a city in Insular Italy, the capital of both the autonomous region of Sicily and the Province of Palermo. The city is noted for its history, culture, architecture and gastronomy, playing an important role throughout much of its existence; it is over 2,700 years old. Palermo is located in the northwest of the island of Sicily, right by the Gulf of Palermo in the Tyrrhenian Sea.
The city was founded in 734 BC by the Phoenicians as Ziz ('flower'). Palermo then became a possession of Carthage, before becoming part of the Roman Republic, the Roman Empire and eventually part of the Byzantine Empire, for over a thousand years. The Greeks named the city Panormus meaning 'complete port'. From 831 to 1072 the city was under Arab rule during the Emirate of Sicily when the city first became a capital. The Arabs shifted the Greek name into Balarm, the root for Palermo's present-day name. Following the Norman reconquest, Palermo became the capital of a new kingdom (from 1130 to 1816), the Kingdom of Sicily and the capital of the Holy Roman Empire under Frederick II Holy Roman Emperor and Conrad IV of Germany, King of the Romans. Eventually Sicily would be united with the Kingdom of Naples to form the Kingdom of the Two Sicilies until the Italian unification of 1860.
Palermo is Sicily's cultural, economic and touristic capital. It is a city rich in history, culture, art, music and food. Numerous tourists are attracted to the city for its good Mediterranean weather, its renowned gastronomy and restaurants, its Romanesque, Gothic and Baroque churches, palaces and buildings, and its nightlife and music. Palermo is the main Sicilian industrial and commercial center: the main industrial sectors include tourism, services, commerce and agriculture. Palermo currently has an international airport, and a significant underground economy.[citation needed] In fact, for cultural, artistic and economic reasons, Palermo was one of the largest cities in the Mediterranean and is now among the top tourist destinations in both Italy and Europe. The city is also going through careful redevelopment, preparing to become one of the major cities of the Euro-Mediterranean area.
Palermo is surrounded by mountains, formed of calcar, which form a cirque around the city. Some districts of the city are divided by the mountains themselves. Historically, it was relatively difficult to reach the inner part of Sicily from the city because of the mounts. The tallest peak of the range is La Pizzuta, about 1,333 m (4,373 ft.) high. However, historically, the most important mount is Monte Pellegrino, which is geographically separated from the rest of the range by a plain. The mount lies right in front of the Tyrrhenian Sea. Monte Pellegrino's cliff was described in the 19th century by Johann Wolfgang von Goethe, as "The most beautiful promontory in the world", in his essay "Italian Journey".
Today both the Papireto river and the Kemonia are covered up by buildings. However, the shape of the former watercourses can still be recognised today, because the streets that were built on them follow their shapes. Today the only waterway not drained yet is the Oreto river that divides the downtown of the city from the western uptown and the industrial districts. In the basins there were, though, many seasonal torrents that helped formed swampy plains, reclaimed during history; a good example of which can be found in the borough of Mondello.
During 734 BC the Phoenicians, a sea trading people from the north of ancient Canaan, built a small settlement on the natural harbor of Palermo. Some sources suggest they named the settlement "Ziz." It became one of the three main Phoenician colonies of Sicily, along with Motya and Soluntum. However, the remains of the Phoenician presence in the city are few and mostly preserved in the very populated center of the downtown area, making any excavation efforts costly and logistically difficult. The site chosen by the Phoenicians made it easy to connect the port to the mountains with a straight road that today has become Corso Calatifimi. This road helped the Phoenicians in trading with the populations that lived beyond the mountains that surround the gulf.
The first settlement is defined as Paleapolis (Παλεάπολις), the Ancient Greek world for "old city", in order to distinguish it from a second settlement built during the 5th century BC, called Neapolis (Νεάπολις), "new city". The neapolis was erected towards the east and along with it, monumental walls around the whole settlement were built to prevent attacks from foreign threats. Some part of this structure can still be seen in the Cassaro district. This district was named after the walls themselves; the word Cassaro deriving from the Arab al-qsr (castle, stronghold). Along the walls there were few doors to access and exit the city, suggesting that trade even toward the inner part of the island occurred frequently. Moreover, according to some studies, it may be possible that there were some walls that divided the old city from the new one too. The colony developed around a central street (decumanus), cut perpendicularly by minor streets. This street today has become the Corso Vittorio Emanuele.
Carthage was Palermo’s major trading partner under the Phoenicians and the city enjoyed a prolonged peace during this period. Palermo came into contact with the Ancient Greeks between the 6th and the 5th centuries BC which preceded the Sicilian Wars, a conflict fought between the Greeks of Syracuse and the Phoenicians of Carthage for control over the island of Sicily. During this war the Greeks named the settlement Panormos (Πάνορμος) from which the current name is derived, meaning "all port" due to the shape of its coast. It was from Palermo that Hamilcar I's fleet (which was defeated at the Battle of Himera) was launched. In 409 B.C. the city was looted by Hermocrates of Syracuse. The Sicilian Wars ended in 265 BC when Carthage and Syracuse stopped warring and united in order to stop the Romans from gaining full control of the island during the First Punic War. In 276 BC, during the Pyrrhic War, Panormos briefly became a Greek colony after being conquered by Pyrrhus of Epirus, but returned to Phoenician Carthage in 275. In 254 BC Panormos was besieged and conquered by the Romans in the first battle of Panormus (the name Latin name). Carthage attempted to reconquer Panormus in 251 BC but failed.
As the Roman Empire was falling apart, Palermo fell under the control of several Germanic tribes. The first were the Vandals in 440 AD under the rule of their king Geiseric. The Vandals had occupied all the Roman provinces in North Africa by 455 establishing themselves as a significant force. They acquired Corsica, Sardinia and Sicily shortly afterwards. However, they soon lost these newly acquired possessions to the Ostrogoths. The Ostrogothic conquest under Theodoric the Great began in 488; Theodoric supported Roman culture and government unlike the Germanic Goths. The Gothic War took place between the Ostrogoths and the Eastern Roman Empire, also known as the Byzantine Empire. Sicily was the first part of Italy to be taken under control of General Belisarius who was commissioned by Eastern Emperor. Justinian I solidified his rule in the following years.
The Muslims took control of the Island in 904, after decades of fierce fighting, and the Emirate of Sicily was established. Muslim rule on the island lasted for about 120 years and was marked by cruelty and brutality against the native population, which was reduced into near slavery[clarification needed] and Christian churches across the island were all completely destroyed.[page needed] Palermo (Balarm during Arab rule) displaced Syracuse as the capital city of Sicily. It was said to have then begun to compete with Córdoba and Cairo in terms of importance and splendor. For more than one hundred years Palermo was the capital of a flourishing emirate. The Arabs also introduced many agricultural crops which remain a mainstay of Sicilian cuisine.
After dynastic quarrels however, there was a Christian reconquest in 1072. The family who returned the city to Christianity were called the Hautevilles, including Robert Guiscard and his army, who is regarded as a hero by the natives. It was under Roger II of Sicily that Norman holdings in Sicily and the southern part of the Italian Peninsula were promoted from the County of Sicily into the Kingdom of Sicily. The Kingdom's capital was Palermo, with the King's Court held at the Palazzo dei Normanni. Much construction was undertaken during this period, such as the building of Palermo Cathedral. The Kingdom of Sicily became one of the wealthiest states in Europe.
Sicily fell under the control of the Holy Roman Empire in 1194. Palermo was the preferred city of the Emperor Frederick II. Muslims of Palermo emigrated or were expelled during Holy Roman rule. After an interval of Angevin rule (1266–1282), Sicily came under control of the Aragon and Barcelona dynasties. By 1330, Palermo's population had declined to 51,000. From 1479 until 1713 Palermo was ruled by the Kingdom of Spain, and again between 1717 and 1718. Palermo was also under Savoy control between 1713 and 1717 and 1718–1720 as a result of the Treaty of Utrecht. It was also ruled by Austria between 1720 and 1734.
After the Treaty of Utrecht (1713), Sicily was handed over to the Savoia, but by 1734 it was again a Bourbon possession. Charles III chose Palermo for his coronation as King of Sicily. Charles had new houses built for the growing population, while trade and industry grew as well. However, by now Palermo was now just another provincial city as the Royal Court resided in Naples. Charles' son Ferdinand, though disliked by the population, took refuge in Palermo after the French Revolution in 1798. His son Alberto died on the way to Palermo and is buried in the city. When the Kingdom of the Two Sicilies was founded, the original capital city was Palermo (1816) but a year later moved to Naples.
From 1820 to 1848 Sicily was shaken by upheavals, which culminated on 12 January 1848, with a popular insurrection, the first one in Europe that year, led by Giuseppe La Masa. A parliament and constitution were proclaimed. The first president was Ruggero Settimo. The Bourbons reconquered Palermo in 1849, and remained under their rule until the time of Giuseppe Garibaldi. The famous general entered Palermo with his troops (the “Thousands”) on 27 May 1860. After the plebiscite later that year Palermo, along with the rest of Sicily, became part of the new Kingdom of Italy (1861).
The majority of Sicilians preferred independence to the Savoia kingdom; in 1866, Palermo became the seat of a week-long popular rebellion, which was finally crushed after Martial law was declared. The Italian government blamed anarchists and the Church, specifically the Archbishop of Palermo, for the rebellion and began enacting anti-Sicilian and anti-clerical policies. A new cultural, economic and industrial growth was spurred by several families, like the Florio, the Ducrot, the Rutelli, the Sandron, the Whitaker, the Utveggio, and others. In the early twentieth century Palermo expanded outside the old city walls, mostly to the north along the new boulevards Via Roma, Via Dante, Via Notarbartolo, and Viale della Libertà. These roads would soon boast a huge number of villas in the Art Nouveau style. Many of these were designed by the famous architect Ernesto Basile. The Grand Hotel Villa Igiea, designed by Ernesto Basile for the Florio family, is a good example of Palermitan Art Nouveau. The huge Teatro Massimo was designed in the same period by Giovan Battista Filippo Basile, and built by the Rutelli & Machì building firm of the industrial and old Rutelli Italian family in Palermo, and was opened in 1897.
The so-called "Sack of Palermo" is one of the major visible faces of the problem. The term is used to indicate the speculative building practices that have filled the city with poor buildings. The reduced importance of agriculture in the Sicilian economy has led to a massive migration to the cities, especially Palermo, which swelled in size, leading to rapid expansion towards the north. The regulatory plans for expansion was largely ignored in the boom. New parts of town appeared almost out of nowhere, but without parks, schools, public buildings, proper roads and the other amenities that characterise a modern city.
Palermo experiences a hot-summer Mediterranean climate (Köppen climate classification: Csa). Winters are cool and wet, while summers are hot and dry. Temperatures in autumn and spring are usually mild. Palermo is one of the warmest cities in Europe (mainly due to its warm nights), with an average annual air temperature of 18.5 °C (65.3 °F). It receives approximately 2,530 hours of sunshine per year. Snow is usually a rare occurrence, but it does occur occasionally if there is a cold front, as the Apennines are too distant to protect the island from cold winds blowing from the Balkans, and the mountains surrounding the city facilite the formation of snow accumulation in Palermo, especially at night. Between the 1940s and the 2000s there have been eleven times when considerable snowfall has occurred: In 1949, in 1956, when the minimum temperature went down to 0 °C (32 °F) and the city was blanketed by several centimeters of snow. Snow also occurred in 1999, 2009 and 2015. The average annual temperature of the sea is above 19 °C (66 °F); from 14 °C (57 °F) in February to 26 °C (79 °F) in August. In the period from May to November, the average sea temperature exceeds 18 °C (64 °F) and in the period from June to October, the average sea temperature exceeds 21 °C (70 °F).
Palermo has at least 2 circuits of City Walls - many pieces of which still survive. The first circuit surrounded the ancient core of the punic City - the so-called Palaeopolis (in the area east of Porta Nuova) and the Neopolis. Via Vittorio Emanuele was the main road east-west through this early walled city. The eastern edge of the walled city was on Via Roma and the ancient port in the vicinity of Piazza Marina. The wall circuit was approximately Porto Nuovo, Corso Alberti, Piazza Peranni, Via Isodoro, Via Candela, Via Venezia, Via Roma, Piazza Paninni, Via Biscottari, Via Del Bastione, Palazzo dei Normanni and back to Porto Nuovo.
In the medieval period the wall circuit was expanded. Via Vittorio Emanuele continued to be the main road east-west through the walled city. West gate was still Porta Nuova, the circuit continued to Corso Alberti, to Piazza Vittorio Emanuele Orlando where it turned east along Via Volturno to Piazza Verdi and along the line of Via Cavour. At this north-east corner there was a defence, Castello a Mare, to protect the port at La Cala. A huge chain was used to block La Cala with the other end at S Maria della Catena (St Mary of the Chain). The sea-side wall was along the western side of Foro Italico Umberto. The wall turns west along the northern side of Via Abramo Lincoln, continues along Corso Tukory. The wall turns north approximately on Via Benedetto, to Palazzo dei Normanni and back to Porta Nuova. Source: Palermo - City Guide by Adriana Chirco, 1998, Dario Flaccovio Editore.
The cathedral has a heliometer (solar "observatory") of 1690, one of a number built in Italy in the 17th and 18th centuries. The device itself is quite simple: a tiny hole in one of the minor domes acts as pinhole camera, projecting an image of the sun onto the floor at solar noon (12:00 in winter, 13:00 in summer). There is a bronze line, la Meridiana on the floor, running precisely N/S. The ends of the line mark the positions as at the summer and winter solstices; signs of the zodiac show the various other dates throughout the year.
In 2010, there were 1.2 million people living in the greater Palermo area, 655,875 of which resided in the City boundaries, of whom 47.4% were male and 52.6% were female. People under age 15 totalled 15.6% compared to pensioners who composed 17.2% of the population. This compares with the Italian average of 14.1% people under 15 years and 20.2% pensioners. The average age of a Palermo resident is 40.4 compared to the Italian average of 42.8. In the ten years between 2001 and 2010, the population of Palermo declined by 4.5%, while the population of Italy, as a whole, grew by 6.0%. The reason for Palermo's decline is a population flight to the suburbs, and to Northern Italy. The current birth rate of Palermo is 10.2 births per 1,000 inhabitants compared to the Italian average of 9.3 births.
Being Sicily's administrative capital, Palermo is a centre for much of the region's finance, tourism and commerce. The city currently hosts an international airport, and Palermo's economic growth over the years has brought the opening of many new businesses. The economy mainly relies on tourism and services, but also has commerce, shipbuilding and agriculture. The city, however, still has high unemployment levels, high corruption and a significant black market empire (Palermo being the home of the Sicilian Mafia). Even though the city still suffers from widespread corruption, inefficient bureaucracy and organized crime, the level of crime in Palermo's has gone down dramatically, unemployment has been decreasing and many new, profitable opportunities for growth (especially regarding tourism) have been introduced, making the city safer and better to live in.
The port of Palermo, founded by the Phoenicians over 2,700 years ago, is, together with the port of Messina, the main port of Sicily. From here ferries link Palermo to Cagliari, Genoa, Livorno, Naples, Tunis and other cities and carry a total of almost 2 million passengers annually. It is also an important port for cruise ships. Traffic includes also almost 5 million tonnes of cargo and 80.000 TEU yearly. The port also has links to minor sicilian islands such as Ustica and the Aeolian Islands (via Cefalù in summer). Inside the Port of Palermo there is a section known as "tourist marina" for sailing yachts and catamarans.
The patron saint of Palermo is Santa Rosalia, who is widely revered. On 14 July, people in Palermo celebrate the annual Festino, the most important religious event of the year. The Festino is a procession which goes through the main street of Palermo to commemorate the miracle attributed to Santa Rosalia who, it is believed, freed the city from the Black Death in 1624. Her remains were discovered in a cave on Monte Pellegrino, and her remains were carried around the city three times, banishing the plague. There is a sanctuary marking the spot where her remains were found which can be reached via a scenic bus ride from the city.
The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of "one world, one dream". Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the "Journey of Harmony", lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics.
After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event.
In many cities along the North American and European route, the torch relay was protested by advocates of Tibetan independence, animal rights, and legal online gambling, and people protesting against China's human rights record, resulting in confrontations at a few of the relay locations. These protests, which ranged from hundreds of people in San Francisco, to effectively none in Pyongyang, forced the path of the torch relay to be changed or shortened on a number of occasions. The torch was extinguished by Chinese security officials several times during the Paris leg for security reasons, and once in protest in Paris.
The attacks on the torch in London and Paris were described as "despicable" by the Chinese government, condemning them as "deliberate disruptions... who gave no thought to the Olympic spirit or the laws of Britain and France" and who "tarnish the lofty Olympic spirit", and vowed they would continue with the relay and not allow the protests to "impede the Olympic spirit". Large-scale counter-protests by overseas Chinese and foreign-based Chinese nationals became prevalent in later segments of the relay. In San Francisco, the number of supporters were much more than the number of protesters, and in Australia, Japan, South Korea, the counter-protesters overwhelmed the protesters. A couple of skirmishes between the protesters and supporters were reported. No major protests were visible in the Latin America, Africa, and Western Asia legs of the torch relay.
Prompted by the chaotic torch relays in Western Europe and North America, the president of the International Olympic Committee, Jacques Rogge described the situation as a "crisis" for the organization and stated that any athletes displaying Tibetan flags at Olympic venues could be expelled from the games. though he stopped short of cancelling the relay altogether despite calls to do so by some IOC members. The outcome of the relay influenced the IOC's decision to scrap global relays in future editions of the games.
In June 2008, the Beijing Games' Organizing Committee announced that the planned international torch relay for the Paralympic Games had been cancelled. The Committee stated that the relay was being cancelled to enable the Chinese government to "focus on the rescue and relief work" following the Sichuan earthquake.
The Olympic Torch is based on traditional scrolls and uses a traditional Chinese design known as "Lucky Cloud". It is made from aluminum. It is 72 centimetres high and weighs 985 grams. The torch is designed to remain lit in 65 kilometre per hour (37 mile per hour) winds, and in rain of up to 50 millimetres (2 inches) per hour. An ignition key is used to ignite and extinguish the flame. The torch is fueled by cans of propane. Each can will light the torch for 15 minutes. It is designed by a team from Lenovo Group. The Torch is designed in reference to the traditional Chinese concept of the 5 elements that make up the entire universe.
Internationally, the torch and its accompanying party traveled in a chartered Air China Airbus A330 (registered B-6075), painted in the red and yellow colors of the Olympic Games. Air China was chosen by the Beijing Committees of the Olympic Game as the designated Olympic torch carrier in March 2008 for its long-standing participation in the Olympic cause. The plane traveled a total of 137,000 km (85,000 mi) for a duration of 130 days through 21 countries and regions.
The route carried the torch through six continents from March 2008 to May 2008 to August 2008. The planned route originally included a stop in Taipei between Ho Chi Minh City and Hong Kong, but there was disagreement in Beijing and Taipei over language used to describe whether it was an international or a domestic part of the route. While the Olympic committees of China and Chinese Taipei reached initial consensus on the approach, the government of the Republic of China in Taiwan intervened, stating that this placement could be interpreted as placing Taiwan on the same level as Hong Kong and Macau, an implication it objected to. The Beijing Organizing Committee attempted to continue negotiation, but further disputes arose over the flag or the anthem of the Republic of China along the 24 km torch route in Taiwan. By the midnight deadline for concluding the negotiation on September 21, 2007, Taiwan and China were unable to come to terms with the issue of the Torch Relay. In the end, both sides of the Taiwan Strait decided to eliminate the Taipei leg.
 Greece: On March 24, 2008, the Olympic Flame was ignited at Olympia, Greece, site of the ancient Olympic Games. The actress Maria Nafpliotou, in the role of a High Priestess, ignited the torch of the first torchbearer, a silver medalist of the 2004 Summer Olympics in taekwondo Alexandros Nikolaidis from Greece, who handed the flame over to the second torchbearer, Olympic champion in women's breaststroke Luo Xuejuan from China. Following the recent unrest in Tibet, three members of Reporters Without Borders, including Robert Ménard, breached security and attempted to disrupt a speech by Liu Qi, the head of Beijing's Olympic organising committee during the torch lighting ceremony in Olympia, Greece. The People's Republic of China called this a "disgraceful" attempt to sabotage the Olympics. On March 30, 2008 in Athens, during ceremonies marking the handing over of the torch from Greek officials to organizers of the Beijing games, demonstrators shouted 'Free Tibet' and unfurled banners; some 10 of the 15 protesters were taken into police detention. After the hand-off, protests continued internationally, with particularly violent confrontations with police in Nepal.

 China: In China, the torch was first welcomed by Politburo Standing Committee member Zhou Yongkang and State Councilor Liu Yandong. It was subsequently passed onto CPC General Secretary Hu Jintao. A call to boycott French hypermart Carrefour from May 1 began spreading through mobile text messaging and online chat rooms amongst the Chinese over the weekend from April 12, accusing the company's major shareholder, the LVMH Group, of donating funds to the Dalai Lama. There were also calls to extend the boycott to include French luxury goods and cosmetic products. According to the Washington Times on April 15, however, the Chinese government was attempting to "calm the situation" through censorship: "All comments posted on popular Internet forum Sohu.com relating to a boycott of Carrefour have been deleted." Chinese protesters organized boycotts of the French-owned retail chain Carrefour in major Chinese cities including Kunming, Hefei and Wuhan, accusing the French nation of pro-secessionist conspiracy and anti-Chinese racism. Some burned French flags, some added Nazism's Swastika to the French flag, and spread short online messages calling for large protests in front of French consulates and embassy. The Carrefour boycott was met with anti-boycott demonstrators who insisted on entering one of the Carrefour stores in Kunming, only to be blocked by boycotters wielding large Chinese flags and hit by water bottles. The BBC reported that hundreds of people demonstrated in Beijing, Wuhan, Hefei, Kunming and Qingdao.
In response to the demonstrations, an editorial in the People's Daily urged Chinese people to "express [their] patriotic enthusiasm calmly and rationally, and express patriotic aspiration in an orderly and legal manner".

 Kazakhstan: The first torchbearer in Almaty, where the Olympic torch arrived for the first time ever on April 2, was the President of Kazakhstan Nursultan Nazarbaev. The route ran 20 km from Medeo stadium to Astana Square. There were reports that Uighur activists were arrested and some were deported back to China.

 Turkey: The torch relay leg in Istanbul, held on April 3, started on Sultanahmet Square and finished in Taksim Square. Uyghurs living in Turkey protested at Chinese treatment of their compatriots living in Xinjiang. Several protesters who tried to disrupt the relay were promptly arrested by the police.

 Russia: On April 5 the Olympic torch arrived at Saint Petersburg, Russia. The length of the torch relay route in the city was 20 km, with the start at the Victory Square and finish at the Palace Square. Mixed martial arts icon and former PRIDE Heavyweight Champion Fedor Emelianenko was one the torch bearers. This gives him the distinction of the being the first active MMA fighter to carry the Olympic flame.

 Great Britain: The torch relay leg held in London, the host city of the 2012 Summer Olympics, on April 6 began at Wembley Stadium, passed through the City of London, and eventually ended at O2 Arena in the eastern part of the city. The 48 km (30 mi) leg took a total of seven and a half hours to complete, and attracted protests by pro-Tibetan independence and pro-Human Rights supporters, prompting changes to the planned route and an unscheduled move onto a bus, which was then briefly halted by protestors. Home Secretary Jacqui Smith has officially complained to Beijing Organising Committee about the conduct of the tracksuit-clad Chinese security guards. The Chinese officials, seen manhandling protesters, were described by both the London Mayor Ken Livingstone and Lord Coe, chairman of the London Olympic Committee as "thugs". A Metropolitan police briefing paper revealed that security for the torch relay cost £750,000 and the participation of the Chinese security team had been agreed in advance, despite the Mayor stating, "We did not know beforehand these thugs were from the security services. Had I known so, we would have said no."
Of the 80 torch-bearers in London, Sir Steve Redgrave, who started the relay, mentioned to the media that he had received e-mailed pleas to boycott the event and could "see why they would like to make an issue" of it. Francesca Martinez and Richard Vaughan refused to carry the torch, while Konnie Huq decided to carry it and also speak out against China. The pro-Tibetan Member of Parliament Norman Baker asked all bearers to reconsider. Amid pressure from both directions, Prime Minister Gordon Brown welcomed the torch outside 10 Downing Street without holding or touching it. The London relay saw the torch surrounded by what the BBC described as "a mobile protective ring." Protests began as soon as Redgrave started the event, leading to at least thirty-five arrests. In Ladbroke Grove a demonstrator attempted to snatch the torch from Konnie Huq in a momentary struggle, and in a separate incident, a fire extinguisher was set off near the torch. The Chinese ambassador carried the torch through Chinatown after an unpublicized change to the route amid security concerns. The torch made an unscheduled move onto a bus along Fleet Street amid security concerns and efforts to evade the protesters. In an effort to counter the pro-Tibet protesters and show their support for the 2008 Beijing Olympics, more than 2,000 Chinese also gathered on the torch route and demonstrated with signs, banners and Chinese flags. A large number of supporters were concentrated in Trafalgar Square, displaying the Olympic slogan "One World, One Dream".

 France: The torch relay leg in Paris, held on April 7, began on the first level of the Eiffel Tower and finished at the Stade Charléty. The relay was initially supposed to cover 28 km, but it was shortened at the demand of Chinese officials following widespread protests by pro-Tibet and human rights activists, who repeatedly attempted to disrupt, hinder or halt the procession. A scheduled ceremony at the town hall was cancelled at the request of the Chinese authorities, and, also at the request of Chinese authorities, the torch finished the relay by bus instead of being carried by athletes. Paris City officials had announced plans to greet the Olympic flame with peaceful protest when the torch was to reach the French capital. The city government attached a banner reading "Paris defends human rights throughout the world" to the City Hall, in an attempt to promote values "of all humanity and of human rights." Members from Reporters Without Borders turned out in large numbers to protest. An estimated 3,000 French police protected the Olympic torch relay as it departed from the Eiffel Tower and criss-crossed Paris amid threat of protests. Widespread pro-Tibet protests, including an attempt by more than one demonstrator to extinguish the flame with water or fire extinguishers, prompted relay authorities to put out the flame five times (according to the police authorities in Paris) and load the torch onto a bus, at the demand of Chinese officials. This was later denied by the Chinese Ministry of Foreign Affairs, despite video footage broadcast by French television network France 2 which showed Chinese flame attendants extinguishing the torch. Backup flames are with the relay at all times to relight the torch. French judoka and torchbearer David Douillet expressed his annoyance at the Chinese flame attendants who extinguished the torch which he was about to hand over to Teddy Riner: "I understand they're afraid of everything, but this is just annoying. They extinguished the flame despite the fact that there was no risk, and they could see it and they knew it. I don't know why they did it."
Chinese officials canceled the torch relay ceremony amidst disruptions, including a Tibetan flag flown from a window in the City Hall by Green Party officials. The third torchbearer in the Paris leg, Jin Jing, who was disabled and carried the torch on a wheelchair, was assaulted several times by unidentified protestors seemingly from the pro-Tibet independent camp. In interviews, Jin Jing said that she was "tugged at, scratched" and "kicked", but that she "did not feel the pain at the time." She received praise from ethnic Chinese worldwide as "Angel in Wheelchair". The Chinese government gave the comment that "the Chinese respect France a lot" but "Paris [has slapped] its own face."
Reporters Without Borders organised several symbolic protests, including scaling the Eiffel Tower to hang a protest banner from it, and hanging an identical banner from the Notre Dame cathedral.
Several hundred pro-Tibet protesters gathered at the Trocadéro with banners and Tibetan flags, and remained there for a peaceful protest, never approaching the torch relay itself. Among them was Jane Birkin, who spoke to the media about the "lack of freedom of speech" in China. Also present was Thupten Gyatso, President of the French Tibetan community, who called upon pro-Tibet demonstrators to "remain calm, non-violent, peaceful".
French members of Parliament and other French politicians also organised a protest. All political parties in Parliament—UMP, Socialists, New Centre, Communists, Democratic Movement (centre) and Greens—jointly requested a pause in the National Assembly's session, which was granted, so that MPs could step outside and unfurl a banner which read "Respect for Human Rights in China". The coach containing the torch drove past the National Assembly and the assembled protesting MPs, who shouted "Freedom for Tibet!" several times as it passed.
French police were criticised for their handling of the events, and notably for confiscating Tibetan flags from demonstrators. The newspaper Libération commented: "The police did so much that only the Chinese were given freedom of expression. The Tibetan flag was forbidden everywhere except on the Trocadéro." Minister of the Interior Michèle Alliot-Marie later stated that the police had not been ordered to do so, and that they had acted on their own initiative. A cameraman for France 2 was struck in the face by a police officer, knocked unconscious, and had to be sent to hospital.
 United States of America: The torch relay's North American leg occurred in San Francisco, California on April 9. On the day of the relay officials diverted the torch run to an unannounced route. The start was at McCovey Cove, where Norman Bellingham of the U.S. Olympic Committee gave the torch to the first torchbearer, Chinese 1992 Olympic champion swimmer Lin Li. The planned closing ceremony at Justin Herman Plaza was cancelled and instead, a ceremony was held at San Francisco International Airport, where the torch was to leave for Buenos Aires. The route changes allowed the run to avoid large numbers of China supporters and protesters against China. As people found out there would be no closing ceremony at Justin Herman Plaza, there were angry reactions. One demonstrator was quoted as saying that the route changes were an effort to "thwart any organized protest that had been planned." San Francisco Board of Supervisors President Aaron Peskin, a critic of Mayor Gavin Newsom, said that it was a "cynical plan to please the Bush State Department and the Chinese government because of the incredible influence of money." Newsom, on the other hand, said he felt it was in "everyone's best interest" and that he believed people had been "afforded the right to protest and support the torch" despite the route changes. Peter Ueberroth, head of the U.S. Olympic Committee, praised the route changes, saying, "The city of San Francisco, from a global perspective, will be applauded." People who saw the torch were surprised and cheered as shown from live video of CBS and NBC. The cost to the city for hosting the event was reported to be USD $726,400, nearly half of which has been recovered by private fundraising. Mayor Gavin Newsom said that "exponential" costs associated with mass arrests were avoided by his decision to change the route in consultation with police chief Heather Fong.
On April 1, 2008, the San Francisco Board of Supervisors approved a resolution addressing human rights concerns when the Beijing Olympic torch arrives in San Francisco on April 9. The resolution would welcome the torch with "alarm and protest at the failure of China to meet its past solemn promises to the international community, including the citizens of San Francisco, to cease the egregious and ongoing human rights abuses in China and occupied Tibet." On April 8, numerous protests were planned including one at the city's United Nations Plaza led by actor Richard Gere and Archbishop Desmond Tutu.
Some advocates for Tibet, Darfur, and the spiritual practice Falun Gong, planned to protest the April 9 arrival of the torch in San Francisco. China had already requested the torch route in San Francisco be shortened. On April 7, 2008, two days prior to the actual torch relay, three activists carrying Tibetan flags scaled the suspension cables of the Golden Gate Bridge to unfurl two banners, one saying "One World, One Dream. Free Tibet", and the other, "Free Tibet '08". Among them was San Francisco resident Laurel Sutherlin, who spoke to the local TV station KPIX-CBS5 live from a cellphone, urging the International Olympic Committee to ask China not to allow the torch to go through Tibet. "Sutherlin said he was worried that the torch's planned route through Tibet would lead to more arrests and Chinese officials would use force to stifle dissent." The three activists and five supporters face charges related to trespassing, conspiracy and causing a public nuisance.
The torch was lit at a park outside at AT&T Park at about 1:17 pm PDT (20:17 UTC), briefly held aloft by American and Chinese Olympic officials. The relay descended into confusion as the first runner in the elaborately planned relay disappeared into a warehouse on a waterfront pier where it stayed for a half-an-hour. There were clashes between thousands of pro-China demonstrators, many of whom said they were bused in by the Chinese Consulate and other pro-China groups, and both pro-Tibet and Darfur protesters. The non-Chinese demonstrators were reported to have been swamped and trailed by angry crowds. Around 2 pm PDT (21:00 UTC), the torch resurfaced about 3 km (1.9 mi) away from the stadium along Van Ness Avenue, a heavily trafficked thoroughfare that was not on official route plans. Television reports showed the flame flanked by motorcycles and uniformed police officers. Two torchbearers carried the flame running slowly behind a truck and surrounded by Olympic security guards. During the torch relay, two torchbearers, Andrew Michael who uses a wheelchair and is the Vice President for Sustainable Development for the Bay Area Council and Director of Partnerships For Change, and an environmental advocate, Majora Carter, managed to display Tibetan flags in protest, resulting in their ejection from the relay. The closing ceremony at Justin Herman Plaza was canceled due to the presence of large numbers of protesters at the site. The torch run ended with a final stretch through San Francisco's Marina district and was then moved by bus to San Francisco International Airport for a makeshift closing ceremony at the terminal, from which the free media was excluded. San Jose Mercury News described the "deceiving" event as "a game of Where's Waldo, played against the landscape of a lovely city." International Olympic Committee President Jacques Rogge said the San Francisco relay had "fortunately" avoided much of the disruptions that marred the legs in London and Paris, but "was, however, not the joyous party that we had wished it to be."
 Argentina: The torch relay leg in Buenos Aires, Argentina, held on April 11, began with an artistic show at the Lola Mora amphitheatre in Costanera Sur. In the end of the show the mayor of Buenos Aires Mauricio Macri gave the torch to the first torchbearer, Carlos Espínola. The leg finished at the Buenos Aires Riding Club in the Palermo district, the last torchbearer being Gabriela Sabatini. The 13.8 km route included landmarks like the obelisk and Plaza de Mayo. The day was marked by several pro-Tibet protests, which included a giant banner reading "Free Tibet", and an alternative "human rights torch" that was lit by protesters and paraded along the route the flame was to take. Most of these protests were peaceful in nature, and the torch was not impeded. Chinese immigrants also turned out in support of the Games, but only minor scuffles were reported between both groups. Runners surrounded by rows of security carried the Olympic flame past thousands of jubilant Argentines in the most trouble-free torch relay in nearly a week. People showered the parade route with confetti as banks, government offices and businesses took an impromptu half-day holiday for the only Latin American stop on the flame's five-continent journey.
Argentine activists told a news conference that they would not try to snuff out the torch's flame as demonstrators had in Paris and London. "I want to announce that we will not put out the Olympic torch," said pro-Tibet activist Jorge Carcavallo. "We'll be carrying out surprise actions throughout the city of Buenos Aires, but all of these will be peaceful." Among other activities, protesters organized an alternative march that went from the Obelisk to the city hall, featuring their own "Human Rights Torch." A giant banner reading "Free Tibet" was also displayed on the torch route. According to a representative from the NGO 'Human Rights Torch Relay', their objective was to "show the contradiction between the Olympic Games and the presence of widespread human rights violations in China"
The outreach director of HRTR, Susan Prager, is also the communication director of "Friends of Falun Gong", a quasi-government non-profit funded by fmr. Congressman Tom Lanto's wife and Ambassador Mark Palmer of NED. A major setback to the event was caused by footballer Diego Maradona, scheduled to open the relay through Buenos Aires, pulling out in an attempt to avoid the Olympic controversy. Trying to avoid the scenes that marred the relay in the UK, France and the US, the city government designed a complex security operative to protect the torch relay, involving 1200 police officers and 3000 other people, including public employees and volunteers. Overall, the protests were peaceful in nature, although there were a few incidents such as the throwing of several water balloons in an attempt to extinguish the Olympic flame, and minor scuffles between Olympic protesters and supporters from Chinese immigrant communities.
 Tanzania: Dar es Salaam was the torch's only stop in Africa, on April 13. The relay began at the grand terminal of the TAZARA Railway, which was China's largest foreign aid project of the 1970s, and continued for 5 km through the old city to the Benjamin Mkapa National Stadium in Temeke, which was built with Chinese aid in 2005. The torch was lit by Vice-President Ali Mohamed Shein. About a thousand people followed the relay, waving the Olympic flag. The only noted instance of protest was Nobel Peace Prize laureate Wangari Maathai's withdrawal from the list of torchbearers, in protest against human rights abuses in Tibet.

 Sultanate of Oman: Muscat was the torch's only stop in the Middle East, on April 14. The relay covered 20 km. No protests or incidents were reported. One of the torchbearers was Syrian actress Sulaf Fawakherji.

 Pakistan: The Olympic torch reached Islamabad for the first time ever on April 16. President Pervez Musharraf and Prime Minister Yousaf Raza Gillani spoke at the opening ceremony of the relay. Security was high, for what one newspaper called the "most sensitive leg" of the torch's Olympic journey. The relay was initially supposed to carry the torch around Islamabad, but the entire relay was cancelled due to security concerns regarding "militant threats or anti-China protests", and replaced by an indoors ceremony with the torch carried around the track of Jinnah Stadium. In fear of violent protests and bomb attacks, the torch relay in Pakistan took place in a stadium behind closed doors. Although the relay was behind closed doors, thousands of policemen and soldiers guarded the flame. As a consequence, no incidents arose.

 India: Due to concerns about pro-Tibet protests, the relay through New Delhi on April 17 was cut to just 2.3 km (less than 1.5 miles), which was shared amongst 70 runners. It concluded at the India Gate. The event was peaceful due to the public not being allowed at the relay. A total of five intended torchbearers -Kiran Bedi, Soha Ali Khan, Sachin Tendulkar, Bhaichung Bhutia and Sunil Gavaskar- withdrew from the event, citing "personal reasons", or, in Bhutia's case, explicitly wishing to "stand by the people of Tibet and their struggle" and protest against the PRC "crackdown" in Tibet. Indian national football captain, Baichung Bhutia refused to take part in the Indian leg of the torch relay, citing concerns over Tibet. Bhutia, who is Sikkimese, is the first athlete to refuse to run with the torch. Indian film star Aamir Khan states on his personal blog that the "Olympic Games do not belong to China" and confirms taking part in the torch relay "with a prayer in his heart for the people of Tibet, and ... for all people across the world who are victims of human rights violations". Rahul Gandhi, son of the Congress President Sonia Gandhi and scion of the Nehru-Gandhi family, also refused to carry the torch.
Wary of protests, the Indian authorities have decided to shorten the route of the relay in New Delhi, and have given it the security normally associated with Republic Day celebrations, which are considered terrorist targets. Chinese intelligence's expectations of points on the relay route that would be particularly 'vulnerable' to protesters were presented to the Indian ambassador to Beijing, Nirupama Sen. The Indian media responded angrily to the news that the ambassador, a distinguished lady diplomat, was summoned to the Foreign Ministry at 2 am local time; the news was later denied by anonymous sources in Delhi. The Indian media reported that India's Commerce Minister, Kamal Nath, cancelled an official trip to Beijing in protest, though both Nath and Chinese sources have denied it.
India rejected Chinese demands that the torch route be clear of India's 150,000-strong Tibetan exile community, by which they required a ban on congregation near the curtailed 3 km route. In response Indian officials said India was a democracy, and "a wholesale ban on protests was out of the question". Contradicting some other reports, Indian officials also refused permission to the "Olympic Holy Flame Protection Unit". The combined effect is a "rapid deterioration" of relations between India and China. Meanwhile, the Tibetan government in exile, which is based in India, has stated that it did not support the disruption of the Olympic torch relay.
The noted Indian social activist and a retired Indian Police Service (IPS) officer Kiran Bedi refused to participate saying "she doesn’t want to run in the event as ‘caged woman’." On April 15, Bollywood actress Soha Ali Khan pulled out of the Olympic torch relay, citing “very strong personal reasons”. On April 16, a protest was organised in Delhi "against Chinese repression in Tibet", and was broken up by the police.

 Thailand: The April 18 relay through Bangkok was the Olympic flame's first visit to Thailand. The relay covered just over 10 km, and included Bangkok's Chinatown. The torch was carried past Democracy Monument, Chitralada Palace and a number of other city landmarks. M.R. Narisa Chakrabongse, Green World Foundation (GWF) chairwoman, withdrew from the torch-running ceremony, protesting against China's actions in Tibet. Several hundred protesters were present, along with Olympic supporters. Thai authorities threatened to arrest foreign protesters and ban them from future entry into Thailand. A coalition of Thai human rights groups announced that it would organise a "small demonstration" during the relay, and several hundred people did indeed take part in protests, facing Beijing supporters. Intended torchbearer Mom Rajawongse Narissara Chakrabongse boycotted the relay, to protest against China's actions in Tibet. In Bangkok, students told the media that the Chinese Embassy provided them with transportation and gave them shirts to wear.

 Malaysia: The event was held in the capital city, Kuala Lumpur, on April 21. The 16.5 km long-relay began from the historic Independence Square, passed in front of several city landmarks before coming to an end at the iconic Petronas Twin Towers. Among the landmarks the Olympic flame passed next to were the Parliament House, National Mosque, KL Tower and Merdeka Stadium. A team of 1000 personnel from the Malaysian police Special Action Squad guarded the event and escorted the torchbearers. The last time an Olympic torch relay was held in Malaysia was the 1964 Tokyo edition.
Just days before the relay supporters of Falun Gong demonstrated in front of the Chinese embassy in the Malaysian capital. As many as 1,000 personnel from the special police unit were expected to be deployed on the day of the relay. A Japanese family with Malaysian citizenship and their 5-year-old child who unfurled a Tibetan flag were hit by a group of Chinese nationals with plastic air-filled batons and heckled by a crowd of Chinese citizens during the confrontation at Independence Square where the relay began, and the Chinese group shouted: "Taiwan and Tibet belong to China." Later during the day, the Chinese volunteers forcefully took away placards from two other Malaysians protesting at the relay. One of the protesting Malaysian was hit in the head.

 Indonesia: The Olympic flame reached Jakarta on April 22. The original 20 km relay through Jakarta was cancelled due to "security worries", at the request of the Chinese embassy, and the torch was instead carried round the city main's stadium, as it had been in Islamabad. Several dozen pro-Tibet protesters gathered near the stadium, and were dispersed by the police. The event was held in the streets around the city main's stadium. The cancelling of the relay through the city itself was decided due to security concerns and at the request of the Chinese embassy. Only invitees and journalists were admitted inside the stadium. Protests took place outside the stadium.

 Australia: The event was held in Canberra, Australian Capital Territory on April 24, and covered around 16 km of Canberra's central areas, from Reconciliation Place to Commonwealth Park. Upon its arrival in Canberra, the Olympic flame was presented by Chinese officials to local Aboriginal elder Agnes Shea, of the Ngunnawal people. She, in turn, offered them a message stick, as a gift of peace and welcome. Hundreds of pro-Tibet protesters and thousands of Chinese students reportedly attended. Demonstrators and counter-demonstrators were kept apart by the Australian Federal Police. Preparations for the event were marred by a disagreement over the role of the Chinese flame attendants, with Australian and Chinese officials arguing publicly over their function and prerogatives during a press conference.
Following the events in Olympia, there were reports that China requested permission to deploy People's Liberation Army personnel along the relay route to protect the flame in Canberra. Australian authorities stated that such a request, if it were to be made, would be refused. Chinese officials labeled it a rumor. Australian police have been given powers to search relay spectators, following a call by the Chinese Students and Scholars Association for Chinese Australian students to "go defend our sacred torch" against "ethnic degenerate scum and anti-China separatists". Tony Goh, chairman of the Australian Council of Chinese Organisations, has said the ACCO would be taking "thousands" of pro-Beijing demonstrators to Canberra by bus, to support the torch relay. Zhang Rongan, a Chinese Australian student organising pro-Beijing demonstrations, told the press that Chinese diplomats were assisting with the organization of buses, meals and accommodation for pro-Beijing demonstrators, and helping them organise a "peaceful show of strength". Foreign Minister Stephen Smith said Chinese officials were urging supporters to "turn up and put a point of view", but that he had no objection to it as long as they remained peaceful.
Intended torchbearer Lin Hatfield Dodds withdrew from the event, explaining that she wished to express concern about China's human rights record. Foreign Minister Stephen Smith said her decision was "a very good example of peacefully making a point".
Up to 600 pro-Tibet protesters were expected to attend the relay, along with between 2,000 and 10,000 Chinese supporters. Taking note of the high number of Chinese supporters, Ted Quinlan, head of the Canberra torch relay committee, said: "We didn't expect this reaction from the Chinese community. It is obviously a well-coordinated plan to take the day by weight of numbers. But we have assurances that it will be done peacefully.". Also, Australia's ACT Chief Minister, Jon Stanhope confirmed that the Chinese embassy was closely involve to ensure that "pro-China demonstrators vastly outnumbered Tibetan activists." Australian freestyle swimmer and five-time Olympic gold medalist Ian Thorpe ended the Australian leg of the torch relay April 24, 2008, touching the flame to light a cauldron after a run that was only marginally marked by protests. People demonstrated both for China and for Tibet. At least five people were arrested during the torch relay. Police said "the five were arrested for interfering with the event under special powers enacted in the wake of massive protests against Chinese policy toward Tibet." At one point, groups of Chinese students surrounded and intimidated pro-Tibet protesters. One person had to be pulled aboard a police launch when a group of pro-Chinese students looked like they might force him into the lake.

 Japan: The event was held in Nagano, which hosted the 1998 Winter Olympics, on April 26. Japanese Buddhist temple Zenkō-ji, which was originally scheduled to be the starting point for the Olympic torch relay in Nagano, refused to host the torch and pulled out of the relay plans, amid speculation that monks there sympathized with anti-Chinese government protesters. as well as the risk of disruption by violent protests. Parts of Zenkō-ji temple's main building (Zenkō-ji Hondō), reconstructed in 1707 and one of the National Treasures of Japan, was then vandalized with spraypaint. A new starting point, previously the site of a municipal building and now a parking lot, was chosen by the city. An event the city had planned to hold at the Minami Nagano Sports Park following the torch relay was also canceled out of concern about disruptions caused by demonstrators protesting against China's recent crackdown in Tibet. Thousands of riot police were mobilized to protect the torch along its route. The show of force kept most protesters in check, but slogans shouted by pro-China or pro-Tibet demonstrators, Japanese nationalists, and human rights organizations flooded the air. Five men were arrested and four injured amidst scenes of mob violence. The torch route was packed with mostly peaceful demonstrators. The public was not allowed at the parking lot where the relay started. After the Zenkoji monks held a prayer ceremony for victims of the recent events in Tibet. More than 100 police officers ran with the torch and riot police lined the streets while three helicopters flew above. Only two Chinese guards were allowed to accompany the torch because of Japan's concern over their treatment of demonstrators at previous relays. A man with a Tibetan flag tried to stop the torch at the beginning of the relay but was dragged off by police. Some raw eggs were also thrown from the crowd.
 South Korea: The event was held in Seoul, which hosted the 1988 Summer Olympics, on April 27. Intended torchbearers Choi Seung-kook and Park Won-sun boycotted the event to protest against the Chinese government's crackdown in Tibet. More than 8,000 riot police were deployed to guard the 24-kilometre route, which began at Olympic Park, which was built when Seoul hosted the 1988 Summer Games. On the day of the torch relay in Seoul, Chinese students clashed with protesters, throwing rocks, bottles, and punches. A North Korean defector whose brother defected to China but was captured and executed by the DPRK, attempted to set himself on fire in protest of China's treatment of North Korean refugees. He poured gasoline on himself but police quickly surrounded him and carried him away. Two other demonstrators tried to storm the torch but failed. Fighting broke out near the beginning of the relay between a group of 500 Chinese supporters and approximately 50 protesters who carried a banner that read: "Free North Korean refugees in China." The students threw stones and water bottles as approximately 2,500 police tried to keep the groups separated. Police said they arrested five people, including a Chinese student who was arrested for allegedly throwing rocks. Thousands of Chinese followed the torch on its 4.5 hour journey, some chanting, "Go China, go Olympics!" By the end of the relay, Chinese students became violent, and it was reported in Korean media that they were "lynching" everyone who was disagreeing with them. One police man was also rushed to hospital after being attacked by Chinese students. On Apr 29, the Secretary of Justice, Kim Kyung Han, told the prime minister that he will find "every single Chinese who was involved and bring them to justice." Later in the day, South Korea's Prosecutor's Office, National Police Agency, Ministry of Foreign Affairs and National Intelligence Service made a joint statement saying that they will be deporting every Chinese student that was involved in the incident. China defended the conduct of the students.

 North Korea: The event was held in Pyongyang on April 28. It was the first time that the Olympic torch has traveled to North Korea. A crowd of thousands waving pink paper flowers and small flags with the Beijing Olympics logo were organized by the authoritarian regime watched the beginning of the relay in Pyongyang, some waving Chinese flags. The event was presided over by the head of the country's parliament, Kim Yong Nam. The North, an ally of China, has been critical of disruptions to the torch relay elsewhere and has supported Beijing in its actions against protests in Tibet. Kim passed the torch to the first runner Pak Du Ik, who played on North Korea's 1966 World Cup soccer team, as he began the 19-kilometre route through Pyongyang. The relay began from the large sculpted flame of the obelisk of the Juche Tower, which commemorates the national ideology of Juche, or "self-reliance", created by the country's late founding President Kim Il Sung, father of leader Kim Jong Il, who did not attend.
The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt. "It was unconscionable," said a UN official who was briefed on the arguments. North Korea is frequently listed among the world’s worst offenders against human rights.
 Vietnam: The event was held in Ho Chi Minh City on April 29. Some 60 torchbearers carried the torch from the downtown Opera House to the Military Zone 7 Competition Hall stadium near Tan Son Nhat International Airport along an undisclosed route. Vietnam is involved in a territorial dispute with China (and other countries) for sovereignty of the Spratly and Paracel Islands; tensions have risen recently[when?] following reports that the Chinese government had established a county-level city named Sansha in the disputed territories, resulting in anti-Chinese demonstrations in December 2007 in Hanoi and Ho Chi Minh City. However to sustain its relationship with China the Vietnamese government has actively sought to head off protests during the torch relay, with Prime Minister Nguyễn Tấn Dũng warning government agencies that "hostile forces" may try to disrupt the torch relay.
Prior to the rally, seven anti-China protestors were arrested in Hanoi after unfurling a banner and shouting "Boycott the Beijing Olympics" through a loudhailer at a market. A Vietnamese American was deported for planning protests against the torch, while a prominent blogger, Điếu Cày (real name Nguyễn Văn Hải), who blogged about protests around the world and who called for demonstrations in Vietnam, was arrested on charges of tax evasion. Outside Vietnam, there were protests by overseas Vietnamese in Paris, San Francisco and Canberra. Lê Minh Phiếu, a torchbearer who is a Vietnamese law student studying in France, wrote a letter to the president of the International Olympic Committee protesting China's "politicisation of the Olympics", citing maps of the torch relay at the official Beijing Olympic website depicting the disputed islands as Chinese territory and posted it on his blog. One day before the relay was to start, the official website appeared to have been updated to remove the disputed islands and dotted lines marking China's maritime claims in the South China Sea.
 Hong Kong: The event was held in Hong Kong on May 2. In the ceremony held at the Hong Kong Cultural Centre in Tsim Sha Tsui, Chief Executive Donald Tsang handed the torch to the first torchbearer, Olympic medalist Lee Lai Shan. The torch relay then traveled through Nathan Road, Lantau Link, Sha Tin (crossed Shing Mun River via a dragon boat, which had been never used before in the history of Olympic torch relays), Victoria Harbour (crossed by Tin Hau, a VIP vessel managed by the Marine Department) before ending in Golden Bauhinia Square in Wan Chai. A total of 120 torchbearers were selected to participate in the event consisting of celebrities, athletes and pro-Beijing camp politicians. No politicians from the pro-democracy camp were selected as torchbearers. One torchbearer could not participate due to flight delay. It was estimated that more than 200,000 spectators came out and watched the relay. Many enthusiastic supporters wore red shirts and waved large Chinese flags. According to Hong Kong Chief Secretary for Administration Henry Tang, 3,000 police were deployed to ensure order.
There were several protests along the torch relay route. Members of the Hong Kong Alliance in Support of Patriotic Democratic Movements in China, including pro-democracy activist Szeto Wah, waved novelty inflatable plastic Olympic flames, which they said symbolised democracy. They wanted accountability for the Tiananmen Square protests of 1989 and the implementation of democracy in Hong Kong. Political activist and Legislative Council member Leung Kwok-hung (Longhair) also joined the protest, saying "I'm very proud that in Hong Kong we still have people brave enough to speak out." Pro-democracy activists were overwhelmed by a crowd of torch supporters with insults like "running dog," "traitor," "get out!," and "I love the Communist Party." At the same time, about 10 members of the Civil Human Rights Front had orange banners calling for human rights improvements and universal suffrage. Onlookers were saying "Aren't you Chinese?" in Mandarin putonghua as they tried to cover the orange banners with a large Chinese national flag. One woman had an orange sign that said, "Olympic flame for democracy", while a man carried a poster with a tank and the slogan "One world, two dreams". A university student and former RDHK radio host Christina Chan wrapped the Tibetan snow lion flag around her body and later began waving it. Several onlookers heckled Chan, shouting "What kind of Chinese are you?" and "What a shame!" In the end, she and some of the protesters were taken away against their will by the authorities via a police vehicle "for their own protection." Chan is currently[when?] suing the Hong Kong government, claiming her human rights were breached. (case number HCAL139/08)
The Color Orange democracy group, led by Danish sculptor Jens Galschiøt, originally planned to join the Hong Kong Alliance relay and paint the "Pillar of Shame", a structure he built in Hong Kong to commemorate the 1989 Tiananmen Square protests. However, Galschiøt and two other people were denied entry to Hong Kong on April 26, 2008 due to "immigration reasons" and were forced to leave Hong Kong. In response, Lee Cheuk Yan, vice chairman of the Hong Kong Alliance in Support of Patriotic Democratic Movements in China, said, "It's outrageous that the government is willing to sacrifice the image of Hong Kong because of the torch relay." Hollywood actress Mia Farrow was also briefly questioned at the Hong Kong airport though officials allowed her to enter. She later gave a speech criticizing China's relations with Sudan in Hong Kong, as there was also a small minority of people protesting about China's role in the crisis of Darfur. Legislator Cheung Man Kwong have also said the government's decision allowing Farrow to enter while denying others is a double standard and a violation to Hong Kong's one country, two systems policy.
 Macao: The event was held in Macau on May 3. It was the first time that the Olympic torch had traveled to Macau. A ceremony was held at Macau Fisherman's Wharf. Afterward, the torch traveled through Macau, passing by a number of landmarks including A-Ma Temple, Macau Tower, Ponte Governador Nobre de Carvalho, Ponte de Sai Van, Macau Cultural Centre, Macau Stadium and then back to the Fisherman's Wharf for the closing ceremony. Parts of the route near Ruins of St. Paul's and Taipa was shortened due to large crowds of supporters blocking narrow streets. A total of 120 torchbearers participated in this event including casino tycoon Stanley Ho. Leong Hong Man and Leong Heng Teng were the first and last torchbearer in the relay respectively. An article published on Macao Daily News criticized that the list of the torchbearers could not fully represent the Macanese and that there were too many non-athletes among the torchbearers. (some of whom had already been torchbearers of other sporting events)
A Macau resident was arrested on April 26 for posting a message on cyberctm.com encouraging people to disrupt the relay. Both orchidbbs.com and cyberctm.com Internet forums were shut down from May 2 to 4. This fueled speculation that the shutdowns were targeting speeches against the relay. The head of the Bureau of Telecommunications Regulation has denied that the shutdowns of the websites were politically motivated. About 2,200 police were deployed on the streets, there were no interruptions.
 China: The torch returned to China for the first time since April. The torch arrived in Sanya, Hainan on May 4 with celebrations attended by International Olympic Committee (IOC) officials and Chinese big names like Jackie Chan. The entire relay through Mainland China was largely a success with many people welcoming the arrival of the torch along the way.
The coverage of the events by the media came under scrutiny during the relay. Chinese media coverage of the torch relay has been distinct in a number of ways from coverage elsewhere. Western reporters in Beijing have described Chinese media coverage as partial and censored (for example when Chinese media did not broadcast Reporters Without Borders' disruption of the torch lighting ceremony), whereas Chinese netizens have in turn accused Western media coverage of being biased. The French newspaper Libération was criticised by the Chinese State press agency Xinhua for its allegedly biased reporting; Xinhua suggested that Libération needed "a stinging slap in the face" for having "insulted the Olympic flame" and "supported a handful of saboteurs".
In response to pro-Tibet and pro-human rights protests, the Chinese media focused on the more disruptive protesters, referring for example to "a very small number of 'Tibet independence' secessionists and a handful of so-called human rights-minded NGO activists" intent on "disrupting and sabotaging the Beijing Olympic Games". However, the Chinese media published articles about crowds supporting the torch relay.
Xinhua and CCTV quoted relay spectators who condemned the protests, to a greater extent than most Western media, but did not quote any alternate viewpoints, providing no coverage of support for the protests by some ordinary citizens in Western countries. It quoted athletes who expressed pride at taking part in the relays, to a greater extent than Western media, but not those who, like Marie-José Pérec, expressed understanding and support for the protestors. The Beijing Organising Committee for the Games mentioned the "smiling faces of the elderly, children and the artists on the streets", of cheering and supportive Londoners. Xinhua said that protesters were "radicals" who "trampled human rights" and whose activities were condemned by "the people of the world who cordially love the Olympic spirit".
Reports on the Delhi relay were similarly distinct. Despite intended torchbearers Kiran Bedi, Soha Ali Khan, Sachin Tendulkar and Bhaichung Bhutia all withdrawing from the event, the official Chinese website for the relay reported "Indian torchbearers vow to run for spirit of Olympics", and quoted torchbearers Manavjit Singh Sandhu, Abhinav Bindra, Ayaan Ali Khan and Rajinder Singh Rahelu all stating that sports and politics should not be mixed.
Some Western media have reported on Chinese accusations of Western media bias. The Daily Telegraph published an opinion piece by the Chinese ambassador to the United Kingdom, Fu Ying, who accused Western media of "demonising" China during their coverage of the torch relays. The Telegraph also asked its readers to send their views in response to the question "Is the West demonising China?" The BBC reported on a demonstration in Sydney by Chinese Australians "voicing support for Beijing amid controversy over Tibet" and protesting against what they saw as Western media bias. The report showed demonstrators carrying signs which read "Shame on some Western media", "BBC CNN lies too" and "Stop media distortion!". One demonstrator interviewed by the BBC stated: "I saw some news from CNN, from the BBC, some media [inaudible], and they are just lying." Libération also reported that it had been accused of bias by the Chinese media.
On April 17, Xinhua condemned what it called "biased coverage of the Lhasa riots and the Olympic torch relay by the U.S.-based Cable News Network (CNN)". The same day, the Chinese government called on CNN to "apologise" for having allegedly insulted the Chinese people, and for "attempting to incite the Chinese people against the government". CNN issued a statement on April 14, responded to China over 'thugs and goons' comment by Jack Cafferty.
On April 19, the BBC reported that 1,300 people had gathered outside BBC buildings in Manchester and London, protesting against what they described as Western media bias. Several days earlier, the BBC had published an article entitled "The challenges of reporting in China", responding to earlier criticism. The BBC's Paul Danahar noted that Chinese people were now "able to access the BBC News website for the first time, after years of strict censorship", and that "many were critical of our coverage". He provided readers with a reminder of censorship in China, and added: "People who criticise the media for their coverage in Tibet should acknowledge that we were and still are banned from reporting there." He also quoted critical Chinese responses, and invited readers to comment.
On April 20, the People's Daily published a report entitled "Overseas Chinese rally against biased media coverage, for Olympics". It included images of Chinese people demonstrating in France, the United Kingdom, Germany and the United States. One picture showed Chinese demonstrators holding a sign which claimed, incorrectly, that the BBC had not reported on Jin Jing. The People's Daily quoted one protestor who claimed the "BBC on some of the recent events has misled the British public and the rest of the world by providing intensive untruthful reports and biased coverage."
On April 4, it was reported that the Chinese government appeared to be running an anti-CNN website that criticizes the cable network’s coverage of recent events. The site claims to have been created by a Beijing citizen. However, foreign correspondents in Beijing voiced suspicions that Anti-cnn may be a semi-government-made website. A Chinese government spokesman insisted the site was spontaneously set up by a Chinese citizen angered over media coverage.
The Beijing Olympic Organizing Committee sent out a team of 30 unarmed attendants selected from the People's Armed Police to escort the flame throughout its journey. According to Asian Times, sworn in as the "Beijing Olympic Games Sacred Flame Protection Unit" during a ceremony in August 2007, their main job is to keep the Olympic flame alight throughout the journey and to assist in transferring the flame between the torches, the lanterns and the cauldrons. They wear matching blue tracksuits and are intended to accompany the torch every step of the way. One of the torch attendants, dubbed "Second Right Brother," has developed a significant online fan-base, particularly among China's female netizens.
In China, a call to boycott French hypermart Carrefour from May 1 began spreading through mobile text messaging and online chat rooms amongst the Chinese over the weekend from April 12, accusing the company's major shareholder, the LVMH Group, of donating funds to the Dalai Lama. There were also calls to extend the boycott to include French luxury goods and cosmetic products. Chinese protesters organized boycotts of the French-owned retail chain Carrefour in major Chinese cities including Kunming, Hefei and Wuhan, accusing the French nation of pro-secessionist conspiracy and anti-Chinese racism. Some burned French flags, some added Swastika (due to its conotaions with Nazism) to the French flag, and spread short online messages calling for large protests in front of French consulates and embassy. Some shoppers who insisted on entering one of the Carrefour stores in Kunming were blocked by boycotters wielding large Chinese flags and hit by water bottles. Hundreds of people joined Anti-French rallies in Beijing, Wuhan, Hefei, Kunming and Qingdao, which quickly spread to other cities like Xi'an, Harbin and Jinan. Carrefour denied any support or involvement in the Tibetan issue, and had its staff in its Chinese stores wear uniforms emblazoned with the Chinese national flag and caps with Olympic insignia and as well as the words "Beijing 2008" to show its support for the games. The effort had to be ceased when the BOCOG deemed the use of official Olympic insignia as illegal and a violation of copyright.
In response to the demonstrations, the Chinese government attempted to calm the situation, possibly fearing the protests may spiral out of control as has happened in recent years, including the anti-Japanese protests in 2005. State media and commentaries began to call for calm, such as an editorial in the People's Daily which urged Chinese people to "express [their] patriotic enthusiasm calmly and rationally, and express patriotic aspiration in an orderly and legal manner". The government also began to patrol and censor the internet forums such as Sohu.com, with comments related to the Carrefour boycott removed. In the days prior to the planned boycott, evidence of efforts by Chinese authorities to choke the mass boycott's efforts online became even more evident, including barring searches of words related to the French protests, but protests broke out nonetheless in front of Carrefour's stores at Beijing, Changsha, Fuzhou and Shenyang on May 1.
In Japan, the Mayor of Nagano, Shoichi Washizawa said that it has become a "great nuisance" for the city to host the torch relay prior to the Nagano leg. Washizawa's aides said the mayor's remark was not criticism about the relay itself but about the potential disruptions and confusion surrounding it.　A city employee of the Nagano City Office ridiculed the protests in Europe, he said "They are doing something foolish", in a televised interview. Nagano City officially apologized later and explained what he had wanted to say was "Such violent protests were not easy to accept". Also citing concerns about protests as well as the recent violence in Tibet, a major Buddhist temple in Nagano cancelled its plans to host the opening stage of the Olympic torch relay, this temple was vandalised by an un-identified person the day after in apparent revenge,
The Olympic Flame is supposed to remain lit for the whole relay. When the Torch is extinguished at night, on airplanes, in bad weather, or during protests (such as the several occasions in Paris), the Olympic Flame is kept alight in a set of 8 lanterns.[citation needed]
A union planned to protest at the relay for better living conditions. Hong Kong legislator Michael Mak Kwok-fung and activist Chan Cheong, both members of the League of Social Democrats, were not allowed to enter Macau.
Chinese media have also reported on Jin Jing, whom the official Chinese torch relay website described as "heroic" and an "angel", whereas Western media initially gave her little mention – despite a Chinese claim that "Chinese Paralympic athlete Jin Jing has garnered much attention from the media".
Two additional teams of 40 attendants each will accompany the flame on its Mainland China route. This arrangement has however sparked several controversies.
Georgian architecture is the name given in most English-speaking countries to the set of architectural styles current between 1714 and 1830. It is eponymous for the first four British monarchs of the House of Hanover—George I, George II, George III, and George IV—who reigned in continuous succession from August 1714 to June 1830. The style was revived in the late 19th century in the United States as Colonial Revival architecture and in the early 20th century in Great Britain as Neo-Georgian architecture; in both it is also called Georgian Revival architecture. In America the term "Georgian" is generally used to describe all building from the period, regardless of style; in Britain it is generally restricted to buildings that are "architectural in intention", and have stylistic characteristics that are typical of the period, though that covers a wide range.
The style of Georgian buildings is very variable, but marked by a taste for symmetry and proportion based on the classical architecture of Greece and Rome, as revived in Renaissance architecture. Ornament is also normally in the classical tradition, but typically rather restrained, and sometimes almost completely absent on the exterior. The period brought the vocabulary of classical architecture to smaller and more modest buildings than had been the case before, replacing English vernacular architecture (or becoming the new vernacular style) for almost all new middle-class homes and public buildings by the end of the period.
In towns, which expanded greatly during the period, landowners turned into property developers, and rows of identical terraced houses became the norm. Even the wealthy were persuaded to live in these in town, especially if provided with a square of garden in front of the house. There was an enormous amount of building in the period, all over the English-speaking world, and the standards of construction were generally high. Where they have not been demolished, large numbers of Georgian buildings have survived two centuries or more, and they still form large parts of the core of cities such as London, Edinburgh, Dublin and Bristol.
The period saw the growth of a distinct and trained architectural profession; before the mid-century "the high-sounding title, 'architect' was adopted by anyone who could get away with it". But most buildings were still designed by builders and landlords together, and the wide spread of Georgian architecture, and the Georgian styles of design more generally, came from dissemination through pattern books and inexpensive suites of engravings. This contrasted with earlier styles, which were primarily disseminated among craftsmen through the direct experience of the apprenticeship system. Authors such as the prolific William Halfpenny (active 1723–1755) received editions in America as well as Britain. From the mid-18th century, Georgian styles were assimilated into an architectural vernacular that became part and parcel of the training of every architect, designer, builder, carpenter, mason and plasterer, from Edinburgh to Maryland.
Georgian succeeded the English Baroque of Sir Christopher Wren, Sir John Vanbrugh, Thomas Archer, William Talman, and Nicholas Hawksmoor; this in fact continued into at least the 1720s, overlapping with a more restrained Georgian style. The architect James Gibbs was a transitional figure, his earlier buildings are Baroque, reflecting the time he spent in Rome in the early 18th century, but he adjusted his style after 1720. Major architects to promote the change in direction from baroque were Colen Campbell, author of the influential book Vitruvius Britannicus (1715-1725); Richard Boyle, 3rd Earl of Burlington and his protégé William Kent; Isaac Ware; Henry Flitcroft and the Venetian Giacomo Leoni, who spent most of his career in England. Other prominent architects of the early Georgian period include James Paine, Robert Taylor, and John Wood, the Elder. The European Grand Tour became very common for wealthy patrons in the period, and Italian influence remained dominant, though at the start of the period Hanover Square, Westminster (1713 on), developed and occupied by Whig supporters of the new dynasty, seems to have deliberately adopted German stylisic elements in their honour, especially vertical bands connecting the windows.
The styles that resulted fall within several categories. In the mainstream of Georgian style were both Palladian architecture— and its whimsical alternatives, Gothic and Chinoiserie, which were the English-speaking world's equivalent of European Rococo. From the mid-1760s a range of Neoclassical modes were fashionable, associated with the British architects Robert Adam, James Gibbs, Sir William Chambers, James Wyatt, George Dance the Younger, Henry Holland and Sir John Soane. John Nash was one of the most prolific architects of the late Georgian era known as The Regency style, he was responsible for designing large areas of London. Greek Revival architecture was added to the repertory, beginning around 1750, but increasing in popularity after 1800. Leading exponents were William Wilkins and Robert Smirke.
Georgian architecture is characterized by its proportion and balance; simple mathematical ratios were used to determine the height of a window in relation to its width or the shape of a room as a double cube. Regularity, as with ashlar (uniformly cut) stonework, was strongly approved, imbuing symmetry and adherence to classical rules: the lack of symmetry, where Georgian additions were added to earlier structures remaining visible, was deeply felt as a flaw, at least before Nash began to introduce it in a variety of styles. Regularity of housefronts along a street was a desirable feature of Georgian town planning. Until the start of the Gothic Revival in the early 19th century, Georgian designs usually lay within the Classical orders of architecture and employed a decorative vocabulary derived from ancient Rome or Greece.
Versions of revived Palladian architecture dominated English country house architecture. Houses were increasingly placed in grand landscaped settings, and large houses were generally made wide and relatively shallow, largely to look more impressive from a distance. The height was usually highest in the centre, and the Baroque emphasis on corner pavilions often found on the continent generally avoided. In grand houses, an entrance hall led to steps up to a piano nobile or mezzanine floor where the main reception rooms were. Typically the basement area or "rustic", with kitchens, offices and service areas, as well as male guests with muddy boots, came some way above ground, and was lit by windows that were high on the inside, but just above ground level outside. A single block was typical, with a perhaps a small court for carriages at the front marked off by railings and a gate, but rarely a stone gatehouse, or side wings around the court.
Windows in all types of buildings were large and regularly placed on a grid; this was partly to minimize window tax, which was in force throughout the period in the United Kingdom. Some windows were subsequently bricked-in. Their height increasingly varied between the floors, and they increasingly began below waist-height in the main rooms, making a small balcony desirable. Before this the internal plan and function of the rooms can generally not be deduced from the outside. To open these large windows the sash window, already developed by the 1670s, became very widespread. Corridor plans became universal inside larger houses.
Internal courtyards became more rare, except beside the stables, and the functional parts of the building were placed at the sides, or in separate buildings nearby hidden by trees. The views to and from the front and rear of the main block were concentrated on, with the side approaches usually much less important. The roof was typically invisible from the ground, though domes were sometimes visible in grander buildings. The roofline was generally clear of ornament except for a balustrade or the top of a pediment. Columns or pilasters, often topped by a pediment, were popular for ornament inside and out, and other ornament was generally geometrical or plant-based, rather than using the human figure.
Inside ornament was far more generous, and could sometimes be overwhelming. The chimneypiece continued to be the usual main focus of rooms, and was now given a classical treatment, and increasingly topped by a painting or a mirror. Plasterwork ceilings, carved wood, and bold schemes of wallpaint formed a backdrop to increasingly rich collections of furniture, paintings, porcelain, mirrors, and objets d'art of all kinds. Wood-panelling, very common since about 1500, fell from favour around the mid-century, and wallpaper included very expensive imports from China.
In towns even most better-off people lived in terraced houses, which typically opened straight onto the street, often with a few steps up to the door. There was often an open space, protected by iron railings, dropping down to the basement level, with a discreet entrance down steps off the street for servants and deliveries; this is known as the "area". This meant that the ground floor front was now removed and protected from the street and encouraged the main reception rooms to move there from the floor above. Where, as often, a new street or set of streets was developed, the road and pavements were raised up, and the gardens or yards behind the houses at a lower level, usually representing the original one.
Town terraced houses for all social classes remained resolutely tall and narrow, each dwelling occupying the whole height of the building. This contrasted with well-off continental dwellings, which had already begun to be formed of wide apartments occupying only one or two floors of a building; such arrangements were only typical in England when housing groups of batchelors, as in Oxbridge colleges, the lawyers in the Inns of Court or The Albany after it was converted in 1802. In the period in question, only in Edinburgh were working-class purpose-built tenements common, though lodgers were common in other cities. A curving crescent, often looking out at gardens or a park, was popular for terraces where space allowed. In early and central schemes of development, plots were sold and built on individually, though there was often an attempt to enforce some uniformity, but as development reached further out schemes were increasingly built as a uniform scheme and then sold.
The late Georgian period saw the birth of the semi-detached house, planned systematically, as a suburban compromise between the terraced houses of the city and the detached "villas" further out, where land was cheaper. There had been occasional examples in town centres going back to medieval times. Most early suburban examples are large, and in what are now the outer fringes of Central London, but were then in areas being built up for the first time. Blackheath, Chalk Farm and St John's Wood are among the areas contesting being the original home of the semi. Sir John Summerson gave primacy to the Eyre Estate of St John's Wood. A plan for this exists dated 1794, where "the whole development consists of pairs of semi-detached houses, So far as I know, this is the first recorded scheme of the kind". In fact the French Wars put an end to this scheme, but when the development was finally built it retained the semi-detached form, "a revolution of striking significance and far-reaching effect".
Until the Church Building Act of 1818, the period saw relatively few churches built in Britain, which was already well-supplied, although in the later years of the period the demand for Non-conformist and Roman Catholic places of worship greatly increased. Anglican churches that were built were designed internally to allow maximum audibility, and visibility, for preaching, so the main nave was generally wider and shorter than in medieval plans, and often there were no side-aisles. Galleries were common in new churches. Especially in country parishes, the external appearance generally retained the familiar signifiers of a Gothic church, with a tower or spire, a large west front with one or more doors, and very large windows along the nave, but all with any ornament drawn from the classical vocabulary. Where funds permitted, a classical temple portico with columns and a pediment might be used at the west front. Decoration inside was very limited, but churches filled up with monuments to the prosperous.
Public buildings generally varied between the extremes of plain boxes with grid windows and Italian Late Renaissance palaces, depending on budget. Somerset House in London, designed by Sir William Chambers in 1776 for government offices, was as magnificent as any country house, though never quite finished, as funds ran out. Barracks and other less prestigious buildings could be as functional as the mills and factories that were growing increasingly large by the end of the period. But as the period came to an end many commercial projects were becoming sufficiently large, and well-funded, to become "architectural in intention", rather than having their design left to the lesser class of "surveyors".
Georgian architecture was widely disseminated in the English colonies during the Georgian era. American buildings of the Georgian period were very often constructed of wood with clapboards; even columns were made of timber, framed up, and turned on an over-sized lathe. At the start of the period the difficulties of obtaining and transporting brick or stone made them a common alternative only in the larger cities, or where they were obtainable locally. Dartmouth College, Harvard University, and the College of William and Mary, offer leading examples of Georgian architecture in the Americas.
Unlike the Baroque style that it replaced, which was mostly used for palaces and churches, and had little representation in the British colonies, simpler Georgian styles were widely used by the upper and middle classes. Perhaps the best remaining house is the pristine Hammond-Harwood House (1774) in Annapolis, Maryland, designed by the colonial architect William Buckland and modelled on the Villa Pisani at Montagnana, Italy as depicted in Andrea Palladio's I quattro libri dell'architettura ("Four Books of Architecture").
After about 1840, Georgian conventions were slowly abandoned as a number of revival styles, including Gothic Revival, that had originated in the Georgian period, developed and contested in Victorian architecture, and in the case of Gothic became better researched, and closer to their originals. Neoclassical architecture remained popular, and was the opponent of Gothic in the Battle of the Styles of the early Victorian period. In the United States the Federalist Style contained many elements of Georgian style, but incorporated revolutionary symbols.
In the early decades of the twentieth century when there was a growing nostalgia for its sense of order, the style was revived and adapted and in the United States came to be known as the Colonial Revival. In Canada the United Empire Loyalists embraced Georgian architecture as a sign of their fealty to Britain, and the Georgian style was dominant in the country for most of the first half of the 19th century. The Grange, for example, a manor built in Toronto, was built in 1817. In Montreal, English born architect John Ostell worked on a significant number of remarkable constructions in the Georgian style such as the Old Montreal Custom House and the Grand séminaire de Montréal.
The revived Georgian style that emerged in Britain at the beginning of the 20th century is usually referred to as Neo-Georgian; the work of Edwin Lutyens includes many examples. Versions of the Neo-Georgian style were commonly used in Britain for certain types of urban architecture until the late 1950s, Bradshaw Gass & Hope's Police Headquarters in Salford of 1958 being a good example. In both the United States and Britain, the Georgian style is still employed by architects like Quinlan Terry Julian Bicknell and Fairfax and Sammons for private residences.