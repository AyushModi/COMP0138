The Premier League is a corporation in which the 20 member clubs act as shareholders. Seasons run from August to May. Teams play 38 matches each (playing each team in the league twice, home and away), totalling 380 matches in the season. Most games are played on Saturday and Sunday afternoons; others during weekday evenings. It is currently sponsored by Barclays Bank and thus officially known as the Barclays Premier League and is colloquially known as the Premiership. Outside the UK it is commonly referred to as the English Premier League (EPL).
The competition formed as the FA Premier League on 20 February 1992 following the decision of clubs in the Football League First Division to break away from the Football League, which was originally founded in 1888, and take advantage of a lucrative television rights deal. The deal was worth £1 billion a year domestically as of 2013–14, with BSkyB and BT Group securing the domestic rights to broadcast 116 and 38 games respectively. The league generates €2.2 billion per year in domestic and international television rights. In 2014/15, teams were apportioned revenues of £1.6 billion.
The Premier League is the most-watched football league in the world, broadcast in 212 territories to 643 million homes and a potential TV audience of 4.7 billion people. In the 2014–15 season, the average Premier League match attendance exceeded 36,000, second highest of any professional football league behind the Bundesliga's 43,500. Most stadium occupancies are near capacity. The Premier League rank second in the UEFA coefficients of leagues based on performances in European competitions over the past five seasons.
Despite significant European success during the 1970s and early 1980s, the late '80s had marked a low point for English football. Stadiums were crumbling, supporters endured poor facilities, hooliganism was rife, and English clubs were banned from European competition for five years following the Heysel Stadium disaster in 1985. The Football League First Division, which had been the top level of English football since 1888, was well behind leagues such as Italy's Serie A and Spain's La Liga in attendances and revenues, and several top English players had moved abroad.
However, by the turn of the 1990s the downward trend was starting to reverse; England had been successful in the 1990 FIFA World Cup, reaching the semi-finals. UEFA, European football's governing body, lifted the five-year ban on English clubs playing in European competitions in 1990 (resulting in Manchester United lifting the UEFA Cup Winners' Cup in 1991) and the Taylor Report on stadium safety standards, which proposed expensive upgrades to create all-seater stadiums in the aftermath of the Hillsborough disaster, was published in January of that year.
Television money had also become much more important; the Football League received £6.3 million for a two-year agreement in 1986, but when that deal was renewed in 1988, the price rose to £44 million over four years. The 1988 negotiations were the first signs of a breakaway league; ten clubs threatened to leave and form a "super league", but were eventually persuaded to stay. As stadiums improved and match attendance and revenues rose, the country's top teams again considered leaving the Football League in order to capitalise on the growing influx of money being pumped into the sport.
At the close of the 1991 season, a proposal was tabled for the establishment of a new league that would bring more money into the game overall. The Founder Members Agreement, signed on 17 July 1991 by the game's top-flight clubs, established the basic principles for setting up the FA Premier League. The newly formed top division would have commercial independence from The Football Association and the Football League, giving the FA Premier League licence to negotiate its own broadcast and sponsorship agreements. The argument given at the time was that the extra income would allow English clubs to compete with teams across Europe.
The managing director of London Weekend Television (LWT), Greg Dyke, met with the representatives of the "big five" football clubs in England in 1990. The meeting was to pave the way for a break away from The Football League. Dyke believed that it would be more lucrative for LWT if only the larger clubs in the country were featured on national television and wanted to establish whether the clubs would be interested in a larger share of television rights money. The five clubs decided it was a good idea and decided to press ahead with it; however, the league would have no credibility without the backing of The Football Association and so David Dein of Arsenal held talks to see whether the FA were receptive to the idea. The FA did not enjoy an amicable relationship with the Football League at the time and considered it as a way to weaken the Football League's position.
In 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate. This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted.
The league held its first season in 1992–93 and was originally composed of 22 clubs. The first ever Premier League goal was scored by Brian Deane of Sheffield United in a 2–1 win against Manchester United. The 22 inaugural members of the new Premier League were Arsenal, Aston Villa, Blackburn Rovers, Chelsea, Coventry City, Crystal Palace, Everton, Ipswich Town, Leeds United, Liverpool, Manchester City, Manchester United, Middlesbrough, Norwich City, Nottingham Forest, Oldham Athletic, Queens Park Rangers, Sheffield United, Sheffield Wednesday, Southampton, Tottenham Hotspur, and Wimbledon. Luton Town, Notts County and West Ham United were the three teams relegated from the old first division at the end of the 1991–92 season, and did not take part in the inaugural Premier League season.
One significant feature of the Premier League in the mid-2000s was the dominance of the so-called "Big Four" clubs: Arsenal, Chelsea, Liverpool and Manchester United. During this decade, and particularly from 2002 to 2009, they dominated the top four spots, which came with UEFA Champions League qualification, taking all top four places in 5 out of 6 seasons from 2003–04 to 2008–09 inclusive, with Arsenal going as far as winning the league without losing a single game in 2003–04, the only time it has ever happened in the Premier League. In May 2008 Kevin Keegan stated that "Big Four" dominance threatened the division, "This league is in danger of becoming one of the most boring but great leagues in the world." Premier League chief executive Richard Scudamore said in defence: "There are a lot of different tussles that go on in the Premier League depending on whether you're at the top, in the middle or at the bottom that make it interesting."
The years following 2009 marked a shift in the structure of the "Big Four" with Tottenham Hotspur and Manchester City both breaking into the top four. In the 2009–10 season, Tottenham finished fourth and became the first team to break the top four since Everton in 2005. Criticism of the gap between an elite group of "super clubs" and the majority of the Premier League has continued, nevertheless, due to their increasing ability to spend more than the other Premier League clubs. Manchester City won the title in the 2011–12 season, becoming the first club outside the "Big Four" to win since 1994–95. That season also saw two of the Big Four (Chelsea and Liverpool) finish outside the top four places for the first time since 1994–95.
Due to insistence by the International Federation of Association Football (FIFA), the international governing body of football, that domestic leagues reduce the number of games clubs played, the number of clubs was reduced to 20 in 1995 when four teams were relegated from the league and only two teams promoted. On 8 June 2006, FIFA requested that all major European leagues, including Italy's Serie A and Spain's La Liga be reduced to 18 teams by the start of the 2007–08 season. The Premier League responded by announcing their intention to resist such a reduction. Ultimately, the 2007–08 season kicked off again with 20 teams.
The Football Association Premier League Ltd (FAPL) is operated as a corporation and is owned by the 20 member clubs. Each club is a shareholder, with one vote each on issues such as rule changes and contracts. The clubs elect a chairman, chief executive, and board of directors to oversee the daily operations of the league. The current chairman is Sir Dave Richards, who was appointed in April 1999, and the chief executive is Richard Scudamore, appointed in November 1999. The former chairman and chief executive, John Quinton and Peter Leaver, were forced to resign in March 1999 after awarding consultancy contracts to former Sky executives Sam Chisholm and David Chance. The Football Association is not directly involved in the day-to-day operations of the Premier League, but has veto power as a special shareholder during the election of the chairman and chief executive and when new rules are adopted by the league.
The Premier League sends representatives to UEFA's European Club Association, the number of clubs and the clubs themselves chosen according to UEFA coefficients. For the 2012–13 season the Premier League has 10 representatives in the Association: Arsenal, Aston Villa, Chelsea, Everton, Fulham, Liverpool, Manchester City, Manchester United, Newcastle United and Tottenham Hotspur. The European Club Association is responsible for electing three members to UEFA's Club Competitions Committee, which is involved in the operations of UEFA competitions such as the Champions League and UEFA Europa League.
There are 20 clubs in the Premier League. During the course of a season (from August to May) each club plays the others twice (a double round-robin system), once at their home stadium and once at that of their opponents, for a total of 38 games. Teams receive three points for a win and one point for a draw. No points are awarded for a loss. Teams are ranked by total points, then goal difference, and then goals scored. If still equal, teams are deemed to occupy the same position. If there is a tie for the championship, for relegation, or for qualification to other competitions, a play-off match at a neutral venue decides rank. The three lowest placed teams are relegated into the Football League Championship, and the top two teams from the Championship, together with the winner of play-offs involving the third to sixth placed Championship clubs, are promoted in their place.
The team placed fifth in the Premier League automatically qualifies for the UEFA Europa League, and the sixth and seventh-placed teams can also qualify, depending on the winners of the two domestic cup competitions i.e. the FA Cup and the Capital One Cup (League Cup). Two Europa League places are reserved for the winners of each tournament; if the winner of either the FA Cup or League Cup qualifies for the Champions League, then that place will go to the next-best placed finisher in the Premier League. A further place in the UEFA Europa League is also available via the Fair Play initiative. If the Premier League has one of the three highest Fair Play rankings in Europe, the highest ranked team in the Premier League Fair Play standings which has not already qualified for Europe will automatically qualify for the UEFA Europa League first qualifying round.
An exception to the usual European qualification system happened in 2005, after Liverpool won the Champions League the year before, but did not finish in a Champions League qualification place in the Premier League that season. UEFA gave special dispensation for Liverpool to enter the Champions League, giving England five qualifiers. UEFA subsequently ruled that the defending champions qualify for the competition the following year regardless of their domestic league placing. However, for those leagues with four entrants in the Champions League, this meant that if the Champions League winner finished outside the top four in its domestic league, it would qualify at the expense of the fourth-placed team in the league. No association can have more than four entrants in the Champions League. This occurred in 2012, when Chelsea – who had won the Champions League the previous year, but finished sixth in the league – qualified for the Champions League in place of Tottenham Hotspur, who went into the Europa League.
Between the 1992–93 season and the 2012–13 season, Premier League clubs had won the UEFA Champions League four times (as well as supplying five of the runners-up), behind Spain's La Liga with six wins, and Italy's Serie A with five wins, and ahead of, among others, Germany's Bundesliga with three wins (see table here). The FIFA Club World Cup (or the FIFA Club World Championship, as it was originally called) has been won by Premier league clubs once (Manchester United in 2008), and they have also been runners-up twice, behind Brazil's Brasileirão with four wins, and Spain's La Liga and Italy's Serie A with two wins each (see table here).
The Premier League has the highest revenue of any football league in the world, with total club revenues of €2.48 billion in 2009–10. In 2013–14, due to improved television revenues and cost controls, the Premier League had net profits in excess of £78 million, exceeding all other football leagues. In 2010 the Premier League was awarded the Queen's Award for Enterprise in the International Trade category for its outstanding contribution to international trade and the value it brings to English football and the United Kingdom's broadcasting industry.
In 2011, a Welsh club participated in the Premier League for the first time after Swansea City gained promotion. The first Premier League match to be played outside England was Swansea City's home match at the Liberty Stadium against Wigan Athletic on 20 August 2011. In 2012–13, Swansea qualified for the Europa League by winning the League Cup. The number of Welsh clubs in the Premier League increased to two for the first time in 2013–14, as Cardiff City gained promotion, but Cardiff City was relegated after its maiden season.
Participation in the Premier League by some Scottish or Irish clubs has sometimes been discussed, but without result. The idea came closest to reality in 1998, when Wimbledon received Premier League approval to relocate to Dublin, Ireland, but the move was blocked by the Football Association of Ireland. Additionally, the media occasionally discusses the idea that Scotland's two biggest teams, Celtic and Rangers, should or will take part in the Premier League, but nothing has come of these discussions.
Television has played a major role in the history of the Premier League. The League's decision to assign broadcasting rights to BSkyB in 1992 was at the time a radical decision, but one that has paid off. At the time pay television was an almost untested proposition in the UK market, as was charging fans to watch live televised football. However, a combination of Sky's strategy, the quality of Premier League football and the public's appetite for the game has seen the value of the Premier League's TV rights soar.
The Premier League sells its television rights on a collective basis. This is in contrast to some other European Leagues, including La Liga, in which each club sells its rights individually, leading to a much higher share of the total income going to the top few clubs. The money is divided into three parts: half is divided equally between the clubs; one quarter is awarded on a merit basis based on final league position, the top club getting twenty times as much as the bottom club, and equal steps all the way down the table; the final quarter is paid out as facilities fees for games that are shown on television, with the top clubs generally receiving the largest shares of this. The income from overseas rights is divided equally between the twenty clubs.
The first Sky television rights agreement was worth £304 million over five seasons. The next contract, negotiated to start from the 1997–98 season, rose to £670 million over four seasons. The third contract was a £1.024 billion deal with BSkyB for the three seasons from 2001–02 to 2003–04. The league brought in £320 million from the sale of its international rights for the three-year period from 2004–05 to 2006–07. It sold the rights itself on a territory-by-territory basis. Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available. This occurred following an insistence by the European Commission that exclusive rights should not be sold to one television company. Sky and Setanta paid a total of £1.7 billion, a two-thirds increase which took many commentators by surprise as it had been widely assumed that the value of the rights had levelled off following many years of rapid growth. Setanta also hold rights to a live 3 pm match solely for Irish viewers. The BBC has retained the rights to show highlights for the same three seasons (on Match of the Day) for £171.6 million, a 63 per cent increase on the £105 million it paid for the previous three-year period. Sky and BT have agreed to jointly pay £84.3 million for delayed television rights to 242 games (that is the right to broadcast them in full on television and over the internet) in most cases for a period of 50 hours after 10 pm on matchday. Overseas television rights fetched £625 million, nearly double the previous contract. The total raised from these deals is more than £2.7 billion, giving Premier League clubs an average media income from league games of around £40 million-a-year from 2007 to 2010.
The TV rights agreement between the Premier League and Sky has faced accusations of being a cartel, and a number of court cases have arisen as a result. An investigation by the Office of Fair Trading in 2002 found BSkyB to be dominant within the pay TV sports market, but concluded that there were insufficient grounds for the claim that BSkyB had abused its dominant position. In July 1999 the Premier League's method of selling rights collectively for all member clubs was investigated by the UK Restrictive Practices Court, who concluded that the agreement was not contrary to the public interest.
The BBC's highlights package on Saturday and Sunday nights, as well as other evenings when fixtures justify, will run until 2016. Television rights alone for the period 2010 to 2013 have been purchased for £1.782 billion. On 22 June 2009, due to troubles encountered by Setanta Sports after it failed to meet a final deadline over a £30 million payment to the Premier League, ESPN was awarded two packages of UK rights containing a total of 46 matches that were available for the 2009–10 season as well as a package of 23 matches per season from 2010–11 to 2012–13. On 13 June 2012, the Premier League announced that BT had been awarded 38 games a season for the 2013–14 through 2015–16 seasons at £246 million-a-year. The remaining 116 games were retained by Sky who paid £760 million-a-year. The total domestic rights have raised £3.018 billion, an increase of 70.2% over the 2010–11 to 2012–13 rights. The value of the licensing deal rose by another 70.2% in 2015, when Sky and BT paid a total of £5.136 billion to renew their contracts with the Premier League for another three years up to the 2018–19 season.
The Premier League is particularly popular in Asia, where it is the most widely distributed sports programme. In Australia, Fox Sports broadcasts almost all of the season's 380 matches live, and Foxtel gives subscribers the option of selecting which Saturday 3pm match to watch. In India, the matches are broadcast live on STAR Sports. In China, the broadcast rights were awarded to Super Sports in a six-year agreement that began in the 2013–14 season. As of the 2013–14 season, Canadian broadcast rights to the Premier League are jointly owned by Sportsnet and TSN, with both rival networks holding rights to 190 matches per season.
The Premier League is broadcast in the United States through NBC Sports. Premier League viewership has increased rapidly, with NBC and NBCSN averaging a record 479,000 viewers in the 2014–15 season, up 118% from 2012–13 when coverage still aired on Fox Soccer and ESPN/ESPN2 (220,000 viewers), and NBC Sports has been widely praised for its coverage. NBC Sports reached a six-year extension with the Premier League in 2015 to broadcast the league through the 2021–22 season in a deal valued at $1 billion (£640 million).
There has been an increasing gulf between the Premier League and the Football League. Since its split with the Football League, many established clubs in the Premier League have managed to distance themselves from their counterparts in lower leagues. Owing in large part to the disparity in revenue from television rights between the leagues, many newly promoted teams have found it difficult to avoid relegation in their first season in the Premier League. In every season except 2001–02 and 2011–12, at least one Premier League newcomer has been relegated back to the Football League. In 1997–98 all three promoted clubs were relegated at the end of the season.
The Premier League distributes a portion of its television revenue to clubs that are relegated from the league in the form of "parachute payments". Starting with the 2013–14 season, these payments are in excess of £60 million over four seasons. Though designed to help teams adjust to the loss of television revenues (the average Premier League team receives £55 million while the average Football League Championship club receives £2 million), critics maintain that the payments actually widen the gap between teams that have reached the Premier League and those that have not, leading to the common occurrence of teams "bouncing back" soon after their relegation. For some clubs who have failed to win immediate promotion back to the Premier League, financial problems, including in some cases administration or even liquidation have followed. Further relegations down the footballing ladder have ensued for several clubs unable to cope with the gap.
As of the 2015–16 season, Premier League football has been played in 53 stadiums since the formation of the Premier League in 1992. The Hillsborough disaster in 1989 and the subsequent Taylor Report saw a recommendation that standing terraces should be abolished; as a result all stadiums in the Premier League are all-seater. Since the formation of the Premier League, football grounds in England have seen constant improvements to capacity and facilities, with some clubs moving to new-build stadiums. Nine stadiums that have seen Premier League football have now been demolished. The stadiums for the 2010–11 season show a large disparity in capacity: Old Trafford, the home of Manchester United has a capacity of 75,957 with Bloomfield Road, the home of Blackpool, having a capacity of 16,220. The combined total capacity of the Premier League in the 2010–11 season is 770,477 with an average capacity of 38,523.
Stadium attendances are a significant source of regular income for Premier League clubs. For the 2009–10 season, average attendances across the league clubs were 34,215 for Premier League matches with a total aggregate attendance figure of 13,001,616. This represents an increase of 13,089 from the average attendance of 21,126 recorded in the league's first season (1992–93). However, during the 1992–93 season the capacities of most stadiums were reduced as clubs replaced terraces with seats in order to meet the Taylor Report's 1994–95 deadline for all-seater stadiums. The Premier League's record average attendance of 36,144 was set during the 2007–08 season. This record was then beaten in the 2013–14 season recording an average attendance of 36,695 with a total attendance of just under 14 million, the highest average in England's top flight since 1950.
Managers in the Premier League are involved in the day-to-day running of the team, including the training, team selection, and player acquisition. Their influence varies from club-to-club and is related to the ownership of the club and the relationship of the manager with fans. Managers are required to have a UEFA Pro Licence which is the final coaching qualification available, and follows the completion of the UEFA 'B' and 'A' Licences. The UEFA Pro Licence is required by every person who wishes to manage a club in the Premier League on a permanent basis (i.e. more than 12 weeks – the amount of time an unqualified caretaker manager is allowed to take control). Caretaker appointments are managers that fill the gap between a managerial departure and a new appointment. Several caretaker managers have gone on to secure a permanent managerial post after performing well as a caretaker; examples include Paul Hart at Portsmouth and David Pleat at Tottenham Hotspur.
At the inception of the Premier League in 1992–93, just eleven players named in the starting line-ups for the first round of matches hailed from outside of the United Kingdom or Ireland. By 2000–01, the number of foreign players participating in the Premier League was 36 per cent of the total. In the 2004–05 season the figure had increased to 45 per cent. On 26 December 1999, Chelsea became the first Premier League side to field an entirely foreign starting line-up, and on 14 February 2005 Arsenal were the first to name a completely foreign 16-man squad for a match. By 2009, under 40% of the players in the Premier League were English.
In response to concerns that clubs were increasingly passing over young English players in favour of foreign players, in 1999, the Home Office tightened its rules for granting work permits to players from countries outside of the European Union. A non-EU player applying for the permit must have played for his country in at least 75 per cent of its competitive 'A' team matches for which he was available for selection during the previous two years, and his country must have averaged at least 70th place in the official FIFA world rankings over the previous two years. If a player does not meet those criteria, the club wishing to sign him may appeal.
Players may only be transferred during transfer windows that are set by the Football Association. The two transfer windows run from the last day of the season to 31 August and from 31 December to 31 January. Player registrations cannot be exchanged outside these windows except under specific licence from the FA, usually on an emergency basis. As of the 2010–11 season, the Premier League introduced new rules mandating that each club must register a maximum 25-man squad of players aged over 21, with the squad list only allowed to be changed in transfer windows or in exceptional circumstances. This was to enable the 'home grown' rule to be enacted, whereby the League would also from 2010 require at least 8 of the named 25 man squad to be made up of 'home-grown players'.
The record transfer fee for a Premier League player has risen steadily over the lifetime of the competition. Prior to the start of the first Premier League season Alan Shearer became the first British player to command a transfer fee of more than £3 million. The record rose steadily in the Premier League's first few seasons, until Alan Shearer made a record breaking £15 million move to Newcastle United in 1996. The three highest transfer in the sport's history had a Premier League club on the selling end, with Tottenham Hotspur selling Gareth Bale to Real Madrid for £85 million in 2013, Manchester United's sale of Cristiano Ronaldo to Real Madrid for £80 million in 2009, and Liverpool selling Luis Suárez to Barcelona for £75 million in 2014.
The Golden Boot is awarded to the top Premier League scorer at the end of each season. Former Blackburn Rovers and Newcastle United striker Alan Shearer holds the record for most Premier League goals with 260. Twenty-four players have reached the 100-goal mark. Since the first Premier League season in 1992–93, 14 different players from 10 different clubs have won or shared the top scorers title. Thierry Henry won his fourth overall scoring title by scoring 27 goals in the 2005–06 season. Andrew Cole and Alan Shearer hold the record for most goals in a season (34) – for Newcastle and Blackburn respectively. Ryan Giggs of Manchester United holds the record for scoring goals in consecutive seasons, having scored in the first 21 seasons of the league.
Its main body is solid sterling silver and silver gilt, while its plinth is made of malachite, a semi-precious stone. The plinth has a silver band around its circumference, upon which the names of the title-winning clubs are listed. Malachite's green colour is also representative of the green field of play. The design of the trophy is based on the heraldry of Three Lions that is associated with English football. Two of the lions are found above the handles on either side of the trophy – the third is symbolised by the captain of the title winning team as he raises the trophy, and its gold crown, above his head at the end of the season. The ribbons that drape the handles are presented in the team colours of the league champions that year.
A police force is a constituted body of persons empowered by the state to enforce the law, protect property, and limit civil disorder. Their powers include the legitimized use of force. The term is most commonly associated with police services of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from military or other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing.
Law enforcement, however, constitutes only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Many police forces suffer from police corruption to a greater or lesser degree. The police force is usually a public sector service, meaning they are paid through taxes.
Law enforcement in Ancient China was carried out by "prefects" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their "prefecture", or jurisdiction. Under each prefect were "subprefects" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. The concept of the "prefecture system" spread to other cultures such as Korea and Japan.
As one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand and Isabella established the centrally organized and efficient Holy Brotherhood (Santa Hermandad) as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835.
In France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' 'War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (French: Connétablie), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under King Francis I (who reigned 1515–1547), the Maréchaussée was merged with the Constabulary. The resulting force was also known as the Maréchaussée, or, formally, the Constabulary and Marshalcy of France.
The first centrally organised police force was created by the government of King Louis XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the Parlement of Paris on March 15, 1667 created the office of lieutenant général de police ("lieutenant general of police"), who was to be the head of the new Paris police force, and defined the task of the police as "ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties".
This office was first held by Gabriel Nicolas de la Reynie, who had 44 commissaires de police (police commissioners) under his authority. In 1709, these commissioners were assisted by inspecteurs de police (police inspectors). The city of Paris was divided into 16 districts policed by the commissaires, each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns.
The word "police" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were "disliked as a symbol of foreign oppression" (according to Britannica 1911). Before the 19th century, the first use of the word "police" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798.
In 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames, to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of £500,000 worth of cargo. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was "perfectly congenial to the principle of the British constitution." Moreover, he went so far as to praise the French system, which had reached "the greatest degree of perfection" in his estimation.
With the initial investment of £4,200, the new trial force of the Thames River Police began with about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and "on the game." The force was a success after its first year, and his men had "established their worth by saving £122,000 worth of cargo and by the rescuing of several lives." Word of this success spread quickly, and the government passed the Marine Police Bill on 28 July 1800, transforming it from a private to public police agency; now the oldest police force in the world. Colquhoun published a book on the experiment, The Commerce and Policing of the River Thames. It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney.
Colquhoun's utilitarian approach to the problem – using a cost-benefit argument to obtain support from businesses standing to benefit – allowed him to achieve what Henry and John Fielding failed for their Bow Street detectives. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames. Colquhoun's innovations were a critical development leading up to Robert Peel's "new" police three decades later.
Meanwhile, the authorities in Glasgow, Scotland successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act of 1822 marked the beginning of the Royal Irish Constabulary. The Act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men.
Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralized, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public.
The 1829 Metropolitan Police Act created a modern police force by limiting the purview of the force and its powers, and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different to the 'Continental model' of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state.
In 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775 a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King João VI established the "Intendência Geral de Polícia" (General Police Intendancy) for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local "military police", with order maintenance tasks. The Federal Railroad Police was created in 1852.
In Canada, the Royal Newfoundland Constabulary was founded in 1729, making it the first police force in present-day Canada. It was followed in 1834 by the Toronto Police, and in 1838 by police forces in Montreal and Quebec City. A national force, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. The famous Royal Northwest Mounted Police was founded in 1873. The merger of these two police forces in 1920 formed the world-famous Royal Canadian Mounted Police.
In the American Old West, policing was often of very poor quality.[citation needed] The Army often provided some policing alongside poorly resourced sheriffs and temporarily organized posses.[citation needed] Public organizations were supplemented by private contractors, notably the Pinkerton National Detective Agency, which was hired by individuals, businessmen, local governments and the federal government. At its height, the Pinkerton Agency's numbers exceeded those of the United States Army.[citation needed]
Michel Foucault claims that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in Public administration and Statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's Traité de la Police ("Treatise on the Police"), first published in 1705. The German Polizeiwissenschaft (Science of Police) first theorized by Philipp von Hörnigk a 17th-century Austrian Political economist and civil servant and much more famously by Johann Heinrich Gottlob Justi who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of Bibliographie der Kameralwissenschaften (1937) in which the author makes note of a substantial bibliography was produced of over 4000 pieces of the practice of Polizeiwissenschaft however, this maybe a mistranslation of Foucault's own work the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520-1850.
As conceptualized by the Polizeiwissenschaft,according to Foucault the police had an administrative,economic and social duty ("procuring abundance"). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'état and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices.
Edwin Chadwick's 1829 article, "Preventive police" in the London Review, argued that prevention ought to be the primary concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that "A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation." In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective - "crime doesn't pay". In the second draft of his 1829 Police Act, the "object" of the new Metropolitan Police, was changed by Robert Peel to the "principal object," which was the "prevention of crime." Later historians would attribute the perception of England's "appearance of orderliness and love of public order" to the preventive principle entrenched in Peel's police system.
Despite popular conceptions promoted by movies and television, many US police departments prefer not to maintain officers in non-patrol bureaus and divisions beyond a certain period of time, such as in the detective bureau, and instead maintain policies that limit service in such divisions to a specified period of time, after which officers must transfer out or return to patrol duties.[citation needed] This is done in part based upon the perception that the most important and essential police work is accomplished on patrol in which officers become acquainted with their beats, prevent crime by their presence, respond to crimes in progress, manage crises, and practice their skills.[citation needed]
The terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state (Nadelmann, 1993), (Sheptycki, 1995). These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention.
Historical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years (Deflem, 2002). For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before the Second World War. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century (Nadelmann, 1993). It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post–Cold War era this type of practice became more significant and frequent (Sheptycki, 2000).
Not a lot of empirical work on the practices of inter/transnational information and intelligence sharing has been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region (2002), which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police case-work. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe (Joubert and Bevers, 1996).
Studies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that low visibility of police information and intelligence sharing was a common feature (Alain, 2001). Intelligence-led policing is now common practice in most advanced countries (Ratcliffe, 2007) and it is likely that police intelligence sharing and information exchange has a common morphology around the world (Ratcliffe, 2007). James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to "compose the panic scenes of the security-control society". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity.
Police development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in States recovering from conflict (Goldsmith and Sheptycki, 2007) With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions (Hills, 2009).
Perhaps the greatest question regarding the future development of transnational policing is: in whose interest is it? At a more practical level, the question translates into one about how to make transnational policing institutions democratically accountable (Sheptycki, 2004). For example, according to the Global Accountability Report for 2007 (Lloyd, et al. 2007) Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities (p. 19). As this report points out, and the existing academic literature on transnational policing seems to confirm, this is a secretive area and one not open to civil society involvement.
They can also be armed with non-lethal (more accurately known as "less than lethal" or "less-lethal") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A "shoot-to-kill" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries.
Modern police forces make extensive use of radio communications equipment, carried both on the person and installed in vehicles, to co-ordinate their work, share information, and get help quickly. In recent years, vehicle-installed computers have enhanced the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other, required reports on a real-time basis. Other common pieces of police equipment include flashlights/torches, whistles, police notebooks and "ticket books" or citations.
Unmarked vehicles are used primarily for sting operations or apprehending criminals without alerting them to their presence. Some police forces use unmarked or minimally marked cars for traffic law enforcement, since drivers slow down at the sight of marked police vehicles and unmarked vehicles make it easier for officers to catch speeders and traffic violators. This practice is controversial, with for example, New York State banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by people impersonating police officers.
Motorcycles are also commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists and often in escort duties where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas because they allow for more open interaction with the public. In addition, their quieter operation can facilitate approaching suspects unawares and can help in pursuing them attempting to escape on foot.
In the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime, rather than broader focus on crime prevention.
In Miranda the court created safeguards against self-incriminating statements made after an arrest. The court held that "The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination"
In Terry v. Ohio (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search " [is] confined to what [is] minimally necessary to determine whether [a suspect] is armed, and the intrusion, which [is] made for the sole purpose of protecting himself and others nearby, [is] confined to ascertaining the presence of weapons" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only.
All police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent.
In contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table.
Valencia (/vəˈlɛnsiə/; Spanish: [baˈlenθja]), or València (Valencian: [vaˈlensia]), is the capital of the autonomous community of Valencia and the third largest city in Spain after Madrid and Barcelona, with around 800,000 inhabitants in the administrative centre. Its urban area extends beyond the administrative city limits with a population of around 1.5 million people. Valencia is Spain's third largest metropolitan area, with a population ranging from 1.7 to 2.5 million. The city has global city status. The Port of Valencia is the 5th busiest container port in Europe and the busiest container port on the Mediterranean Sea.
Valencia enjoyed strong economic growth over the last decade, much of it spurred by tourism and the construction industry,[citation needed] with concurrent development and expansion of telecommunications and transport. The city's economy is service-oriented, as nearly 84% of the working population is employed in service sector occupations[citation needed]. However, the city still maintains an important industrial base, with 5.5% of the population employed in this sector. Agricultural activities are still carried on in the municipality, even though of relatively minor importance with only 1.9% of the working population and 3973 hectares planted mostly in orchards and citrus groves.
Public transport is provided by the Ferrocarrils de la Generalitat Valenciana (FGV), which operates the Metrovalencia and other rail and bus services. The Estació del Nord (North Station) is the main railway terminus in Valencia. A new temporary station, Estación de València-Joaquín Sorolla, has been built on land adjacent to this terminus to accommodate high speed AVE trains to and from Madrid, Barcelona, Seville and Alicante. Valencia Airport is situated 9 km (5.6 mi) west of Valencia city centre. Alicante Airport is situated about 170 km (110 mi) south of Valencia.
Starting in the mid-1990s, Valencia, formerly an industrial centre, saw rapid development that expanded its cultural and touristic possibilities, and transformed it into a newly vibrant city. Many local landmarks were restored, including the ancient Towers of the medieval city (Serrano Towers and Quart Towers), and the San Miguel de los Reyes monastery, which now holds a conservation library. Whole sections of the old city, for example the Carmen Quarter, have been extensively renovated. The Paseo Marítimo, a 4 km (2 mi) long palm tree-lined promenade was constructed along the beaches of the north side of the port (Playa Las Arenas, Playa Cabañal and Playa de la Malvarrosa).
The English held the city for 16 months and defeated several attempts to expel them. English soldiers advanced as far as Requena on the road to Madrid. After the victory of the Bourbons at the Battle of Almansa on 25 April 1707, the English army evacuated Valencia and Philip V ordered the repeal of the privileges of Valencia as punishment for the kingdom's support of Charles of Austria. By the Nueva Planta decrees (Decretos de Nueva Planta) the ancient Charters of Valencia were abolished and the city was governed by the Castilian Charter. The Bourbon forces burned important cities like Xativa, where pictures of the Spanish Bourbons in public places are hung upside down as a protest to this day. The capital of the Kingdom of Valencia was moved to Orihuela, an outrage to the citizens of Valencia. Philip ordered the Cortes to meet with the Viceroy of Valencia, Cardinal Luis de Belluga, who opposed the change of capital because of the proximity of Orihuela, a religious, cultural and now political centre, to Murcia (capital of another viceroyalty and his diocese). Because of his hatred of the city of Orihuela, which had bombarded and looted Valencia during the War of Succession, the cardinal resigned the viceroyalty in protest against the actions of Philip, who finally relented and returned the capital to Valencia.
The city remained in the hands of Christian troops until 1102, when the Almoravids retook the city and restored the Muslim religion. Although the self-styled 'Emperor of All Spain', Alfonso VI of León and Castile, drove them from the city, he was not strong enough to hold it. The Christians set it afire before abandoning it, and the Almoravid Masdali took possession on 5 May 1109. The event was commemorated in a poem by Ibn Khafaja in which he thanked Yusuf ibn Tashfin for the city's liberation.The declining power of the Almoravids coincided with the rise of a new dynasty in North Africa, the Almohads, who seized control of the peninsula from the year 1145, although their entry into Valencia was deterred by Ibn Mardanis, King of Valencia and Murcia until 1171, at which time the city finally fell to the North Africans. The two Muslim dynasties would rule Valencia for more than a century.
The 15th century was a time of Islamic economic expansion, known as the Valencian Golden Age, in which culture and the arts flourished. Concurrent population growth made Valencia the most populous city in the Crown of Aragon. Local industry, led by textile production, reached a great development, and a financial institution, the Canvi de Taula, was created to support municipal banking operations; Valencian bankers lent funds to Queen Isabella I of Castile for Columbus's voyage in 1492. At the end of the century the Silk Exchange (Llotja de la Seda) building was erected as the city became a commercial emporium that attracted merchants from all over Europe.
The vicereine Germaine of Foix brutally repressed the uprising and its leaders, and this accelerated the authoritarian centralisation of the government of Charles I. Queen Germaine favoured harsh treatment of the agermanats. She is thought to have signed the death warrants of 100 former rebels personally, and sources indicate that as many as 800 executions may have occurred. The agermanats are comparable to the comuneros of neighbouring Castile, who fought a similar revolt against Charles from 1520–1522.
In the early 20th century Valencia was an industrialised city. The silk industry had disappeared, but there was a large production of hides and skins, wood, metals and foodstuffs, this last with substantial exports, particularly of wine and citrus. Small businesses predominated, but with the rapid mechanisation of industry larger companies were being formed. The best expression of this dynamic was in the regional exhibitions, including that of 1909 held next to the pedestrian avenue L'Albereda (Paseo de la Alameda), which depicted the progress of agriculture and industry. Among the most architecturally successful buildings of the era were those designed in the Art Nouveau style, such as the North Station (Gare du Nord) and the Central and Columbus markets.
Its average annual temperature is 18.4 °C (65.1 °F). 22.8 °C (73.0 °F) during the day and 13.8 °C (56.8 °F) at night. In the coldest month – January, the maximum temperature typically during the day ranges from 13 to 21 °C (55 to 70 °F), the minimum temperature typically at night ranges from 4 to 12 °C (39 to 54 °F). In the warmest month – August, the maximum temperature during the day typically ranges from 28–34 °C (82–93 °F), about 23 °C (73 °F) at night. Generally, temperatures similar to those experienced in the northern part of Europe in summer last about 8 months, from April to November. March is transitional, the temperature often exceeds 20 °C (68 °F), with an average temperature of 19.0 °C (66 °F) during the day and 10.0 °C (50 °F) at night. December, January and February are the coldest months, with average temperatures around 17 °C (63 °F) during the day and 7 °C (45 °F) at night. Valencia has one of the mildest winters in Europe, owing to its southern location on the Mediterranean Sea and the Foehn phenomenon. The January average is comparable to temperatures expected for May and September in the major cities of northern Europe.
The Valencian economy recovered during the 18th century with the rising manufacture of woven silk and ceramic tiles. The Palau de Justícia is an example of the affluence manifested in the most prosperous times of Bourbon rule (1758–1802) during the rule of Charles III. The 18th century was the age of the Enlightenment in Europe, and its humanistic ideals influenced such men as Gregory Maians and Perez Bayer in Valencia, who maintained correspondence with the leading French and German thinkers of the time. In this atmosphere of the exaltation of ideas the Economic Society of Friends of the Country (Societat Econòmica d'Amics del País) was founded in 1776; it introduced numerous improvements in agriculture and industry and promoted various cultural, civic, and economic institutions in Valencia.
The dictatorship of Franco forbade political parties and began a harsh ideological and cultural repression countenanced and sometimes even led by the Church. The financial markets were destabilised, causing a severe economic crisis that led to rationing. A black market in rationed goods existed for over a decade. The Francoist administrations of Valencia silenced publicity of the catastrophic consequences of the floods of 1949 with the attendant dozens of deaths, but could not do the same after the even more tragic flood of 1957 when the river Turia overflowed its banks again, killing many Valencians (officially, eighty-one died; the actual figure is not known). To prevent further disasters, the river was eventually diverted to a new course. The old river bed was abandoned for years, and successive Francoist mayors proposed making it a motorway, but that option was finally rejected with the advent of democracy and fervent neighbourhood protests. The river was divided in two at the western city limits (Plan Sur de Valencia), and diverted southwards along a new course that skirts the city, before meeting the Mediterranean. The old course of the river continues, dry, through the city centre, almost to the sea. The old riverbed is now a verdant sunken park called the 'Garden of the Turia' (Jardí del Túria or Jardín del Turia) that allows cyclists and pedestrians to traverse much of the city without the use of roads; overhead bridges carry motor traffic across the park.
Valencia's port is the biggest on the Mediterranean western coast, the first of Spain in container traffic as of 2008 and the second of Spain in total traffic, handling 20% of Spain's exports. The main exports are foodstuffs and beverages. Other exports include oranges, furniture, ceramic tiles, fans, textiles and iron products. Valencia's manufacturing sector focuses on metallurgy, chemicals, textiles, shipbuilding and brewing. Small and medium-sized industries are an important part of the local economy, and before the current crisis unemployment was lower than the Spanish average.
A fervent follower of the absolutist cause, Elío had played an important role in the repression of the supporters of the Constitution of 1812. For this, he was arrested in 1820 and executed in 1822 by garroting. Conflict between absolutists and liberals continued, and in the period of conservative rule called the Ominous Decade (1823–1833), which followed the Trienio Liberal, there was ruthless repression by government forces and the Catholic Inquisition. The last victim of the Inquisition was Gaietà Ripoli, a teacher accused of being a deist and a Mason who was hanged in Valencia in 1824.
On 9 July 2006, during Mass at Valencia's Cathedral, Our Lady of the Forsaken Basilica, Pope Benedict XVI used, at the World Day of Families, the Santo Caliz, a 1st-century Middle-Eastern artifact that some Catholics believe is the Holy Grail. It was supposedly brought to that church by Emperor Valerian in the 3rd century, after having been brought by St. Peter to Rome from Jerusalem. The Santo Caliz (Holy Chalice) is a simple, small stone cup. Its base was added in Medieval Times and consists of fine gold, alabaster and gem stones.
During the Cantonal Revolution of 1873, a cantonalist uprising that took place during the First Spanish Republic, the city was consolidated with most of the nearby cities in the Federal Canton of Valencia (proclaimed on 19 July and dissolved on 7 August). It did not have the revolutionary fervor of the movement in cities like Alcoy, as it was initiated by the bourgeoisie, but the Madrid government sent General Martinez-Campos to stifle the rebellion by force of arms and subjected Valencia to an intense bombardment. The city surrendered on 7 August; Alfonso XII was proclaimed king on 29 December 1874, and arrived in Valencia on 11 January 1875 on his way to Madrid, marking the end of the first republic. Despite the Bourbon restoration, the roughly even balance between conservatives and liberals in the government was sustained in Valencia until the granting of universal male suffrage in 1890, after which the Republicans, led by Vicente Blasco Ibáñez, gained considerably more of the popular vote.
World-renowned (and city-born) architect Santiago Calatrava produced the futuristic City of Arts and Sciences (Ciutat de les Arts i les Ciències), which contains an opera house/performing arts centre, a science museum, an IMAX cinema/planetarium, an oceanographic park and other structures such as a long covered walkway and restaurants. Calatrava is also responsible for the bridge named after him in the centre of the city. The Music Palace (Palau De La Música) is another noteworthy example of modern architecture in Valencia.
Valencia is a bilingual city: Valencian and Spanish are the two official languages. Spanish is official in all of Spain, whereas Valencian is official in the Valencian Country, as well as in Catalonia and the Balearic Islands, where it receives the name of Catalan. Despite the differentiated denomination, the distinct dialectal traits and political tension between Catalonia and the Valencian Country, Catalan and Valencian are mutually intelligible and are considered two varieties of the same language.
Valencian has been historically repressed in favour of Spanish. The effects have been more noticeable in the city proper, whereas the language has remained active in the rural and metropolitan areas. After the Castille-Aragon unification, a Spanish-speaking elite established itself in the city. In more recent history, the establishment of Franco's military and administrative apparatus in Valencia further excluded Valencian from public life. Valencian recovered its official status, prestige and use in education after the transition to democracy in 1978. However, due to industrialisation in recent decades, Valencia has attracted immigration from other regions in Spain, and hence there is also a demographic factor for its declining social use. Due to a combination of these reasons, Valencia has become the bastion of anti-Catalan blaverism, which celebrates Valencian as merely folkloric, but rejects the existing standard which was adapted from Catalan orthography.
Spanish is currently the predominant language in the city proper but, thanks to the education system, most Valencians have basic knowledge of both Spanish and Valencian, and either can be used in the city. Valencia is therefore the second biggest Catalan-speaking city after Barcelona. Institutional buildings and streets are named in Valencian. The city is also home to many pro-Valencian political and civil organisations. Furthermore, education entirely in Valencian is offered in more than 70 state-owned schools in the city, as well as by the University of Valencia across all disciplines.
Valencia has experienced a surge in its cultural development during the last thirty years, exemplified by exhibitions and performances at such iconic institutions as the Palau de la Música, the Palacio de Congresos, the Metro, the City of Arts and Sciences (Ciutat de les Arts i les Ciències), the Valencian Museum of Enlightenment and Modernity (Museo Valenciano de la Ilustracion y la Modernidad), and the Institute of Modern Art (Instituto Valenciano de Arte Moderno). The various productions of Santiago Calatrava, a renowned structural engineer, architect, and sculptor and of the architect Félix Candela have contributed to Valencia's international reputation. These public works and the ongoing rehabilitation of the Old City (Ciutat Vella) have helped improve the city's livability and tourism is continually increasing.
Among the parish churches are Saints John (Baptist and Evangelist), rebuilt in 1368, whose dome, decorated by Palonino, contains some of the best frescoes in Spain; El Templo (the Temple), the ancient church of the Knights Templar, which passed into the hands of the Order of Montesa and was rebuilt in the reigns of Ferdinand VI and Charles III; the former convent of the Dominicans, at one time the headquarters of the Capital General, the cloister of which has a beautiful Gothic wing and the chapter room, large columns imitating palm trees; the Colegio del Corpus Christi, which is devoted to the Blessed Sacrament, and in which perpetual adoration is carried on; the Jesuit college, which was destroyed in 1868 by the revolutionary Committee of the Popular Front, but later rebuilt; and the Colegio de San Juan (also of the Society), the former college of the nobles, now a provincial institute for secondary instruction.
A few centuries later, coinciding with the first waves of the invading Germanic peoples (Suevi, Vandals and Alans, and later the Visigoths) and the power vacuum left by the demise of the Roman imperial administration, the church assumed the reins of power in the city and replaced the old Roman temples with religious buildings. With the Byzantine invasion of the southwestern Iberian peninsula in 554 the city acquired strategic importance. After the expulsion of the Byzantines in 625, Visigothic military contingents were posted there and the ancient Roman amphitheatre was fortified. Little is known of its history for nearly a hundred years; although this period is only scarcely documented by archeology, excavations suggest that there was little development of the city. During Visigothic times Valencia was an episcopal See of the Catholic Church, albeit a suffragan diocese subordinate to the archdiocese of Toledo, comprising the ancient Roman province of Carthaginensis in Hispania.
In the 15th century the dome was added and the naves extended back of the choir, uniting the building to the tower and forming a main entrance. Archbishop Luis Alfonso de los Cameros began the building of the main chapel in 1674; the walls were decorated with marbles and bronzes in the Baroque style of that period. At the beginning of the 18th century the German Conrad Rudolphus built the façade of the main entrance. The other two doors lead into the transept; one, that of the Apostles in pure pointed Gothic, dates from the 14th century, the other is that of the Paláu. The additions made to the back of the cathedral detract from its height. The 18th-century restoration rounded the pointed arches, covered the Gothic columns with Corinthian pillars, and redecorated the walls. The dome has no lantern, its plain ceiling being pierced by two large side windows. There are four chapels on either side, besides that at the end and those that open into the choir, the transept, and the sanctuary. It contains many paintings by eminent artists. A silver reredos, which was behind the altar, was carried away in the war of 1808, and converted into coin to meet the expenses of the campaign. There are two paintings by Francisco Goya in the San Francesco chapel. Behind the Chapel of the Blessed Sacrament is a small Renaissance chapel built by Calixtus III. Beside the cathedral is the chapel dedicated to the Our Lady of the Forsaken (Virgen de los desamparados or Mare de Déu dels Desamparats).
In 1238, King James I of Aragon, with an army composed of Aragonese, Catalans, Navarrese and crusaders from the Order of Calatrava, laid siege to Valencia and on 28 September obtained a surrender. Fifty thousand Moors were forced to leave. Poets such as Ibn al-Abbar and Ibn Amira mourned this exile from their beloved Valencia. After the Christian victory and the expulsion of the Muslim population the city was divided between those who had participated in the conquest, according to the testimony in the Llibre del Repartiment (Book of Distribution). James I granted the city new charters of law, the Furs of Valencia, which later were extended to the whole kingdom of Valencia. Thenceforth the city entered a new historical stage in which a new society and a new language developed, forming the basis of the character of the Valencian people as they are known today.
In its long history, Valencia has acquired many local traditions and festivals, among them the Falles, which were declared Celebrations of International Touristic Interest (Fiestas de Interés Turístico Internacional) on 25 January 1965, and the Water Tribunal of Valencia (Tribunal de las Aguas de Valencia), which was declared an intangible cultural heritage of humanity (Patrimonio Cultural Inmaterial de la Humanidad) in 2009. In addition to these Valencia has hosted world-class events that helped shape the city's reputation and put it in the international spotlight, e.g., the Regional Exhibition of 1909, the 32nd and the 33rd America's Cup competitions, the European Grand Prix of Formula One auto racing, the Valencia Open 500 tennis tournament, and the Global Champions Tour of equestrian sports.
The city had surrendered without a fight to the invading Moors (Berbers and Arabs) by 714 AD, and the cathedral of Saint Vincent was turned into a mosque. Abd al-Rahman I, the first emir of Cordoba, ordered the city destroyed in 755 during his wars against other nobility, but several years later his son, Abdullah, had a form of autonomous rule over the province of Valencia. Among his administrative acts he ordered the building of a luxurious palace, the Russafa, on the outskirts of the city in the neighbourhood of the same name. So far no remains have been found. Also at this time Valencia received the name Medina al-Turab (City of Sand). When Islamic culture settled in, Valencia, then called Balansiyya, prospered from the 10th century, due to a booming trade in paper, silk, leather, ceramics, glass and silver-work. The architectural legacy of this period is abundant in Valencia and can still be appreciated today in the remnants of the old walls, the Baños del Almirante bath house, Portal de Valldigna street and even the Cathedral and the tower, El Micalet (El Miguelete), which was the minaret of the old mosque.
This boom was reflected in the growth of artistic and cultural pursuits. Some of the most emblematic buildings of the city were built during this period, including the Serranos Towers (1392), the Lonja (1482), the Miguelete and the Chapel of the Kings of the Convent of Santo Domingo. In painting and sculpture, Flemish and Italian trends had an influence on artists such as Lluís Dalmau, Peris Gonçal and Damià Forment. Literature flourished with the patronage of the court of Alfonso the Magnanimous, supporting authors like Ausiàs March, Roiç de Corella, and Isabel de Villena. By 1460 Joanot Martorell wrote Tirant lo Blanch, an innovative novel of chivalry that influenced many later writers, from Cervantes to Shakespeare. Ausiàs March was one of the first poets to use the everyday language Valencian, instead of the troubadour language, Occitan. Also around this time, between 1499 and 1502, the University of Valencia was founded under the parsimonious name of Estudio General ("studium generale", place of general studies).
The decline of the city reached its nadir with the War of Spanish Succession (1702–1709) that marked the end of the political and legal independence of the Kingdom of Valencia. During the War of the Spanish Succession, Valencia sided with Charles of Austria. On 24 January 1706, Charles Mordaunt, 3rd Earl of Peterborough, 1st Earl of Monmouth, led a handful of English cavalrymen into the city after riding south from Barcelona, capturing the nearby fortress at Sagunt, and bluffing the Spanish Bourbon army into withdrawal.
The mutineers seized the Citadel, a Supreme Junta government took over, and on 26–28 June, Napoleon's Marshal Moncey attacked the city with a column of 9,000 French imperial troops in the First Battle of Valencia. He failed to take the city in two assaults and retreated to Madrid. Marshal Suchet began a long siege of the city in October 1811, and after intense bombardment forced it to surrender on 8 January 1812. After the capitulation, the French instituted reforms in Valencia, which became the capital of Spain when the Bonapartist pretender to the throne, José I (Joseph Bonaparte, Napoleon's elder brother), moved the Court there in the summer of 1812. The disaster of the Battle of Vitoria on 21 June 1813 obliged Suchet to quit Valencia, and the French troops withdrew in July.
The crisis deepened during the 17th century with the expulsion in 1609 of the Jews and the Moriscos, descendants of the Muslim population that converted to Christianity under threat of exile from Ferdinand and Isabella in 1502. From 1609 through 1614, the Spanish government systematically forced Moriscos to leave the kingdom for Muslim North Africa. They were concentrated in the former Kingdom of Aragon, where they constituted a fifth of the population, and the Valencia area specifically, where they were roughly a third of the total population. The expulsion caused the financial ruin of some of the nobility and the bankruptcy of the Taula de Canvi in 1613. The Crown endeavoured to compensate the nobles, who had lost much of their agricultural labour force; this harmed the economy of the city for generations to come. Later, during the so-called Catalan Revolt (1640–1652), Valencia contributed to the cause of Philip IV with militias and money, resulting in a period of further economic hardship exacerbated by the arrival of troops from other parts of Spain.
During the second half of the 19th century the bourgeoisie encouraged the development of the city and its environs; land-owners were enriched by the introduction of the orange crop and the expansion of vineyards and other crops,. This economic boom corresponded with a revival of local traditions and of the Valencian language, which had been ruthlessly suppressed from the time of Philip V. Around 1870, the Valencian Renaissance, a movement committed to the revival of the Valencian language and traditions, began to gain ascendancy. In its early stages the movement inclined to the romanticism of the poet Teodor Llorente, and resisted the more assertive remonstrances of Constantine Llombart, founder of the still extant cultural society, Lo Rat Penat, which is dedicated to the promotion and dissemination of the Valencian language and culture.
During the regency of Maria Cristina, Espartero ruled Spain for two years as its 18th Prime Minister from 16 September 1840 to 21 May 1841. Under his progressive government the old regime was tenuously reconciled to his liberal policies. During this period of upheaval in the provinces he declared that all the estates of the Church, its congregations, and its religious orders were national property—though in Valencia, most of this property was subsequently acquired by the local bourgeoisie. City life in Valencia carried on in a revolutionary climate, with frequent clashes between liberals and republicans, and the constant threat of reprisals by the Carlist troops of General Cabrera.
The Valencia Metro derailment occurred on 3 July 2006 at 1 pm. CEST (1100 UTC) between Jesús and Plaça d'Espanya stations on Line 1 of the Metrovalencia mass transit system. 43 people were killed and more than ten were seriously injured. It was not immediately clear what caused the crash. Both the Valencian government spokesman Vicente Rambla and Mayor Rita Barberá called the accident a "fortuitous" event. However, the trade union CC.OO. accused the authorities of "rushing" to say anything but admit that Line 1 is in a state of "constant deterioration" with a "failure to carry out maintenance".
During the 20th century Valencia remained the third most populous city of Spain as its population tripled, rising from 213,550 inhabitants in 1900 to 739,014 in 2000. Valencia was also third in industrial and economic development; notable milestones include urban expansion of the city in the latter 1800s, the creation of the Banco de Valencia in 1900, construction of the Central and Columbus markets, and the construction of the Gare du Nord railway station, completed in 1921. The new century was marked in Valencia with a major event, the Valencian regional exhibition of 1909 (La Exposición Regional Valenciana de 1909), which emulated the national and universal expositions held in other cities. This production was promoted by the Ateneo Mercantil de Valencia (Mercantile Athenaeum of Valencia), especially by its chairman, Tomás Trénor y Palavicino, and had the support of the Government and the Crown; it was officially inaugurated by King Alfonso XIII himself.
The inevitable march to civil war and the combat in Madrid resulted in the removal of the capital of the Republic to Valencia. On 6 November 1936 the city became the capital of Republican Spain under the control of the prime minister Manuel Azana; the government moved to the Palau de Benicarló, its ministries occupying various other buildings. The city was heavily bombarded by air and sea, necessitating the construction of over two hundred bomb shelters to protect the population. On 13 January 1937 the city was first shelled by a vessel of the Fascist Italian Navy, which was blockading the port by the order of Benito Mussolini. The bombardment intensified and inflicted massive destruction on several occasions; by the end of the war the city had survived 442 bombardments, leaving 2,831 dead and 847 wounded, although it is estimated that the death toll was higher, as the data given are those recognised by Francisco Franco's government. The Republican government passed to Juan Negrín on 17 May 1937 and on 31 October of that year moved to Barcelona. On 30 March 1939 Valencia surrendered and the Nationalist troops entered the city. The postwar years were a time of hardship for Valencians. During Franco's regime speaking or teaching Valencian was prohibited; in a significant reversal it is now compulsory for every schoolchild in Valencia.
In March 2012, the newspaper El Mundo published a story according to which FGV had instructed employees who were to testify at the crash commission investigation, providing a set of possible questions and guidelines to prepare the answers. In April 2013, the television program Salvados questioned the official version of the incident as there were indications that the Valencian Government had tried to downplay the accident, which coincided with the visit of the pope to Valencia, or even to hide evidence, as the book of train breakdowns was never found. The day after the broadcast of this report, which received extensive media coverage, several voices called for the reopening of the investigation. The investigation was effectively reopened and the accident is currently under re-examination.
In 1409, a hospital was founded and placed under the patronage of Santa María de los Inocentes; to this was attached a confraternity devoted to recovering the bodies of the unfriended dead in the city and within a radius of three miles (4.8 km) around it. At the end of the 15th century this confraternity separated from the hospital, and continued its work under the name of "Cofradia para el ámparo de los desamparados". King Philip IV of Spain and the Duke of Arcos suggested the building of the new chapel, and in 1647 the Viceroy, Conde de Oropesa, who had been preserved from the bubonic plague, insisted on carrying out their project. The Blessed Virgin was proclaimed patroness of the city under the title of Virgen de los desamparados (Virgin of the Forsaken), and Archbishop Pedro de Urbina, on 31 June 1652, laid the cornerstone of the new chapel of this name. The archiepiscopal palace, a grain market in the time of the Moors, is simple in design, with an inside cloister and a handsome chapel. In 1357, the arch that connects it with the cathedral was built. In the council chamber are preserved the portraits of all the prelates of Valencia.
Valencia is also internationally famous for its football club, Valencia C.F., which won the Spanish league in 2002 and 2004 (the year it also won the UEFA Cup), for a total of six times, and was a UEFA Champions League runner-up in 2000 and 2001. The team's stadium is the Mestalla; its city rival Levante UD also plays in the highest division after gaining promotion in 2010, their stadium is Estadi Ciutat de València. From the year 2011 there has been a third team in the city, Huracán Valencia, who play their games in Municipal de Manises, in the Segunda División B.
Valencia was founded as a Roman colony in 138 BC. The city is situated on the banks of the Turia, on the east coast of the Iberian Peninsula, fronting the Gulf of Valencia on the Mediterranean Sea. Its historic centre is one of the largest in Spain, with approximately 169 hectares; this heritage of ancient monuments, views and cultural attractions makes Valencia one of the country's most popular tourist destinations. Major monuments include Valencia Cathedral, the Torres de Serrans, the Torres de Quart, the Llotja de la Seda (declared a World Heritage Site by UNESCO in 1996), and the Ciutat de les Arts i les Ciències (City of Arts and Sciences), an entertainment-based cultural and architectural complex designed by Santiago Calatrava and Félix Candela. The Museu de Belles Arts de València houses a large collection of paintings from the 14th to the 18th centuries, including works by Velázquez, El Greco, and Goya, as well as an important series of engravings by Piranesi. The Institut Valencià d'Art Modern (Valencian Institute of Modern Art) houses both permanent collections and temporary exhibitions of contemporary art and photography.
Valencia stands on the banks of the Turia River, located on the eastern coast of the Iberian Peninsula and the western part of the Mediterranean Sea, fronting the Gulf of Valencia. At its founding by the Romans, it stood on a river island in the Turia, 6.4 km (4 mi) from the sea. The Albufera, a freshwater lagoon and estuary about 11 km (7 mi) south of the city, is one of the largest lakes in Spain. The City Council bought the lake from the Crown of Spain for 1,072,980 pesetas in 1911, and today it forms the main portion of the Parc Natural de l'Albufera (Albufera Nature Reserve), with a surface area of 21,120 hectares (52,200 acres). In 1986, because of its cultural, historical, and ecological value, the Generalitat Valenciana declared it a natural park.
The third largest city in Spain and the 24th most populous municipality in the European Union, Valencia has a population of 809,267 within its administrative limits on a land area of 134.6 km2 (52 sq mi). The urban area of Valencia extending beyond the administrative city limits has a population of between 1,561,000 and 1,564,145. 1,705,742 or 2,300,000 or 2,516,818 people live in the Valencia metropolitan area. Between 2007 and 2008 there was a 14% increase in the foreign born population with the largest numeric increases by country being from Bolivia, Romania and Italy.
About two thousand Roman colonists were settled there in 138 BC during the rule of consul Decimus Junius Brutus Galaico. The Roman historian Florus says that Brutus transferred the soldiers who had fought under him to that province. This was a typical Roman city in its conception, as it was located in a strategic location near the sea on a river island crossed by the Via Augusta, the imperial road that connected the province to Rome, the capital of the empire. The centre of the city was located in the present-day neighbourhood of the Plaza de la Virgen. Here was the forum and the crossing of the Cardo Maximus and the Decumanus Maximus, which remain the two main axes of the city. The Cardo corresponds to the existing Calle de Salvador, Almoina, and the Decumanus corresponds to Calle de los Caballeros.
Balansiyya had a rebirth of sorts with the beginning of the Taifa of Valencia kingdom in the 11th century. The town grew, and during the reign of Abd al-Aziz a new city wall was built, remains of which are preserved throughout the Old City (Ciutat Vella) today. The Castilian nobleman Rodrigo Diaz de Vivar, known as El Cid, who was intent on possessing his own principality on the Mediterranean, entered the province in command of a combined Christian and Moorish army and besieged the city beginning in 1092. By the time the siege ended in May 1094, he had carved out his own fiefdom—which he ruled from 15 June 1094 to July 1099. This victory was immortalised in the Lay of the Cid. During his rule, he converted nine mosques into churches and installed the French monk Jérôme as bishop of the See of Valencia. El Cid was killed in July 1099 while defending the city from an Almoravid siege, whereupon his wife Ximena Díaz ruled in his place for two years.
The city went through serious troubles in the mid-fourteenth century. On the one hand were the decimation of the population by the Black Death of 1348 and subsequent years of epidemics — and on the other, the series of wars and riots that followed. Among these were the War of the Union, a citizen revolt against the excesses of the monarchy, led by Valencia as the capital of the kingdom — and the war with Castile, which forced the hurried raising of a new wall to resist Castilian attacks in 1363 and 1364. In these years the coexistence of the three communities that occupied the city—Christian, Jewish and Muslim — was quite contentious. The Jews who occupied the area around the waterfront had progressed economically and socially, and their quarter gradually expanded its boundaries at the expense of neighbouring parishes. Meanwhile, Muslims who remained in the city after the conquest were entrenched in a Moorish neighbourhood next to the present-day market Mosen Sorel. In 1391 an uncontrolled mob attacked the Jewish quarter, causing its virtual disappearance and leading to the forced conversion of its surviving members to Christianity. The Muslim quarter was attacked during a similar tumult among the populace in 1456, but the consequences were minor.
Faced with this loss of business, Valencia suffered a severe economic crisis. This manifested early in 1519–1523 when the artisan guilds known as the Germanies revolted against the government of the Habsburg king Charles I in Valencia, now part of the Crown of Aragon, with most of the fighting done in 1521. The revolt was an anti-monarchist, anti-feudal autonomist movement inspired by the Italian republics, and a social revolt against the nobility who had fled the city before an epidemic of plague in 1519. It also bore a strong anti-Islamic aspect, as rebels rioted against Aragon's population of mudéjars and imposed forced conversions to Christianity.
With the abolition of the charters of Valencia and most of its institutions, and the conformation of the kingdom and its capital to the laws and customs of Castile, top civil officials were no longer elected, but instead were appointed directly from Madrid, the king's court city, the offices often filled by foreign aristocrats. Valencia had to become accustomed to being an occupied city, living with the presence of troops quartered in the Citadel near the convent of Santo Domingo and in other buildings such as the Lonja, which served as a barracks until 1762.
Ferdinand refused and went to Valencia instead of Madrid. Here, on 17 April, General Elio invited the King to reclaim his absolute rights and put his troops at the King's disposition. The king abolished the Constitution of 1812. He followed this act by dissolving the two chambers of the Spanish Parliament on 10 May. Thus began six years (1814–1820) of absolutist rule, but the constitution was reinstated during the Trienio Liberal, a period of three years of liberal government in Spain from 1820–1823.
The public water supply network was completed in 1850, and in 1858 the architects Sebastián Monleón Estellés, Antonino Sancho, and Timoteo Calvo drafted a general expansion project for the city that included demolishing its ancient walls (a second version was printed in 1868). Neither proposed project received final approval, but they did serve as a guide, though not closely followed, for future growth. By 1860 the municipality had 140,416 inhabitants, and beginning in 1866 the ancient city walls were almost entirely demolished to facilitate urban expansion. Electricity was introduced to Valencia in 1882.
The economy began to recover in the early 1960s, and the city experienced explosive population growth through immigration spurred by the jobs created with the implementation of major urban projects and infrastructure improvements. With the advent of democracy in Spain, the ancient kingdom of Valencia was established as a new autonomous entity, the Valencian Community, the Statute of Autonomy of 1982 designating Valencia as its capital. On the night of 23 February 1981, shortly after Antonio Tejero had stormed Congress, the Captain General of the Third Military Region, Jaime Milans del Bosch, rose up in Valencia, put tanks on the streets, declared a state of emergency and tried to convince other senior military figures to support the coup. After the televised message of King Juan Carlos I, those in the military who had not yet aligned themselves decided to remain loyal to the government, and the coup failed. Despite this lack of support, Milans del Bosch only surrendered at 5 a.m. on the next day, 24 February.
The largest plaza in Valencia is the Plaza del Ayuntamiento; it is home to the City Hall (Ayuntamiento) on its western side and the central post office (Edificio de Correos) on its eastern side, a cinema that shows classic movies, and many restaurants and bars. The plaza is triangular in shape, with a large cement lot at the southern end, normally surrounded by flower vendors. It serves as ground zero during the Les Falles when the fireworks of the Mascletà can be heard every afternoon. There is a large fountain at the northern end.
The Valencia Cathedral was called Iglesia Mayor in the early days of the Reconquista, then Iglesia de la Seo (Seo is from the Latin sedes, i.e., (archiepiscopal) See), and by virtue of the papal concession of 16 October 1866, it was called the Basilica Metropolitana. It is situated in the centre of the ancient Roman city where some believe the temple of Diana stood. In Gothic times, it seems to have been dedicated to the Holy Saviour; the Cid dedicated it to the Blessed Virgin; King James I of Aragon did likewise, leaving in the main chapel the image of the Blessed Virgin, which he carried with him and is reputed to be the one now preserved in the sacristy. The Moorish mosque, which had been converted into a Christian Church by the conqueror, was deemed unworthy of the title of the cathedral of Valencia, and in 1262 Bishop Andrés de Albalat laid the cornerstone of the new Gothic building, with three naves; these reach only to the choir of the present building. Bishop Vidal de Blanes built the chapter hall, and James I added the tower, called El Miguelete because it was blessed on St. Michael's day in 1418. The tower is about 58 m high and topped with a belfry (1660–1736).
Once a year between 2008–2012 the European Formula One Grand Prix took place in the Valencia Street Circuit. Valencia is among with Barcelona, Porto and Monte Carlo the only European cities ever to host Formula One World Championship Grands Prix on public roads in the middle of cities. The final race in 2012 European Grand Prix saw an extremely popular winner, since home driver Fernando Alonso won for Ferrari in spite of starting halfway down the field. The Valencian Community motorcycle Grand Prix (Gran Premi de la Comunitat Valenciana de motociclisme) is part of the Grand Prix motorcycle racing season at the Circuit Ricardo Tormo (also known as Circuit de Valencia). Periodically the Spanish round of the Deutsche Tourenwagen Masters touring car racing Championship (DTM) is held in Valencia.
On October 9, 2006 at 6:00 a.m., the network switched to a 24-hour schedule, becoming one of the last major English-language broadcasters to transition to such a schedule. Most CBC-owned stations previously signed off the air during the early morning hours (typically from 1:00 a.m. to 6:00 a.m.). Instead of the infomercials aired by most private stations, or a simulcast of CBC News Network in the style of BBC One's nightly simulcast of BBC News Channel, the CBC uses the time to air repeats, including local news, primetime series, movies and other programming from the CBC library. Its French counterpart, Ici Radio-Canada Télé, still signs off every night.
Until 1998, the network carried a variety of American programs in addition to its core Canadian programming, directly competing with private Canadian broadcasters such as CTV and Global. Since then, it has restricted itself to Canadian programs, a handful of British programs, and a few American movies and off-network repeats. Since this change, the CBC has sometimes struggled to maintain ratings comparable to those it achieved before 1995, although it has seen somewhat of a ratings resurgence in recent years. In the 2007-08 season, hit series such as Little Mosque on the Prairie and The Border helped the network achieve its strongest ratings performance in over half a decade.
Under the CBC's current arrangement with Rogers Communications for National Hockey League broadcast rights, Hockey Night in Canada broadcasts on CBC-owned stations and affiliates are not technically aired over the CBC Television network, but over a separate CRTC-licensed part-time network operated by Rogers. This was required by the CRTC as Rogers exercises editorial control and sells all advertising time during the HNIC broadcasts, even though the CBC bug and promos for other CBC Television programs appear throughout HNIC.
The CBC's flagship newscast, The National, airs Sunday through Fridays at 10:00 p.m. EST and Saturdays at 6:00 p.m. EST. Until October 2006, CBC owned-and-operated stations aired a second broadcast of the program at 11:00 p.m.; this later broadcast included only the main news portion of the program, and excluded the analysis and documentary segment. This second airing was later replaced with other programming, and as of the 2012-13 television season, was replaced on CBC's major market stations by a half-hour late newscast. There is also a short news update, at most, on late Saturday evenings. During hockey season, this update is usually found during the first intermission of the second game of the doubleheader on Hockey Night in Canada.
In addition to the mentioned late local newscasts, CBC stations in most markets fill early evenings with local news programs, generally from 5:00 p.m. to 6:30 p.m., while most stations also air a single local newscast on weekend evenings (comprising a supper hour broadcast on Saturdays and a late evening newscast on Sundays). Other newscasts include parts of CBC News Now airing weekday at 6:00 a.m. and noon. Weekly newsmagazine the fifth estate is also a CBC mainstay, as are documentary series such as Doc Zone.
One of the most popular shows on CBC Television is the weekly Saturday night broadcast of NHL hockey games, Hockey Night in Canada. It has been televised by the network since 1952. During the NHL lockout and subsequent cancellation of the 2004-2005 hockey season, CBC instead aired various recent and classic movies, branded as Movie Night in Canada, on Saturday nights. Many cultural groups criticized this and suggested the CBC air games from minor hockey leagues; the CBC responded that most such broadcast rights were already held by other groups, but it did base each Movie Night broadcast from a different Canadian hockey venue. Other than hockey, CBC Sports properties include Toronto Raptors basketball, Toronto FC Soccer, and various other amateur and professional events.
It was also the exclusive carrier of Canadian Curling Association events during the 2004–2005 season. Due to disappointing results and fan outrage over many draws being carried on CBC Country Canada (now called Cottage Life Television, the association tried to cancel its multiyear deal with the CBC signed in 2004. After the CBC threatened legal action, both sides eventually came to an agreement under which early-round rights reverted to TSN. On June 15, 2006, the CCA announced that TSN would obtain exclusive rights to curling broadcasts in Canada as of the 2008-09 season, shutting the CBC out of the championship weekend for the first time in 40-plus years.
Many were surprised by these changes to the CBC schedule, which were apparently intended to attract a younger audience to the network; some suggested they might alienate the core CBC viewership. Another note of criticism was made when the network decided to move The National in some time zones to simulcast the American version of The One over the summer. This later became a moot point, as The One was taken off the air after two weeks after extremely low American and Canadian ratings, and the newscast resumed its regular schedule.
Beginning in 2005, the CBC has contributed production funds for the BBC Wales revival of Doctor Who, for which it received a special credit at the end of each episode. This arrangement continued until the end of fourth season, broadcast in 2008. The CBC similarly contributed to the first season of the spin-off series, Torchwood. More recently, the network has also begun picking up Canadian rights to some Australian series, including the drama series Janet King and Love Child, and the comedy-drama series Please Like Me.
On March 5, 2005, CBC Television launched a high definition simulcast of its Toronto (CBLT-DT) and Montreal (CBMT-DT) stations. Since that time, the network has also launched HD simulcasts in Vancouver (CBUT-DT), Ottawa (CBOT-DT), Edmonton (CBXT-DT), Calgary (CBRT-DT), Halifax (CBHT-DT), Windsor, (CBET-DT), Winnipeg (CBWT-DT) and St. John's (CBNT-DT). CBC HD is available nationally via satellite and on digital cable as well as for free over-the-air using a regular TV antenna and a digital tuner (included in most new television sets) on the following channels:
Most CBC television stations, including those in the major cities, are owned and operated by the CBC itself. CBC O&O stations operate as a mostly seamless national service with few deviations from the main network schedule, although there are some regional differences from time to time. For on-air identification, most CBC stations use the CBC brand rather than their call letters, not identifying themselves specifically until sign-on or sign-off (though some, like Toronto's CBLT, do not ID themselves at all except through PSIP). All CBC O&O stations have a standard call letter naming convention, in that the first two letters are "CB" (an ITU prefix allocated not to Canada, but to Chile) and the last letter is "T". Only the third letter varies from market to market; however, that letter is typically the same as the third letter of the CBC Radio One and CBC Radio 2 stations in the same market. An exception to this rule are the CBC North stations in Yellowknife, Whitehorse and Iqaluit, whose call signs begin with "CF" due to their historic association with the CBC's Frontier Coverage Package prior to the advent of microwave and satellite broadcasting.
Some stations that broadcast from smaller cities are private affiliates of the CBC, that is, stations which are owned by commercial broadcasters but predominantly incorporate CBC programming within their schedules. Such stations generally follow the CBC schedule, airing a minimum 40 hours per week of network programming. However, they may opt out of some CBC programming in order to air locally produced programs, syndicated series or programs purchased from other broadcasters, such as CTV Two, which do not have a broadcast outlet in the same market. In these cases, the CBC programming being displaced may be broadcast at a different time than the network, or may not be broadcast on the station at all. Most private affiliates generally opt out of CBC's afternoon schedule and Thursday night arts programming. Private affiliates carry the 10 p.m. broadcast of The National as a core part of the CBC schedule, but generally omitted the 11 p.m. repeat (which is no longer broadcast). Most private affiliates produce their own local newscasts for a duration of at least 35 minutes. Some of the private affiliates have begun adding CBC's overnight programming to their schedules since the network began broadcasting 24 hours a day.
Private CBC affiliates are not as common as they were in the past, as many such stations have been purchased either by the CBC itself or by Canwest Global or CHUM Limited, respectively becoming E! or A-Channel (later A, now CTV Two) stations. One private CBC affiliate, CHBC-TV in Kelowna, joined E! (then known as CH) on February 27, 2006. When a private CBC affiliate reaffiliates with another network, the CBC has normally added a retransmitter of its nearest O&O station to ensure that CBC service is continued. However, due to an agreement between CHBC and CFJC-TV in Kamloops, CFJC also disaffiliated from the CBC on February 27, 2006, but no retransmitters were installed in the licence area. Former private CBC affiliates CKPG-TV Prince George and CHAT-TV Medicine Hat disaffiliated on August 31, 2008 and joined E!, but the CBC announced it will not add new retransmitters to these areas. Incidentally, CFJC, CKPG and CHAT are all owned by an independent media company, Jim Pattison Group. With the closure of E! and other changes in the media landscape, several former CBC affiliates have since joined City or Global, or closed altogether.
According to filings to the Canadian Radio-television and Telecommunications Commission (CRTC) by Thunder Bay Electronics (owner of CBC's Thunder Bay affiliate CKPR-DT) and Bell Media (owner of CBC affiliates CFTK-TV in Terrace and CJDC-TV in Dawson Creek),[citation needed] the CBC informed them that it will not extend its association with any of its private affiliates beyond August 31, 2011. Incidentally, that was also the date for analogue to digital transition in Canada. Given recent practice and the CBC's decision not to convert any retransmitters to digital, even in markets with populations in the hundreds in thousands, it is not expected that the CBC will open new transmitters to replace its affiliates, and indeed may pare back its existing transmitter network. However, in March 2011, CKPR announced that it had come to a programming agreement with the CBC, in which the station will continue to provide CBC programming in Thunder Bay for a period of five years. On March 16, 2012, Astral Media announced the sale of its assets to Bell Media, owners of CTV and CTV Two, for $3.38 billion with CFTK and CJDC included in the acquisition. Whether the stations will remain CBC affiliates or become owned-and-operated stations of CTV or CTV Two following the completion of the merger is undetermined.
CBC Television stations can be received in many United States communities along the Canadian border over-the-air and have a significant audience in those areas. Such a phenomenon can also take place within Great Lakes communities such as Ashtabula, Ohio, which received programming from the CBC's London, Ontario, transmitter, based upon prevailing atmospheric conditions over Lake Erie. As of September 2010 CBC shut down its analogue transmitter and decided not to replace it with a digital transmitter. As a result, there is now a giant hole in the coverage of CBC in South-Western Ontario. Both CBC - Toronto and CBC - Windsor are both over 100 miles from London, ON and out of range for even the largest antennas[citation needed].
CBC's sports coverage has also attained high viewership in border markets, including its coverage of the NHL's Stanley Cup Playoffs, which was generally considered to be more complete and consistent than coverage by other networks such as NBC. Its coverage of the Olympic Games also found a significant audience in border regions, primarily due to the fact that CBC aired more events live than NBC's coverage, which had been criticized in recent years for tape delaying events to air in primetime, even if the event is being held in a market in the Pacific Time Zone during primetime hours on the East (where it would still be delayed for West coast primetime).
While its fellow Canadian broadcasters converted most of their transmitters to digital by the Canadian digital television transition deadline of August 31, 2011, CBC converted only about half of the analogue transmitters in mandatory areas to digital (15 of 28 markets with CBC Television stations, and 14 of 28 markets with Télévision de Radio-Canada stations). Due to financial difficulties reported by the corporation, the corporation published digital transition plans for none of its analogue retransmitters in mandatory markets to be converted to digital by the deadline. Under this plan, communities that receive analogue signals by rebroadcast transmitters in mandatory markets would lose their over-the-air signals as of the deadline. Rebroadcast transmitters account for 23 of the 48 CBC and Radio-Canada transmitters in mandatory markets. Mandatory markets losing both CBC and Radio-Canada over-the-air signals include London, Ontario (metropolitan area population 457,000) and Saskatoon, Saskatchewan (metro area population 257,000). In both of those markets, the corporation's television transmitters are the only ones that were not planned to be converted to digital by the deadline.
Because rebroadcast transmitters were not planned to be converted to digital, many markets stood to lose over-the-air coverage from CBC or Radio-Canada, or both. As a result, only seven of the markets subject to the August 31, 2011 transition deadline were planned to have both CBC and Radio-Canada in digital, and 13 other markets were planned to have either CBC or Radio-Canada in digital. In mid-August 2011, the CRTC granted the CBC an extension, until August 31, 2012, to continue operating its analogue transmitters in markets subject to the August 31, 2011 transition deadline. This CRTC decision prevented many markets subject to the transition deadline from losing signals for CBC or Radio-Canada, or both at the transition deadline. At the transition deadline, Barrie, Ontario lost both CBC and Radio-Canada signals as the CBC did not request that the CRTC allow these transmitters to continue operating.
In markets where a digital transmitter was installed, existing coverage areas were not necessarily maintained. For instance, the CBC implemented a digital transmitter covering Fredericton, New Brunswick in the place of the existing transmitter covering Saint John, New Brunswick and Fredericton, and decided to maintain analogue service to Saint John. According to CBC's application for this transmitter to the CRTC, the population served by the digital transmitter would be 113,930 people versus 303,465 served by the existing analogue transmitter. In Victoria, the replacement of the Vancouver analogue transmitters with digital ones only allowed only some northeastern parts of the metropolitan area (total population 330,000) to receive either CBC or Radio-Canada.
CBC announced on April 4, 2012, that it will shut down all of its approximately 620 analogue television transmitters on July 31, 2012 with no plans to install digital transmitters in their place, thus reducing the total number of the corporation's television transmitters across the country down to 27. According to the CBC, this would reduce the corporation's yearly costs by $10 million. No plans have been announced to use subchannels to maintain over-the-air signals for both CBC and Radio-Canada in markets where the corporation has one digital transmitter. In fact, in its CRTC application to shut down all of its analogue television transmitters, the CBC communicated its opposition to use of subchannels, citing costs, amongst other reasons.
On August 6, 2010, the CBC issued a press release stating that due to financial reasons, the CBC and Radio-Canada would only transition 27 transmitters total, one in each market where there was an originating station (i.e. a CBC or Radio-Canada television station located in that market). Further, the CBC stated in the release, that only 15 of the transmitters would be in place by August 31, 2011 due to lack of available funds, and that the remainder would not be on the air until as late as August 31, 2012. Additionally, the CBC stated in the release that it was asking the CRTC for permission to continue broadcasting in analogue until the identified transmitters for transition were up and running. At the time of the press release, only eight of the corporation's transmitters (four CBC and four Radio Canada) were broadcasting in digital.
On November 30, 2010, CBC's senior director of regulatory affairs issued a letter to the CRTC regarding CBC's plans for transitioning to digital. The letter states, "CBC/Radio-Canada will not be converting its analogue retransmitters in mandatory markets to digital after August 31, 2011."  On December 16, 2010, some months after the CRTC issued a bulletin reminding broadcasters that analog transmitters had to be shut off by the deadline in mandatory markets, the CBC revised the documents accompanying its August 6, 2010 news release to state that it had the money for and is striving to transition all 27 transmitters by August 31, 2011.
On March 23, 2011, the CRTC rejected an application by the CBC to install a digital transmitter serving Fredricton, New Brunswick in place of the analogue transmitter serving Fredericton and Saint John, New Brunswick, which would have served only 62.5% of the population served by the existing analogue transmitter. The CBC issued a press release stating "CBC/Radio-Canada intends to re-file its application with the CRTC to provide more detailed cost estimates that will allow the Commission to better understand the unfeasibility of replicating the Corporation’s current analogue coverage." The press release further added that the CBC suggests coverage could be maintained if the CRTC were to "allow CBC Television to continue providing the analogue service it offers today – much in the same way the Commission permitted recently in the case of Yellowknife, Whitehorse and Iqaluit."
On August 18, 2011, the CRTC issued a decision that allows CBC's mandatory market rebroadcasting transmitters in analogue to remain on-air until August 31, 2012. Before that deadline, CBC's licence renewal process would take place and CBC's digital transition plans would be examined as part of that process. The requirement remains for all of CBC's full-power transmitters occupying channels 52 to 69 to either relocate to channels 2 to 51 or become low-power transmitters. In some cases, CBC has opted to reduce the power of existing transmitters to low-power transmitters, which will result in signal loss for some viewers.
On July 17, 2012, the CRTC approved the shut down of CBC's analogue transmitters, noting that "while the Commission has the discretion to refuse to revoke broadcasting licences, even on application from a licensee, it cannot direct the CBC or any other broadcaster to continue to operate its stations and transmitters." On July 31, 2012, at around 11:59 p.m. in each time zone, the remaining 620 analogue transmitters were shut down, leaving the network with 27 digital television transmitters across the country, and some transmitters operated by some affiliated stations.
The Ministry of Defence (MoD) is the British government department responsible for implementing the defence policy set by Her Majesty's Government, and is the headquarters of the British Armed Forces.
The MoD states that its principal objectives are to defend the United Kingdom of Great Britain and Northern Ireland and its interests and to strengthen international peace and stability. With the collapse of the Soviet Union and the end of the Cold War, the MoD does not foresee any short-term conventional military threat; rather, it has identified weapons of mass destruction, international terrorism, and failed and failing states as the overriding threats to Britain's interests. The MoD also manages day-to-day running of the armed forces, contingency planning and defence procurement.
During the 1920s and 1930s, British civil servants and politicians, looking back at the performance of the state during World War I, concluded that there was a need for greater co-ordination between the three Services that made up the armed forces of the United Kingdom—the British Army, the Royal Navy, and the Royal Air Force. The formation of a united ministry of defence was rejected by David Lloyd George's coalition government in 1921; but the Chiefs of Staff Committee was formed in 1923, for the purposes of inter-Service co-ordination. As rearmament became a concern during the 1930s, Stanley Baldwin created the position of Minister for Coordination of Defence. Lord Chatfield held the post until the fall of Neville Chamberlain's government in 1940; his success was limited by his lack of control over the existing Service departments and his limited political influence.
Winston Churchill, on forming his government in 1940, created the office of Minister of Defence to exercise ministerial control over the Chiefs of Staff Committee and to co-ordinate defence matters. The post was held by the Prime Minister of the day until Clement Attlee's government introduced the Ministry of Defence Act of 1946. The new ministry was headed by a Minister of Defence who possessed a seat in the Cabinet. The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
From 1946 to 1964 five Departments of State did the work of the modern Ministry of Defence: the Admiralty, the War Office, the Air Ministry, the Ministry of Aviation, and an earlier form of the Ministry of Defence. These departments merged in 1964; the defence functions of the Ministry of Aviation Supply merged into the Ministry of Defence in 1971.
The Ministers and Chiefs of the Defence Staff are supported by a number of civilian, scientific and professional military advisors. The Permanent Under-Secretary of State for Defence (generally known as the Permanent Secretary) is the senior civil servant at the MoD. His or her role is to ensure the MoD operates effectively as a department of the government.
The current Chief of the Defence Staff, the professional head of the British Armed Forces, is General Sir Nicholas Houghton, late Green Howards. He is supported by the Vice Chief of the Defence Staff, by the professional heads of the three services of HM Armed Forces and by the Commander of Joint Forces Command.
There are also three Deputy Chiefs of the Defence Staff with particular remits, Deputy Chief of the Defence Staff (Capability), Deputy CDS (Personnel and Training) and Deputy CDS (Operations). The Surgeon General, represents the Defence Medical Services on the Defence Staff, and is the clinical head of that service.
Additionally, there are a number of Assistant Chiefs of Defence Staff, including the Assistant Chief of the Defence Staff (Reserves and Cadets) and the Defence Services Secretary in the Royal Household of the Sovereign of the United Kingdom, who is also the Assistant Chief of Defence Staff (Personnel).
The 1998 Strategic Defence Review and the 2003 Delivering Security in a Changing World White Paper outlined the following posture for the British Armed Forces:
The MoD has since been regarded as a leader in elaborating the post-Cold War organising concept of "defence diplomacy". As a result of the Strategic Defence and Security Review 2010, Prime Minister David Cameron signed a 50-year treaty with French President Nicolas Sarkozy that would have the two countries co-operate intensively in military matters. The UK is establishing air and naval bases in the Persian Gulf, located in the UAE and Bahrain. A presence in Oman is also being considered.
The Strategic Defence and Security Review 2015 included £178 billion investment in new equipment and capabilities. The review set a defence policy with four primary missions for the Armed Forces:
Following the end of the Cold War, the threat of direct conventional military confrontation with other states has been replaced by terrorism. Sir Richard Dannatt predicted British forces to be involved in combating "predatory non-state actors" for the foreseeable future, in what he called an "era of persistent conflict". He told the Chatham House think tank that the fight against al-Qaeda and other militant Islamist groups was "probably the fight of our generation".
Dannatt criticised a remnant "Cold War mentality", with military expenditures based on retaining a capability against a direct conventional strategic threat; He said currently only 10% of the MoD's equipment programme budget between 2003 and 2018 was to be invested in the "land environment"—at a time when Britain was engaged in land-based wars in Afghanistan and Iraq.
The Defence Committee—Third Report "Defence Equipment 2009" cites an article from the Financial Times website stating that the Chief of Defence Materiel, General Sir Kevin O’Donoghue, had instructed staff within Defence Equipment and Support (DE&S) through an internal memorandum to reprioritize the approvals process to focus on supporting current operations over the next three years; deterrence related programmes; those that reflect defence obligations both contractual or international; and those where production contracts are already signed. The report also cites concerns over potential cuts in the defence science and technology research budget; implications of inappropriate estimation of Defence Inflation within budgetary processes; underfunding in the Equipment Programme; and a general concern over striking the appropriate balance over a short-term focus (Current Operations) and long-term consequences of failure to invest in the delivery of future UK defence capabilities on future combatants and campaigns. The then Secretary of State for Defence, Bob Ainsworth MP, reinforced this reprioritisation of focus on current operations and had not ruled out "major shifts" in defence spending. In the same article the First Sea Lord and Chief of the Naval Staff, Admiral Sir Mark Stanhope, Royal Navy, acknowledged that there was not enough money within the defence budget and it is preparing itself for tough decisions and the potential for cutbacks. According to figures published by the London Evening Standard the defence budget for 2009 is "more than 10% overspent" (figures cannot be verified) and the paper states that this had caused Gordon Brown to say that the defence spending must be cut. The MoD has been investing in IT to cut costs and improve services for its personnel.
The Ministry of Defence is one of the United Kingdom's largest landowners, owning 227,300 hectares of land and foreshore (either freehold or leasehold) at April 2014, which was valued at "about £20 billion". The MoD also has "rights of access" to a further 222,000 hectares. In total, this is about 1.8% of the UK land mass. The total annual cost to support the defence estate is "in excess of £3.3 billion".
The defence estate is divided as training areas & ranges (84.0%), research & development (5.4%), airfields (3.4%), barracks & camps (2.5%), storage & supply depots (1.6%), and other (3.0%). These are largely managed by the Defence Infrastructure Organisation.
The headquarters of the MoD are in Whitehall and are now known as Main Building. This structure is neoclassical in style and was originally built between 1938 and 1959 to designs by Vincent Harris to house the Air Ministry and the Board of Trade. The northern entrance in Horse Guards Avenue is flanked by two monumental statues, Earth and Water, by Charles Wheeler. Opposite stands the Gurkha Monument, sculpted by Philip Jackson and unveiled in 1997 by Queen Elizabeth II. Within it is the Victoria Cross and George Cross Memorial, and nearby are memorials to the Fleet Air Arm and RAF (to its east, facing the riverside). A major refurbishment of the building was completed under a PFI contract by Skanska in 2004.
Henry VIII's wine cellar at the Palace of Whitehall, built in 1514–1516 for Cardinal Wolsey, is in the basement of Main Building, and is used for entertainment. The entire vaulted brick structure of the cellar was encased in steel and concrete and relocated nine feet to the west and nearly 19 feet (5.8 m) deeper in 1949, when construction was resumed at the site after World War II. This was carried out without any significant damage to the structure.
The most notable fraud conviction was that of Gordon Foxley, head of defence procurement at the Ministry of Defence from 1981 to 1984. Police claimed he received at least £3.5m in total in corrupt payments, such as substantial bribes from overseas arms contractors aiming to influence the allocation of contracts.
A government report covered by the Guardian in 2002 indicates that between 1940 and 1979, the Ministry of Defence "turned large parts of the country into a giant laboratory to conduct a series of secret germ warfare tests on the public" and many of these tests "involved releasing potentially dangerous chemicals and micro-organisms over vast swaths of the population without the public being told." The Ministry of Defence claims that these trials were to simulate germ warfare and that the tests were harmless. Still, families who have been in the area of many of the tests are experiencing children with birth defects and physical and mental handicaps and many are asking for a public inquiry. According to the report these tests affected estimated millions of people including one period between 1961 and 1968 where "more than a million people along the south coast of England, from Torquay to the New Forest, were exposed to bacteria including e.coli and bacillus globigii, which mimics anthrax." Two scientists commissioned by the Ministry of Defence stated that these trials posed no risk to the public. This was confirmed by Sue Ellison, a representative of Porton Down who said that the results from these trials "will save lives, should the country or our forces face an attack by chemical and biological weapons." Asked whether such tests are still being carried out, she said: "It is not our policy to discuss ongoing research." It is unknown whether or not the harmlessness of the trials was known at the time of their occurrence.
The MoD has been criticised for an ongoing fiasco, having spent £240m on eight Chinook HC3 helicopters which only started to enter service in 2010, years after they were ordered in 1995 and delivered in 2001. A National Audit Office report reveals that the helicopters have been stored in air conditioned hangars in Britain since their 2001[why?] delivery, while troops in Afghanistan have been forced to rely on helicopters which are flying with safety faults. By the time the Chinooks are airworthy, the total cost of the project could be as much as £500m.
In April 2008, a £90m contract was signed with Boeing for a "quick fix" solution, so they can fly by 2010: QinetiQ will downgrade the Chinooks—stripping out some of their more advanced equipment.
In October 2009, the MoD was heavily criticized for withdrawing the bi-annual non-operational training £20m budget for the volunteer Territorial Army (TA), ending all non-operational training for 6 months until April 2010. The government eventually backed down and restored the funding. The TA provides a small percentage of the UK's operational troops. Its members train on weekly evenings and monthly weekends, as well as two-week exercises generally annually and occasionally bi-annually for troops doing other courses. The cuts would have meant a significant loss of personnel and would have had adverse effects on recruitment.
In 2013 it was found that the Ministry of Defence had overspent on its equipment budget by £6.5bn on orders that could take up to 39 years to fulfil. The Ministry of Defence has been criticised in the past for poor management and financial control, investing in projects that have taken up to 10 and even as much as 15 years to be delivered.
Boston (pronounced i/ˈbɒstən/) is the capital and largest city of the Commonwealth of Massachusetts in the United States. Boston also served as the historic county seat of Suffolk County until Massachusetts disbanded county government in 1999. The city proper covers 48 square miles (124 km2) with an estimated population of 655,884 in 2014, making it the largest city in New England and the 24th largest city in the United States. The city is the economic and cultural anchor of a substantially larger metropolitan area called Greater Boston, home to 4.7 million people and the tenth-largest metropolitan statistical area in the country. Greater Boston as a commuting region is home to 8.1 million people, making it the sixth-largest combined statistical area in the United States.
One of the oldest cities in the United States, Boston was founded on the Shawmut Peninsula in 1630 by Puritan settlers from England. It was the scene of several key events of the American Revolution, such as the Boston Massacre, the Boston Tea Party, the Battle of Bunker Hill, and the Siege of Boston. Upon American independence from Great Britain, the city continued to be an important port and manufacturing hub, as well as a center for education and culture. Through land reclamation and municipal annexation, Boston has expanded beyond the original peninsula. Its rich history attracts many tourists, with Faneuil Hall alone drawing over 20 million visitors per year. Boston's many firsts include the United States' first public school, Boston Latin School (1635), and first subway system (1897).
The area's many colleges and universities make Boston an international center of higher education and medicine, and the city is considered to be a world leader in innovation. Boston's economic base also includes finance, professional and business services, biotechnology, information technology, and government activities. Households in the city claim the highest average rate of philanthropy in the United States; businesses and institutions rank amongst the top in the country for environmental sustainability and investment. The city has one of the highest costs of living in the United States, though it remains high on world livability rankings.
Boston's early European settlers had first called the area Trimountaine (after its "three mountains"—only traces of which remain today) but later renamed it Boston after Boston, Lincolnshire, England, the origin of several prominent colonists. The renaming, on September 7, 1630 (Old Style),[b] was by Puritan colonists from England, who had moved over from Charlestown earlier that year in quest of fresh water. Their settlement was initially limited to the Shawmut Peninsula, at that time surrounded by the Massachusetts Bay and Charles River and connected to the mainland by a narrow isthmus. The peninsula is known to have been inhabited as early as 5000 BC.
In 1629, the Massachusetts Bay Colony's first governor, John Winthrop, led the signing of the Cambridge Agreement, a key founding document of the city. Puritan ethics and their focus on education influenced its early history; America's first public school was founded in Boston in 1635. Over the next 130 years, the city participated in four French and Indian Wars, until the British defeated the French and their native allies in North America. Boston was the largest town in British North America until Philadelphia grew larger in the mid 18th century.
Many of the crucial events of the American Revolution—the Boston Massacre, the Boston Tea Party, Paul Revere's midnight ride, the battles of Lexington and Concord and Bunker Hill, the Siege of Boston, and many others—occurred in or near Boston. After the Revolution, Boston's long seafaring tradition helped make it one of the world's wealthiest international ports, with rum, fish, salt, and tobacco being particularly important.
The Embargo Act of 1807, adopted during the Napoleonic Wars, and the War of 1812 significantly curtailed Boston's harbor activity. Although foreign trade returned after these hostilities, Boston's merchants had found alternatives for their capital investments in the interim. Manufacturing became an important component of the city's economy, and by the mid-19th century, the city's industrial manufacturing overtook international trade in economic importance. Until the early 20th century, Boston remained one of the nation's largest manufacturing centers and was notable for its garment production and leather-goods industries. A network of small rivers bordering the city and connecting it to the surrounding region facilitated shipment of goods and led to a proliferation of mills and factories. Later, a dense network of railroads furthered the region's industry and commerce.
During this period Boston flourished culturally as well, admired for its rarefied literary life and generous artistic patronage, with members of old Boston families—eventually dubbed Boston Brahmins—coming to be regarded as the nation's social and cultural elites.
Boston was an early port of the Atlantic triangular slave trade in the New England colonies, but was soon overtaken by Salem, Massachusetts and Newport, Rhode Island. Eventually Boston became a center of the abolitionist movement. The city reacted strongly to the Fugitive Slave Law of 1850, contributing to President Franklin Pierce's attempt to make an example of Boston after the Anthony Burns Fugitive Slave Case.
In 1822, the citizens of Boston voted to change the official name from "the Town of Boston" to "the City of Boston", and on March 4, 1822, the people of Boston accepted the charter incorporating the City. At the time Boston was chartered as a city, the population was about 46,226, while the area of the city was only 4.7 square miles (12 km2).
In the 1820s, Boston's population grew rapidly, and the city's ethnic composition changed dramatically with the first wave of European immigrants. Irish immigrants dominated the first wave of newcomers during this period, especially following the Irish Potato Famine; by 1850, about 35,000 Irish lived in Boston. In the latter half of the 19th century, the city saw increasing numbers of Irish, Germans, Lebanese, Syrians, French Canadians, and Russian and Polish Jews settled in the city. By the end of the 19th century, Boston's core neighborhoods had become enclaves of ethnically distinct immigrants—Italians inhabited the North End, Irish dominated South Boston and Charlestown, and Russian Jews lived in the West End. Irish and Italian immigrants brought with them Roman Catholicism. Currently, Catholics make up Boston's largest religious community, and since the early 20th century, the Irish have played a major role in Boston politics—prominent figures include the Kennedys, Tip O'Neill, and John F. Fitzgerald.
Between 1631 and 1890, the city tripled its area through land reclamation by filling in marshes, mud flats, and gaps between wharves along the waterfront. The largest reclamation efforts took place during the 19th century; beginning in 1807, the crown of Beacon Hill was used to fill in a 50-acre (20 ha) mill pond that later became the Haymarket Square area. The present-day State House sits atop this lowered Beacon Hill. Reclamation projects in the middle of the century created significant parts of the South End, the West End, the Financial District, and Chinatown. After The Great Boston Fire of 1872, workers used building rubble as landfill along the downtown waterfront. During the mid-to-late 19th century, workers filled almost 600 acres (2.4 km2) of brackish Charles River marshlands west of Boston Common with gravel brought by rail from the hills of Needham Heights. The city annexed the adjacent towns of South Boston (1804), East Boston (1836), Roxbury (1868), Dorchester (including present day Mattapan and a portion of South Boston) (1870), Brighton (including present day Allston) (1874), West Roxbury (including present day Jamaica Plain and Roslindale) (1874), Charlestown (1874), and Hyde Park (1912). Other proposals, for the annexation of Brookline, Cambridge, and Chelsea, were unsuccessful.
By the early and mid-20th century, the city was in decline as factories became old and obsolete, and businesses moved out of the region for cheaper labor elsewhere. Boston responded by initiating various urban renewal projects under the direction of the Boston Redevelopment Authority (BRA), which was established in 1957. In 1958, BRA initiated a project to improve the historic West End neighborhood. Extensive demolition was met with vociferous public opposition.
The BRA subsequently reevaluated its approach to urban renewal in its future projects, including the construction of Government Center. In 1965, the first Community Health Center in the United States opened, the Columbia Point Health Center, in the Dorchester neighborhood. It mostly served the massive Columbia Point public housing complex adjoining it, which was built in 1953. The health center is still in operation and was rededicated in 1990 as the Geiger-Gibson Community Health Center. The Columbia Point complex itself was redeveloped and revitalized into a mixed-income community called Harbor Point Apartments from 1984 to 1990. By the 1970s, the city's economy boomed after 30 years of economic downturn. A large number of high rises were constructed in the Financial District and in Boston's Back Bay during this time period. This boom continued into the mid-1980s and later began again. Hospitals such as Massachusetts General Hospital, Beth Israel Deaconess Medical Center, and Brigham and Women's Hospital lead the nation in medical innovation and patient care. Schools such as Boston College, Boston University, the Harvard Medical School, Northeastern University, Wentworth Institute of Technology, Berklee College of Music and Boston Conservatory attract students to the area. Nevertheless, the city experienced conflict starting in 1974 over desegregation busing, which resulted in unrest and violence around public schools throughout the mid-1970s.
Boston is an intellectual, technological, and political center but has lost some important regional institutions, including the acquisition of The Boston Globe by The New York Times, and the loss to mergers and acquisitions of local financial institutions such as FleetBoston Financial, which was acquired by Charlotte-based Bank of America in 2004. Boston-based department stores Jordan Marsh and Filene's have both been merged into the Cincinnati–based Macy's. Boston has experienced gentrification in the latter half of the 20th century, with housing prices increasing sharply since the 1990s. Living expenses have risen, and Boston has one of the highest costs of living in the United States, and was ranked the 129th most expensive major city in the world in a 2011 survey of 214 cities. Despite cost of living issues, Boston ranks high on livability ratings, ranking 36th worldwide in quality of living in 2011 in a survey of 221 major cities.
On April 15, 2013, two Chechen Islamist brothers exploded two bombs near the finish line of the Boston Marathon, killing three people and injuring roughly 264.
Boston has an area of 89.6 square miles (232.1 km2)—48.4 square miles (125.4 km2) (54.0%) of land and 41.2 square miles (106.7 km2) (46.0%) of water. The city's official elevation, as measured at Logan International Airport, is 19 ft (5.8 m) above sea level. The highest point in Boston is Bellevue Hill at 330 feet (100 m) above sea level, and the lowest point is at sea level. Situated onshore of the Atlantic Ocean, Boston is the only state capital in the contiguous United States with an oceanic coastline.
Boston is surrounded by the "Greater Boston" region and is contiguously bordered by the cities and towns of Winthrop, Revere, Chelsea, Everett, Somerville, Cambridge, Newton, Brookline, Needham, Dedham, Canton, Milton, and Quincy. The Charles River separates Boston from Watertown and the majority of Cambridge, and the mass of Boston from its own Charlestown neighborhood. To the east lie Boston Harbor and the Boston Harbor Islands National Recreation Area (which includes part of the city's territory, specifically Calf Island, Gallops Island, Great Brewster Island, Green Island, Little Brewster Island, Little Calf Island, Long Island, Lovells Island, Middle Brewster Island, Nixes Mate, Outer Brewster Island, Rainsford Island, Shag Rocks, Spectacle Island, The Graves, and Thompson Island). The Neponset River forms the boundary between Boston's southern neighborhoods and the city of Quincy and the town of Milton. The Mystic River separates Charlestown from Chelsea and Everett, and Chelsea Creek and Boston Harbor separate East Boston from Boston proper.
Boston is sometimes called a "city of neighborhoods" because of the profusion of diverse subsections; the city government's Office of Neighborhood Services has officially designated 23 neighborhoods.
More than two-thirds of inner Boston's modern land area did not exist when the city was founded, but was created via the gradual filling in of the surrounding tidal areas over the centuries, notably with earth from the leveling or lowering of Boston's three original hills (the "Trimountain", after which Tremont Street is named), and with gravel brought by train from Needham to fill the Back Bay. Downtown and its immediate surroundings consists largely of low-rise (often Federal style and Greek Revival) masonry buildings, interspersed with modern highrises, notably in the Financial District, Government Center, and South Boston. Back Bay includes many prominent landmarks, such as the Boston Public Library, Christian Science Center, Copley Square, Newbury Street, and New England's two tallest buildings—the John Hancock Tower and the Prudential Center. Near the John Hancock Tower is the old John Hancock Building with its prominent illuminated beacon, the color of which forecasts the weather. Smaller commercial areas are interspersed among areas of single-family homes and wooden/brick multi-family row houses. The South End Historic District is the largest surviving contiguous Victorian-era neighborhood in the US. The geography of downtown and South Boston was particularly impacted by the Central Artery/Tunnel Project (known unofficially as the "Big Dig"), which allowed for the removal of the unsightly elevated Central Artery and the incorporation of new green spaces and open areas.
Boston has a continental climate with some maritime influence, and using the −3 °C (27 °F) coldest month (January) isotherm, the city lies within the transition zone from a humid subtropical climate (Köppen Cfa) to a humid continental climate (Köppen Dfa), although the suburbs north and west of the city are significantly colder in winter and solidly fall under the latter categorization; the city lies at the transition between USDA plant hardiness zones 6b (most of the city) and 7a (Downtown, South Boston, and East Boston neighborhoods). Summers are typically warm to hot, rainy, and humid, while winters oscillate between periods of cold rain and snow, with cold temperatures. Spring and fall are usually mild, with varying conditions dependent on wind direction and jet stream positioning. Prevailing wind patterns that blow offshore minimize the influence of the Atlantic Ocean. The hottest month is July, with a mean temperature of 73.4 °F (23.0 °C). The coldest month is January, with a mean of 29.0 °F (−1.7 °C). Periods exceeding 90 °F (32 °C) in summer and below freezing in winter are not uncommon but rarely extended, with about 13 and 25 days per year seeing each, respectively. The most recent sub-0 °F (−18 °C) reading occurring on February 14, 2016 when the temperature dipped down to −9 °F (−23 °C), the coldest reading since 1957. In addition, several decades may pass between 100 °F (38 °C) readings, with the most recent such occurrence on July 22, 2011 when the temperature reached 103 °F (39 °C). The city's average window for freezing temperatures is November 9 through April 5.[c] Official temperature records have ranged from −18 °F (−28 °C) on February 9, 1934, up to 104 °F (40 °C) on July 4, 1911; the record cold daily maximum is 2 °F (−17 °C) on December 30, 1917, while, conversely, the record warm daily minimum is 83 °F (28 °C) on August 2, 1975.
Boston's coastal location on the North Atlantic moderates its temperature, but makes the city very prone to Nor'easter weather systems that can produce much snow and rain. The city averages 43.8 inches (1,110 mm) of precipitation a year, with 43.8 inches (111 cm) of snowfall per season. Snowfall increases dramatically as one goes inland away from the city (especially north and west of the city)—away from the moderating influence of the ocean. Most snowfall occurs from December through March, as most years see no measurable snow in April and November, and snow is rare in May and October. There is also high year-to-year variability in snowfall; for instance, the winter of 2011–12 saw only 9.3 in (23.6 cm) of accumulating snow, but the previous winter, the corresponding figure was 81.0 in (2.06 m).[d]
Fog is fairly common, particularly in spring and early summer, and the occasional tropical storm or hurricane can threaten the region, especially in late summer and early autumn. Due to its situation along the North Atlantic, the city often receives sea breezes, especially in the late spring, when water temperatures are still quite cold and temperatures at the coast can be more than 20 °F (11 °C) colder than a few miles inland, sometimes dropping by that amount near midday. Thunderstorms occur from May to September, that are occasionally severe with large hail, damaging winds and heavy downpours. Although downtown Boston has never been struck by a violent tornado, the city itself has experienced many tornado warnings. Damaging storms are more common to areas north, west, and northwest of the city. Boston has a relatively sunny climate for a coastal city at its latitude, averaging over 2,600 hours of sunshine per annum.
In 2010, Boston was estimated to have 617,594 residents (a density of 12,200 persons/sq mile, or 4,700/km2) living in 272,481 housing units— a 5% population increase over 2000. The city is the third most densely populated large U.S. city of over half a million residents. Some 1.2 million persons may be within Boston's boundaries during work hours, and as many as 2 million during special events. This fluctuation of people is caused by hundreds of thousands of suburban residents who travel to the city for work, education, health care, and special events.
In the city, the population was spread out with 21.9% at age 19 and under, 14.3% from 20 to 24, 33.2% from 25 to 44, 20.4% from 45 to 64, and 10.1% who were 65 years of age or older. The median age was 30.8 years. For every 100 females, there were 92.0 males. For every 100 females age 18 and over, there were 89.9 males. There were 252,699 households, of which 20.4% had children under the age of 18 living in them, 25.5% were married couples living together, 16.3% had a female householder with no husband present, and 54.0% were non-families. 37.1% of all households were made up of individuals and 9.0% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 3.08.
The median household income in Boston was $51,739, while the median income for a family was $61,035. Full-time year-round male workers had a median income of $52,544 versus $46,540 for full-time year-round female workers. The per capita income for the city was $33,158. 21.4% of the population and 16.0% of families are below the poverty line. Of the total population, 28.8% of those under the age of 18 and 20.4% of those 65 and older were living below the poverty line.
In 1950, whites represented 94.7% of Boston's population. From the 1950s to the end of the 20th century, the proportion of non-Hispanic whites in the city declined; in 2000, non-Hispanic whites made up 49.5% of the city's population, making the city majority-minority for the first time. However, in recent years the city has experienced significant gentrification, in which affluent whites have moved into formerly non-white areas. In 2006, the US Census Bureau estimated that non-Hispanic whites again formed a slight majority. But as of 2010, in part due to the housing crash, as well as increased efforts to make more affordable housing more available, the minority population has rebounded. This may also have to do with an increased Latino population and more clarity surrounding US Census statistics, which indicate a Non-Hispanic White population of 47 percent (some reports give slightly lower figures).
People of Irish descent form the largest single ethnic group in the city, making up 15.8% of the population, followed by Italians, accounting for 8.3% of the population. People of West Indian and Caribbean ancestry are another sizable group, at 6.0%, about half of whom are of Haitian ancestry. Over 27,000 Chinese Americans made their home in Boston city proper in 2013, and the city hosts a growing Chinatown accommodating heavily traveled Chinese-owned bus lines to and from Chinatown, Manhattan. Some neighborhoods, such as Dorchester, have received an influx of people of Vietnamese ancestry in recent decades. Neighborhoods such as Jamaica Plain and Roslindale have experienced a growing number of Dominican Americans. The city and greater area also has a growing immigrant population of South Asians, including the tenth-largest Indian community in the country.
The city has a sizable Jewish population with an estimated 25,000 Jews within the city and 227,000 within the Boston metro area; the number of congregations in Boston is estimated at 22. The adjacent communities of Brookline and Newton are both approximately one-third Jewish.
The city, especially the East Boston neighborhood, has a significant Hispanic community. Hispanics in Boston are mostly of Puerto Rican (30,506 or 4.9% of total city population), Dominican (25,648 or 4.2% of total city population), Salvadoran (10,850 or 1.8% of city population), Colombian (6,649 or 1.1% of total city population), Mexican (5,961 or 1.0% of total city population), and Guatemalan (4,451 or 0.7% of total city population) ethnic origin. When including all Hispanic national origins, they number 107,917. In Greater Boston, these numbers grow significantly with Puerto Ricans numbering 175,000+, Dominicans 95,000+, Salvadorans 40,000+, Guatemalans 31,000+, Mexicans 25,000+, and Colombians numbering 22,000+.
According to a 2014 study by the Pew Research Center, 57% of the population of the city identified themselves as Christians, with 25% professing attendance at a variety of churches that could be considered Protestant, and 29% professing Roman Catholic beliefs. while 33% claim no religious affiliation. The same study says that other religions (including Judaism, Buddhism, Islam, and Hinduism) collectively make up about 10% of the population.
As of 2010 the Catholic Church had the highest number of adherents as a single denomination in the Boston-Cambridge-Newton Metro area, with more than two million members and 339 churches, followed by the Episcopal Church with 58,000 adherents in 160 churches. The United Church of Christ had 55,000 members and 213 churches. The UCC is the successor of the city's Puritan religious traditions. Old South Church in Boston is one of the oldest congregations in the United States. It was organized in 1669 by dissenters from the First Church in Boston (1630). Notable past members include Samuel Adams, William Dawes, Benjamin Franklin, Samuel Sewall, and Phillis Wheatley. In 1773, Adams gave the signals from the Old South Meeting House that started the Boston Tea Party.
A global city, Boston is placed among the top 30 most economically powerful cities in the world. Encompassing $363 billion, the Greater Boston metropolitan area has the sixth-largest economy in the country and 12th-largest in the world.
Boston's colleges and universities have a significant effect on the regional economy. Boston attracts more than 350,000 college students from around the world, who contribute more than $4.8 billion annually to the city's economy. The area's schools are major employers and attract industries to the city and surrounding region. The city is home to a number of technology companies and is a hub for biotechnology, with the Milken Institute rating Boston as the top life sciences cluster in the country. Boston receives the highest absolute amount of annual funding from the National Institutes of Health of all cities in the United States.
The city is considered highly innovative for a variety of reasons, including the presence of academia, access to venture capital, and the presence of many high-tech companies. The Route 128 corridor and Greater Boston continue to be a major center for venture capital investment, and high technology remains an important sector.
Tourism also composes a large part of Boston's economy, with 21.2 million domestic and international visitors spending $8.3 billion in 2011; excluding visitors from Canada and Mexico, over 1.4 million international tourists visited Boston in 2014, with those from China and the United Kingdom leading the list. Boston's status as a state capital as well as the regional home of federal agencies has rendered law and government to be another major component of the city's economy. The city is a major seaport along the United States' East Coast and the oldest continuously operated industrial and fishing port in the Western Hemisphere.
Other important industries are financial services, especially mutual funds and insurance. Boston-based Fidelity Investments helped popularize the mutual fund in the 1980s and has made Boston one of the top financial cities in the United States. The city is home to the headquarters of Santander Bank, and Boston is a center for venture capital firms. State Street Corporation, which specializes in asset management and custody services, is based in the city. Boston is a printing and publishing center — Houghton Mifflin Harcourt is headquartered within the city, along with Bedford-St. Martin's Press and Beacon Press. Pearson PLC publishing units also employ several hundred people in Boston. The city is home to three major convention centers—the Hynes Convention Center in the Back Bay, and the Seaport World Trade Center and Boston Convention and Exhibition Center on the South Boston waterfront. The General Electric Corporation announced in January 2016 its decision to move the company's global headquarters to the Seaport District in Boston, from Fairfield, Connecticut, citing factors including Boston's preeminence in the realm of higher education.
The Boston Public Schools enrolls 57,000 students attending 145 schools, including the renowned Boston Latin Academy, John D. O'Bryant School of Math & Science, and Boston Latin School. The Boston Latin School, established 1635, is the oldest public high school in the US; Boston also operates the United States' second oldest public high school, and its oldest public elementary school. The system's students are 40% Hispanic or Latino, 35% Black or African American, 13% White, and 9% Asian. There are private, parochial, and charter schools as well, and approximately 3,300 minority students attend participating suburban schools through the Metropolitan Educational Opportunity Council.
Some of the most renowned and highly ranked universities in the world are located in the Boston area. Four members of the Association of American Universities are in Greater Boston (more than any other metropolitan area): Harvard University, the Massachusetts Institute of Technology, Boston University, and Brandeis University. Hospitals, universities, and research institutions in Greater Boston received more than $1.77 billion in National Institutes of Health grants in 2013, more money than any other American metropolitan area. Greater Boston has more than 100 colleges and universities, with 250,000 students enrolled in Boston and Cambridge alone. Its largest private universities include Boston University (the city's fourth-largest employer) with its main campus along Commonwealth Avenue and a medical campus in the South End; Northeastern University in the Fenway area; Suffolk University near Beacon Hill, which includes law school and business school; and Boston College, which straddles the Boston (Brighton)–Newton border. Boston's only public university is the University of Massachusetts Boston, on Columbia Point in Dorchester. Roxbury Community College and Bunker Hill Community College are the city's two public community colleges. Altogether, Boston's colleges and universities employ over 42,600 people, accounting for nearly 7 percent of the city's workforce.
Smaller private schools include Babson College, Bentley University, Boston Architectural College, Emmanuel College, Fisher College, MGH Institute of Health Professions, Massachusetts College of Pharmacy and Health Sciences, Simmons College, Wellesley College, Wheelock College, Wentworth Institute of Technology, New England School of Law (originally established as America's first all female law school), and Emerson College.
Metropolitan Boston is home to several conservatories and art schools, including Lesley University College of Art and Design, Massachusetts College of Art, the School of the Museum of Fine Arts, New England Institute of Art, New England School of Art and Design (Suffolk University), Longy School of Music of Bard College, and the New England Conservatory (the oldest independent conservatory in the United States). Other conservatories include the Boston Conservatory and Berklee College of Music, which has made Boston an important city for jazz music.
Several universities located outside Boston have a major presence in the city. Harvard University, the nation's oldest institute of higher education, is centered across the Charles River in Cambridge but has the majority of its land holdings and a substantial amount of its educational activities in Boston. Its business, medical, dental, and public health schools are located in Boston's Allston and Longwood neighborhoods. Harvard has plans for additional expansion into Allston. The Massachusetts Institute of Technology (MIT), which originated in Boston and was long known as "Boston Tech", moved across the river to Cambridge in 1916. Tufts University, whose main campus is north of the city in Somerville and Medford, locates its medical and dental school in Boston's Chinatown at Tufts Medical Center, a 451-bed academic medical institution that is home to both a full-service hospital for adults and the Floating Hospital for Children.
Like many major American cities, Boston has seen a great reduction in violent crime since the early 1990s. Boston's low crime rate since the 1990s has been credited to the Boston Police Department's collaboration with neighborhood groups and church parishes to prevent youths from joining gangs, as well as involvement from the United States Attorney and District Attorney's offices. This helped lead in part to what has been touted as the "Boston Miracle". Murders in the city dropped from 152 in 1990 (for a murder rate of 26.5 per 100,000 people) to just 31—not one of them a juvenile—in 1999 (for a murder rate of 5.26 per 100,000).
In 2008, there were 62 reported homicides. Through December 20 each of 2014 and 2015, the Boston Police Department reported 52 and 39 homicides, respectively.
Boston shares many cultural roots with greater New England, including a dialect of the non-rhotic Eastern New England accent known as Boston English, and a regional cuisine with a large emphasis on seafood, salt, and dairy products. Irish Americans are a major influence on Boston's politics and religious institutions. Boston also has its own collection of neologisms known as Boston slang.
Boston has been called the "Athens of America" for its literary culture, earning a reputation as "the intellectual capital of the United States." In the nineteenth century, Ralph Waldo Emerson, Henry David Thoreau, Nathaniel Hawthorne, Margaret Fuller, James Russell Lowell, and Henry Wadsworth Longfellow wrote in Boston. Some consider the Old Corner Bookstore, where these writers met and where The Atlantic Monthly was first published, to be "cradle of American literature. In 1852, the Boston Public Library was founded as the first free library in the United States. Boston's literary culture continues today thanks to the city's many universities and the Boston Book Festival.
Music is cherished in Boston. The Boston Symphony Orchestra is one of the "Big Five," a group of the greatest American orchestras, and the classical music magazine Gramophone called it one of the "world's best" orchestras. Symphony Hall (located west of Back Bay) is home to the Boston Symphony Orchestra, (and the related Boston Youth Symphony Orchestra, which is the largest youth orchestra in the nation) and the Boston Pops Orchestra. The British newspaper The Guardian called Boston Symphony Hall "one of the top venues for classical music in the world," adding that "Symphony Hall in Boston was where science became an essential part of concert hall design." Other concerts are held at the New England Conservatory's Jordan Hall. The Boston Ballet performs at the Boston Opera House. Other performing-arts organizations located in the city include the Boston Lyric Opera Company, Opera Boston, Boston Baroque (the first permanent Baroque orchestra in the US), and the Handel and Haydn Society (one of the oldest choral companies in the United States). The city is a center for contemporary classical music with a number of performing groups, several of which are associated with the city's conservatories and universities. These include the Boston Modern Orchestra Project and Boston Musica Viva. Several theaters are located in or near the Theater District south of Boston Common, including the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre.
There are several major annual events such as First Night, which occurs on New Year's Eve, the Boston Early Music Festival, the annual Boston Arts Festival at Christopher Columbus Waterfront Park, and Italian summer feasts in the North End honoring Catholic saints. The city is the site of several events during the Fourth of July period. They include the week-long Harborfest festivities and a Boston Pops concert accompanied by fireworks on the banks of the Charles River.
Because of the city's prominent role in the American Revolution, several historic sites relating to that period are preserved as part of the Boston National Historical Park. Many are found along the Freedom Trail, which is marked by a red line of bricks embedded in the ground. The city is also home to several art museums, including the Museum of Fine Arts and the Isabella Stewart Gardner Museum. The Institute of Contemporary Art is housed in a contemporary building designed by Diller Scofidio + Renfro in the Seaport District. The University of Massachusetts Boston campus on Columbia Point houses the John F. Kennedy Library. The Boston Athenaeum (one of the oldest independent libraries in the United States), Boston Children's Museum, Bull & Finch Pub (whose building is known from the television show Cheers), Museum of Science, and the New England Aquarium are within the city.
Boston has been a noted religious center from its earliest days. The Roman Catholic Archdiocese of Boston serves nearly 300 parishes and is based in the Cathedral of the Holy Cross (1875) in the South End, while the Episcopal Diocese of Massachusetts, with the Cathedral Church of St. Paul (1819) as its episcopal seat, serves just under 200 congregations. Unitarian Universalism has its headquarters on Beacon Hill. The Christian Scientists are headquartered in Back Bay at the Mother Church (1894). The oldest church in Boston is First Church in Boston, founded in 1630. King's Chapel, the city's first Anglican church, was founded in 1686 and converted to Unitarianism in 1785. Other churches include Christ Church (better known as Old North Church, 1723), the oldest church building in the city, Trinity Church (1733), Park Street Church (1809), Old South Church (1874), Jubilee Christian Church and Basilica and Shrine of Our Lady of Perpetual Help on Mission Hill (1878).
Air quality in Boston is generally very good: during the ten-year period 2004–2013, there were only 4 days in which the air was unhealthy for the general public, according to the EPA.
Some of the cleaner energy facilities in Boston include the Allston green district, with three ecologically compatible housing facilities. Boston is also breaking ground on multiple green affordable housing facilities to help reduce the carbon footprint of the city while simultaneously making these initiatives financially available to a greater population. Boston's climate plan is updated every three years and was most recently modified in 2013. This legislature includes the Building Energy Reporting and Disclosure Ordinance, which requires the city's larger buildings to disclose their yearly energy and water use statistics and partake in an energy assessment every five years. These statistics are made public by the city, thereby increasing incentives for buildings to be more environmentally conscious.
Another initiative, presented by the late Mayor Thomas Menino, is the Renew Boston Whole Building Incentive, which reduces the cost of living in buildings that are deemed energy efficient. This, much like the green housing developments, gives people of low socioeconomic status an opportunity to find housing in communities that support the environment. The ultimate goal of this initiative is to enlist 500 Bostonians to participate in a free, in-home energy assessment.
Many older buildings in certain areas of Boston are supported by wooden piles driven into the area's fill; these piles remain sound if submerged in water, but are subject to dry rot if exposed to air for long periods. Groundwater levels have been dropping, to varying degrees, in many areas of the city, due in part to an increase in the amount of rainwater discharged directly into sewers rather than absorbed by the ground. A city agency, the Boston Groundwater Trust, coordinates monitoring of groundwater levels throughout the city via a network of public and private monitoring wells. However, Boston's drinking water supply, from the Quabbin and Wachusett Reservoirs to the west, is one of the very few in the country so pure as to satisfy federal water quality standards without filtration.
Boston has teams in the four major North American professional sports leagues plus Major League Soccer, and has won 36 championships in these leagues, As of 2014[update]. It is one of six cities (along with Chicago, Detroit, Los Angeles, New York and Philadelphia) to have won championships in all four major sports. It has been suggested that Boston is the new "TitleTown, USA", as the city's professional sports teams have won nine championships since 2001: Patriots (2001, 2003, 2004, and 2014), Red Sox (2004, 2007, and 2013), Celtics (2008), and Bruins (2011). This love of sports has made Boston the United States Olympic Committee's choice to bid to hold the 2024 Summer Olympic Games, but the city cited financial concerns when it withdrew its bid on July 27, 2015.
The Boston Red Sox, a founding member of the American League of Major League Baseball in 1901, play their home games at Fenway Park, near Kenmore Square in the city's Fenway section. Built in 1912, it is the oldest sports arena or stadium in active use in the United States among the four major professional American sports leagues, encompassing Major League Baseball, the National Football League, National Basketball Association, and the National Hockey League. Boston was the site of the first game of the first modern World Series, in 1903. The series was played between the AL Champion Boston Americans and the NL champion Pittsburgh Pirates. Persistent reports that the team was known in 1903 as the "Boston Pilgrims" appear to be unfounded. Boston's first professional baseball team was the Red Stockings, one of the charter members of the National Association in 1871, and of the National League in 1876. The team played under that name until 1883, under the name Beaneaters until 1911, and under the name Braves from 1912 until they moved to Milwaukee after the 1952 season. Since 1966 they have played in Atlanta as the Atlanta Braves.
The TD Garden, formerly called the FleetCenter and built to replace the old, since-demolished Boston Garden, is adjoined to North Station and is the home of two major league teams: the Boston Bruins of the National Hockey League and the Boston Celtics of the National Basketball Association. The arena seats 18,624 for basketball games and 17,565 for ice hockey games. The Bruins were the first American member of the National Hockey League and an Original Six franchise. The Boston Celtics were founding members of the Basketball Association of America, one of the two leagues that merged to form the NBA. The Celtics have the distinction of having won more championships than any other NBA team, with seventeen.
While they have played in suburban Foxborough since 1971, the New England Patriots of the National Football League were founded in 1960 as the Boston Patriots, changing their name after relocating. The team won the Super Bowl after the 2001, 2003, 2004, and 2014 seasons. They share Gillette Stadium with the New England Revolution of Major League Soccer. The Boston Breakers of Women's Professional Soccer, which formed in 2009, play their home games at Dilboy Stadium in Somerville.
The area's many colleges and universities are active in college athletics. Four NCAA Division I members play in the city—Boston College, Boston University, Harvard University, and Northeastern University. Of the four, only Boston College participates in college football at the highest level, the Football Bowl Subdivision. Harvard participates in the second-highest level, the Football Championship Subdivision.
One of the best known sporting events in the city is the Boston Marathon, the 26.2-mile (42.2 km) race which is the world's oldest annual marathon, run on Patriots' Day in April. On April 15, 2013, two explosions killed three people and injured hundreds at the marathon. Another major annual event is the Head of the Charles Regatta, held in October.
Boston Common, located near the Financial District and Beacon Hill, is the oldest public park in the United States. Along with the adjacent Boston Public Garden, it is part of the Emerald Necklace, a string of parks designed by Frederick Law Olmsted to encircle the city. The Emerald Necklace includes Jamaica Pond, Boston's largest body of freshwater, and Franklin Park, the city's largest park and home of the Franklin Park Zoo. Another major park is the Esplanade, located along the banks of the Charles River. The Hatch Shell, an outdoor concert venue, is located adjacent to the Charles River Esplanade. Other parks are scattered throughout the city, with the major parks and beaches located near Castle Island; in Charlestown; and along the Dorchester, South Boston, and East Boston shorelines.
Boston's park system is well-reputed nationally. In its 2013 ParkScore ranking, The Trust for Public Land reported that Boston was tied with Sacramento and San Francisco for having the third-best park system among the 50 most populous US cities. ParkScore ranks city park systems by a formula that analyzes the city's median park size, park acres as percent of city area, the percent of residents within a half-mile of a park, spending of park services per resident, and the number of playgrounds per 10,000 residents.
Boston has a strong mayor – council government system in which the mayor (elected every fourth year) has extensive executive power. Marty Walsh became Mayor in January 2014, his predecessor Thomas Menino's twenty-year tenure having been the longest in the city's history. The Boston City Council is elected every two years; there are nine district seats, and four citywide "at-large" seats. The School Committee, which oversees the Boston Public Schools, is appointed by the mayor.
In addition to city government, numerous commissions and state authorities—including the Massachusetts Department of Conservation and Recreation, the Boston Public Health Commission, the Massachusetts Water Resources Authority (MWRA), and the Massachusetts Port Authority (Massport)—play a role in the life of Bostonians. As the capital of Massachusetts, Boston plays a major role in state politics.
The city has several federal facilities, including the John F. Kennedy Federal Office Building, the Thomas P. O'Neill Federal Building, the United States Court of Appeals for the First Circuit, the United States District Court for the District of Massachusetts, and the Federal Reserve Bank of Boston.
Federally, Boston is split between two congressional districts. The northern three-fourths of the city is in the 7th district, represented by Mike Capuano since 1998. The southern fourth is in the 8th district, represented by Stephen Lynch. Both are Democrats; a Republican has not represented a significant portion of Boston in over a century. The state's senior member of the United States Senate is Democrat Elizabeth Warren, first elected in 2012. The state's junior member of the United States Senate is Democrat Ed Markey, who was elected in 2013 to succeed John Kerry after Kerry's appointment and confirmation as the United States Secretary of State.
The Boston Globe and the Boston Herald are two of the city's major daily newspapers. The city is also served by other publications such as Boston magazine, The Improper Bostonian, DigBoston, and the Boston edition of Metro. The Christian Science Monitor, headquartered in Boston, was formerly a worldwide daily newspaper but ended publication of daily print editions in 2009, switching to continuous online and weekly magazine format publications. The Boston Globe also releases a teen publication to the city's public high schools, called Teens in Print or T.i.P., which is written by the city's teens and delivered quarterly within the school year.
The city's growing Latino population has given rise to a number of local and regional Spanish-language newspapers. These include El Planeta (owned by the former publisher of The Boston Phoenix), El Mundo, and La Semana. Siglo21, with its main offices in nearby Lawrence, is also widely distributed.
Various LGBT publications serve the city's large LGBT (lesbian, gay, bisexual and transgender) community such as The Rainbow Times, the only minority and lesbian-owned LGBT newsmagazine. Founded in 2006, The Rainbow Times is now based out of Boston, but serves all of New England.
Boston is the largest broadcasting market in New England, with the radio market being the 11th largest in the United States. Several major AM stations include talk radio WRKO, sports/talk station WEEI, and CBS Radio WBZ. WBZ (AM) broadcasts a news radio format. A variety of commercial FM radio formats serve the area, as do NPR stations WBUR and WGBH. College and university radio stations include WERS (Emerson), WHRB (Harvard), WUMB (UMass Boston), WMBR (MIT), WZBC (Boston College), WMFO (Tufts University), WBRS (Brandeis University), WTBU (Boston University, campus and web only), WRBB (Northeastern University) and WMLN-FM (Curry College).
The Boston television DMA, which also includes Manchester, New Hampshire, is the 8th largest in the United States. The city is served by stations representing every major American network, including WBZ-TV and its sister station WSBK-TV (the former a CBS O&O, the latter an MyNetwork TV affiliate), WCVB-TV (ABC), WHDH (NBC), WFXT (Fox), and WLVI (The CW). The city is also home to PBS station WGBH-TV, a major producer of PBS programs, which also operates WGBX. Spanish-language television networks, including MundoFox (WFXZ-CD), Univision (WUNI), Telemundo (WNEU), and Telefutura (WUTF-DT), have a presence in the region, with WNEU and WUTF serving as network owned-and-operated stations. Most of the area's television stations have their transmitters in nearby Needham and Newton along the Route 128 corridor. Six Boston television stations are carried by Canadian satellite television provider Bell TV and by cable television providers in Canada.
The Longwood Medical and Academic Area, adjacent to the Fenway district, is home to a large number of medical and research facilities, including Beth Israel Deaconess Medical Center, Brigham and Women's Hospital, Children's Hospital Boston, Dana-Farber Cancer Institute, Harvard Medical School, Joslin Diabetes Center, and the Massachusetts College of Pharmacy and Health Sciences. Prominent medical facilities, including Massachusetts General Hospital, Massachusetts Eye and Ear Infirmary and Spaulding Rehabilitation Hospital are located in the Beacon Hill area. St. Elizabeth's Medical Center is in Brighton Center of the city's Brighton neighborhood. New England Baptist Hospital is in Mission Hill. The city has Veterans Affairs medical centers in the Jamaica Plain and West Roxbury neighborhoods. The Boston Public Health Commission, an agency of the Massachusetts government, oversees health concerns for city residents. Boston EMS provides pre-hospital emergency medical services to residents and visitors.
Many of Boston's medical facilities are associated with universities. The facilities in the Longwood Medical and Academic Area and in Massachusetts General Hospital are affiliated with Harvard Medical School. Tufts Medical Center (formerly Tufts-New England Medical Center), located in the southern portion of the Chinatown neighborhood, is affiliated with Tufts University School of Medicine. Boston Medical Center, located in the South End neighborhood, is the primary teaching facility for the Boston University School of Medicine as well as the largest trauma center in the Boston area; it was formed by the merger of Boston University Hospital and Boston City Hospital, which was the first municipal hospital in the United States.
Logan Airport, located in East Boston and operated by the Massachusetts Port Authority (Massport), is Boston's principal airport. Nearby general aviation airports are Beverly Municipal Airport to the north, Hanscom Field to the west, and Norwood Memorial Airport to the south. Massport also operates several major facilities within the Port of Boston, including a cruise ship terminal and facilities to handle bulk and container cargo in South Boston, and other facilities in Charlestown and East Boston.
Downtown Boston's streets grew organically, so they do not form a planned grid, unlike those in later-developed Back Bay, East Boston, the South End, and South Boston. Boston is the eastern terminus of I-90, which in Massachusetts runs along the Massachusetts Turnpike. The elevated portion of the Central Artery, which carried most of the through traffic in downtown Boston, was replaced with the O'Neill Tunnel during the Big Dig, substantially completed in early 2006.
With nearly a third of Bostonians using public transit for their commute to work, Boston has the fifth-highest rate of public transit usage in the country. Boston's subway system, the Massachusetts Bay Transportation Authority (MBTA—known as the "T") operates the oldest underground rapid transit system in the Americas, and is the fourth-busiest rapid transit system in the country, with 65.5 miles (105 km) of track on four lines. The MBTA also operates busy bus and commuter rail networks, and water shuttles.
Amtrak's Northeast Corridor and Chicago lines originate at South Station, which serves as a major intermodal transportation hub, and stop at Back Bay. Fast Northeast Corridor trains, which serve New York City, Washington, D.C., and points in between, also stop at Route 128 Station in the southwestern suburbs of Boston. Meanwhile, Amtrak's Downeaster service to Maine originates at North Station, despite the current lack of a dedicated passenger rail link between the two railhubs, other than the "T" subway lines.
Nicknamed "The Walking City", Boston hosts more pedestrian commuters than do other comparably populated cities. Owing to factors such as the compactness of the city and large student population, 13 percent of the population commutes by foot, making it the highest percentage of pedestrian commuters in the country out of the major American cities. In 2011, Walk Score ranked Boston the third most walkable city in the United States. As of 2015[update], Walk Score still ranks Boston as the third most walkable US city, with a Walk Score of 80, a Transit Score of 75, and a Bike Score of 70.
Between 1999 and 2006, Bicycling magazine named Boston three times as one of the worst cities in the US for cycling; regardless, it has one of the highest rates of bicycle commuting. In 2008, as a consequence of improvements made to bicycling conditions within the city, the same magazine put Boston on its "Five for the Future" list as a "Future Best City" for biking, and Boston's bicycle commuting percentage increased from 1% in 2000 to 2.1% in 2009. The bikeshare program called Hubway launched in late July 2011, logging more than 140,000 rides before the close of its first season. The neighboring municipalities of Cambridge, Somerville, and Brookline joined the Hubway program in summer 2012.
The emergence of resistance of bacteria to antibiotics is a common phenomenon. Emergence of resistance often reflects evolutionary processes that take place during antibiotic therapy. The antibiotic treatment may select for bacterial strains with physiologically or genetically enhanced capacity to survive high doses of antibiotics. Under certain conditions, it may result in preferential growth of resistant bacteria, while growth of susceptible bacteria is inhibited by the drug. For example, antibacterial selection for strains having previously acquired antibacterial-resistance genes was demonstrated in 1943 by the Luria–Delbrück experiment. Antibiotics such as penicillin and erythromycin, which used to have a high efficacy against many bacterial species and strains, have become less effective, due to the increased resistance of many bacterial strains.
The successful outcome of antimicrobial therapy with antibacterial compounds depends on several factors. These include host defense mechanisms, the location of infection, and the pharmacokinetic and pharmacodynamic properties of the antibacterial. A bactericidal activity of antibacterials may depend on the bacterial growth phase, and it often requires ongoing metabolic activity and division of bacterial cells. These findings are based on laboratory studies, and in clinical settings have also been shown to eliminate bacterial infection. Since the activity of antibacterials depends frequently on its concentration, in vitro characterization of antibacterial activity commonly includes the determination of the minimum inhibitory concentration and minimum bactericidal concentration of an antibacterial. To predict clinical outcome, the antimicrobial activity of an antibacterial is usually combined with its pharmacokinetic profile, and several pharmacological parameters are used as markers of drug efficacy.
Antibacterial antibiotics are commonly classified based on their mechanism of action, chemical structure, or spectrum of activity. Most target bacterial functions or growth processes. Those that target the bacterial cell wall (penicillins and cephalosporins) or the cell membrane (polymyxins), or interfere with essential bacterial enzymes (rifamycins, lipiarmycins, quinolones, and sulfonamides) have bactericidal activities. Those that target protein synthesis (macrolides, lincosamides and tetracyclines) are usually bacteriostatic (with the exception of bactericidal aminoglycosides). Further categorization is based on their target specificity. "Narrow-spectrum" antibacterial antibiotics target specific types of bacteria, such as Gram-negative or Gram-positive bacteria, whereas broad-spectrum antibiotics affect a wide range of bacteria. Following a 40-year hiatus in discovering new classes of antibacterial compounds, four new classes of antibacterial antibiotics have been brought into clinical use in the late 2000s and early 2010s: cyclic lipopeptides (such as daptomycin), glycylcyclines (such as tigecycline), oxazolidinones (such as linezolid), and lipiarmycins (such as fidaxomicin).
With advances in medicinal chemistry, most modern antibacterials are semisynthetic modifications of various natural compounds. These include, for example, the beta-lactam antibiotics, which include the penicillins (produced by fungi in the genus Penicillium), the cephalosporins, and the carbapenems. Compounds that are still isolated from living organisms are the aminoglycosides, whereas other antibacterials—for example, the sulfonamides, the quinolones, and the oxazolidinones—are produced solely by chemical synthesis. Many antibacterial compounds are relatively small molecules with a molecular weight of less than 2000 atomic mass units.[citation needed]
Antibiotics revolutionized medicine in the 20th century, and have together with vaccination led to the near eradication of diseases such as tuberculosis in the developed world. Their effectiveness and easy access led to overuse, especially in livestock raising, prompting bacteria to develop resistance. This has led to widespread problems with antimicrobial and antibiotic resistance, so much as to prompt the World Health Organization to classify antimicrobial resistance as a "serious threat [that] is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country".
In empirical therapy, a patient has proven or suspected infection, but the responsible microorganism is not yet unidentified. While the microorgainsim is being identified the doctor will usually administer the best choice of antibiotic that will be most active against the likely cause of infection usually a broad spectrum antibiotic. Empirical therapy is usually initiated before the doctor knows the exact identification of microorgansim causing the infection as the identification process make take several days in the laboratory.
Antibiotics are screened for any negative effects on humans or other mammals before approval for clinical use, and are usually considered safe and most are well tolerated. However, some antibiotics have been associated with a range of adverse side effects. Side-effects range from mild to very serious depending on the antibiotics used, the microbial organisms targeted, and the individual patient. Side effects may reflect the pharmacological or toxicological properties of the antibiotic or may involve hypersensitivity reactions or anaphylaxis. Safety profiles of newer drugs are often not as well established as for those that have a long history of use. Adverse effects range from fever and nausea to major allergic reactions, including photodermatitis and anaphylaxis. Common side-effects include diarrhea, resulting from disruption of the species composition in the intestinal flora, resulting, for example, in overgrowth of pathogenic bacteria, such as Clostridium difficile. Antibacterials can also affect the vaginal flora, and may lead to overgrowth of yeast species of the genus Candida in the vulvo-vaginal area. Additional side-effects can result from interaction with other drugs, such as elevated risk of tendon damage from administration of a quinolone antibiotic with a systemic corticosteroid. Some scientists have hypothesized that the indiscriminate use of antibiotics alter the host microbiota and this has been associated with chronic disease.
Exposure to antibiotics early in life is associated with increased body mass in humans and mouse models. Early life is a critical period for the establishment of the intestinal microbiota and for metabolic development. Mice exposed to subtherapeutic antibiotic treatment (STAT)– with either penicillin, vancomycin, penicillin and vancomycin, or chlortetracycline had altered composition of the gut microbiota as well as its metabolic capabilities. Moreover, research have shown that mice given low-dose penicillin (1 μg/g body weight) around birth and throughout the weaning process had an increased body mass and fat mass, accelerated growth, and increased hepatic expression of genes involved in adipogenesis, compared to controlled mice. In addition, penicillin in combination with a high-fat diet increased fasting insulin levels in mice. However, it is unclear whether or not antibiotics cause obesity in humans. Studies have found a correlation between early exposure of antibiotics (<6 months) and increased body mass (at 10 and 20 months). Another study found that the type of antibiotic exposure was also significant with the highest risk of being overweight in those given macrolides compared to penicillin and cephalosporin. Therefore, there is correlation between antibiotic exposure in early life and obesity in humans, but whether or not there is a causal relationship remains unclear. Although there is a correlation between antibiotic use in early life and obesity, the effect of antibiotics on obesity in humans needs to be weighed against the beneficial effects of clinically indicated treatment with antibiotics in infancy.
The majority of studies indicate antibiotics do interfere with contraceptive pills, such as clinical studies that suggest the failure rate of contraceptive pills caused by antibiotics is very low (about 1%). In cases where antibacterials have been suggested to affect the efficiency of birth control pills, such as for the broad-spectrum antibacterial rifampicin, these cases may be due to an increase in the activities of hepatic liver enzymes' causing increased breakdown of the pill's active ingredients. Effects on the intestinal flora, which might result in reduced absorption of estrogens in the colon, have also been suggested, but such suggestions have been inconclusive and controversial. Clinicians have recommended that extra contraceptive measures be applied during therapies using antibacterials that are suspected to interact with oral contraceptives.
Interactions between alcohol and certain antibiotics may occur and may cause side-effects and decreased effectiveness of antibiotic therapy. While moderate alcohol consumption is unlikely to interfere with many common antibiotics, there are specific types of antibiotics with which alcohol consumption may cause serious side-effects. Therefore, potential risks of side-effects and effectiveness depend on the type of antibiotic administered. Despite the lack of a categorical counterindication, the belief that alcohol and antibiotics should never be mixed is widespread.
Several molecular mechanisms of antibacterial resistance exist. Intrinsic antibacterial resistance may be part of the genetic makeup of bacterial strains. For example, an antibiotic target may be absent from the bacterial genome. Acquired resistance results from a mutation in the bacterial chromosome or the acquisition of extra-chromosomal DNA. Antibacterial-producing bacteria have evolved resistance mechanisms that have been shown to be similar to, and may have been transferred to, antibacterial-resistant strains. The spread of antibacterial resistance often occurs through vertical transmission of mutations during growth and by genetic recombination of DNA by horizontal genetic exchange. For instance, antibacterial resistance genes can be exchanged between different bacterial strains or species via plasmids that carry these resistance genes. Plasmids that carry several different resistance genes can confer resistance to multiple antibacterials. Cross-resistance to several antibacterials may also occur when a resistance mechanism encoded by a single gene conveys resistance to more than one antibacterial compound.
Antibacterial-resistant strains and species, sometimes referred to as "superbugs", now contribute to the emergence of diseases that were for a while well controlled. For example, emergent bacterial strains causing tuberculosis (TB) that are resistant to previously effective antibacterial treatments pose many therapeutic challenges. Every year, nearly half a million new cases of multidrug-resistant tuberculosis (MDR-TB) are estimated to occur worldwide. For example, NDM-1 is a newly identified enzyme conveying bacterial resistance to a broad range of beta-lactam antibacterials. The United Kingdom's Health Protection Agency has stated that "most isolates with NDM-1 enzyme are resistant to all standard intravenous antibiotics for treatment of severe infections."
Inappropriate antibiotic treatment and overuse of antibiotics have contributed to the emergence of antibiotic-resistant bacteria. Self prescription of antibiotics is an example of misuse. Many antibiotics are frequently prescribed to treat symptoms or diseases that do not respond to antibiotics or that are likely to resolve without treatment. Also, incorrect or suboptimal antibiotics are prescribed for certain bacterial infections. The overuse of antibiotics, like penicillin and erythromycin, has been associated with emerging antibiotic resistance since the 1950s. Widespread usage of antibiotics in hospitals has also been associated with increases in bacterial strains and species that no longer respond to treatment with the most common antibiotics.
Common forms of antibiotic misuse include excessive use of prophylactic antibiotics in travelers and failure of medical professionals to prescribe the correct dosage of antibiotics on the basis of the patient's weight and history of prior use. Other forms of misuse include failure to take the entire prescribed course of the antibiotic, incorrect dosage and administration, or failure to rest for sufficient recovery. Inappropriate antibiotic treatment, for example, is their prescription to treat viral infections such as the common cold. One study on respiratory tract infections found "physicians were more likely to prescribe antibiotics to patients who appeared to expect them". Multifactorial interventions aimed at both physicians and patients can reduce inappropriate prescription of antibiotics.
Several organizations concerned with antimicrobial resistance are lobbying to eliminate the unnecessary use of antibiotics. The issues of misuse and overuse of antibiotics have been addressed by the formation of the US Interagency Task Force on Antimicrobial Resistance. This task force aims to actively address antimicrobial resistance, and is coordinated by the US Centers for Disease Control and Prevention, the Food and Drug Administration (FDA), and the National Institutes of Health (NIH), as well as other US agencies. An NGO campaign group is Keep Antibiotics Working. In France, an "Antibiotics are not automatic" government campaign started in 2002 and led to a marked reduction of unnecessary antibiotic prescriptions, especially in children.
The emergence of antibiotic resistance has prompted restrictions on their use in the UK in 1970 (Swann report 1969), and the EU has banned the use of antibiotics as growth-promotional agents since 2003. Moreover, several organizations (e.g., The American Society for Microbiology (ASM), American Public Health Association (APHA) and the American Medical Association (AMA)) have called for restrictions on antibiotic use in food animal production and an end to all nontherapeutic uses.[citation needed] However, commonly there are delays in regulatory and legislative actions to limit the use of antibiotics, attributable partly to resistance against such regulation by industries using or selling antibiotics, and to the time required for research to test causal links between their use and resistance to them. Two federal bills (S.742 and H.R. 2562) aimed at phasing out nontherapeutic use of antibiotics in US food animals were proposed, but have not passed. These bills were endorsed by public health and medical organizations, including the American Holistic Nurses' Association, the American Medical Association, and the American Public Health Association (APHA).
There has been extensive use of antibiotics in animal husbandry. In the United States, the question of emergence of antibiotic-resistant bacterial strains due to use of antibiotics in livestock was raised by the US Food and Drug Administration (FDA) in 1977. In March 2012, the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock, which violated FDA regulations.
Before the early 20th century, treatments for infections were based primarily on medicinal folklore. Mixtures with antimicrobial properties that were used in treatments of infections were described over 2000 years ago. Many ancient cultures, including the ancient Egyptians and ancient Greeks, used specially selected mold and plant materials and extracts to treat infections. More recent observations made in the laboratory of antibiosis between microorganisms led to the discovery of natural antibacterials produced by microorganisms. Louis Pasteur observed, "if we could intervene in the antagonism observed between some bacteria, it would offer perhaps the greatest hopes for therapeutics". The term 'antibiosis', meaning "against life", was introduced by the French bacteriologist Jean Paul Vuillemin as a descriptive name of the phenomenon exhibited by these early antibacterial drugs. Antibiosis was first described in 1877 in bacteria when Louis Pasteur and Robert Koch observed that an airborne bacillus could inhibit the growth of Bacillus anthracis. These drugs were later renamed antibiotics by Selman Waksman, an American microbiologist, in 1942. Synthetic antibiotic chemotherapy as a science and development of antibacterials began in Germany with Paul Ehrlich in the late 1880s. Ehrlich noted certain dyes would color human, animal, or bacterial cells, whereas others did not. He then proposed the idea that it might be possible to create chemicals that would act as a selective drug that would bind to and kill bacteria without harming the human host. After screening hundreds of dyes against various organisms, in 1907, he discovered a medicinally useful drug, the synthetic antibacterial salvarsan now called arsphenamine.
The effects of some types of mold on infection had been noticed many times over the course of history (see: History of penicillin). In 1928, Alexander Fleming noticed the same effect in a Petri dish, where a number of disease-causing bacteria were killed by a fungus of the genus Penicillium. Fleming postulated that the effect is mediated by an antibacterial compound he named penicillin, and that its antibacterial properties could be exploited for chemotherapy. He initially characterized some of its biological properties, and attempted to use a crude preparation to treat some infections, but he was unable to pursue its further development without the aid of trained chemists.
The first sulfonamide and first commercially available antibacterial, Prontosil, was developed by a research team led by Gerhard Domagk in 1932 at the Bayer Laboratories of the IG Farben conglomerate in Germany. Domagk received the 1939 Nobel Prize for Medicine for his efforts. Prontosil had a relatively broad effect against Gram-positive cocci, but not against enterobacteria. Research was stimulated apace by its success. The discovery and development of this sulfonamide drug opened the era of antibacterials.
In 1939, coinciding with the start of World War II, Rene Dubos reported the discovery of the first naturally derived antibiotic, tyrothricin, a compound of 20% gramicidin and 80% tyrocidine, from B. brevis. It was one of the first commercially manufactured antibiotics universally and was very effective in treating wounds and ulcers during World War II. Gramicidin, however, could not be used systemically because of toxicity. Tyrocidine also proved too toxic for systemic usage. Research results obtained during that period were not shared between the Axis and the Allied powers during the war.
Florey and Chain succeeded in purifying the first penicillin, penicillin G, in 1942, but it did not become widely available outside the Allied military before 1945. Later, Norman Heatley developed the back extraction technique for efficiently purifying penicillin in bulk. The chemical structure of penicillin was determined by Dorothy Crowfoot Hodgkin in 1945. Purified penicillin displayed potent antibacterial activity against a wide range of bacteria and had low toxicity in humans. Furthermore, its activity was not inhibited by biological constituents such as pus, unlike the synthetic sulfonamides. The discovery of such a powerful antibiotic was unprecedented, and the development of penicillin led to renewed interest in the search for antibiotic compounds with similar efficacy and safety. For their successful development of penicillin, which Fleming had accidentally discovered but could not develop himself, as a therapeutic drug, Ernst Chain and Howard Florey shared the 1945 Nobel Prize in Medicine with Fleming. Florey credited Dubos with pioneering the approach of deliberately and systematically searching for antibacterial compounds, which had led to the discovery of gramicidin and had revived Florey's research in penicillin.
Vaccines rely on immune modulation or augmentation. Vaccination either excites or reinforces the immune competence of a host to ward off infection, leading to the activation of macrophages, the production of antibodies, inflammation, and other classic immune reactions. Antibacterial vaccines have been responsible for a drastic reduction in global bacterial diseases. Vaccines made from attenuated whole cells or lysates have been replaced largely by less reactogenic, cell-free vaccines consisting of purified components, including capsular polysaccharides and their conjugates, to protein carriers, as well as inactivated toxins (toxoids) and proteins.
Phage therapy is another option that is being looked into for treating resistant strains of bacteria. The way that researchers are doing this is by infecting pathogenic bacteria with their own viruses, more specifically, bacteriophages. Bacteriophages, also known simply as phages, are precisely bacterial viruses that infect bacteria by disrupting pathogenic bacterium lytic cycles. By disrupting the lytic cycles of bacterium, phages destroy their metabolism, which eventually results in the cell's death. Phages will insert their DNA into the bacterium, allowing their DNA to be transcribed. Once their DNA is transcribed the cell will proceed to make new phages and as soon as they are ready to be released, the cell will lyse. One of the worries about using phages to fight pathogens is that the phages will infect "good" bacteria, or the bacteria that are important in the everyday function of human beings. However, studies have proven that phages are very specific when they target bacteria, which makes researchers confident that bacteriophage therapy is the definite route to defeating antibiotic resistant bacteria.
In April 2013, the Infectious Disease Society of America (IDSA) reported that the weak antibiotic pipeline does not match bacteria's increasing ability to develop resistance. Since 2009, only 2 new antibiotics were approved in the United States. The number of new antibiotics approved for marketing per year declines continuously. The report identified seven antibiotics against the Gram-negative bacilli (GNB) currently in phase 2 or phase 3 clinical trials. However, these drugs do not address the entire spectrum of resistance of GNB. Some of these antibiotics are combination of existent treatments:
Possible improvements include clarification of clinical trial regulations by FDA. Furthermore, appropriate economic incentives could persuade pharmaceutical companies to invest in this endeavor. Antibiotic Development to Advance Patient Treatment (ADAPT) Act aims to fast track the drug development to combat the growing threat of 'superbugs'. Under this Act, FDA can approve antibiotics and antifungals treating life-threatening infections based on smaller clinical trials. The CDC will monitor the use of antibiotics and the emerging resistance, and publish the data. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints', will provide accurate data to healthcare professionals. According to Allan Coukell, senior director for health programs at The Pew Charitable Trusts, "By allowing drug developers to rely on smaller datasets, and clarifying FDA's authority to tolerate a higher level of uncertainty for these drugs when making a risk/benefit calculation, ADAPT would make the clinical trials more feasible."
The contemporary Liberal Party generally advocates economic liberalism (see New Right). Historically, the party has supported a higher degree of economic protectionism and interventionism than it has in recent decades. However, from its foundation the party has identified itself as anti-socialist. Strong opposition to socialism and communism in Australia and abroad was one of its founding principles. The party's founder and longest-serving leader Robert Menzies envisaged that Australia's middle class would form its main constituency.
Throughout their history, the Liberals have been in electoral terms largely the party of the middle class (whom Menzies, in the era of the party's formation called "The forgotten people"), though such class-based voting patterns are no longer as clear as they once were. In the 1970s a left-wing middle class emerged that no longer voted Liberal.[citation needed] One effect of this was the success of a breakaway party, the Australian Democrats, founded in 1977 by former Liberal minister Don Chipp and members of minor liberal parties; other members of the left-leaning section of the middle-class became Labor supporters.[citation needed] On the other hand, the Liberals have done increasingly well in recent years among socially conservative working-class voters.[citation needed]However the Liberal Party's key support base remains the upper-middle classes; 16 of the 20 richest federal electorates are held by the Liberals, most of which are safe seats. In country areas they either compete with or have a truce with the Nationals, depending on various factors.
Domestically, Menzies presided over a fairly regulated economy in which utilities were publicly owned, and commercial activity was highly regulated through centralised wage-fixing and high tariff protection. Liberal leaders from Menzies to Malcolm Fraser generally maintained Australia's high tariff levels. At that time the Liberals' coalition partner, the Country Party, the older of the two in the coalition (now known as the "National Party"), had considerable influence over the government's economic policies. It was not until the late 1970s and through their period out of power federally in the 1980s that the party came to be influenced by what was known as the "New Right" – a conservative liberal group who advocated market deregulation, privatisation of public utilities, reductions in the size of government programs and tax cuts.
The Liberals' immediate predecessor was the United Australia Party (UAP). More broadly, the Liberal Party's ideological ancestry stretched back to the anti-Labor groupings in the first Commonwealth parliaments. The Commonwealth Liberal Party was a fusion of the Free Trade Party and the Protectionist Party in 1909 by the second prime minister, Alfred Deakin, in response to Labor's growing electoral prominence. The Commonwealth Liberal Party merged with several Labor dissidents (including Billy Hughes) to form the Nationalist Party of Australia in 1917. That party, in turn, merged with Labor dissidents to form the UAP in 1931.
The UAP had been formed as a new conservative alliance in 1931, with Labor defector Joseph Lyons as its leader. The stance of Lyons and other Labor rebels against the more radical proposals of the Labor movement to deal the Great Depression had attracted the support of prominent Australian conservatives. With Australia still suffering the effects of the Great Depression, the newly formed party won a landslide victory at the 1931 Election, and the Lyons Government went on to win three consecutive elections. It largely avoided Keynesian pump-priming and pursued a more conservative fiscal policy of debt reduction and balanced budgets as a means of stewarding Australia out of the Depression. Lyons' death in 1939 saw Robert Menzies assume the Prime Ministership on the eve of war. Menzies served as Prime Minister from 1939 to 1941 but resigned as leader of the minority World War II government amidst an unworkable parliamentary majority. The UAP, led by Billy Hughes, disintegrated after suffering a heavy defeat in the 1943 election.
Menzies called a conference of conservative parties and other groups opposed to the ruling Australian Labor Party, which met in Canberra on 13 October 1944 and again in Albury, New South Wales in December 1944. From 1942 onward Menzies had maintained his public profile with his series of "The Forgotten People" radio talks–similar to Franklin D. Roosevelt's "fireside chats" of the 1930s–in which he spoke of the middle class as the "backbone of Australia" but as nevertheless having been "taken for granted" by political parties.
The formation of the party was formally announced at Sydney Town Hall on 31 August 1945. It took the name "Liberal" in honour of the old Commonwealth Liberal Party. The new party was dominated by the remains of the old UAP; with few exceptions, the UAP party room became the Liberal party room. The Australian Women's National League, a powerful conservative women's organisation, also merged with the new party. A conservative youth group Menzies had set up, the Young Nationalists, was also merged into the new party. It became the nucleus of the Liberal Party's youth division, the Young Liberals. By September 1945 there were more than 90,000 members, many of whom had not previously been members of any political party.
After an initial loss to Labor at the 1946 election, Menzies led the Liberals to victory at the 1949 election, and the party stayed in office for a record 23 years—still the longest unbroken run in government at the federal level. Australia experienced prolonged economic growth during the post-war boom period of the Menzies Government (1949–1966) and Menzies fulfilled his promises at the 1949 election to end rationing of butter, tea and petrol and provided a five-shilling endowment for first-born children, as well as for others. While himself an unashamed anglophile, Menzies' government concluded a number of major defence and trade treaties that set Australia on its post-war trajectory out of Britain's orbit; opened Australia to multi-ethnic immigration; and instigated important legal reforms regarding Aboriginal Australians.
Menzies came to power the year the Communist Party of Australia had led a coal strike to improve pit miners' working conditions. That same year Joseph Stalin's Soviet Union exploded its first atomic bomb, and Mao Zedong led the Communist Party of China to power in China; a year later came the invasion of South Korea by Communist North Korea. Anti-communism was a key political issue of the 1950s and 1960s. Menzies was firmly anti-Communist; he committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of that war. The Labor Party split over concerns about the influence of the Communist Party over the Trade Union movement, leading to the foundation of the breakaway Democratic Labor Party whose preferences supported the Liberal and Country parties.
In 1951, during the early stages of the Cold War, Menzies spoke of the possibility of a looming third world war. The Menzies Government entered Australia's first formal military alliance outside of the British Commonwealth with the signing of the ANZUS Treaty between Australia, New Zealand and the United States in San Francisco in 1951. External Affairs Minister Percy Spender had put forward the proposal to work along similar lines to the NATO Alliance. The Treaty declared that any attack on one of the three parties in the Pacific area would be viewed as a threat to each, and that the common danger would be met in accordance with each nation's constitutional processes. In 1954 the Menzies Government signed the South East Asia Collective Defence Treaty (SEATO) as a South East Asian counterpart to NATO. That same year, Soviet diplomat Vladimir Petrov and his wife defected from the Soviet embassy in Canberra, revealing evidence of Russian spying activities; Menzies called a Royal Commission to investigate.
Menzies continued the expanded immigration program established under Chifley, and took important steps towards dismantling the White Australia Policy. In the early 1950s, external affairs minister Percy Spender helped to establish the Colombo Plan for providing economic aid to underdeveloped nations in Australia's region. Under that scheme many future Asian leaders studied in Australia. In 1958 the government replaced the Immigration Act's arbitrarily applied European language dictation test with an entry permit system, that reflected economic and skills criteria. In 1962, Menzies' Commonwealth Electoral Act provided that all Indigenous Australians should have the right to enrol and vote at federal elections (prior to this, indigenous people in Queensland, Western Australia and some in the Northern Territory had been excluded from voting unless they were ex-servicemen). In 1949 the Liberals appointed Dame Enid Lyons as the first woman to serve in an Australian Cabinet. Menzies remained a staunch supporter of links to the monarchy and British Commonwealth but formalised an alliance with the United States and concluded the Agreement on Commerce between Australia and Japan which was signed in July 1957 and launched post-war trade with Japan, beginning a growth of Australian exports of coal, iron ore and mineral resources that would steadily climb until Japan became Australia's largest trading partner.
Holt increased Australian commitment to the growing War in Vietnam, which met with some public opposition. His government oversaw conversion to decimal currency. Holt faced Britain's withdrawal from Asia by visiting and hosting many Asian leaders and by expanding ties to the United States, hosting the first visit to Australia by an American president, his friend Lyndon B. Johnson. Holt's government introduced the Migration Act 1966, which effectively dismantled the White Australia Policy and increased access to non-European migrants, including refugees fleeing the Vietnam War. Holt also called the 1967 Referendum which removed the discriminatory clause in the Australian Constitution which excluded Aboriginal Australians from being counted in the census – the referendum was one of the few to be overwhelmingly endorsed by the Australian electorate (over 90% voted 'yes'). By the end of 1967, the Liberals' initially popular support for the war in Vietnam was causing increasing public protest.
The Gorton Government increased funding for the arts, setting up the Australian Council for the Arts, the Australian Film Development Corporation and the National Film and Television Training School. The Gorton Government passed legislation establishing equal pay for men and women and increased pensions, allowances and education scholarships, as well as providing free health care to 250,000 of the nation's poor (but not universal health care). Gorton's government kept Australia in the Vietnam War but stopped replacing troops at the end of 1970.
Gorton maintained good relations with the United States and Britain, but pursued closer ties with Asia. The Gorton government experienced a decline in voter support at the 1969 election. State Liberal leaders saw his policies as too Centralist, while other Liberals didn't like his personal behaviour. In 1971, Defence Minister Malcolm Fraser, resigned and said Gorton was "not fit to hold the great office of Prime Minister". In a vote on the leadership the Liberal Party split 50/50, and although this was insufficient to remove him as the leader, Gorton decided this was also insufficient support for him, and he resigned.
During McMahon's period in office, Neville Bonner joined the Senate and became the first Indigenous Australian in the Australian Parliament. Bonner was chosen by the Liberal Party to fill a Senate vacancy in 1971 and celebrated his maiden parliamentary speech with a boomerang throwing display on the lawns of Parliament. Bonner went on to win election at the 1972 election and served as a Liberal Senator for 12 years. He worked on Indigenous and social welfare issues and proved an independent minded Senator, often crossing the floor on Parliamentary votes.
Following the 1974–75 Loans Affair, the Malcolm Fraser led Liberal-Country Party Coalition argued that the Whitlam Government was incompetent and delayed passage of the Government's money bills in the Senate, until the government would promise a new election. Whitlam refused, Fraser insisted leading to the divisive 1975 Australian constitutional crisis. The deadlock came to an end when the Whitlam government was dismissed by the Governor-General, Sir John Kerr on 11 November 1975 and Fraser was installed as caretaker Prime Minister, pending an election. Fraser won in a landslide at the resulting 1975 election.
Fraser maintained some of the social reforms of the Whitlam era, while seeking increased fiscal restraint. His government included the first Aboriginal federal parliamentarian, Neville Bonner, and in 1976, Parliament passed the Aboriginal Land Rights Act 1976, which, while limited to the Northern Territory, affirmed "inalienable" freehold title to some traditional lands. Fraser established the multicultural broadcaster SBS, accepted Vietnamese refugees, opposed minority white rule in Apartheid South Africa and Rhodesia and opposed Soviet expansionism. A significant program of economic reform however was not pursued. By 1983, the Australian economy was suffering with the early 1980s recession and amidst the effects of a severe drought. Fraser had promoted "states' rights" and his government refused to use Commonwealth powers to stop the construction of the Franklin Dam in Tasmania in 1982. Liberal minister, Don Chipp split off from the party to form a new social liberal party, the Australian Democrats in 1977. Fraser won further substantial majorities at the 1977 and 1980 elections, before losing to the Bob Hawke led Australian Labor Party in the 1983 election.
Howard differed from his Labor predecessor Paul Keating in that he supported traditional Australian institutions like the Monarchy in Australia, the commemoration of ANZAC Day and the design of the Australian flag, but like Keating he pursued privatisation of public utilities and the introduction of a broad based consumption tax (although Keating had dropped support for a GST by the time of his 1993 election victory). Howard's premiership coincided with Al Qaeda's 11 September attacks on the United States. The Howard Government invoked the ANZUS treaty in response to the attacks and supported America's campaigns in Afghanistan and Iraq.
Through 2010, the party improved its vote in the Tasmanian and South Australian state elections and achieved state government in Victoria. In March 2011, the New South Wales Liberal-National Coalition led by Barry O'Farrell won government with the largest election victory in post-war Australian history at the State Election. In Queensland, the Liberal and National parties merged in 2008 to form the new Liberal National Party of Queensland (registered as the Queensland Division of the Liberal Party of Australia). In March 2012, the new party achieved Government in an historic landslide, led by former Brisbane Lord Mayor, Campbell Newman.
Following the 2007 Federal Election, Dr Brendan Nelson was elected leader by the Parliamentary Liberal Party. On 16 September 2008, in a second contest following a spill motion, Nelson lost the leadership to Malcolm Turnbull. On 1 December 2009, a subsequent leadership election saw Turnbull lose the leadership to Tony Abbott by 42 votes to 41 on the second ballot. Abbott led the party to the 2010 federal election, which saw an increase in the Liberal Party vote and resulted in the first hung parliament since the 1940 election.
The party's leader is Malcolm Turnbull and its deputy leader is Julie Bishop. The pair were elected to their positions at the September 2015 Liberal leadership ballot, Bishop as the incumbent deputy leader and Turnbull as a replacement for Tony Abbott, whom he consequently succeeded as Prime Minister of Australia. Now the Turnbull Government, the party had been elected at the 2013 federal election as the Abbott Government which took office on 18 September 2013. At state and territory level, the Liberal Party is in office in three states: Colin Barnett has been Premier of Western Australia since 2008, Will Hodgman Premier of Tasmania since 2014 and Mike Baird Premier of New South Wales since 2014. Adam Giles is also the Chief Minister of the Northern Territory, having led a Country Liberal minority government since 2015. The party is in opposition in Victoria, Queensland, South Australia and the Australian Capital Territory.
Socially, while liberty and freedom of enterprise form the basis of its beliefs, elements of the party have wavered between what is termed "small-l liberalism" and social conservatism. Historically, Liberal Governments have been responsible for the carriage of a number of notable "socially liberal" reforms, including the opening of Australia to multiethnic immigration under Menzies and Harold Holt; Holt's 1967 Referendum on Aboriginal Rights; Sir John Gorton's support for cinema and the arts; selection of the first Aboriginal Senator, Neville Bonner, in 1971; and Malcolm Fraser's Aboriginal Land Rights Act 1976. A West Australian Liberal, Ken Wyatt, became the first Indigenous Australian elected to the House of Representatives in 2010.
The Liberal Party's organisation is dominated by the six state divisions, reflecting the party's original commitment to a federalised system of government (a commitment which was strongly maintained by all Liberal governments until 1983, but was to a large extent abandoned by the Howard Government, which showed strong centralising tendencies). Menzies deliberately created a weak national party machine and strong state divisions. Party policy is made almost entirely by the parliamentary parties, not by the party's rank-and-file members, although Liberal party members do have a degree of influence over party policy.
Menzies ran strongly against Labor's plans to nationalise the Australian banking system and, following victory in the 1949 election, secured a double dissolution election for April 1951, after the Labor-controlled Senate refused to pass his banking legislation. The Liberal-Country Coalition was returned with control of the Senate. The Government was returned again in the 1954 election; the formation of the anti-Communist Democratic Labor Party (DLP) and the consequent split in the Australian Labor Party early in 1955 helped the Liberals to another victory in December 1955. John McEwen replaced Arthur Fadden as leader of the Country Party in March 1958 and the Menzies-McEwen Coalition was returned again at elections in November 1958 – their third victory against Labor's H. V. Evatt. The Coalition was narrowly returned against Labor's Arthur Calwell in the December 1961 election, in the midst of a credit squeeze. Menzies stood for office for the last time in the November 1963 election, again defeating Calwell, with the Coalition winning back its losses in the House of Representatives. Menzies went on to resign from parliament on 26 January 1966.
A period of division for the Liberals followed, with former Treasurer John Howard competing with former Foreign Minister Andrew Peacock for supremacy. The Australian economy was facing the early 1990s recession. Unemployment reached 11.4% in 1992. Under Dr John Hewson, in November 1991, the opposition launched the 650-page Fightback! policy document − a radical collection of "dry", economic liberal measures including the introduction of a Goods and Services Tax (GST), various changes to Medicare including the abolition of bulk billing for non-concession holders, the introduction of a nine-month limit on unemployment benefits, various changes to industrial relations including the abolition of awards, a $13 billion personal income tax cut directed at middle and upper income earners, $10 billion in government spending cuts, the abolition of state payroll taxes and the privatisation of a large number of government owned enterprises − representing the start of a very different future direction to the keynesian economic conservatism practiced by previous Liberal/National Coalition governments. The 15 percent GST was the centerpiece of the policy document. Through 1992, Labor Prime Minister Paul Keating mounted a campaign against the Fightback package, and particularly against the GST, which he described as an attack on the working class in that it shifted the tax burden from direct taxation of the wealthy to indirect taxation as a broad-based consumption tax. Pressure group activity and public opinion was relentless, which led Hewson to exempt food from the proposed GST − leading to questions surrounding the complexity of what food was and wasn't to be exempt from the GST. Hewson's difficulty in explaining this to the electorate was exemplified in the infamous birthday cake interview, considered by some as a turning point in the election campaign. Keating won a record fifth consecutive Labor term at the 1993 election. A number of the proposals were later adopted in to law in some form, to a small extent during the Keating Labor government, and to a larger extent during the Howard Liberal government (most famously the GST), while unemployment benefits and bulk billing were re-targeted for a time by the Abbott Liberal government.
In South Australia, initially a Liberal and Country Party affiliated party, the Liberal and Country League (LCL), mostly led by Premier of South Australia Tom Playford, was in power from the 1933 election to the 1965 election, though with assistance from an electoral malapportionment, or gerrymander, known as the Playmander. The LCL's Steele Hall governed for one term from the 1968 election to the 1970 election and during this time began the process of dismantling the Playmander. David Tonkin, as leader of the South Australian Division of the Liberal Party of Australia, became Premier at the 1979 election for one term, losing office at the 1982 election. The Liberals returned to power at the 1993 election, led by Premiers Dean Brown, John Olsen and Rob Kerin through two terms, until their defeat at the 2002 election. They have since remained in opposition under a record five Opposition Leaders.
In a career spanning more than four decades, Spielberg's films have covered many themes and genres. Spielberg's early science-fiction and adventure films were seen as archetypes of modern Hollywood blockbuster filmmaking. In later years, his films began addressing humanistic issues such as the Holocaust (in Schindler's List), the transatlantic slave trade (in Amistad), war (in Empire of the Sun, Saving Private Ryan, War Horse and Bridge of Spies) and terrorism (in Munich). His other films include Close Encounters of the Third Kind, the Indiana Jones film series, and A.I. Artificial Intelligence.
Spielberg was born in Cincinnati, Ohio, to an Orthodox Jewish family. His mother, Leah (Adler) Posner (born 1920), was a restaurateur and concert pianist, and his father, Arnold Spielberg (born 1917), was an electrical engineer involved in the development of computers. His paternal grandparents were immigrants from Ukraine who settled in Cincinnati in the first decade of the 1900s. In 1950, his family moved to Haddon Township, New Jersey when his father took a job with RCA. Three years later, the family moved to Phoenix, Arizona.:548 Spielberg attended Hebrew school from 1953 to 1957, in classes taught by Rabbi Albert L. Lewis.
Spielberg won the Academy Award for Best Director for Schindler's List (1993) and Saving Private Ryan (1998). Three of Spielberg's films—Jaws (1975), E.T. the Extra-Terrestrial (1982), and Jurassic Park (1993)—achieved box office records, originated and came to epitomize the blockbuster film. The unadjusted gross of all Spielberg-directed films exceeds $9 billion worldwide, making him the highest-grossing director in history. His personal net worth is estimated to be more than $3 billion. He has been associated with composer John Williams since 1974, who composed music for all save five of Spielberg's feature films.
As a child, Spielberg faced difficulty reconciling being an Orthodox Jew with the perception of him by other children he played with. "It isn't something I enjoy admitting," he once said, "but when I was seven, eight, nine years old, God forgive me, I was embarrassed because we were Orthodox Jews. I was embarrassed by the outward perception of my parents' Jewish practices. I was never really ashamed to be Jewish, but I was uneasy at times." Spielberg also said he suffered from acts of anti-Semitic prejudice and bullying: "In high school, I got smacked and kicked around. Two bloody noses. It was horrible."
In 1958, he became a Boy Scout and fulfilled a requirement for the photography merit badge by making a nine-minute 8 mm film entitled The Last Gunfight. Years later, Spielberg recalled to a magazine interviewer, "My dad's still-camera was broken, so I asked the scoutmaster if I could tell a story with my father's movie camera. He said yes, and I got an idea to do a Western. I made it and got my merit badge. That was how it all started." At age thirteen, while living in Phoenix, Spielberg won a prize for a 40-minute war film he titled Escape to Nowhere, using a cast composed of other high school friends. That motivated him to make 15 more amateur 8mm films.:548 In 1963, at age sixteen, Spielberg wrote and directed his first independent film, a 140-minute science fiction adventure called Firelight, which would later inspire Close Encounters. The film was made for $500, most of which came from his father, and was shown in a local cinema for one evening, which earned back its cost.
While still a student, he was offered a small unpaid intern job at Universal Studios with the editing department. He was later given the opportunity to make a short film for theatrical release, the 26-minute, 35mm, Amblin', which he wrote and directed. Studio vice president Sidney Sheinberg was impressed by the film, which had won a number of awards, and offered Spielberg a seven-year directing contract. It made him the youngest director ever to be signed for a long-term deal with a major Hollywood studio.:548 He subsequently dropped out of college to begin professionally directing TV productions with Universal.
His first professional TV job came when he was hired to direct one of the segments for the 1969 pilot episode of Night Gallery. The segment, "Eyes," starred Joan Crawford; she and Spielberg were reportedly close friends until her death. The episode is unusual in his body of work, in that the camerawork is more highly stylized than his later, more "mature" films. After this, and an episode of Marcus Welby, M.D., Spielberg got his first feature-length assignment: an episode of The Name of the Game called "L.A. 2017". This futuristic science fiction episode impressed Universal Studios and they signed him to a short contract. He did another segment on Night Gallery and did some work for shows such as Owen Marshall: Counselor at Law and The Psychiatrist, before landing the first series episode of Columbo (previous episodes were actually TV films).
Based on the strength of his work, Universal signed Spielberg to do four TV films. The first was a Richard Matheson adaptation called Duel. The film is about a psychotic Peterbilt 281 tanker truck driver who chases the terrified driver (Dennis Weaver) of a small Plymouth Valiant and tries to run him off the road. Special praise of this film by the influential British critic Dilys Powell was highly significant to Spielberg's career. Another TV film (Something Evil) was made and released to capitalize on the popularity of The Exorcist, then a major best-selling book which had not yet been released as a film. He fulfilled his contract by directing the TV film-length pilot of a show called Savage, starring Martin Landau. Spielberg's debut full-length feature film was The Sugarland Express, about a married couple who are chased by police as the couple tries to regain custody of their baby. Spielberg's cinematography for the police chase was praised by reviewers, and The Hollywood Reporter stated that "a major new director is on the horizon.":223 However, the film fared poorly at the box office and received a limited release.
Studio producers Richard D. Zanuck and David Brown offered Spielberg the director's chair for Jaws, a thriller-horror film based on the Peter Benchley novel about an enormous killer shark. Spielberg has often referred to the gruelling shoot as his professional crucible. Despite the film's ultimate, enormous success, it was nearly shut down due to delays and budget over-runs. But Spielberg persevered and finished the film. It was an enormous hit, winning three Academy Awards (for editing, original score and sound) and grossing more than $470 million worldwide at the box office. It also set the domestic record for box office gross, leading to what the press described as "Jawsmania.":248 Jaws made Spielberg a household name and one of America's youngest multi-millionaires, allowing him a great deal of autonomy for his future projects.:250 It was nominated for Best Picture and featured Spielberg's first of three collaborations with actor Richard Dreyfuss.
Rejecting offers to direct Jaws 2, King Kong and Superman, Spielberg and actor Richard Dreyfuss re-convened to work on a film about UFOs, which became Close Encounters of the Third Kind (1977). One of the rare films both written and directed by Spielberg, Close Encounters was a critical and box office hit, giving Spielberg his first Best Director nomination from the Academy as well as earning six other Academy Awards nominations. It won Oscars in two categories (Cinematography, Vilmos Zsigmond, and a Special Achievement Award for Sound Effects Editing, Frank E. Warner). This second blockbuster helped to secure Spielberg's rise. His next film, 1941, a big-budgeted World War II farce, was not nearly as successful and though it grossed over $92.4 million worldwide (and did make a small profit for co-producing studios Columbia and Universal) it was seen as a disappointment, mainly with the critics.
Spielberg then revisited his Close Encounters project and, with financial backing from Columbia Pictures, released Close Encounters: The Special Edition in 1980. For this, Spielberg fixed some of the flaws he thought impeded the original 1977 version of the film and also, at the behest of Columbia, and as a condition of Spielberg revising the film, shot additional footage showing the audience the interior of the mothership seen at the end of the film (a decision Spielberg would later regret as he felt the interior of the mothership should have remained a mystery). Nevertheless, the re-release was a moderate success, while the 2001 DVD release of the film restored the original ending.
Next, Spielberg teamed with Star Wars creator and friend George Lucas on an action adventure film, Raiders of the Lost Ark, the first of the Indiana Jones films. The archaeologist and adventurer hero Indiana Jones was played by Harrison Ford (whom Lucas had previously cast in his Star Wars films as Han Solo). The film was considered an homage to the cliffhanger serials of the Golden Age of Hollywood. It became the biggest film at the box office in 1981, and the recipient of numerous Oscar nominations including Best Director (Spielberg's second nomination) and Best Picture (the second Spielberg film to be nominated for Best Picture). Raiders is still considered a landmark example of the action-adventure genre. The film also led to Ford's casting in Ridley Scott's Blade Runner.
His next directorial feature was the Raiders prequel Indiana Jones and the Temple of Doom. Teaming up once again with Lucas and Ford, the film was plagued with uncertainty for the material and script. This film and the Spielberg-produced Gremlins led to the creation of the PG-13 rating due to the high level of violence in films targeted at younger audiences. In spite of this, Temple of Doom is rated PG by the MPAA, even though it is the darkest and, possibly, most violent Indy film. Nonetheless, the film was still a huge blockbuster hit in 1984. It was on this project that Spielberg also met his future wife, actress Kate Capshaw.
In 1985, Spielberg released The Color Purple, an adaptation of Alice Walker's Pulitzer Prize-winning novel of the same name, about a generation of empowered African-American women during depression-era America. Starring Whoopi Goldberg and future talk-show superstar Oprah Winfrey, the film was a box office smash and critics hailed Spielberg's successful foray into the dramatic genre. Roger Ebert proclaimed it the best film of the year and later entered it into his Great Films archive. The film received eleven Academy Award nominations, including two for Goldberg and Winfrey. However, much to the surprise of many, Spielberg did not get a Best Director nomination.
In 1987, as China began opening to Western capital investment, Spielberg shot the first American film in Shanghai since the 1930s, an adaptation of J. G. Ballard's autobiographical novel Empire of the Sun, starring John Malkovich and a young Christian Bale. The film garnered much praise from critics and was nominated for several Oscars, but did not yield substantial box office revenues. Reviewer Andrew Sarris called it the best film of the year and later included it among the best films of the decade. Spielberg was also a co-producer of the 1987 film *batteries not included.
After two forays into more serious dramatic films, Spielberg then directed the third Indiana Jones film, 1989's Indiana Jones and the Last Crusade. Once again teaming up with Lucas and Ford, Spielberg also cast actor Sean Connery in a supporting role as Indy's father. The film earned generally positive reviews and was another box office success, becoming the highest grossing film worldwide that year; its total box office receipts even topped those of Tim Burton's much-anticipated film Batman, which had been the bigger hit domestically. Also in 1989, he re-united with actor Richard Dreyfuss for the romantic comedy-drama Always, about a daredevil pilot who extinguishes forest fires. Spielberg's first romantic film, Always was only a moderate success and had mixed reviews.
Spielberg's next film, Schindler's List, was based on the true story of Oskar Schindler, a man who risked his life to save 1,100 Jews from the Holocaust. Schindler's List earned Spielberg his first Academy Award for Best Director (it also won Best Picture). With the film a huge success at the box office, Spielberg used the profits to set up the Shoah Foundation, a non-profit organization that archives filmed testimony of Holocaust survivors. In 1997, the American Film Institute listed it among the 10 Greatest American Films ever Made (#9) which moved up to (#8) when the list was remade in 2007.
His next theatrical release in that same year was the World War II film Saving Private Ryan, about a group of U.S. soldiers led by Capt. Miller (Tom Hanks) sent to bring home a paratrooper whose three older brothers were killed in the same twenty-four hours, June 5–6, of the Normandy landing. The film was a huge box office success, grossing over $481 million worldwide and was the biggest film of the year at the North American box office (worldwide it made second place after Michael Bay's Armageddon). Spielberg won his second Academy Award for his direction. The film's graphic, realistic depiction of combat violence influenced later war films such as Black Hawk Down and Enemy at the Gates. The film was also the first major hit for DreamWorks, which co-produced the film with Paramount Pictures (as such, it was Spielberg's first release from the latter that was not part of the Indiana Jones series). Later, Spielberg and Tom Hanks produced a TV mini-series based on Stephen Ambrose's book Band of Brothers. The ten-part HBO mini-series follows Easy Company of the 101st Airborne Division's 506th Parachute Infantry Regiment. The series won a number of awards at the Golden Globes and the Emmys.
Spielberg and actor Tom Cruise collaborated for the first time for the futuristic neo-noir Minority Report, based upon the science fiction short story written by Philip K. Dick about a Washington D.C. police captain in the year 2054 who has been foreseen to murder a man he has not yet met. The film received strong reviews with the review tallying website Rotten Tomatoes giving it a 92% approval rating, reporting that 206 out of the 225 reviews they tallied were positive. The film earned over $358 million worldwide. Roger Ebert, who named it the best film of 2002, praised its breathtaking vision of the future as well as for the way Spielberg blended CGI with live-action.
Also in 2005, Spielberg directed a modern adaptation of War of the Worlds (a co-production of Paramount and DreamWorks), based on the H. G. Wells book of the same name (Spielberg had been a huge fan of the book and the original 1953 film). It starred Tom Cruise and Dakota Fanning, and, as with past Spielberg films, Industrial Light & Magic (ILM) provided the visual effects. Unlike E.T. and Close Encounters of the Third Kind, which depicted friendly alien visitors, War of the Worlds featured violent invaders. The film was another huge box office smash, grossing over $591 million worldwide.
Spielberg's film Munich, about the events following the 1972 Munich Massacre of Israeli athletes at the Olympic Games, was his second film essaying Jewish relations in the world (the first being Schindler's List). The film is based on Vengeance, a book by Canadian journalist George Jonas. It was previously adapted into the 1986 made-for-TV film Sword of Gideon. The film received strong critical praise, but underperformed at the U.S. and world box-office; it remains one of Spielberg's most controversial films to date. Munich received five Academy Awards nominations, including Best Picture, Film Editing, Original Music Score (by John Williams), Best Adapted Screenplay, and Best Director for Spielberg. It was Spielberg's sixth Best Director nomination and fifth Best Picture nomination.
In June 2006, Steven Spielberg announced he would direct a scientifically accurate film about "a group of explorers who travel through a worm hole and into another dimension", from a treatment by Kip Thorne and producer Lynda Obst. In January 2007, screenwriter Jonathan Nolan met with them to discuss adapting Obst and Thorne's treatment into a narrative screenplay. The screenwriter suggested the addition of a "time element" to the treatment's basic idea, which was welcomed by Obst and Thorne. In March of that year, Paramount hired Nolan, as well as scientists from Caltech, forming a workshop to adapt the treatment under the title Interstellar. The following July, Kip Thorne said there was a push by people for him to portray himself in the film. Spielberg later abandoned Interstellar, which was eventually directed by Christopher Nolan.
In early 2009, Spielberg shot the first film in a planned trilogy of motion capture films based on The Adventures of Tintin, written by Belgian artist Hergé, with Peter Jackson. The Adventures of Tintin: The Secret of the Unicorn, was not released until October 2011, due to the complexity of the computer animation involved. The world premiere took place on October 22, 2011 in Brussels, Belgium. The film was released in North American theaters on December 21, 2011, in Digital 3D and IMAX. It received generally positive reviews from critics, and grossed over $373 million worldwide. The Adventures of Tintin won the award for Best Animated Feature Film at the Golden Globe Awards that year. It is the first non-Pixar film to win the award since the category was first introduced. Jackson has been announced to direct the second film.
Spielberg followed with War Horse, shot in England in the summer of 2010. It was released just four days after The Adventures of Tintin, on December 25, 2011. The film, based on the novel of the same name written by Michael Morpurgo and published in 1982, follows the long friendship between a British boy and his horse Joey before and during World War I – the novel was also adapted into a hit play in London which is still running there, as well as on Broadway. The film was released and distributed by Disney, with whom DreamWorks made a distribution deal in 2009. War Horse received generally positive reviews from critics, and was nominated for six Academy Awards, including Best Picture.
Spielberg next directed the historical drama film Lincoln, starring Daniel Day-Lewis as United States President Abraham Lincoln and Sally Field as Mary Todd Lincoln. Based on Doris Kearns Goodwin's bestseller Team of Rivals: The Political Genius of Abraham Lincoln, the film covered the final four months of Lincoln's life. Written by Tony Kushner, the film was shot in Richmond, Virginia, in late 2011, and was released in the United States by Disney in November 2012. The film's international distribution was handled by 20th Century Fox. Upon release, Lincoln received widespread critical acclaim, and was nominated for twelve Academy Awards (the most of any film that year) including Best Picture and Best Director for Spielberg. It won the award for Best Production Design and Day-Lewis won the Academy Award for Best Actor for his portrayal of Lincoln, becoming the first three time winner in that category as well as the first to win for a performance directed by Spielberg.
Spielberg directed 2015's Bridge of Spies, a Cold War thriller based on the 1960 U-2 incident, and focusing on James B. Donovan's negotiations with the Soviets for the release of pilot Gary Powers after his aircraft was shot down over Soviet territory. The film starred Tom Hanks as Donovan, as well as Mark Rylance, Amy Ryan, and Alan Alda, with a script by the Coen brothers. The film was shot from September to December 2014 on location in New York City, Berlin and Wroclaw, Poland (which doubled for East Berlin), and was released by Disney on October 16, 2015. Bridge of Spies received positive reviews from critics, and was nominated for six Academy Awards, including Best Picture.
Since the mid-1980s, Spielberg has increased his role as a film producer. He headed up the production team for several cartoons, including the Warner Bros. hits Tiny Toon Adventures, Animaniacs, Pinky and the Brain, Toonsylvania, and Freakazoid!, for which he collaborated with Jean MacCurdy and Tom Ruegger. Due to his work on these series, in the official titles, most of them say, "Steven Spielberg presents" as well as making numerous cameos on the shows. Spielberg also produced the Don Bluth animated features, An American Tail and The Land Before Time, which were released by Universal Studios. He also served as one of the executive producers of Who Framed Roger Rabbit and its three related shorts (Tummy Trouble, Roller Coaster Rabbit, Trail Mix-Up), which were all released by Disney, under both the Walt Disney Pictures and the Touchstone Pictures banners. He was furthermore, for a short time, the executive producer of the long-running medical drama ER. In 1989, he brought the concept of The Dig to LucasArts. He contributed to the project from that time until 1995 when the game was released. He also collaborated with software publishers Knowledge Adventure on the multimedia game Steven Spielberg's Director's Chair, which was released in 1996. Spielberg appears, as himself, in the game to direct the player. The Spielberg name provided branding for a Lego Moviemaker kit, the proceeds of which went to the Starbright Foundation.
Spielberg served as an uncredited executive producer on The Haunting, The Prince of Egypt, Just Like Heaven, Shrek, Road to Perdition, and Evolution. He served as an executive producer for the 1997 film Men in Black, and its sequels, Men in Black II and Men in Black III. In 2005, he served as a producer of Memoirs of a Geisha, an adaptation of the novel by Arthur Golden, a film to which he was previously attached as director. In 2006, Spielberg co-executive produced with famed filmmaker Robert Zemeckis a CGI children's film called Monster House, marking their eighth collaboration since 1990's Back to the Future Part III. He also teamed with Clint Eastwood for the first time in their careers, co-producing Eastwood's Flags of Our Fathers and Letters from Iwo Jima with Robert Lorenz and Eastwood himself. He earned his twelfth Academy Award nomination for the latter film as it was nominated for Best Picture. Spielberg served as executive producer for Disturbia and the Transformers live action film with Brian Goldner, an employee of Hasbro. The film was directed by Michael Bay and written by Roberto Orci and Alex Kurtzman, and Spielberg continued to collaborate on the sequels, Transformers: Revenge of the Fallen and Transformers: Dark of the Moon. In 2011, he produced the J. J. Abrams science fiction thriller film Super 8 for Paramount Pictures.
Other major television series Spielberg produced were Band of Brothers, Taken and The Pacific. He was an executive producer on the critically acclaimed 2005 TV miniseries Into the West which won two Emmy awards, including one for Geoff Zanelli's score. For his 2010 miniseries The Pacific he teamed up once again with co-producer Tom Hanks, with Gary Goetzman also co-producing'. The miniseries is believed to have cost $250 million and is a 10-part war miniseries centered on the battles in the Pacific Theater during World War II. Writer Bruce McKenna, who penned several installments of (Band of Brothers), was the head writer.
In 2011, Spielberg launched Falling Skies, a science fiction television series, on the TNT network. He developed the series with Robert Rodat and is credited as an executive producer. Spielberg is also producing the Fox TV series Terra Nova. Terra Nova begins in the year 2149 when all life on the planet Earth is threatened with extinction resulting in scientists opening a door that allows people to travel back 85 million years to prehistoric times. Spielberg also produced The River, Smash, Under the Dome, Extant and The Whispers, as well as a TV adaptation of Minority Report.
Apart from being an ardent gamer Spielberg has had a long history of involvement in video games. He has been giving thanks to his games of his division DreamWorks Interactive most notable as Someone's in the Kitchen with script written by Animaniacs' Paul Rugg, Goosebumps: Escape from HorrorLand, The Neverhood (all in 1996), Skullmonkeys, Dilbert's Desktop Games, Goosebumps: Attack of the Mutant (all 1997), Boombots (1999), T'ai Fu: Wrath of the Tiger (1999), and Clive Barker's Undying (2001). In 2005 the director signed with Electronic Arts to collaborate on three games including an action game and an award winning puzzle game for the Wii called Boom Blox (and its 2009 sequel: Boom Blox Bash Party). Previously, he was involved in creating the scenario for the adventure game The Dig. In 1996, Spielberg worked on and shot original footage for a movie-making simulation game called Steven Spielberg's Director's Chair. He is the creator of the Medal of Honor series by Electronic Arts. He is credited in the special thanks section of the 1998 video game Trespasser. In 2013, Spielberg has announced he is collaborating with 343 Industries for a live-action TV show of Halo.
Spielberg has filmed and is currently in post-production on an adaptation of Roald Dahl's celebrated children's story The BFG. Spielberg's DreamWorks bought the rights in 2010, originally intending John Madden to direct. The film was written by E.T. screenwriter Melissa Mathison and is co-produced by Walt Disney Pictures, marking the first Disney-branded film to be directed by Spielberg. The BFG is set to premiere out of competition at the Cannes Film Festival in May 2016, before its wide release in the US on July 1, 2016.
After completing filming on Ready Player One, while it is in its lengthy, effects-heavy post-production, he will film his long-planned adaptation of David Kertzer's acclaimed The Kidnapping of Edgardo Mortara. The book follows the true story of a young Jewish boy in 1858 Italy who was secretly baptized by a family servant and then kidnapped from his family by the Papal States, where he was raised and trained as a priest, causing international outrage and becoming a media sensation. First announced in 2014, the book has been adapted by Tony Kushner and the film will again star Mark Rylance, as Pope Pius IX. It will be filmed in early 2017 for release at the end of that year, before Ready Player One is completed and released in 2018.
Spielberg was scheduled to shoot a $200 million adaptation of Daniel H. Wilson's novel Robopocalypse, adapted for the screen by Drew Goddard. The film would follow a global human war against a robot uprising about 15–20 years in the future. Like Lincoln, it was to be released by Disney in the United States and Fox overseas. It was set for release on April 25, 2014, with Anne Hathaway and Chris Hemsworth set to star, but Spielberg postponed production indefinitely in January 2013, just before it had been set to begin.
Spielberg's films often deal with several recurring themes. Most of his films deal with ordinary characters searching for or coming in contact with extraordinary beings or finding themselves in extraordinary circumstances. In an AFI interview in August 2000 Spielberg commented on his interest in the possibility of extra terrestrial life and how it has influenced some of his films. Spielberg described himself as feeling like an alien during childhood, and his interest came from his father, a science fiction fan, and his opinion that aliens would not travel light years for conquest, but instead curiosity and sharing of knowledge.
A strong consistent theme in his family-friendly work is a childlike, even naïve, sense of wonder and faith, as attested by works such as Close Encounters of the Third Kind, E.T. the Extra-Terrestrial, Hook, A.I. Artificial Intelligence and The BFG. According to Warren Buckland, these themes are portrayed through the use of low height camera tracking shots, which have become one of Spielberg's directing trademarks. In the cases when his films include children (E.T. the Extra-Terrestrial, Empire of the Sun, Jurassic Park, etc.), this type of shot is more apparent, but it is also used in films like Munich, Saving Private Ryan, The Terminal, Minority Report, and Amistad. If one views each of his films, one will see this shot utilized by the director, notably the water scenes in Jaws are filmed from the low-angle perspective of someone swimming. Another child oriented theme in Spielberg's films is that of loss of innocence and coming-of-age. In Empire of the Sun, Jim, a well-groomed and spoiled English youth, loses his innocence as he suffers through World War II China. Similarly, in Catch Me If You Can, Frank naively and foolishly believes that he can reclaim his shattered family if he accumulates enough money to support them.
The most persistent theme throughout his films is tension in parent-child relationships. Parents (often fathers) are reluctant, absent or ignorant. Peter Banning in Hook starts off in the beginning of the film as a reluctant married-to-his-work parent who through the course of the film regains the respect of his children. The notable absence of Elliott's father in E.T., is the most famous example of this theme. In Indiana Jones and the Last Crusade, it is revealed that Indy has always had a very strained relationship with his father, who is a professor of medieval literature, as his father always seemed more interested in his work, specifically in his studies of the Holy Grail, than in his own son, although his father does not seem to realize or understand the negative effect that his aloof nature had on Indy (he even believes he was a good father in the sense that he taught his son "self reliance," which is not how Indy saw it). Even Oskar Schindler, from Schindler's List, is reluctant to have a child with his wife. Munich depicts Avner as a man away from his wife and newborn daughter. There are of course exceptions; Brody in Jaws is a committed family man, while John Anderton in Minority Report is a shattered man after the disappearance of his son. This theme is arguably the most autobiographical aspect of Spielberg's films, since Spielberg himself was affected by his parents' divorce as a child and by the absence of his father. Furthermore, to this theme, protagonists in his films often come from families with divorced parents, most notably E.T. the Extra-Terrestrial (protagonist Elliot's mother is divorced) and Catch Me If You Can (Frank Abagnale's mother and father split early on in the film). Little known also is Tim in Jurassic Park (early in the film, another secondary character mentions Tim and Lex's parents' divorce). The family often shown divided is often resolved in the ending as well. Following this theme of reluctant fathers and father figures, Tim looks to Dr. Alan Grant as a father figure. Initially, Dr. Grant is reluctant to return those paternal feelings to Tim. However, by the end of the film, he has changed, and the kids even fall asleep with their heads on his shoulders.
In terms of casting and production itself, Spielberg has a known penchant for working with actors and production members from his previous films. For instance, he has cast Richard Dreyfuss in several films: Jaws, Close Encounters of the Third Kind, and Always. Aside from his role as Indiana Jones, Spielberg also cast Harrison Ford as a headteacher in E.T. the Extra-Terrestrial (though the scene was ultimately cut). Although Spielberg directed veteran voice actor Frank Welker only once (in Raiders of the Lost Ark, for which he voiced many of the animals), Welker has lent his voice in a number of productions Spielberg has executive produced from Gremlins to its sequel Gremlins 2: The New Batch, as well as The Land Before Time, Who Framed Roger Rabbit, and television shows such as Tiny Toons, Animaniacs, and SeaQuest DSV. Spielberg has used Tom Hanks on several occasions and has cast him in Saving Private Ryan, Catch Me If You Can, The Terminal, and Bridge of Spies. Spielberg has collaborated with Tom Cruise twice on Minority Report and War of the Worlds, and cast Shia LaBeouf in five films: Transformers, Eagle Eye, Indiana Jones and the Kingdom of the Crystal Skull, Transformers: Revenge of the Fallen, and Transformers: Dark of the Moon.
Spielberg prefers working with production members with whom he has developed an existing working relationship. An example of this is his production relationship with Kathleen Kennedy who has served as producer on all his major films from E.T. the Extra-Terrestrial to the recent Lincoln. For cinematography, Allen Daviau, a childhood friend and cinematographer, shot the early Spielberg film Amblin and most of his films up to Empire of the Sun; Janusz Kamiński who has shot every Spielberg film since Schindler's List (see List of film director and cinematographer collaborations); and the film editor Michael Kahn who has edited every film directed by Spielberg from Close Encounters to Munich (except E.T. the Extra-Terrestrial). Most of the DVDs of Spielberg's films have documentaries by Laurent Bouzereau.
A famous example of Spielberg working with the same professionals is his long-time collaboration with John Williams and the use of his musical scores in all of his films since The Sugarland Express (except Bridge of Spies, The Color Purple and Twilight Zone: The Movie). One of Spielberg's trademarks is his use of music by Williams to add to the visual impact of his scenes and to try and create a lasting picture and sound of the film in the memories of the film audience. These visual scenes often uses images of the sun (e.g. Empire of the Sun, Saving Private Ryan, the final scene of Jurassic Park, and the end credits of Indiana Jones and the Last Crusade (where they ride into the sunset)), of which the last two feature a Williams score at that end scene. Spielberg is a contemporary of filmmakers George Lucas, Francis Ford Coppola, Martin Scorsese, John Milius, and Brian De Palma, collectively known as the "Movie Brats". Aside from his principal role as a director, Spielberg has acted as a producer for a considerable number of films, including early hits for Joe Dante and Robert Zemeckis. Spielberg has often never worked with the same screenwriter in his films, beside Tony Kushner and David Koepp, who have written a few of his films more than once.
Spielberg first met actress Amy Irving in 1976 at the suggestion of director Brian De Palma, who knew he was looking for an actress to play in Close Encounters. After meeting her, Spielberg told his co-producer Julia Phillips, "I met a real heartbreaker last night.":293 Although she was too young for the role, she and Spielberg began dating and she eventually moved in to what she described as his "bachelor funky" house.:294 They lived together for four years, but the stresses of their professional careers took a toll on their relationship. Irving wanted to be certain that whatever success she attained as an actress would be her own: "I don't want to be known as Steven's girlfriend," she said, and chose not to be in any of his films during those years.:295
As a result, they broke up in 1979, but remained close friends. Then in 1984 they renewed their romance, and in November 1985, they married, already having had a son, Max Samuel. After three and a half years of marriage, however, many of the same competing stresses of their careers caused them to divorce in 1989. They agreed to maintain homes near each other as to facilitate the shared custody and parenting of their son.:403 Their divorce was recorded as the third most costly celebrity divorce in history.
In 2002, Spielberg was one of eight flagbearers who carried the Olympic Flag into Rice-Eccles Stadium at the Opening Ceremonies of the 2002 Winter Olympic Games in Salt Lake City. In 2006, Premiere listed him as the most powerful and influential figure in the motion picture industry. Time listed him as one of the 100 Most Important People of the Century. At the end of the 20th century, Life named him the most influential person of his generation. In 2009, Boston University presented him an honorary Doctor of Humane Letters degree.
According to Forbes' Most Influential Celebrities 2014 list, Spielberg was listed as the most influential celebrity in America. The annual list is conducted by E-Poll Market Research and it gave more than 6,600 celebrities on 46 different personality attributes a score representing "how that person is perceived as influencing the public, their peers, or both." Spielberg received a score of 47, meaning 47% of the US believes he is influential. Gerry Philpott, president of E-Poll Market Research, supported Spielberg's score by stating, "If anyone doubts that Steven Spielberg has greatly influenced the public, think about how many will think for a second before going into the water this summer."
A collector of film memorabilia, Spielberg purchased a balsa Rosebud sled from Citizen Kane (1941) in 1982. He bought Orson Welles's own directorial copy of the script for the radio broadcast The War of the Worlds (1938) in 1994. Spielberg has purchased Academy Award statuettes being sold on the open market and donated them to the Academy of Motion Picture Arts and Sciences, to prevent their further commercial exploitation. His donations include the Oscars that Bette Davis received for Dangerous (1935) and Jezebel (1938), and Clark Gable's Oscar for It Happened One Night (1934).
Since playing Pong while filming Jaws in 1974, Spielberg has been an avid video gamer. Spielberg played many of LucasArts adventure games, including the first Monkey Island games. He owns a Wii, a PlayStation 3, a PSP, and Xbox 360, and enjoys playing first-person shooters such as the Medal of Honor series and Call of Duty 4: Modern Warfare. He has also criticized the use of cut scenes in games, calling them intrusive, and feels making story flow naturally into the gameplay is a challenge for future game developers.
Drawing from his own experiences in Scouting, Spielberg helped the Boy Scouts of America develop a merit badge in cinematography in order to help promote filmmaking as a marketable skill. The badge was launched at the 1989 National Scout Jamboree, which Spielberg attended, and where he personally counseled many boys in their work on requirements. That same year, 1989, saw the release of Indiana Jones and the Last Crusade. The opening scene shows a teenage Indiana Jones in scout uniform bearing the rank of a Life Scout. Spielberg stated he made Indiana Jones a Boy Scout in honor of his experience in Scouting. For his career accomplishments, service to others, and dedication to a new merit badge Spielberg was awarded the Distinguished Eagle Scout Award.
In 2004 he was admitted as knight of the Légion d'honneur by president Jacques Chirac. On July 15, 2006, Spielberg was also awarded the Gold Hugo Lifetime Achievement Award at the Summer Gala of the Chicago International Film Festival, and also was awarded a Kennedy Center honour on December 3. The tribute to Spielberg featured a short, filmed biography narrated by Tom Hanks and included thank-yous from World War II veterans for Saving Private Ryan, as well as a performance of the finale to Leonard Bernstein's Candide, conducted by John Williams (Spielberg's frequent composer).[citation needed]
The Science Fiction Hall of Fame inducted Spielberg in 2005, the first year it considered non-literary contributors. In November 2007, he was chosen for a Lifetime Achievement Award to be presented at the sixth annual Visual Effects Society Awards in February 2009. He was set to be honored with the Cecil B. DeMille Award at the January 2008 Golden Globes; however, the new, watered-down format of the ceremony resulting from conflicts in the 2007–08 writers strike, the HFPA postponed his honor to the 2009 ceremony. In 2008, Spielberg was awarded the Légion d'honneur.
Brasília (Portuguese pronunciation: [bɾaˈziljɐ]) is the federal capital of Brazil and seat of government of the Federal District. The city is located atop the Brazilian highlands in the country's center-western region. It was founded on April 21, 1960, to serve as the new national capital. Brasília and its metro (encompassing the whole of the Federal District) had a population of 2,556,149 in 2011, making it the 4th most populous city in Brazil. Among major Latin American cities, Brasília has the highest GDP per capita at R$61,915 (US$36,175).
The city has a unique status in Brazil, as it is an administrative division rather than a legal municipality like other cities in Brazil. The name 'Brasília' is commonly used as a synonym for the Federal District through synecdoche; However, the Federal District is composed of 31 administrative regions, only one of which is Brasília proper, with a population of 209,926 in a 2011 survey; Demographic publications generally do not make this distinction and list the population of Brasília as synonymous with the population of the Federal District, considering the whole of it as its metropolitan area. The city was one of the main host cities of the 2014 FIFA World Cup. Additionally, Brasília hosted the 2013 FIFA Confederations Cup.
Juscelino Kubitschek, President of Brazil from 1956 to 1961, ordered the construction of Brasília, fulfilling the promise of the Constitution and his own political campaign promise. Building Brasília was part of Juscelino's "fifty years of prosperity in five" plan. Lúcio Costa won a contest and was the main urban planner in 1957, with 5550 people competing. Oscar Niemeyer, a close friend, was the chief architect of most public buildings and Roberto Burle Marx was the landscape designer. Brasília was built in 41 months, from 1956 to April 21, 1960, when it was officially inaugurated.
Until the 1980s, the governor of the Federal District was appointed by the Federal Government, and the laws of Brasília were issued by the Brazilian Federal Senate. With the Constitution of 1988 Brasília gained the right to elect its Governor, and a District Assembly (Câmara Legislativa) was elected to exercise legislative power. The Federal District does not have a Judicial Power of its own. The Judicial Power which serves the Federal District also serves federal territories. Currently, Brazil does not have any territories, therefore, for now the courts serve only cases from the Federal District.
Brasília has a tropical savanna climate (Aw) according to the Köppen system, with two distinct seasons: the rainy season, from October to April, and a dry season, from May to September. The average temperature is 20.6 °C (69.1 °F). September, at the end of the dry season, has the highest average maximum temperature, 28.3 °C (82.9 °F), has major and minor lower maximum average temperature, of 25.1 °C (77.2 °F) and 12.9 °C (55.2 °F), respectively. Average temperatures from September through March are a consistent 22 °C (72 °F). With 247.4 mm (9.7 in), January is the month with the highest rainfall of the year, while June is the lowest, with only 8.7 mm (0.3 in).
The Portuguese language is the official national language and the primary language taught in schools. English and Spanish are also part of the official curriculum. The city has six international schools: American School of Brasília, Brasília International School (BIS), Escola das Nações, Swiss International School (SIS), Lycée français François-Mitterrand (LfFM) and Maple Bear Canadian School. August 2016 will see the opening of a new international school - The British School of Brasilia. Brasília has two universities, three university centers, and many private colleges.
The Cathedral of Brasília in the capital of the Federative Republic of Brazil, is an expression of the architect Oscar Niemeyer. This concrete-framed hyperboloid structure, seems with its glass roof reaching up, open, to the heavens. On 31 May 1970, the Cathedral’s structure was finished, and only the 70 m (229.66 ft) diameter of the circular area were visible. Niemeyer's project of Cathedral of Brasília is based in the hyperboloid of revolution which sections are asymmetric. The hyperboloid structure itself is a result of 16 identical assembled concrete columns. These columns, having hyperbolic section and weighing 90 t, represent two hands moving upwards to heaven. The Cathedral was dedicated on 31 May 1970.
A series of low-lying annexes (largely hidden) flank both ends. Also in the square are the glass-faced Planalto Palace housing the presidential offices, and the Palace of the Supreme Court. Farther east, on a triangle of land jutting into the lake, is the Palace of the Dawn (Palácio da Alvorada; the presidential residence). Between the federal and civic buildings on the Monumental Axis is the city's cathedral, considered by many to be Niemeyer's finest achievement (see photographs of the interior). The parabolically shaped structure is characterized by its 16 gracefully curving supports, which join in a circle 115 feet (35 meters) above the floor of the nave; stretched between the supports are translucent walls of tinted glass. The nave is entered via a subterranean passage rather than conventional doorways. Other notable buildings are Buriti Palace, Itamaraty Palace, the National Theater, and several foreign embassies that creatively embody features of their national architecture. The Brazilian landscape architect Roberto Burle Marx designed landmark modernist gardens for some of the principal buildings.
Both low-cost and luxury housing were built by the government in the Brasília. The residential zones of the inner city are arranged into superquadras ("superblocks"): groups of apartment buildings along with a prescribed number and type of schools, retail stores, and open spaces. At the northern end of Lake Paranoá, separated from the inner city, is a peninsula with many fashionable homes, and a similar city exists on the southern lakeshore. Originally the city planners envisioned extensive public areas along the shores of the artificial lake, but during early development private clubs, hotels, and upscale residences and restaurants gained footholds around the water. Set well apart from the city are satellite cities, including Gama, Ceilândia, Taguatinga, Núcleo Bandeirante, Sobradinho, and Planaltina. These cities, with the exception of Gama and Sobradinho were not planned.
After a visit to Brasília, the French writer Simone de Beauvoir complained that all of its superquadras exuded "the same air of elegant monotony," and other observers have equated the city's large open lawns, plazas, and fields to wastelands. As the city has matured, some of these have gained adornments, and many have been improved by landscaping, giving some observers a sense of "humanized" spaciousness. Although not fully accomplished, the "Brasília utopia" has produced a city of relatively high quality of life, in which the citizens live in forested areas with sporting and leisure structure (the superquadras) flanked by small commercial areas, bookstores and cafes; the city is famous for its cuisine and efficiency of transit.
The major roles of construction and of services (government, communications, banking and finance, food production, entertainment, and legal services) in Brasília's economy reflect the city's status as a governmental rather than an industrial center. Industries connected with construction, food processing, and furnishings are important, as are those associated with publishing, printing, and computer software. GDP is divided in Public Administration 54.8%, Services 28.7%, Industry 10.2%, Commerce 6.1%, Agribusiness 0.2%.
Besides being the political center, Brasília is an important economic center. Brasília has the highest city gross domestic product (GDP) of 99.5 billion reais representing 3.76% of the total Brazilian GDP. The main economic activity of the federal capital results from its administrative function. Its industrial planning is studied carefully by the Government of the Federal District. Being a city registered by UNESCO, the government in Brasília has opted to encourage the development of non-polluting industries such as software, film, video, and gemology among others, with emphasis on environmental preservation and maintaining ecological balance, preserving the city property.
The city's planned design included specific areas for almost everything, including accommodation, Hotels Sectors North and South. New hotel facilities are being developed elsewhere, such as the hotels and tourism Sector North, located on the shores of Lake Paranoá. Brasília has a range of tourist accommodation from inns, pensions and hostels to larger international chain hotels. The city's restaurants cater to a wide range of foods from local and regional Brazilian dishes to international cuisine.
Brasília has also been the focus of modern-day literature. Published in 2008, The World In Grey: Dom Bosco's Prophecy, by author Ryan J. Lucero, tells an apocalypticle story based on the famous prophecy from the late 19th century by the Italian saint Don Bosco. According to Don Bosco's prophecy: "Between parallels 15 and 20, around a lake which shall be formed; A great civilization will thrive, and that will be the Promised Land." Brasília lies between the parallels 15° S and 20° S, where an artificial lake (Paranoá Lake) was formed. Don Bosco is Brasília's patron saint.
Praça dos Três Poderes (Portuguese for Square of the Three Powers) is a plaza in Brasília. The name is derived from the encounter of the three federal branches around the plaza: the Executive, represented by the Palácio do Planalto (presidential office); the Legislative, represented by the National Congress (Congresso Nacional); and the Judicial branch, represented by the Supreme Federal Court (Supremo Tribunal Federal). It is a tourist attraction in Brasília, designed by Lúcio Costa and Oscar Niemeyer as a place where the three branches would meet harmoniously.
The Palácio da Alvorada is the official residence of the President of Brazil. The palace was designed, along with the rest of the city of Brasília, by Oscar Niemeyer and inaugurated in 1958. One of the first structures built in the republic's new capital city, the "Alvorada" lies on a peninsula at the margins of Lake Paranoá. The principles of simplicity and modernity, that in the past characterized the great works of architecture, motivated Niemeyer. The viewer has an impression of looking at a glass box, softly landed on the ground with the support of thin external columns. The building has an area of 7,000 m2 with three floors consisting of the basement, landing, and second floor. The auditorium, kitchen, laundry, medical center, and administration offices are at basement level. The rooms used by the presidency for official receptions are on the landing. The second floor has four suites, two apartments, and various private rooms which make up the residential part of the palace. The building also has a library, a heated Olympic-sized swimming pool, a music room, two dining rooms and various meeting rooms. A chapel and heliport are in adjacent buildings.
The Palácio do Planalto is the official workplace of the President of Brazil. It is located at the Praça dos Três Poderes in Brasília. As the seat of government, the term "Planalto" is often used as a metonym for the executive branch of government. The main working office of the President of the Republic is in the Palácio do Planalto. The President and his or her family do not live in it, rather in the official residence, the Palácio da Alvorada. Besides the President, senior advisors also have offices in the "Planalto," including the Vice-President of Brazil and the Chief of Staff. The other Ministries are along the Esplanada dos Ministérios. The architect of the Palácio do Planalto was Oscar Niemeyer, creator of most of the important buildings in Brasília. The idea was to project an image of simplicity and modernity using fine lines and waves to compose the columns and exterior structures. The Palace is four stories high, and has an area of 36,000 m2. Four other adjacent buildings are also part of the complex.
This makes for a large number of takeoffs and landings and it is not unusual for flights to be delayed in the holding pattern before landing. Following the airport's master plan, Infraero built a second runway, which was finished in 2006. In 2007, the airport handled 11,119,872 passengers. The main building's third floor, with 12 thousand square meters, has a panoramic deck, a food court, shops, four movie theatres with total capacity of 500 people, and space for exhibitions. Brasília Airport has 136 vendor spaces. The airport is located about 11 km (6.8 mi) from the central area of Brasília, outside the metro system. The area outside the airport's main gate is lined with taxis as well as several bus line services which connect the airport to Brasília's central district. The parking lot accommodates 1,200 cars. The airport is serviced by domestic and regional airlines (TAM, GOL, Azul, WebJET, Trip and Avianca), in addition to a number of international carriers. In 2012, Brasília's International Airport was won by the InfrAmerica consortium, formed by the Brazilian engineering company ENGEVIX and the Argentine Corporacion America holding company, with a 50% stake each. During the 25-year concession, the airport may be expanded to up to 40 million passengers a year.
In 2014, the airport received 15 new boarding bridges, totalling 28 in all. This was the main requirement made by the federal government, which transferred the operation of the terminal to the Inframerica Group after an auction. The group invested R$750 million in the project. In the same year, the number of parking spaces doubled, reaching three thousand. The airport's entrance have a new rooftop cover and a new access road. Furthermore, a VIP room was created on Terminal 1's third floor. The investments resulted an increase the capacity of Brasília's airport from approximately 15 million passengers per year to 21 million by 2014. Brasília has direct flights to all states of Brazil and direct international flights to Atlanta, Buenos Aires, Lisbon, Miami, Panama City, and Paris.
The Juscelino Kubitschek bridge, also known as the 'President JK Bridge' or the 'JK Bridge', crosses Lake Paranoá in Brasília. It is named after Juscelino Kubitschek de Oliveira, former president of Brazil. It was designed by architect Alexandre Chan and structural engineer Mário Vila Verde. Chan won the Gustav Lindenthal Medal for this project at the 2003 International Bridge Conference in Pittsburgh due to "...outstanding achievement demonstrating harmony with the environment, aesthetic merit and successful community participation".
The metro leaves the Rodoviária (bus station) and goes south, avoiding most of the political and tourist areas. The main purpose of the metro is to serve cities, such as Samambaia, Taguatinga and Ceilândia, as well as Guará and Águas Claras. The satellite cities served are more populated in total than the Plano Piloto itself (the census of 2000 indicated that Ceilândia had 344,039 inhabitants, Taguatinga had 243,575, whereas the Plano Piloto had approximately 400,000 inhabitants), and most residents of the satellite cities depend on public transportation.
In the original city plan, the interstate buses should also stop at the Central Station. Because of the growth of Brasília (and corresponding growth in the bus fleet), today the interstate buses leave from the older interstate station (called Rodoferroviária), located at the western end of the Eixo Monumental. The Central Bus Station also contains a main metro station. A new bus station was opened in July 2010. It is on Saída Sul (South Exit) near Parkshopping Mall and with its metro station, and it's also an inter-state bus station, used only to leave the Federal District.
Brasília is known as a departing point for the practice of unpowered air sports, sports that may be practiced with hang gliding or paragliding wings. Practitioners of such sports reveal that, because of the city's dry weather, the city offers strong thermal winds and great "cloud-streets", which is also the name for a manoeuvre quite appreciated by practitioners. In 2003, Brasília hosted the 14th Hang Gliding World Championship, one of the categories of free flying. In August 2005, the city hosted the 2nd stage of the Brazilian Hang Gliding Championship.
Unlike the Federal Bureau of Investigation (FBI), which is a domestic security service, CIA has no law enforcement function and is mainly focused on overseas intelligence gathering, with only limited domestic collection. Though it is not the only U.S. government agency specializing in HUMINT, CIA serves as the national manager for coordination and deconfliction of HUMINT activities across the entire intelligence community. Moreover, CIA is the only agency authorized by law to carry out and oversee covert action on behalf of the President, unless the President determines that another agency is better suited for carrying out such action. It can, for example, exert foreign political influence through its tactical divisions, such as the Special Activities Division.
The Executive Office also supports the U.S. military by providing it with information it gathers, receiving information from military intelligence organizations, and cooperating on field activities. The Executive Director is in charge of the day to day operation of the CIA, and each branch of the service has its own Director. The Associate Director of military affairs, a senior military officer, manages the relationship between the CIA and the Unified Combatant Commands, who produce regional/operational intelligence and consume national intelligence.
The Directorate of Analysis produces all-source intelligence investigation on key foreign and intercontinental issues relating to powerful and sometimes anti-government sensitive topics. It has four regional analytic groups, six groups for transnational issues, and three focus on policy, collection, and staff support. There is an office dedicated to Iraq, and regional analytical Offices covering the Near Eastern and South Asian Analysis, the Office of Russian and European Analysis, and the Office of Asian Pacific, Asian Pacific, Latin American, and African Analysis and African Analysis.
The Directorate of Operations is responsible for collecting foreign intelligence, mainly from clandestine HUMINT sources, and covert action. The name reflects its role as the coordinator of human intelligence activities among other elements of the wider U.S. intelligence community with their own HUMINT operations. This Directorate was created in an attempt to end years of rivalry over influence, philosophy and budget between the United States Department of Defense (DOD) and the CIA. In spite of this, the Department of Defense recently organized its own global clandestine intelligence service, the Defense Clandestine Service (DCS), under the Defense Intelligence Agency (DIA).
The CIA established its first training facility, the Office of Training and Education, in 1950. Following the end of the Cold War, the CIA's training budget was slashed, which had a negative effect on employee retention. In response, Director of Central Intelligence George Tenet established CIA University in 2002. CIA University holds between 200 and 300 courses each year, training both new hires and experienced intelligence officers, as well as CIA support staff. The facility works in partnership with the National Intelligence University, and includes the Sherman Kent School for Intelligence Analysis, the Directorate of Analysis' component of the university.
Details of the overall United States intelligence budget are classified. Under the Central Intelligence Agency Act of 1949, the Director of Central Intelligence is the only federal government employee who can spend "un-vouchered" government money. The government has disclosed a total figure for all non-military intelligence spending since 2007; the fiscal 2013 figure is $52.6 billion. According to the 2013 mass surveillance disclosures, the CIA's fiscal 2013 budget is $14.7 billion, 28% of the total and almost 50% more than the budget of the National Security Agency. CIA's HUMINT budget is $2.3 billion, the SIGINT budget is $1.7 billion, and spending for security and logistics of CIA missions is $2.5 billion. "Covert action programs", including a variety of activities such as the CIA's drone fleet and anti-Iranian nuclear program activities, accounts for $2.6 billion.
There were numerous previous attempts to obtain general information about the budget. As a result, it was revealed that CIA's annual budget in Fiscal Year 1963 was US $550 million (inflation-adjusted US$ 4.3 billion in 2016), and the overall intelligence budget in FY 1997 was US $26.6 billion (inflation-adjusted US$ 39.2 billion in 2016). There have been accidental disclosures; for instance, Mary Margaret Graham, a former CIA official and deputy director of national intelligence for collection in 2005, said that the annual intelligence budget was $44 billion, and in 1994 Congress accidentally published a budget of $43.4 billion (in 2012 dollars) in 1994 for the non-military National Intelligence Program, including $4.8 billion for the CIA. After the Marshall Plan was approved, appropriating $13.7 billion over five years, 5% of those funds or $685 million were made available to the CIA.
The role and functions of the CIA are roughly equivalent to those of the United Kingdom's Secret Intelligence Service (the SIS or MI6), the Australian Secret Intelligence Service (ASIS), the Egyptian General Intelligence Service, the Russian Foreign Intelligence Service (Sluzhba Vneshney Razvedki) (SVR), the Indian Research and Analysis Wing (RAW), the Pakistani Inter-Services Intelligence (ISI), the French foreign intelligence service Direction Générale de la Sécurité Extérieure (DGSE) and Israel's Mossad. While the preceding agencies both collect and analyze information, some like the U.S. State Department's Bureau of Intelligence and Research are purely analytical agencies.[citation needed]
The closest links of the U.S. IC to other foreign intelligence agencies are to Anglophone countries: Australia, Canada, New Zealand, and the United Kingdom. There is a special communications marking that signals that intelligence-related messages can be shared with these four countries. An indication of the United States' close operational cooperation is the creation of a new message distribution label within the main U.S. military communications network. Previously, the marking of NOFORN (i.e., No Foreign Nationals) required the originator to specify which, if any, non-U.S. countries could receive the information. A new handling caveat, USA/AUS/CAN/GBR/NZL Five Eyes, used primarily on intelligence messages, gives an easier way to indicate that the material can be shared with Australia, Canada, United Kingdom, and New Zealand.
The success of the British Commandos during World War II prompted U.S. President Franklin D. Roosevelt to authorize the creation of an intelligence service modeled after the British Secret Intelligence Service (MI6), and Special Operations Executive. This led to the creation of the Office of Strategic Services (OSS). On September 20, 1945, shortly after the end of World War II, Harry S. Truman signed an executive order dissolving the OSS, and by October 1945 its functions had been divided between the Departments of State and War. The division lasted only a few months. The first public mention of the "Central Intelligence Agency" appeared on a command-restructuring proposal presented by Jim Forrestal and Arthur Radford to the U.S. Senate Military Affairs Committee at the end of 1945. Despite opposition from the military establishment, the United States Department of State and the Federal Bureau of Investigation (FBI), Truman established the National Intelligence Authority in January 1946, which was the direct predecessor of the CIA. Its operational extension was known as the Central Intelligence Group (CIG)
Lawrence Houston, head counsel of the SSU, CIG, and, later CIA, was a principle draftsman of the National Security Act of 1947 which dissolved the NIA and the CIG, and established both the National Security Council and the Central Intelligence Agency. In 1949, Houston would help draft the Central Intelligence Agency Act, (Public law 81-110) which authorized the agency to use confidential fiscal and administrative procedures, and exempted it from most limitations on the use of Federal funds. It also exempted the CIA from having to disclose its "organization, functions, officials, titles, salaries, or numbers of personnel employed." It created the program "PL-110", to handle defectors and other "essential aliens" who fell outside normal immigration procedures.
At the outset of the Korean War the CIA still only had a few thousand employees, a thousand of whom worked in analysis. Intelligence primarily came from the Office of Reports and Estimates, which drew its reports from a daily take of State Department telegrams, military dispatches, and other public documents. The CIA still lacked its own intelligence gathering abilities. On 21 August 1950, shortly after the invasion of South Korea, Truman announced Walter Bedell Smith as the new Director of the CIA to correct what was seen as a grave failure of Intelligence.[clarification needed]
The CIA had different demands placed on it by the different bodies overseeing it. Truman wanted a centralized group to organize the information that reached him, the Department of Defense wanted military intelligence and covert action, and the State Department wanted to create global political change favorable to the US. Thus the two areas of responsibility for the CIA were covert action and covert intelligence. One of the main targets for intelligence gathering was the Soviet Union, which had also been a priority of the CIA's predecessors.
US army general Hoyt Vandenberg, the CIG's second director, created the Office of Special Operations (OSO), as well as the Office of Reports and Estimates (ORE). Initially the OSO was tasked with spying and subversion overseas with a budget of $15 million, the largesse of a small number of patrons in congress. Vandenberg's goals were much like the ones set out by his predecessor; finding out "everything about the Soviet forces in Eastern and Central Europe - their movements, their capabilities, and their intentions." This task fell to the 228 overseas personnel covering Germany, Austria, Switzerland, Poland, Czechoslovakia, and Hungary.
On 18 June 1948, the National Security Council issued Directive 10/2 calling for covert action against the USSR, and granting the authority to carry out covert operations against "hostile foreign states or groups" that could, if needed, be denied by the U.S. government. To this end, the Office of Policy Coordination was created inside the new CIA. The OPC was quite unique; Frank Wisner, the head of the OPC, answered not to the CIA Director, but to the secretaries of defense, state, and the NSC, and the OPC's actions were a secret even from the head of the CIA. Most CIA stations had two station chiefs, one working for the OSO, and one working for the OPC.
The early track record of the CIA was poor, with the agency unable to provide sufficient intelligence about the Soviet takeovers of Romania and Czechoslovakia, the Soviet blockade of Berlin, and the Soviet atomic bomb project. In particular, the agency failed to predict the Chinese entry into the Korean War with 300,000 troops. The famous double agent Kim Philby was the British liaison to American Central Intelligence. Through him the CIA coordinated hundreds of airdrops inside the iron curtain, all compromised by Philby. Arlington Hall, the nerve center of CIA cryptanalysisl was compromised by Bill Weisband, a Russian translator and Soviet spy. The CIA would reuse the tactic of dropping plant agents behind enemy lines by parachute again on China, and North Korea. This too would be fruitless.
Many applications of silicate glasses derive from their optical transparency, which gives rise to one of silicate glasses' primary uses as window panes. Glass will transmit, reflect and refract light; these qualities can be enhanced by cutting and polishing to make optical lenses, prisms, fine glassware, and optical fibers for high speed data transmission by light. Glass can be colored by adding metallic salts, and can also be painted and printed with vitreous enamels. These qualities have led to the extensive use of glass in the manufacture of art objects and in particular, stained glass windows. Although brittle, silicate glass is extremely durable, and many examples of glass fragments exist from early glass-making cultures. Because glass can be formed or molded into any shape, and also because it is a sterile product, it has been traditionally used for vessels: bowls, vases, bottles, jars and drinking glasses. In its most solid forms it has also been used for paperweights, marbles, and beads. When extruded as glass fiber and matted as glass wool in a way to trap air, it becomes a thermal insulating material, and when these glass fibers are embedded into an organic polymer plastic, they are a key structural reinforcement part of the composite material fiberglass. Some objects historically were so commonly made of silicate glass that they are simply called by the name of the material, such as drinking glasses and reading glasses.
Most common glass contains other ingredients to change its properties. Lead glass or flint glass is more 'brilliant' because the increased refractive index causes noticeably more specular reflection and increased optical dispersion. Adding barium also increases the refractive index. Thorium oxide gives glass a high refractive index and low dispersion and was formerly used in producing high-quality lenses, but due to its radioactivity has been replaced by lanthanum oxide in modern eyeglasses.[citation needed] Iron can be incorporated into glass to absorb infrared energy, for example in heat absorbing filters for movie projectors, while cerium(IV) oxide can be used for glass that absorbs UV wavelengths.
Fused quartz is a glass made from chemically-pure SiO2 (silica). It has excellent thermal shock characteristics, being able to survive immersion in water while red hot. However, its high melting-temperature (1723 °C) and viscosity make it difficult to work with. Normally, other substances are added to simplify processing. One is sodium carbonate (Na2CO3, "soda"), which lowers the glass transition temperature. The soda makes the glass water-soluble, which is usually undesirable, so lime (calcium oxide [CaO], generally obtained from limestone), some magnesium oxide (MgO) and aluminium oxide (Al2O3) are added to provide for a better chemical durability. The resulting glass contains about 70 to 74% silica by weight and is called a soda-lime glass. Soda-lime glasses account for about 90% of manufactured glass.
Following the glass batch preparation and mixing, the raw materials are transported to the furnace. Soda-lime glass for mass production is melted in gas fired units. Smaller scale furnaces for specialty glasses include electric melters, pot furnaces, and day tanks. After melting, homogenization and refining (removal of bubbles), the glass is formed. Flat glass for windows and similar applications is formed by the float glass process, developed between 1953 and 1957 by Sir Alastair Pilkington and Kenneth Bickerstaff of the UK's Pilkington Brothers, who created a continuous ribbon of glass using a molten tin bath on which the molten glass flows unhindered under the influence of gravity. The top surface of the glass is subjected to nitrogen under pressure to obtain a polished finish. Container glass for common bottles and jars is formed by blowing and pressing methods. This glass is often slightly modified chemically (with more alumina and calcium oxide) for greater water resistance. Further glass forming techniques are summarized in the table Glass forming techniques.
Glass has the ability to refract, reflect, and transmit light following geometrical optics, without scattering it. It is used in the manufacture of lenses and windows. Common glass has a refraction index around 1.5. This may be modified by adding low-density materials such as boron, which lowers the index of refraction (see crown glass), or increased (to as much as 1.8) with high-density materials such as (classically) lead oxide (see flint glass and lead glass), or in modern uses, less toxic oxides of zirconium, titanium, or barium. These high-index glasses (inaccurately known as "crystal" when used in glass vessels) cause more chromatic dispersion of light, and are prized for their diamond-like optical properties.
The most familiar, and historically the oldest, types of glass are "silicate glasses" based on the chemical compound silica (silicon dioxide, or quartz), the primary constituent of sand. The term glass, in popular usage, is often used to refer only to this type of material, which is familiar from use as window glass and in glass bottles. Of the many silica-based glasses that exist, ordinary glazing and container glass is formed from a specific type called soda-lime glass, composed of approximately 75% silicon dioxide (SiO2), sodium oxide (Na2O) from sodium carbonate (Na2CO3), calcium oxide, also called lime (CaO), and several minor additives. A very clear and durable quartz glass can be made from pure silica, but the high melting point and very narrow glass transition of quartz make glassblowing and hot working difficult. In glasses like soda lime, the compounds added to quartz are used to lower the melting temperature and improve workability, at a cost in the toughness, thermal stability, and optical transmittance.
Glass is in widespread use largely due to the production of glass compositions that are transparent to visible light. In contrast, polycrystalline materials do not generally transmit visible light. The individual crystallites may be transparent, but their facets (grain boundaries) reflect or scatter light resulting in diffuse reflection. Glass does not contain the internal subdivisions associated with grain boundaries in polycrystals and hence does not scatter light in the same manner as a polycrystalline material. The surface of a glass is often smooth since during glass formation the molecules of the supercooled liquid are not forced to dispose in rigid crystal geometries and can follow surface tension, which imposes a microscopically smooth surface. These properties, which give glass its clearness, can be retained even if glass is partially light-absorbing—i.e., colored.
Naturally occurring glass, especially the volcanic glass obsidian, has been used by many Stone Age societies across the globe for the production of sharp cutting tools and, due to its limited source areas, was extensively traded. But in general, archaeological evidence suggests that the first true glass was made in coastal north Syria, Mesopotamia or ancient Egypt. The earliest known glass objects, of the mid third millennium BCE, were beads, perhaps initially created as accidental by-products of metal-working (slags) or during the production of faience, a pre-glass vitreous material made by a process similar to glazing.
Color in glass may be obtained by addition of electrically charged ions (or color centers) that are homogeneously distributed, and by precipitation of finely dispersed particles (such as in photochromic glasses). Ordinary soda-lime glass appears colorless to the naked eye when it is thin, although iron(II) oxide (FeO) impurities of up to 0.1 wt% produce a green tint, which can be viewed in thick pieces or with the aid of scientific instruments. Further FeO and Cr2O3 additions may be used for the production of green bottles. Sulfur, together with carbon and iron salts, is used to form iron polysulfides and produce amber glass ranging from yellowish to almost black. A glass melt can also acquire an amber color from a reducing combustion atmosphere. Manganese dioxide can be added in small amounts to remove the green tint given by iron(II) oxide. When used in art glass or studio glass is colored using closely guarded recipes that involve specific combinations of metal oxides, melting temperatures and 'cook' times. Most colored glass used in the art market is manufactured in volume by vendors who serve this market although there are some glassmakers with the ability to make their own color from raw materials.
Glass remained a luxury material, and the disasters that overtook Late Bronze Age civilizations seem to have brought glass-making to a halt. Indigenous development of glass technology in South Asia may have begun in 1730 BCE. In ancient China, though, glassmaking seems to have a late start, compared to ceramics and metal work. The term glass developed in the late Roman Empire. It was in the Roman glassmaking center at Trier, now in modern Germany, that the late-Latin term glesum originated, probably from a Germanic word for a transparent, lustrous substance. Glass objects have been recovered across the Roman empire in domestic, industrial and funerary contexts.[citation needed]
Glass was used extensively during the Middle Ages. Anglo-Saxon glass has been found across England during archaeological excavations of both settlement and cemetery sites. Glass in the Anglo-Saxon period was used in the manufacture of a range of objects including vessels, beads, windows and was also used in jewelry. From the 10th-century onwards, glass was employed in stained glass windows of churches and cathedrals, with famous examples at Chartres Cathedral and the Basilica of Saint Denis. By the 14th-century, architects were designing buildings with walls of stained glass such as Sainte-Chapelle, Paris, (1203–1248) and the East end of Gloucester Cathedral. Stained glass had a major revival with Gothic Revival architecture in the 19th-century. With the Renaissance, and a change in architectural style, the use of large stained glass windows became less prevalent. The use of domestic stained glass increased until most substantial houses had glass windows. These were initially small panes leaded together, but with the changes in technology, glass could be manufactured relatively cheaply in increasingly larger sheets. This led to larger window panes, and, in the 20th-century, to much larger windows in ordinary domestic and commercial buildings.
In the 20th century, new types of glass such as laminated glass, reinforced glass and glass bricks have increased the use of glass as a building material and resulted in new applications of glass. Multi-storey buildings are frequently constructed with curtain walls made almost entirely of glass. Similarly, laminated glass has been widely applied to vehicles for windscreens. While glass containers have always been used for storage and are valued for their hygienic properties, glass has been utilized increasingly in industry. Optical glass for spectacles has been used since the late Middle Ages. The production of lenses has become increasingly proficient, aiding astronomers as well as having other application in medicine and science. Glass is also employed as the aperture cover in many solar energy systems.
From the 19th century, there was a revival in many ancient glass-making techniques including cameo glass, achieved for the first time since the Roman Empire and initially mostly used for pieces in a neo-classical style. The Art Nouveau movement made great use of glass, with René Lalique, Émile Gallé, and Daum of Nancy producing colored vases and similar pieces, often in cameo glass, and also using luster techniques. Louis Comfort Tiffany in America specialized in stained glass, both secular and religious, and his famous lamps. The early 20th-century saw the large-scale factory production of glass art by firms such as Waterford and Lalique. From about 1960 onwards there have been an increasing number of small studios hand-producing glass artworks, and glass artists began to class themselves as in effect sculptors working in glass, and their works as part fine arts.
Addition of lead(II) oxide lowers melting point, lowers viscosity of the melt, and increases refractive index. Lead oxide also facilitates solubility of other metal oxides and is used in colored glasses. The viscosity decrease of lead glass melt is very significant (roughly 100 times in comparison with soda glasses); this allows easier removal of bubbles and working at lower temperatures, hence its frequent use as an additive in vitreous enamels and glass solders. The high ionic radius of the Pb2+ ion renders it highly immobile in the matrix and hinders the movement of other ions; lead glasses therefore have high electrical resistance, about two orders of magnitude higher than soda-lime glass (108.5 vs 106.5 Ohm·cm, DC at 250 °C). For more details, see lead glass.
There are three classes of components for oxide glasses: network formers, intermediates, and modifiers. The network formers (silicon, boron, germanium) form a highly cross-linked network of chemical bonds. The intermediates (titanium, aluminium, zirconium, beryllium, magnesium, zinc) can act as both network formers and modifiers, according to the glass composition. The modifiers (calcium, lead, lithium, sodium, potassium) alter the network structure; they are usually present as ions, compensated by nearby non-bridging oxygen atoms, bound by one covalent bond to the glass network and holding one negative charge to compensate for the positive ion nearby. Some elements can play multiple roles; e.g. lead can act both as a network former (Pb4+ replacing Si4+), or as a modifier.
The alkali metal ions are small and mobile; their presence in glass allows a degree of electrical conductivity, especially in molten state or at high temperature. Their mobility decreases the chemical resistance of the glass, allowing leaching by water and facilitating corrosion. Alkaline earth ions, with their two positive charges and requirement for two non-bridging oxygen ions to compensate for their charge, are much less mobile themselves and also hinder diffusion of other ions, especially the alkalis. The most common commercial glasses contain both alkali and alkaline earth ions (usually sodium and calcium), for easier processing and satisfying corrosion resistance. Corrosion resistance of glass can be achieved by dealkalization, removal of the alkali ions from the glass surface by reaction with e.g. sulfur or fluorine compounds. Presence of alkaline metal ions has also detrimental effect to the loss tangent of the glass, and to its electrical resistance; glasses for electronics (sealing, vacuum tubes, lamps...) have to take this in account.
New chemical glass compositions or new treatment techniques can be initially investigated in small-scale laboratory experiments. The raw materials for laboratory-scale glass melts are often different from those used in mass production because the cost factor has a low priority. In the laboratory mostly pure chemicals are used. Care must be taken that the raw materials have not reacted with moisture or other chemicals in the environment (such as alkali or alkaline earth metal oxides and hydroxides, or boron oxide), or that the impurities are quantified (loss on ignition). Evaporation losses during glass melting should be considered during the selection of the raw materials, e.g., sodium selenite may be preferred over easily evaporating SeO2. Also, more readily reacting raw materials may be preferred over relatively inert ones, such as Al(OH)3 over Al2O3. Usually, the melts are carried out in platinum crucibles to reduce contamination from the crucible material. Glass homogeneity is achieved by homogenizing the raw materials mixture (glass batch), by stirring the melt, and by crushing and re-melting the first melt. The obtained glass is usually annealed to prevent breakage during processing.
In the past, small batches of amorphous metals with high surface area configurations (ribbons, wires, films, etc.) have been produced through the implementation of extremely rapid rates of cooling. This was initially termed "splat cooling" by doctoral student W. Klement at Caltech, who showed that cooling rates on the order of millions of degrees per second is sufficient to impede the formation of crystals, and the metallic atoms become "locked into" a glassy state. Amorphous metal wires have been produced by sputtering molten metal onto a spinning metal disk. More recently a number of alloys have been produced in layers with thickness exceeding 1 millimeter. These are known as bulk metallic glasses (BMG). Liquidmetal Technologies sell a number of zirconium-based BMGs. Batches of amorphous steel have also been produced that demonstrate mechanical properties far exceeding those found in conventional steel alloys.
In 2004, NIST researchers presented evidence that an isotropic non-crystalline metallic phase (dubbed "q-glass") could be grown from the melt. This phase is the first phase, or "primary phase," to form in the Al-Fe-Si system during rapid cooling. Interestingly, experimental evidence indicates that this phase forms by a first-order transition. Transmission electron microscopy (TEM) images show that the q-glass nucleates from the melt as discrete particles, which grow spherically with a uniform growth rate in all directions. The diffraction pattern shows it to be an isotropic glassy phase. Yet there is a nucleation barrier, which implies an interfacial discontinuity (or internal surface) between the glass and the melt.
Glass-ceramic materials share many properties with both non-crystalline glass and crystalline ceramics. They are formed as a glass, and then partially crystallized by heat treatment. For example, the microstructure of whiteware ceramics frequently contains both amorphous and crystalline phases. Crystalline grains are often embedded within a non-crystalline intergranular phase of grain boundaries. When applied to whiteware ceramics, vitreous means the material has an extremely low permeability to liquids, often but not always water, when determined by a specified test regime.
The term mainly refers to a mix of lithium and aluminosilicates that yields an array of materials with interesting thermomechanical properties. The most commercially important of these have the distinction of being impervious to thermal shock. Thus, glass-ceramics have become extremely useful for countertop cooking. The negative thermal expansion coefficient (CTE) of the crystalline ceramic phase can be balanced with the positive CTE of the glassy phase. At a certain point (~70% crystalline) the glass-ceramic has a net CTE near zero. This type of glass-ceramic exhibits excellent mechanical properties and can sustain repeated and quick temperature changes up to 1000 °C.
Mass production of glass window panes in the early twentieth century caused a similar effect. In glass factories, molten glass was poured onto a large cooling table and allowed to spread. The resulting glass is thicker at the location of the pour, located at the center of the large sheet. These sheets were cut into smaller window panes with nonuniform thickness, typically with the location of the pour centered in one of the panes (known as "bull's-eyes") for decorative effect. Modern glass intended for windows is produced as float glass and is very uniform in thickness.
The observation that old windows are sometimes found to be thicker at the bottom than at the top is often offered as supporting evidence for the view that glass flows over a timescale of centuries, the assumption being that the glass has exhibited the liquid property of flowing from one shape to another. This assumption is incorrect, as once solidified, glass stops flowing. The reason for the observation is that in the past, when panes of glass were commonly made by glassblowers, the technique used was to spin molten glass so as to create a round, mostly flat and even plate (the crown glass process, described above). This plate was then cut to fit a window. The pieces were not absolutely flat; the edges of the disk became a different thickness as the glass spun. When installed in a window frame, the glass would be placed with the thicker side down both for the sake of stability and to prevent water accumulating in the lead cames at the bottom of the window. Occasionally such glass has been found installed with the thicker side at the top, left or right.
In physics, the standard definition of a glass (or vitreous solid) is a solid formed by rapid melt quenching. The term glass is often used to describe any amorphous solid that exhibits a glass transition temperature Tg. If the cooling is sufficiently rapid (relative to the characteristic crystallization time) then crystallization is prevented and instead the disordered atomic configuration of the supercooled liquid is frozen into the solid state at Tg. The tendency for a material to form a glass while quenched is called glass-forming ability. This ability can be predicted by the rigidity theory. Generally, the structure of a glass exists in a metastable state with respect to its crystalline form, although in certain circumstances, for example in atactic polymers, there is no crystalline analogue of the amorphous phase.
Some people consider glass to be a liquid due to its lack of a first-order phase transition where certain thermodynamic variables such as volume, entropy and enthalpy are discontinuous through the glass transition range. The glass transition may be described as analogous to a second-order phase transition where the intensive thermodynamic variables such as the thermal expansivity and heat capacity are discontinuous. Nonetheless, the equilibrium theory of phase transformations does not entirely hold for glass, and hence the glass transition cannot be classed as one of the classical equilibrium phase transformations in solids.
Although the atomic structure of glass shares characteristics of the structure in a supercooled liquid, glass tends to behave as a solid below its glass transition temperature. A supercooled liquid behaves as a liquid, but it is below the freezing point of the material, and in some cases will crystallize almost instantly if a crystal is added as a core. The change in heat capacity at a glass transition and a melting transition of comparable materials are typically of the same order of magnitude, indicating that the change in active degrees of freedom is comparable as well. Both in a glass and in a crystal it is mostly only the vibrational degrees of freedom that remain active, whereas rotational and translational motion is arrested. This helps to explain why both crystalline and non-crystalline solids exhibit rigidity on most experimental time scales.
In Japanese, they are usually referred to as bushi (武士?, [bu.ɕi]) or buke (武家?). According to translator William Scott Wilson: "In Chinese, the character 侍 was originally a verb meaning "to wait upon" or "accompany persons" in the upper ranks of society, and this is also true of the original term in Japanese, saburau. In both countries the terms were nominalized to mean "those who serve in close attendance to the nobility", the pronunciation in Japanese changing to saburai. According to Wilson, an early reference to the word "samurai" appears in the Kokin Wakashū (905–914), the first imperial anthology of poems, completed in the first part of the 10th century.
By the end of the 12th century, samurai became almost entirely synonymous with bushi, and the word was closely associated with the middle and upper echelons of the warrior class. The samurai were usually associated with a clan and their lord, were trained as officers in military tactics and grand strategy, and they followed a set of rules that later came to be known as the bushidō. While the samurai numbered less than 10% of then Japan's population, their teachings can still be found today in both everyday life and in modern Japanese martial arts.
Following the Battle of Hakusukinoe against Tang China and Silla in 663 AD that led to a Japanese retreat from Korean affairs, Japan underwent widespread reform. One of the most important was that of the Taika Reform, issued by Prince Naka no Ōe (Emperor Tenji) in 646 AD. This edict allowed the Japanese aristocracy to adopt the Tang dynasty political structure, bureaucracy, culture, religion, and philosophy. As part of the Taihō Code, of 702 AD, and the later Yōrō Code, the population was required to report regularly for census, a precursor for national conscription. With an understanding of how the population was distributed, Emperor Mommu introduced a law whereby 1 in 3–4 adult males was drafted into the national military. These soldiers were required to supply their own weapons, and in return were exempted from duties and taxes. This was one of the first attempts by the Imperial government to form an organized army modeled after the Chinese system. It was called "Gundan-Sei" (軍団制) by later historians and is believed to have been short-lived.[citation needed]
In the early Heian period, the late 8th and early 9th centuries, Emperor Kammu sought to consolidate and expand his rule in northern Honshū, but the armies he sent to conquer the rebellious Emishi people lacked motivation and discipline, and failed in their task.[citation needed] Emperor Kammu introduced the title of sei'i-taishōgun (征夷大将軍) or Shogun, and began to rely on the powerful regional clans to conquer the Emishi. Skilled in mounted combat and archery (kyūdō), these clan warriors became the Emperor's preferred tool for putting down rebellions.[citation needed] Though this is the first known use of the "Shogun" title, it was a temporary title, and was not imbued with political power until the 13th century. At this time (the 7th to 9th century) the Imperial Court officials considered them merely a military section under the control of the Imperial Court.
After the Genpei war of the late 12th century, a clan leader Minamoto no Yoritomo obtained the right to appoint shugo and jito, and was allowed to organize soldiers and police, and to collect a certain amount of tax. Initially, their responsibility was restricted to arresting rebels and collecting needed army provisions, and they were forbidden from interfering with Kokushi Governors, but their responsibility gradually expanded and thus the samurai-class appeared as the political ruling power in Japan. Minamoto no Yoritomo opened the Kamakura Bakufu Shogunate in 1192.
Originally the Emperor and non-warrior nobility employed these warrior nobles. In time, they amassed enough manpower, resources and political backing in the form of alliances with one another, to establish the first samurai-dominated government. As the power of these regional clans grew, their chief was typically a distant relative of the Emperor and a lesser member of either the Fujiwara, Minamoto, or Taira clans. Though originally sent to provincial areas for a fixed four-year term as a magistrate, the toryo declined to return to the capital when their terms ended, and their sons inherited their positions and continued to lead the clans in putting down rebellions throughout Japan during the middle- and later-Heian period. Because of their rising military and economic power, the warriors ultimately became a new force in the politics of the court. Their involvement in the Hōgen in the late Heian period consolidated their power, and finally pitted the rival Minamoto and Taira clans against each other in the Heiji Rebellion of 1160.
The winner, Taira no Kiyomori, became an imperial advisor, and was the first warrior to attain such a position. He eventually seized control of the central government, establishing the first samurai-dominated government and relegating the Emperor to figurehead status. However, the Taira clan was still very conservative when compared to its eventual successor, the Minamoto, and instead of expanding or strengthening its military might, the clan had its women marry Emperors and exercise control through the Emperor.
The Taira and the Minamoto clashed again in 1180, beginning the Gempei War, which ended in 1185. Samurai fought at the naval battle of Dan-no-ura, at the Shimonoseki Strait which separates Honshu and Kyushu in 1185. The victorious Minamoto no Yoritomo established the superiority of the samurai over the aristocracy. In 1190 he visited Kyoto and in 1192 became Sei'i-taishōgun, establishing the Kamakura Shogunate, or Kamakura Bakufu. Instead of ruling from Kyoto, he set up the Shogunate in Kamakura, near his base of power. "Bakufu" means "tent government", taken from the encampments the soldiers would live in, in accordance with the Bakufu's status as a military government.
In 1274, the Mongol-founded Yuan dynasty in China sent a force of some 40,000 men and 900 ships to invade Japan in northern Kyūshū. Japan mustered a mere 10,000 samurai to meet this threat. The invading army was harassed by major thunderstorms throughout the invasion, which aided the defenders by inflicting heavy casualties. The Yuan army was eventually recalled and the invasion was called off. The Mongol invaders used small bombs, which was likely the first appearance of bombs and gunpowder in Japan.
The Japanese defenders recognized the possibility of a renewed invasion, and began construction of a great stone barrier around Hakata Bay in 1276. Completed in 1277, this wall stretched for 20 kilometers around the border of the bay. This would later serve as a strong defensive point against the Mongols. The Mongols attempted to settle matters in a diplomatic way from 1275 to 1279, but every envoy sent to Japan was executed. This set the stage for one of the most famous engagements in Japanese history.
In 1592, and again in 1597, Toyotomi Hideyoshi, aiming to invade China (唐入り) through Korea, mobilized an army of 160,000 peasants and samurai and deployed them to Korea. (See Hideyoshi's invasions of Korea, Chōsen-seibatsu (朝鮮征伐?). Taking advantage of arquebus mastery and extensive wartime experience from the Sengoku period, Japanese samurai armies made major gains in most of Korea. Kato Kiyomasa advanced to Orangkai territory (present-day Manchuria) bordering Korea to the northeast and crossed the border into Manchuria, but withdrew after retaliatory attacks from the Jurchens there, as it was clear he had outpaced the rest of the Japanese invasion force. A few of the more famous samurai generals of this war were Katō Kiyomasa, Konishi Yukinaga, and Shimazu Yoshihiro. Shimazu Yoshihiro led some 7,000 samurai and, despite being heavily outnumbered, defeated a host of allied Ming and Korean forces at the Battle of Sacheon in 1598, near the conclusion of the campaigns. Yoshihiro was feared as Oni-Shimazu ("Shimazu ogre") and his nickname spread across not only Korea but to Ming Dynasty China. In spite of the superiority of Japanese land forces, ultimately the two expeditions failed (though they did devastate the Korean landmass) from factors such as Korean naval superiority (which, led by Admiral Yi Sun-shin, harassed Japanese supply lines continuously throughout the wars, resulting in supply shortages on land), the commitment of sizeable Ming forces to Korea, Korean guerrilla actions, the underestimation of resistance by Japanese commanders (in the first campaign of 1592, Korean defenses on land were caught unprepared, under-trained, and under-armed; they were rapidly overrun, with only a limited number of successfully resistant engagements against the more-experienced and battle-hardened Japanese forces - in the second campaign of 1597, Korean and Ming forces proved to be a far more difficult challenge and, with the support of continued Korean naval superiority, limited Japanese gains to parts southeastern Korea), and wavering Japanese commitment to the campaigns as the wars dragged on. The final death blow to the Japanese campaigns in Korea came with Hideyoshi's death in late 1598 and the recall of all Japanese forces in Korea by the Council of Five Elders (established by Hideyoshi to oversee the transition from his regency to that of his son Hideyori).
It should be noted that many samurai forces that were active throughout this period were not deployed to Korea; most importantly, the daimyo Tokugawa Ieyasu carefully kept forces under his command out of the Korean campaigns, and other samurai commanders who were opposed to Hideyoshi's domination of Japan either mulled Hideyoshi's call to invade Korea or contributed a small token force. Most commanders who did opposed or otherwise resisted/resented Hideyoshi ended up as part of the so-called Eastern Army, while commanders loyal to Hideyoshi and his son (a notable exception to this trend was Katō Kiyomasa, who deployed with Tokugawa and the Eastern Army) were largely committed to the Western Army; the two opposing sides (so named for the relative geographical locations of their respective commanders' domains) would later clash, most notably at the Battle of Sekigahara, which was won by Tokugawa Ieyasu and the Eastern Forces, paving the way for the establishment of the Tokugawa Shogunate.
Oda Nobunaga made innovations in the fields of organization and war tactics, heavily used arquebuses, developed commerce and industry and treasured innovation. Consecutive victories enabled him to realize the termination of the Ashikaga Bakufu and the disarmament of the military powers of the Buddhist monks, which had inflamed futile struggles among the populace for centuries. Attacking from the "sanctuary" of Buddhist temples, they were constant headaches to any warlord and even the Emperor who tried to control their actions. He died in 1582 when one of his generals, Akechi Mitsuhide, turned upon him with his army.
During the Tokugawa shogunate, samurai increasingly became courtiers, bureaucrats, and administrators rather than warriors. With no warfare since the early 17th century, samurai gradually lost their military function during the Tokugawa era (also called the Edo period). By the end of the Tokugawa era, samurai were aristocratic bureaucrats for the daimyo, with their daisho, the paired long and short swords of the samurai (cf. katana and wakizashi) becoming more of a symbolic emblem of power rather than a weapon used in daily life. They still had the legal right to cut down any commoner who did not show proper respect kiri-sute gomen (斬り捨て御免?), but to what extent this right was used is unknown. When the central government forced daimyos to cut the size of their armies, unemployed rōnin became a social problem.
Theoretical obligations between a samurai and his lord (usually a daimyo) increased from the Genpei era to the Edo era. They were strongly emphasized by the teachings of Confucius and Mencius (ca 550 BC), which were required reading for the educated samurai class. Bushido was formalized by several influential leaders and families before the Edo Period. Bushido was an ideal, and it remained fairly uniform from the 13th century to the 19th century — the ideals of Bushido transcended social class, time and geographic location of the warrior class.
The relative peace of the Tokugawa era was shattered with the arrival of Commodore Matthew Perry's massive U.S. Navy steamships in 1853. Perry used his superior firepower to force Japan to open its borders to trade. Prior to that only a few harbor towns, under strict control from the Shogunate, were allowed to participate in Western trade, and even then, it was based largely on the idea of playing the Franciscans and Dominicans off against one another (in exchange for the crucial arquebus technology, which in turn was a major contributor to the downfall of the classical samurai).
From 1854, the samurai army and the navy were modernized. A Naval training school was established in Nagasaki in 1855. Naval students were sent to study in Western naval schools for several years, starting a tradition of foreign-educated future leaders, such as Admiral Enomoto. French naval engineers were hired to build naval arsenals, such as Yokosuka and Nagasaki. By the end of the Tokugawa shogunate in 1867, the Japanese navy of the shogun already possessed eight western-style steam warships around the flagship Kaiyō Maru, which were used against pro-imperial forces during the Boshin war, under the command of Admiral Enomoto. A French Military Mission to Japan (1867) was established to help modernize the armies of the Bakufu.
Emperor Meiji abolished the samurai's right to be the only armed force in favor of a more modern, western-style, conscripted army in 1873. Samurai became Shizoku (士族) who retained some of their salaries, but the right to wear a katana in public was eventually abolished along with the right to execute commoners who paid them disrespect. The samurai finally came to an end after hundreds of years of enjoyment of their status, their powers, and their ability to shape the government of Japan. However, the rule of the state by the military class was not yet over. In defining how a modern Japan should be, members of the Meiji government decided to follow the footsteps of the United Kingdom and Germany, basing the country on the concept of noblesse oblige. Samurai were not a political force under the new order. With the Meiji reforms in the late 19th century, the samurai class was abolished, and a western-style national army was established. The Imperial Japanese Armies were conscripted, but many samurai volunteered as soldiers, and many advanced to be trained as officers. Much of the Imperial Army officer class was of samurai origin, and were highly motivated, disciplined, and exceptionally trained.
Samurai were many of the early exchange students, not directly because they were samurai, but because many samurai were literate and well-educated scholars. Some of these exchange students started private schools for higher educations, while many samurai took pens instead of guns and became reporters and writers, setting up newspaper companies, and others entered governmental service. Some samurai became businessmen. For example, Iwasaki Yatarō, who was the great-grandson of a samurai, established Mitsubishi.
The philosophies of Buddhism and Zen, and to a lesser extent Confucianism and Shinto, influenced the samurai culture. Zen meditation became an important teaching due to it offering a process to calm one's mind. The Buddhist concept of reincarnation and rebirth led samurai to abandon torture and needless killing, while some samurai even gave up violence altogether and became Buddhist monks after realizing how fruitless their killings were. Some were killed as they came to terms with these realizations in the battlefield. The most defining role that Confucianism played in samurai philosophy was to stress the importance of the lord-retainer relationship—the loyalty that a samurai was required to show his lord.
In the 13th century, Hōjō Shigetoki (1198–1261 AD) wrote: "When one is serving officially or in the master's court, he should not think of a hundred or a thousand people, but should consider only the importance of the master." Carl Steenstrup noted that 13th and 14th century warrior writings (gunki) "portrayed the bushi in their natural element, war, eulogizing such virtues as reckless bravery, fierce family pride, and selfless, at times senseless devotion of master and man". Feudal lords such as Shiba Yoshimasa (1350–1410 AD) stated that a warrior looked forward to a glorious death in the service of a military leader or the Emperor: "It is a matter of regret to let the moment when one should die pass by....First, a man whose profession is the use of arms should think and then act upon not only his own fame, but also that of his descendants. He should not scandalize his name forever by holding his one and only life too dear....One's main purpose in throwing away his life is to do so either for the sake of the Emperor or in some great undertaking of a military general. It is that exactly that will be the great fame of one's descendants."
"First of all, a samurai who dislikes battle and has not put his heart in the right place even though he has been born in the house of the warrior, should not be reckoned among one's retainers....It is forbidden to forget the great debt of kindness one owes to his master and ancestors and thereby make light of the virtues of loyalty and filial piety....It is forbidden that one should...attach little importance to his duties to his master...There is a primary need to distinguish loyalty from disloyalty and to establish rewards and punishments."
Katō Kiyomasa was one of the most powerful and well-known lords of the Sengoku Era. He commanded most of Japan's major clans during the invasion of Korea (1592–1598). In a handbook he addressed to "all samurai, regardless of rank" he told his followers that a warrior's only duty in life was to "...grasp the long and the short swords and to die". He also ordered his followers to put forth great effort in studying the military classics, especially those related to loyalty and filial piety. He is best known for his quote: "If a man does not investigate into the matter of Bushido daily, it will be difficult for him to die a brave and manly death. Thus it is essential to engrave this business of the warrior into one's mind well."
Torii Mototada (1539–1600) was a feudal lord in the service of Tokugawa Ieyasu. On the eve of the battle of Sekigahara, he volunteered to remain behind in the doomed Fushimi Castle while his lord advanced to the east. Torii and Tokugawa both agreed that the castle was indefensible. In an act of loyalty to his lord, Torii chose to remain behind, pledging that he and his men would fight to the finish. As was custom, Torii vowed that he would not be taken alive. In a dramatic last stand, the garrison of 2,000 men held out against overwhelming odds for ten days against the massive army of Ishida Mitsunari's 40,000 warriors. In a moving last statement to his son Tadamasa, he wrote:
The rival of Takeda Shingen (1521–1573) was Uesugi Kenshin (1530–1578), a legendary Sengoku warlord well-versed in the Chinese military classics and who advocated the "way of the warrior as death". Japanese historian Daisetz Teitaro Suzuki describes Uesugi's beliefs as: "Those who are reluctant to give up their lives and embrace death are not true warriors.... Go to the battlefield firmly confident of victory, and you will come home with no wounds whatever. Engage in combat fully determined to die and you will be alive; wish to survive in the battle and you will surely meet death. When you leave the house determined not to see it again you will come home safely; when you have any thought of returning you will not return. You may not be in the wrong to think that the world is always subject to change, but the warrior must not entertain this way of thinking, for his fate is always determined."
Historian H. Paul Varley notes the description of Japan given by Jesuit leader St. Francis Xavier (1506–1552): "There is no nation in the world which fears death less." Xavier further describes the honour and manners of the people: "I fancy that there are no people in the world more punctilious about their honour than the Japanese, for they will not put up with a single insult or even a word spoken in anger." Xavier spent the years 1549–1551 converting Japanese to Christianity. He also observed: "The Japanese are much braver and more warlike than the people of China, Korea, Ternate and all of the other nations around the Philippines."
In December 1547, Francis was in Malacca (Malaysia) waiting to return to Goa (India) when he met a low-ranked samurai named Anjiro (possibly spelled "Yajiro"). Anjiro was not an intellectual, but he impressed Xavier because he took careful notes of everything he said in church. Xavier made the decision to go to Japan in part because this low-ranking samurai convinced him in Portuguese that the Japanese people were highly educated and eager to learn. They were hard workers and respectful of authority. In their laws and customs they were led by reason, and, should the Christian faith convince them of its truth, they would accept it en masse.
In his book "Ideals of the Samurai" translator William Scott Wilson states: "The warriors in the Heike Monogatari served as models for the educated warriors of later generations, and the ideals depicted by them were not assumed to be beyond reach. Rather, these ideals were vigorously pursued in the upper echelons of warrior society and recommended as the proper form of the Japanese man of arms. With the Heike Monogatari, the image of the Japanese warrior in literature came to its full maturity." Wilson then translates the writings of several warriors who mention the Heike Monogatari as an example for their men to follow.
As aristocrats for centuries, samurai developed their own cultures that influenced Japanese culture as a whole. The culture associated with the samurai such as the tea ceremony, monochrome ink painting, rock gardens and poetry were adopted by warrior patrons throughout the centuries 1200–1600. These practices were adapted from the Chinese arts. Zen monks introduced them to Japan and they were allowed to flourish due to the interest of powerful warrior elites. Musō Soseki (1275–1351) was a Zen monk who was advisor to both Emperor Go-Daigo and General Ashikaga Takauji (1304–58). Musō, as well as other monks, acted as political and cultural diplomat between Japan and China. Musō was particularly well known for his garden design. Another Ashikaga patron of the arts was Yoshimasa. His cultural advisor, the Zen monk Zeami, introduced tea ceremony to him. Previously, tea had been used primarily for Buddhist monks to stay awake during meditation.
For example, the full name of Oda Nobunaga would be "Oda Kazusanosuke Saburo Nobunaga" (織田上総介三郎信長), in which "Oda" is a clan or family name, "Kazusanosuke" is a title of vice-governor of Kazusa province, "Saburo" is a formal nickname (yobina), and "Nobunaga" is an adult name (nanori) given at genpuku, the coming of age ceremony. A man was addressed by his family name and his title, or by his yobina if he did not have a title. However, the nanori was a private name that could be used by only a very few, including the Emperor.
A samurai could take concubines but their backgrounds were checked by higher-ranked samurai. In many cases, taking a concubine was akin to a marriage. Kidnapping a concubine, although common in fiction, would have been shameful, if not criminal. If the concubine was a commoner, a messenger was sent with betrothal money or a note for exemption of tax to ask for her parents' acceptance. Even though the woman would not be a legal wife, a situation normally considered a demotion, many wealthy merchants believed that being the concubine of a samurai was superior to being the legal wife of a commoner. When a merchant's daughter married a samurai, her family's money erased the samurai's debts, and the samurai's social status improved the standing of the merchant family. If a samurai's commoner concubine gave birth to a son, the son could inherit his father's social status.
A samurai could divorce his wife for a variety of reasons with approval from a superior, but divorce was, while not entirely nonexistent, a rare event. A wife's failure to produce a son was cause for divorce, but adoption of a male heir was considered an acceptable alternative to divorce. A samurai could divorce for personal reasons, even if he simply did not like his wife, but this was generally avoided as it would embarrass the person who had arranged the marriage. A woman could also arrange a divorce, although it would generally take the form of the samurai divorcing her. After a divorce samurai had to return the betrothal money, which often prevented divorces.
Maintaining the household was the main duty of samurai women. This was especially crucial during early feudal Japan, when warrior husbands were often traveling abroad or engaged in clan battles. The wife, or okugatasama (meaning: one who remains in the home), was left to manage all household affairs, care for the children, and perhaps even defend the home forcibly. For this reason, many women of the samurai class were trained in wielding a polearm called a naginata or a special knife called the kaiken in an art called tantojutsu (lit. the skill of the knife), which they could use to protect their household, family, and honor if the need arose.
Traits valued in women of the samurai class were humility, obedience, self-control, strength, and loyalty. Ideally, a samurai wife would be skilled at managing property, keeping records, dealing with financial matters, educating the children (and perhaps servants, too), and caring for elderly parents or in-laws that may be living under her roof. Confucian law, which helped define personal relationships and the code of ethics of the warrior class required that a woman show subservience to her husband, filial piety to her parents, and care to the children. Too much love and affection was also said to indulge and spoil the youngsters. Thus, a woman was also to exercise discipline.
This does not mean that samurai women were always powerless. Powerful women both wisely and unwisely wielded power at various occasions. After Ashikaga Yoshimasa, 8th shogun of the Muromachi shogunate, lost interest in politics, his wife Hino Tomiko largely ruled in his place. Nene, wife of Toyotomi Hideyoshi, was known to overrule her husband's decisions at times and Yodo-dono, his concubine, became the de facto master of Osaka castle and the Toyotomi clan after Hideyoshi's death. Tachibana Ginchiyo was chosen to lead the Tachibana clan after her father's death. Chiyo, wife of Yamauchi Kazutoyo, has long been considered the ideal samurai wife. According to legend, she made her kimono out of a quilted patchwork of bits of old cloth and saved pennies to buy her husband a magnificent horse, on which he rode to many victories. The fact that Chiyo (though she is better known as "Wife of Yamauchi Kazutoyo") is held in such high esteem for her economic sense is illuminating in the light of the fact that she never produced an heir and the Yamauchi clan was succeeded by Kazutoyo's younger brother. The source of power for women may have been that samurai left their finances to their wives.
As the Tokugawa period progressed more value became placed on education, and the education of females beginning at a young age became important to families and society as a whole. Marriage criteria began to weigh intelligence and education as desirable attributes in a wife, right along with physical attractiveness. Though many of the texts written for women during the Tokugawa period only pertained to how a woman could become a successful wife and household manager, there were those that undertook the challenge of learning to read, and also tackled philosophical and literary classics. Nearly all women of the samurai class were literate by the end of the Tokugawa period.
The English sailor and adventurer William Adams (1564–1620) was the first Westerner to receive the dignity of samurai. The Shogun Tokugawa Ieyasu presented him with two swords representing the authority of a samurai, and decreed that William Adams the sailor was dead and that Anjin Miura (三浦按針), a samurai, was born. Adams also received the title of hatamoto (bannerman), a high-prestige position as a direct retainer in the Shogun's court. He was provided with generous revenues: "For the services that I have done and do daily, being employed in the Emperor's service, the Emperor has given me a living" (Letters). He was granted a fief in Hemi (逸見) within the boundaries of present-day Yokosuka City, "with eighty or ninety husbandmen, that be my slaves or servants" (Letters). His estate was valued at 250 koku. He finally wrote "God hath provided for me after my great misery", (Letters) by which he meant the disaster-ridden voyage that initially brought him to Japan.
Jan Joosten van Lodensteijn (1556?–1623?), a Dutch colleague of Adams' on their ill-fated voyage to Japan in the ship De Liefde, was also given similar privileges by Tokugawa Ieyasu. It appears Joosten became a samurai[citation needed] and was given a residence within Ieyasu's castle at Edo. Today, this area at the east exit of Tokyo Station is known as Yaesu (八重洲). Yaesu is a corruption of the Dutchman's Japanese name, Yayousu (耶楊子). Also in common with Adam's, Joostens was given a Red Seal Ship (朱印船) allowing him to trade between Japan and Indo-China. On a return journey from Batavia Joosten drowned after his ship ran aground.
In the same war, the Prussian Edward Schnell served the Aizu domain as a military instructor and procurer of weapons. He was granted the Japanese name Hiramatsu Buhei (平松武兵衛), which inverted the characters of the daimyo's name Matsudaira. Hiramatsu (Schnell) was given the right to wear swords, as well as a residence in the castle town of Wakamatsu, a Japanese wife, and retainers. In many contemporary references, he is portrayed wearing a Japanese kimono, overcoat, and swords, with Western riding trousers and boots.
As far back as the seventh century Japanese warriors wore a form of lamellar armor, this armor eventually evolved into the armor worn by the samurai. The first types of Japanese armors identified as samurai armor were known as yoroi. These early samurai armors were made from small individual scales known as kozane. The kozane were made from either iron or leather and were bound together into small strips, the strips were coated with lacquer to protect the kozane from water. A series of strips of kozane were then laced together with silk or leather lace and formed into a complete chest armor (dou or dō).
In the 1500s a new type of armor started to become popular due to the advent of firearms, new fighting tactics and the need for additional protection. The kozane dou made from individual scales was replaced by plate armor. This new armor, which used iron plated dou (dō), was referred to as Tosei-gusoku, or modern armor. Various other components of armor protected the samurai's body. The helmet kabuto was an important part of the samurai's armor. Samurai armor changed and developed as the methods of samurai warfare changed over the centuries. The known last use of samurai armor occurring in 1877 during the satsuma rebellion. As the last samurai rebellion was crushed, Japan modernized its defenses and turned to a national conscription army that used uniforms.
The term samurai originally meant "those who serve in close attendance to nobility", and was written with a Chinese character (or kanji) that had the same meaning. In Japanese, it was originally recorded in the Nara Period as a verb *samorapu ("to watch, to keep watch, to observe, to be on the lookout for something; to serve, to attend"), which is believed to be derived from the frequentative form (*morapu 守らふ) of the verb moru (守る, "to watch, to guard, to be on the lookout; to keep, to protect, to take care of, to be in charge of, to have as one's ward"). By the Heian period, this word had developed into the verb saburahu (さぶらふ, "to serve, to attend"), from which a deverbal noun saburahi (さぶらひ, "servant, attendant") was later derived, and this noun then yielded samurahi (さむらひ) in the Edo period. In Japanese literature, there is an early reference to samurai in the Kokinshū (古今集, early 10th century):
Bushi was the name given to the ancient Japanese soldiers from traditional warrior families. The bushi class was developed mainly in the north of Japan. They formed powerful clans, which in the 12th century were against the noble families who were grouping themselves to support the imperial family who lived in Kyoto. Samurai was a word used by the Kuge aristocratic class with warriors themselves preferring the word bushi. The term Bushidō, the "way of the warrior", is derived from this term and the mansion of a warrior was called bukeyashiki.
Most samurai were bound by a code of honor and were expected to set an example for those below them. A notable part of their code is seppuku (切腹, seppuku?) or hara kiri, which allowed a disgraced samurai to regain his honor by passing into death, where samurai were still beholden to social rules. Whilst there are many romanticized characterizations of samurai behavior such as the writing of Bushido (武士道, Bushidō?) in 1905, studies of Kobudo and traditional Budō indicate that the samurai were as practical on the battlefield as were any other warrior.
Despite the rampant romanticism of the 20th century, samurai could be disloyal and treacherous (e.g., Akechi Mitsuhide), cowardly, brave, or overly loyal (e.g., Kusunoki Masashige). Samurai were usually loyal to their immediate superiors, who in turn allied themselves with higher lords. These loyalties to the higher lords often shifted; for example, the high lords allied under Toyotomi Hideyoshi (豊臣秀吉) were served by loyal samurai, but the feudal lords under them could shift their support to Tokugawa, taking their samurai with them. There were, however, also notable instances where samurai would be disloyal to their lord or daimyo, when loyalty to the Emperor was seen to have supremacy.
Jidaigeki (literally historical drama) has always been a staple program on Japanese movies and television. The programs typically feature a samurai. Samurai films and westerns share a number of similarities and the two have influenced each other over the years. One of Japan’s most renowned directors, Akira Kurosawa, greatly influenced the samurai aspect in western film-making.[citation needed] George Lucas’ Star Wars series incorporated many aspects from the Seven Samurai film. One example is that in the Japanese film, seven samurai warriors are hired by local farmers to protect their land from being overrun by bandits; In George Lucas’ Star Wars: A New Hope, a similar situation arises. Kurosawa was inspired by the works of director John Ford and in turn Kurosawa's works have been remade into westerns such as The Seven Samurai into The Magnificent Seven and Yojimbo into A Fistful of Dollars. There is also a 26 episode anime adaptation (Samurai 7) of The Seven Samurai. Along with film, literature containing samurai influences are seen as well.
Most common are historical works where the protagonist is either a samurai or former samurai (or another rank or position) who possesses considerable martial skill. Eiji Yoshikawa is one of the most famous Japanese historical novelists. His retellings of popular works, including Taiko, Musashi and Heike Tale, are popular among readers for their epic narratives and rich realism in depicting samurai and warrior culture.[citation needed] The samurai have also appeared frequently in Japanese comics (manga) and animation (anime). Samurai-like characters are not just restricted to historical settings and a number of works set in the modern age, and even the future, include characters who live, train and fight like samurai. Examples are Samurai Champloo, Requiem from the Darkness, Muramasa: The Demon Blade, and Afro Samurai. Some of these works have made their way to the west, where it has been increasing in popularity with America.
Just in the last two decades,[when?] samurai have become more popular in America. “Hyperbolizing the samurai in such a way that they appear as a whole to be a loyal body of master warriors provides international interest in certain characters due to admirable traits” (Moscardi, N.D.). Through various medium, producers and writers have been capitalizing on the notion that Americans admire the samurai lifestyle. The animated series, Afro Samurai, became well-liked in American popular culture due to its blend of hack-and-slash animation and gritty urban music.
Created by Takashi Okazaki, Afro Samurai was initially a doujinshi, or manga series, which was then made into an animated series by Studio Gonzo. In 2007 the animated series debuted on American cable television on the Spike TV channel (Denison, 2010). The series was produced for American viewers which “embodies the trend... comparing hip-hop artists to samurai warriors, an image some rappers claim for themselves (Solomon, 2009). The storyline keeps in tone with the perception of a samurais finding vengeance against someone who has wronged him. Starring the voice of well known American actor Samuel L. Jackson, “Afro is the second-strongest fighter in a futuristic, yet, still feudal Japan and seeks revenge upon the gunman who killed his father” (King 2008). Due to its popularity, Afro Samurai was adopted into a full feature animated film and also became titles on gaming consoles such as the PlayStation 3 and Xbox. Not only has the samurai culture been adopted into animation and video games, it can also be seen in comic books.
American comic books have adopted the character type for stories of their own like the mutant-villain Silver Samurai of Marvel Comics. The design of this character preserves the samurai appearance; the villain is “Clad in traditional gleaming samurai armor and wielding an energy charged katana” (Buxton, 2013). Not only does the Silver Samurai make over 350 comic book appearances, the character is playable in several video games, such as Marvel Vs. Capcom 1 and 2. In 2013, the samurai villain was depicted in James Mangold’s film The Wolverine. Ten years before the Wolverine debuted, another film helped pave the way to ensure the samurai were made known to American cinema: A film released in 2003 titled The Last Samurai, starring Tom Cruise, is inspired by the samurai way of life. In the film, Cruise’s character finds himself deeply immersed in samurai culture. The character in the film, “Nathan Algren, is a fictional contrivance to make nineteenth-century Japanese history less foreign to American viewers”.(Ravina, 2010) After being captured by a group of samurai rebels, he becomes empathetic towards the cause they fight for. Taking place during the Meiji Period, Tom Cruise plays the role of US Army Captain Nathan Algren, who travels to Japan to train a rookie army in fighting off samurai rebel groups. Becoming a product of his environment, Algren joins the samurai clan in an attempt to rescue a captured samurai leader. “By the end of the film, he has clearly taken on many of the samurai traits, such as zen-like mastery of the sword, and a budding understanding of spirituality”. (Manion, 2006)
The term dialect (from Latin dialectus, dialectos, from the ancient Greek word διάλεκτος diálektos, "discourse", from διά diá, "through" and λέγω legō, "I speak") is used in two distinct ways to refer to two different types of linguistic phenomena.
One usage—the more common among linguists—refers to a variety of a language that is a characteristic of a particular group of the language's speakers. The term is applied most often to regional speech patterns, but a dialect may also be defined by other factors, such as social class. A dialect that is associated with a particular social class can be termed a sociolect, a dialect that is associated with a particular ethnic group can be termed as ethnolect, and a regional dialect may be termed a regiolect. According to this definition, any variety of a language constitutes "a dialect", including any standard varieties.
The other usage refers to a language that is socially subordinated to a regional or national standard language, often historically cognate or related to the standard language, but not actually derived from it. In this sense, unlike in the first usage, the standard language would not itself be considered a "dialect," as it is the dominant language in a particular state or region, whether in terms of social or political status, official status, predominance or prevalence, or all of the above. Meanwhile, the "dialects" subordinate to the standard language are generally not variations on the standard language but rather separate (but often related) languages in and of themselves. For example, most of the various regional Romance languages of Italy, often colloquially referred to as Italian "dialects," are, in fact, not actually derived from modern standard Italian, but rather evolved from Vulgar Latin separately and individually from one another and independently of standard Italian, long prior to the diffusion of a national standardized language throughout what is now Italy. These various Latin-derived regional languages are therefore, in a linguistic sense, not truly "dialects" of the standard Italian language, but are instead better defined as their own separate languages. Conversely, with the spread of standard Italian throughout Italy in the 20th century, various regional versions or varieties of standard Italian developed, generally as a mix of the national standard Italian with local regional languages and local accents. These variations on standard Italian, known as regional Italian, would more appropriately be called "dialects" in accordance with the first linguistic definition of "dialect," as they are in fact derived partially or mostly from standard Italian. 
A dialect is distinguished by its vocabulary, grammar, and pronunciation (phonology, including prosody). Where a distinction can be made only in terms of pronunciation (including prosody, or just prosody itself), the term accent may be preferred over dialect. Other types of speech varieties include jargons, which are characterized by differences in lexicon (vocabulary); slang; patois; pidgins; and argots.
A standard dialect (also known as a standardized dialect or "standard language") is a dialect that is supported by institutions. Such institutional support may include government recognition or designation; presentation as being the "correct" form of a language in schools; published grammars, dictionaries, and textbooks that set forth a correct spoken and written form; and an extensive formal literature that employs that dialect (prose, poetry, non-fiction, etc.). There may be multiple standard dialects associated with a single language. For example, Standard American English, Standard British English, Standard Canadian English, Standard Indian English, Standard Australian English, and Standard Philippine English may all be said to be standard dialects of the English language.
A nonstandard dialect, like a standard dialect, has a complete vocabulary, grammar, and syntax, but is usually not the beneficiary of institutional support. Examples of a nonstandard English dialect are Southern American English, Western Australian English, Scouse and Tyke. The Dialect Test was designed by Joseph Wright to compare different English dialects with each other.
There is no universally accepted criterion for distinguishing two different languages from two dialects (i.e. varieties) of the same language. A number of rough measures exist, sometimes leading to contradictory results. The distinction is therefore subjective and depends on the user's frame of reference. For example, there is discussion about if the Limón Creole English must be considered as "a kind" of English or a different language. This creole is spoken in the Caribbean coast of Costa Rica (Central America) by descendant of Jamaican people. The position that Costa Rican linguists support depends on the University they belong.
The most common, and most purely linguistic, criterion is that of mutual intelligibility: two varieties are said to be dialects of the same language if being a speaker of one variety confers sufficient knowledge to understand and be understood by a speaker of the other; otherwise, they are said to be different languages. However, this definition becomes problematic in the case of dialect continua, in which it may be the case that dialect B is mutually intelligible with both dialect A and dialect C but dialects A and C are not mutually intelligible with each other. In this case the criterion of mutual intelligibility makes it impossible to decide whether A and C are dialects of the same language or not. Cases may also arise in which a speaker of dialect X can understand a speaker of dialect Y, but not vice versa; the mutual intelligibility criterion flounders here as well.
Another occasionally used criterion for discriminating dialects from languages is that of linguistic authority, a more sociolinguistic notion. According to this definition, two varieties are considered dialects of the same language if (under at least some circumstances) they would defer to the same authority regarding some questions about their language. For instance, to learn the name of a new invention, or an obscure foreign species of plant, speakers of Bavarian German and East Franconian German might each consult a German dictionary or ask a German-speaking expert in the subject. By way of contrast, although Yiddish is classified by linguists as a language in the "Middle High German" group of languages, a Yiddish speaker would not consult a German dictionary to determine the word to use in such a case.
By the definition most commonly used by linguists, any linguistic variety can be considered a "dialect" of some language—"everybody speaks a dialect". According to that interpretation, the criteria above merely serve to distinguish whether two varieties are dialects of the same language or dialects of different languages.
A framework was developed in 1967 by Heinz Kloss, abstand and ausbau languages, to describe speech communities, that while unified politically and/or culturally, include multiple dialects which though closely related genetically may be divergent to the point of inter-dialect unintelligibility.
The terms "language" and "dialect" are not necessarily mutually exclusive: There is nothing contradictory in the statement "the language of the Pennsylvania Dutch is a dialect of German".
There are various terms that linguists may use to avoid taking a position on whether the speech of a community is an independent language in its own right or a dialect of another language. Perhaps the most common is "variety"; "lect" is another. A more general term is "languoid", which does not distinguish between dialects, languages, and groups of languages, whether genealogically related or not.
In many societies, however, a particular dialect, often the sociolect of the elite class, comes to be identified as the "standard" or "proper" version of a language by those seeking to make a social distinction, and is contrasted with other varieties. As a result of this, in some contexts the term "dialect" refers specifically to varieties with low social status. In this secondary sense of "dialect", language varieties are often called dialects rather than languages:
The status of "language" is not solely determined by linguistic criteria, but it is also the result of a historical and political development. Romansh came to be a written language, and therefore it is recognized as a language, even though it is very close to the Lombardic alpine dialects. An opposite example is the case of Chinese, whose variations such as Mandarin and Cantonese are often called dialects and not languages, despite their mutual unintelligibility.
Modern Nationalism, as developed especially since the French Revolution, has made the distinction between "language" and "dialect" an issue of great political importance. A group speaking a separate "language" is often seen as having a greater claim to being a separate "people", and thus to be more deserving of its own independent state, while a group speaking a "dialect" tends to be seen not as "a people" in its own right, but as a sub-group, part of a bigger people, which must content itself with regional autonomy.[citation needed] The distinction between language and dialect is thus inevitably made at least as much on a political basis as on a linguistic one, and can lead to great political controversy, or even armed conflict.
The Yiddish linguist Max Weinreich published the expression, A shprakh iz a dialekt mit an armey un flot ("אַ שפּראַך איז אַ דיאַלעקט מיט אַן אַרמײ און פֿלאָט"‎: "A language is a dialect with an army and navy") in YIVO Bleter 25.1, 1945, p. 13. The significance of the political factors in any attempt at answering the question "what is a language?" is great enough to cast doubt on whether any strictly linguistic definition, without a socio-cultural approach, is possible. This is illustrated by the frequency with which the army-navy aphorism is cited.
When talking about the German language, the term German dialects is only used for the traditional regional varieties. That allows them to be distinguished from the regional varieties of modern standard German.
The German dialects show a wide spectrum of variation. Most of them are not mutually intelligible. German dialectology traditionally names the major dialect groups after Germanic tribes from which they were assumed to have descended.[citation needed]
The extent to which the dialects are spoken varies according to a number of factors: In Northern Germany, dialects are less common than in the South. In cities, dialects are less common than on the countryside. In a public environment, dialects are less common than in a familiar environment.
The situation in Switzerland and Liechtenstein is different from the rest of the German-speaking countries. The Swiss German dialects are the default everyday language in virtually every situation, whereas standard German is seldom spoken. Some Swiss German speakers perceive standard German to be a foreign language.
The Low German varieties spoken in Germany are often counted among the German dialects. This reflects the modern situation where they are roofed by standard German. This is different from the situation in the Middle Ages when Low German had strong tendencies towards an ausbau language.
Italy is home to a vast array of native regional minority languages, most of which are Romance-based and have their own local variants. These regional languages are often referred to colloquially or in non-linguistic circles as Italian "dialects," or dialetti (standard Italian for "dialects"). However, the majority of the regional languages in Italy are in fact not actually "dialects" of standard Italian in the strict linguistic sense, as they are not derived from modern standard Italian but instead evolved locally from Vulgar Latin independent of standard Italian, with little to no influence from what is now known as "standard Italian." They are therefore better classified as individual languages rather than "dialects."
In addition to having evolved, for the most part, separately from one another and with distinct individual histories, the Latin-based regional Romance languages of Italy are also better classified as separate languages rather than true "dialects" due to the often high degree in which they lack mutual intelligibility. Though mostly mutually unintelligible, the exact degree to which the regional Italian languages are mutual unintelligible varies, often correlating with geographical distance or geographical barriers between the languages, with some regional Italian languages that are closer in geographical proximity to each other or closer to each other on the dialect continuum being more or less mutually intelligible. For instance, a speaker of purely Eastern Lombard, a language in Northern Italy's Lombardy region that includes the Bergamasque dialect, would have severely limited mutual intelligibility with a purely standard Italian speaker and would be nearly completely unintelligible to a speaker of a pure Sicilian language variant. Due to Eastern Lombard's status as a Gallo-Italic language, an Eastern Lombard speaker may, in fact, have more mutual intelligibility with a Occitan, Catalan, or French speaker than a standard Italian or Sicilian language speaker. Meanwhile, a Sicilian language speaker would have an greater degree of mutual intelligibility with a speaker of the more closely related Neapolitan language, but far less mutual intelligibility with a person speaking Sicilian Gallo-Italic, a language that developed in isolated Lombard emigrant communities on the same island as the Sicilian language.
Modern standard Italian itself is heavily based on the Latin-derived Florentine Tuscan language. The Tuscan-based language that would eventually become modern standard Italian had been used in poetry and literature since at least the 12th century, and it first became widely known in Italy through the works of authors such as Dante Alighieri, Giovanni Boccaccio, Niccolò Machiavelli, and Petrarch. Dante's Florentine-Tuscan literary Italian thus became the language of the literate and upper class in Italy, and it spread throughout the peninsula as the lingua franca among the Italian educated class as well as Italian traveling merchants. The economic prowess and cultural and artistic importance of Tuscany in the Late Middle Ages and the Renaissance further encouraged the diffusion of the Florentine-Tuscan Italian throughout Italy and among the educated and powerful, though local and regional languages remained the main languages of the common people.
During the Risorgimento, proponents of Italian republicanism and Italian nationalism, such as Alessandro Manzoni, stressed the importance of establishing a uniform national language in order to better create an Italian national identity. With the unification of Italy in the 1860s, standard Italian became the official national language of the new Italian state, while the various unofficial regional languages of Italy gradually became regarded as subordinate "dialects" to Italian, increasingly associated negatively with lack of education or provincialism. However, at the time of the Italian Unification, standard Italian still existed mainly as a literary language, and only 2.5% of Italy's population could speak standard Italian.
In the early 20th century, the vast conscription of Italian men from all throughout Italy during World War I is credited with facilitating the diffusion of standard Italian among less educated Italian men, as these men from various regions with various regional languages were forced to communicate with each other in a common tongue while serving in the Italian military. With the eventual spread of the radio and television throughout Italy and the establishment of public education, Italians from all regions were increasingly exposed to standard Italian, while literacy rates among all social classes improved. Today, the majority of Italians are able to speak standard Italian, though many Italians still speak their regional language regularly or as their primary day-to-day language, especially at home with family or when communicating with Italians from the same town or region. However, to some Italians, speaking a regional language, especially in a formal setting or outside of one's region, may carry a stigma or negative connotations associated with being lower class, uneducated, boorish, or overly informal.
Italians in different regions today may also speak regional varieties of standard Italian, or regional Italian dialects, which, unlike the majority of languages of Italy, are actually dialects of standard Italian rather than separate languages. A regional Italian dialect is generally standard Italian that has been heavily influenced or mixed with local or regional native languages and accents.
The languages of Italy are primarily Latin-based Romance languages, with the most widely spoken languages falling within the Italo-Dalmatian language family. This wide category includes:
The Sardinian language is considered to be its own Romance language family, separate not only from standard Italian but also the wider Italo-Dalmatian family, and it includes the Campidanese Sardinian and Logudorese Sardinian variants. However, Gallurese, Sassarese, and Corsican are also spoken in Sardinia, and these languages are considered closely related or derived from the Italian Tuscan language and thus are Italo-Dalmatian languages. Furthermore, the Gallo-Romance language of Ligurian and the Catalan Algherese dialect are also spoken in Sardinia.
The classification of speech varieties as dialects or languages and their relationship to other varieties of speech can be controversial and the verdicts inconsistent. English and Serbo-Croatian illustrate the point. English and Serbo-Croatian each have two major variants (British and American English, and Serbian and Croatian, respectively), along with numerous other varieties. For political reasons, analyzing these varieties as "languages" or "dialects" yields inconsistent results: British and American English, spoken by close political and military allies, are almost universally regarded as dialects of a single language, whereas the standard languages of Serbia and Croatia, which differ from each other to a similar extent as the dialects of English, are being treated by some linguists from the region as distinct languages, largely because the two countries oscillate from being brotherly to being bitter enemies. (The Serbo-Croatian language article deals with this topic much more fully.)
Similar examples abound. Macedonian, although mutually intelligible with Bulgarian, certain dialects of Serbian and to a lesser extent the rest of the South Slavic dialect continuum, is considered by Bulgarian linguists to be a Bulgarian dialect, in contrast with the contemporary international view and the view in the Republic of Macedonia, which regards it as a language in its own right. Nevertheless, before the establishment of a literary standard of Macedonian in 1944, in most sources in and out of Bulgaria before the Second World War, the southern Slavonic dialect continuum covering the area of today's Republic of Macedonia were referred to as Bulgarian dialects.
In Lebanon, a part of the Christian population considers "Lebanese" to be in some sense a distinct language from Arabic and not merely a dialect. During the civil war Christians often used Lebanese Arabic officially, and sporadically used the Latin script to write Lebanese, thus further distinguishing it from Arabic. All Lebanese laws are written in the standard literary form of Arabic, though parliamentary debate may be conducted in Lebanese Arabic.
In Tunisia, Algeria, and Morocco, the Darijas (spoken North African languages) are sometimes considered more different from other Arabic dialects. Officially, North African countries prefer to give preference to the Literary Arabic and conduct much of their political and religious life in it (adherence to Islam), and refrain from declaring each country's specific variety to be a separate language, because Literary Arabic is the liturgical language of Islam and the language of the Islamic sacred book, the Qur'an. Although, especially since the 1960s, the Darijas are occupying an increasing use and influence in the cultural life of these countries. Examples of cultural elements where Darijas' use became dominant include: theatre, film, music, television, advertisement, social media, folk-tale books and companies' names.
In the 19th century, the Tsarist Government of the Russian Empire claimed that Ukrainian was merely a dialect of Russian and not a language on its own. The differences were few and caused by the conquest of western Ukraine by the Polish-Lithuanian Commonwealth. However, the dialects in Ukraine eventually differed substantially from the dialects in Russia.
The German Empire conquered Ukraine during World War I and was planning on either annexing it or installing a puppet king, but was defeated by the Entente, with major involvement by the Ukrainian Bolsheviks. After conquering the rest of Ukraine from the Whites, Ukraine joined the USSR and was enlarged (gaining Crimea and then Eastern Galicia), whence a process of Ukrainization was begun, with encouragement from Moscow.
After World War II, due to Ukrainian collaborationism with the Axis powers in an attempt to gain independence, Moscow changed its policy towards repression of the Ukrainian language.
Today the boundaries of the Ukrainian language to the Russian language are still not drawn clearly, with an intermediate dialect between them, called Surzhyk, developing in Ukraine.
There have been cases of a variety of speech being deliberately reclassified to serve political purposes. One example is Moldovan. In 1996, the Moldovan parliament, citing fears of "Romanian expansionism", rejected a proposal from President Mircea Snegur to change the name of the language to Romanian, and in 2003 a Moldovan–Romanian dictionary was published, purporting to show that the two countries speak different languages. Linguists of the Romanian Academy reacted by declaring that all the Moldovan words were also Romanian words; while in Moldova, the head of the Academy of Sciences of Moldova, Ion Bărbuţă, described the dictionary as a politically motivated "absurdity".
Unlike most languages that use alphabets to indicate the pronunciation, Chinese characters have developed from logograms that do not always give hints to its pronunciation. Although the written characters remained relatively consistent for the last two thousand years, the pronunciation and grammar in different regions has developed to an extent that the varieties of the spoken language are often mutually unintelligible. As a series of migration to the south throughout the history, the regional languages of the south, including Xiang, Wu, Gan, Min, Yue (Cantonese), and Hakka often show traces of Old Chinese or Middle Chinese. From the Ming dynasty onward, Beijing has been the capital of China and the dialect spoken in Beijing has had the most prestige among other varieties. With the founding of the Republic of China, Standard Mandarin was designated as the official language, based on the spoken language of Beijing. Since then, other spoken varieties are regarded as fangyan (dialects). Cantonese is still the most commonly used language in Hong Kong, Macau and among some overseas Chinese communities, whereas Southern Min has been accepted in Taiwan as an important local language along with Mandarin.
Many historical linguists view any speech form as a dialect of the older medium of communication from which it developed.[citation needed] This point of view sees the modern Romance languages as dialects of Latin, modern Greek as a dialect of Ancient Greek, Tok Pisin as a dialect of English, and North Germanic as dialects of Old Norse. This paradigm is not entirely problem-free. It sees genetic relationships as paramount: the "dialects" of a "language" (which itself may be a "dialect" of a yet older language) may or may not be mutually intelligible. Moreover, a parent language may spawn several "dialects" which themselves subdivide any number of times, with some "branches" of the tree changing more rapidly than others.
This can give rise to the situation in which two dialects (defined according to this paradigm) with a somewhat distant genetic relationship are mutually more readily comprehensible than more closely related dialects. In one opinion, this pattern is clearly present among the modern Romance languages, with Italian and Spanish having a high degree of mutual comprehensibility, which neither language shares with French, despite some claiming that both languages are genetically closer to French than to each other:[citation needed] In fact, French-Italian and French-Spanish relative mutual incomprehensibility is due to French having undergone more rapid and more pervasive phonological change than have Spanish and Italian, not to real or imagined distance in genetic relationship. In fact, Italian and French share many more root words in common that do not even appear in Spanish.
For example, the Italian and French words for various foods, some family relationships, and body parts are very similar to each other, yet most of those words are completely different in Spanish. Italian "avere" and "essere" as auxiliaries for forming compound tenses are used similarly to French "avoir" and "être". Spanish only retains "haber" and has done away with "ser" in forming compound tenses. However, when it comes to phonological structures, Italian and Spanish have undergone less change than French, with the result that some native speakers of Italian and Spanish may attain a degree of mutual comprehension that permits extensive communication.[citation needed]
One language, Interlingua, was developed so that the languages of Western civilization would act as its dialects. Drawing from such concepts as the international scientific vocabulary and Standard Average European, linguists[who?] developed a theory that the modern Western languages were actually dialects of a hidden or latent language.[citation needed] Researchers at the International Auxiliary Language Association extracted words and affixes that they considered to be part of Interlingua's vocabulary. In theory, speakers of the Western languages would understand written or spoken Interlingua immediately, without prior study, since their own languages were its dialects. This has often turned out to be true, especially, but not solely, for speakers of the Romance languages and educated speakers of English. Interlingua has also been found to assist in the learning of other languages. In one study, Swedish high school students learning Interlingua were able to translate passages from Spanish, Portuguese, and Italian that students of those languages found too difficult to understand. It should be noted, however, that the vocabulary of Interlingua extends beyond the Western language families.
Mammals include the largest animals on the planet, the rorquals and other large whales, as well as some of the most intelligent, such as elephants, primates, including humans, and cetaceans. The basic body type is a four-legged land-borne animal, but some mammals are adapted for life at sea, in the air, in trees, or on two legs. The largest group of mammals, the placentals, have a placenta, which enables feeding the fetus during gestation. Mammals range in size from the 30–40 mm (1.2–1.6 in) bumblebee bat to the 33-meter (108 ft) blue whale.
The word "mammal" is modern, from the scientific name Mammalia coined by Carl Linnaeus in 1758, derived from the Latin mamma ("teat, pap"). All female mammals nurse their young with milk, which is secreted from special glands, the mammary glands. According to Mammal Species of the World, 5,416 species were known in 2006. These were grouped in 1,229 genera, 153 families and 29 orders. In 2008 the IUCN completed a five-year, 1,700-scientist Global Mammal Assessment for its IUCN Red List, which counted 5,488 accepted species.
Except for the five species of monotremes (egg-laying mammals), all modern mammals give birth to live young. Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders in numbers, are first Rodentia: mice, rats, porcupines, beavers, capybaras, and other gnawing mammals; then Chiroptera: bats; and then Soricomorpha: shrews, moles and solenodons. The next three orders, depending on the biological classification scheme used, are the Primates including the humans; the Cetartiodactyla including the whales and the even-toed hoofed mammals; and the Carnivora, that is, cats, dogs, weasels, bears, seals, and their relatives.
The early synapsid mammalian ancestors were sphenacodont pelycosaurs, a group that produced the non-mammalian Dimetrodon. At the end of the Carboniferous period, this group diverged from the sauropsid line that led to today's reptiles and birds. The line following the stem group Sphenacodontia split-off several diverse groups of non-mammalian synapsids—sometimes referred to as mammal-like reptiles—before giving rise to the proto-mammals (Therapsida) in the early Mesozoic era. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of the non-avian dinosaurs 66 million years ago.
In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and therian mammals (marsupials and placentals) and all descendants of that ancestor. Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century.
If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. Ambondro is more closely related to monotremes than to therian mammals while Amphilestes and Amphitherium are more closely related to the therians; as fossils of all three genera are dated about 167 million years ago in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group. The earliest known synapsid satisfying Kemp's definitions is Tikitherium, dated 225 Ma, so the appearance of mammals in this broader sense can be given this Late Triassic date. In any case, the temporal range of the group extends to the present day.
George Gaylord Simpson's "Principles of Classification and a Classification of Mammals" (AMNH Bulletin v. 85, 1945) was the original source for the taxonomy listed here. Simpson laid out a systematics of mammal origins and relationships that was universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though field work gradually made Simpson's classification outdated, it remained the closest thing to an official classification of mammals.
In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, Classification of Mammals above the Species Level, is the most comprehensive work to date on the systematics, relationships, and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though recent molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History, New York. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia.
Molecular studies based on DNA analysis have suggested new relationships among mammal families over the last few years. Most of these findings have been independently validated by retrotransposon presence/absence data. Classification systems based on molecular studies reveal three major groups or lineages of placental mammals- Afrotheria, Xenarthra, and Boreoeutheria- which diverged from early common ancestors in the Cretaceous. The relationships between these three lineages is contentious, and all three possible different hypotheses have been proposed with respect to which group is basal with respect to other placentals. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra), and Exafroplacentalia (basal Afrotheria). Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
The first amniotes apparently arose in the Late Carboniferous. They descended from earlier reptiliomorph amphibious tetrapods, which lived on land that was already inhabited by insects and other invertebrates as well as by ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which would later include the common ancestor of the mammals; and the sauropsids, which would eventually come to include turtles, lizards, snakes, crocodilians, dinosaurs and birds. Synapsids have a single hole (temporal fenestra) low on each side of the skull.
Therapsids descended from pelycosaurs in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates. They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger temporal fenestrae and incisors which are equal in size. The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very like their pelycosaur ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:
The Permian–Triassic extinction event, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of the carnivores among the therapsids. In the early Triassic, all the medium to large land carnivore niches were taken over by archosaurs which, over an extended period of time (35 million years), came to include the crocodylomorphs, the pterosaurs, and the dinosaurs. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.
The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike Juramaia sinensis, or "Jurassic mother from China", dated to 160 million years ago in the Late Jurassic. A later eutherian, Eomaia, dated to 125 million years ago in the Early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. In particular:
Recent molecular phylogenetic studies suggest that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene. But paleontologists object that no placental fossils have been found from before the end of the Cretaceous. The earliest undisputed fossils of placentals come from the early Paleocene, after the extinction of the dinosaurs. In particular, scientists have recently identified an early Paleocene animal named Protungulatum donnae as one of the first placental mammals. The earliest known ancestor of primates is Archicebus achilles from around 55 million years ago. This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.
The earliest clear evidence of hair or fur is in fossils of Castorocauda, from 164 million years ago in the Middle Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur; it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard Tupinambis has foramina that are almost identical to those found in the nonmammalian cynodont Thrinaxodon. Popular sources, nevertheless, continue to attribute whiskers to Thrinaxodon.
When endothermy first appeared in the evolution of mammals is uncertain. Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. Some of the evidence found so far suggests that Triassic cynodonts had fairly high metabolic rates, but it is not conclusive. For small animals, an insulative covering like fur is necessary for the maintenance of a high and stable body temperature.
Breathing is largely driven by the muscular diaphragm, which divides the thorax from the abdominal cavity, forming a dome with its convexity towards the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the cavity in which the lung is enclosed. Air enters through the oral and nasal cavities; it flows through the larynx, trachea and bronchi and expands the alveoli. Relaxation of the diaphragm has the opposite effect, passively recoiling during normal breathing. During exercise, the abdominal wall contracts, increasing visceral pressure on the diaphragm, thus forcing the air out more quickly and forcefully. The rib cage itself also is able to expand and contract the thoracic cavity to some degree, through the action of other respiratory and accessory respiratory muscles. As a result, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. This type of lung is known as a bellows lung as it resembles a blacksmith's bellows. Mammals take oxygen into their lungs, and discard carbon dioxide.
The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue. Its job is to store lipids, and to provide cushioning and insulation. The thickness of this layer varies widely from species to species.
Mammalian hair, also known as pelage, can vary in color between populations, organisms within a population, and even on the individual organism. Light-dark color variation is common in the mammalian taxa. Sometimes, this color variation is determined by age variation, however, in other cases, it is determined by other factors. Selective pressures, such as ecological interactions with other populations or environmental conditions, often lead to the variation in mammalian coloration. These selective pressures favor certain colors in order to increase survival. Camouflage is thought to be a major selection pressure shaping coloration in mammals, although there is also evidence that sexual selection, communication, and physiological processes may influence the evolution of coloration as well. Camouflage is the most predominant mechanism for color variation, as it aids in the concealment of the organisms from predators or from their prey. Coat color can also be for intraspecies communication such as warning members of their species about predators, indicating health for reproductive purposes, communicating between mother and young, and intimidating predators. Studies have shown that in some cases, differences in female and male coat color could indicate information nutrition and hormone levels, which are important in the mate selection process. One final mechanism for coat color variation is physiological response purposes, such as temperature regulation in tropical or arctic environments. Although much has been observed about color variation, much of the genetic that link coat color to genes is still unknown. The genetic sites where pigmentation genes are found are known to affect phenotype by: 1) altering the spatial distribution of pigmentation of the hairs, and 2) altering the density and distribution of the hairs. Quantitative trait mapping is being used to better understand the distribution of loci responsible for pigmentation variation. However, although the genetic sites are known, there is still much to learn about how these genes are expressed.
Most mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypuses and the echidnas, lay eggs. The monotremes have a sex determination system different from that of most other mammals. In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal. Like marsupials and most other mammals, monotreme young are larval and fetus-like, as the presence of epipubic bones prevents the expansion of the torso, forcing them to produce small young.
Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. A marsupial has a short gestation period, typically shorter than its estrous cycle, and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. This is the plesyomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy. Even non-placental eutherians probably reproduced this way.
In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.
To maintain a high constant body temperature is energy expensive – mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals – this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants. A herbivorous diet includes subtypes such as fruit-eating and grass-eating. An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract, because the proteins, lipids, and minerals found in meat require little in the way of specialized digestion. Plants, on the other hand, contain complex carbohydrates, such as cellulose. The digestive tract of an herbivore is therefore host to bacteria that ferment these substances, and make them available for digestion. The bacteria are either housed in the multichambered stomach or in a large cecum. The size of an animal is also a factor in determining diet type. Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about 18 oz (500 g) are mostly insectivorous because they cannot tolerate the slow, complex digestive process of a herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (those that prey on larger vertebrates) or a slower digestive process (herbivores). Furthermore, mammals that weigh more than 18 oz (500 g) usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).
The deliberate or accidental hybridising of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown in recent times for economic purposes. The number of successful interspecific mammalian hybrids is relatively small, although it has come to be known that there is a significant number of naturally occurring hybrids between forms or regional varieties of a single species.[citation needed] These may form zones of gradation known as clines. Indeed, the distinction between some hitherto distinct species can become clouded once it can be shown that they may not only breed but produce fertile offspring. Some hybrid animals exhibit greater strength and resilience than either parent. This is known as hybrid vigor. The existence of the mule (donkey sire; horse dam) being used widely as a hardy draught animal throughout ancient and modern history is testament to this. Other well known examples are the lion/tiger hybrid, the liger, which is by far the largest big cat and sometimes used in circuses; and cattle hybrids such as between European and Indian domestic cattle or between domestic cattle and American bison, which are used in the meat industry and marketed as Beefalo. There is some speculation that the donkey itself may be the result of an ancient hybridisation between two wild ass species or sub-species. Hybrid animals are normally infertile partly because their parents usually have slightly different numbers of chromosomes, resulting in unpaired chromosomes in their cells, which prevents division of sex cells and the gonads from operating correctly, particularly in males. There are exceptions to this rule, especially if the speciation process was relatively recent or incomplete as is the case with many cattle and dog species. Normally behavior traits, natural hostility, natural ranges and breeding cycle differences maintain the separateness of closely related species and prevent natural hybridisation. However, the widespread disturbances to natural animal behaviours and range caused by human activity, cities, dumping grounds with food, agriculture, fencing, roads and so on do force animals together which would not normally breed. Clear examples exist between the various sub-species of grey wolf, coyote and domestic dog in North America. As many birds and mammals imprint on their mother and immediate family from infancy, a practice used by animal hybridizers is to foster a planned parent in a hybridization program with the same species as the one with which they are planned to mate.
A nonprofit organization (NPO, also known as a non-business entity) is an organization whose purposes are other than making a profit. A nonprofit organization is often dedicated to furthering a particular social cause or advocating for a particular point of view. In economic terms, a nonprofit organization uses its surplus revenues to further achieve its purpose or mission, rather than distributing its surplus income to the organization's shareholders (or equivalents) as profit or dividends. This is known as the distribution constraint. The decision to adopt a nonprofit legal structure is one that will often have taxation implications, particularly where the nonprofit seeks income tax exemption, charitable status and so on.
The nonprofit landscape is highly varied, although many people have come to associate NPOs with charitable organizations. Although charities do comprise an often high profile or visible aspect of the sector, there are many other types of nonprofits. Overall, they tend to be either member-serving or community-serving. Member-serving organizations include mutual societies, cooperatives, trade unions, credit unions, industry associations, sports clubs, retired serviceman's clubs and peak bodies – organizations that benefit a particular group of people i.e. the members of the organization. Typically, community-serving organizations are focused on providing services to the community in general, either globally or locally: organizations delivering human services programs or projects, aid and development programs, medical research, education and health services, and so on. It could be argued many nonprofits sit across both camps, at least in terms of the impact they make. For example, the grassroots support group that provides a lifeline to those with a particular condition or disease could be deemed to be serving both its members (by directly supporting them) and the broader community (through the provision of a helping service for fellow citizens).
Although NPOs are permitted to generate surplus revenues, they must be retained by the organization for its self-preservation, expansion, or plans. NPOs have controlling members or a board of directors. Many have paid staff including management, whereas others employ unpaid volunteers and even executives who work with or without compensation (occasionally nominal). In some countries, where there is a token fee, in general it is used to meet legal requirements for establishing a contract between the executive and the organization.
Some NPOs may also be a charity or service organization; they may be organized as a profit corporation or as a trust, a cooperative, or they exist informally. A very similar type of organization termed a supporting organization operates like a foundation, but they are more complicated to administer, hold more favorable tax status and are restricted in the public charities they support. Their mole is not to be successful in terms of wealth, but in terms of giving value to the groups of people they administer to.
The two major types of nonprofit organization are membership and board-only. A membership organization elects the board and has regular meetings and the power to amend the bylaws. A board-only organization typically has a self-selected board, and a membership whose powers are limited to those delegated to it by the board. A board-only organization's bylaws may even state that the organization does not have any membership, although the organization's literature may refer to its donors or service recipients as "members"; examples of such organizations are Fairvote and the National Organization for the Reform of Marijuana Laws. The Model Nonprofit Corporation Act imposes many complexities and requirements on membership decision-making. Accordingly, many organizations, such as Wikimedia, have formed board-only structures. The National Association of Parliamentarians has generated concerns about the implications of this trend for the future of openness, accountability, and understanding of public concerns in nonprofit organizations. Specifically, they note that nonprofit organizations, unlike business corporations, are not subject to market discipline for products and shareholder discipline of their capital; therefore, without membership control of major decisions such as election of the board, there are few inherent safeguards against abuse. A rebuttal to this might be that as nonprofit organizations grow and seek larger donations, the degree of scrutiny increases, including expectations of audited financial statements. A further rebuttal might be that NPOs are constrained, by their choice of legal structure, from financial benefit as far as distribution of profit to its members/directors is concerned. Beware of board-only organizations- review the board members annual income before donating, such as the Clinton Foundation. Board members who decide what percentage of your donations will increase their personal wealth are rampant in abusing this designation of an NPO, and this is why they attempt to avoid audits and use a double bottom line for taxing.
Canada allows nonprofits to be incorporated or unincorporated. Nonprofits may incorporate either federally, under Part II of the Canada Business Corporations Act or under provincial legislation. Many of the governing Acts for Canadian nonprofits date to the early 1900s, meaning that nonprofit legislation has not kept pace with legislation that governs for-profit corporations; particularly with regards to corporate governance. Federal, and in some provinces (such as Ontario), incorporation is by way of Letters Patent, and any change to the Letters Patent (even a simple name change) requires formal approval by the appropriate government, as do by-law changes. Other provinces (such as Alberta) permit incorporation as of right, by the filing of Articles of Incorporation or Articles of Association.
During 2009, the federal government enacted new legislation repealing the Canada Corporations Act, Part II - the Canada Not-for-Profit Corporations Act. This Act was last amended on 10 October 2011 and the act was current till 4 March 2013. It allows for incorporation as of right, by Articles of Incorporation; does away with the ultra vires doctrine for nonprofits; establishes them as legal persons; and substantially updates the governance provisions for nonprofits. Ontario also overhauled its legislation, adopting the Ontario Not-for-Profit Corporations Act during 2010; pending the outcome of an anticipated election during October 2011,[dated info] the new Act is expected to be in effect as of 1 July 2013.
Canada also permits a variety of charities (including public and private foundations). Charitable status is granted by the Canada Revenue Agency (CRA) upon application by a nonprofit; charities are allowed to issue income tax receipts to donors, must spend a certain percentage of their assets (including cash, investments and fixed assets) and file annual reports in order to maintain their charitable status. In determining whether an organization can become a charity, CRA applies a common law test to its stated objects and activities. These must be:
In South Africa, charities issue a tax certificate when requested by donors which can be used as a tax deduction by the donor. Non Profit Organisations are registered under Companies and Intellectual Property Commission as Nonprofit Companies (NPCs) but may voluntarily register with The Nonprofit Companies Directorate. Trusts are registered by the Master of the High Court. Section 21 Companies are registered under the Company's Act. All are classified as Voluntary Organisations and all must be registered with the South Africa Revenue Services "SARS".[citation needed]
A charity is a nonprofit organisation that meets stricter criteria regarding its purpose and the method in which it makes decisions and reports its finances. For example, a charity is generally not allowed to pay its Trustees. In England and Wales, charities may be registered with the Charity Commission. In Scotland, the Office of the Scottish Charity Regulator serves the same function. Other organizations which are classified as nonprofit organizations elsewhere, such as trade unions, are subject to separate regulations, and are not regarded as "charities" in the technical sense.
After a nonprofit organization has been formed at the state level, the organization may seek recognition of tax exempt status with respect to U.S. federal income tax. That is done typically by applying to the Internal Revenue Service (IRS), although statutory exemptions exist for limited types of nonprofit organizations. The IRS, after reviewing the application to ensure the organization meets the conditions to be recognized as a tax exempt organization (such as the purpose, limitations on spending, and internal safeguards for a charity), may issue an authorization letter to the nonprofit granting it tax exempt status for income tax payment, filing, and deductibility purposes. The exemption does not apply to other Federal taxes such as employment taxes. Additionally, a tax-exempt organization must pay federal tax on income that is unrelated to their exempt purpose. Failure to maintain operations in conformity to the laws may result in an organization losing its tax exempt status.
Individual states and localities offer nonprofits exemptions from other taxes such as sales tax or property tax. Federal tax-exempt status does not guarantee exemption from state and local taxes, and vice versa. These exemptions generally have separate applications and their requirements may differ from the IRS requirements. Furthermore, even a tax exempt organization may be required to file annual financial reports (IRS Form 990) at the state and federal level. A tax exempt organization's 990 forms are required to be made available for public scrutiny. An example of nonprofit organization in the US is Project Vote Smart.
The board of directors has ultimate control over the organization, but typically an executive director is hired. In some cases, the board is elected by a membership, but commonly, the board of directors is self-perpetuating. In these "board-only" organizations, board members nominate new members and vote on their fellow directors nominations. Part VI, section A, question 7a of the Form 990 asks "members, stockholders, or other persons who had the power to elect or appoint one or more members of the governing body?".
Capacity building is an ongoing problem experienced by NPOs for a number of reasons. Most rely on external funding (government funds, grants from charitable foundations, direct donations) to maintain their operations and changes in these sources of revenue may influence the reliability or predictability with which the organization can hire and retain staff, sustain facilities, create programs, or maintain tax-exempt status. For example, a university that sells research to for-profit companies may have tax exemption problems. In addition, unreliable funding, long hours and low pay can result in employee retention problems. During 2009, the US government acknowledged this critical need by the inclusion of the Nonprofit Capacity Building Program in the Serve America Act. Further efforts to quantify the scope of the sector and propose policy solutions for community benefit were included in the Nonprofit Sector and Community Solutions Act, proposed during 2010.
In Australia, nonprofit organisations include trade unions, charitable entities, co-operatives, universities and hospitals, mutual societies, grass-root and support groups, political parties, religious groups, incorporated associations, not-for-profit companies, trusts and more. Furthermore, they operate across a multitude of domains and industries, from health, employment, disability and other human services to local sporting clubs, credit unions and research institutes. A nonprofit organisation in Australia can choose from a number of legal forms depending on the needs and activities of the organisation: co-operative, company limited by guarantee, unincorporated association, incorporated association (by the Associations Incorporation Act 1985) or incorporated association or council (by the Commonwealth Aboriginal Councils and Associations Act 1976). From an academic perspective, social enterprise is for the most part considered a sub-set of the nonprofit sector as typically they too are concerned with a purpose relating to a public good, however these are not bound to adhere to a nonprofit legal structure and many incorporate and operate as for-profit entities.
Many nonprofit organizations find it difficult to create consistent messaging that resonates with their various stakeholders as marketing budgets are minimal or nonexistent. Marketing is in many cases a taboo word that NPOs or others don't like to associate with such community benefit organizations. There are strategic ways in which nonprofits can leverage their access to various community stakeholders to get their name and cause recognized by the public, but it is imperative to have an outreach strategy which includes a financial plan to execute that outreach/marketing strategy, particularly if the organization has plans to rebrand or expand their initiaives.
Resource mismanagement is a particular problem with NPOs because the employees are not accountable to anybody with a direct stake in the organization. For example, an employee may start a new program without disclosing its complete liabilities. The employee may be rewarded for improving the NPO's reputation, making other employees happy, and attracting new donors. Liabilities promised on the full faith and credit of the organization but not recorded anywhere constitute accounting fraud. But even indirect liabilities negatively affect the financial sustainability of the NPO, and the NPO will have financial problems unless strict controls are instated. Some commentators have also argued that receiving significant funding from large for-profit corporations can ultimately alter the NPO's functions.
Competition for employees with the public and private sector is another problem that Nonprofit organizations will inevitably face, particularly for management positions. There are reports of major talent shortages in the nonprofit sector today regarding newly graduated workers, and NPOs have for too long relegated hiring to a secondary priority, which could be why they find themselves in the position many do. While many established NPO's are well-funded and comparative to their public sector competetitors, many more are independent and must be creative with which incentives they use to attract and maintain vibrant personalities. The initial interest for many is the wage and benefits package, though many who have been questioned after leaving an NPO have reported that it was stressful work environments and implacable work that drove them away.
Public and private sector employment has, for the most part, been able to offer more for their employees than most nonprofit agencies throughout history. Either in the form of higher wages, more comprehensive benefit packages, or less tedious work, the public and private sector has enjoyed an advantage in attracting employees over NPOs. Traditionally, the NPO has attracted mission-driven individuals who want to assist their chosen cause. Compounding the issue is that some NPOs do not operate in a manner similar to most businesses, or only seasonally. This leads many young and driven employees to forego NPOs in favor of more stable employment. Today however, Nonprofit organizations are adopting methods used by their competitors and finding new means to retain their employees and attract the best of the newly minted workforce.
It has been mentioned that most nonprofits will never be able to match the pay of the private sector and therefore should focus their attention on benefits packages, incentives and implementing pleasurable work environments. Pleasurable work conditions are ranked as being more preferable than a high salary and implacable work. NPOs are encouraged to pay as much as they are able, and offer a low stress work environment that the employee can associate him or herself positively with. Other incentives that should be implemented are generous vacation allowances or flexible work hours.
In the United States, two of the wealthiest nonprofit organizations are the Bill and Melinda Gates Foundation, which has an endowment of US$38 billion, and the Howard Hughes Medical Institute originally funded by Hughes Aircraft prior to divestiture, which has an endowment of approximately $14.8 billion. Outside the United States, another large NPO is the British Wellcome Trust, which is a "charity" by British usage. See: List of wealthiest foundations. Note that this assessment excludes universities, at least a few of which have assets in the tens of billions of dollars. For example; List of U.S. colleges and universities by endowment.
Some NPOs which are particularly well known, often for the charitable or social nature of their activities performed during a long period of time, include Amnesty International, Oxfam, Rotary International, Kiwanis International, Carnegie Corporation of New York, Nourishing USA, DEMIRA Deutsche Minenräumer (German Mine Clearers), FIDH International Federation for Human Rights, Goodwill Industries, United Way, ACORN (now defunct), Habitat for Humanity, Teach For America, the Red Cross and Red Crescent organizations, UNESCO, IEEE, INCOSE, World Wide Fund for Nature, Heifer International, Translators Without Borders and SOS Children's Villages.
In the traditional domain noted in RFC 1591, .org is for "organizations that didn't fit anywhere else" in the naming system, which implies that it is the proper category for non-commercial organizations if they are not governmental, educational, or one of the other types with a specific TLD. It is not designated specifically for charitable organizations or any specific organizational or tax-law status, however; it encompasses anything that is not classifiable as another category. Currently, no restrictions are enforced on registration of .com or .org, so one can find organizations of all sorts in either of these domains, as well as other top-level domains including newer, more specific ones which may apply to particular sorts of organizations such as .museum for museums or .coop for cooperatives. Organizations might also register by the appropriate country code top-level domain for their country.
Instead of being defined by "non" words, some organizations are suggesting new, positive-sounding terminology to describe the sector. The term "civil society organization" (CSO) has been used by a growing number of organizations, such as the Center for the Study of Global Governance. The term "citizen sector organization" (CSO) has also been advocated to describe the sector – as one of citizens, for citizens – by organizations such as Ashoka: Innovators for the Public. A more broadly applicable term, "Social Benefit Organization" (SBO) has been advocated for by organizations such as MiniDonations. Advocates argue that these terms describe the sector in its own terms, without relying on terminology used for the government or business sectors. However, use of terminology by a nonprofit of self-descriptive language that is not legally compliant risks confusing the public about nonprofit abilities, capabilities and limitations.
Jefferson's metaphor of a wall of separation has been cited repeatedly by the U.S. Supreme Court. In Reynolds v. United States (1879) the Court wrote that Jefferson's comments "may be accepted almost as an authoritative declaration of the scope and effect of the [First] Amendment." In Everson v. Board of Education (1947), Justice Hugo Black wrote: "In the words of Thomas Jefferson, the clause against establishment of religion by law was intended to erect a wall of separation between church and state."
Many early immigrant groups traveled to America to worship freely, particularly after the English Civil War and religious conflict in France and Germany. They included nonconformists like the Puritans, who were Protestant Christians fleeing religious persecution from the Anglican King of England. Despite a common background, the groups' views on religious toleration were mixed. While some such as Roger Williams of Rhode Island and William Penn of Pennsylvania ensured the protection of religious minorities within their colonies, others like the Plymouth Colony and Massachusetts Bay Colony had established churches. The Dutch colony of New Netherland established the Dutch Reformed Church and outlawed all other worship, though enforcement was sparse. Religious conformity was desired partly for financial reasons: the established Church was responsible for poverty relief, putting dissenting churches at a significant disadvantage.
^Note 2: in 1789 the Georgia Constitution was amended as follows: "Article IV. Section 10. No person within this state shall, upon any pretense, be deprived of the inestimable privilege of worshipping God in any manner agreeable to his own conscience, nor be compelled to attend any place of worship contrary to his own faith and judgment; nor shall he ever be obliged to pay tithes, taxes, or any other rate, for the building or repairing any place of worship, or for the maintenance of any minister or ministry, contrary to what he believes to be right, or hath voluntarily engaged to do. No one religious society shall ever be established in this state, in preference to another; nor shall any person be denied the enjoyment of any civil right merely on account of his religious principles."
^Note 5: The North Carolina Constitution of 1776 disestablished the Anglican church, but until 1835 the NC Constitution allowed only Protestants to hold public office. From 1835-1876 it allowed only Christians (including Catholics) to hold public office. Article VI, Section 8 of the current NC Constitution forbids only atheists from holding public office. Such clauses were held by the United States Supreme Court to be unenforceable in the 1961 case of Torcaso v. Watkins, when the court ruled unanimously that such clauses constituted a religious test incompatible with First and Fourteenth Amendment protections.
The Flushing Remonstrance shows support for separation of church and state as early as the mid-17th century, stating their opposition to religious persecution of any sort: "The law of love, peace and liberty in the states extending to Jews, Turks and Egyptians, as they are considered sons of Adam, which is the glory of the outward state of Holland, so love, peace and liberty, extending to all in Christ Jesus, condemns hatred, war and bondage." The document was signed December 27, 1657 by a group of English citizens in America who were affronted by persecution of Quakers and the religious policies of the Governor of New Netherland, Peter Stuyvesant. Stuyvesant had formally banned all religions other than the Dutch Reformed Church from being practiced in the colony, in accordance with the laws of the Dutch Republic. The signers indicated their "desire therefore in this case not to judge lest we be judged, neither to condemn least we be condemned, but rather let every man stand or fall to his own Master." Stuyvesant fined the petitioners and threw them in prison until they recanted. However, John Bowne allowed the Quakers to meet in his home. Bowne was arrested, jailed, and sent to the Netherlands for trial; the Dutch court exonerated Bowne.
There were also opponents to the support of any established church even at the state level. In 1773, Isaac Backus, a prominent Baptist minister in New England, wrote against a state sanctioned religion, saying: "Now who can hear Christ declare, that his kingdom is, not of this world, and yet believe that this blending of church and state together can be pleasing to him?" He also observed that when "church and state are separate, the effects are happy, and they do not at all interfere with each other: but where they have been confounded together, no tongue nor pen can fully describe the mischiefs that have ensued." Thomas Jefferson's influential Virginia Statute for Religious Freedom was enacted in 1786, five years before the Bill of Rights.
The phrase "[A] hedge or wall of separation between the garden of the church and the wilderness of the world" was first used by Baptist theologian Roger Williams, the founder of the colony of Rhode Island, in his 1644 book The Bloody Tenent of Persecution. The phrase was later used by Thomas Jefferson as a description of the First Amendment and its restriction on the legislative branch of the federal government, in an 1802 letter to the Danbury Baptists (a religious minority concerned about the dominant position of the Congregationalist church in Connecticut):
Jefferson and James Madison's conceptions of separation have long been debated. Jefferson refused to issue Proclamations of Thanksgiving sent to him by Congress during his presidency, though he did issue a Thanksgiving and Prayer proclamation as Governor of Virginia. Madison issued four religious proclamations while President, but vetoed two bills on the grounds they violated the first amendment. On the other hand, both Jefferson and Madison attended religious services at the Capitol. Years before the ratification of the Constitution, Madison contended "Because if Religion be exempt from the authority of the Society at large, still less can it be subject to that of the Legislative Body." After retiring from the presidency, Madison wrote of "total separation of the church from the state." " "Strongly guarded as is the separation between Religion & Govt in the Constitution of the United States," Madison wrote, and he declared, "practical distinction between Religion and Civil Government is essential to the purity of both, and as guaranteed by the Constitution of the United States." In a letter to Edward Livingston Madison further expanded, "We are teaching the world the great truth that Govts. do better without Kings & Nobles than with them. The merit will be doubled by the other lesson that Religion flourishes in greater purity, without than with the aid of Govt." Madison's original draft of the Bill of Rights had included provisions binding the States, as well as the Federal Government, from an establishment of religion, but the House did not pass them.[citation needed]
Jefferson's opponents said his position was the destruction and the governmental rejection of Christianity, but this was a caricature. In setting up the University of Virginia, Jefferson encouraged all the separate sects to have preachers of their own, though there was a constitutional ban on the State supporting a Professorship of Divinity, arising from his own Virginia Statute for Religious Freedom. Some have argued that this arrangement was "fully compatible with Jefferson's views on the separation of church and state;" however, others point to Jefferson's support for a scheme in which students at the University would attend religious worship each morning as evidence that his views were not consistent with strict separation. Still other scholars, such as Mark David Hall, attempt to sidestep the whole issue by arguing that American jurisprudence focuses too narrowly on this one Jeffersonian letter while failing to account for other relevant history
Jefferson's letter entered American jurisprudence in the 1878 Mormon polygamy case Reynolds v. U.S., in which the court cited Jefferson and Madison, seeking a legal definition for the word religion. Writing for the majority, Justice Stephen Johnson Field cited Jefferson's Letter to the Danbury Baptists to state that "Congress was deprived of all legislative power over mere opinion, but was left free to reach actions which were in violation of social duties or subversive of good order." Considering this, the court ruled that outlawing polygamy was constitutional.
Jefferson and Madison's approach was not the only one taken in the eighteenth century. Jefferson's Statute of Religious Freedom was drafted in opposition to a bill, chiefly supported by Patrick Henry, which would permit any Virginian to belong to any denomination, but which would require him to belong to some denomination and pay taxes to support it. Similarly, the Constitution of Massachusetts originally provided that "no subject shall be hurt, molested, or restrained, in his person, liberty, or estate, for worshipping God in the manner and season most agreeable to the dictates of his own conscience... provided he doth not disturb the public peace, or obstruct others in their religious worship," (Article II) but also that:
The Duke of York had required that every community in his new lands of New York and New Jersey support some church, but this was more often Dutch Reformed, Quaker or Presbyterian, than Anglican. Some chose to support more than one church. He also ordained that the tax-payers were free, having paid his local tax, to choose their own church. The terms for the surrender of New Amsterdam had provided that the Dutch would have liberty of conscience, and the Duke, as an openly divine-right Catholic, was no friend of Anglicanism. The first Anglican minister in New Jersey arrived in 1698, though Anglicanism was more popular in New York.
The original charter of the Province of East Jersey had restricted membership in the Assembly to Christians; the Duke of York was fervently Catholic, and the proprietors of Perth Amboy, New Jersey were Scottish Catholic peers. The Province of West Jersey had declared, in 1681, that there should be no religious test for office. An oath had also been imposed on the militia during the French and Indian War requiring them to abjure the pretensions of the Pope, which may or may not have been applied during the Revolution. That law was replaced by 1799.
The first amendment to the US Constitution states "Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof" The two parts, known as the "establishment clause" and the "free exercise clause" respectively, form the textual basis for the Supreme Court's interpretations of the "separation of church and state" doctrine. Three central concepts were derived from the 1st Amendment which became America's doctrine for church-state separation: no coercion in religious matters, no expectation to support a religion against one's will, and religious liberty encompasses all religions. In sum, citizens are free to embrace or reject a faith, any support for religion - financial or physical - must be voluntary, and all religions are equal in the eyes of the law with no special preference or favoritism.
Some legal scholars, such as John Baker of LSU, theorize that Madison's initial proposed language—that Congress should make no law regarding the establishment of a "national religion"—was rejected by the House, in favor of the more general "religion" in an effort to appease the Anti-Federalists. To both the Anti-Federalists and the Federalists, the very word "national" was a cause for alarm because of the experience under the British crown. During the debate over the establishment clause, Rep. Elbridge Gerry of Massachusetts took issue with Madison's language regarding whether the government was a national or federal government (in which the states retained their individual sovereignty), which Baker suggests compelled Madison to withdraw his language from the debate.
Others, such as Rep. Roger Sherman of Connecticut, believed the clause was unnecessary because the original Constitution only gave Congress stated powers, which did not include establishing a national religion. Anti-Federalists such as Rep. Thomas Tucker of South Carolina moved to strike the establishment clause completely because it could preempt the religious clauses in the state constitutions. However, the Anti-Federalists were unsuccessful in persuading the House of Representatives to drop the clause from the first amendment.
The Fourteenth Amendment to the United States Constitution (Amendment XIV) is one of the post-Civil War amendments, intended to secure rights for former slaves. It includes the due process and equal protection clauses among others. The amendment introduces the concept of incorporation of all relevant federal rights against the states. While it has not been fully implemented, the doctrine of incorporation has been used to ensure, through the Due Process Clause and Privileges and Immunities Clause, the application of most of the rights enumerated in the Bill of Rights to the states.
The incorporation of the First Amendment establishment clause in the landmark case of Everson v. Board of Education has impacted the subsequent interpretation of the separation of church and state in regard to the state governments. Although upholding the state law in that case, which provided for public busing to private religious schools, the Supreme Court held that the First Amendment establishment clause was fully applicable to the state governments. A more recent case involving the application of this principle against the states was Board of Education of Kiryas Joel Village School District v. Grumet (1994).
Jefferson's concept of "separation of church and state" first became a part of Establishment Clause jurisprudence in Reynolds v. U.S., 98 U.S. 145 (1878). In that case, the court examined the history of religious liberty in the US, determining that while the constitution guarantees religious freedom, "The word 'religion' is not defined in the Constitution. We must go elsewhere, therefore, to ascertain its meaning, and nowhere more appropriately, we think, than to the history of the times in the midst of which the provision was adopted." The court found that the leaders in advocating and formulating the constitutional guarantee of religious liberty were James Madison and Thomas Jefferson. Quoting the "separation" paragraph from Jefferson's letter to the Danbury Baptists, the court concluded that, "coming as this does from an acknowledged leader of the advocates of the measure, it may be accepted almost as an authoritative declaration of the scope and effect of the amendment thus secured."
The centrality of the "separation" concept to the Religion Clauses of the Constitution was made explicit in Everson v. Board of Education, 330 U.S. 1 (1947), a case dealing with a New Jersey law that allowed government funds to pay for transportation of students to both public and Catholic schools. This was the first case in which the court applied the Establishment Clause to the laws of a state, having interpreted the due process clause of the Fourteenth Amendment as applying the Bill of Rights to the states as well as the federal legislature. Citing Jefferson, the court concluded that "The First Amendment has erected a wall between church and state. That wall must be kept high and impregnable. We could not approve the slightest breach."
While the decision (with four dissents) ultimately upheld the state law allowing the funding of transportation of students to religious schools, the majority opinion (by Justice Hugo Black) and the dissenting opinions (by Justice Wiley Blount Rutledge and Justice Robert H. Jackson) each explicitly stated that the Constitution has erected a "wall between church and state" or a "separation of Church from State": their disagreement was limited to whether this case of state funding of transportation to religious schools breached that wall. Rutledge, on behalf of the four dissenting justices, took the position that the majority had indeed permitted a violation of the wall of separation in this case: "Neither so high nor so impregnable today as yesterday is the wall raised between church and state by Virginia's great statute of religious freedom and the First Amendment, now made applicable to all the states by the Fourteenth." Writing separately, Justice Jackson argued that "[T]here are no good grounds upon which to support the present legislation. In fact, the undertones of the opinion, advocating complete and uncompromising separation of Church from State, seem utterly discordant with its conclusion yielding support to their commingling in educational matters."
In 1962, the Supreme Court addressed the issue of officially-sponsored prayer or religious recitations in public schools. In Engel v. Vitale, 370 U.S. 421 (1962), the Court, by a vote of 6-1, determined it unconstitutional for state officials to compose an official school prayer and require its recitation in public schools, even when the prayer is non-denominational and students may excuse themselves from participation. (The prayer required by the New York State Board of Regents prior to the Court's decision consisted of: "Almighty God, we acknowledge our dependence upon Thee, and we beg Thy blessings upon us, our parents, our teachers, and our country. Amen.") As the Court stated:
The court noted that it "is a matter of history that this very practice of establishing governmentally composed prayers for religious services was one of the reasons which caused many of our early colonists to leave England and seek religious freedom in America." The lone dissenter, Justice Potter Stewart, objected to the court's embrace of the "wall of separation" metaphor: "I think that the Court's task, in this as in all areas of constitutional adjudication, is not responsibly aided by the uncritical invocation of metaphors like the "wall of separation," a phrase nowhere to be found in the Constitution."
In Epperson v. Arkansas, 393 U.S. 97 (1968), the Supreme Court considered an Arkansas law that made it a crime "to teach the theory or doctrine that mankind ascended or descended from a lower order of animals," or "to adopt or use in any such institution a textbook that teaches" this theory in any school or university that received public funds. The court's opinion, written by Justice Abe Fortas, ruled that the Arkansas law violated "the constitutional prohibition of state laws respecting an establishment of religion or prohibiting the free exercise thereof. The overriding fact is that Arkansas' law selects from the body of knowledge a particular segment which it proscribes for the sole reason that it is deemed to conflict with a particular religious doctrine; that is, with a particular interpretation of the Book of Genesis by a particular religious group." The court held that the Establishment Clause prohibits the state from advancing any religion, and that "[T]he state has no legitimate interest in protecting any or all religions from views distasteful to them." 
In Lemon v. Kurtzman, 403 U.S. 602 (1971), the court determined that a Pennsylvania state policy of reimbursing the salaries and related costs of teachers of secular subjects in private religious schools violated the Establishment Clause. The court's decision argued that the separation of church and state could never be absolute: "Our prior holdings do not call for total separation between church and state; total separation is not possible in an absolute sense. Some relationship between government and religious organizations is inevitable," the court wrote. "Judicial caveats against entanglement must recognize that the line of separation, far from being a "wall," is a blurred, indistinct, and variable barrier depending on all the circumstances of a particular relationship."
Subsequent to this decision, the Supreme Court has applied a three-pronged test to determine whether government action comports with the Establishment Clause, known as the "Lemon Test". First, the law or policy must have been adopted with a neutral or non-religious purpose. Second, the principle or primary effect must be one that neither advances nor inhibits religion. Third, the statute or policy must not result in an "excessive entanglement" of government with religion. (The decision in Lemon v. Kurtzman hinged upon the conclusion that the government benefits were flowing disproportionately to Catholic schools, and that Catholic schools were an integral component of the Catholic Church's religious mission, thus the policy involved the state in an "excessive entanglement" with religion.) Failure to meet any of these criteria is a proof that the statute or policy in question violates the Establishment Clause.
In 2002, a three judge panel on the Ninth Circuit Court of Appeals held that classroom recitation of the Pledge of Allegiance in a California public school was unconstitutional, even when students were not compelled to recite it, due to the inclusion of the phrase "under God." In reaction to the case, Elk Grove Unified School District v. Newdow, both houses of Congress passed measures reaffirming their support for the pledge, and condemning the panel's ruling. The case was appealed to the Supreme Court, where the case was ultimately overturned in June 2004, solely on procedural grounds not related to the substantive constitutional issue. Rather, a five-justice majority held that Newdow, a non-custodial parent suing on behalf of his daughter, lacked standing to sue.
On December 20, 2005, the United States Court of Appeals for the Sixth Circuit ruled in the case of ACLU v. Mercer County that the continued display of the Ten Commandments as part of a larger display on American legal traditions in a Kentucky courthouse was allowed, because the purpose of the display (educating the public on American legal traditions) was secular in nature. In ruling on the Mount Soledad cross controversy on May 3, 2006, however, a federal judge ruled that the cross on public property on Mount Soledad must be removed.
In what will be the case is Town of Greece v. Galloway, 12-696, the Supreme Court agreed to hear a case regarding whether prayers at town meetings, which are allowed, must allow various faiths to lead prayer, or whether the prayers can be predominately Christian. On May 5, 2014, the U.S. Supreme Court ruled 5-4 in favor of the Town of Greece by holding that the U.S. Constitution not only allows for prayer at government meetings, but also for sectarian prayers like predominately Christian prayers.
Some scholars and organizations disagree with the notion of "separation of church and state", or the way the Supreme Court has interpreted the constitutional limitation on religious establishment. Such critics generally argue that the phrase misrepresents the textual requirements of the Constitution, while noting that many aspects of church and state were intermingled at the time the Constitution was ratified. These critics argue that the prevalent degree of separation of church and state could not have been intended by the constitutional framers. Some of the intermingling between church and state include religious references in official contexts, and such other founding documents as the United States Declaration of Independence, which references the idea of a "Creator" and "Nature's God", though these references did not ultimately appear in the Constitution nor do they mention any particular religious view of a "Creator" or "Nature's God."
These critics of the modern separation of church and state also note the official establishment of religion in several of the states at the time of ratification, to suggest that the modern incorporation of the Establishment Clause as to state governments goes against the original constitutional intent.[citation needed] The issue is complex, however, as the incorporation ultimately bases on the passage of the 14th Amendment in 1868, at which point the first amendment's application to the state government was recognized. Many of these constitutional debates relate to the competing interpretive theories of originalism versus modern, progressivist theories such as the doctrine of the Living Constitution. Other debates center on the principle of the law of the land in America being defined not just by the Constitution's Supremacy Clause, but also by legal precedence, making an accurate reading of the Constitution subject to the mores and values of a given era, and rendering the concept of historical revisionism irrelevant when discussing the Constitution.
The "religious test" clause has been interpreted to cover both elected officials and appointed ones, career civil servants as well as political appointees. Religious beliefs or the lack of them have therefore not been permissible tests or qualifications with regard to federal employees since the ratification of the Constitution. Seven states, however, have language included in their Bill of Rights, Declaration of Rights, or in the body of their constitutions that require state office-holders to have particular religious beliefs, though some of these have been successfully challenged in court. These states are Texas, Massachusetts, Maryland, North Carolina, Pennsylvania, South Carolina, and Tennessee.
The required beliefs of these clauses include belief in a Supreme Being and belief in a future state of rewards and punishments. (Tennessee Constitution Article IX, Section 2 is one such example.) Some of these same states specify that the oath of office include the words "so help me God." In some cases these beliefs (or oaths) were historically required of jurors and witnesses in court. At one time, such restrictions were allowed under the doctrine of states' rights; today they are deemed to be in violation of the federal First Amendment, as applied to the states via the 14th amendment, and hence unconstitutional and unenforceable.
Relaxed zoning rules and special parking privileges for churches, the tax-free status of church property, the fact that Christmas is a federal holiday, etc., have also been questioned, but have been considered examples of the governmental prerogative in deciding practical and beneficial arrangements for the society. The national motto "In God We Trust" has been challenged as a violation, but the Supreme Court has ruled that ceremonial deism is not religious in nature. A circuit court ruling affirmed Ohio's right to use as its motto a passage from the Bible, "With God, all things are possible", because it displayed no preference for a particular religion.
Jeffries and Ryan (2001) argue that the modern concept of separation of church and state dates from the mid-twentieth century rulings of the Supreme Court. The central point, they argue, was a constitutional ban against aid to religious schools, followed by a later ban on religious observance in public education. Jeffries and Ryan argue that these two propositions—that public aid should not go to religious schools and that public schools should not be religious—make up the separationist position of the modern Establishment Clause.
Jeffries and Ryan argue that no-aid position drew support from a coalition of separationist opinion. Most important was "the pervasive secularism that came to dominate American public life," which sought to confine religion to a private sphere. Further, the ban against government aid to religious schools was supported before 1970 by most Protestants (and most Jews), who opposed aid to religious schools, which were mostly Catholic at the time. After 1980, however, anti-Catholic sentiment has diminished among mainline Protestants, and the crucial coalition of public secularists and Protestant churches has collapsed. While mainline Protestant denominations are more inclined towards strict separation of church and state, much evangelical opinion has now largely deserted that position. As a consequence, strict separationism is opposed today by members of many Protestant faiths, even perhaps eclipsing the opposition of Roman Catholics.[citation needed]
Critics of the modern concept of the "separation of church and state" argue that it is untethered to anything in the text of the constitution and is contrary to the conception of the phrase as the Founding Fathers understood it. Philip Hamburger, Columbia Law school professor and prominent critic of the modern understanding of the concept, maintains that the modern concept, which deviates from the constitutional establishment clause jurisprudence, is rooted in American anti-Catholicism and Nativism.[citation needed] Briefs before the Supreme Court, including by the U.S. government, have argued that some state constitutional amendments relating to the modern conception of separation of church and state (Blaine Amendments) were motivated by and intended to enact anti-Catholicism.
J. Brent Walker, Executive Director of the Baptist Joint Committee, responded to Hamburger's claims noting; "The fact that the separation of church and state has been supported by some who exhibited an anti-Catholic animus or a secularist bent does not impugn the validity of the principle. Champions of religious liberty have argued for the separation of church and state for reasons having nothing to do with anti-Catholicism or desire for a secular culture. Of course, separationists have opposed the Catholic Church when it has sought to tap into the public till to support its parochial schools or to argue for on-campus released time in the public schools. But that principled debate on the issues does not support a charge of religious bigotry"
Steven Waldman notes that; "The evangelicals provided the political muscle for the efforts of Madison and Jefferson, not merely because they wanted to block official churches but because they wanted to keep the spiritual and secular worlds apart." "Religious freedom resulted from an alliance of unlikely partners," writes the historian Frank Lambert in his book The Founding Fathers and the Place of Religion in America. "New Light evangelicals such as Isaac Bachus and John Leland joined forces with Deists and skeptics such as James Madison and Thomas Jefferson to fight for a complete separation of church and state."
Robert N. Bellah has in his writings that although the separation of church and state is grounded firmly in the constitution of the United States, this does not mean that there is no religious dimension in the political society of the United States. He used the term "Civil Religion" to describe the specific relation between politics and religion in the United States. His 1967 article analyzes the inaugural speech of John F. Kennedy: "Considering the separation of church and state, how is a president justified in using the word 'God' at all? The answer is that the separation of church and state has not denied the political realm a religious dimension."
Robert S. Wood has argued that the United States is a model for the world in terms of how a separation of church and state—no state-run or state-established church—is good for both the church and the state, allowing a variety of religions to flourish. Speaking at the Toronto-based Center for New Religions, Wood said that the freedom of conscience and assembly allowed under such a system has led to a "remarkable religiosity" in the United States that isn't present in other industrialized nations. Wood believes that the U.S. operates on "a sort of civic religion," which includes a generally-shared belief in a creator who "expects better of us." Beyond that, individuals are free to decide how they want to believe and fill in their own creeds and express their conscience. He calls this approach the "genius of religious sentiment in the United States."
Grapes are a type of fruit that grow in clusters of 15 to 300, and can be crimson, black, dark blue, yellow, green, orange, and pink. "White" grapes are actually green in color, and are evolutionarily derived from the purple grape. Mutations in two regulatory genes of white grapes turn off production of anthocyanins, which are responsible for the color of purple grapes. Anthocyanins and other pigment chemicals of the larger family of polyphenols in purple grapes are responsible for the varying shades of purple in red wines. Grapes are typically an ellipsoid shape resembling a prolate spheroid.
The cultivation of the domesticated grape began 6,000–8,000 years ago in the Near East. Yeast, one of the earliest domesticated microorganisms, occurs naturally on the skins of grapes, leading to the innovation of alcoholic drinks such as wine. The earliest archeological evidence for a dominant position of wine-making in human culture dates from 8,000 years ago in Georgia. The oldest winery was found in Armenia, dating to around 4000 BC.[citation needed] By the 9th century AD the city of Shiraz was known to produce some of the finest wines in the Middle East. Thus it has been proposed that Syrah red wine is named after Shiraz, a city in Persia where the grape was used to make Shirazi wine.[citation needed] Ancient Egyptian hieroglyphics record the cultivation of purple grapes,[citation needed] and history attests to the ancient Greeks, Phoenicians, and Romans growing purple grapes for both eating and wine production[citation needed]. The growing of grapes would later spread to other regions in Europe, as well as North Africa, and eventually in North America.
Comparing diets among Western countries, researchers have discovered that although the French tend to eat higher levels of animal fat, the incidence of heart disease remains low in France. This phenomenon has been termed the French paradox, and is thought to occur from protective benefits of regularly consuming red wine. Apart from potential benefits of alcohol itself, including reduced platelet aggregation and vasodilation, polyphenols (e.g., resveratrol) mainly in the grape skin provide other suspected health benefits, such as:
Grape juice is obtained from crushing and blending grapes into a liquid. The juice is often sold in stores or fermented and made into wine, brandy, or vinegar. Grape juice that has been pasteurized, removing any naturally occurring yeast, will not ferment if kept sterile, and thus contains no alcohol. In the wine industry, grape juice that contains 7–23% of pulp, skins, stems and seeds is often referred to as "must". In North America, the most common grape juice is purple and made from Concord grapes, while white grape juice is commonly made from Niagara grapes, both of which are varieties of native American grapes, a different species from European wine grapes. In California, Sultana (known there as Thompson Seedless) grapes are sometimes diverted from the raisin or table market to produce white juice.
Red wine may offer health benefits more so than white because potentially beneficial compounds are present in grape skin, and only red wine is fermented with skins. The amount of fermentation time a wine spends in contact with grape skins is an important determinant of its resveratrol content. Ordinary non-muscadine red wine contains between 0.2 and 5.8 mg/L, depending on the grape variety, because it is fermented with the skins, allowing the wine to absorb the resveratrol. By contrast, a white wine contains lower phenolic contents because it is fermented after removal of skins.
Commercially cultivated grapes can usually be classified as either table or wine grapes, based on their intended method of consumption: eaten raw (table grapes) or used to make wine (wine grapes). While almost all of them belong to the same species, Vitis vinifera, table and wine grapes have significant differences, brought about through selective breeding. Table grape cultivars tend to have large, seedless fruit (see below) with relatively thin skin. Wine grapes are smaller, usually seeded, and have relatively thick skins (a desirable characteristic in winemaking, since much of the aroma in wine comes from the skin). Wine grapes also tend to be very sweet: they are harvested at the time when their juice is approximately 24% sugar by weight. By comparison, commercially produced "100% grape juice", made from table grapes, is usually around 15% sugar by weight.
In the Bible, grapes are first mentioned when Noah grows them on his farm (Genesis 9:20–21). Instructions concerning wine are given in the book of Proverbs and in the book of Isaiah, such as in Proverbs 20:1 and Isaiah 5:20–25. Deuteronomy 18:3–5,14:22–27,16:13–15 tell of the use of wine during Jewish feasts. Grapes were also significant to both the Greeks and Romans, and their god of agriculture, Dionysus, was linked to grapes and wine, being frequently portrayed with grape leaves on his head. Grapes are especially significant for Christians, who since the Early Church have used wine in their celebration of the Eucharist. Views on the significance of the wine vary between denominations. In Christian art, grapes often represent the blood of Christ, such as the grape leaves in Caravaggio’s John the Baptist.
There are several sources of the seedlessness trait, and essentially all commercial cultivators get it from one of three sources: Thompson Seedless, Russian Seedless, and Black Monukka, all being cultivars of Vitis vinifera. There are currently more than a dozen varieties of seedless grapes. Several, such as Einset Seedless, Benjamin Gunnels's Prime seedless grapes, Reliance, and Venus, have been specifically cultivated for hardiness and quality in the relatively cold climates of northeastern United States and southern Ontario.
Anthocyanins tend to be the main polyphenolics in purple grapes whereas flavan-3-ols (i.e. catechins) are the more abundant phenolic in white varieties. Total phenolic content, a laboratory index of antioxidant strength, is higher in purple varieties due almost entirely to anthocyanin density in purple grape skin compared to absence of anthocyanins in white grape skin. It is these anthocyanins that are attracting the efforts of scientists to define their properties for human health. Phenolic content of grape skin varies with cultivar, soil composition, climate, geographic origin, and cultivation practices or exposure to diseases, such as fungal infections.
The Catholic Church uses wine in the celebration of the Eucharist because it is part of the tradition passed down through the ages starting with Jesus Christ at the Last Supper, where Catholics believe the consecrated bread and wine literally become the body and blood of Jesus Christ, a dogma known as transubstantiation. Wine is used (not grape juice) both due to its strong Scriptural roots, and also to follow the tradition set by the early Christian Church. The Code of Canon Law of the Catholic Church (1983), Canon 924 says that the wine used must be natural, made from grapes of the vine, and not corrupt. In some circumstances, a priest may obtain special permission to use grape juice for the consecration, however this is extremely rare and typically requires sufficient impetus to warrant such a dispensation, such as personal health of the priest.
Before forming Queen, Brian May and Roger Taylor had played together in a band named Smile. Freddie Mercury (then known by his birth name of Farrokh "Freddie" Bulsara) was a fan of Smile and encouraged them to experiment with more elaborate stage and recording techniques. Mercury joined the band in 1970, suggested "Queen" as a new band name, and adopted his familiar stage name. John Deacon was recruited prior to recording their eponymous debut album in 1973. Queen first charted in the UK with their second album, Queen II, in 1974, but it was the release of Sheer Heart Attack later that year and A Night at the Opera in 1975 which brought them international success. The latter featured "Bohemian Rhapsody", which stayed at number one in the UK for nine weeks and popularised the music video. Their 1977 album, News of the World, contained "We Will Rock You" and "We Are the Champions", which have become anthems at sporting events. By the early 1980s, Queen were one of the biggest stadium rock bands in the world. Their performance at 1985's Live Aid is ranked among the greatest in rock history by various music publications, with a 2005 industry poll ranking it the best. In 1991, Mercury died of bronchopneumonia, a complication of AIDS, and Deacon retired in 1997. Since then, May and Taylor have occasionally performed together, including with Paul Rodgers (2004–09) and with Adam Lambert (since 2011). In November 2014, Queen released a new album, Queen Forever, featuring vocals from the late Mercury.
While attending Ealing Art College, Tim Staffell became friends with Farrokh Bulsara, a fellow student who had assumed the English name of Freddie. Bulsara felt that he and the band had the same tastes and soon became a keen fan of Smile. In late 1970, after Staffell left to join the band Humpy Bong, the remaining Smile members, encouraged by Bulsara, changed their name to "Queen" and continued working together. When asked about the name, Bulsara explained, "I thought up the name Queen. It's just a name, but it's very regal obviously, and it sounds splendid. It's a strong name, very universal and immediate. It had a lot of visual potential and was open to all sorts of interpretations. I was certainly aware of gay connotations, but that was just one facet of it."
The band had a number of bass players during this period who did not fit with the band's chemistry. It was not until February 1971 that they settled on John Deacon and began to rehearse for their first album. They recorded four of their own songs, "Liar", "Keep Yourself Alive", "The Night Comes Down" and "Jesus", for a demo tape; no record companies were interested. It was also around this time Freddie changed his surname to "Mercury", inspired by the line "Mother Mercury, look what they've done to me" in the song "My Fairy King". On 2 July 1971, Queen played their first show in the classic line-up of Mercury, May, Taylor and Deacon at a Surrey college outside London.
Having attended art college, Mercury also designed Queen's logo, called the Queen crest, shortly before the release of the band's first album. The logo combines the zodiac signs of all four members: two lions for Leo (Deacon and Taylor), a crab for Cancer (May), and two fairies for Virgo (Mercury). The lions embrace a stylised letter Q, the crab rests atop the letter with flames rising directly above it, and the fairies are each sheltering below a lion. There is also a crown inside the Q and the whole logo is over-shadowed by an enormous phoenix. The whole symbol bears a passing resemblance to the Royal coat of arms of the United Kingdom, particularly with the lion supporters. The original logo, as found on the reverse-side of the first album cover, was a simple line drawing but more intricate colour versions were used on later sleeves.
In 1972, Queen entered discussions with Trident Studios after being spotted at De La Lane Studios by John Anthony and after discussions were offered a management deal by Norman Sheffield under Neptune Productions, a subsidiary of Trident to manage the band and enable them to use the facilities at Trident to record new material whilst the management search for a record label to sign Queen. This suited both parties at the time as Trident were expanding into management and Queen under the deal were able to make use of the hi-tech recording facilities shared by bands at the time such as the Beatles and Elton John to produce new material. However, Trident found it difficult to find a label for a band bearing a name with such connotation during the early 1970s.
In July 1973, Queen finally under a Trident/EMI deal released their eponymous debut album, an effort influenced by the heavy metal and progressive rock of the day. The album was received well by critics; Gordon Fletcher of Rolling Stone said "their debut album is superb", and Chicago's Daily Herald called it an "above average debut". It drew little mainstream attention, and the lead single "Keep Yourself Alive", a Brian May composition, sold poorly. Retrospectively, "Keep Yourself Alive" is cited as the highlight of the album, and in 2008 Rolling Stone ranked it 31st in the "100 Greatest Guitar Songs of All Time", describing it as "an entire album's worth of riffs crammed into a single song". The album was certified gold in the UK and the US.
The group's second LP, Queen II, was released in 1974, and features rock photographer Mick Rock's iconic image of the band on the cover. This image would be used as the basis for the 1975 "Bohemian Rhapsody" music video production. The album reached number five on the British album chart and became the first Queen album to chart in the UK. The Freddie Mercury-written lead single "Seven Seas of Rhye" reached number ten in the UK, giving the band their first hit. The album is the first real testament to the band's distinctive layered sound, and features long complex instrumental passages, fantasy-themed lyrics, and musical virtuosity. Aside from its only single, the album also included the song "The March of the Black Queen", a six-minute epic which lacks a chorus. The Daily Vault described the number as "menacing". Critical reaction was mixed; the Winnipeg Free Press, while praising the band's debut album, described Queen II as a "over-produced monstrosity". Allmusic has described the album as a favourite among the band's hardcore fans, and it is the first of three Queen albums to feature in the book 1001 Albums You Must Hear Before You Die.
After the band's six-night stand at New York's Uris Theatre in May 1974, Brian May collapsed and was diagnosed as having hepatitis. While recuperating, May was initially absent when the band started work on their third album, but he returned midway through the recording process. Released in 1974, Sheer Heart Attack reached number two in the United Kingdom, sold well throughout Europe, and went gold in the United States. It gave the band their first real experience of international success, and was a hit on both sides of the Atlantic. The album experimented with a variety of musical genres, including British music hall, heavy metal, ballads, ragtime, and Caribbean. At this point, Queen started to move away from the progressive tendencies of their first two releases into a more radio-friendly, song-orientated style. Sheer Heart Attack introduced new sound and melody patterns that would be refined on their next album, A Night at the Opera.
The single "Killer Queen" from Sheer Heart Attack reached number two on the British charts, and became their first US hit, reaching number 12 on the Billboard Hot 100. It combines camp, vaudeville, and British music hall with May's guitar virtuosity. The album's second single, "Now I'm Here", a more traditional hard rock composition, was a number eleven hit in Britain, while the high speed rocker "Stone Cold Crazy" featuring May's uptempo riffs is a precursor to speed metal. In recent years, the album has received acclaim from music publications: In 2006, Classic Rock ranked it number 28 in "The 100 Greatest British Rock Albums Ever", and in 2007, Mojo ranked it No.88 in "The 100 Records That Changed the World". It is also the second of three Queen albums to feature in the book 1001 Albums You Must Hear Before You Die.
In 1975, the band left for a world tour with each member in Zandra Rhodes-created costumes and accompanied with banks of lights and effects. They toured the US as headliners, and played in Canada for the first time. In September, after an acromonious split with Trident, the band negotiated themselves out of their Trident Studios contract and searched for new management. One of the options they considered was an offer from Led Zeppelin's manager, Peter Grant. Grant wanted them to sign with Led Zeppelin's own production company, Swan Song Records. The band found the contract unacceptable and instead contacted Elton John's manager, John Reid, who accepted the position.
In late 1975, Queen recorded and released A Night at the Opera, taking its name from the popular Marx Brothers movie. At the time, it was the most expensive album ever produced. Like its predecessor, the album features diverse musical styles and experimentation with stereo sound. In "The Prophet's Song", an eight-minute epic, the middle section is a canon, with simple phrases layered to create a full-choral sound. The Mercury penned ballad, "Love of My Life", featured a harp and overdubbed vocal harmonies. The album was very successful in Britain, and went triple platinum in the United States. The British public voted it the 13th greatest album of all time in a 2004 Channel 4 poll. It has also ranked highly in international polls; in a worldwide Guinness poll, it was voted the 19th greatest of all time, while an ABC poll saw the Australian public vote it the 28th greatest of all time. A Night at the Opera has frequently appeared in "greatest albums" lists reflecting the opinions of critics. Among other accolades, it was ranked number 16 in Q Magazine's "The 50 Best British Albums Ever" in 2004, and number 11 in Rolling Stone's "The 100 Greatest Albums of All Time" as featured in their Mexican edition in 2004. It was also placed at No. 230 on Rolling Stone magazine's list of "The 500 Greatest Albums of All Time" in 2003. A Night at the Opera is the third and final Queen album to be featured in the book 1001 Albums You Must Hear Before You Die.
The album also featured the hit single "Bohemian Rhapsody", which was number one in the UK for nine weeks. Mercury's close friend and advisor, Capital London radio DJ Kenny Everett, played a pivotal role in giving the single exposure. It is the third-best-selling single of all time in the UK, surpassed only by Band Aid's "Do They Know It's Christmas?" and Elton John's "Candle in the Wind 1997", and is the best-selling commercial single in the UK. It also reached number nine in the United States (a 1992 re-release reached number two on the Billboard Hot 100 for five weeks). It is the only single ever to sell a million copies on two separate occasions, and became the Christmas number one twice in the UK, the only single ever to do so. "Bohemian Rhapsody" has been voted numerous times the greatest song of all time. The band decided to make a video to help go with the single and hired Trilion, a subsidiary of the former management company Trident Studios, using new technology to create the video; the result is generally considered to have been the first "true" music video ever produced, and popularised the medium. The album's first track "Death on Two Legs" is said to be written by Mercury about Norman Sheffield and the former management at Trident who helped make the video so popular. Although other bands, including the Beatles, had made short promotional films or videos of songs prior to this, generally, those were specifically made to be aired on specific television shows. On the impact of "Bohemian Rhapsody", Rolling Stone states: "Its influence cannot be overstated, practically inventing the music video seven years before MTV went on the air." The second single from the album, "You're My Best Friend", the second song composed by John Deacon, and his first single, peaked at number sixteen in the United States and went on to become a worldwide Top Ten hit. The band's A Night at the Opera Tour began in November 1975, and covered Europe, the United States, Japan, and Australia.
By 1976, Queen were back in the studio recording A Day at the Races, which is often regarded as a sequel album to A Night at the Opera. It again borrowed the name of a Marx Brothers movie, and its cover was similar to that of A Night at the Opera, a variation on the same Queen Crest. The most recognisable of the Marx Brothers, Groucho Marx, invited Queen to visit him in his Los Angeles home in March 1977; there the band thanked him in person, and performed "'39" a cappella. Musically, A Day at the Races was by both fans' and critics' standards a strong effort, reaching number one in the UK and Japan, and number five in the US. The major hit on the album was "Somebody to Love", a gospel-inspired song in which Mercury, May, and Taylor multi-tracked their voices to create a 100-voice gospel choir. The song went to number two in the UK, and number thirteen in the US. The album also featured one of the band's heaviest songs, May's "Tie Your Mother Down", which became a staple of their live shows.
During 1976, Queen played one of their most famous gigs, a free concert in Hyde Park, London. A concert organised by the entrepreneur Richard Branson, it set an attendance record with 150,000 people confirmed in the audience. On 1 December 1976, Queen were the intended guests on London's early evening Today programme, but they pulled out at the last-minute, which saw their late replacement on the show, EMI labelmate the Sex Pistols, give their seminal interview. During the A Day at the Races Tour in 1977, Queen performed sold-out shows at Madison Square Garden, New York, in February, and Earls Court, London, in June.
The band's sixth studio album News of the World was released in 1977, which has gone four times platinum in the United States, and twice in the UK. The album contained many songs tailor-made for live performance, including two of rock's most recognisable anthems, "We Will Rock You" and the rock ballad "We Are the Champions", both of which became enduring international sports anthems, and the latter reached number four in the US. Queen commenced the News of the World Tour in October 1977, and Robert Hilburn of the Los Angeles Times called this concert tour the band's "most spectacularly staged and finely honed show".
In 1978, the band released Jazz, which reached number two in the UK and number six on the Billboard 200 in the US. The album included the hit singles "Fat Bottomed Girls" and "Bicycle Race" on a double-sided record. Queen rented Wimbledon Stadium for a day to shoot the video, with 65 naked female models hired to stage a nude bicycle race. Reviews of the album in recent years have been more favourable. Another notable track from Jazz, "Don't Stop Me Now", provides another example of the band's exuberant vocal harmonies.
In 1978, Queen toured the US and Canada, and spent much of 1979 touring in Europe and Japan. They released their first live album, Live Killers, in 1979; it went platinum twice in the US. Queen also released the very successful single "Crazy Little Thing Called Love", a rockabilly inspired song done in the style of Elvis Presley. The song made the top 10 in many countries, topped the Australian ARIA Charts for seven consecutive weeks, and was the band's first number one single in the United States where it topped the Billboard Hot 100 for four weeks. Having written the song on guitar and played rhythm on the record, Mercury played rhythm guitar while performing the song live, which was the first time he ever played guitar in concert. In December 1979, Queen played the opening night at the Concert for the People of Kampuchea in London, having accepted a request by the event's organiser Paul McCartney.
Queen began their 1980s career with The Game. It featured the singles "Crazy Little Thing Called Love" and "Another One Bites the Dust", both of which reached number one in the US. After attending a Queen concert in Los Angeles, Michael Jackson suggested to Mercury backstage that "Another One Bites the Dust" be released as a single, and in October 1980 it spent three weeks at number one. The album topped the Billboard 200 for five weeks, and sold over four million copies in the US. It was also the first appearance of a synthesiser on a Queen album. Heretofore, their albums featured a distinctive "No Synthesisers!" sleeve note. The note is widely assumed to reflect an anti-synth, pro-"hard"-rock stance by the band, but was later revealed by producer Roy Thomas Baker to be an attempt to clarify that those albums' multi-layered solos were created with guitars, not synths, as record company executives kept assuming at the time. In September 1980, Queen performed three sold-out shows at Madison Square Garden. In 1980, Queen also released the soundtrack they had recorded for Flash Gordon. At the 1981 American Music Awards in January, "Another One Bites the Dust" won the award for Favorite Pop/Rock Single, and Queen were nominated for Favorite Pop/Rock Band, Duo, or Group.
In February 1981, Queen travelled to South America as part of The Game Tour, and became the first major rock band to play in Latin American stadiums. The tour included five shows in Argentina, one of which drew the largest single concert crowd in Argentine history with an audience of 300,000 in Buenos Aires and two concerts at the Morumbi Stadium in São Paulo, Brazil, where they played to an audience of more than 131,000 people in the first night (then the largest paying audience for a single band anywhere in the world) and more than 120,000 people the following night. In October of the same year, Queen performed for more than 150,000 fans on 9 October at Monterrey (Estadio Universitario) and 17 and 18 at Puebla (Estadio Zaragoza), Mexico. On 24 and 25 November, Queen played two sell out nights at the Montreal Forum, Quebec, Canada. One of Mercury's most notable performances of The Game's final track, "Save Me", took place in Montreal, and the concert is recorded in the live album, Queen Rock Montreal.
In 1982, the band released the album Hot Space, a departure from their trademark seventies sound, this time being a mixture of rock, pop rock, dance, funk, and R&B. Most of the album was recorded in Munich during the most turbulent period in the band's history, and Taylor and May lamented the new sound, with both being very critical of the influence Mercury's personal manager Paul Prenter had on the singer. May was also scathing of Prenter, who was Mercury's manager from the early 1980s to 1984, for being dismissive of the importance of radio stations, such as the US networks, and their vital connection between the artist and the community, and for denying them access to Mercury. The band stopped touring North America after their Hot Space Tour, as their success there had waned, although they would perform on American television for the only time during the eighth season premiere of Saturday Night Live. Queen left Elektra Records, their label in the United States, Canada, Japan, Australia, and New Zealand, and signed onto EMI/Capitol Records.
That year, Queen began The Works Tour, the first tour to feature keyboardist Spike Edney as an extra live musician. The tour featured nine sold-out dates in October in Bophuthatswana, South Africa, at the arena in Sun City. Upon returning to England, they were the subject of outrage, having played in South Africa during the height of apartheid and in violation of worldwide divestment efforts and a United Nations cultural boycott. The band responded to the critics by stating that they were playing music for fans in South Africa, and they also stressed that the concerts were played before integrated audiences. Queen donated to a school for the deaf and blind as a philanthropic gesture but were fined by the British Musicians' Union and placed on the United Nations' blacklisted artists.
At Live Aid, held at Wembley on 13 July 1985, in front of the biggest-ever TV audience of 1.9 billion, Queen performed some of their greatest hits, during which the sold-out stadium audience of 72,000 people clapped, sang, and swayed in unison. The show's organisers, Bob Geldof and Midge Ure, other musicians such as Elton John, Cliff Richard and Dave Grohl, and music journalists writing for the BBC, CNN, Rolling Stone, MTV, The Telegraph among others, stated that Queen stole the show. An industry poll in 2005 ranked it the greatest rock performance of all time. Mercury's powerful, sustained note during the a cappella section came to be known as "The Note Heard Round the World".
When interviewed for Mojo magazine the band said the most amazing sight at Live Aid was to see the audience clapping to "Radio Ga Ga". Brian May stated: "I'd never seen anything like that in my life and it wasn't calculated either. We understood our audience and played to them but that was one of those weird accidents because of the (music) video. I remember thinking 'oh great, they've picked it up' and then I thought 'this is not a Queen audience'. This is a general audience who've bought tickets before they even knew we were on the bill. And they all did it. How did they know? Nobody told them to do it."
The band, now revitalised by the response to Live Aid – a "shot in the arm" Roger Taylor called it, — and the ensuing increase in record sales, ended 1985 by releasing the single "One Vision", which was the third time after "Stone Cold Crazy" and "Under Pressure (with David Bowie)" that all four bandmembers received a writing credit for the one song. Also, a limited-edition boxed set containing all Queen albums to date was released under the title of The Complete Works. The package included previously unreleased material, most notably Queen's non-album single of Christmas 1984, titled "Thank God It's Christmas".
In summer of 1986, Queen went on their final tour with Freddie Mercury. A sold-out tour in support of A Kind of Magic, once again they hired Spike Edney, leading to him being dubbed the unofficial fifth member. The Magic Tour's highlight was at Wembley Stadium in London and resulted in the live double album, Queen at Wembley, released on CD and as a live concert DVD, which has gone five times platinum in the US and four times platinum in the UK. Queen could not book Wembley for a third night, but they did play at Knebworth Park. The show sold out within two hours and over 120,000 fans packed the park for what was Queen's final live performance with Mercury. Queen began the tour at the Råsunda Stadium in Stockholm, Sweden, and during the tour the band performed a concert at Slane Castle, Ireland, in front of an audience of 95,000, which broke the venue's attendance record. The band also played behind the Iron Curtain when they performed to a crowd of 80,000 at the Népstadion in Budapest, in what was one of the biggest rock concerts ever held in Eastern Europe. More than one million people saw Queen on the tour—400,000 in the United Kingdom alone, a record at the time.
After working on various solo projects during 1988 (including Mercury's collaboration with Montserrat Caballé, Barcelona), the band released The Miracle in 1989. The album continued the direction of A Kind of Magic, using a pop-rock sound mixed with a few heavy numbers. It spawned the European hits "I Want It All", "Breakthru", "The Invisible Man", "Scandal", and "The Miracle". The Miracle also began a change in direction of Queen's songwriting philosophy. Since the band's beginning, nearly all songs had been written by and credited to a single member, with other members adding minimally. With The Miracle, the band's songwriting became more collaborative, and they vowed to credit the final product only to Queen as a group.
After fans noticed Mercury's increasingly gaunt appearance in 1988, rumours began to spread that Mercury was suffering from AIDS. Mercury flatly denied this, insisting he was merely "exhausted" and too busy to provide interviews. The band decided to continue making albums, starting with The Miracle in 1989 and continuing with Innuendo in 1991. Despite his deteriorating health, the lead singer continued to contribute. For the last two albums made while Mercury was still alive, the band credited all songs to Queen, rather than specific members of the group, freeing them of internal conflict and differences. In 1990, Queen ended their contract with Capitol and signed with Disney's Hollywood Records, which has since remained the group's music catalogue owner in the United States and Canada. That same year, Mercury made his final public appearance when he joined the rest of Queen to collect the Brit Award for Outstanding Contribution to British Music.
Innuendo was released in early 1991 with an eponymous number 1 UK hit and other charting singles including, "The Show Must Go On". Mercury was increasingly ill and could barely walk when the band recorded "The Show Must Go On" in 1990. Because of this, May had concerns about whether he was physically capable of singing it. Recalling Mercury's successful performance May states; "he went in and killed it, completely lacerated that vocal". The rest of the band were ready to record when Mercury felt able to come in to the studio, for an hour or two at a time. May says of Mercury: “He just kept saying. 'Write me more. Write me stuff. I want to just sing this and do it and when I am gone you can finish it off.’ He had no fear, really.” The band's second greatest hits compilation, Greatest Hits II, followed in October 1991, which is the eighth best-selling album of all time in the UK and has sold 16 million copies worldwide.
On 23 November 1991, in a prepared statement made on his deathbed, Mercury confirmed that he had AIDS. Within 24 hours of the statement, he died of bronchial pneumonia, which was brought on as a complication of AIDS. His funeral service on 27 November in Kensal Green, West London was private, and held in accordance with the Zoroastrian religious faith of his family. "Bohemian Rhapsody" was re-released as a single shortly after Mercury's death, with "These Are the Days of Our Lives" as the double A-side. The music video for "These Are the Days of Our Lives" contains Mercury's final scenes in front of the camera. The single went to number one in the UK, remaining there for five weeks – the only recording to top the Christmas chart twice and the only one to be number one in four different years (1975, 1976, 1991, and 1992). Initial proceeds from the single – approximately £1,000,000 – were donated to the Terrence Higgins Trust.
Queen's popularity was stimulated in North America when "Bohemian Rhapsody" was featured in the 1992 comedy film Wayne's World. Its inclusion helped the song reach number two on the Billboard Hot 100 for five weeks in 1992 (it remained in the Hot 100 for over 40 weeks), and won the band an MTV Award at the 1992 MTV Video Music Awards. The compilation album Classic Queen also reached number four on the Billboard 200, and is certified three times platinum in the US. Wayne's World footage was used to make a new music video for "Bohemian Rhapsody", with which the band and management were delighted.
On 20 April 1992, The Freddie Mercury Tribute Concert was held at London's Wembley Stadium to a 72,000-strong crowd. Performers, including Def Leppard, Robert Plant, Guns N' Roses, Elton John, David Bowie, George Michael, Annie Lennox, Seal, Extreme, and Metallica performed various Queen songs along with the three remaining Queen members (and Spike Edney.) The concert is listed in the Guinness Book of Records as "The largest rock star benefit concert", as it was televised to over 1.2 billion viewers worldwide, and raised over £20,000,000 for AIDS charities.
Queen's last album featuring Mercury, titled Made in Heaven, was finally released in 1995, four years after his death. Featuring tracks such as "Too Much Love Will Kill You" and "Heaven for Everyone", it was constructed from Mercury's final recordings in 1991, material left over from their previous studio albums and re-worked material from May, Taylor, and Mercury's solo albums. The album also featured the song "Mother Love", the last vocal recording Mercury made prior to his death, which he completed using a drum machine, over which May, Taylor and Deacon later added the instrumental track. After completing the penultimate verse, Mercury had told the band he "wasn't feeling that great" and stated, "I will finish it when I come back, next time"; however, he never made it back into the studio, so May later recorded the final verse of the song. Both stages of recording, before and after Mercury's death, were completed at the band's studio in Montreux, Switzerland. The album reached No. 1 on the UK charts immediately following its release, and has sold 20 million copies worldwide. On 25 November 1996, a statue of Mercury was unveiled in Montreux overlooking Lake Geneva, almost five years to the day since his death.
In 1997, Queen returned to the studio to record "No-One but You (Only the Good Die Young)", a song dedicated to Mercury and all those that die too soon. It was released as a bonus track on the Queen Rocks compilation album later that year. In January 1997, Queen performed "The Show Must Go On" live with Elton John and the Béjart Ballet in Paris on a night Mercury was remembered, and it marked the last performance and public appearance of John Deacon, who chose to retire. The Paris concert was only the second time Queen had played live since Mercury's death, prompting Elton John to urge them to perform again.
Brian May and Roger Taylor performed together at several award ceremonies and charity concerts, sharing vocals with various guest singers. During this time, they were billed as Queen + followed by the guest singer's name. In 1998, the duo appeared at Luciano Pavarotti's benefit concert with May performing "Too Much Love Will Kill You" with Pavarotti, later playing "Radio Ga Ga", "We Will Rock You", and "We Are the Champions" with Zucchero. They again attended and performed at Pavarotti's benefit concert in Modena, Italy in May 2003. Several of the guest singers recorded new versions of Queen's hits under the Queen + name, such as Robbie Williams providing vocals for "We Are the Champions" for the soundtrack of A Knight's Tale (2001).
In 1999, a Greatest Hits III album was released. This featured, among others, "Queen + Wyclef Jean" on a rap version of "Another One Bites the Dust". A live version of "Somebody to Love" by George Michael and a live version of "The Show Must Go On" with Elton John were also featured in the album. By this point, Queen's vast amount of record sales made them the second best selling artist in the UK of all time, behind the Beatles. In 2002, Queen were awarded the 2,207th star on the Hollywood Walk of Fame, which is located at 6358 Hollywood Blvd. On 29 November 2003, May and Taylor performed at the 46664 Concert hosted by Nelson Mandela at Green Point Stadium, Cape Town, to raise awareness of the spread of HIV/AIDS in South Africa. May and Taylor spent time at Mandela's home, discussing how Africa's problems might be approached, and two years later the band was made ambassadors for the 46664 cause.
At the end of 2004, May and Taylor announced that they would reunite and return to touring in 2005 with Paul Rodgers (founder and former lead singer of Free and Bad Company). Brian May's website also stated that Rodgers would be "featured with" Queen as "Queen + Paul Rodgers", not replacing Mercury. The retired John Deacon would not be participating. In November 2004, Queen were among the inaugural inductees into the UK Music Hall of Fame, and the award ceremony was the first event at which Rodgers joined May and Taylor as vocalist.
Between 2005 and 2006, Queen + Paul Rodgers embarked on a world tour, which was the first time Queen toured since their last tour with Freddie Mercury in 1986. The band's drummer Roger Taylor commented; "We never thought we would tour again, Paul [Rodgers] came along by chance and we seemed to have a chemistry. Paul is just such a great singer. He's not trying to be Freddie." The first leg was in Europe, the second in Japan, and the third in the US in 2006. Queen received the inaugural VH1 Rock Honors at the Mandalay Bay Events Center in Las Vegas, Nevada, on 25 May 2006. The Foo Fighters paid homage to the band in performing "Tie Your Mother Down" to open the ceremony before being joined on stage by May, Taylor, and Paul Rodgers, who played a selection of Queen hits.
On 15 August 2006, Brian May confirmed through his website and fan club that Queen + Paul Rodgers would begin producing their first studio album beginning in October, to be recorded at a "secret location". Queen + Paul Rodgers performed at the Nelson Mandela 90th Birthday Tribute held in Hyde Park, London on 27 June 2008, to commemorate Mandela's ninetieth birthday, and again promote awareness of the HIV/AIDS pandemic. The first Queen + Paul Rodgers album, titled The Cosmos Rocks, was released in Europe on 12 September 2008 and in the United States on 28 October 2008. Following the release of the album, the band again went on a tour through Europe, opening on Kharkiv's Freedom Square in front of 350,000 Ukrainian fans. The Kharkiv concert was later released on DVD. The tour then moved to Russia, and the band performed two sold-out shows at the Moscow Arena. Having completed the first leg of its extensive European tour, which saw the band play 15 sold-out dates across nine countries, the UK leg of the tour sold out within 90 minutes of going on sale and included three London dates, the first of which was The O2 on 13 October. The last leg of the tour took place in South America, and included a sold-out concert at the Estadio José Amalfitani, Buenos Aires.
On 20 May 2009, May and Taylor performed "We Are the Champions" live on the season finale of American Idol with winner Kris Allen and runner-up Adam Lambert providing a vocal duet. In mid-2009, after the split of Queen + Paul Rodgers, the Queen online website announced a new greatest hits compilation named Absolute Greatest. The album was released on 16 November and peaked at number 3 in the official UK Chart. The album contains 20 of Queen's biggest hits spanning their entire career and was released in four different formats: single disc, double disc (with commentary), double disc with feature book, and a vinyl record. Prior to its release, a competition was run by Queen online to guess the track listing as a promotion for the album.
On 30 October 2009, May wrote a fanclub letter on his website stating that Queen had no intentions to tour in 2010 but that there was a possibility of a performance. He was quoted as saying, "The greatest debate, though, is always about when we will next play together as Queen. At the moment, in spite of the many rumours that are out there, we do not have plans to tour in 2010. The good news, though, is that Roger and I have a much closer mutual understanding these days—privately and professionally ... and all ideas are carefully considered. Music is never far away from us. As I write, there is an important one-off performance on offer, in the USA, and it remains to be decided whether we will take up this particular challenge. Every day, doors seem to open, and every day, we interact, perhaps more than ever before, with the world outside. It is a time of exciting transition in Rock music and in 'The Business'. It's good that the pulse still beats". On 15 November 2009, May and Taylor performed "Bohemian Rhapsody" live on the British TV show The X Factor alongside the finalists.
On 7 May 2010, May and Taylor announced that they were quitting their record label, EMI, after almost 40 years. On 20 August 2010, Queen's manager Jim Beach put out a Newsletter stating that the band had signed a new contract with Universal Music. During an interview for Hardtalk on the BBC on 22 September, May confirmed that the band's new deal was with Island Records, a subsidiary of Universal Music Group. For the first time since the late 1980s, Queen's catalogue will have the same distributor worldwide, as their current North American label—Hollywood Records—is currently distributed by Universal (for a time in the late 1980s, Queen was on EMI-owned Capitol Records in the US).
In May 2011, Jane's Addiction vocalist Perry Farrell noted that Queen are currently scouting their once former and current live bassist Chris Chaney to join the band. Farrell stated: "I have to keep Chris away from Queen, who want him and they're not gonna get him unless we're not doing anything. Then they can have him." In the same month, Paul Rodgers stated he may tour with Queen again in the near future. At the 2011 Broadcast Music, Incorporated (BMI) Awards held in London on 4 October, Queen received the BMI Icon Award in recognition for their airplay success in the US. At the 2011 MTV Europe Music Awards on 6 November, Queen received the Global Icon Award, which Katy Perry presented to Brian May. Queen closed the awards ceremony, with Adam Lambert on vocals, performing "The Show Must Go On", "We Will Rock You" and "We Are the Champions". The collaboration garnered a positive response from both fans and critics, resulting in speculation about future projects together.
On 25 and 26 April, May and Taylor appeared on the eleventh series of American Idol at the Nokia Theatre, Los Angeles, performing a Queen medley with the six finalists on the first show, and the following day performed "Somebody to Love" with the 'Queen Extravaganza' band. Queen were scheduled to headline Sonisphere at Knebworth on 7 July 2012 with Adam Lambert before the festival was cancelled. Queen's final concert with Freddie Mercury was in Knebworth in 1986. Brian May commented, "It's a worthy challenge for us, and I'm sure Adam would meet with Freddie's approval." Queen expressed disappointment at the cancellation and released a statement to the effect that they were looking to find another venue. It was later announced that Queen + Adam Lambert would play two shows at the Hammersmith Apollo, London on 11 and 12 July 2012. Both shows sold out within 24 hours of tickets going on open sale. A third London date was scheduled for 14 July. On 30 June, Queen + Lambert performed in Kiev, Ukraine at a joint concert with Elton John for the Elena Pinchuk ANTIAIDS Foundation. Queen also performed with Lambert on 3 July 2012 at Moscow's Olympic Stadium, and on 7 July 2012 at the Municipal Stadium in Wroclaw, Poland.
On 20 September 2013, Queen + Adam Lambert performed at the iHeartRadio Music Festival at the MGM Grand Hotel & Casino in Las Vegas. On 6 March 2014, the band announced on Good Morning America that Queen + Adam Lambert will tour North America in Summer 2014. The band will also tour Australia and New Zealand in August/September 2014. In an interview with Rolling Stone, May and Taylor said that although the tour with Lambert is a limited thing, they are open to him becoming an official member, and cutting new material with him.
Queen drew artistic influence from British rock acts of the 1960s and early 1970s, such as the Beatles, the Kinks, Cream, Led Zeppelin, Pink Floyd, the Who, Black Sabbath, Slade, Deep Purple, David Bowie, Genesis and Yes, in addition to American guitarist Jimi Hendrix, with Mercury also inspired by the gospel singer Aretha Franklin. May referred to the Beatles as being "our bible in the way they used the studio and they painted pictures and this wonderful instinctive use of harmonies." At their outset in the early 1970s, Queen's music has been characterised as "Led Zeppelin meets Yes" due to its combination of "acoustic/electric guitar extremes and fantasy-inspired multi-part song epics".
Queen composed music that drew inspiration from many different genres of music, often with a tongue-in-cheek attitude. The genres they have been associated with include progressive rock, symphonic rock, art rock, glam rock, hard rock, heavy metal, pop rock, and psychedelic rock. Queen also wrote songs that were inspired by diverse musical styles which are not typically associated with rock groups, such as opera, music hall, folk music, gospel, ragtime, and dance/disco. Several Queen songs were written with audience participation in mind, such as "We Will Rock You" and "We Are the Champions". Similarly, "Radio Ga Ga" became a live favourite because it would have "crowds clapping like they were at a Nuremberg rally".
In 1963, the teenage Brian May and his father custom-built his signature guitar Red Special, which was purposely designed to feedback. Sonic experimentation figured heavily in Queen's songs. A distinctive characteristic of Queen's music are the vocal harmonies which are usually composed of the voices of May, Mercury, and Taylor best heard on the studio albums A Night at the Opera and A Day at the Races. Some of the ground work for the development of this sound can be attributed to their former producer Roy Thomas Baker, and their engineer Mike Stone. Besides vocal harmonies, Queen were also known for multi-tracking voices to imitate the sound of a large choir through overdubs. For instance, according to Brian May, there are over 180 vocal overdubs in "Bohemian Rhapsody". The band's vocal structures have been compared with the Beach Boys, but May stated they were not "much of an influence".
Queen have been recognised as having made significant contributions to such genres as hard rock, and heavy metal, among others. Hence, the band have been cited as an influence by many other musicians. Moreover, like their music, the bands and artists that have claimed to be influenced by Queen and have expressed admiration for them are diverse, spanning different generations, countries, and genres, including heavy metal: Judas Priest, Iron Maiden, Metallica, Dream Theater, Trivium, Megadeth, Anthrax, Slipknot and Rage Against the Machine; hard rock: Guns N' Roses, Def Leppard, Van Halen, Mötley Crüe, Steve Vai, the Cult, the Darkness, Manic Street Preachers, Kid Rockand Foo Fighters; alternative rock: Nirvana, Radiohead, Trent Reznor, Muse, Franz Ferdinand, Red Hot Chili Peppers, Jane's Addiction, Faith No More, Melvins, the Flaming Lips, Yeah Yeah Yeahs and The Smashing Pumpkins; pop rock: Meat Loaf, The Killers, My Chemical Romance, Fall Out Boy and Panic! At the Disco; and pop: Michael Jackson, George Michael, Robbie Williams, Adele, Lady Gaga and Katy Perry.
In 2002, Queen's "Bohemian Rhapsody" was voted "the UK's favourite hit of all time" in a poll conducted by the Guinness World Records British Hit Singles Book. In 2004 the song was inducted into the Grammy Hall of Fame. Many scholars consider the "Bohemian Rhapsody" music video ground-breaking, and credit it with popularising the medium. Rock historian Paul Fowles states the song is "widely credited as the first global hit single for which an accompanying video was central to the marketing strategy". It has been hailed as launching the MTV age. Acclaimed for their stadium rock, in 2005 an industry poll ranked Queen's performance at Live Aid in 1985 as the best live act in history. In 2007, they were also voted the greatest British band in history by BBC Radio 2 listeners.
The band have released a total of eighteen number one albums, eighteen number one singles, and ten number one DVDs worldwide, making them one of the world's best-selling music artists. Queen have sold over 150 million records, with some estimates in excess of 300 million records worldwide, including 34.5 million albums in the US as of 2004. Inducted into the Rock and Roll Hall of Fame in 2001, the band is the only group in which every member has composed more than one chart-topping single, and all four members were inducted into the Songwriters Hall of Fame in 2003. In 2009, "We Will Rock You" and "We Are the Champions" were inducted into the Grammy Hall of Fame, and the latter was voted the world's favourite song in a global music poll.
Queen are one of the most bootlegged bands ever, according to Nick Weymouth, who manages the band's official website. A 2001 survey discovered the existence of 12,225 websites dedicated to Queen bootlegs, the highest number for any band. Bootleg recordings have contributed to the band's popularity in certain countries where Western music is censored, such as Iran. In a project called Queen: The Top 100 Bootlegs, many of these have been made officially available to download for a nominal fee from Queen's website, with profits going to the Mercury Phoenix Trust. Rolling Stone ranked Queen at number 52 on its list of the "100 Greatest Artists of All Time", while ranking Mercury the 18th greatest singer, and May the twenty-sixth greatest guitarist. Queen were named 13th on VH1's 100 Greatest Artists of Hard Rock list, and in 2010 were ranked 17th on VH1's 100 Greatest Artists of All Time list. In 2012, Gigwise readers named Queen the best band of past 60 years.
The original London production was scheduled to close on Saturday, 7 October 2006, at the Dominion Theatre, but due to public demand, the show ran until May 2014. We Will Rock You has become the longest running musical ever to run at this prime London theatre, overtaking the previous record holder, the Grease musical. Brian May stated in 2008 that they were considering writing a sequel to the musical. The musical toured around the UK in 2009, playing at Manchester Palace Theatre, Sunderland Empire, Birmingham Hippodrome, Bristol Hippodrome, and Edinburgh Playhouse.
Under the supervision of May and Taylor, numerous restoration projects have been under way involving Queen's lengthy audio and video catalogue. DVD releases of their 1986 Wembley concert (titled Live at Wembley Stadium), 1982 Milton Keynes concert (Queen on Fire – Live at the Bowl), and two Greatest Video Hits (Volumes 1 and 2, spanning the 1970s and 1980s) have seen the band's music remixed into 5.1 and DTS surround sound. So far, only two of the band's albums, A Night at the Opera and The Game, have been fully remixed into high-resolution multichannel surround on DVD-Audio. A Night at the Opera was re-released with some revised 5.1 mixes and accompanying videos in 2005 for the 30th anniversary of the album's original release (CD+DVD-Video set). In 2007, a Blu-ray edition of Queen's previously released concerts, Queen Rock Montreal & Live Aid, was released, marking their first project in 1080p HD.
Queen have been featured multiple times in the Guitar Hero franchise: a cover of "Killer Queen" in the original Guitar Hero, "We Are The Champions", "Fat Bottomed Girls", and the Paul Rodgers collaboration "C-lebrity" in a track pack for Guitar Hero World Tour, "Under Pressure" with David Bowie in Guitar Hero 5, "I Want It All" in Guitar Hero: Van Halen, "Stone Cold Crazy" in Guitar Hero: Metallica, and "Bohemian Rhapsody" in Guitar Hero: Warriors of Rock. On 13 October 2009, Brian May revealed there was "talk" going on "behind the scenes" about a dedicated Queen Rock Band game.
Queen contributed music directly to the films Flash Gordon (1980), with "Flash" as the theme song, and Highlander (the original 1986 film), with "A Kind of Magic", "One Year of Love", "Who Wants to Live Forever", "Hammer to Fall", and the theme "Princes of the Universe", which was also used as the theme of the Highlander TV series (1992–1998). In the United States, "Bohemian Rhapsody" was re-released as a single in 1992 after appearing in the comedy film Wayne's World. The single subsequently reached number two on the Billboard Hot 100 (with "The Show Must Go On" as the first track on the single) and helped rekindle the band's popularity in North America.
Several films have featured their songs performed by other artists. A version of "Somebody to Love" by Anne Hathaway was in the 2004 film Ella Enchanted. In 2006, Brittany Murphy also recorded a cover of the same song for the 2006 film Happy Feet. In 2001, a version of "The Show Must Go On" was performed by Jim Broadbent and Nicole Kidman in the film musical Moulin Rouge!. The 2001 film A Knight's Tale has a version of "We Are the Champions" performed by Robbie Williams and Queen; the film also features "We Will Rock You" played by the medieval audience.
On 11 April 2006, Brian May and Roger Taylor appeared on the American singing contest television show American Idol. Each contestant was required to sing a Queen song during that week of the competition. Songs which appeared on the show included "Bohemian Rhapsody", "Fat Bottomed Girls", "The Show Must Go On", "Who Wants to Live Forever", and "Innuendo". Brian May later criticised the show for editing specific scenes, one of which made the group's time with contestant Ace Young look negative, despite it being the opposite. Taylor and May again appeared on the American Idol season 8 finale in May 2009, performing "We Are the Champions" with finalists Adam Lambert and Kris Allen. On 15 November 2009, Brian May and Roger Taylor appeared on the singing contest television show X Factor in the UK.
In the autumn of 2009, Glee featured the fictional high school's show choir singing "Somebody to Love" as their second act performance in the episode "The Rhodes Not Taken". The performance was included on the show's Volume 1 soundtrack CD. In June 2010, the choir performed "Another One Bites the Dust" in the episode "Funk". The following week's episode, "Journey to Regionals", features a rival choir performing "Bohemian Rhapsody" in its entirety. The song was featured on the episode's EP. In May 2012, the choir performed "We Are the Champions" in the episode "Nationals", and the song features in The Graduation Album.
In September 2010, Brian May announced in a BBC interview that Sacha Baron Cohen was to play Mercury in a film of the same name. Time commented with approval on his singing ability and visual similarity to Mercury. However, in July 2013, Baron Cohen dropped out of the role due to "creative differences" between him and the surviving band members. In December 2013, it was announced that Ben Whishaw, best known for playing Q in the James Bond film Skyfall, had been chosen to replace Cohen in the role of Mercury. The motion picture is being written by Peter Morgan, who had been nominated for Oscars for his screenplays The Queen and Frost/Nixon. The film, which is being co-produced by Robert De Niro's TriBeCa Productions, will focus on Queen's formative years and the period leading up to the celebrated performance at the 1985 Live Aid concert.

A gene is a locus (or region) of DNA that encodes a functional RNA or protein product, and is the molecular unit of heredity.:Glossary The transmission of genes to an organism's offspring is the basis of the inheritance of phenotypic traits. Most biological traits are under the influence of polygenes (many different genes) as well as the gene–environment interactions. Some genetic traits are instantly visible, such as eye colour or number of limbs, and some are not, such as blood type, risk for specific diseases, or the thousands of basic biochemical processes that comprise life.
Genes can acquire mutations in their sequence, leading to different variants, known as alleles, in the population. These alleles encode slightly different versions of a protein, which cause different phenotype traits. Colloquial usage of the term "having a gene" (e.g., "good genes," "hair colour gene") typically refers to having a different allele of the gene. Genes evolve due to natural selection or survival of the fittest of the alleles.
The concept of a gene continues to be refined as new phenomena are discovered. For example, regulatory regions of a gene can be far removed from its coding regions, and coding regions can be split into several exons. Some viruses store their genome in RNA instead of DNA and some gene products are functional non-coding RNAs. Therefore, a broad, modern working definition of a gene is any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product or by regulation of gene expression.
The existence of discrete inheritable units was first suggested by Gregor Mendel (1822–1884). From 1857 to 1864, he studied inheritance patterns in 8000 common edible pea plants, tracking distinct traits from parent to offspring. He described these mathematically as 2n combinations where n is the number of differing characteristics in the original peas. Although he did not use the term gene, he explained his results in terms of discrete inherited units that give rise to observable physical characteristics. This description prefigured the distinction between genotype (the genetic material of an organism) and phenotype (the visible traits of that organism). Mendel was also the first to demonstrate independent assortment, the distinction between dominant and recessive traits, the distinction between a heterozygote and homozygote, and the phenomenon of discontinuous inheritance.
Prior to Mendel's work, the dominant theory of heredity was one of blending inheritance, which suggested that each parent contributed fluids to the fertilisation process and that the traits of the parents blended and mixed to produce the offspring. Charles Darwin developed a theory of inheritance he termed pangenesis, which used the term gemmule to describe hypothetical particles that would mix during reproduction. Although Mendel's work was largely unrecognized after its first publication in 1866, it was 'rediscovered' in 1900 by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak, who claimed to have reached similar conclusions in their own research.
The word gene is derived (via pangene) from the Ancient Greek word γένος (génos) meaning "race, offspring". Gene was coined in 1909 by Danish botanist Wilhelm Johannsen to describe the fundamental physical and functional unit of heredity, while the related word genetics was first used by William Bateson in 1905.
Advances in understanding genes and inheritance continued throughout the 20th century. Deoxyribonucleic acid (DNA) was shown to be the molecular repository of genetic information by experiments in the 1940s to 1950s. The structure of DNA was studied by Rosalind Franklin using X-ray crystallography, which led James D. Watson and Francis Crick to publish a model of the double-stranded DNA molecule whose paired nucleotide bases indicated a compelling hypothesis for the mechanism of genetic replication. Collectively, this body of research established the central dogma of molecular biology, which states that proteins are translated from RNA, which is transcribed from DNA. This dogma has since been shown to have exceptions, such as reverse transcription in retroviruses. The modern study of genetics at the level of DNA is known as molecular genetics.
In 1972, Walter Fiers and his team at the University of Ghent were the first to determine the sequence of a gene: the gene for Bacteriophage MS2 coat protein. The subsequent development of chain-termination DNA sequencing in 1977 by Frederick Sanger improved the efficiency of sequencing and turned it into a routine laboratory tool. An automated version of the Sanger method was used in early phases of the Human Genome Project.
The theories developed in the 1930s and 1940s to integrate molecular genetics with Darwinian evolution are called the modern evolutionary synthesis, a term introduced by Julian Huxley. Evolutionary biologists subsequently refined this concept, such as George C. Williams' gene-centric view of evolution. He proposed an evolutionary concept of the gene as a unit of natural selection with the definition: "that which segregates and recombines with appreciable frequency.":24 In this view, the molecular gene transcribes as a unit, and the evolutionary gene inherits as a unit. Related ideas emphasizing the centrality of genes in evolution were popularized by Richard Dawkins.
The vast majority of living organisms encode their genes in long strands of DNA (deoxyribonucleic acid). DNA consists of a chain made from four types of nucleotide subunits, each composed of: a five-carbon sugar (2'-deoxyribose), a phosphate group, and one of the four bases adenine, cytosine, guanine, and thymine.:2.1
Two chains of DNA twist around each other to form a DNA double helix with the phosphate-sugar backbone spiralling around the outside, and the bases pointing inwards with adenine base pairing to thymine and guanine to cytosine. The specificity of base pairing occurs because adenine and thymine align form two hydrogen bonds, whereas cytosine and guanine form three hydrogen bonds. The two strands in a double helix must therefore be complementary, with their sequence of bases matching such that the adenines of one strand are paired with the thymines of the other strand, and so on.:4.1
Due to the chemical composition of the pentose residues of the bases, DNA strands have directionality. One end of a DNA polymer contains an exposed hydroxyl group on the deoxyribose; this is known as the 3' end of the molecule. The other end contains an exposed phosphate group; this is the 5' end. The two strands of a double-helix run in opposite directions. Nucleic acid synthesis, including DNA replication and transcription occurs in the 5'→3' direction, because new nucleotides are added via a dehydration reaction that uses the exposed 3' hydroxyl as a nucleophile.:27.2
The expression of genes encoded in DNA begins by transcribing the gene into RNA, a second type of nucleic acid that is very similar to DNA, but whose monomers contain the sugar ribose rather than deoxyribose. RNA also contains the base uracil in place of thymine. RNA molecules are less stable than DNA and are typically single-stranded. Genes that encode proteins are composed of a series of three-nucleotide sequences called codons, which serve as the "words" in the genetic "language". The genetic code specifies the correspondence during protein translation between codons and amino acids. The genetic code is nearly the same for all known organisms.:4.1
The total complement of genes in an organism or cell is known as its genome, which may be stored on one or more chromosomes. A chromosome consists of a single, very long DNA helix on which thousands of genes are encoded.:4.2 The region of the chromosome at which a particular gene is located is called its locus. Each locus contains one allele of a gene; however, members of a population may have different alleles at the locus, each with a slightly different gene sequence.
The majority of eukaryotic genes are stored on a set of large, linear chromosomes. The chromosomes are packed within the nucleus in complex with storage proteins called histones to form a unit called a nucleosome. DNA packaged and condensed in this way is called chromatin.:4.2 The manner in which DNA is stored on the histones, as well as chemical modifications of the histone itself, regulate whether a particular region of DNA is accessible for gene expression. In addition to genes, eukaryotic chromosomes contain sequences involved in ensuring that the DNA is copied without degradation of end regions and sorted into daughter cells during cell division: replication origins, telomeres and the centromere.:4.2 Replication origins are the sequence regions where DNA replication is initiated to make two copies of the chromosome. Telomeres are long stretches of repetitive sequence that cap the ends of the linear chromosomes and prevent degradation of coding and regulatory regions during DNA replication. The length of the telomeres decreases each time the genome is replicated and has been implicated in the aging process. The centromere is required for binding spindle fibres to separate sister chromatids into daughter cells during cell division.:18.2
Prokaryotes (bacteria and archaea) typically store their genomes on a single large, circular chromosome. Similarly, some eukaryotic organelles contain a remnant circular chromosome with a small number of genes.:14.4 Prokaryotes sometimes supplement their chromosome with additional small circles of DNA called plasmids, which usually encode only a few genes and are transferable between individuals. For example, the genes for antibiotic resistance are usually encoded on bacterial plasmids and can be passed between individual cells, even those of different species, via horizontal gene transfer.
Whereas the chromosomes of prokaryotes are relatively gene-dense, those of eukaryotes often contain regions of DNA that serve no obvious function. Simple single-celled eukaryotes have relatively small amounts of such DNA, whereas the genomes of complex multicellular organisms, including humans, contain an absolute majority of DNA without an identified function. This DNA has often been referred to as "junk DNA". However, more recent analyses suggest that, although protein-coding DNA makes up barely 2% of the human genome, about 80% of the bases in the genome may be expressed, so the term "junk DNA" may be a misnomer.
The structure of a gene consists of many elements of which the actual protein coding sequence is often only a small part. These include DNA regions that are not transcribed as well as untranslated regions of the RNA.
Firstly, flanking the open reading frame, all genes contain a regulatory sequence that is required for their expression. In order to be expressed, genes require a promoter sequence. The promoter is recognized and bound by transcription factors and RNA polymerase to initiate transcription.:7.1 A gene can have more than one promoter, resulting in messenger RNAs (mRNA) that differ in how far they extend in the 5' end. Promoter regions have a consensus sequence, however highly transcribed genes have "strong" promoter sequences that bind the transcription machinery well, whereas others have "weak" promoters that bind poorly and initiate transcription less frequently.:7.2 Eukaryotic promoter regions are much more complex and difficult to identify than prokaryotic promoters.:7.3
Additionally, genes can have regulatory regions many kilobases upstream or downstream of the open reading frame. These act by binding to transcription factors which then cause the DNA to loop so that the regulatory sequence (and bound transcription factor) become close to the RNA polymerase binding site. For example, enhancers increase transcription by binding an activator protein which then helps to recruit the RNA polymerase to the promoter; conversely silencers bind repressor proteins and make the DNA less available for RNA polymerase.
The transcribed pre-mRNA contains untranslated regions at both ends which contain a ribosome binding site, terminator and start and stop codons. In addition, most eukaryotic open reading frames contain untranslated introns which are removed before the exons are translated. The sequences at the ends of the introns, dictate the splice sites to generate the final mature mRNA which encodes the protein or RNA product.
Many prokaryotic genes are organized into operons, with multiple protein-coding sequences that are transcribed as a unit. The products of operon genes typically have related functions and are involved in the same regulatory network.:7.3
Defining exactly what section of a DNA sequence comprises a gene is difficult. Regulatory regions of a gene such as enhancers do not necessarily have to be close to the coding sequence on the linear molecule because the intervening DNA can be looped out to bring the gene and its regulatory region into proximity. Similarly, a gene's introns can be much larger than its exons. Regulatory regions can even be on entirely different chromosomes and operate in trans to allow regulatory regions on one chromosome to come in contact with target genes on another chromosome.
Early work in molecular genetics suggested the model that one gene makes one protein. This model has been refined since the discovery of genes that can encode multiple proteins by alternative splicing and coding sequences split in short section across the genome whose mRNAs are concatenated by trans-splicing.
A broad operational definition is sometimes used to encompass the complexity of these diverse phenomena, where a gene is defined as a union of genomic sequences encoding a coherent set of potentially overlapping functional products. This definition categorizes genes by their functional products (proteins or RNA) rather than their specific DNA loci, with regulatory elements classified as gene-associated regions.
In all organisms, two steps are required to read the information encoded in a gene's DNA and produce the protein it specifies. First, the gene's DNA is transcribed to messenger RNA (mRNA).:6.1 Second, that mRNA is translated to protein.:6.2 RNA-coding genes must still go through the first step, but are not translated into protein. The process of producing a biologically functional molecule of either RNA or protein is called gene expression, and the resulting molecule is called a gene product.
The nucleotide sequence of a gene's DNA specifies the amino acid sequence of a protein through the genetic code. Sets of three nucleotides, known as codons, each correspond to a specific amino acid.:6 Additionally, a "start codon", and three "stop codons" indicate the beginning and end of the protein coding region. There are 64 possible codons (four possible nucleotides at each of three positions, hence 43 possible codons) and only 20 standard amino acids; hence the code is redundant and multiple codons can specify the same amino acid. The correspondence between codons and amino acids is nearly universal among all known living organisms.
Transcription produces a single-stranded RNA molecule known as messenger RNA, whose nucleotide sequence is complementary to the DNA from which it was transcribed.:6.1 The mRNA acts as an intermediate between the DNA gene and its final protein product. The gene's DNA is used as a template to generate a complementary mRNA. The mRNA matches the sequence of the gene's DNA coding strand because it is synthesised as the complement of the template strand. Transcription is performed by an enzyme called an RNA polymerase, which reads the template strand in the 3' to 5' direction and synthesizes the RNA from 5' to 3'. To initiate transcription, the polymerase first recognizes and binds a promoter region of the gene. Thus, a major mechanism of gene regulation is the blocking or sequestering the promoter region, either by tight binding by repressor molecules that physically block the polymerase, or by organizing the DNA so that the promoter region is not accessible.:7
In prokaryotes, transcription occurs in the cytoplasm; for very long transcripts, translation may begin at the 5' end of the RNA while the 3' end is still being transcribed. In eukaryotes, transcription occurs in the nucleus, where the cell's DNA is stored. The RNA molecule produced by the polymerase is known as the primary transcript and undergoes post-transcriptional modifications before being exported to the cytoplasm for translation. One of the modifications performed is the splicing of introns which are sequences in the transcribed region that do not encode protein. Alternative splicing mechanisms can result in mature transcripts from the same gene having different sequences and thus coding for different proteins. This is a major form of regulation in eukaryotic cells and also occurs in some prokaryotes.:7.5
Translation is the process by which a mature mRNA molecule is used as a template for synthesizing a new protein.:6.2 Translation is carried out by ribosomes, large complexes of RNA and protein responsible for carrying out the chemical reactions to add new amino acids to a growing polypeptide chain by the formation of peptide bonds. The genetic code is read three nucleotides at a time, in units called codons, via interactions with specialized RNA molecules called transfer RNA (tRNA). Each tRNA has three unpaired bases known as the anticodon that are complementary to the codon it reads on the mRNA. The tRNA is also covalently attached to the amino acid specified by the complementary codon. When the tRNA binds to its complementary codon in an mRNA strand, the ribosome attaches its amino acid cargo to the new polypeptide chain, which is synthesized from amino terminus to carboxyl terminus. During and after synthesis, most new proteins must folds to their active three-dimensional structure before they can carry out their cellular functions.:3
Genes are regulated so that they are expressed only when the product is needed, since expression draws on limited resources.:7 A cell regulates its gene expression depending on its external environment (e.g. available nutrients, temperature and other stresses), its internal environment (e.g. cell division cycle, metabolism, infection status), and its specific role if in a multicellular organism. Gene expression can be regulated at any step: from transcriptional initiation, to RNA processing, to post-translational modification of the protein. The regulation of lactose metabolism genes in E. coli (lac operon) was the first such mechanism to be described in 1961.
A typical protein-coding gene is first copied into RNA as an intermediate in the manufacture of the final protein product.:6.1 In other cases, the RNA molecules are the actual functional products, as in the synthesis of ribosomal RNA and transfer RNA. Some RNAs known as ribozymes are capable of enzymatic function, and microRNA has a regulatory role. The DNA sequences from which such RNAs are transcribed are known as non-coding RNA genes.
Some viruses store their entire genomes in the form of RNA, and contain no DNA at all. Because they use RNA to store genes, their cellular hosts may synthesize their proteins as soon as they are infected and without the delay in waiting for transcription. On the other hand, RNA retroviruses, such as HIV, require the reverse transcription of their genome from RNA into DNA before their proteins can be synthesized. RNA-mediated epigenetic inheritance has also been observed in plants and very rarely in animals.
Organisms inherit their genes from their parents. Asexual organisms simply inherit a complete copy of their parent's genome. Sexual organisms have two copies of each chromosome because they inherit one complete set from each parent.:1
According to Mendelian inheritance, variations in an organism's phenotype (observable physical and behavioral characteristics) are due in part to variations in its genotype (particular set of genes). Each gene specifies a particular trait with different sequence of a gene (alleles) giving rise to different phenotypes. Most eukaryotic organisms (such as the pea plants Mendel worked on) have two alleles for each trait, one inherited from each parent.:20
Alleles at a locus may be dominant or recessive; dominant alleles give rise to their corresponding phenotypes when paired with any other allele for the same trait, whereas recessive alleles give rise to their corresponding phenotype only when paired with another copy of the same allele. For example, if the allele specifying tall stems in pea plants is dominant over the allele specifying short stems, then pea plants that inherit one tall allele from one parent and one short allele from the other parent will also have tall stems. Mendel's work demonstrated that alleles assort independently in the production of gametes, or germ cells, ensuring variation in the next generation. Although Mendelian inheritance remains a good model for many traits determined by single genes (including a number of well-known genetic disorders) it does not include the physical processes of DNA replication and cell division.
The growth, development, and reproduction of organisms relies on cell division, or the process by which a single cell divides into two usually identical daughter cells. This requires first making a duplicate copy of every gene in the genome in a process called DNA replication.:5.2 The copies are made by specialized enzymes known as DNA polymerases, which "read" one strand of the double-helical DNA, known as the template strand, and synthesize a new complementary strand. Because the DNA double helix is held together by base pairing, the sequence of one strand completely specifies the sequence of its complement; hence only one strand needs to be read by the enzyme to produce a faithful copy. The process of DNA replication is semiconservative; that is, the copy of the genome inherited by each daughter cell contains one original and one newly synthesized strand of DNA.:5.2
After DNA replication is complete, the cell must physically separate the two copies of the genome and divide into two distinct membrane-bound cells.:18.2 In prokaryotes (bacteria and archaea) this usually occurs via a relatively simple process called binary fission, in which each circular genome attaches to the cell membrane and is separated into the daughter cells as the membrane invaginates to split the cytoplasm into two membrane-bound portions. Binary fission is extremely fast compared to the rates of cell division in eukaryotes. Eukaryotic cell division is a more complex process known as the cell cycle; DNA replication occurs during a phase of this cycle known as S phase, whereas the process of segregating chromosomes and splitting the cytoplasm occurs during M phase.:18.1
The duplication and transmission of genetic material from one generation of cells to the next is the basis for molecular inheritance, and the link between the classical and molecular pictures of genes. Organisms inherit the characteristics of their parents because the cells of the offspring contain copies of the genes in their parents' cells. In asexually reproducing organisms, the offspring will be a genetic copy or clone of the parent organism. In sexually reproducing organisms, a specialized form of cell division called meiosis produces cells called gametes or germ cells that are haploid, or contain only one copy of each gene.:20.2 The gametes produced by females are called eggs or ova, and those produced by males are called sperm. Two gametes fuse to form a diploid fertilized egg, a single cell that has two sets of genes, with one copy of each gene from the mother and one from the father.:20
During the process of meiotic cell division, an event called genetic recombination or crossing-over can sometimes occur, in which a length of DNA on one chromatid is swapped with a length of DNA on the corresponding sister chromatid. This has no effect if the alleles on the chromatids are the same, but results in reassortment of otherwise linked alleles if they are different.:5.5 The Mendelian principle of independent assortment asserts that each of a parent's two genes for each trait will sort independently into gametes; which allele an organism inherits for one trait is unrelated to which allele it inherits for another trait. This is in fact only true for genes that do not reside on the same chromosome, or are located very far from one another on the same chromosome. The closer two genes lie on the same chromosome, the more closely they will be associated in gametes and the more often they will appear together; genes that are very close are essentially never separated because it is extremely unlikely that a crossover point will occur between them. This is known as genetic linkage.
DNA replication is for the most part extremely accurate, however errors (mutations) do occur.:7.6 The error rate in eukaryotic cells can be as low as 10−8 per nucleotide per replication, whereas for some RNA viruses it can be as high as 10−3. This means that each generation, each human genome accumulates 1–2 new mutations. Small mutations can be caused by DNA replication and the aftermath of DNA damage and include point mutations in which a single base is altered and frameshift mutations in which a single base is inserted or deleted. Either of these mutations can change the gene by missense (change a codon to encode a different amino acid) or nonsense (a premature stop codon). Larger mutations can be caused by errors in recombination to cause chromosomal abnormalities including the duplication, deletion, rearrangement or inversion of large sections of a chromosome. Additionally, the DNA repair mechanisms that normally revert mutations can introduce errors when repairing the physical damage to the molecule is more important than restoring an exact copy, for example when repairing double-strand breaks.:5.4
When multiple different alleles for a gene are present in a species's population it is called polymorphic. Most different alleles are functionally equivalent, however some alleles can give rise to different phenotypic traits. A gene's most common allele is called the wild type, and rare alleles are called mutants. The genetic variation in relative frequencies of different alleles in a population is due to both natural selection and genetic drift. The wild-type allele is not necessarily the ancestor of less common alleles, nor is it necessarily fitter.
Most mutations within genes are neutral, having no effect on the organism's phenotype (silent mutations). Some mutations do not change the amino acid sequence because multiple codons encode the same amino acid (synonymous mutations). Other mutations can be neutral if they lead to amino acid sequence changes, but the protein still functions similarly with the new amino acid (e.g. conservative mutations). Many mutations, however, are deleterious or even lethal, and are removed from populations by natural selection. Genetic disorders are the result of deleterious mutations and can be due to spontaneous mutation in the affected individual, or can be inherited. Finally, a small fraction of mutations are beneficial, improving the organism's fitness and are extremely important for evolution, since their directional selection leads to adaptive evolution.:7.6
Genes with a most recent common ancestor, and thus a shared evolutionary ancestry, are known as homologs. These genes appear either from gene duplication within an organism's genome, where they are known as paralogous genes, or are the result of divergence of the genes after a speciation event, where they are known as orthologous genes,:7.6 and often perform the same or similar functions in related organisms. It is often assumed that the functions of orthologous genes are more similar than those of paralogous genes, although the difference is minimal.
The relationship between genes can be measured by comparing the sequence alignment of their DNA.:7.6 The degree of sequence similarity between homologous genes is called conserved sequence. Most changes to a gene's sequence do not affect its function and so genes accumulate mutations over time by neutral molecular evolution. Additionally, any selection on a gene will cause its sequence to diverge at a different rate. Genes under stabilizing selection are constrained and so change more slowly whereas genes under directional selection change sequence more rapidly. The sequence differences between genes can be used for phylogenetic analyses to study how those genes have evolved and how the organisms they come from are related.
The most common source of new genes in eukaryotic lineages is gene duplication, which creates copy number variation of an existing gene in the genome. The resulting genes (paralogs) may then diverge in sequence and in function. Sets of genes formed in this way comprise a gene family. Gene duplications and losses within a family are common and represent a major source of evolutionary biodiversity. Sometimes, gene duplication may result in a nonfunctional copy of a gene, or a functional copy may be subject to mutations that result in loss of function; such nonfunctional genes are called pseudogenes.:7.6
De novo or "orphan" genes, whose sequence shows no similarity to existing genes, are extremely rare. Estimates of the number of de novo genes in the human genome range from 18 to 60. Such genes are typically shorter and simpler in structure than most eukaryotic genes, with few if any introns. Two primary sources of orphan protein-coding genes are gene duplication followed by extremely rapid sequence change, such that the original relationship is undetectable by sequence comparisons, and formation through mutation of "cryptic" transcription start sites that introduce a new open reading frame in a region of the genome that did not previously code for a protein.
Horizontal gene transfer refers to the transfer of genetic material through a mechanism other than reproduction. This mechanism is a common source of new genes in prokaryotes, sometimes thought to contribute more to genetic variation than gene duplication. It is a common means of spreading antibiotic resistance, virulence, and adaptive metabolic functions. Although horizontal gene transfer is rare in eukaryotes, likely examples have been identified of protist and alga genomes containing genes of bacterial origin.
The genome size, and the number of genes it encodes varies widely between organisms. The smallest genomes occur in viruses (which can have as few as 2 protein-coding genes), and viroids (which act as a single non-coding RNA gene). Conversely, plants can have extremely large genomes, with rice containing >46,000 protein-coding genes. The total number of protein-coding genes (the Earth's proteome) is estimated to be 5 million sequences.
Although the number of base-pairs of DNA in the human genome has been known since the 1960s, the estimated number of genes has changed over time as definitions of genes, and methods of detecting them have been refined. Initial theoretical predictions of the number of human genes were as high as 2,000,000. Early experimental measures indicated there to be 50,000–100,000 transcribed genes (expressed sequence tags). Subsequently, the sequencing in the Human Genome Project indicated that many of these transcripts were alternative variants of the same genes, and the total number of protein-coding genes was revised down to ~20,000 with 13 genes encoded on the mitochondrial genome. Of the human genome, only 1–2% consists of protein-coding genes, with the remainder being 'noncoding' DNA such as introns, retrotransposons, and noncoding RNAs.
Essential genes are the set of genes thought to be critical for an organism's survival. This definition assumes the abundant availability of all relevant nutrients and the absence of environmental stress. Only a small portion of an organism's genes are essential. In bacteria, an estimated 250–400 genes are essential for Escherichia coli and Bacillus subtilis, which is less than 10% of their genes. Half of these genes are orthologs in both organisms and are largely involved in protein synthesis. In the budding yeast Saccharomyces cerevisiae the number of essential genes is slightly higher, at 1000 genes (~20% of their genes). Although the number is more difficult to measure in higher eukaryotes, mice and humans are estimated to have around 2000 essential genes (~10% of their genes).
Housekeeping genes are critical for carrying out basic cell functions and so are expressed at a relatively constant level (constitutively). Since their expression is constant, housekeeping genes are used as experimental controls when analysing gene expression. Not all essential genes are housekeeping genes since some essential genes are developmentally regulated or expressed at certain times during the organism's life cycle.
Gene nomenclature has been established by the HUGO Gene Nomenclature Committee (HGNC) for each known human gene in the form of an approved gene name and symbol (short-form abbreviation), which can be accessed through a database maintained by HGNC. Symbols are chosen to be unique, and each gene has only one symbol (although approved symbols sometimes change). Symbols are preferably kept consistent with other members of a gene family and with homologs in other species, particularly the mouse due to its role as a common model organism.
Genetic engineering is the modification of an organism's genome through biotechnology. Since the 1970s, a variety of techniques have been developed to specifically add, remove and edit genes in an organism. Recently developed genome engineering techniques use engineered nuclease enzymes to create targeted DNA repair in a chromosome to either disrupt or edit a gene when the break is repaired. The related term synthetic biology is sometimes used to refer to extensive genetic engineering of an organism.
Genetic engineering is now a routine research tool with model organisms. For example, genes are easily added to bacteria and lineages of knockout mice with a specific gene's function disrupted are used to investigate that gene's function. Many organisms have been genetically modified for applications in agriculture, industrial biotechnology, and medicine.
For multicellular organisms, typically the embryo is engineered which grows into the adult genetically modified organism. However, the genomes of cells in an adult organism can be edited using gene therapy techniques to treat genetic diseases.
This article covers numbered east-west streets in Manhattan, New York City. Major streets have their own linked articles; minor streets are discussed here. The streets do not run exactly east–west, because the grid plan is aligned with the Hudson River rather than with the cardinal directions. "West" is approximately 29 degrees north of true west.
The numbered streets carry crosstown traffic. In general, even-numbered streets are one-way eastbound and odd-numbered streets are one-way west. Several exceptions reverse this. Most wider streets carry two-way traffic, as do a few of the narrow ones.
Streets' names change from West to East (for instance, East 10th Street to West 10th Street) at Broadway below 8th Street, and at Fifth Avenue from 8th Street and above.
Although the numbered streets begin just north of East Houston Street in the East Village, they generally do not extend west into Greenwich Village, which already had streets when the grid plan was laid out by the Commissioners' Plan of 1811. Streets that do continue farther west change direction before reaching the Hudson River. The grid covers the length of the island from 14th Street north.
220th Street is the highest numbered street on Manhattan Island. Marble Hill is also within the borough of Manhattan, so the highest street number in the borough is 228th Street. However, the numbering continues in the Bronx up to 263rd Street. The lowest number is East First Street—which runs in Alphabet City near East Houston Street—as well as First Place in Battery Park City.
East 1st Street begins just North of East Houston Street at Avenue A and continues to Bowery. Peretz Square, a small triangular sliver park where Houston Street, First Street and First Avenue meet marks the spot where the grid takes hold.
East 2nd Street begins just North of East Houston Street at Avenue C and also continues to Bowery. The East end of East 3rd, 4th, 5th, and 7th Streets is Avenue D, with East 6th Street continuing further Eastward and connecting to FDR Drive.
The west end of these streets is Bowery and Third Avenue, except for 3rd Street (formerly Amity Place; to Sixth Avenue) and 4th Street (to 13th Street), which extend west and north, respectively, into Greenwich Village. Great Jones Street connects East 3rd to West 3rd.
East 5th Street goes west to Cooper Square, but is interrupted between Avenues B and C by The Earth School, Public School 364, and between First Avenue and Avenue A by the Village View Apartments.
8th and 9th Streets run parallel to each other, beginning at Avenue D, interrupted by Tompkins Square Park at Avenue B, resuming at Avenue A and continuing to Sixth Avenue. West 8th Street is an important local shopping street. 8th Street between Avenue A and Third Avenue is called St Mark's Place, but it is counted in the length below.
10th Street (40°44′03″N 74°00′11″W﻿ / ﻿40.7342580°N 74.0029670°W﻿ / 40.7342580; -74.0029670) begins at the FDR Drive and Avenue C. West of Sixth Avenue, it turns southward about 40 degrees to join the Greenwich Village street grid and continue to West Street on the Hudson River. Because West 4th Street turns northward at Sixth Avenue, it intersects 10th, 11th and 12th and 13th Streets in the West Village. The M8 bus operates on 10th Street in both directions between Avenue D and Avenue A, and eastbound between West Street and Sixth Avenue. 10th Street has an eastbound bike lane from West Street to the East River. In 2009, the two-way section of 10th Street between Avenue A and the East River had bicycle markings and sharrows installed, but it still has no dedicated bike lane. West 10th Street was previously named Amos Street for Richard Amos. The end of West 10th Street toward the Hudson River was once the home of Newgate Prison, New York City's first prison and the United States' second.
11th Street is in two parts. It is interrupted by the block containing Grace Church between Broadway and Fourth Avenue. East 11th streets runs from Fourth Avenue to Avenue C and runs past Webster Hall. West 11th Street runs from Broadway to West Street. 11th Street and 6th Avenue was the location of the Old Grapevine tavern from the 1700s to its demolition in the early 20th century.
13th Street is in three parts. The first is a dead end from Avenue C. The second starts at a dead end, just before Avenue B, and runs to Greenwich Avenue, and the third part is from Eighth Avenue to Tenth Avenue.
14th Street is a main numbered street in Manhattan. It begins at Avenue C and ends at West Street. Its length is 3.4 km (2.1 mi). It has six subway stations:
15th Street starts at FDR Drive, and 16th Street starts at a dead end half way between FDR Drive and Avenue C. They are both stopped at Avenue C and continue from First Avenue to West Street, stopped again at Union Square, and 16th Street also pauses at Stuyvesant Square.
On 17th Street (40°44′08″N 73°59′12″W﻿ / ﻿40.735532°N 73.986575°W﻿ / 40.735532; -73.986575), traffic runs one way along the street, from east to west excepting the stretch between Broadway and Park Avenue South, where traffic runs in both directions. It forms the northern borders of both Union Square (between Broadway and Park Avenue South) and Stuyvesant Square. Composer Antonín Dvořák's New York home was located at 327 East 17th Street, near Perlman Place. The house was razed by Beth Israel Medical Center after it received approval of a 1991 application to demolish the house and replace it with an AIDS hospice. Time Magazine was started at 141 East 17th Street.
18th Street has a local subway station at the crossing with Seventh Avenue, served by the 1 2 trains on the IRT Broadway – Seventh Avenue Line. There used to be an 18th Street station on the IRT Lexington Avenue Line at the crossing with Park Avenue South.
20th Street starts at Avenue C, and 21st and 22nd Streets begin at First Avenue. They all end at Eleventh Avenue. Travel on the last block of the 20th, 21st and 22nd Streets, between Tenth and Eleventh Avenues, is in the opposite direction than it is on the rest of the respective street. 20th Street is very wide from the Avenue C to First Avenue.
Between Second and Third Avenues, 21st Street is alternatively known as Police Officer Anthony Sanchez Way. Along the northern perimeter of Gramercy Park, between Gramercy Park East and Gramercy Park West, 21st Street is known as Gramercy Park North.
23rd Street is another main numbered street in Manhattan. It begins at FDR Drive and ends at Eleventh Avenue. Its length is 3.1 km/1.9m. It has two-way travel. On 23rd Street there are five local subway stations:
24th Street is in two parts. 24th Street starts at First Avenue and it ends at Madison Avenue, because of Madison Square Park. 25th Street, which is in three parts, starts at FDR Drive, is a pedestrian plaza between Third Avenue and Lexington Avenue, and ends at Madison. Then West 24th and 25th Streets continue from Fifth Avenue to Eleventh Avenue (25th) or Twelfth Avenue (24th).
27th Street is a one-way street runs from Second Avenue to the West Side Highway with an interruption between Eighth Avenue and Tenth Avenue. It is most noted for its strip between Tenth and Eleventh Avenues, known as Club Row because it features numerous nightclubs and lounges.
In recent years, the nightclubs on West 27th Street have succumbed to stiff competition from Manhattan's Meatpacking District about fifteen blocks south, and other venues in downtown Manhattan.
Heading east, 27th Street passes through Chelsea Park between Tenth and Ninth Avenues, with the Fashion Institute of Technology (FIT) on the corner of Eighth. On Madison Avenue between 26th and 27th streets, on the site of the old Madison Square Garden, is the New York Life Building, built in 1928 and designed by Cass Gilbert, with a square tower topped by a striking gilded pyramid. Twenty-Seventh Street passes one block north of Madison Square Park and culminates at Bellevue Hospital Center on First Avenue.
31st Street begins on the West Side at the West Side Yard, while 32nd Street, which includes a segment officially known as Korea Way between Fifth Avenue and Broadway in Manhattan's Koreatown, begins at the entrance to Penn Station and Madison Square Garden. On the East Side, both streets end at Second Avenue at Kips Bay Towers and NYU Medical Center which occupy the area between 30th and 34th Streets. The Catholic church of St. Francis of Assisi is situated at 135–139 West 31st Street. At 210 West is the Capuchin Monastery of St. John the Baptist, part of St. John the Baptist Church on 30th Street. At the corner of Broadway and West 31st Street is the Grand Hotel. The former Hotel Pierrepont was located at 43 West 32nd Street, The Continental NYC tower is at the corner of Sixth Avenue and 32nd Street. 29 East 32nd Street was the location of the first building owned by the Grolier Club between 1890 and 1917.
35th Street runs from FDR Drive to Eleventh Avenue. Notable locations include East River Ferry, LaptopMD headquarters, Mercy College Manhattan Campus, and Jacob K. Javits Convention Center.
A section of East 58th Street 40°45′40.3″N 73°57′56.9″W﻿ / ﻿40.761194°N 73.965806°W﻿ / 40.761194; -73.965806 between Lexington and Second Avenues is known as Designers' Way and features a number of high end interior design and decoration establishments, including
90th Street is split into two segments. The first segment, West 90th Street begins at Riverside Drive and ends at Central Park West or West Drive, when it is open, in Central Park on the Upper West Side. The second segment of East 90th Street begins at East Drive, at Engineers Gate of Central Park. When East Drive is closed, East 90th Street begins at Fifth Avenue on the Upper East Side and curves to the right at the FDR Drive becoming East End Avenue. Our Lady of Good Counsel Church, is located on East 90th Street between Third Avenue and Second Avenue, across the street from Ruppert Towers (1601 and 1619 Third Avenue) and Ruppert Park. Asphalt Green, which is located on East 90th Street between York Avenue and East End Avenue.
112th Street starts in Morningside Heights and runs from Riverside Drive to Amsterdam Avenue, where it meets the steps of the Cathedral of Saint John the Divine. The street resumes at the eastern edge of Morningside Park and extends through Harlem before ending at First Avenue adjacent Thomas Jefferson Park in East Harlem. Notable locations include:
114th Street marks the southern boundary of Columbia University’s Morningside Heights Campus and is the location of Butler Library, which is the University’s largest.
Above 114th Street between Amsterdam Avenue and Morningside Drive, there is a private indoor pedestrian bridge connecting two buildings on the campus of St. Luke's–Roosevelt Hospital Center.
40°48′27″N 73°57′18″W﻿ / ﻿40.8076°N 73.9549°W﻿ / 40.8076; -73.9549 120th Street traverses the neighborhoods of Morningside Heights, Harlem, and Spanish Harlem. It begins on Riverside Drive at the Interchurch Center. It then runs east between the campuses of Barnard College and the Union Theological Seminary, then crosses Broadway and runs between the campuses of Columbia University and Teacher's College. The street is interrupted by Morningside Park. It then continues east, eventually running along the southern edge of Marcus Garvey Park, passing by 58 West, the former residence of Maya Angelou. It then continues through Spanish Harlem; when it crosses Pleasant Avenue it becomes a two‑way street and continues nearly to the East River, where for automobiles, it turns north and becomes Paladino Avenue, and for pedestrians, continues as a bridge across FDR Drive.
40°48′32″N 73°57′14″W﻿ / ﻿40.8088°N 73.9540°W﻿ / 40.8088; -73.9540 122nd Street is divided into three noncontiguous segments, E 122nd Street, W 122nd Street, and W 122nd Street Seminary Row, by Marcus Garvey Memorial Park and Morningside Park.
E 122nd Street runs four blocks (2,250 feet (690 m)) west from the intersection of Second Avenue and terminates at the intersection of Madison Avenue at Marcus Garvey Memorial Park. This segment runs in East Harlem and crosses portions of Third Avenue, Lexington, and Park (Fourth Avenue).
W 122nd Street runs six blocks (3,280 feet (1,000 m)) west from the intersection of Mount Morris Park West at Marcus Garvey Memorial Park and terminates at the intersection of Morningside Avenue at Morningside Park. This segment runs in the Mount Morris Historical District and crosses portions of Lenox Avenue (Sixth Avenue), Seventh Avenue, Frederick Douglass Boulevard (Eighth Avenue), and Manhattan Avenue.
W 122nd Street Seminary Row runs three blocks (1,500 feet (460 m)) west from the intersection of Amsterdam Avenue (Tenth Avenue) and terminates at the intersection of Riverside Drive. East of Amsterdam, Seminary Row bends south along Morningside Park and is resigned as Morningside Drive (Ninth Avenue). Seminary row runs in Morningside Heights, the district surrounding Columbia University, and crosses portions of Broadway and Claremont Avenue.
Seminary Row is named for the Union Theological Seminary and the Jewish Theological Seminary which it touches. Seminary Row also runs by the Manhattan School of Music, Riverside Church, Sakura Park, Grant's Tomb, and Morningside Park.
122nd Street is mentioned in the movie Taxi Driver by main character Travis Bickle as the location where a fellow cab driver is assaulted with a knife. The street and the surrounding neighborhood of Harlem is then referred to as "Mau Mau Land" by another character named Wizard, slang indicating it is a majority black area.
40°48′47″N 73°57′27″W﻿ / ﻿40.813°N 73.9575°W﻿ / 40.813; -73.9575 La Salle Street is a street in West Harlem that runs just two blocks between Amsterdam Avenue and Claremont Avenue. West of Convent Avenue, 125th Street was re-routed onto the old Manhattan Avenue. The original 125th Street west of Convent Avenue was swallowed up to make the super-blocks where the low income housing projects now exist. La Salle Street is the only vestige of the original routing.
40°48′52″N 73°56′53″W﻿ / ﻿40.814583°N 73.947944°W﻿ / 40.814583; -73.947944 132nd Street runs east-west above Central Park and is located in Harlem just south of Hamilton Heights. The main portion of 132nd Street runs eastbound from Frederick Douglass Boulevard to northern end of Park Avenue where there is a southbound exit from/entrance to the Harlem River Drive. After an interruption from St. Nicholas Park and City College, there is another small stretch of West 132nd Street between Broadway and Twelfth Avenue
The 132nd Street Community Garden is located on 132nd Street between Adam Clayton Powell Jr. Boulevard and Malcolm X Boulevard. In 1997, the lot received a garden makeover; the Borough President's office funded the installation of a $100,000 water distribution system that keeps the wide variety of trees green. The garden also holds a goldfish pond and several benches. The spirit of the neighborhood lives in gardens like this one, planted and tended by local residents.
The Manhattanville Bus Depot (formerly known as the 132nd Street Bus Depot) is located on West 132nd and 133rd Street between Broadway and Riverside Drive in the Manhattanville neighborhood.
155th Street is a major crosstown street considered to form the boundary between Harlem and Washington Heights. It is the northernmost of the 155 crosstown streets mapped out in the Commissioner's Plan of 1811 that established the numbered street grid in Manhattan.
155th Street starts on the West Side at Riverside Drive, crossing Broadway, Amsterdam Avenue and St. Nicholas Avenue. At St. Nicholas Place, the terrain drops off steeply, and 155th Street is carried on a 1,600-foot (490 m) long viaduct, a City Landmark constructed in 1893, that slopes down towards the Harlem River, continuing onto the Macombs Dam Bridge, crossing over (but not intersecting with) the Harlem River Drive. A separate, unconnected section of 155th Street runs under the viaduct, connecting Bradhurst Avenue and the Harlem River Drive.
181st Street is a major thoroughfare running through the Washington Heights neighborhood. It runs from the Washington Bridge in the east, to the Henry Hudson Parkway in the west, near the George Washington Bridge and the Hudson River. The west end is called Plaza Lafayette.
West of Fort Washington Avenue, 181st Street is largely residential, bordering Hudson Heights and having a few shops to serve the local residents. East of Fort Washington Avenue, the street becomes increasingly commercial, becoming dominated entirely by retail stores where the street reaches Broadway and continues as such until reaching the Harlem River. It is the area's major shopping district.
181st Street is served by two New York City Subway lines; there is a 181st Street station at Fort Washington Avenue on the IND Eighth Avenue Line (A trains) and a 181st Street station at St. Nicholas Avenue on the IRT Broadway – Seventh Avenue Line (1 trains). The stations are about 500 metres (550 yd) from each other and are not connected. The George Washington Bridge Bus Terminal is a couple of blocks south on Fort Washington Avenue. 181st Street is also the last south/west exit in New York on the Trans-Manhattan Expressway (I-95), just before crossing the George Washington Bridge to New Jersey.
187th Street crosses Washington Heights and running from Laurel Hill Terrace in the east to Chittenden Avenue in the west near the George Washington Bridge and Hudson River. The street is interrupted by a long set of stairs east of Fort Washington Avenue leading to the Broadway valley. West of there, it is mostly lined with store fronts and serves as a main shopping district for the Hudson Heights neighborhood.
187th Street intersects with, from East to West, Laurel Hill Terrace, Amsterdam Avenue, Audubon Avenue, St. Nicholas Avenue, Wadsworth Avenue, Broadway, Bennett Avenue, Overlook Terrace, Fort Washington Avenue, Pinehurst Avenue, Cabrini Boulevard and Chittenden Avenue.
The many institutions on 187th Street include Mount Sinai Jewish Center, the Dombrov Shtiebel, and the uptown campus of Yeshiva University. The local public elementary school P.S. 187 is located on Cabrini Boulevard, just north of the eponymous 187th Street 
Greece is a developed country with an economy based on the service (82.8%) and industrial sectors (13.3%). The agricultural sector contributed 3.9% of national economic output in 2015. Important Greek industries include tourism and shipping. With 18 million international tourists in 2013, Greece was the 7th most visited country in the European Union and 16th in the world. The Greek Merchant Navy is the largest in the world, with Greek-owned vessels accounting for 15% of global deadweight tonnage as of 2013. The increased demand for international maritime transportation between Greece and Asia has resulted in unprecedented investment in the shipping industry.
The country is a significant agricultural producer within the EU. Greece has the largest economy in the Balkans and is as an important regional investor. Greece was the largest foreign investor in Albania in 2013, the third in Bulgaria, in the top-three in Romania and Serbia and the most important trading partner and largest foreign investor in the former Yugoslav Republic of Macedonia. The Greek telecommunications company OTE has become a strong investor in former Yugoslavia and in other Balkan countries.
Greece is classified as an advanced, high-income economy, and was a founding member of the Organisation for Economic Co-operation and Development (OECD) and of the Organization of the Black Sea Economic Cooperation (BSEC). The country joined what is now the European Union in 1981. In 2001 Greece adopted the euro as its currency, replacing the Greek drachma at an exchange rate of 340.75 drachmae per euro. Greece is a member of the International Monetary Fund and of the World Trade Organization, and ranked 34th on Ernst & Young's Globalization Index 2011.
World War II (1939-1945) devastated the country's economy, but the high levels of economic growth that followed from 1950 to 1980 have been called the Greek economic miracle. From 2000 Greece saw high levels of GDP growth above the Eurozone average, peaking at 5.8% in 2003 and 5.7% in 2006. The subsequent Great Recession and Greek government-debt crisis, a central focus of the wider European debt crisis, plunged the economy into a sharp downturn, with real GDP growth rates of −0.3% in 2008, −4.3% in 2009, −5.5% in 2010, −9.1% in 2011, −7.3% in 2012 and −3.2% in 2013. In 2011, the country's public debt reached €356 billion (172% of nominal GDP). After negotiating the biggest debt restructuring in history with the private sector, Greece reduced its sovereign debt burden to €280 billion (137% of GDP) in the first quarter of 2012. Greece achieved a real GDP growth rate of 0.7% in 2014 after 6 years of economic decline, but fell back into recession in 2015.
The evolution of the Greek economy during the 19th century (a period that transformed a large part of the world because of the Industrial Revolution) has been little researched. Recent research from 2006 examines the gradual development of industry and further development of shipping in a predominantly agricultural economy, calculating an average rate of per capita GDP growth between 1833 and 1911 that was only slightly lower than that of the other Western European nations. Industrial activity, (including heavy industry like shipbuilding) was evident, mainly in Ermoupolis and Piraeus. Nonetheless, Greece faced economic hardships and defaulted on its external loans in 1826, 1843, 1860 and 1894.
After fourteen consecutive years of economic growth, Greece went into recession in 2008. By the end of 2009, the Greek economy faced the highest budget deficit and government debt-to-GDP ratios in the EU. After several upward revisions, the 2009 budget deficit is now estimated at 15.7% of GDP. This, combined with rapidly rising debt levels (127.9% of GDP in 2009) led to a precipitous increase in borrowing costs, effectively shutting Greece out of the global financial markets and resulting in a severe economic crisis.
Greece was accused of trying to cover up the extent of its massive budget deficit in the wake of the global financial crisis. The allegation was prompted by the massive revision of the 2009 budget deficit forecast by the new PASOK government elected in October 2009, from "6–8%" (estimated by the previous New Democracy government) to 12.7% (later revised to 15.7%). However, the accuracy of the revised figures has also been questioned, and in February 2012 the Hellenic Parliament voted in favor of an official investigation following accusations by a former member of the Hellenic Statistical Authority that the deficit had been artificially inflated in order to justify harsher austerity measures.
Most of the differences in the revised budget deficit numbers were due to a temporary change of accounting practices by the new government, i.e., recording expenses when military material was ordered rather than received. However, it was the retroactive application of ESA95 methodology (applied since 2000) by Eurostat, that finally raised the reference year (1999) budget deficit to 3.38% of GDP, thus exceeding the 3% limit. This led to claims that Greece (similar claims have been made about other European countries like Italy) had not actually met all five accession criteria, and the common perception that Greece entered the Eurozone through "falsified" deficit numbers.
In the 2005 OECD report for Greece, it was clearly stated that "the impact of new accounting rules on the fiscal figures for the years 1997 to 1999 ranged from 0.7 to 1 percentage point of GDP; this retroactive change of methodology was responsible for the revised deficit exceeding 3% in 1999, the year of [Greece's] EMU membership qualification". The above led the Greek minister of finance to clarify that the 1999 budget deficit was below the prescribed 3% limit when calculated with the ESA79 methodology in force at the time of Greece's application, and thus the criteria had been met.
An error sometimes made is the confusion of discussion regarding Greece’s Eurozone entry with the controversy regarding usage of derivatives’ deals with U.S. Banks by Greece and other Eurozone countries to artificially reduce their reported budget deficits. A currency swap arranged with Goldman Sachs allowed Greece to "hide" 2.8 billion Euros of debt, however, this affected deficit values after 2001 (when Greece had already been admitted into the Eurozone) and is not related to Greece’s Eurozone entry.
According to Der Spiegel, credits given to European governments were disguised as "swaps" and consequently did not get registered as debt because Eurostat at the time ignored statistics involving financial derivatives. A German derivatives dealer had commented to Der Spiegel that "The Maastricht rules can be circumvented quite legally through swaps," and "In previous years, Italy used a similar trick to mask its true debt with the help of a different US bank." These conditions had enabled Greek as well as many other European governments to spend beyond their means, while meeting the deficit targets of the European Union and the monetary union guidelines. In May 2010, the Greek government deficit was again revised and estimated to be 13.6% which was the second highest in the world relative to GDP with Iceland in first place at 15.7% and Great Britain third with 12.6%. Public debt was forecast, according to some estimates, to hit 120% of GDP during 2010.
As a consequence, there was a crisis in international confidence in Greece's ability to repay its sovereign debt, as reflected by the rise of the country's borrowing rates (although their slow rise – the 10-year government bond yield only exceeded 7% in April 2010 – coinciding with a large number of negative articles, has led to arguments about the role of international news media in the evolution of the crisis). In order to avert a default (as high borrowing rates effectively prohibited access to the markets), in May 2010 the other Eurozone countries, and the IMF, agreed to a "rescue package" which involved giving Greece an immediate €45 billion in bail-out loans, with more funds to follow, totaling €110 billion. In order to secure the funding, Greece was required to adopt harsh austerity measures to bring its deficit under control. Their implementation will be monitored and evaluated by the European Commission, the European Central Bank and the IMF.
Between 2005 and 2011, Greece has had the highest percentage increase in industrial output compared to 2005 levels out of all European Union members, with an increase of 6%. Eurostat statistics show that the industrial sector was hit by the Greek financial crisis throughout 2009 and 2010, with domestic output decreasing by 5.8% and industrial production in general by 13.4%. Currently, Greece is ranked third in the European Union in the production of marble (over 920,000 tons), after Italy and Spain.
Greece has the largest merchant navy in the world, accounting for more than 15% of the world's total deadweight tonnage (dwt) according to the United Nations Conference on Trade and Development. The Greek merchant navy's total dwt of nearly 245 million is comparable only to Japan's, which is ranked second with almost 224 million. Additionally, Greece represents 39.52% of all of the European Union's dwt. However, today's fleet roster is smaller than an all-time high of 5,000 ships in the late 1970s.
In terms of ship categories, Greek companies have 22.6% of the world's tankers and 16.1% of the world's bulk carriers (in dwt). An additional equivalent of 27.45% of the world's tanker dwt is on order, with another 12.7% of bulk carriers also on order. Shipping accounts for an estimated 6% of Greek GDP, employs about 160,000 people (4% of the workforce), and represents 1/3 of the country's trade deficit. Earnings from shipping amounted to €14.1 billion in 2011, while between 2000 and 2010 Greek shipping contributed a total of €140 billion (half of the country's public debt in 2009 and 3.5 times the receipts from the European Union in the period 2000–2013). The 2011 ECSA report showed that there are approximately 750 Greek shipping companies in operation.
Counting shipping as quasi-exports and in terms of monetary value, Greece ranked 4th globally in 2011 having "exported" shipping services worth 17,704.132 million $; only Denmark, Germany and South Korea ranked higher during that year. Similarly counting shipping services provided to Greece by other countries as quasi-imports and the difference between "exports" and "imports" as a "trade balance", Greece in 2011 ranked in the latter second behind Germany, having "imported" shipping services worth 7,076.605 million US$ and having run a "trade surplus" of 10,712.342 million US$.
Between 1949 and the 1980s, telephone communications in Greece were a state monopoly by the Hellenic Telecommunications Organization, better known by its acronym, OTE. Despite the liberalization of telephone communications in the country in the 1980s, OTE still dominates the Greek market in its field and has emerged as one of the largest telecommunications companies in Southeast Europe. Since 2011, the company's major shareholder is Deutsche Telekom with a 40% stake, while the Greek state continues to own 10% of the company's shares. OTE owns several subsidiaries across the Balkans, including Cosmote, Greece's top mobile telecommunications provider, Cosmote Romania and Albanian Mobile Communications.
Greece has tended to lag behind its European Union partners in terms of Internet use, with the gap closing rapidly in recent years. The percentage of households with access to the Internet more than doubled between 2006 and 2013, from 23% to 56% respectively (compared with an EU average of 49% and 79%). At the same time, there has been a massive increase in the proportion of households with a broadband connection, from 4% in 2006 to 55% in 2013 (compared with an EU average of 30% and 76%). However, Greece also has the EU's third highest percentage of people who have never used the Internet: 36% in 2013, down from 65% in 2006 (compared with an EU average of 21% and 42%).
Greece attracts more than 16 million tourists each year, thus contributing 18.2% to the nation's GDP in 2008 according to an OECD report. The same survey showed that the average tourist expenditure while in Greece was $1,073, ranking Greece 10th in the world. The number of jobs directly or indirectly related to the tourism sector were 840,000 in 2008 and represented 19% of the country's total labor force. In 2009, Greece welcomed over 19.3 million tourists, a major increase from the 17.7 million tourists the country welcomed in 2008.
In recent years a number of well-known tourism-related organizations have placed Greek destinations in the top of their lists. In 2009 Lonely Planet ranked Thessaloniki, the country's second-largest city, the world's fifth best "Ultimate Party Town", alongside cities such as Montreal and Dubai, while in 2011 the island of Santorini was voted as the best island in the world by Travel + Leisure. The neighbouring island of Mykonos was ranked as the 5th best island Europe. Thessaloniki was the European Youth Capital in 2014.
Between 1975 and 2009, Olympic Airways (known after 2003 as Olympic Airlines) was the country’s state-owned flag carrier, but financial problems led to its privatization and relaunch as Olympic Air in 2009. Both Aegean Airlines and Olympic Air have won awards for their services; in 2009 and 2011, Aegean Airlines was awarded the "Best regional airline in Europe" award by Skytrax, and also has two gold and one silver awards by the ERA, while Olympic Air holds one silver ERA award for "Airline of the Year" as well as a "Condé Nast Traveller 2011 Readers Choice Awards: Top Domestic Airline" award.
Greece's rail network is estimated to be at 2,548 km. Rail transport in Greece is operated by TrainOSE, a subsidiary of the Hellenic Railways Organization (OSE). Most of the country's network is standard gauge (1,565 km), while the country also has 983 km of narrow gauge. A total of 764 km of rail are electrified. Greece has rail connections with Bulgaria, the Republic of Macedonia and Turkey. A total of three suburban railway systems (Proastiakos) are in operation (in Athens, Thessaloniki and Patras), while one metro system is operational in Athens with another under construction.
According to Eurostat, Greece's largest port by tons of goods transported in 2010 is the port of Aghioi Theodoroi, with 17.38 million tons. The Port of Thessaloniki comes second with 15.8 million tons, followed by the Port of Piraeus, with 13.2 million tons, and the port of Eleusis, with 12.37 million tons. The total number of goods transported through Greece in 2010 amounted to 124.38 million tons, a considerable drop from the 164.3 million tons transported through the country in 2007. Since then, Piraeus has grown to become the Mediterranean's third-largest port thanks to heavy investment by Chinese logistics giant COSCO. In 2013, Piraeus was declared the fastest-growing port in the world.
In 2010 Piraeus handled 513,319 TEUs, followed by Thessaloniki, which handled 273,282 TEUs. In the same year, 83.9 million people passed through Greece's ports, 12.7 million through the port of Paloukia in Salamis, another 12.7 through the port of Perama, 9.5 million through Piraeus and 2.7 million through Igoumenitsa. In 2013, Piraeus handled a record 3.16 million TEUs, the third-largest figure in the Mediterranean, of which 2.52 million were transported through Pier II, owned by COSCO and 644,000 were transported through Pier I, owned by the Greek state.
Energy production in Greece is dominated by the Public Power Corporation (known mostly by its acronym ΔΕΗ, or in English DEI). In 2009 DEI supplied for 85.6% of all energy demand in Greece, while the number fell to 77.3% in 2010. Almost half (48%) of DEI's power output is generated using lignite, a drop from the 51.6% in 2009. Another 12% comes from Hydroelectric power plants and another 20% from natural gas. Between 2009 and 2010, independent companies' energy production increased by 56%, from 2,709 Gigawatt hour in 2009 to 4,232 GWh in 2010.
In 2008 renewable energy accounted for 8% of the country's total energy consumption, a rise from the 7.2% it accounted for in 2006, but still below the EU average of 10% in 2008. 10% of the country's renewable energy comes from solar power, while most comes from biomass and waste recycling. In line with the European Commission's Directive on Renewable Energy, Greece aims to get 18% of its energy from renewable sources by 2020. In 2013 and for several months, Greece produced more than 20% of its electricity from renewable energy sources and hydroelectric power plants. Greece currently does not have any nuclear power plants in operation, however in 2009 the Academy of Athens suggested that research in the possibility of Greek nuclear power plants begin.
In addition to the above, Greece is also to start oil and gas exploration in other locations in the Ionian Sea, as well as the Libyan Sea, within the Greek exclusive economic zone, south of Crete. The Ministry of the Environment, Energy and Climate Change announced that there was interest from various countries (including Norway and the United States) in exploration, and the first results regarding the amount of oil and gas in these locations were expected in the summer of 2012. In November 2012, a report published by Deutsche Bank estimated the value of natural gas reserves south of Crete at €427 billion.
Between 1832 and 2002 the currency of Greece was the drachma. After signing the Maastricht Treaty, Greece applied to join the eurozone. The two main convergence criteria were a maximum budget deficit of 3% of GDP and a declining public debt if it stood above 60% of GDP. Greece met the criteria as shown in its 1999 annual public account. On 1 January 2001, Greece joined the eurozone, with the adoption of the euro at the fixed exchange rate ₯340.75 to €1. However, in 2001 the euro only existed electronically, so the physical exchange from drachma to euro only took place on 1 January 2002. This was followed by a ten-year period for eligible exchange of drachma to euro, which ended on 1 March 2012.
IMF's forecast said that Greece's unemployment rate would hit the highest 14.8 percent in 2012 and decrease to 14.1 in 2014.  But in fact, the Greek economy suffered a prolonged high unemployemnt. The unemployment figure was between 9 per cent and 11 per cent in 2009, and it soared to 28 per cent in 2013. In 2015, Greece's jobless rate is around 24 per cent. It is thought that Greece's potential output has been eroded by this prolonged massive unemployment due to the associated hysteresis effects.
A prime minister is the most senior minister of cabinet in the executive branch of government, often in a parliamentary or semi-presidential system. In many systems, the prime minister selects and may dismiss other members of the cabinet, and allocates posts to members within the government. In most systems, the prime minister is the presiding member and chairman of the cabinet. In a minority of systems, notably in semi-presidential systems of government, a prime minister is the official who is appointed to manage the civil service and execute the directives of the head of state.
In parliamentary systems fashioned after the Westminster system, the prime minister is the presiding and actual head of government and head of the executive branch. In such systems, the head of state or the head of state's official representative (i.e. the monarch, president, or governor-general) usually holds a largely ceremonial position, although often with reserve powers.
The prime minister is often, but not always, a member of parliament[clarification needed] and is expected with other ministers to ensure the passage of bills through the legislature. In some monarchies the monarch may also exercise executive powers (known as the royal prerogative) that are constitutionally vested in the crown and may be exercised without the approval of parliament.
As well as being head of government, a prime minister may have other roles or titles—the Prime Minister of the United Kingdom, for example, is also First Lord of the Treasury and Minister for the Civil Service. Prime ministers may take other ministerial posts—for example, during the Second World War, Winston Churchill was also Minister of Defence (although there was then no Ministry of Defence), and in the current cabinet of Israel, Benjamin Netanyahu also serves as Minister of Communications, Foreign Affairs, Regional Cooperation, Economy and Interior
The first actual usage of the term prime minister or Premier Ministre[citation needed] was used by Cardinal Richelieu when in 1625 he was named to head the royal council as prime minister of France. Louis XIV and his descendants generally attempted to avoid giving this title to their chief ministers.
The term prime minister in the sense that we know it originated in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole. Over time, the title became honorific and remains so in the 21st century.
The monarchs of England and the United Kingdom had ministers in whom they placed special trust and who were regarded as the head of the government. Examples were Thomas Cromwell under Henry VIII; William Cecil, Lord Burghley under Elizabeth I; Clarendon under Charles II and Godolphin under Queen Anne. These ministers held a variety of formal posts, but were commonly known as "the minister", the "chief minister", the "first minister" and finally the "prime minister".
The power of these ministers depended entirely on the personal favour of the monarch. Although managing the parliament was among the necessary skills of holding high office, they did not depend on a parliamentary majority for their power. Although there was a cabinet, it was appointed entirely by the monarch, and the monarch usually presided over its meetings.
When the monarch grew tired of a first minister, he or she could be dismissed, or worse: Cromwell was executed and Clarendon driven into exile when they lost favour. Kings sometimes divided power equally between two or more ministers to prevent one minister from becoming too powerful. Late in Anne's reign, for example, the Tory ministers Harley and St John shared power.
In the mid 17th century, after the English Civil War (1642–1651), Parliament strengthened its position relative to the monarch then gained more power through the Glorious Revolution of 1688 and passage of the Bill of Rights in 1689. The monarch could no longer establish any law or impose any tax without its permission and thus the House of Commons became a part of the government. It is at this point that a modern style of prime minister begins to emerge.
A tipping point in the evolution of the prime ministership came with the death of Anne in 1714 and the accession of George I to the throne. George spoke no English, spent much of his time at his home in Hanover, and had neither knowledge of, nor interest in, the details of English government. In these circumstances it was inevitable that the king's first minister would become the de facto head of the government.
From 1721 this was the Whig politician Robert Walpole, who held office for twenty-one years. Walpole chaired cabinet meetings, appointed all the other ministers, dispensed the royal patronage and packed the House of Commons with his supporters. Under Walpole, the doctrine of cabinet solidarity developed. Walpole required that no minister other than himself have private dealings with the king, and also that when the cabinet had agreed on a policy, all ministers must defend it in public, or resign. As a later prime minister, Lord Melbourne, said, "It matters not what we say, gentlemen, so long as we all say the same thing."
Walpole always denied that he was "prime minister", and throughout the 18th century parliamentarians and legal scholars continued to deny that any such position was known to the Constitution. George II and George III made strenuous efforts to reclaim the personal power of the monarch, but the increasing complexity and expense of government meant that a minister who could command the loyalty of the Commons was increasingly necessary. The long tenure of the wartime prime minister William Pitt the Younger (1783–1801), combined with the mental illness of George III, consolidated the power of the post. The title was first referred to on government documents during the administration of Benjamin Disraeli but did not appear in the formal British Order of precedence until 1905.
By the late 20th century, the majority of the world's countries had a prime minister or equivalent minister, holding office under either a constitutional monarchy or a ceremonial president. The main exceptions to this system have been the United States and the presidential republics in Latin America modelled on the U.S. system, in which the president directly exercises executive authority.
Bahrain's prime minister, Sheikh Khalifah bin Sulman Al Khalifah has been in the post since 1970, making him the longest serving non-elected prime minister.
The post of prime minister may be encountered both in constitutional monarchies (such as Belgium, Denmark, Japan, Luxembourg, the Netherlands, Norway, Malaysia, Morocco, Spain, Sweden, Thailand, Canada, Australia, New Zealand, and the United Kingdom), and in parliamentary republics in which the head of state is an elected official (such as Finland ,the Czech Republic, France, Greece, Hungary, India, Indonesia, Ireland, Pakistan, Portugal, Montenegro, Croatia, Bulgaria, Romania, Serbia and Turkey). See also "First Minister", "Premier", "Chief Minister", "Chancellor", "Taoiseach", "Statsminister" and "Secretary of State": alternative titles usually equivalent in meaning to, or translated as, "prime minister".
This contrasts with the presidential system, in which the president (or equivalent) is both the head of state and the head of the government. In some presidential or semi-presidential systems, such as those of France, Russia or South Korea, the prime minister is an official generally appointed by the president but usually approved by the legislature and responsible for carrying out the directives of the president and managing the civil service. The head of government of the People's Republic of China is referred to as the Premier of the State Council and the premier of the Republic of China (Taiwan) is also appointed by the president, but requires no approval by the legislature.
Appointment of the prime minister of France requires no approval by the parliament either, but the parliament may force the resignation of the government. In these systems, it is possible for the president and the prime minister to be from different political parties if the legislature is controlled by a party different from that of the president. When it arises, such a state of affairs is usually referred to as (political) cohabitation.
Bangladesh's constitution clearly outlines the functions and powers of the Prime Minister, and also details the process of his/her appointment and dismissal.
The People's Republic of China constitution set a premier just one place below the National People's Congress in China. Premier read as (Simplified Chinese: 总理; pinyin: Zŏnglĭ) in Chinese.
Canada's constitution, being a 'mixed' or hybrid constitution (a constitution that is partly formally codified and partly uncodified) originally did not make any reference whatsoever to a prime minister, with her or his specific duties and method of appointment instead dictated by "convention". In the Constitution Act, 1982, passing reference to a "Prime Minister of Canada" is added, though only regarding the composition of conferences of federal and provincial first ministers.
Czech Republic's constitution clearly outlines the functions and powers of the Prime Minister of the Czech Republic, and also details the process of his/her appointment and dismissal.
The United Kingdom's constitution, being uncodified and largely unwritten, makes no mention of a prime minister. Though it had de facto existed for centuries, its first mention in official state documents did not occur until the first decade of the twentieth century. Accordingly, it is often said "not to exist", indeed there are several instances of parliament declaring this to be the case. The prime minister sits in the cabinet solely by virtue of occupying another office, either First Lord of the Treasury (office in commission), or more rarely Chancellor of the Exchequer (the last of whom was Balfour in 1905).
Most prime ministers in parliamentary systems are not appointed for a specific term in office and in effect may remain in power through a number of elections and parliaments. For example, Margaret Thatcher was only ever appointed prime minister on one occasion, in 1979. She remained continuously in power until 1990, though she used the assembly of each House of Commons after a general election to reshuffle her cabinet.
Some states, however, do have a term of office of the prime minister linked to the period in office of the parliament. Hence the Irish Taoiseach is formally 'renominated' after every general election. (Some constitutional experts have questioned whether this process is actually in keeping with the provisions of the Irish constitution, which appear to suggest that a taoiseach should remain in office, without the requirement of a renomination, unless s/he has clearly lost the general election.) The position of prime minister is normally chosen from the political party that commands majority of seats in the lower house of parliament.
In parliamentary systems, governments are generally required to have the confidence of the lower house of parliament (though a small minority of parliaments, by giving a right to block supply to upper houses, in effect make the cabinet responsible to both houses, though in reality upper houses, even when they have the power, rarely exercise it). Where they lose a vote of confidence, have a motion of no confidence passed against them, or where they lose supply, most constitutional systems require either:
The latter in effect allows the government to appeal the opposition of parliament to the electorate. However, in many jurisdictions a head of state may refuse a parliamentary dissolution, requiring the resignation of the prime minister and his or her government. In most modern parliamentary systems, the prime minister is the person who decides when to request a parliamentary dissolution.
Older constitutions often vest this power in the cabinet. In the United Kingdom, for example, the tradition whereby it is the prime minister who requests a dissolution of parliament dates back to 1918. Prior to then, it was the entire government that made the request. Similarly, though the modern 1937 Irish constitution grants to the Taoiseach the right to make the request, the earlier 1922 Irish Free State Constitution vested the power in the Executive Council (the then name for the Irish cabinet).
In Australia, the Prime Minister is expected to step down if s/he loses the majority support of his/her party under a spill motion as have many such as Tony Abbott, Julia Gillard and Kevin Rudd.
In the Russian constitution the prime minister is actually titled Chairman of the government while the Irish prime minister is called the Taoiseach (which is rendered into English as prime minister), and in Israel he is Rosh HaMemshalah meaning "head of the government". In many cases, though commonly used, "prime minister" is not the official title of the office-holder; the Spanish prime minister is the President of the Government (Presidente del Gobierno).
Other common forms include president of the council of ministers (for example in Italy, Presidente del Consiglio dei Ministri), President of the Executive Council, or Minister-President. In the Scandinavian countries the prime minister is called statsminister in the native languages (i.e. minister of state). In federations, the head of government of subnational entities such as provinces is most commonly known as the premier, chief minister, governor or minister-president.
The convention in the English language is to call nearly all national heads of government "prime minister" (sometimes modified to the equivalent term of premier), regardless of the correct title of the head of government as applied in his or her respective country. The few exceptions to the rule are Germany and Austria, whose heads of government titles are almost always translated as Chancellor; Monaco, whose head of government is referred to as the Minister of State; and Vatican City, for which the head of government is titled the Secretary of State. In the case of Ireland, the head of government is occasionally referred to as the Taoiseach by English speakers. A stand-out case is the President of Iran, who is not actually a head of state, but the head of the government of Iran. He is referred to as "president" in both the Persian and English languages.
In non-Commonwealth countries the prime minister may be entitled to the style of Excellency like a president. In some Commonwealth countries prime ministers and former prime ministers are styled Right Honourable due to their position, for example in the Prime Minister of Canada. In the United Kingdom the prime minister and former prime ministers may appear to also be styled Right Honourable, however this is not due to their position as head of government but as a privilege of being current members of Her Majesty's Most Honourable Privy Council.
In the UK, where devolved government is in place, the leaders of the Scottish, Northern Irish and Welsh Governments are styled First Minister. In India, The Prime Minister is referred to as "Pradhan Mantri", meaning "prime minister". In Pakistan, the prime minister is referred to as "Wazir-e-Azam", meaning "Grand Vizier".
The Prime Minister's executive office is usually called the Office of the Prime Minister in the case of the Canada and other Commonwealth countries, it is called Cabinet Office in United Kingdom. Some Prime Minister's office do include the role of Cabinet. In other countries, it is called the Prime Minister's Department or the Department of the Prime Minister and Cabinet as for Australia.
Strasbourg (/ˈstræzbɜːrɡ/, French pronunciation: ​[stʁaz.buʁ, stʁas.buʁ]; Alsatian: Strossburi; German: Straßburg, [ˈʃtʁaːsbʊɐ̯k]) is the capital and largest city of the Alsace-Champagne-Ardenne-Lorraine (ACAL) region in eastern France and is the official seat of the European Parliament. Located close to the border with Germany, it is the capital of the Bas-Rhin département. The city and the region of Alsace were historically predominantly Alemannic-speaking, hence the city's Germanic name. In 2013, the city proper had 275,718 inhabitants, Eurométropole de Strasbourg (Greater Strasbourg) had 475,934 inhabitants and the Arrondissement of Strasbourg had 482,384 inhabitants. With a population of 768,868 in 2012, Strasbourg's metropolitan area (only the part of the metropolitan area on French territory) is the ninth largest in France and home to 13% of the ACAL region's inhabitants. The transnational Eurodistrict Strasbourg-Ortenau had a population of 915,000 inhabitants in 2014.
Strasbourg's historic city centre, the Grande Île (Grand Island), was classified a World Heritage site by UNESCO in 1988, the first time such an honour was placed on an entire city centre. Strasbourg is immersed in the Franco-German culture and although violently disputed throughout history, has been a bridge of unity between France and Germany for centuries, especially through the University of Strasbourg, currently the second largest in France, and the coexistence of Catholic and Protestant culture. The largest Islamic place of worship in France, the Strasbourg Grand Mosque, was inaugurated by French Interior Minister Manuel Valls on 27 September 2012.
Strasbourg is situated on the eastern border of France with Germany. This border is formed by the River Rhine, which also forms the eastern border of the modern city, facing across the river to the German town Kehl. The historic core of Strasbourg however lies on the Grande Île in the River Ill, which here flows parallel to, and roughly 4 kilometres (2.5 mi) from, the Rhine. The natural courses of the two rivers eventually join some distance downstream of Strasbourg, although several artificial waterways now connect them within the city.
The Romans under Nero Claudius Drusus established a military outpost belonging to the Germania Superior Roman province at Strasbourg's current location, and named it Argentoratum. (Hence the town is commonly called Argentina in medieval Latin.) The name "Argentoratum" was first mentioned in 12 BC and the city celebrated its 2,000th birthday in 1988. "Argentorate" as the toponym of the Gaulish settlement preceded it before being Latinized, but it is not known by how long. The Roman camp was destroyed by fire and rebuilt six times between the first and the fifth centuries AD: in 70, 97, 235, 355, in the last quarter of the fourth century, and in the early years of the fifth century. It was under Trajan and after the fire of 97 that Argentoratum received its most extended and fortified shape. From the year 90 on, the Legio VIII Augusta was permanently stationed in the Roman camp of Argentoratum. It then included a cavalry section and covered an area of approximately 20 hectares. Other Roman legions temporarily stationed in Argentoratum were the Legio XIV Gemina and the Legio XXI Rapax, the latter during the reign of Nero.
The centre of Argentoratum proper was situated on the Grande Île (Cardo: current Rue du Dôme, Decumanus: current Rue des Hallebardes). The outline of the Roman "castrum" is visible in the street pattern in the Grande Ile. Many Roman artifacts have also been found along the current Route des Romains, the road that led to Argentoratum, in the suburb of Kœnigshoffen. This was where the largest burial places were situated, as well as the densest concentration of civilian dwelling places and commerces next to the camp. Among the most outstanding finds in Kœnigshoffen were (found in 1911–12) the fragments of a grand Mithraeum that had been shattered by early Christians in the fourth century. From the fourth century, Strasbourg was the seat of the Bishopric of Strasbourg (made an Archbishopric in 1988). Archaeological excavations below the current Église Saint-Étienne in 1948 and 1956 unearthed the apse of a church dating back to the late fourth or early fifth century, considered to be the oldest church in Alsace. It is supposed that this was the first seat of the Roman Catholic Diocese of Strasbourg.
In the fifth century Strasbourg was occupied successively by Alemanni, Huns, and Franks. In the ninth century it was commonly known as Strazburg in the local language, as documented in 842 by the Oaths of Strasbourg. This trilingual text contains, alongside texts in Latin and Old High German (teudisca lingua), the oldest written variety of Gallo-Romance (lingua romana) clearly distinct from Latin, the ancestor of Old French. The town was also called Stratisburgum or Strateburgus in Latin, from which later came Strossburi in Alsatian and Straßburg in Standard German, and then Strasbourg in French. The Oaths of Strasbourg is considered as marking the birth of the two countries of France and Germany with the division of the Carolingian Empire.
A revolution in 1332 resulted in a broad-based city government with participation of the guilds, and Strasbourg declared itself a free republic. The deadly bubonic plague of 1348 was followed on 14 February 1349 by one of the first and worst pogroms in pre-modern history: over a thousand Jews were publicly burnt to death, with the remainder of the Jewish population being expelled from the city. Until the end of the 18th century, Jews were forbidden to remain in town after 10 pm. The time to leave the city was signalled by a municipal herald blowing the Grüselhorn (see below, Museums, Musée historique);. A special tax, the Pflastergeld (pavement money), was furthermore to be paid for any horse that a Jew would ride or bring into the city while allowed to.
In the 1520s during the Protestant Reformation, the city, under the political guidance of Jacob Sturm von Sturmeck and the spiritual guidance of Martin Bucer embraced the religious teachings of Martin Luther. Their adherents established a Gymnasium, headed by Johannes Sturm, made into a University in the following century. The city first followed the Tetrapolitan Confession, and then the Augsburg Confession. Protestant iconoclasm caused much destruction to churches and cloisters, notwithstanding that Luther himself opposed such a practice. Strasbourg was a centre of humanist scholarship and early book-printing in the Holy Roman Empire, and its intellectual and political influence contributed much to the establishment of Protestantism as an accepted denomination in the southwest of Germany. (John Calvin spent several years as a political refugee in the city). The Strasbourg Councillor Sturm and guildmaster Matthias represented the city at the Imperial Diet of Speyer (1529), where their protest led to the schism of the Catholic Church and the evolution of Protestantism. Together with four other free cities, Strasbourg presented the confessio tetrapolitana as its Protestant book of faith at the Imperial Diet of Augsburg in 1530, where the slightly different Augsburg Confession was also handed over to Charles V, Holy Roman Emperor.
Louis' advisors believed that, as long as Strasbourg remained independent, it would endanger the King's newly annexed territories in Alsace, and, that to defend these large rural lands effectively, a garrison had to be placed in towns such as Strasbourg. Indeed, the bridge over the Rhine at Strasbourg had been used repeatedly by Imperial (Holy Roman Empire) forces, and three times during the Franco-Dutch War Strasbourg had served as a gateway for Imperial invasions into Alsace. In September 1681 Louis' forces, though lacking a clear casus belli, surrounded the city with overwhelming force. After some negotiation, Louis marched into the city unopposed on 30 September 1681 and proclaimed its annexation.
This annexation was one of the direct causes of the brief and bloody War of the Reunions whose outcome left the French in possession. The French annexation was recognized by the Treaty of Ryswick (1697). The official policy of religious intolerance which drove most Protestants from France after the revocation of the Edict of Nantes in 1685 was not applied in Strasbourg and in Alsace, because both had a special status as a province à l'instar de l'étranger effectif (a kind of foreign province of the king of France). Strasbourg Cathedral, however, was taken from the Lutherans to be returned to the Catholics as the French authorities tried to promote Catholicism wherever they could (some other historic churches remained in Protestant hands). Its language also remained overwhelmingly German: the German Lutheran university persisted until the French Revolution. Famous students included Goethe and Herder.
Strasbourg's status as a free city was revoked by the French Revolution. Enragés, most notoriously Eulogius Schneider, ruled the city with an increasingly iron hand. During this time, many churches and monasteries were either destroyed or severely damaged. The cathedral lost hundreds of its statues (later replaced by copies in the 19th century) and in April 1794, there was talk of tearing its spire down, on the grounds that it was against the principle of equality. The tower was saved, however, when in May of the same year citizens of Strasbourg crowned it with a giant tin Phrygian cap. This artifact was later kept in the historical collections of the city until it was destroyed by the Germans in 1870 during the Franco-Prussian war.
During the Franco-Prussian War and the Siege of Strasbourg, the city was heavily bombarded by the Prussian army. The bombardment of the city was meant to break the morale of the people of Strasbourg. On 24 and 26 August 1870, the Museum of Fine Arts was destroyed by fire, as was the Municipal Library housed in the Gothic former Dominican church, with its unique collection of medieval manuscripts (most famously the Hortus deliciarum), rare Renaissance books, archeological finds and historical artifacts. The gothic cathedral was damaged as well as the medieval church of Temple Neuf, the theatre, the city hall, the court of justice and many houses. At the end of the siege 10,000 inhabitants were left without shelter; over 600 died, including 261 civilians, and 3200 were injured, including 1,100 civilians.
In 1871, after the end of the war, the city was annexed to the newly established German Empire as part of the Reichsland Elsass-Lothringen under the terms of the Treaty of Frankfurt. As part of Imperial Germany, Strasbourg was rebuilt and developed on a grand and representative scale, such as the Neue Stadt, or "new city" around the present Place de la République. Historian Rodolphe Reuss and Art historian Wilhelm von Bode were in charge of rebuilding the municipal archives, libraries and museums. The University, founded in 1567 and suppressed during the French Revolution as a stronghold of German sentiment,[citation needed] was reopened in 1872 under the name Kaiser-Wilhelms-Universität.
A belt of massive fortifications was established around the city, most of which still stands today, renamed after French generals and generally classified as Monuments historiques; most notably Fort Roon (now Fort Desaix) and Fort Podbielski (now Fort Ducrot) in Mundolsheim, Fort von Moltke (now Fort Rapp) in Reichstett, Fort Bismarck (now Fort Kléber) in Wolfisheim, Fort Kronprinz (now Fort Foch) in Niederhausbergen, Fort Kronprinz von Sachsen (now Fort Joffre) in Holtzheim and Fort Großherzog von Baden (now Fort Frère) in Oberhausbergen.
Following the defeat of the German empire in World War I and the abdication of the German Emperor, some revolutionary insurgents declared Alsace-Lorraine as an independent Republic, without preliminary referendum or vote. On 11 November 1918 (Armistice Day), communist insurgents proclaimed a "soviet government" in Strasbourg, following the example of Kurt Eisner in Munich as well as other German towns. French troops commanded by French general Henri Gouraud entered triumphantly in the city on 22 November. A major street of the city now bears the name of that date (Rue du 22 Novembre) which celebrates the entry of the French in the city. Viewing the massive cheering crowd gathered under the balcony of Strasbourg's town hall, French President Raymond Poincaré stated that "the plebiscite is done".
In 1919, following the Treaty of Versailles, the city was restituted to France in accordance with U.S. President Woodrow Wilson's "Fourteen Points" without a referendum. The date of the assignment was retroactively established on Armistice Day. It is doubtful whether a referendum in Strasbourg would have ended in France's favour since the political parties striving for an autonomous Alsace or a connection to France accounted only for a small proportion of votes in the last Reichstag as well as in the local elections. The Alsatian autonomists who were pro French had won many votes in the more rural parts of the region and other towns since the annexation of the region by Germany in 1871. The movement started with the first election for the Reichstag; those elected were called "les députés protestataires", and until the fall of Bismarck in 1890, they were the only deputies elected by the Alsatians to the German parliament demanding the return of those territories to France. At the last Reichstag election in Strasbourg and its periphery, the clear winners were the Social Democrats; the city was the administrative capital of the region, was inhabited by many Germans appointed by the central government in Berlin and its flourishing economy attracted many Germans. This could explain the difference between the rural vote and the one in Strasbourg. After the war, many Germans left Strasbourg and went back to Germany; some of them were denounced by the locals or expelled by the newly appointed authorities. The Saverne Affair was vivid in the memory among the Alsatians.
Between the German invasion of Poland on 1 September 1939 and the Anglo-French declaration of War against the German Reich on 3 September 1939, the entire city (a total of 120,000 people) was evacuated, like other border towns as well. Until the arrival of the Wehrmacht troops mid-June 1940, the city was, for ten months, completely empty, with the exception of the garrisoned soldiers. The Jews of Strasbourg had been evacuated to Périgueux and Limoges, the University had been evacuated to Clermont-Ferrand.
After the ceasefire following the Fall of France in June 1940, Alsace was annexed to Germany and a rigorous policy of Germanisation was imposed upon it by the Gauleiter Robert Heinrich Wagner. When, in July 1940, the first evacuees were allowed to return, only residents of Alsatian origin were admitted. The last Jews were deported on 15 July 1940 and the main synagogue, a huge Romanesque revival building that had been a major architectural landmark with its 54-metre-high dome since its completion in 1897, was set ablaze, then razed.
In September 1940 the first Alsatian resistance movement led by Marcel Weinum called La main noire (The black hand) was created. It was composed by a group of 25 young men aged from 14 to 18 years old who led several attacks against the German occupation. The actions culminated with the attack of the Gauleiter Robert Wagner, the highest commander of Alsace directly under the order of Hitler. In March 1942, Marcel Weinum was prosecuted by the Gestapo and sentenced to be beheaded at the age of 18 in April 1942 in Stuttgart, Germany. His last words will be: "If I have to die, I shall die but with a pure heart". From 1943 the city was bombarded by Allied aircraft. While the First World War had not notably damaged the city, Anglo-American bombing caused extensive destruction in raids of which at least one was allegedly carried out by mistake. In August 1944, several buildings in the Old Town were damaged by bombs, particularly the Palais Rohan, the Old Customs House (Ancienne Douane) and the Cathedral. On 23 November 1944, the city was officially liberated by the 2nd French Armoured Division under General Leclerc. He achieved the oath that he made with his soldiers, after the decisive Capture of Kufra. With the Oath of Kuffra, they swore to keep up the fight until the French flag flew over the Cathedral of Strasbourg.
Many people from Strasbourg were incorporated in the German Army against their will, and were sent to the eastern front, those young men and women were called Malgré-nous. Many tried to escape from the incorporation, join the French Resistance, or desert the Wehrmacht but many couldn't because they were running the risk of having their families sent to work or concentration camps by the Germans. Many of these men, especially those who did not answer the call immediately, were pressured to "volunteer" for service with the SS, often by direct threats on their families. This threat obliged the majority of them to remain in the German army. After the war, the few that survived were often accused of being traitors or collaborationists, because this tough situation was not known in the rest of France, and they had to face the incomprehension of many. In July 1944, 1500 malgré-nous were released from Soviet captivity and sent to Algiers, where they joined the Free French Forces. Nowadays history recognizes the suffering of those people, and museums, public discussions and memorials have been built to commemorate this terrible period of history of this part of Eastern France (Alsace and Moselle). Liberation of Strasbourg took place on 23 November 1944.
In 1949, the city was chosen to be the seat of the Council of Europe with its European Court of Human Rights and European Pharmacopoeia. Since 1952, the European Parliament has met in Strasbourg, which was formally designated its official 'seat' at the Edinburgh meeting of the European Council of EU heads of state and government in December 1992. (This position was reconfirmed and given treaty status in the 1997 Treaty of Amsterdam). However, only the (four-day) plenary sessions of the Parliament are held in Strasbourg each month, with all other business being conducted in Brussels and Luxembourg. Those sessions take place in the Immeuble Louise Weiss, inaugurated in 1999, which houses the largest parliamentary assembly room in Europe and of any democratic institution in the world. Before that, the EP sessions had to take place in the main Council of Europe building, the Palace of Europe, whose unusual inner architecture had become a familiar sight to European TV audiences. In 1992, Strasbourg became the seat of the Franco-German TV channel and movie-production society Arte.
In addition to the cathedral, Strasbourg houses several other medieval churches that have survived the many wars and destructions that have plagued the city: the Romanesque Église Saint-Étienne, partly destroyed in 1944 by Allied bombing raids, the part Romanesque, part Gothic, very large Église Saint-Thomas with its Silbermann organ on which Wolfgang Amadeus Mozart and Albert Schweitzer played, the Gothic Église protestante Saint-Pierre-le-Jeune with its crypt dating back to the seventh century and its cloister partly from the eleventh century, the Gothic Église Saint-Guillaume with its fine early-Renaissance stained glass and furniture, the Gothic Église Saint-Jean, the part Gothic, part Art Nouveau Église Sainte-Madeleine, etc. The Neo-Gothic church Saint-Pierre-le-Vieux Catholique (there is also an adjacent church Saint-Pierre-le-Vieux Protestant) serves as a shrine for several 15th-century wood worked and painted altars coming from other, now destroyed churches and installed there for public display. Among the numerous secular medieval buildings, the monumental Ancienne Douane (old custom-house) stands out.
The German Renaissance has bequeathed the city some noteworthy buildings (especially the current Chambre de commerce et d'industrie, former town hall, on Place Gutenberg), as did the French Baroque and Classicism with several hôtels particuliers (i.e. palaces), among which the Palais Rohan (1742, now housing three museums) is the most spectacular. Other buildings of its kind are the "Hôtel de Hanau" (1736, now the city hall), the Hôtel de Klinglin (1736, now residence of the préfet), the Hôtel des Deux-Ponts (1755, now residence of the military governor), the Hôtel d'Andlau-Klinglin (1725, now seat of the administration of the Port autonome de Strasbourg) etc. The largest baroque building of Strasbourg though is the 150 m (490 ft) long 1720s main building of the Hôpital civil. As for French Neo-classicism, it is the Opera House on Place Broglie that most prestigiously represents this style.
Strasbourg also offers high-class eclecticist buildings in its very extended German district, the Neustadt, being the main memory of Wilhelmian architecture since most of the major cities in Germany proper suffered intensive damage during World War II. Streets, boulevards and avenues are homogeneous, surprisingly high (up to seven stories) and broad examples of German urban lay-out and of this architectural style that summons and mixes up five centuries of European architecture as well as Neo-Egyptian, Neo-Greek and Neo-Babylonian styles. The former imperial palace Palais du Rhin, the most political and thus heavily criticized of all German Strasbourg buildings epitomizes the grand scale and stylistic sturdiness of this period. But the two most handsome and ornate buildings of these times are the École internationale des Pontonniers (the former Höhere Mädchenschule, girls college) with its towers, turrets and multiple round and square angles and the École des Arts décoratifs with its lavishly ornate façade of painted bricks, woodwork and majolica.
As for modern and contemporary architecture, Strasbourg possesses some fine Art Nouveau buildings (such as the huge Palais des Fêtes and houses and villas like Villa Schutzenberger and Hôtel Brion), good examples of post-World War II functional architecture (the Cité Rotterdam, for which Le Corbusier did not succeed in the architectural contest) and, in the very extended Quartier Européen, some spectacular administrative buildings of sometimes utterly large size, among which the European Court of Human Rights building by Richard Rogers is arguably the finest. Other noticeable contemporary buildings are the new Music school Cité de la Musique et de la Danse, the Musée d'Art moderne et contemporain and the Hôtel du Département facing it, as well as, in the outskirts, the tramway-station Hoenheim-Nord designed by Zaha Hadid.
Strasbourg features a number of prominent parks, of which several are of cultural and historical interest: the Parc de l'Orangerie, laid out as a French garden by André le Nôtre and remodeled as an English garden on behalf of Joséphine de Beauharnais, now displaying noteworthy French gardens, a neo-classical castle and a small zoo; the Parc de la Citadelle, built around impressive remains of the 17th-century fortress erected close to the Rhine by Vauban; the Parc de Pourtalès, laid out in English style around a baroque castle (heavily restored in the 19th century) that now houses a small three-star hotel, and featuring an open-air museum of international contemporary sculpture. The Jardin botanique de l'Université de Strasbourg (botanical garden) was created under the German administration next to the Observatory of Strasbourg, built in 1881, and still owns some greenhouses of those times. The Parc des Contades, although the oldest park of the city, was completely remodeled after World War II. The futuristic Parc des Poteries is an example of European park-conception in the late 1990s. The Jardin des deux Rives, spread over Strasbourg and Kehl on both sides of the Rhine opened in 2004 and is the most extended (60-hectare) park of the agglomeration. The most recent park is Parc du Heyritz (8,7 ha), opened in 2014 along a canal facing the hôpital civil.
Unlike most other cities, Strasbourg's collections of European art are divided into several museums according not only to type and area, but also to epoch. Old master paintings from the Germanic Rhenish territories and until 1681 are displayed in the Musée de l'Œuvre Notre-Dame, old master paintings from all the rest of Europe (including the Dutch Rhenish territories) and until 1871 as well as old master paintings from the Germanic Rhenish territories between 1681 and 1871 are displayed in the Musée des Beaux-Arts. Old master graphic arts until 1871 is displayed in the Cabinet des estampes et dessins. Decorative arts until 1681 ("German period") are displayed in the Musée de l'Œuvre Notre-Dame, decorative arts from 1681 to 1871 ("French period") are displayed in the Musée des Arts décoratifs. International art (painting, sculpture, graphic arts) and decorative art since 1871 is displayed in the Musée d'art moderne et contemporain. The latter museum also displays the city's photographic library.
Strasbourg, well known as centre of humanism, has a long history of excellence in higher-education, at the crossroads of French and German intellectual traditions. Although Strasbourg had been annexed by the Kingdom of France in 1683, it still remained connected to the German-speaking intellectual world throughout the 18th century and the university attracted numerous students from the Holy Roman Empire, including Goethe, Metternich and Montgelas, who studied law in Strasbourg, among the most prominent. Nowadays, Strasbourg is known to offer among the best university courses in France, after Paris.
The Bibliothèque nationale et universitaire (BNU) is, with its collection of more than 3,000,000 titles, the second largest library in France after the Bibliothèque nationale de France. It was founded by the German administration after the complete destruction of the previous municipal library in 1871 and holds the unique status of being simultaneously a students' and a national library. The Strasbourg municipal library had been marked erroneously as "City Hall" in a French commercial map, which had been captured and used by the German artillery to lay their guns. A librarian from Munich later pointed out "...that the destruction of the precious collection was not the fault of a German artillery officer, who used the French map, but of the slovenly and inaccurate scholarship of a Frenchman."
As one of the earliest centers of book-printing in Europe (see above: History), Strasbourg for a long time held a large number of incunabula—documents printed before 1500—in her library as one of her most precious heritages. After the total destruction of this institution in 1870, however, a new collection had to be reassembled from scratch. Today, Strasbourg's different public and institutional libraries again display a sizable total number of incunabula, distributed as follows: Bibliothèque nationale et universitaire, ca. 2 098 Médiathèque de la ville et de la communauté urbaine de Strasbourg, 394 Bibliothèque du Grand Séminaire, 238 Médiathèque protestante, 94 and Bibliothèque alsatique du Crédit Mutuel, 5.
City transportation in Strasbourg includes the futurist-looking Strasbourg tramway that opened in 1994 and is operated by the regional transit company Compagnie des Transports Strasbourgeois (CTS), consisting of 6 lines with a total length of 55.8 km (34.7 mi). The CTS also operates a comprehensive bus network throughout the city that is integrated with the trams. With more than 500 km (311 mi) of bicycle paths, biking in the city is convenient and the CTS operates a cheap bike-sharing scheme named Vélhop'. The CTS, and its predecessors, also operated a previous generation of tram system between 1878 and 1960, complemented by trolleybus routes between 1939 and 1962.
Being a city on the Ill and close to the Rhine, Strasbourg has always been an important centre of fluvial navigation, as is attested by archeological findings. In 1682 the Canal de la Bruche was added to the river navigations, initially to provide transport for sandstone from quarries in the Vosges for use in the fortification of the city. That canal has since closed, but the subsequent Canal du Rhone au Rhine, Canal de la Marne au Rhin and Grand Canal d'Alsace are still in use, as is the important activity of the Port autonome de Strasbourg. Water tourism inside the city proper attracts hundreds of thousands of tourists yearly.
The tram system that now criss-crosses the historic city centre complements walking and biking in it. The centre has been transformed into a pedestrian priority zone that enables and invites walking and biking by making these active modes of transport comfortable, safe and enjoyable. These attributes are accomplished by applying the principle of "filtered permeability" to the existing irregular network of streets. It means that the network adaptations favour active transportation and, selectively, "filter out" the car by reducing the number of streets that run through the centre. While certain streets are discontinuous for cars, they connect to a network of pedestrian and bike paths which permeate the entire centre. In addition, these paths go through public squares and open spaces increasing the enjoyment of the trip. This logic of filtering a mode of transport is fully expressed in a comprehensive model for laying out neighbourhoods and districts – the Fused Grid.
At present the A35 autoroute, which parallels the Rhine between Karlsruhe and Basel, and the A4 autoroute, which links Paris with Strasbourg, penetrate close to the centre of the city. The Grand contournement ouest (GCO) project, programmed since 1999, plans to construct a 24 km (15 mi) long highway connection between the junctions of the A4 and the A35 autoroutes in the north and of the A35 and A352 autoroutes in the south. This routes well to the west of the city and is meant to divest a significant portion of motorized traffic from the unité urbaine.
Appointments to the Order of the British Empire were at first made on the nomination of the self-governing Dominions of the Empire, the Viceroy of India, and the colonial governors, as well as on nominations from within the United Kingdom. As the Empire evolved into the Commonwealth, nominations continued to come from the Commonwealth realms, in which the monarch remained head of state. These overseas nominations have been discontinued in realms that have established their own Orders—such as the Order of Australia, the Order of Canada, and the New Zealand Order of Merit—but members of the Order are still appointed in the British Overseas Territories.
Any individual made a member of the Order for gallantry could wear an emblem of two crossed silver oak leaves on the same riband, ribbon or bow as the badge. It could not be awarded posthumously and was effectively replaced in 1974 with the Queen's Gallantry Medal. If recipients of the Order of the British Empire for Gallantry received promotion within the Order, whether for gallantry or otherwise, they continued to wear also the insignia of the lower grade with the oak leaves. However, they only used the post-nominal letters of the higher grade.
Honorary knighthoods are appointed to citizens of nations where Queen Elizabeth II is not Head of State, and may permit use of post-nominal letters but not the title of Sir or Dame. Occasionally honorary appointees are, incorrectly, referred to as Sir or Dame - Bill Gates or Bob Geldof, for example. Honorary appointees who later become a citizen of a Commonwealth realm can convert their appointment from honorary to substantive, then enjoy all privileges of membership of the order including use of the title of Sir and Dame for the senior two ranks of the Order. An example is Irish broadcaster Terry Wogan, who was appointed an honorary Knight Commander of the Order in 2005 and on successful application for dual British and Irish citizenship was made a substantive member and subsequently styled as "Sir Terry Wogan KBE".
The Order has six officials: the Prelate; the Dean; the Secretary; the Registrar; the King of Arms; and the Usher. The Bishop of London, a senior bishop in the Church of England, serves as the Order's Prelate. The Dean of St Paul's is ex officio the Dean of the Order. The Order's King of Arms is not a member of the College of Arms, as are many other heraldic officers. The Usher of the Order is known as the Gentleman Usher of the Purple Rod; he does not – unlike his Order of the Garter equivalent, the Gentleman Usher of the Black Rod – perform any duties related to the House of Lords.
Appointments to the Order of the British Empire were discontinued in those Commonwealth realms that established a national system of honours and awards such as the Order of Australia, the Order of Canada, and the New Zealand Order of Merit. In many of these systems, the different levels of award and honour reflect the Imperial system they replaced. Canada, Australia, and New Zealand all have (in increasing level of precedence) Members of, Officers of, and Companions to (rather than Commanders of) their respective orders, with both Australia and New Zealand having Knights and Dames as their highest classes.
The members of The Beatles were made MBEs in 1965. John Lennon justified the comparative merits of his investiture by comparing military membership in the Order: "Lots of people who complained about us receiving the MBE [status] received theirs for heroism in the war – for killing people… We received ours for entertaining other people. I'd say we deserve ours more." Lennon later returned his MBE insignia on 25 November 1969 as part of his ongoing peace protests. Other criticism centres on the claim that many recipients of the Order are being rewarded with honours for simply doing their jobs; critics claim that the civil service and judiciary receive far more orders and honours than leaders of other professions.
The Most Excellent Order of the British Empire is the "order of chivalry of British constitutional monarchy", rewarding contributions to the arts and sciences, work with charitable and welfare organisations and public service outside the Civil Service. It was established on 4 June 1917 by King George V, and comprises five classes, in civil and military divisions, the most senior two of which make the recipient either a knight if male, or dame if female. There is also the related British Empire Medal, whose recipients are affiliated with, but not members of, the order.
At the foundation of the Order, the "Medal of the Order of the British Empire" was instituted, to serve as a lower award granting recipients affiliation but not membership. In 1922, this was renamed the "British Empire Medal". It stopped being awarded by the United Kingdom as part of the 1993 reforms to the honours system, but was again awarded beginning in 2012, starting with 293 BEMs awarded for the Queen's Diamond Jubilee. In addition, the BEM is awarded by the Cook Islands and by some other Commonwealth nations. In 2004, a report entitled "A Matter of Honour: Reforming Our Honours System" by a Commons committee recommended to phase out the Order of the British Empire, as its title was "now considered to be unacceptable, being thought to embody values that are no longer shared by many of the country’s population".
From 1940, the Sovereign could appoint a person as a Commander, Officer or Member of the Order of the British Empire for gallantry for acts of bravery (not in the face of the enemy) below the level required for the George Medal. The grade was determined by the same criteria as usual, and not by the level of gallantry (and with more junior people instead receiving the British Empire Medal). Oddly, this meant that it was awarded for lesser acts of gallantry than the George Medal, but, as an Order, was worn before it and listed before it in post-nominal initials. From 14 January 1958, these awards were designated the Order of the British Empire for Gallantry.
Knights Grand Cross and Knights Commander prefix Sir, and Dames Grand Cross and Dames Commander prefix Dame, to their forenames.[b] Wives of Knights may prefix Lady to their surnames, but no equivalent privilege exists for husbands of Knights or spouses of Dames. Such forms are not used by peers and princes, except when the names of the former are written out in their fullest forms. Clergy of the Church of England or the Church of Scotland do not use the title Sir or Dame as they do not receive the accolade (i.e., they are not dubbed "knight" with a sword), although they do append the post-nominal letters.
India, while remaining an active member of the Commonwealth, chose as a republic to institute its own set of honours awarded by the President of India who holds a republican position some consider similar to that of the monarch in Britain. These are commonly referred to as the Padma Awards and consist of Padma Vibhushan, Padma Bhushan and Padma Shri in descending order. These do not carry any decoration or insignia that can be worn on the person and may not be used as titles along with individuals' names.
The Order is limited to 300 Knights and Dames Grand Cross, 845 Knights and Dames Commander, and 8,960 Commanders. There are no limits applied to the total number of members of the fourth and fifth classes, but no more than 858 Officers and 1,464 Members may be appointed per year. Foreign recipients, as honorary members, do not contribute to the numbers restricted to the Order as full members do. Although the Order of the British Empire has by far the highest number of members of the British Orders of Chivalry, with over 100,000 living members worldwide, there are fewer appointments to knighthoods than in other orders.
Members of all classes of the Order are assigned positions in the order of precedence. Wives of male members of all classes also feature on the order of precedence, as do sons, daughters and daughters-in-law of Knights Grand Cross and Knights Commander; relatives of Ladies of the Order, however, are not assigned any special precedence. As a general rule, individuals can derive precedence from their fathers or husbands, but not from their mothers or wives (see order of precedence in England and Wales for the exact positions).
On 16 September 2001, at Camp David, President George W. Bush used the phrase war on terrorism in an unscripted and controversial comment when he said, "This crusade – this war on terrorism – is going to take a while, ... " Bush later apologized for this remark due to the negative connotations the term crusade has to people, e.g. of Muslim faith. The word crusade was not used again. On 20 September 2001, during a televised address to a joint session of congress, Bush stated that, "(o)ur 'war on terror' begins with al-Qaeda, but it does not end there. It will not end until every terrorist group of global reach has been found, stopped, and defeated."
U.S. President Barack Obama has rarely used the term, but in his inaugural address on 20 January 2009, he stated "Our nation is at war, against a far-reaching network of violence and hatred." In March 2009 the Defense Department officially changed the name of operations from "Global War on Terror" to "Overseas Contingency Operation" (OCO). In March 2009, the Obama administration requested that Pentagon staff members avoid use of the term, instead using "Overseas Contingency Operation". Basic objectives of the Bush administration "war on terror", such as targeting al Qaeda and building international counterterrorism alliances, remain in place. In December 2012, Jeh Johnson, the General Counsel of the Department of Defense, stated that the military fight will be replaced by a law enforcement operation when speaking at Oxford University, predicting that al Qaeda will be so weakened to be ineffective, and has been "effectively destroyed", and thus the conflict will not be an armed conflict under international law. In May 2013, Obama stated that the goal is "to dismantle specific networks of violent extremists that threaten America"; which coincided with the U.S. Office of Management and Budget having changed the wording from "Overseas Contingency Operations" to "Countering Violent Extremism" in 2010.
Because the actions involved in the "war on terrorism" are diffuse, and the criteria for inclusion are unclear, political theorist Richard Jackson has argued that "the 'war on terrorism' therefore, is simultaneously a set of actual practices—wars, covert operations, agencies, and institutions—and an accompanying series of assumptions, beliefs, justifications, and narratives—it is an entire language or discourse." Jackson cites among many examples a statement by John Ashcroft that "the attacks of September 11 drew a bright line of demarcation between the civil and the savage". Administration officials also described "terrorists" as hateful, treacherous, barbarous, mad, twisted, perverted, without faith, parasitical, inhuman, and, most commonly, evil. Americans, in contrast, were described as brave, loving, generous, strong, resourceful, heroic, and respectful of human rights.
The origins of al-Qaeda can be traced to the Soviet war in Afghanistan (December 1979 – February 1989). The United States, United Kingdom, Saudi Arabia, Pakistan, and the People's Republic of China supported the Islamist Afghan mujahadeen guerillas against the military forces of the Soviet Union and the Democratic Republic of Afghanistan. A small number of "Afghan Arab" volunteers joined the fight against the Soviets, including Osama bin Laden, but there is no evidence they received any external assistance. In May 1996 the group World Islamic Front for Jihad Against Jews and Crusaders (WIFJAJC), sponsored by bin Laden (and later re-formed as al-Qaeda), started forming a large base of operations in Afghanistan, where the Islamist extremist regime of the Taliban had seized power earlier in the year. In February 1998, Osama bin Laden signed a fatwā, as head of al-Qaeda, declaring war on the West and Israel, later in May of that same year al-Qaeda released a video declaring war on the U.S. and the West.
On 7 August 1998, al-Qaeda struck the U.S. embassies in Kenya and Tanzania, killing 224 people, including 12 Americans. In retaliation, U.S. President Bill Clinton launched Operation Infinite Reach, a bombing campaign in Sudan and Afghanistan against targets the U.S. asserted were associated with WIFJAJC, although others have questioned whether a pharmaceutical plant in Sudan was used as a chemical warfare plant. The plant produced much of the region's antimalarial drugs and around 50% of Sudan's pharmaceutical needs. The strikes failed to kill any leaders of WIFJAJC or the Taliban.
On the morning of 11 September 2001, 19 men affiliated with al-Qaeda hijacked four airliners all bound for California. Once the hijackers assumed control of the airliners, they told the passengers that they had the bomb on board and would spare the lives of passengers and crew once their demands were met – no passenger and crew actually suspected that they would use the airliners as suicide weapons since it had never happened before in history. The hijackers – members of al-Qaeda's Hamburg cell – intentionally crashed two airliners into the Twin Towers of the World Trade Center in New York City. Both buildings collapsed within two hours from fire damage related to the crashes, destroying nearby buildings and damaging others. The hijackers crashed a third airliner into the Pentagon in Arlington County, Virginia, just outside Washington D.C. The fourth plane crashed into a field near Shanksville, Pennsylvania, after some of its passengers and flight crew attempted to retake control of the plane, which the hijackers had redirected toward Washington D.C., to target the White House, or the U.S. Capitol. No flights had survivors. A total of 2,977 victims and the 19 hijackers perished in the attacks.
The Authorization for Use of Military Force Against Terrorists or "AUMF" was made law on 14 September 2001, to authorize the use of United States Armed Forces against those responsible for the attacks on 11 September 2001. It authorized the President to use all necessary and appropriate force against those nations, organizations, or persons he determines planned, authorized, committed, or aided the terrorist attacks that occurred on 11 September 2001, or harbored such organizations or persons, in order to prevent any future acts of international terrorism against the United States by such nations, organizations or persons. Congress declares this is intended to constitute specific statutory authorization within the meaning of section 5(b) of the War Powers Resolution of 1973.
Subsequently, in October 2001, U.S. forces (with UK and coalition allies) invaded Afghanistan to oust the Taliban regime. On 7 October 2001, the official invasion began with British and U.S. forces conducting airstrike campaigns over enemy targets. Kabul, the capital city of Afghanistan, fell by mid-November. The remaining al-Qaeda and Taliban remnants fell back to the rugged mountains of eastern Afghanistan, mainly Tora Bora. In December, Coalition forces (the U.S. and its allies) fought within that region. It is believed that Osama bin Laden escaped into Pakistan during the battle.
The Taliban regrouped in western Pakistan and began to unleash an insurgent-style offensive against Coalition forces in late 2002. Throughout southern and eastern Afghanistan, firefights broke out between the surging Taliban and Coalition forces. Coalition forces responded with a series of military offensives and an increase in the amount of troops in Afghanistan. In February 2010, Coalition forces launched Operation Moshtarak in southern Afghanistan along with other military offensives in the hopes that they would destroy the Taliban insurgency once and for all. Peace talks are also underway between Taliban affiliated fighters and Coalition forces. In September 2014, Afghanistan and the United States signed a security agreement, which permits United States and NATO forces to remain in Afghanistan until at least 2024. The United States and other NATO and non-NATO forces are planning to withdraw; with the Taliban claiming it has defeated the United States and NATO, and the Obama Administration viewing it as a victory. In December 2014, ISAF encasing its colors, and Resolute Support began as the NATO operation in Afghanistan. Continued United States operations within Afghanistan will continue under the name "Operation Freedom's Sentinel".
In January 2002, the United States Special Operations Command, Pacific deployed to the Philippines to advise and assist the Armed Forces of the Philippines in combating Filipino Islamist groups. The operations were mainly focused on removing the Abu Sayyaf group and Jemaah Islamiyah (JI) from their stronghold on the island of Basilan. The second portion of the operation was conducted as a humanitarian program called "Operation Smiles". The goal of the program was to provide medical care and services to the region of Basilan as part of a "Hearts and Minds" program. Joint Special Operations Task Force – Philippines disbanded in June 2014, ending a 14-year mission. After JSOTF-P disbanded, as late as November 2014, American forces continued to operate in the Philippines under the name "PACOM Augmentation Team".
On 14 September 2009, U.S. Special Forces killed two men and wounded and captured two others near the Somali village of Baarawe. Witnesses claim that helicopters used for the operation launched from French-flagged warships, but that could not be confirmed. A Somali-based al-Qaida affiliated group, the Al-Shabaab, has confirmed the death of "sheik commander" Saleh Ali Saleh Nabhan along with an unspecified number of militants. Nabhan, a Kenyan, was wanted in connection with the 2002 Mombasa attacks.
The conflict in northern Mali began in January 2012 with radical Islamists (affiliated to al-Qaeda) advancing into northern Mali. The Malian government had a hard time maintaining full control over their country. The fledgling government requested support from the international community on combating the Islamic militants. In January 2013, France intervened on behalf of the Malian government's request and deployed troops into the region. They launched Operation Serval on 11 January 2013, with the hopes of dislodging the al-Qaeda affiliated groups from northern Mali.
Following the ceasefire agreement that suspended hostilities (but not officially ended) in the 1991 Gulf War, the United States and its allies instituted and began patrolling Iraqi no-fly zones, to protect Iraq's Kurdish and Shi'a Arab population—both of which suffered attacks from the Hussein regime before and after the Gulf War—in Iraq's northern and southern regions, respectively. U.S. forces continued in combat zone deployments through November 1995 and launched Operation Desert Fox against Iraq in 1998 after it failed to meet U.S. demands of "unconditional cooperation" in weapons inspections.
The first ground attack came at the Battle of Umm Qasr on 21 March 2003 when a combined force of British, American and Polish forces seized control of the port city of Umm Qasr. Baghdad, Iraq's capital city, fell to American forces in April 2003 and Saddam Hussein's government quickly dissolved. On 1 May 2003, Bush announced that major combat operations in Iraq had ended. However, an insurgency arose against the U.S.-led coalition and the newly developing Iraqi military and post-Saddam government. The insurgency, which included al-Qaeda affiliated groups, led to far more coalition casualties than the invasion. Other elements of the insurgency were led by fugitive members of President Hussein's Ba'ath regime, which included Iraqi nationalists and pan-Arabists. Many insurgency leaders are Islamists and claim to be fighting a religious war to reestablish the Islamic Caliphate of centuries past. Iraq's former president, Saddam Hussein was captured by U.S. forces in December 2003. He was executed in 2006.
In a major split in the ranks of Al Qaeda's organization, the Iraqi franchise, known as Al Qaeda in Iraq covertly invaded Syria and the Levant and began participating in the ongoing Syrian Civil War, gaining enough support and strength to re-invade Iraq's western provinces under the name of the Islamic State of Iraq and the Levant (ISIS/ISIL), taking over much of the country in a blitzkrieg-like action and combining the Iraq insurgency and Syrian Civil War into a single conflict. Due to their extreme brutality and a complete change in their overall ideology, Al Qaeda's core organization in Central Asia eventually denounced ISIS and directed their affiliates to cut off all ties with this organization. Many analysts[who?] believe that because of this schism, Al Qaeda and ISIL are now in a competition to retain the title of the world's most powerful terrorist organization.
The Obama administration began to reengage in Iraq with a series of airstrikes aimed at ISIS beginning on 10 August 2014. On 9 September 2014 President Obama said that he had the authority he needed to take action to destroy the militant group known as the Islamic State of Iraq and the Levant, citing the 2001 Authorization for Use of Military Force Against Terrorists, and thus did not require additional approval from Congress. The following day on 10 September 2014 President Barack Obama made a televised speech about ISIL, which he stated "Our objective is clear: We will degrade, and ultimately destroy, ISIL through a comprehensive and sustained counter-terrorism strategy". Obama has authorized the deployment of additional U.S. Forces into Iraq, as well as authorizing direct military operations against ISIL within Syria. On the night of 21/22 September the United States, Saudi Arabia, Bahrain, the UAE, Jordan and Qatar started air attacks against ISIS in Syria.[citation needed]
Following the 11 September 2001 attacks, former President of Pakistan Pervez Musharraf sided with the U.S. against the Taliban government in Afghanistan after an ultimatum by then U.S. President George W. Bush. Musharraf agreed to give the U.S. the use of three airbases for Operation Enduring Freedom. United States Secretary of State Colin Powell and other U.S. administration officials met with Musharraf. On 19 September 2001, Musharraf addressed the people of Pakistan and stated that, while he opposed military tactics against the Taliban, Pakistan risked being endangered by an alliance of India and the U.S. if it did not cooperate. In 2006, Musharraf testified that this stance was pressured by threats from the U.S., and revealed in his memoirs that he had "war-gamed" the United States as an adversary and decided that it would end in a loss for Pakistan.
On 12 January 2002, Musharraf gave a speech against Islamic extremism. He unequivocally condemned all acts of terrorism and pledged to combat Islamic extremism and lawlessness within Pakistan itself. He stated that his government was committed to rooting out extremism and made it clear that the banned militant organizations would not be allowed to resurface under any new name. He said, "the recent decision to ban extremist groups promoting militancy was taken in the national interest after thorough consultations. It was not taken under any foreign influence".
In 2002, the Musharraf-led government took a firm stand against the jihadi organizations and groups promoting extremism, and arrested Maulana Masood Azhar, head of the Jaish-e-Mohammed, and Hafiz Muhammad Saeed, chief of the Lashkar-e-Taiba, and took dozens of activists into custody. An official ban was imposed on the groups on 12 January. Later that year, the Saudi born Zayn al-Abidn Muhammed Hasayn Abu Zubaydah was arrested by Pakistani officials during a series of joint U.S.-Pakistan raids. Zubaydah is said to have been a high-ranking al-Qaeda official with the title of operations chief and in charge of running al-Qaeda training camps. Other prominent al-Qaeda members were arrested in the following two years, namely Ramzi bin al-Shibh, who is known to have been a financial backer of al-Qaeda operations, and Khalid Sheikh Mohammed, who at the time of his capture was the third highest-ranking official in al-Qaeda and had been directly in charge of the planning for the 11 September attacks.
The use of drones by the Central Intelligence Agency in Pakistan to carry out operations associated with the Global War on Terror sparks debate over sovereignty and the laws of war. The U.S. Government uses the CIA rather than the U.S. Air Force for strikes in Pakistan in order to avoid breaching sovereignty through military invasion. The United States was criticized by[according to whom?] a report on drone warfare and aerial sovereignty for abusing the term 'Global War on Terror' to carry out military operations through government agencies without formally declaring war.
In a 'Letter to American People' written by Osama bin Laden in 2002, he stated that one of the reasons he was fighting America is because of its support of India on the Kashmir issue. While on a trip to Delhi in 2002, U.S. Secretary of Defense Donald Rumsfeld suggested that Al-Qaeda was active in Kashmir, though he did not have any hard evidence. An investigation in 2002 unearthed evidence that Al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence. A team of Special Air Service and Delta Force was sent into Indian-administered Kashmir in 2002 to hunt for Osama bin Laden after reports that he was being sheltered by the Kashmiri militant group Harkat-ul-Mujahideen. U.S. officials believed that Al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Fazlur Rehman Khalil, the leader of the Harkat-ul-Mujahideen, signed al-Qaeda's 1998 declaration of holy war, which called on Muslims to attack all Americans and their allies. Indian sources claimed that In 2006, Al-Qaeda claimed they had established a wing in Kashmir; this worried the Indian government. India also claimed that Al-Qaeda has strong ties with the Kashmir militant groups Lashkar-e-Taiba and Jaish-e-Mohammed in Pakistan. While on a visit to Pakistan in January 2010, U.S. Defense secretary Robert Gates stated that Al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan.
In September 2009, a U.S. Drone strike reportedly killed Ilyas Kashmiri, who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with Al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' Al-Qaeda member, while others described him as the head of military operations for Al-Qaeda. Waziristan had now become the new battlefield for Kashmiri militants, who were now fighting NATO in support of Al-Qaeda. On 8 July 2012, Al-Badar Mujahideen, a breakaway faction of Kashmir centric terror group Hizbul Mujahideen, on conclusion of their two-day Shuhada Conference called for mobilisation of resources for continuation of jihad in Kashmir.
In the following months, NATO took a wide range of measures to respond to the threat of terrorism. On 22 November 2002, the member states of the Euro-Atlantic Partnership Council (EAPC) decided on a Partnership Action Plan against Terrorism, which explicitly states, "EAPC States are committed to the protection and promotion of fundamental freedoms and human rights, as well as the rule of law, in combating terrorism." NATO started naval operations in the Mediterranean Sea designed to prevent the movement of terrorists or weapons of mass destruction as well as to enhance the security of shipping in general called Operation Active Endeavour.
Support for the U.S. cooled when America made clear its determination to invade Iraq in late 2002. Even so, many of the "coalition of the willing" countries that unconditionally supported the U.S.-led military action have sent troops to Afghanistan, particular neighboring Pakistan, which has disowned its earlier support for the Taliban and contributed tens of thousands of soldiers to the conflict. Pakistan was also engaged in the War in North-West Pakistan (Waziristan War). Supported by U.S. intelligence, Pakistan was attempting to remove the Taliban insurgency and al-Qaeda element from the northern tribal areas.
The British 16th Air Assault Brigade (later reinforced by Royal Marines) formed the core of the force in southern Afghanistan, along with troops and helicopters from Australia, Canada and the Netherlands. The initial force consisted of roughly 3,300 British, 2,000 Canadian, 1,400 from the Netherlands and 240 from Australia, along with special forces from Denmark and Estonia and small contingents from other nations. The monthly supply of cargo containers through Pakistani route to ISAF in Afghanistan is over 4,000 costing around 12 billion in Pakistani Rupees.
In addition to military efforts abroad, in the aftermath of 9/11 the Bush Administration increased domestic efforts to prevent future attacks. Various government bureaucracies that handled security and military functions were reorganized. A new cabinet-level agency called the United States Department of Homeland Security was created in November 2002 to lead and coordinate the largest reorganization of the U.S. federal government since the consolidation of the armed forces into the Department of Defense.[citation needed]
The USA PATRIOT Act of October 2001 dramatically reduces restrictions on law enforcement agencies' ability to search telephone, e-mail communications, medical, financial, and other records; eases restrictions on foreign intelligence gathering within the United States; expands the Secretary of the Treasury's authority to regulate financial transactions, particularly those involving foreign individuals and entities; and broadens the discretion of law enforcement and immigration authorities in detaining and deporting immigrants suspected of terrorism-related acts. The act also expanded the definition of terrorism to include domestic terrorism, thus enlarging the number of activities to which the USA PATRIOT Act's expanded law enforcement powers could be applied. A new Terrorist Finance Tracking Program monitored the movements of terrorists' financial resources (discontinued after being revealed by The New York Times). Global telecommunication usage, including those with no links to terrorism, is being collected and monitored through the NSA electronic surveillance program. The Patriot Act is still in effect.
Political interest groups have stated that these laws remove important restrictions on governmental authority, and are a dangerous encroachment on civil liberties, possible unconstitutional violations of the Fourth Amendment. On 30 July 2003, the American Civil Liberties Union (ACLU) filed the first legal challenge against Section 215 of the Patriot Act, claiming that it allows the FBI to violate a citizen's First Amendment rights, Fourth Amendment rights, and right to due process, by granting the government the right to search a person's business, bookstore, and library records in a terrorist investigation, without disclosing to the individual that records were being searched. Also, governing bodies in a number of communities have passed symbolic resolutions against the act.
In 2005, the UN Security Council adopted Resolution 1624 concerning incitement to commit acts of terrorism and the obligations of countries to comply with international human rights laws. Although both resolutions require mandatory annual reports on counter-terrorism activities by adopting nations, the United States and Israel have both declined to submit reports. In the same year, the United States Department of Defense and the Chairman of the Joint Chiefs of Staff issued a planning document, by the name "National Military Strategic Plan for the War on Terrorism", which stated that it constituted the "comprehensive military plan to prosecute the Global War on Terror for the Armed Forces of the United States...including the findings and recommendations of the 9/11 Commission and a rigorous examination with the Department of Defense".
Criticism of the War on Terror addresses the issues, morality, efficiency, economics, and other questions surrounding the War on Terror and made against the phrase itself, calling it a misnomer. The notion of a "war" against "terrorism" has proven highly contentious, with critics charging that it has been exploited by participating governments to pursue long-standing policy/military objectives, reduce civil liberties, and infringe upon human rights. It is argued that the term war is not appropriate in this context (as in War on Drugs), since there is no identifiable enemy, and that it is unlikely international terrorism can be brought to an end by military means.
Other critics, such as Francis Fukuyama, note that "terrorism" is not an enemy, but a tactic; calling it a "war on terror", obscures differences between conflicts such as anti-occupation insurgents and international mujahideen. With a military presence in Iraq and Afghanistan and its associated collateral damage Shirley Williams maintains this increases resentment and terrorist threats against the West. There is also perceived U.S. hypocrisy, media-induced hysteria, and that differences in foreign and security policy have damaged America's image in most of the world.
Unlike the Spanish milled dollar the U.S. dollar is based upon a decimal system of values. In addition to the dollar the coinage act officially established monetary units of mill or one-thousandth of a dollar (symbol ₥), cent or one-hundredth of a dollar (symbol ¢), dime or one-tenth of a dollar, and eagle or ten dollars, with prescribed weights and composition of gold, silver, or copper for each. It was proposed in the mid-1800s that one hundred dollars be known as a union, but no union coins were ever struck and only patterns for the $50 half union exist. However, only cents are in everyday use as divisions of the dollar; "dime" is used solely as the name of the coin with the value of 10¢, while "eagle" and "mill" are largely unknown to the general public, though mills are sometimes used in matters of tax levies, and gasoline prices are usually in the form of $X.XX9 per gallon, e.g., $3.599, sometimes written as $3.599⁄10. When currently issued in circulating form, denominations equal to or less than a dollar are emitted as U.S. coins while denominations equal to or greater than a dollar are emitted as Federal Reserve notes (with the exception of gold, silver and platinum coins valued up to $100 as legal tender, but worth far more as bullion). Both one-dollar coins and notes are produced today, although the note form is significantly more common. In the past, "paper money" was occasionally issued in denominations less than a dollar (fractional currency) and gold coins were issued for circulation up to the value of $20 (known as the "double eagle", discontinued in the 1930s). The term eagle was used in the Coinage Act of 1792 for the denomination of ten dollars, and subsequently was used in naming gold coins. Paper currency less than one dollar in denomination, known as "fractional currency", was also sometimes pejoratively referred to as "shinplasters". In 1854, James Guthrie, then Secretary of the Treasury, proposed creating $100, $50 and $25 gold coins, which were referred to as a "Union", "Half Union", and "Quarter Union", thus implying a denomination of 1 Union = $100.
The symbol $, usually written before the numerical amount, is used for the U.S. dollar (as well as for many other currencies). The sign was the result of a late 18th-century evolution of the scribal abbreviation "ps" for the peso, the common name for the Spanish dollars that were in wide circulation in the New World from the 16th to the 19th centuries. These Spanish pesos or dollars were minted in Spanish America, namely in Mexico City, Potosí, Bolivia; and Lima, Peru. The p and the s eventually came to be written over each other giving rise to $.
Though still predominantly green, post-2004 series incorporate other colors to better distinguish different denominations. As a result of a 2008 decision in an accessibility lawsuit filed by the American Council of the Blind, the Bureau of Engraving and Printing is planning to implement a raised tactile feature in the next redesign of each note, except the $1 and the version of the $100 bill already in process. It also plans larger, higher-contrast numerals, more color differences, and distribution of currency readers to assist the visually impaired during the transition period.
The Constitution of the United States of America provides that the United States Congress has the power "To coin money". Laws implementing this power are currently codified at 31 U.S.C. § 5112. Section 5112 prescribes the forms, in which the United States dollars should be issued. These coins are both designated in Section 5112 as "legal tender" in payment of debts. The Sacagawea dollar is one example of the copper alloy dollar. The pure silver dollar is known as the American Silver Eagle. Section 5112 also provides for the minting and issuance of other coins, which have values ranging from one cent to 50 dollars. These other coins are more fully described in Coins of the United States dollar.
In the 16th century, Count Hieronymus Schlick of Bohemia began minting coins known as Joachimstalers (from German thal, or nowadays usually Tal, "valley", cognate with "dale" in English), named for Joachimstal, the valley where the silver was mined (St. Joachim's Valley, now Jáchymov; then part of the Kingdom of Bohemia, now part of the Czech Republic). Joachimstaler was later shortened to the German Taler, a word that eventually found its way into Danish and Swedish as daler, Norwegian as dalar and daler, Dutch as daler or daalder, Ethiopian as ታላሪ (talari), Hungarian as tallér, Italian as tallero, and English as dollar. Alternatively, thaler is said to come from the German coin Guldengroschen ("great guilder", being of silver but equal in value to a gold guilder), minted from the silver from Joachimsthal.
The early currency of the United States did not exhibit faces of presidents, as is the custom now; although today, by law, only the portrait of a deceased individual may appear on United States currency. In fact, the newly formed government was against having portraits of leaders on the currency, a practice compared to the policies of European monarchs. The currency as we know it today did not get the faces they currently have until after the early 20th century; before that "heads" side of coinage used profile faces and striding, seated, and standing figures from Greek and Roman mythology and composite Native Americans. The last coins to be converted to profiles of historic Americans were the dime (1946) and the Dollar (1971).
In 1862, paper money was issued without the backing of precious metals, due to the Civil War. Silver and gold coins continued to be issued and in 1878 the link between paper money and coins was reinstated. This disconnection from gold and silver backing also occurred during the War of 1812. The use of paper money not backed by precious metals had also occurred under the Articles of Confederation from 1777 to 1788. With no solid backing and being easily counterfeited, the continentals quickly lost their value, giving rise to the phrase "not worth a continental". This was a primary reason for the "No state shall... make any thing but gold and silver coin a tender in payment of debts" clause in article 1, section 10 of the United States Constitution.
In February 2007, the U.S. Mint, under the Presidential $1 Coin Act of 2005, introduced a new $1 U.S. Presidential dollar coin. Based on the success of the "50 State Quarters" series, the new coin features a sequence of presidents in order of their inaugurations, starting with George Washington, on the obverse side. The reverse side features the Statue of Liberty. To allow for larger, more detailed portraits, the traditional inscriptions of "E Pluribus Unum", "In God We Trust", the year of minting or issuance, and the mint mark will be inscribed on the edge of the coin instead of the face. This feature, similar to the edge inscriptions seen on the British £1 coin, is not usually associated with U.S. coin designs. The inscription "Liberty" has been eliminated, with the Statue of Liberty serving as a sufficient replacement. In addition, due to the nature of U.S. coins, this will be the first time there will be circulating U.S. coins of different denominations with the same president featured on the obverse (heads) side (Lincoln/penny, Jefferson/nickel, Franklin D. Roosevelt/dime, Washington/quarter, Kennedy/half dollar, and Eisenhower/dollar). Another unusual fact about the new $1 coin is Grover Cleveland will have two coins with his portrait issued due to the fact he was the only U.S. President to be elected to two non-consecutive terms.
When the Federal Reserve makes a purchase, it credits the seller's reserve account (with the Federal Reserve). This money is not transferred from any existing funds—it is at this point that the Federal Reserve has created new high-powered money. Commercial banks can freely withdraw in cash any excess reserves from their reserve account at the Federal Reserve. To fulfill those requests, the Federal Reserve places an order for printed money from the U.S. Treasury Department. The Treasury Department in turn sends these requests to the Bureau of Engraving and Printing (to print new dollar bills) and the Bureau of the Mint (to stamp the coins).
The value of the U.S. dollar declined significantly during wartime, especially during the American Civil War, World War I, and World War II. The Federal Reserve, which was established in 1913, was designed to furnish an "elastic" currency subject to "substantial changes of quantity over short periods", which differed significantly from previous forms of high-powered money such as gold, national bank notes, and silver coins. Over the very long run, the prior gold standard kept prices stable—for instance, the price level and the value of the U.S. dollar in 1914 was not very different from the price level in the 1880s. The Federal Reserve initially succeeded in maintaining the value of the U.S. dollar and price stability, reversing the inflation caused by the First World War and stabilizing the value of the dollar during the 1920s, before presiding over a 30% deflation in U.S. prices in the 1930s.
There is ongoing debate about whether central banks should target zero inflation (which would mean a constant value for the U.S. dollar over time) or low, stable inflation (which would mean a continuously but slowly declining value of the dollar over time, as is the case now). Although some economists are in favor of a zero inflation policy and therefore a constant value for the U.S. dollar, others contend that such a policy limits the ability of the central bank to control interest rates and stimulate the economy when needed.
The word "dollar" is one of the words in the first paragraph of Section 9 of Article 1 of the U.S. Constitution. In that context, "dollars" is a reference to the Spanish milled dollar, a coin that had a monetary value of 8 Spanish units of currency, or reales. In 1792 the U.S. Congress adopted legislation titled An act establishing a mint, and regulating the Coins of the United States. Section 9 of that act authorized the production of various coins, including "DOLLARS OR UNITS—each to be of the value of a Spanish milled dollar as the same is now current, and to contain three hundred and seventy-one grains and four sixteenth parts of a grain of pure, or four hundred and sixteen grains of standard silver". Section 20 of the act provided, "That the money of account of the United States shall be expressed in dollars, or units... and that all accounts in the public offices and all proceedings in the courts of the United States shall be kept and had in conformity to this regulation". In other words, this act designated the United States dollar as the unit of currency of the United States.
A "grand", sometimes shortened to simply "G", is a common term for the amount of $1,000. The suffix "K" or "k" (from "kilo-") is also commonly used to denote this amount (such as "$10k" to mean $10,000). However, the $1,000 note is no longer in general use. A "large" or "stack", it is usually a reference to a multiple of $1,000 (such as "fifty large" meaning $50,000). The $100 note is nicknamed "Benjamin", "Benji", "Ben", or "Franklin" (after Benjamin Franklin), "C-note" (C being the Roman numeral for 100), "Century note" or "bill" (e.g. "two bills" being $200). The $50 note is occasionally called a "yardstick" or a "grant" (after President Ulysses S. Grant, pictured on the obverse). The $20 note is referred to as a "double sawbuck", "Jackson" (after Andrew Jackson), or "double eagle". The $10 note is referred to as a "sawbuck", "ten-spot" or "Hamilton" (after Alexander Hamilton). The $5 note as "Lincoln", "fin", "fiver" or "five-spot". The infrequently-used $2 note is sometimes called "deuce", "Tom", or "Jefferson" (after Thomas Jefferson). The $1 note as a "single" or "buck". The dollar has also been, referred to as a "bone" and "bones" in plural (e.g. "twenty bones" is equal to $20). The newer designs, with portraits displayed in the main body of the obverse rather than in cameo insets upon paper color-coded by denomination, are sometimes referred to as "bigface" notes or "Monopoly money".
The U.S. dollar was created by the Constitution and defined by the Coinage Act of 1792. It specified a "dollar" to be based in the Spanish milled dollar and of 371 grains and 4 sixteenths part of a grain of pure or 416 grains (27.0 g) of standard silver and an "eagle" to be 247 and 4 eighths of a grain or 270 grains (17 g) of gold (again depending on purity). The choice of the value 371 grains arose from Alexander Hamilton's decision to base the new American unit on the average weight of a selection of worn Spanish dollars. Hamilton got the treasury to weigh a sample of Spanish dollars and the average weight came out to be 371 grains. A new Spanish dollar was usually about 377 grains in weight, and so the new U.S. dollar was at a slight discount in relation to the Spanish dollar.
The United States Mint produces Proof Sets specifically for collectors and speculators. Silver Proofs tend to be the standard designs but with the dime, quarter, and half dollar containing 90% silver. Starting in 1983 and ending in 1997, the Mint also produced proof sets containing the year's commemorative coins alongside the regular coins. Another type of proof set is the Presidential Dollar Proof Set where four special $1 coins are minted each year featuring a president. Because of budget constraints and increasing stockpiles of these relatively unpopular coins, the production of new Presidential dollar coins for circulation was suspended on December 13, 2011, by U.S. Treasury Secretary Timothy F. Geithner. Future minting of such coins will be made solely for collectors.
The Constitution provides that "a regular Statement and Account of the Receipts and Expenditures of all public Money shall be published from time to time". That provision of the Constitution is made specific by Section 331 of Title 31 of the United States Code. The sums of money reported in the "Statements" are currently being expressed in U.S. dollars (for example, see the 2009 Financial Report of the United States Government). The U.S. dollar may therefore be described as the unit of account of the United States.
Currently printed denominations are $1, $2, $5, $10, $20, $50, and $100. Notes above the $100 denomination stopped being printed in 1946 and were officially withdrawn from circulation in 1969. These notes were used primarily in inter-bank transactions or by organized crime; it was the latter usage that prompted President Richard Nixon to issue an executive order in 1969 halting their use. With the advent of electronic banking, they became less necessary. Notes in denominations of $500, $1,000, $5,000, $10,000 and $100,000 were all produced at one time; see large denomination bills in U.S. currency for details. These notes are now collectors' items and are worth more than their face value to collectors.
The colloquialism "buck"(s) (much like the British word "quid"(s, pl) for the pound sterling) is often used to refer to dollars of various nations, including the U.S. dollar. This term, dating to the 18th century, may have originated with the colonial leather trade. It may also have originated from a poker term. "Greenback" is another nickname originally applied specifically to the 19th century Demand Note dollars created by Abraham Lincoln to finance the costs of the Civil War for the North. The original note was printed in black and green on the back side. It is still used to refer to the U.S. dollar (but not to the dollars of other countries). Other well-known names of the dollar as a whole in denominations include "greenmail", "green" and "dead presidents" (the last because deceased presidents are pictured on most bills).
The value of the U.S. dollar was therefore no longer anchored to gold, and it fell upon the Federal Reserve to maintain the value of the U.S. currency. The Federal Reserve, however, continued to increase the money supply, resulting in stagflation and a rapidly declining value of the U.S. dollar in the 1970s. This was largely due to the prevailing economic view at the time that inflation and real economic growth were linked (the Phillips curve), and so inflation was regarded as relatively benign. Between 1965 and 1981, the U.S. dollar lost two thirds of its value.
The dollar was first based on the value and look of the Spanish dollar, used widely in Spanish America from the 16th to the 19th centuries. The first dollar coins issued by the United States Mint (founded 1792) were similar in size and composition to the Spanish dollar, minted in Mexico and Peru. The Spanish, U.S. silver dollars, and later, Mexican silver pesos circulated side by side in the United States, and the Spanish dollar and Mexican peso remained legal tender until the Coinage Act of 1857. The coinage of various English colonies also circulated. The lion dollar was popular in the Dutch New Netherland Colony (New York), but the lion dollar also circulated throughout the English colonies during the 17th century and early 18th century. Examples circulating in the colonies were usually worn so that the design was not fully distinguishable, thus they were sometimes referred to as "dog dollars".
The Gold Standard Act of 1900 abandoned the bimetallic standard and defined the dollar as 23.22 grains (1.505 g) of gold, equivalent to setting the price of 1 troy ounce of gold at $20.67. Silver coins continued to be issued for circulation until 1964, when all silver was removed from dimes and quarters, and the half dollar was reduced to 40% silver. Silver half dollars were last issued for circulation in 1970. Gold coins were confiscated by Executive Order 6102 issued in 1933 by Franklin Roosevelt. The gold standard was changed to 13.71 grains (0.888 g), equivalent to setting the price of 1 troy ounce of gold at $35. This standard persisted until 1968.
Early releases of the Washington coin included error coins shipped primarily from the Philadelphia mint to Florida and Tennessee banks. Highly sought after by collectors, and trading for as much as $850 each within a week of discovery, the error coins were identified by the absence of the edge impressions "E PLURIBUS UNUM IN GOD WE TRUST 2007 P". The mint of origin is generally accepted to be mostly Philadelphia, although identifying the source mint is impossible without opening a mint pack also containing marked units. Edge lettering is minted in both orientations with respect to "heads", some amateur collectors were initially duped into buying "upside down lettering error" coins. Some cynics also erroneously point out that the Federal Reserve makes more profit from dollar bills than dollar coins because they wear out in a few years, whereas coins are more permanent. The fallacy of this argument arises because new notes printed to replace worn out notes, which have been withdrawn from circulation, bring in no net revenue to the government to offset the costs of printing new notes and destroying the old ones. As most vending machines are incapable of making change in banknotes, they commonly accept only $1 bills, though a few will give change in dollar coins.
The U.S. Constitution provides that Congress shall have the power to "borrow money on the credit of the United States". Congress has exercised that power by authorizing Federal Reserve Banks to issue Federal Reserve Notes. Those notes are "obligations of the United States" and "shall be redeemed in lawful money on demand at the Treasury Department of the United States, in the city of Washington, District of Columbia, or at any Federal Reserve bank". Federal Reserve Notes are designated by law as "legal tender" for the payment of debts. Congress has also authorized the issuance of more than 10 other types of banknotes, including the United States Note and the Federal Reserve Bank Note. The Federal Reserve Note is the only type that remains in circulation since the 1970s.
Usually, the short-term goal of open market operations is to achieve a specific short-term interest rate target. In other instances, monetary policy might instead entail the targeting of a specific exchange rate relative to some foreign currency or else relative to gold. For example, in the case of the United States the Federal Reserve targets the federal funds rate, the rate at which member banks lend to one another overnight. The other primary means of conducting monetary policy include: (i) Discount window lending (as lender of last resort); (ii) Fractional deposit lending (changes in the reserve requirement); (iii) Moral suasion (cajoling certain market players to achieve specified outcomes); (iv) "Open mouth operations" (talking monetary policy with the market).
Under the Bretton Woods system established after World War II, the value of gold was fixed to $35 per ounce, and the value of the U.S. dollar was thus anchored to the value of gold. Rising government spending in the 1960s, however, led to doubts about the ability of the United States to maintain this convertibility, gold stocks dwindled as banks and international investors began to convert dollars to gold, and as a result the value of the dollar began to decline. Facing an emerging currency crisis and the imminent danger that the United States would no longer be able to redeem dollars for gold, gold convertibility was finally terminated in 1971 by President Nixon, resulting in the "Nixon shock".
The U.S. dollar is fiat money. It is the currency most used in international transactions and is the world's most dominant reserve currency. Several countries use it as their official currency, and in many others it is the de facto currency. Besides the United States, it is also used as the sole currency in two British Overseas Territories in the Caribbean: the British Virgin Islands and the Turks and Caicos islands. A few countries use only the U.S. Dollar for paper money, while the country mints its own coins, or also accepts U.S. coins that can be used as payment in U.S. dollars, such as the Susan B. Anthony dollar.
Today, USD notes are made from cotton fiber paper, unlike most common paper, which is made of wood fiber. U.S. coins are produced by the United States Mint. U.S. dollar banknotes are printed by the Bureau of Engraving and Printing and, since 1914, have been issued by the Federal Reserve. The "large-sized notes" issued before 1928 measured 7.42 inches (188 mm) by 3.125 inches (79.4 mm); small-sized notes, introduced that year, measure 6.14 inches (156 mm) by 2.61 inches (66 mm) by 0.0043 inches (0.11 mm). When the current, smaller sized U.S. currency was introduced it was referred to as Philippine-sized currency because the Philippines had previously adopted the same size for its legal currency.
From 1792, when the Mint Act was passed, the dollar was defined as 371.25 grains (24.056 g) of silver. Many historians[who?] erroneously assume gold was standardized at a fixed rate in parity with silver; however, there is no evidence of Congress making this law. This has to do with Alexander Hamilton's suggestion to Congress of a fixed 15:1 ratio of silver to gold, respectively. The gold coins that were minted however, were not given any denomination whatsoever and traded for a market value relative to the Congressional standard of the silver dollar. 1834 saw a shift in the gold standard to 23.2 grains (1.50 g), followed by a slight adjustment to 23.22 grains (1.505 g) in 1837 (16:1 ratio).[citation needed]
Technically, all these coins are still legal tender at face value, though some are far more valuable today for their numismatic value, and for gold and silver coins, their precious metal value. From 1965 to 1970 the Kennedy half dollar was the only circulating coin with any silver content, which was removed in 1971 and replaced with cupronickel. However, since 1992, the U.S. Mint has produced special Silver Proof Sets in addition to the regular yearly proof sets with silver dimes, quarters, and half dollars in place of the standard copper-nickel versions. In addition, an experimental $4.00 (Stella) coin was also minted in 1879, but never placed into circulation, and is properly considered to be a pattern rather than an actual coin denomination.
Dollar coins have not been very popular in the United States. Silver dollars were minted intermittently from 1794 through 1935; a copper-nickel dollar of the same large size, featuring President Dwight D. Eisenhower, was minted from 1971 through 1978. Gold dollars were also minted in the 19th century. The Susan B. Anthony dollar coin was introduced in 1979; these proved to be unpopular because they were often mistaken for quarters, due to their nearly equal size, their milled edge, and their similar color. Minting of these dollars for circulation was suspended in 1980 (collectors' pieces were struck in 1981), but, as with all past U.S. coins, they remain legal tender. As the number of Anthony dollars held by the Federal Reserve and dispensed primarily to make change in postal and transit vending machines had been virtually exhausted, additional Anthony dollars were struck in 1999. In 2000, a new $1 coin, featuring Sacagawea, (the Sacagawea dollar) was introduced, which corrected some of the problems of the Anthony dollar by having a smooth edge and a gold color, without requiring changes to vending machines that accept the Anthony dollar. However, this new coin has failed to achieve the popularity of the still-existing $1 bill and is rarely used in daily transactions. The failure to simultaneously withdraw the dollar bill and weak publicity efforts have been cited by coin proponents as primary reasons for the failure of the dollar coin to gain popular support.
The monetary base consists of coins and Federal Reserve Notes in circulation outside the Federal Reserve Banks and the U.S. Treasury, plus deposits held by depository institutions at Federal Reserve Banks. The adjusted monetary base has increased from approximately 400 billion dollars in 1994, to 800 billion in 2005, and over 3000 billion in 2013. The amount of cash in circulation is increased (or decreased) by the actions of the Federal Reserve System. Eight times a year, the 12-person Federal Open Market Committee meet to determine U.S. monetary policy. Every business day, the Federal Reserve System engages in Open market operations to carry out that monetary policy. If the Federal Reserve desires to increase the money supply, it will buy securities (such as U.S. Treasury Bonds) anonymously from banks in exchange for dollars. Conversely, it will sell securities to the banks in exchange for dollars, to take dollars out of circulation.
The decline in the value of the U.S. dollar corresponds to price inflation, which is a rise in the general level of prices of goods and services in an economy over a period of time. A consumer price index (CPI) is a measure estimating the average price of consumer goods and services purchased by households. The United States Consumer Price Index, published by the Bureau of Labor Statistics, is a measure estimating the average price of consumer goods and services in the United States. It reflects inflation as experienced by consumers in their day-to-day living expenses. A graph showing the U.S. CPI relative to 1982–1984 and the annual year-over-year change in CPI is shown at right.
Oklahoma City is the capital and largest city of the state of Oklahoma. The county seat of Oklahoma County, the city ranks 27th among United States cities in population. The population grew following the 2010 Census, with the population estimated to have increased to 620,602 as of July 2014. As of 2014, the Oklahoma City metropolitan area had a population of 1,322,429, and the Oklahoma City-Shawnee Combined Statistical Area had a population of 1,459,758 (Chamber of Commerce) residents, making it Oklahoma's largest metropolitan area. Oklahoma City's city limits extend into Canadian, Cleveland, and Pottawatomie counties, though much of those areas outside of the core Oklahoma County area are suburban or rural (watershed). The city ranks as the eighth-largest city in the United States by land area (including consolidated city-counties; it is the largest city in the United States by land area whose government is not consolidated with that of a county or borough).
Oklahoma City, lying in the Great Plains region, features one of the largest livestock markets in the world. Oil, natural gas, petroleum products and related industries are the largest sector of the local economy. The city is situated in the middle of an active oil field and oil derricks dot the capitol grounds. The federal government employs large numbers of workers at Tinker Air Force Base and the United States Department of Transportation's Mike Monroney Aeronautical Center (these two sites house several offices of the Federal Aviation Administration and the Transportation Department's Enterprise Service Center, respectively).
Oklahoma City is on the I-35 Corridor and is one of the primary travel corridors into neighboring Texas and Mexico. Located in the Frontier Country region of the state, the city's northeast section lies in an ecological region known as the Cross Timbers. The city was founded during the Land Run of 1889, and grew to a population of over 10,000 within hours of its founding. The city was the scene of the April 19, 1995 bombing of the Alfred P. Murrah Federal Building, in which 168 people died. It was the deadliest terror attack in the history of the United States until the attacks of September 11, 2001, and remains the deadliest act of domestic terrorism in U.S. history.
Oklahoma City was settled on April 22, 1889, when the area known as the "Unassigned Lands" was opened for settlement in an event known as "The Land Run". Some 10,000 homesteaders settled the area that would become the capital of Oklahoma. The town grew quickly; the population doubled between 1890 and 1900. Early leaders of the development of the city included Anton Classen, John Shartel, Henry Overholser and James W. Maney.
By the time Oklahoma was admitted to the Union in 1907, Oklahoma City had surpassed Guthrie, the territorial capital, as the population center and commercial hub of the new state. Soon after, the capital was moved from Guthrie to Oklahoma City. Oklahoma City was a major stop on Route 66 during the early part of the 20th century; it was prominently mentioned in Bobby Troup's 1946 jazz classic, "(Get Your Kicks on) Route 66", later made famous by artist Nat King Cole.
Before World War II, Oklahoma City developed major stockyards, attracting jobs and revenue formerly in Chicago and Omaha, Nebraska. With the 1928 discovery of oil within the city limits (including under the State Capitol), Oklahoma City became a major center of oil production. Post-war growth accompanied the construction of the Interstate Highway System, which made Oklahoma City a major interchange as the convergence of I-35, I-40 and I-44. It was also aided by federal development of Tinker Air Force Base.
Patience Latting was elected Mayor of Oklahoma City in 1971, becoming the city's first female mayor. Latting was also the first woman to serve as mayor of a U.S. city with over 350,000 residents.
In 1993, the city passed a massive redevelopment package known as the Metropolitan Area Projects (MAPS), intended to rebuild the city's core with civic projects to establish more activities and life to downtown. The city added a new baseball park; central library; renovations to the civic center, convention center and fairgrounds; and a water canal in the Bricktown entertainment district. Water taxis transport passengers within the district, adding color and activity along the canal. MAPS has become one of the most successful public-private partnerships undertaken in the U.S., exceeding $3 billion in private investment as of 2010. As a result of MAPS, the population living in downtown housing has exponentially increased, together with demand for additional residential and retail amenities, such as grocery, services, and shops.
Since the MAPS projects' completion, the downtown area has seen continued development. Several downtown buildings are undergoing renovation/restoration. Notable among these was the restoration of the Skirvin Hotel in 2007. The famed First National Center is being renovated.
Residents of Oklahoma City suffered substantial losses on April 19, 1995 when Timothy McVeigh detonated a bomb in front of the Murrah building. The building was destroyed (the remnants of which had to be imploded in a controlled demolition later that year), more than 100 nearby buildings suffered severe damage, and 168 people were killed. The site has been commemorated as the Oklahoma City National Memorial and Museum. Since its opening in 2000, over three million people have visited. Every year on April 19, survivors, families and friends return to the memorial to read the names of each person lost.
The "Core-to-Shore" project was created to relocate I-40 one mile (1.6 km) south and replace it with a boulevard to create a landscaped entrance to the city. This also allows the central portion of the city to expand south and connect with the shore of the Oklahoma River. Several elements of "Core to Shore" were included in the MAPS 3 proposal approved by voters in late 2009.
According to the United States Census Bureau, the city has a total area of 620.34 square miles (1,606.7 km2), of which, 601.11 square miles (1,556.9 km2) of it is land and 19.23 square miles (49.8 km2) of it is water. The total area is 3.09 percent water.
Oklahoma City lies in the Sandstone Hills region of Oklahoma, known for hills of 250 to 400 feet (120 m) and two species of oak: blackjack oak (Quercus marilandica) and post oak (Q. stellata). The northeastern part of the city and its eastern suburbs fall into an ecological region known as the Cross Timbers.
The city is roughly bisected by the North Canadian River (recently renamed the Oklahoma River inside city limits). The North Canadian once had sufficient flow to flood every year, wreaking destruction on surrounding areas, including the central business district and the original Oklahoma City Zoo. In the 1940s, a dam was built on the river to manage the flood control and reduced its level. In the 1990s, as part of the citywide revitalization project known as MAPS, the city built a series of low-water dams, returning water to the portion of the river flowing near downtown. The city has three large lakes: Lake Hefner and Lake Overholser, in the northwestern quarter of the city; and the largest, Lake Stanley Draper, in the sparsely populated far southeast portion of the city.
The population density normally reported for Oklahoma City using the area of its city limits can be a bit misleading. Its urbanized zone covers roughly 244 sq mi (630 km2) resulting in a density of 2,500 per square mile (2013 est), compared with larger rural watershed areas incorporated by the city, which cover the remaining 377 sq mi (980 km2) of the city limits.
The city is bisected geographically and culturally by the North Canadian River, which basically divides North Oklahoma City and South Oklahoma City. The two halves of the city were actually founded and plotted as separate cities, but soon grew together. The north side is characterized by very diverse and fashionable urban neighborhoods near the city center and sprawling suburbs further north. South Oklahoma City is generally more blue collar working class and significantly more industrial, having grown up around the Stockyards and meat packing plants at the turn of the century, and is currently the center of the city's rapidly growing Latino community.
Downtown Oklahoma City, which has 7,600 residents, is currently seeing an influx of new private investment and large scale public works projects, which have helped to resuscitate a central business district left almost deserted by the Oil Bust of the early 1980s. The centerpiece of downtown is the newly renovated Crystal Bridge and Myriad Botanical Gardens, one of the few elements of the Pei Plan to be completed. In the next few years a massive new central park will link the gardens near the CBD and the new convention center to be built just south of it to the North Canadian River, as part of a massive works project known as Core to Shore; the new park is part of MAPS3, a collection of civic projects funded by a 1-cent temporary (seven-year) sales tax increase.
Oklahoma City has a humid subtropical climate (Köppen: Cfa), with frequent variations in weather daily and seasonally, except during the consistently hot and humid summer months. Prolonged and severe droughts (sometimes leading to wildfires in the vicinity) as well as very heavy rainfall leading to flash flooding and flooding occur with some regularity. Consistent winds, usually from the south or south-southeast during the summer, help temper the hotter weather. Consistent northerly winds during the winter can intensify cold periods. Severe ice storms and snowstorms happen sporadically during the winter.
The average temperature is 61.4 °F (16.3 °C), with the monthly daily average ranging from 39.2 °F (4.0 °C) in January to 83.0 °F (28.3 °C) in July. Extremes range from −17 °F (−27 °C) on February 12, 1899 to 113 °F (45 °C) on August 11, 1936 and August 3, 2012; the last sub-zero (°F) reading was −5 °F (−21 °C) on February 10, 2011. Temperatures reach 100 °F (38 °C) on 10.4 days of the year, 90 °F (32 °C) on nearly 70 days, and fail to rise above freezing on 8.3 days. The city receives about 35.9 inches (91.2 cm) of precipitation annually, of which 8.6 inches (21.8 cm) is snow.
Oklahoma City has a very active severe weather season from March through June, especially during April and May. Being in the center of what is colloquially referred to as Tornado Alley, it is prone to especially frequent and severe tornadoes, as well as very severe hailstorms and occasional derechoes. Tornadoes have occurred in every month of the year and a secondary smaller peak also occurs during autumn, especially October. The Oklahoma City metropolitan area is one of the most tornado-prone major cities in the world, with about 150 tornadoes striking within the city limits since 1890. Since the time weather records have been kept, Oklahoma City has been struck by thirteen violent tornadoes, eleven F/EF4s and two F/EF5. On May 3, 1999 parts of southern Oklahoma City and nearby suburban communities suffered from one of the most powerful tornadoes on record, an F5 on the Fujita scale, with wind speeds estimated by radar at 318 mph (510 km/h). On May 20, 2013, far southwest Oklahoma City, along with Newcastle and Moore, was hit again by a EF5 tornado; it was 0.5 to 1.3 miles (0.80 to 2.09 km) wide and killed 23 people. Less than two weeks later, on May 31, another outbreak affected the Oklahoma City area, including an EF1 and an EF0 within the city and a tornado several miles west of the city that was 2.6 miles (4.2 km) in width, the widest tornado ever recorded.
With 19.48 inches of rainfall, May 2015 was by far Oklahoma City's record-wettest month since record keeping began in 1890. Across Oklahoma and Texas generally, there was record flooding in the latter part of the month 
As of the 2010 census, there were 579,999 people, 230,233 households, and 144,120 families residing in the city. The population density was 956.4 inhabitants per square mile (321.9/km²). There were 256,930 housing units at an average density of 375.9 per square mile (145.1/km²).
There were 230,233 households, 29.4% of which had children under the age of 18 living with them, 43.4% were married couples living together, 13.9% had a female householder with no husband present, and 37.4% were non-families. One person households account for 30.5% of all households and 8.7% of all households had someone living alone who is 65 years of age or older. The average household size was 2.47 and the average family size was 3.11.
In the 2000 Census Oklahoma City's age composition was 25.5% under the age of 18, 10.7% from 18 to 24, 30.8% from 25 to 44, 21.5% from 45 to 64, and 11.5% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 92.7 males.
Oklahoma City has experienced significant population increases since the late 1990s. In May 2014, the U.S. Census announced Oklahoma City had an estimated population of 620,602 in 2014 and that it had grown 5.3 percent between April 2010 and June 2013. Since the official Census in 2000, Oklahoma City had grown 21 percent (a 114,470 raw increase) according to the Bureau estimates. The 2014 estimate of 620,602 is the largest population Oklahoma City has ever recorded. It is the first city in the state to record a population greater than 600,000 residents and the largest municipal population of the Great Plains region (OK, KS, NE, SD, ND).
Oklahoma City is the principal city of the eight-county Oklahoma City Metropolitan Statistical Area in Central Oklahoma and is the state's largest urbanized area. Based on population rank, the metropolitan area was the 42nd largest in the nation as of 2012.
With regards to Mexican drug cartels, Oklahoma City has traditionally been the territory of the notorious Juárez Cartel, but the Sinaloa Cartel has been reported as trying to establish a foothold in Oklahoma City. There are many rival gangs in Oklahoma City, one whose headquarters has been established in the city, the Southside Locos, traditionally known as Sureños.
Oklahoma City also has its share of very brutal crimes, particularly in the 1970s. The worst of which occurred in 1978, when six employees of a Sirloin Stockade restaurant on the city's south side were murdered execution-style in the restaurant's freezer. An intensive investigation followed, and the three individuals involved, who also killed three others in Purcell, Oklahoma, were identified. One, Harold Stafford, died in a motorcycle accident in Tulsa not long after the restaurant murders. Another, Verna Stafford, was sentenced to life without parole after being granted a new trial after she had previously been sentenced to death. Roger Dale Stafford, considered the mastermind of the murder spree, was executed by lethal injection at the Oklahoma State Penitentiary in 1995.
The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size.
On April 19, 1995, the Alfred P. Murrah Federal Building was destroyed by a fertilizer bomb manufactured and detonated by Timothy McVeigh. The blast and catastrophic collapse killed 168 people and injured over 680. The blast shockwave destroyed or damaged 324 buildings within a 340-meter radius, destroyed or burned 86 cars, and shattered glass in 258 nearby buildings, causing at least an estimated $652 million worth of damage. The main suspect- Timothy McVeigh, was executed by lethal injection on June 11, 2001. It was the deadliest single domestic terrorist attack in US history, prior to 9/11.
While not in Oklahoma City proper, other large employers within the MSA region include: Tinker Air Force Base (27,000); University of Oklahoma (11,900); University of Central Oklahoma (2,900); and Norman Regional Hospital (2,800).
According to the Oklahoma City Chamber of Commerce, the metropolitan area's economic output grew by 33 percent between 2001 and 2005 due chiefly to economic diversification. Its gross metropolitan product was $43.1 billion in 2005 and grew to $61.1 billion in 2009.
In 2008, Forbes magazine named Oklahoma City the most "recession proof city in America". The magazine reported that the city had falling unemployment, one of the strongest housing markets in the country and solid growth in energy, agriculture and manufacturing. However, during the early 1980s, Oklahoma City had one of the worst job and housing markets due to the bankruptcy of Penn Square Bank in 1982 and then the post-1985 crash in oil prices.[citation needed]
Other theaters include Lyric Theatre, Jewel Box Theatre, Kirkpatrick Auditorium, the Poteet Theatre, the Oklahoma City Community College Bruce Owen Theater and the 488-seat Petree Recital Hall, at the Oklahoma City University campus. The university also opened the Wanda L Bass School of Music and auditorium in April 2006.
The Science Museum Oklahoma (formerly Kirkpatrick Science and Air Space Museum at Omniplex) houses exhibits on science, aviation, and an IMAX theater. The museum formerly housed the International Photography Hall of Fame (IPHF) that exhibits photographs and artifacts from a large collection of cameras and other artifacts preserving the history of photography. IPHF honors those who have made significant contributions to the art and/or science of photography and relocated to St. Louis, Missouri in 2013.
The Museum of Osteology houses more than 300 real animal skeletons. Focusing on the form and function of the skeletal system, this 7,000 sq ft (650 m2) museum displays hundreds of skulls and skeletons from all corners of the world. Exhibits include adaptation, locomotion, classification and diversity of the vertebrate kingdom. The Museum of Osteology is the only one of its kind in America.
The National Cowboy & Western Heritage Museum has galleries of western art and is home to the Hall of Great Western Performers. In contrast, the city will also be home to The American Indian Cultural Center and Museum that began construction in 2009 (although completion of the facility has been held up due to insufficient funding), on the south side of Interstate 40, southeast from Bricktown.
The Oklahoma City National Memorial in the northern part of Oklahoma City's downtown was created as the inscription on its eastern gate of the Memorial reads, "to honor the victims, survivors, rescuers, and all who were changed forever on April 19, 1995"; the memorial was built on the land formerly occupied by the Alfred P. Murrah Federal Building complex prior to its 1995 bombing. The outdoor Symbolic Memorial can be visited 24 hours a day for free, and the adjacent Memorial Museum, located in the former Journal Record building damaged by the bombing, can be entered for a small fee. The site is also home to the National Memorial Institute for the Prevention of Terrorism, a non-partisan, nonprofit think tank devoted to the prevention of terrorism.
The American Banjo Museum located in the Bricktown Entertainment district is dedicated to preserving and promoting the music and heritage of America's native musical instrument – the banjo. With a collection valued at $3.5 million it is truly a national treasure. An interpretive exhibits tells the evolution of the banjo from its humble roots in American slavery, to bluegrass, to folk and world music.
The Oklahoma History Center is the history museum of the state of Oklahoma. Located across the street from the governor's mansion at 800 Nazih Zuhdi Drive in northeast Oklahoma City, the museum opened in 2005 and is operated by the Oklahoma Historical Society. It preserves the history of Oklahoma from the prehistoric to the present day.
Oklahoma City is home to several professional sports teams, including the Oklahoma City Thunder of the National Basketball Association. The Thunder is the city's second "permanent" major professional sports franchise after the now-defunct AFL Oklahoma Wranglers and is the third major-league team to call the city home when considering the temporary hosting of the New Orleans/Oklahoma City Hornets for the 2005–06 and 2006–07 NBA seasons.
Other professional sports clubs in Oklahoma City include the Oklahoma City Dodgers, the Triple-A affiliate of the Los Angeles Dodgers, the Oklahoma City Energy FC of the United Soccer League, and the Crusaders of Oklahoma Rugby Football Club USA Rugby.
Chesapeake Energy Arena in downtown is the principal multipurpose arena in the city which hosts concerts, NHL exhibition games, and many of the city's pro sports teams. In 2008, the Oklahoma City Thunder became the major tenant. Located nearby in Bricktown, the Chickasaw Bricktown Ballpark is the home to the city's baseball team, the Dodgers. "The Brick", as it is locally known, is considered one of the finest minor league parks in the nation.[citation needed]
Oklahoma City is the annual host of the Big 12 Baseball Tournament, the World Cup of Softball, and the annual NCAA Women's College World Series. The city has held the 2005 NCAA Men's Basketball First and Second round and hosted the Big 12 Men's and Women's Basketball Tournaments in 2007 and 2009. The major universities in the area – University of Oklahoma, Oklahoma City University, and Oklahoma State University – often schedule major basketball games and other sporting events at Chesapeake Energy Arena and Chickasaw Bricktown Ballpark, although most home games are played at their campus stadiums.
Other major sporting events include Thoroughbred and Quarter horse racing circuits at Remington Park and numerous horse shows and equine events that take place at the state fairgrounds each year. There are numerous golf courses and country clubs spread around the city.
The state of Oklahoma hosts a highly competitive high school football culture, with many teams in the Oklahoma City metropolitan area. The Oklahoma Secondary School Activities Association (OSSAA) organizes high school football into eight distinct classes based on the size of school enrollment. Beginning with the largest, the classes are: 6A, 5A, 4A, 3A, 2A, A, B, and C. Class 6A is broken into two divisions. Oklahoma City area schools in this division include: Edmond North, Mustang, Moore, Yukon, Edmond Memorial, Edmond Santa Fe, Norman North, Westmoore, Southmoore, Putnam City North, Norman, Putnam City, Putnam City West, U.S. Grant, Midwest City.
The Oklahoma City Thunder of the National Basketball Association (NBA) has called Oklahoma City home since the 2008–09 season, when owner Clayton Bennett relocated the franchise from Seattle, Washington. The Thunder plays home games at the Chesapeake Energy Arena in downtown Oklahoma City, known affectionately in the national media as 'the Peake' and 'Loud City'. The Thunder is known by several nicknames, including "OKC Thunder" and simply "OKC", and its mascot is Rumble the Bison.
After a lackluster arrival to Oklahoma City for the 2008–09 season, the Oklahoma City Thunder secured a berth (8th) in the 2010 NBA Playoffs the next year after boasting its first 50-win season, winning two games in the first round against the Los Angeles Lakers. In 2012, Oklahoma City made it to the NBA Finals, but lost to the Miami Heat in five games. In 2013 the Thunder reached the Western Conference semifinals without All-Star guard Russell Westbrook, who was injured in their first round series against the Houston Rockets, only to lose to the Memphis Grizzlies. In 2014 Oklahoma City again reached the NBA's Western Conference Finals but eventually lost to the San Antonio Spurs in six games.
The Oklahoma City Thunder has been regarded by sports analysts as one of the elite franchises of the NBA's Western Conference and that of a media darling as the future of the league. Oklahoma City has earned Northwest Division titles every year since 2009 and has consistently improved its win record to 59-wins in 2014. The Thunder is led by first year head coach Billy Donovan and is anchored by several NBA superstars, including perennial All-Star point guard Russell Westbrook, 2014 MVP and four-time NBA scoring champion Kevin Durant, and Defensive Player of the Year nominee and shot-blocker Serge Ibaka.
In the aftermath of Hurricane Katrina, the NBA's New Orleans Hornets (now the New Orleans Pelicans) temporarily relocated to the Ford Center, playing the majority of its home games there during the 2005–06 and 2006–07 seasons. The team became the first NBA franchise to play regular-season games in the state of Oklahoma.[citation needed] The team was known as the New Orleans/Oklahoma City Hornets while playing in Oklahoma City. The team ultimately returned to New Orleans full-time for the 2007–08 season. The Hornets played their final home game in Oklahoma City during the exhibition season on October 9, 2007 against the Houston Rockets.
One of the more prominent landmarks downtown is the Crystal Bridge at the Myriad Botanical Gardens, a large downtown urban park. Designed by I. M. Pei, the Crystal Bridge is a tropical conservatory in the area. The park has an amphitheater, known as the Water Stage. In 2007, following a renovation of the stage, Oklahoma Shakespeare in the Park relocated to the Myriad Gardens. The Myriad Gardens will undergo a massive renovation in conjunction with the recently built Devon Tower directly north of it.
The Oklahoma City Zoo and Botanical Garden is home to numerous natural habitats, WPA era architecture and landscaping, and hosts major touring concerts during the summer at its amphitheater. Oklahoma City also has two amusement parks, Frontier City theme park and White Water Bay water park. Frontier City is an 'Old West'-themed amusement park. The park also features a recreation of a western gunfight at the 'OK Corral' and many shops that line the "Western" town's main street. Frontier City also hosts a national concert circuit at its amphitheater during the summer. Oklahoma City also has a combination racetrack and casino open year-round, Remington Park, which hosts both Quarter horse (March – June) and Thoroughbred (August – December) seasons.
Walking trails line Lake Hefner and Lake Overholser in the northwest part of the city and downtown at the canal and the Oklahoma River. The majority of the east shore area is taken up by parks and trails, including a new leashless dog park and the postwar-era Stars and Stripes Park. Lake Stanley Draper is the city's largest and most remote lake.
Oklahoma City has a major park in each quadrant of the city, going back to the first parks masterplan. Will Rogers Park, Lincoln Park, Trosper Park, and Woodson Park were once connected by the Grand Boulevard loop, some sections of which no longer exist. Martin Park Nature Center is a natural habitat in far northwest Oklahoma City. Will Rogers Park is home to the Lycan Conservatory, the Rose Garden, and Butterfly Garden, all built in the WPA era. Oklahoma City is home to the American Banjo Museum, which houses a large collection of highly decorated banjos from the early 20th century and exhibits on the history of the banjo and its place in American history. Concerts and lectures are also held there.
In April 2005, the Oklahoma City Skate Park at Wiley Post Park was renamed the Mat Hoffman Action Sports Park to recognize Mat Hoffman, an Oklahoma City area resident and businessman that was instrumental in the design of the skate park and is a 10-time BMX World Vert champion. In March 2009, the Mat Hoffman Action Sports Park was named by the National Geographic Society Travel Guide as one of the "Ten Best."
The City of Oklahoma City has operated under a council-manager form of city government since 1927. Mick Cornett serves as Mayor, having first been elected in 2004, and re-elected in 2006, 2010, and 2014. Eight councilpersons represent each of the eight wards of Oklahoma City. City Manager Jim Couch was appointed in late 2000. Couch previously served as assistant city manager, Metropolitan Area Projects Plan (MAPS) director and utilities director prior to his service as city manager.
The city is home to several colleges and universities. Oklahoma City University, formerly known as Epworth University, was founded by the United Methodist Church on September 1, 1904 and is renowned for its performing arts, science, mass communications, business, law, and athletic programs. OCU has its main campus in the north-central section of the city, near the city's chinatown area. OCU Law is located in the Midtown district near downtown, in the old Central High School building.
The University of Oklahoma has several institutions of higher learning in the city and metropolitan area, with OU Medicine and the University of Oklahoma Health Sciences Center campuses located east of downtown in the Oklahoma Health Center district, and the main campus located to the south in the suburb of Norman. The OU Medicine hosting the state's only Level-One trauma center. OU Health Sciences Center is one of the nation's largest independent medical centers, employing more than 12,000 people. OU is one of only four major universities in the nation to operate six medical schools.[clarification needed]
The third-largest university in the state, the University of Central Oklahoma, is located just north of the city in the suburb of Edmond. Oklahoma Christian University, one of the state's private liberal arts institutions, is located just south of the Edmond border, inside the Oklahoma City limits.
Oklahoma City Community College in south Oklahoma City is the second-largest community college in the state. Rose State College is located east of Oklahoma City in suburban Midwest City. Oklahoma State University–Oklahoma City is located in the "Furniture District" on the Westside. Northeast of the city is Langston University, the state's historically black college (HBCU). Langston also has an urban campus in the eastside section of the city. Southern Nazarene University, which was founded by the Church of the Nazarene, is a university located in suburban Bethany, which is surrounded by the Oklahoma City city limits.
Although technically not a university, the FAA's Mike Monroney Aeronautical Center has many aspects of an institution of higher learning. Its FAA Academy is accredited by the North Central Association of Colleges and Schools. Its Civil Aerospace Medical Institute (CAMI) has a medical education division responsible for aeromedical education in general as well as the education of aviation medical examiners in the U.S. and 93 other countries. In addition, The National Academy of Science offers Research Associateship Programs for fellowship and other grants for CAMI research.
Oklahoma City is home to the state's largest school district, Oklahoma City Public Schools. The district's Classen School of Advanced Studies and Harding Charter Preparatory High School rank high among public schools nationally according to a formula that looks at the number of Advanced Placement, International Baccalaureate and/or Cambridge tests taken by the school's students divided by the number of graduating seniors. In addition, OKCPS's Belle Isle Enterprise Middle School was named the top middle school in the state according to the Academic Performance Index, and recently received the Blue Ribbon School Award, in 2004 and again in 2011. KIPP Reach College Preparatory School in Oklahoma City received the 2012 National Blue Ribbon along with its school leader, Tracy McDaniel Sr., being awarded the Terrel H. Bell Award for Outstanding Leadership.
The Oklahoma School of Science and Mathematics, a school for some of the state's most gifted math and science pupils, is also located in Oklahoma City.
Oklahoma City has several public career and technology education schools associated with the Oklahoma Department of Career and Technology Education, the largest of which are Metro Technology Center and Francis Tuttle Technology Center.
Private career and technology education schools in Oklahoma City include Oklahoma Technology Institute, Platt College, Vatterott College, and Heritage College. The Dale Rogers Training Center in Oklahoma City is a nonprofit vocational training center for individuals with disabilities.
The Oklahoman is Oklahoma City's major daily newspaper and is the most widely circulated in the state. NewsOK.com is the Oklahoman's online presence. Oklahoma Gazette is Oklahoma City's independent newsweekly, featuring such staples as local commentary, feature stories, restaurant reviews and movie listings and music and entertainment. The Journal Record is the city's daily business newspaper and okcBIZ is a monthly publication that covers business news affecting those who live and work in Central Oklahoma.
There are numerous community and international newspapers locally that cater to the city's ethnic mosaic; such as The Black Chronicle, headquartered in the Eastside, the OK VIETIMES and Oklahoma Chinese Times, located in Asia District, and various Hispanic community publications. The Campus is the student newspaper at Oklahoma City University. Gay publications include The Gayly Oklahoman.
An upscale lifestyle publication called Slice Magazine is circulated throughout the metropolitan area. In addition, there is a magazine published by Back40 Design Group called The Edmond Outlook. It contains local commentary and human interest pieces direct-mailed to over 50,000 Edmond residents.
Oklahoma City was home to several pioneers in radio and television broadcasting. Oklahoma City's WKY Radio was the first radio station transmitting west of the Mississippi River and the third radio station in the United States. WKY received its federal license in 1921 and has continually broadcast under the same call letters since 1922. In 1928, WKY was purchased by E.K. Gaylord's Oklahoma Publishing Company and affiliated with the NBC Red Network; in 1949, WKY-TV (channel 4) went on the air and later became the first independently owned television station in the U.S. to broadcast in color. In mid-2002, WKY radio was purchased outright by Citadel Broadcasting, who was bought out by Cumulus Broadcasting in 2011. The Gaylord family earlier sold WKY-TV in 1976, which has gone through a succession of owners (what is now KFOR-TV is currently owned by Tribune Broadcasting as of December 2013).
The major U.S. broadcast television networks have affiliates in the Oklahoma City market (ranked 41st for television by Nielsen and 48th for radio by Arbitron, covering a 34-county area serving the central, northern-central and west-central sections Oklahoma); including NBC affiliate KFOR-TV (channel 4), ABC affiliate KOCO-TV (channel 5), CBS affiliate KWTV-DT (channel 9, the flagship of locally based Griffin Communications), PBS station KETA-TV (channel 13, the flagship of the state-run OETA member network), Fox affiliate KOKH-TV (channel 25), CW affiliate KOCB (channel 34), independent station KAUT-TV (channel 43), MyNetworkTV affiliate KSBI-TV (channel 52), and Ion Television owned-and-operated station KOPX-TV (channel 62). The market is also home to several religious stations including TBN owned-and-operated station KTBO-TV (channel 14) and Norman-based Daystar owned-and-operated station KOCM (channel 46).
Oklahoma City is protected by the Oklahoma City Fire Department (OKCFD), which employs 1015 paid, professional firefighters. The current Chief of Department is G. Keith Bryant, the department is also commanded by three Deputy Chiefs, who – along with the department chief – oversee the Operational Services, Prevention Services, and Support Services bureaus. The OKCFD currently operates out of 37 fire stations, located throughout the city in six battalions. The OKCFD also operates a fire apparatus fleet of 36 engines (including 30 paramedic engines), 13 ladders, 16 brush patrol units, six water tankers, two hazardous materials units, one Technical Rescue Unit, one Air Supply Unit, six Arson Investigation Units, and one Rehabilitation Unit. Each engine is staffed with a driver, an officer, and one to two firefighters, while each ladder company is staffed with a driver, an officer, and one firefighter. Minimum staffing per shift is 213 personnel. The Oklahoma City Fire Department responds to over 70,000 emergency calls annually.
Oklahoma City is an integral point on the United States Interstate Network, with three major interstate highways – Interstate 35, Interstate 40, and Interstate 44 – bisecting the city. Interstate 240 connects Interstate 40 and Interstate 44 in south Oklahoma City, while Interstate 235 spurs from Interstate 44 in north-central Oklahoma City into downtown.
Major state expressways through the city include Lake Hefner Parkway (SH-74), the Kilpatrick Turnpike, Airport Road (SH-152), and Broadway Extension (US-77) which continues from I-235 connecting Central Oklahoma City to Edmond. Lake Hefner Parkway runs through northwest Oklahoma City, while Airport Road runs through southwest Oklahoma City and leads to Will Rogers World Airport. The Kilpatrick Turnpike loops around north and west Oklahoma City.
Oklahoma City also has several major national and state highways within its city limits. Shields Boulevard (US-77) continues from E.K. Gaylord Boulevard in downtown Oklahoma City and runs south eventually connecting to I-35 near the suburb of Moore. Northwest Expressway (Oklahoma State Highway 3) runs from North Classen Boulevard in north-central Oklahoma City to the northwestern suburbs.
Oklahoma City is served by two primary airports, Will Rogers World Airport and the much smaller Wiley Post Airport (incidentally, the two honorees died in the same plane crash in Alaska) Will Rogers World Airport is the state's busiest commercial airport, with over 3.6 million passengers annually. Tinker Air Force Base, in southeast Oklahoma City, is the largest military air depot in the nation; a major maintenance and deployment facility for the Navy and the Air Force, and the second largest military institution in the state (after Fort Sill in Lawton).
METRO Transit is the city's public transit company. The main transfer terminal is located downtown at NW 5th Street and Hudson Avenue. METRO Transit maintains limited coverage of the city's main street grid using a hub-and-spoke system from the main terminal, making many journeys impractical due to the rather small number of bus routes offered and that most trips require a transfer downtown. The city has recognized that transit as a major issue for the rapidly growing and urbanizing city and has initiated several studies in recent times to improve upon the existing bus system starting with a plan known as the Fixed Guideway Study. This study identified several potential commuter transit routes from the suburbs into downtown OKC as well as feeder-line bus and/or rail routes throughout the city.
On December 2009, Oklahoma City voters passed MAPS 3, the $777 million (7-year 1-cent tax) initiative, which will include funding (appx $130M) for an estimated 5-to-6-mile (8.0 to 9.7 km) modern streetcar in downtown Oklahoma City and the establishment of a transit hub. It is believed the streetcar would begin construction in 2014 and be in operation around 2017.
Oklahoma City and the surrounding metropolitan area are home to a number of health care facilities and specialty hospitals. In Oklahoma City's MidTown district near downtown resides the state's oldest and largest single site hospital, St. Anthony Hospital and Physicians Medical Center.
OU Medicine, an academic medical institution located on the campus of The University of Oklahoma Health Sciences Center, is home to OU Medical Center. OU Medicine operates Oklahoma's only level-one trauma center at the OU Medical Center and the state's only level-one trauma center for children at Children's Hospital at OU Medicine, both of which are located in the Oklahoma Health Center district. Other medical facilities operated by OU Medicine include OU Physicians and OU Children's Physicians, the OU College of Medicine, the Oklahoma Cancer Center and OU Medical Center Edmond, the latter being located in the northern suburb of Edmond.
INTEGRIS Health owns several hospitals, including INTEGRIS Baptist Medical Center, the INTEGRIS Cancer Institute of Oklahoma, and the INTEGRIS Southwest Medical Center. INTEGRIS Health operates hospitals, rehabilitation centers, physician clinics, mental health facilities, independent living centers and home health agencies located throughout much of Oklahoma. INTEGRIS Baptist Medical Center was named in U.S. News & World Report's 2012 list of Best Hospitals. INTEGRIS Baptist Medical Center ranks high-performing in the following categories: Cardiology and Heart Surgery; Diabetes and Endocrinology; Ear, Nose and Throat; Gastroenterology; Geriatrics; Nephrology; Orthopedics; Pulmonology and Urology.
The Midwest Regional Medical Center located in the suburb of Midwest City; other major hospitals in the city include the Oklahoma Heart Hospital and the Mercy Health Center. There are 347 physicians for every 100,000 people in the city.
In the American College of Sports Medicine's annual ranking of the United States' 50 most populous metropolitan areas on the basis of community health, Oklahoma City took last place in 2010, falling five places from its 2009 rank of 45. The ACSM's report, published as part of its American Fitness Index program, cited, among other things, the poor diet of residents, low levels of physical fitness, higher incidences of obesity, diabetes, and cardiovascular disease than the national average, low access to recreational facilities like swimming pools and baseball diamonds, the paucity of parks and low investment by the city in their development, the high percentage of households below the poverty level, and the lack of state-mandated physical education curriculum as contributing factors.
Burke was born in Dublin, Ireland. His mother Mary née Nagle (c. 1702 – 1770) was a Roman Catholic who hailed from a déclassé County Cork family (and a cousin of Nano Nagle), whereas his father, a successful solicitor, Richard (died 1761), was a member of the Church of Ireland; it remains unclear whether this is the same Richard Burke who converted from Catholicism. The Burke dynasty descends from an Anglo-Norman knight surnamed de Burgh (latinised as de Burgo) who arrived in Ireland in 1185 following Henry II of England's 1171 invasion of Ireland.
In 1744, Burke started at Trinity College Dublin, a Protestant establishment, which up until 1793, did not permit Catholics to take degrees. In 1747, he set up a debating society, "Edmund Burke's Club", which, in 1770, merged with TCD's Historical Club to form the College Historical Society; it is the oldest undergraduate society in the world. The minutes of the meetings of Burke's Club remain in the collection of the Historical Society. Burke graduated from Trinity in 1748. Burke's father wanted him to read Law, and with this in mind he went to London in 1750, where he entered the Middle Temple, before soon giving up legal study to travel in Continental Europe. After eschewing the Law, he pursued a livelihood through writing.
Burke claimed that Bolingbroke's arguments against revealed religion could apply to all social and civil institutions as well. Lord Chesterfield and Bishop Warburton (and others) initially thought that the work was genuinely by Bolingbroke rather than a satire. All the reviews of the work were positive, with critics especially appreciative of Burke's quality of writing. Some reviewers failed to notice the ironic nature of the book, which led to Burke stating in the preface to the second edition (1757) that it was a satire.
Richard Hurd believed that Burke's imitation was near-perfect and that this defeated his purpose: an ironist "should take care by a constant exaggeration to make the ridicule shine through the Imitation. Whereas this Vindication is everywhere enforc'd, not only in the language, and on the principles of L. Bol., but with so apparent, or rather so real an earnestness, that half his purpose is sacrificed to the other". A minority of scholars have taken the position that, in fact, Burke did write the Vindication in earnest, later disowning it only for political reasons.
On 25 February 1757, Burke signed a contract with Robert Dodsley to write a "history of England from the time of Julius Caesar to the end of the reign of Queen Anne", its length being eighty quarto sheets (640 pages), nearly 400,000 words. It was to be submitted for publication by Christmas 1758. Burke completed the work to the year 1216 and stopped; it was not published until after Burke's death, being included in an 1812 collection of his works, entitled An Essay Towards an Abridgement of the English History. G. M. Young did not value Burke's history and claimed that it was "demonstrably a translation from the French". Lord Acton, on commenting on the story that Burke stopped his history because David Hume published his, said "it is ever to be regretted that the reverse did not occur".
During the year following that contract, with Dodsley, Burke founded the influential Annual Register, a publication in which various authors evaluated the international political events of the previous year. The extent to which Burke contributed to the Annual Register is unclear: in his biography of Burke, Robert Murray quotes the Register as evidence of Burke's opinions, yet Philip Magnus in his biography does not cite it directly as a reference. Burke remained the chief editor of the publication until at least 1789 and there is no evidence that any other writer contributed to it before 1766.
At about this same time, Burke was introduced to William Gerard Hamilton (known as "Single-speech Hamilton"). When Hamilton was appointed Chief Secretary for Ireland, Burke accompanied him to Dublin as his private secretary, a position he held for three years. In 1765 Burke became private secretary to the liberal Whig statesman, Charles, Marquess of Rockingham, then Prime Minister of Great Britain, who remained Burke's close friend and associate until his untimely death in 1782. Rockingham also introduced Burke as a Freemason.
Burke took a leading role in the debate regarding the constitutional limits to the executive authority of the king. He argued strongly against unrestrained royal power and for the role of political parties in maintaining a principled opposition capable of preventing abuses, either by the monarch, or by specific factions within the government. His most important publication in this regard was his Thoughts on the Cause of the Present Discontents of 23 April 1770. Burke identified the "discontents" as stemming from the "secret influence" of a neo-Tory group he labelled as, the "king's friends", whose system "comprehending the exterior and interior administrations, is commonly called, in the technical language of the Court, Double Cabinet". Britain needed a party with "an unshaken adherence to principle, and attachment to connexion, against every allurement of interest". Party divisions "whether operating for good or evil, are things inseparable from free government".
In May 1778, Burke supported a parliamentary motion revising restrictions on Irish trade. His constituents, citizens of the great trading city of Bristol, however urged Burke to oppose free trade with Ireland. Burke resisted their protestations and said: "If, from this conduct, I shall forfeit their suffrages at an ensuing election, it will stand on record an example to future representatives of the Commons of England, that one man at least had dared to resist the desires of his constituents when his judgment assured him they were wrong".
Burke was not merely presenting a peace agreement to Parliament; rather, he stepped forward with four reasons against using force, carefully reasoned. He laid out his objections in an orderly manner, focusing on one before moving to the next. His first concern was that the use of force would have to be temporary, and that the uprisings and objections to British governance in America would not be. Second, Burke worried about the uncertainty surrounding whether Britain would win a conflict in America. "An armament", Burke said, "is not a victory". Third, Burke brought up the issue of impairment; it would do the British Government no good to engage in a scorched earth war and have the object they desired (America) become damaged or even useless. The American colonists could always retreat into the mountains, but the land they left behind would most likely be unusable, whether by accident or design. The fourth and final reason to avoid the use of force was experience; the British had never attempted to rein in an unruly colony by force, and they did not know if it could be done, let alone accomplished thousands of miles away from home. Not only were all of these concerns reasonable, but some turned out to be prophetic – the American colonists did not surrender, even when things looked extremely bleak, and the British were ultimately unsuccessful in their attempts to win a war fought on American soil.
Among the reasons this speech was so greatly admired was its passage on Lord Bathurst (1684–1775); Burke describes an angel in 1704 prophesying to Bathurst the future greatness of England and also of America: "Young man, There is America – which at this day serves little more than to amuse you with stories of savage men, and uncouth manners; yet shall, before you taste of death, shew itself equal to the whole of that commerce which now attracts the envy of the world". Samuel Johnson was so irritated at hearing it continually praised, that he made a parody of it, where the devil appears to a young Whig and predicts that in short time, Whiggism will poison even the paradise of America!
The administration of Lord North (1770–1782) tried to defeat the colonist rebellion by military force. British and American forces clashed in 1775 and, in 1776, came the American Declaration of Independence. Burke was appalled by celebrations in Britain of the defeat of the Americans at New York and Pennsylvania. He claimed the English national character was being changed by this authoritarianism. Burke wrote: "As to the good people of England, they seem to partake every day more and more of the Character of that administration which they have been induced to tolerate. I am satisfied, that within a few years there has been a great Change in the National Character. We seem no longer that eager, inquisitive, jealous, fiery people, which we have been formerly".
The Paymaster General Act 1782 ended the post as a lucrative sinecure. Previously, Paymasters had been able to draw on money from HM Treasury at their discretion. Now they were required to put the money they had requested to withdraw from the Treasury into the Bank of England, from where it was to be withdrawn for specific purposes. The Treasury would receive monthly statements of the Paymaster's balance at the Bank. This act was repealed by Shelburne's administration, but the act that replaced it repeated verbatim almost the whole text of the Burke Act.
Burke was a leading sceptic with respect to democracy. While admitting that theoretically, in some cases it might be desirable, he insisted a democratic government in Britain in his day would not only be inept, but also oppressive. He opposed democracy for three basic reasons. First, government required a degree of intelligence and breadth of knowledge of the sort that occurred rarely among the common people. Second, he thought that if they had the vote, common people had dangerous and angry passions that could be aroused easily by demagogues; he feared that the authoritarian impulses that could be empowered by these passions would undermine cherished traditions and established religion, leading to violence and confiscation of property. Third, Burke warned that democracy would create a tyranny over unpopular minorities, who needed the protection of the upper classes.
For years Burke pursued impeachment efforts against Warren Hastings, formerly Governor-General of Bengal, that resulted in the trial during 1786. His interaction with the British dominion of India began well before Hastings' impeachment trial. For two decades prior to the impeachment, Parliament had dealt with the Indian issue. This trial was the pinnacle of years of unrest and deliberation. In 1781 Burke was first able to delve into the issues surrounding the East India Company when he was appointed Chairman of the Commons Select Committee on East Indian Affairs—from that point until the end of the trial; India was Burke's primary concern. This committee was charged "to investigate alleged injustices in Bengal, the war with Hyder Ali, and other Indian difficulties". While Burke and the committee focused their attention on these matters, a second 'secret' committee was formed to assess the same issues. Both committee reports were written by Burke. Among other purposes, the reports conveyed to the Indian princes that Britain would not wage war on them, along with demanding that the HEIC recall Hastings. This was Burke's first call for substantive change regarding imperial practices. When addressing the whole House of Commons regarding the committee report, Burke described the Indian issue as one that "began 'in commerce' but 'ended in empire.'"
On 4 April 1786, Burke presented the Commons with the Article of Charge of High Crimes and Misdemeanors against Hastings. The impeachment in Westminster Hall, which did not begin until 14 February 1788, would be the "first major public discursive event of its kind in England", bringing the morality and duty of imperialism to the forefront of public perception. Burke already was known for his eloquent rhetorical skills and his involvement in the trial only enhanced its popularity and significance. Burke's indictment, fuelled by emotional indignation, branded Hastings a 'captain-general of iniquity'; who never dined without 'creating a famine'; whose heart was 'gangrened to the core', and who resembled both a 'spider of Hell' and a 'ravenous vulture devouring the carcasses of the dead'. The House of Commons eventually impeached Hastings, but subsequently, the House of Lords acquitted him of all charges.
Initially, Burke did not condemn the French Revolution. In a letter of 9 August 1789, Burke wrote: "England gazing with astonishment at a French struggle for Liberty and not knowing whether to blame or to applaud! The thing indeed, though I thought I saw something like it in progress for several years, has still something in it paradoxical and Mysterious. The spirit it is impossible not to admire; but the old Parisian ferocity has broken out in a shocking manner". The events of 5–6 October 1789, when a crowd of Parisian women marched on Versailles to compel King Louis XVI to return to Paris, turned Burke against it. In a letter to his son, Richard Burke, dated 10 October he said: "This day I heard from Laurence who has sent me papers confirming the portentous state of France—where the Elements which compose Human Society seem all to be dissolved, and a world of Monsters to be produced in the place of it—where Mirabeau presides as the Grand Anarch; and the late Grand Monarch makes a figure as ridiculous as pitiable". On 4 November Charles-Jean-François Depont wrote to Burke, requesting that he endorse the Revolution. Burke replied that any critical language of it by him should be taken "as no more than the expression of doubt" but he added: "You may have subverted Monarchy, but not recover'd freedom". In the same month he described France as "a country undone". Burke's first public condemnation of the Revolution occurred on the debate in Parliament on the army estimates on 9 February 1790, provoked by praise of the Revolution by Pitt and Fox:
In January 1790, Burke read Dr. Richard Price's sermon of 4 November 1789 entitled, A Discourse on the Love of our Country, to the Revolution Society. That society had been founded to commemorate the Glorious Revolution of 1688. In this sermon Price espoused the philosophy of universal "Rights of Men". Price argued that love of our country "does not imply any conviction of the superior value of it to other countries, or any particular preference of its laws and constitution of government". Instead, Price asserted that Englishmen should see themselves "more as citizens of the world than as members of any particular community".
Immediately after reading Price's sermon, Burke wrote a draft of what eventually became, Reflections on the Revolution in France. On 13 February 1790, a notice in the press said that shortly, Burke would publish a pamphlet on the Revolution and its British supporters, however he spent the year revising and expanding it. On 1 November he finally published the Reflections and it was an immediate best-seller. Priced at five shillings, it was more expensive than most political pamphlets, but by the end of 1790, it had gone through ten printings and sold approximately 17,500 copies. A French translation appeared on 29 November and on 30 November the translator, Pierre-Gaëton Dupont, wrote to Burke saying 2,500 copies had already been sold. The French translation ran to ten printings by June 1791.
Burke put forward that "We fear God, we look up with awe to kings; with affection to parliaments; with duty to magistrates; with reverence to priests; and with respect to nobility. Why? Because when such ideas are brought before our minds, it is natural to be so affected". Burke defended this prejudice on the grounds that it is "the general bank and capital of nations, and of ages" and superior to individual reason, which is small in comparison. "Prejudice", Burke claimed, "is of ready application in the emergency; it previously engages the mind in a steady course of wisdom and virtue, and does not leave the man hesitating in the moment of decision, skeptical, puzzled, and unresolved. Prejudice renders a man's virtue his habit". Burke criticised social contract theory by claiming that society is indeed, a contract, but "a partnership not only between those who are living, but between those who are living, those who are dead, and those who are to be born".
The most famous passage in Burke's Reflections was his description of the events of 5–6 October 1789 and the part of Marie-Antoinette in them. Burke's account differs little from modern historians who have used primary sources. His use of flowery language to describe it, however, provoked both praise and criticism. Philip Francis wrote to Burke saying that what he wrote of Marie-Antoinette was "pure foppery". Edward Gibbon, however, reacted differently: "I adore his chivalry". Burke was informed by an Englishman who had talked with the Duchesse de Biron, that when Marie-Antoinette was reading the passage, she burst into tears and took considerable time to finish reading it. Price had rejoiced that the French king had been "led in triumph" during the October Days, but to Burke this symbolised the opposing revolutionary sentiment of the Jacobins and the natural sentiments of those who shared his own view with horror—that the ungallant assault on Marie-Antoinette—was a cowardly attack on a defenceless woman.
Louis XVI translated the Reflections "from end to end" into French. Fellow Whig MPs Richard Sheridan and Charles James Fox, disagreed with Burke and split with him. Fox thought the Reflections to be "in very bad taste" and "favouring Tory principles". Other Whigs such as the Duke of Portland and Earl Fitzwilliam privately agreed with Burke, but did not wish for a public breach with their Whig colleagues. Burke wrote on 29 November 1790: "I have received from the Duke of Portland, Lord Fitzwilliam, the Duke of Devonshire, Lord John Cavendish, Montagu (Frederick Montagu MP), and a long et cetera of the old Stamina of the Whiggs a most full approbation of the principles of that work and a kind indulgence to the execution". The Duke of Portland said in 1791 that when anyone criticised the Reflections to him, he informed them that he had recommended the book to his sons as containing the true Whig creed.
Burke's Reflections sparked a pamphlet war. Thomas Paine penned the Rights of Man in 1791 as a response to Burke; Mary Wollstonecraft published A Vindication of the Rights of Men and James Mackintosh wrote Vindiciae Gallicae. Mackintosh was the first to see the Reflections as "the manifesto of a Counter Revolution". Mackintosh later agreed with Burke's views, remarking in December 1796 after meeting him, that Burke was "minutely and accurately informed, to a wonderful exactness, with respect to every fact relating to the French Revolution". Mackintosh later said: "Burke was one of the first thinkers as well as one of the greatest orators of his time. He is without parallel in any age, excepting perhaps Lord Bacon and Cicero; and his works contain an ampler store of political and moral wisdom than can be found in any other writer whatever".
In November 1790, François-Louis-Thibault de Menonville, a member of the National Assembly of France, wrote to Burke, praising Reflections and requesting more "very refreshing mental food" that he could publish. This Burke did in April 1791 when he published A Letter to a Member of the National Assembly. Burke called for external forces to reverse the revolution and included an attack on the late French philosopher Jean-Jacques Rousseau, as being the subject of a personality cult that had developed in revolutionary France. Although Burke conceded that Rousseau sometimes showed "a considerable insight into human nature" he mostly was critical. Although he did not meet Rousseau on his visit to Britain in 1766–7 Burke was a friend of David Hume, with whom Rousseau had stayed. Burke said Rousseau "entertained no principle either to influence of his heart, or to guide his understanding—but vanity"—which he "was possessed to a degree little short of madness". He also cited Rousseau's Confessions as evidence that Rousseau had a life of "obscure and vulgar vices" that was not "chequered, or spotted here and there, with virtues, or even distinguished by a single good action". Burke contrasted Rousseau's theory of universal benevolence and his having sent his children to a foundling hospital: "a lover of his kind, but a hater of his kindred".
These events and the disagreements that arose from them within the Whig Party, led to its break-up and to the rupture of Burke's friendship with Fox. In debate in Parliament on Britain's relations with Russia, Fox praised the principles of the revolution, although Burke was not able to reply at this time as he was "overpowered by continued cries of question from his own side of the House". When Parliament was debating the Quebec Bill for a constitution for Canada, Fox praised the revolution and criticised some of Burke's arguments, such as hereditary power. On 6 May 1791, during another debate in Parliament on the Quebec Bill, Burke used the opportunity to answer Fox, and to condemn the new French Constitution and "the horrible consequences flowing from the French idea of the Rights of Man". Burke asserted that those ideas were the antithesis of both the British and the American constitutions. Burke was interrupted, and Fox intervened, saying that Burke should be allowed to carry on with his speech. A vote of censure was moved against Burke, however, for noticing the affairs of France, which was moved by Lord Sheffield and seconded by Fox. Pitt made a speech praising Burke, and Fox made a speech—both rebuking and complimenting Burke. He questioned the sincerity of Burke, who seemed to have forgotten the lessons he had learned from him, quoting from Burke's own speeches of fourteen and fifteen years before.
At this point, Fox whispered that there was "no loss of friendship". "I regret to say there is", Burke replied, "I have indeed made a great sacrifice; I have done my duty though I have lost my friend. There is something in the detested French constitution that envenoms every thing it touches". This provoked a reply from Fox, yet he was unable to give his speech for some time since he was overcome with tears and emotion, he appealed to Burke to remember their inalienable friendship, but also repeated his criticisms of Burke and uttered "unusually bitter sarcasms". This only aggravated the rupture between the two men. Burke demonstrated his separation from the party on 5 June 1791 by writing to Fitzwilliam, declining money from him.
Burke knew that many members of the Whig Party did not share Fox's views and he wanted to provoke them into condemning the French Revolution. Burke wrote that he wanted to represent the whole Whig party "as tolerating, and by a toleration, countenancing those proceedings" so that he could "stimulate them to a public declaration of what every one of their acquaintance privately knows to be...their sentiments". Therefore, on 3 August 1791 Burke published his Appeal from the New to the Old Whigs, in which he renewed his criticism of the radical revolutionary programmes inspired by the French Revolution and attacked the Whigs who supported them, as holding principles contrary to those traditionally held by the Whig party.
Although Whig grandees such as Portland and Fitzwilliam privately agreed with Burke's Appeal, they wished he had used more moderate language. Fitzwilliam saw the Appeal as containing "the doctrines I have sworn by, long and long since". Francis Basset, a backbench Whig MP, wrote to Burke: "...though for reasons which I will not now detail I did not then deliver my sentiments, I most perfectly differ from Mr. Fox & from the great Body of opposition on the French Revolution". Burke sent a copy of the Appeal to the king and the king requested a friend to communicate to Burke that he had read it "with great Satisfaction". Burke wrote of its reception: "Not one word from one of our party. They are secretly galled. They agree with me to a title; but they dare not speak out for fear of hurting Fox. ... They leave me to myself; they see that I can do myself justice". Charles Burney viewed it as "a most admirable book—the best & most useful on political subjects that I have ever seen" but believed the differences in the Whig Party between Burke and Fox should not be aired publicly.
Burke supported the war against revolutionary France, seeing Britain as fighting on the side of the royalists and émigres in a civil war, rather than fighting against the whole nation of France. Burke also supported the royalist uprising in La Vendée, describing it on 4 November 1793 in a letter to William Windham, as "the sole affair I have much heart in". Burke wrote to Henry Dundas on 7 October urging him to send reinforcements there, as he viewed it as the only theatre in the war that might lead to a march on Paris. Dundas did not follow Burke's advice, however.
Burke believed the Government was not taking the uprising seriously enough, a view reinforced by a letter he had received from the Prince Charles of France (S.A.R. le comte d'Artois), dated 23 October, requesting that he intercede on behalf of the royalists to the Government. Burke was forced to reply on 6 November: "I am not in His Majesty's Service; or at all consulted in his Affairs". Burke published his Remarks on the Policy of the Allies with Respect to France, begun in October, where he said: "I am sure every thing has shewn us that in this war with France, one Frenchman is worth twenty foreigners. La Vendée is a proof of this".
On 20 June 1794, Burke received a vote of thanks from the Commons for his services in the Hastings Trial and he immediately resigned his seat, being replaced by his son Richard. A tragic blow fell upon Burke with the loss of Richard in August 1794, to whom he was tenderly attached, and in whom he saw signs of promise, which were not patent to others and which, in fact, appear to have been non-existent (though this view may have rather reflected the fact that Richard Burke had worked successfully in the early battle for Catholic emancipation). King George III, whose favour he had gained by his attitude on the French Revolution, wished to create him Earl of Beaconsfield, but the death of his son deprived the opportunity of such an honour and all its attractions, so the only award he would accept was a pension of £2,500. Even this modest reward was attacked by the Duke of Bedford and the Earl of Lauderdale, to whom Burke replied in his Letter to a Noble Lord (1796): "It cannot at this time be too often repeated; line upon line; precept upon precept; until it comes into the currency of a proverb, To innovate is not to reform". He argued that he was rewarded on merit, but the Duke of Bedford received his rewards from inheritance alone, his ancestor being the original pensioner: "Mine was from a mild and benevolent sovereign; his from Henry the Eighth". Burke also hinted at what would happen to such people if their revolutionary ideas were implemented, and included a description of the British constitution:
Burke's last publications were the Letters on a Regicide Peace (October 1796), called forth by negotiations for peace with France by the Pitt government. Burke regarded this as appeasement, injurious to national dignity and honour. In his Second Letter, Burke wrote of the French Revolutionary Government: "Individuality is left out of their scheme of government. The State is all in all. Everything is referred to the production of force; afterwards, everything is trusted to the use of it. It is military in its principle, in its maxims, in its spirit, and in all its movements. The State has dominion and conquest for its sole objects—dominion over minds by proselytism, over bodies by arms".
This is held to be the first explanation of the modern concept of totalitarian state. Burke regarded the war with France as ideological, against an "armed doctrine". He wished that France would not be partitioned due to the effect this would have on the balance of power in Europe, and that the war was not against France, but against the revolutionaries governing her. Burke said: "It is not France extending a foreign empire over other nations: it is a sect aiming at universal empire, and beginning with the conquest of France".
In November 1795, there was a debate in Parliament on the high price of corn and Burke wrote a memorandum to Pitt on the subject. In December Samuel Whitbread MP introduced a bill giving magistrates the power to fix minimum wages and Fox said he would vote for it. This debate probably led Burke to editing his memorandum, as there appeared a notice that Burke would soon publish a letter on the subject to the Secretary of the Board of Agriculture, Arthur Young; but he failed to complete it. These fragments were inserted into the memorandum after his death and published posthumously in 1800 as, Thoughts and Details on Scarcity. In it, Burke expounded "some of the doctrines of political economists bearing upon agriculture as a trade". Burke criticised policies such as maximum prices and state regulation of wages, and set out what the limits of government should be:
Writing to a friend in May 1795, Burke surveyed the causes of discontent: "I think I can hardly overrate the malignity of the principles of Protestant ascendency, as they affect Ireland; or of Indianism [i.e. corporate tyranny, as practiced by the British East Indies Company], as they affect these countries, and as they affect Asia; or of Jacobinism, as they affect all Europe, and the state of human society itself. The last is the greatest evil". By March 1796, however Burke had changed his mind: "Our Government and our Laws are beset by two different Enemies, which are sapping its foundations, Indianism, and Jacobinism. In some Cases they act separately, in some they act in conjunction: But of this I am sure; that the first is the worst by far, and the hardest to deal with; and for this amongst other reasons, that it weakens discredits, and ruins that force, which ought to be employed with the greatest Credit and Energy against the other; and that it furnishes Jacobinism with its strongest arms against all formal Government".
Burke believed that property was essential to human life. Because of his conviction that people desire to be ruled and controlled, the division of property formed the basis for social structure, helping develop control within a property-based hierarchy. He viewed the social changes brought on by property as the natural order of events, which should be taking place as the human race progressed. With the division of property and the class system, he also believed that it kept the monarch in check to the needs of the classes beneath the monarch. Since property largely aligned or defined divisions of social class, class too, was seen as natural—part of a social agreement that the setting of persons into different classes, is the mutual benefit of all subjects. Concern for property is not Burke's only influence. As Christopher Hitchens summarises, "If modern conservatism can be held to derive from Burke, it is not just because he appealed to property owners in behalf of stability but also because he appealed to an everyday interest in the preservation of the ancestral and the immemorial."
In the nineteenth century Burke was praised by both liberals and conservatives. Burke's friend Philip Francis wrote that Burke "was a man who truly & prophetically foresaw all the consequences which would rise from the adoption of the French principles" but because Burke wrote with so much passion, people were doubtful of his arguments. William Windham spoke from the same bench in the House of Commons as Burke had, when he had separated from Fox, and an observer said Windham spoke "like the ghost of Burke" when he made a speech against peace with France in 1801. William Hazlitt, a political opponent of Burke, regarded him as amongst his three favourite writers (the others being Junius and Rousseau), and made it "a test of the sense and candour of any one belonging to the opposite party, whether he allowed Burke to be a great man". William Wordsworth was originally a supporter of the French Revolution and attacked Burke in 'A Letter to the Bishop of Llandaff' (1793), but by the early nineteenth century he had changed his mind and came to admire Burke. In his Two Addresses to the Freeholders of Westmorland Wordsworth called Burke "the most sagacious Politician of his age" whose predictions "time has verified". He later revised his poem The Prelude to include praise of Burke ("Genius of Burke! forgive the pen seduced/By specious wonders") and portrayed him as an old oak. Samuel Taylor Coleridge came to have a similar conversion: he had criticised Burke in The Watchman, but in his Friend (1809–10) Coleridge defended Burke from charges of inconsistency. Later, in his Biographia Literaria (1817) Coleridge hails Burke as a prophet and praises Burke for referring "habitually to principles. He was a scientific statesman; and therefore a seer". Henry Brougham wrote of Burke: "... all his predictions, save one momentary expression, had been more than fulfilled: anarchy and bloodshed had borne sway in France; conquest and convulsion had desolated Europe...the providence of mortals is not often able to penetrate so far as this into futurity". George Canning believed that Burke's Reflections "has been justified by the course of subsequent events; and almost every prophecy has been strictly fulfilled". In 1823 Canning wrote that he took Burke's "last works and words [as] the manual of my politics". The Conservative Prime Minister Benjamin Disraeli "was deeply penetrated with the spirit and sentiment of Burke's later writings".
The 19th-century Liberal Prime Minister William Ewart Gladstone considered Burke "a magazine of wisdom on Ireland and America" and in his diary recorded: "Made many extracts from Burke—sometimes almost divine". The Radical MP and anti-Corn Law activist Richard Cobden often praised Burke's Thoughts and Details on Scarcity. The Liberal historian Lord Acton considered Burke one of the three greatest Liberals, along with William Gladstone and Thomas Babington Macaulay. Lord Macaulay recorded in his diary: "I have now finished reading again most of Burke's works. Admirable! The greatest man since Milton". The Gladstonian Liberal MP John Morley published two books on Burke (including a biography) and was influenced by Burke, including his views on prejudice. The Cobdenite Radical Francis Hirst thought Burke deserved "a place among English libertarians, even though of all lovers of liberty and of all reformers he was the most conservative, the least abstract, always anxious to preserve and renovate rather than to innovate. In politics he resembled the modern architect who would restore an old house instead of pulling it down to construct a new one on the site". Burke's Reflections on the Revolution in France was controversial at the time of its publication, but after his death, it was to become his best known and most influential work, and a manifesto for Conservative thinking.
The historian Piers Brendon asserts that Burke laid the moral foundations for the British Empire, epitomised in the trial of Warren Hastings, that was ultimately to be its undoing: when Burke stated that "The British Empire must be governed on a plan of freedom, for it will be governed by no other", this was "...an ideological bacillus that would prove fatal. This was Edmund Burke's paternalistic doctrine that colonial government was a trust. It was to be so exercised for the benefit of subject people that they would eventually attain their birthright—freedom". As a consequence of this opinion, Burke objected to the opium trade, which he called a "smuggling adventure" and condemned "the great Disgrace of the British character in India".
Burke's religious writing comprises published works and commentary on the subject of religion. Burke's religious thought was grounded in the belief that religion is the foundation of civil society. He sharply criticised deism and atheism, and emphasised Christianity as a vehicle of social progress. Born in Ireland to a Catholic mother and a Protestant father, Burke vigorously defended the Anglican Church, but also demonstrated sensitivity to Catholic concerns. He linked the conservation of a state (established) religion with the preservation of citizens' constitutional liberties and highlighted Christianity's benefit not only to the believer's soul, but also to political arrangements.
Physically, clothing serves many purposes: it can serve as protection from the elements, and can enhance safety during hazardous activities such as hiking and cooking. It protects the wearer from rough surfaces, rash-causing plants, insect bites, splinters, thorns and prickles by providing a barrier between the skin and the environment. Clothes can insulate against cold or hot conditions. Further, they can provide a hygienic barrier, keeping infectious and toxic materials away from the body. Clothing also provides protection from harmful UV radiation.
There is no easy way to determine when clothing was first developed, but some information has been inferred by studying lice. The body louse specifically lives in clothing, and diverge from head lice about 107,000 years ago, suggesting that clothing existed at that time. Another theory is that modern humans are the only survivors of several species of primates who may have worn clothes and that clothing may have been used as long ago as 650 thousand years ago. Other louse-based estimates put the introduction of clothing at around 42,000–72,000 BP.
The most obvious function of clothing is to improve the comfort of the wearer, by protecting the wearer from the elements. In hot climates, clothing provides protection from sunburn or wind damage, while in cold climates its thermal insulation properties are generally more important. Shelter usually reduces the functional need for clothing. For example, coats, hats, gloves, and other superficial layers are normally removed when entering a warm home, particularly if one is residing or sleeping there. Similarly, clothing has seasonal and regional aspects, so that thinner materials and fewer layers of clothing are generally worn in warmer seasons and regions than in colder ones.
Clothing can and has in history been made from a very wide variety of materials. Materials have ranged from leather and furs, to woven materials, to elaborate and exotic natural and synthetic fabrics. Not all body coverings are regarded as clothing. Articles carried rather than worn (such as purses), worn on a single part of the body and easily removed (scarves), worn purely for adornment (jewelry), or those that serve a function other than protection (eyeglasses), are normally considered accessories rather than clothing, as are footwear and hats.
Clothing protects against many things that might injure the uncovered human body. Clothes protect people from the elements, including rain, snow, wind, and other weather, as well as from the sun. However, clothing that is too sheer, thin, small, tight, etc., offers less protection. Clothes also reduce risk during activities such as work or sport. Some clothing protects from specific environmental hazards, such as insects, noxious chemicals, weather, weapons, and contact with abrasive substances. Conversely, clothing may protect the environment from the clothing wearer, as with doctors wearing medical scrubs.
Humans have shown extreme inventiveness in devising clothing solutions to environmental hazards. Examples include: space suits, air conditioned clothing, armor, diving suits, swimsuits, bee-keeper gear, motorcycle leathers, high-visibility clothing, and other pieces of protective clothing. Meanwhile, the distinction between clothing and protective equipment is not always clear-cut—since clothes designed to be fashionable often have protective value and clothes designed for function often consider fashion in their design. Wearing clothes also has social implications. They cover parts of the body that social norms require to be covered, act as a form of adornment, and serve other social purposes.
Although dissertations on clothing and its function appear from the 19th century as colonising countries dealt with new environments, concerted scientific research into psycho-social, physiological and other functions of clothing (e.g. protective, cartage) occurred in the first half of the 20th century, with publications such as J. C. Flügel's Psychology of Clothes in 1930, and Newburgh's seminal Physiology of Heat Regulation and The Science of Clothing in 1949. By 1968, the field of environmental physiology had advanced and expanded significantly, but the science of clothing in relation to environmental physiology had changed little. While considerable research has since occurred and the knowledge-base has grown significantly, the main concepts remain unchanged, and indeed Newburgh's book is still cited by contemporary authors, including those attempting to develop thermoregulatory models of clothing development.
In Western societies, skirts, dresses and high-heeled shoes are usually seen as women's clothing, while neckties are usually seen as men's clothing. Trousers were once seen as exclusively male clothing, but are nowadays worn by both genders. Male clothes are often more practical (that is, they can function well under a wide variety of situations), but a wider range of clothing styles are available for females. Males are typically allowed to bare their chests in a greater variety of public places. It is generally acceptable for a woman to wear traditionally male clothing, while the converse is unusual.
In some societies, clothing may be used to indicate rank or status. In ancient Rome, for example, only senators could wear garments dyed with Tyrian purple. In traditional Hawaiian society, only high-ranking chiefs could wear feather cloaks and palaoa, or carved whale teeth. Under the Travancore Kingdom of Kerala, (India), lower caste women had to pay a tax for the right to cover their upper body. In China, before establishment of the republic, only the emperor could wear yellow. History provides many examples of elaborate sumptuary laws that regulated what people could wear. In societies without such laws, which includes most modern societies, social status is instead signaled by the purchase of rare or luxury items that are limited by cost to those with wealth or status. In addition, peer pressure influences clothing choice.
According to archaeologists and anthropologists, the earliest clothing likely consisted of fur, leather, leaves, or grass that were draped, wrapped, or tied around the body. Knowledge of such clothing remains inferential, since clothing materials deteriorate quickly compared to stone, bone, shell and metal artifacts. Archeologists have identified very early sewing needles of bone and ivory from about 30,000 BC, found near Kostenki, Russia in 1988. Dyed flax fibers that could have been used in clothing have been found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP.
Scientists are still debating when people started wearing clothes. Ralf Kittler, Manfred Kayser and Mark Stoneking, anthropologists at the Max Planck Institute for Evolutionary Anthropology, have conducted a genetic analysis of human body lice that suggests clothing originated quite recently, around 170,000 years ago. Body lice is an indicator of clothes-wearing, since most humans have sparse body hair, and lice thus require human clothing to survive. Their research suggests the invention of clothing may have coincided with the northward migration of modern Homo sapiens away from the warm climate of Africa, thought to have begun between 50,000 and 100,000 years ago. However, a second group of researchers using similar genetic methods estimate that clothing originated around 540,000 years ago (Reed et al. 2004. PLoS Biology 2(11): e340). For now, the date of the origin of clothing remains unresolved.[citation needed]
Different cultures have evolved various ways of creating clothes out of cloth. One approach simply involves draping the cloth. Many people wore, and still wear, garments consisting of rectangles of cloth wrapped to fit – for example, the dhoti for men and the sari for women in the Indian subcontinent, the Scottish kilt or the Javanese sarong. The clothes may simply be tied up, as is the case of the first two garments; or pins or belts hold the garments in place, as in the case of the latter two. The precious cloth remains uncut, and people of various sizes or the same person at different sizes can wear the garment.
By the early years of the 21st century, western clothing styles had, to some extent, become international styles. This process began hundreds of years earlier, during the periods of European colonialism. The process of cultural dissemination has perpetuated over the centuries as Western media corporations have penetrated markets throughout the world, spreading Western culture and styles. Fast fashion clothing has also become a global phenomenon. These garments are less expensive, mass-produced Western clothing. Donated used clothing from Western countries are also delivered to people in poor countries by charity organizations.
Most sports and physical activities are practiced wearing special clothing, for practical, comfort or safety reasons. Common sportswear garments include shorts, T-shirts, tennis shirts, leotards, tracksuits, and trainers. Specialized garments include wet suits (for swimming, diving or surfing), salopettes (for skiing) and leotards (for gymnastics). Also, spandex materials are often used as base layers to soak up sweat. Spandex is also preferable for active sports that require form fitting garments, such as volleyball, wrestling, track & field, dance, gymnastics and swimming.
The world of clothing is always changing, as new cultural influences meet technological innovations. Researchers in scientific labs have been developing prototypes for fabrics that can serve functional purposes well beyond their traditional roles, for example, clothes that can automatically adjust their temperature, repel bullets, project images, and generate electricity. Some practical advances already available to consumers are bullet-resistant garments made with kevlar and stain-resistant fabrics that are coated with chemical mixtures that reduce the absorption of liquids.
Though mechanization transformed most aspects of human industry by the mid-20th century, garment workers have continued to labor under challenging conditions that demand repetitive manual labor. Mass-produced clothing is often made in what are considered by some to be sweatshops, typified by long work hours, lack of benefits, and lack of worker representation. While most examples of such conditions are found in developing countries, clothes made in industrialized nations may also be manufactured similarly, often staffed by undocumented immigrants.[citation needed]
Outsourcing production to low wage countries like Bangladesh, China, India and Sri Lanka became possible when the Multi Fibre Agreement (MFA) was abolished. The MFA, which placed quotas on textiles imports, was deemed a protectionist measure.[citation needed] Globalization is often quoted as the single most contributing factor to the poor working conditions of garment workers. Although many countries recognize treaties like the International Labor Organization, which attempt to set standards for worker safety and rights, many countries have made exceptions to certain parts of the treaties or failed to thoroughly enforce them. India for example has not ratified sections 87 and 92 of the treaty.[citation needed]
The use of animal fur in clothing dates to prehistoric times. It is currently associated in developed countries with expensive, designer clothing, although fur is still used by indigenous people in arctic zones and higher elevations for its warmth and protection. Once uncontroversial, it has recently been the focus of campaigns on the grounds that campaigners consider it cruel and unnecessary. PETA, along with other animal rights and animal liberation groups have called attention to fur farming and other practices they consider cruel.
Many kinds of clothing are designed to be ironed before they are worn to remove wrinkles. Most modern formal and semi-formal clothing is in this category (for example, dress shirts and suits). Ironed clothes are believed to look clean, fresh, and neat. Much contemporary casual clothing is made of knit materials that do not readily wrinkle, and do not require ironing. Some clothing is permanent press, having been treated with a coating (such as polytetrafluoroethylene) that suppresses wrinkles and creates a smooth appearance without ironing.
A resin used for making non-wrinkle shirts releases formaldehyde, which could cause contact dermatitis for some people; no disclosure requirements exist, and in 2008 the U.S. Government Accountability Office tested formaldehyde in clothing and found that generally the highest levels were in non-wrinkle shirts and pants. In 1999, a study of the effect of washing on the formaldehyde levels found that after 6 months after washing, 7 of 27 shirts had levels in excess of 75 ppm, which is a safe limit for direct skin exposure.
In past times, mending was an art. A meticulous tailor or seamstress could mend rips with thread raveled from hems and seam edges so skillfully that the tear was practically invisible. When the raw material – cloth – was worth more than labor, it made sense to expend labor in saving it. Today clothing is considered a consumable item. Mass-manufactured clothing is less expensive than the labor required to repair it. Many people buy a new piece of clothing rather than spend time mending. The thrifty still replace zippers and buttons and sew up ripped hems.
Neptune is the eighth and farthest known planet from the Sun in the Solar System. It is the fourth-largest planet by diameter and the third-largest by mass. Among the giant planets in the Solar System, Neptune is the most dense. Neptune is 17 times the mass of Earth and is slightly more massive than its near-twin Uranus, which is 15 times the mass of Earth and slightly larger than Neptune.[c] Neptune orbits the Sun once every 164.8 years at an average distance of 30.1 astronomical units (4.50×109 km). Named after the Roman god of the sea, its astronomical symbol is ♆, a stylised version of the god Neptune's trident.
Neptune is not visible to the unaided eye and is the only planet in the Solar System found by mathematical prediction rather than by empirical observation. Unexpected changes in the orbit of Uranus led Alexis Bouvard to deduce that its orbit was subject to gravitational perturbation by an unknown planet. Neptune was subsequently observed with a telescope on 23 September 1846 by Johann Galle within a degree of the position predicted by Urbain Le Verrier. Its largest moon, Triton, was discovered shortly thereafter, though none of the planet's remaining known 14 moons were located telescopically until the 20th century. The planet's distance from Earth gives it a very small apparent size, making it challenging to study with Earth-based telescopes. Neptune was visited by Voyager 2, when it flew by the planet on 25 August 1989. The advent of Hubble Space Telescope and large ground-based telescopes with adaptive optics has recently allowed for additional detailed observations from afar.
Neptune is similar in composition to Uranus, and both have compositions that differ from those of the larger gas giants, Jupiter and Saturn. Like Jupiter and Saturn, Neptune's atmosphere is composed primarily of hydrogen and helium, along with traces of hydrocarbons and possibly nitrogen, but contains a higher proportion of "ices" such as water, ammonia, and methane. However, its interior, like that of Uranus, is primarily composed of ices and rock, and hence Uranus and Neptune are normally considered "ice giants" to emphasise this distinction. Traces of methane in the outermost regions in part account for the planet's blue appearance.
In contrast to the hazy, relatively featureless atmosphere of Uranus, Neptune's atmosphere has active and visible weather patterns. For example, at the time of the Voyager 2 flyby in 1989, the planet's southern hemisphere had a Great Dark Spot comparable to the Great Red Spot on Jupiter. These weather patterns are driven by the strongest sustained winds of any planet in the Solar System, with recorded wind speeds as high as 2,100 kilometres per hour (580 m/s; 1,300 mph). Because of its great distance from the Sun, Neptune's outer atmosphere is one of the coldest places in the Solar System, with temperatures at its cloud tops approaching 55 K (−218 °C). Temperatures at the planet's centre are approximately 5,400 K (5,100 °C). Neptune has a faint and fragmented ring system (labelled "arcs"), which was first detected during the 1960s and confirmed by Voyager 2.
Some of the earliest recorded observations ever made through a telescope, Galileo's drawings on 28 December 1612 and 27 January 1613, contain plotted points that match up with what is now known to be the position of Neptune. On both occasions, Galileo seems to have mistaken Neptune for a fixed star when it appeared close—in conjunction—to Jupiter in the night sky; hence, he is not credited with Neptune's discovery. At his first observation in December 1612, Neptune was almost stationary in the sky because it had just turned retrograde that day. This apparent backward motion is created when Earth's orbit takes it past an outer planet. Because Neptune was only beginning its yearly retrograde cycle, the motion of the planet was far too slight to be detected with Galileo's small telescope. In July 2009, University of Melbourne physicist David Jamieson announced new evidence suggesting that Galileo was at least aware that the 'star' he had observed had moved relative to the fixed stars.
In 1821, Alexis Bouvard published astronomical tables of the orbit of Neptune's neighbour Uranus. Subsequent observations revealed substantial deviations from the tables, leading Bouvard to hypothesise that an unknown body was perturbing the orbit through gravitational interaction. In 1843, John Couch Adams began work on the orbit of Uranus using the data he had. Via Cambridge Observatory director James Challis, he requested extra data from Sir George Airy, the Astronomer Royal, who supplied it in February 1844. Adams continued to work in 1845–46 and produced several different estimates of a new planet.
Meanwhile, Le Verrier by letter urged Berlin Observatory astronomer Johann Gottfried Galle to search with the observatory's refractor. Heinrich d'Arrest, a student at the observatory, suggested to Galle that they could compare a recently drawn chart of the sky in the region of Le Verrier's predicted location with the current sky to seek the displacement characteristic of a planet, as opposed to a fixed star. On the evening of 23 September 1846, the day Galle received the letter, he discovered Neptune within 1° of where Le Verrier had predicted it to be, about 12° from Adams' prediction. Challis later realised that he had observed the planet twice, on 4 and 12 August, but did not recognise it as a planet because he lacked an up-to-date star map and was distracted by his concurrent work on comet observations.
In the wake of the discovery, there was much nationalistic rivalry between the French and the British over who deserved credit for the discovery. Eventually, an international consensus emerged that both Le Verrier and Adams jointly deserved credit. Since 1966, Dennis Rawlins has questioned the credibility of Adams's claim to co-discovery, and the issue was re-evaluated by historians with the return in 1998 of the "Neptune papers" (historical documents) to the Royal Observatory, Greenwich. After reviewing the documents, they suggest that "Adams does not deserve equal credit with Le Verrier for the discovery of Neptune. That credit belongs only to the person who succeeded both in predicting the planet's place and in convincing astronomers to search for it."
Claiming the right to name his discovery, Le Verrier quickly proposed the name Neptune for this new planet, though falsely stating that this had been officially approved by the French Bureau des Longitudes. In October, he sought to name the planet Le Verrier, after himself, and he had loyal support in this from the observatory director, François Arago. This suggestion met with stiff resistance outside France. French almanacs quickly reintroduced the name Herschel for Uranus, after that planet's discoverer Sir William Herschel, and Leverrier for the new planet.
Most languages today, even in countries that have no direct link to Greco-Roman culture, use some variant of the name "Neptune" for the planet. However, in Chinese, Japanese, and Korean, the planet's name was translated as "sea king star" (海王星), because Neptune was the god of the sea. In Mongolian, Neptune is called Dalain Van (Далайн ван), reflecting its namesake god's role as the ruler of the sea. In modern Greek the planet is called Poseidon (Ποσειδώνας, Poseidonas), the Greek counterpart of Neptune. In Hebrew, "Rahab" (רהב), from a Biblical sea monster mentioned in the Book of Psalms, was selected in a vote managed by the Academy of the Hebrew Language in 2009 as the official name for the planet, even though the existing Latin term "Neptun" (נפטון) is commonly used. In Māori, the planet is called Tangaroa, named after the Māori god of the sea. In Nahuatl, the planet is called Tlāloccītlalli, named after the rain god Tlāloc.
From its discovery in 1846 until the subsequent discovery of Pluto in 1930, Neptune was the farthest known planet. When Pluto was discovered it was considered a planet, and Neptune thus became the penultimate known planet, except for a 20-year period between 1979 and 1999 when Pluto's elliptical orbit brought it closer to the Sun than Neptune. The discovery of the Kuiper belt in 1992 led many astronomers to debate whether Pluto should be considered a planet or as part of the Kuiper belt. In 2006, the International Astronomical Union defined the word "planet" for the first time, reclassifying Pluto as a "dwarf planet" and making Neptune once again the outermost known planet in the Solar System.
Neptune's mass of 1.0243×1026 kg, is intermediate between Earth and the larger gas giants: it is 17 times that of Earth but just 1/19th that of Jupiter.[d] Its gravity at 1 bar is 11.15 m/s2, 1.14 times the surface gravity of Earth, and surpassed only by Jupiter. Neptune's equatorial radius of 24,764 km is nearly four times that of Earth. Neptune, like Uranus, is an ice giant, a subclass of giant planet, due to their smaller size and higher concentrations of volatiles relative to Jupiter and Saturn. In the search for extrasolar planets, Neptune has been used as a metonym: discovered bodies of similar mass are often referred to as "Neptunes", just as scientists refer to various extrasolar bodies as "Jupiters".
The mantle is equivalent to 10 to 15 Earth masses and is rich in water, ammonia and methane. As is customary in planetary science, this mixture is referred to as icy even though it is a hot, dense fluid. This fluid, which has a high electrical conductivity, is sometimes called a water–ammonia ocean. The mantle may consist of a layer of ionic water in which the water molecules break down into a soup of hydrogen and oxygen ions, and deeper down superionic water in which the oxygen crystallises but the hydrogen ions float around freely within the oxygen lattice. At a depth of 7000 km, the conditions may be such that methane decomposes into diamond crystals that rain downwards like hailstones. Very-high-pressure experiments at the Lawrence Livermore National Laboratory suggest that the base of the mantle may comprise an ocean of liquid carbon with floating solid 'diamonds'.
At high altitudes, Neptune's atmosphere is 80% hydrogen and 19% helium. A trace amount of methane is also present. Prominent absorption bands of methane exist at wavelengths above 600 nm, in the red and infrared portion of the spectrum. As with Uranus, this absorption of red light by the atmospheric methane is part of what gives Neptune its blue hue, although Neptune's vivid azure differs from Uranus's milder cyan. Because Neptune's atmospheric methane content is similar to that of Uranus, some unknown atmospheric constituent is thought to contribute to Neptune's colour.
Models suggest that Neptune's troposphere is banded by clouds of varying compositions depending on altitude. The upper-level clouds lie at pressures below one bar, where the temperature is suitable for methane to condense. For pressures between one and five bars (100 and 500 kPa), clouds of ammonia and hydrogen sulfide are thought to form. Above a pressure of five bars, the clouds may consist of ammonia, ammonium sulfide, hydrogen sulfide and water. Deeper clouds of water ice should be found at pressures of about 50 bars (5.0 MPa), where the temperature reaches 273 K (0 °C). Underneath, clouds of ammonia and hydrogen sulfide may be found.
High-altitude clouds on Neptune have been observed casting shadows on the opaque cloud deck below. There are also high-altitude cloud bands that wrap around the planet at constant latitude. These circumferential bands have widths of 50–150 km and lie about 50–110 km above the cloud deck. These altitudes are in the layer where weather occurs, the troposphere. Weather does not occur in the higher stratosphere or thermosphere. Unlike Uranus, Neptune's composition has a higher volume of ocean, whereas Uranus has a smaller mantle.
For reasons that remain obscure, the planet's thermosphere is at an anomalously high temperature of about 750 K. The planet is too far from the Sun for this heat to be generated by ultraviolet radiation. One candidate for a heating mechanism is atmospheric interaction with ions in the planet's magnetic field. Other candidates are gravity waves from the interior that dissipate in the atmosphere. The thermosphere contains traces of carbon dioxide and water, which may have been deposited from external sources such as meteorites and dust.
Neptune also resembles Uranus in its magnetosphere, with a magnetic field strongly tilted relative to its rotational axis at 47° and offset at least 0.55 radii, or about 13500 km from the planet's physical centre. Before Voyager 2's arrival at Neptune, it was hypothesised that Uranus's tilted magnetosphere was the result of its sideways rotation. In comparing the magnetic fields of the two planets, scientists now think the extreme orientation may be characteristic of flows in the planets' interiors. This field may be generated by convective fluid motions in a thin spherical shell of electrically conducting liquids (probably a combination of ammonia, methane and water) resulting in a dynamo action.
The dipole component of the magnetic field at the magnetic equator of Neptune is about 14 microteslas (0.14 G). The dipole magnetic moment of Neptune is about 2.2 × 1017 T·m3 (14 μT·RN3, where RN is the radius of Neptune). Neptune's magnetic field has a complex geometry that includes relatively large contributions from non-dipolar components, including a strong quadrupole moment that may exceed the dipole moment in strength. By contrast, Earth, Jupiter and Saturn have only relatively small quadrupole moments, and their fields are less tilted from the polar axis. The large quadrupole moment of Neptune may be the result of offset from the planet's centre and geometrical constraints of the field's dynamo generator.
Neptune has a planetary ring system, though one much less substantial than that of Saturn. The rings may consist of ice particles coated with silicates or carbon-based material, which most likely gives them a reddish hue. The three main rings are the narrow Adams Ring, 63,000 km from the centre of Neptune, the Le Verrier Ring, at 53,000 km, and the broader, fainter Galle Ring, at 42,000 km. A faint outward extension to the Le Verrier Ring has been named Lassell; it is bounded at its outer edge by the Arago Ring at 57,000 km.
Neptune's weather is characterised by extremely dynamic storm systems, with winds reaching speeds of almost 600 m/s (2,200 km/h; 1,300 mph)—nearly reaching supersonic flow. More typically, by tracking the motion of persistent clouds, wind speeds have been shown to vary from 20 m/s in the easterly direction to 325 m/s westward. At the cloud tops, the prevailing winds range in speed from 400 m/s along the equator to 250 m/s at the poles. Most of the winds on Neptune move in a direction opposite the planet's rotation. The general pattern of winds showed prograde rotation at high latitudes vs. retrograde rotation at lower latitudes. The difference in flow direction is thought to be a "skin effect" and not due to any deeper atmospheric processes. At 70° S latitude, a high-speed jet travels at a speed of 300 m/s.
In 2007, it was discovered that the upper troposphere of Neptune's south pole was about 10 K warmer than the rest of its atmosphere, which averages approximately 73 K (−200 °C). The temperature differential is enough to let methane, which elsewhere is frozen in the troposphere, escape into the stratosphere near the pole. The relative "hot spot" is due to Neptune's axial tilt, which has exposed the south pole to the Sun for the last quarter of Neptune's year, or roughly 40 Earth years. As Neptune slowly moves towards the opposite side of the Sun, the south pole will be darkened and the north pole illuminated, causing the methane release to shift to the north pole.
The Scooter is another storm, a white cloud group farther south than the Great Dark Spot. This nickname first arose during the months leading up to the Voyager 2 encounter in 1989, when they were observed moving at speeds faster than the Great Dark Spot (and images acquired later would subsequently reveal the presence of clouds moving even faster than those that had initially been detected by Voyager 2). The Small Dark Spot is a southern cyclonic storm, the second-most-intense storm observed during the 1989 encounter. It was initially completely dark, but as Voyager 2 approached the planet, a bright core developed and can be seen in most of the highest-resolution images.
Neptune's dark spots are thought to occur in the troposphere at lower altitudes than the brighter cloud features, so they appear as holes in the upper cloud decks. As they are stable features that can persist for several months, they are thought to be vortex structures. Often associated with dark spots are brighter, persistent methane clouds that form around the tropopause layer. The persistence of companion clouds shows that some former dark spots may continue to exist as cyclones even though they are no longer visible as a dark feature. Dark spots may dissipate when they migrate too close to the equator or possibly through some other unknown mechanism.
Neptune's more varied weather when compared to Uranus is due in part to its higher internal heating. Although Neptune lies over 50% further from the Sun than Uranus, and receives only 40% its amount of sunlight, the two planets' surface temperatures are roughly equal. The upper regions of Neptune's troposphere reach a low temperature of 51.8 K (−221.3 °C). At a depth where the atmospheric pressure equals 1 bar (100 kPa), the temperature is 72.00 K (−201.15 °C). Deeper inside the layers of gas, the temperature rises steadily. As with Uranus, the source of this heating is unknown, but the discrepancy is larger: Uranus only radiates 1.1 times as much energy as it receives from the Sun; whereas Neptune radiates about 2.61 times as much energy as it receives from the Sun. Neptune is the farthest planet from the Sun, yet its internal energy is sufficient to drive the fastest planetary winds seen in the Solar System. Depending on the thermal properties of its interior, the heat left over from Neptune's formation may be sufficient to explain its current heat flow, though it is more difficult to simultaneously explain Uranus's lack of internal heat while preserving the apparent similarity between the two planets.
On 11 July 2011, Neptune completed its first full barycentric orbit since its discovery in 1846, although it did not appear at its exact discovery position in the sky, because Earth was in a different location in its 365.26-day orbit. Because of the motion of the Sun in relation to the barycentre of the Solar System, on 11 July Neptune was also not at its exact discovery position in relation to the Sun; if the more common heliocentric coordinate system is used, the discovery longitude was reached on 12 July 2011.
Neptune's orbit has a profound impact on the region directly beyond it, known as the Kuiper belt. The Kuiper belt is a ring of small icy worlds, similar to the asteroid belt but far larger, extending from Neptune's orbit at 30 AU out to about 55 AU from the Sun. Much in the same way that Jupiter's gravity dominates the asteroid belt, shaping its structure, so Neptune's gravity dominates the Kuiper belt. Over the age of the Solar System, certain regions of the Kuiper belt became destabilised by Neptune's gravity, creating gaps in the Kuiper belt's structure. The region between 40 and 42 AU is an example.
There do exist orbits within these empty regions where objects can survive for the age of the Solar System. These resonances occur when Neptune's orbital period is a precise fraction of that of the object, such as 1:2, or 3:4. If, say, an object orbits the Sun once for every two Neptune orbits, it will only complete half an orbit by the time Neptune returns to its original position. The most heavily populated resonance in the Kuiper belt, with over 200 known objects, is the 2:3 resonance. Objects in this resonance complete 2 orbits for every 3 of Neptune, and are known as plutinos because the largest of the known Kuiper belt objects, Pluto, is among them. Although Pluto crosses Neptune's orbit regularly, the 2:3 resonance ensures they can never collide. The 3:4, 3:5, 4:7 and 2:5 resonances are less populated.
Neptune has a number of known trojan objects occupying both the Sun–Neptune L4 and L5 Lagrangian points—gravitationally stable regions leading and trailing Neptune in its orbit, respectively. Neptune trojans can be viewed as being in a 1:1 resonance with Neptune. Some Neptune trojans are remarkably stable in their orbits, and are likely to have formed alongside Neptune rather than being captured. The first and so far only object identified as associated with Neptune's trailing L5 Lagrangian point is 2008 LC18. Neptune also has a temporary quasi-satellite, (309239) 2007 RW10. The object has been a quasi-satellite of Neptune for about 12,500 years and it will remain in that dynamical state for another 12,500 years.
The formation of the ice giants, Neptune and Uranus, has proven difficult to model precisely. Current models suggest that the matter density in the outer regions of the Solar System was too low to account for the formation of such large bodies from the traditionally accepted method of core accretion, and various hypotheses have been advanced to explain their formation. One is that the ice giants were not formed by core accretion but from instabilities within the original protoplanetary disc and later had their atmospheres blasted away by radiation from a nearby massive OB star.
An alternative concept is that they formed closer to the Sun, where the matter density was higher, and then subsequently migrated to their current orbits after the removal of the gaseous protoplanetary disc. This hypothesis of migration after formation is favoured, due to its ability to better explain the occupancy of the populations of small objects observed in the trans-Neptunian region. The current most widely accepted explanation of the details of this hypothesis is known as the Nice model, which explores the effect of a migrating Neptune and the other giant planets on the structure of the Kuiper belt.
Neptune has 14 known moons. Triton is the largest Neptunian moon, comprising more than 99.5% of the mass in orbit around Neptune,[e] and it is the only one massive enough to be spheroidal. Triton was discovered by William Lassell just 17 days after the discovery of Neptune itself. Unlike all other large planetary moons in the Solar System, Triton has a retrograde orbit, indicating that it was captured rather than forming in place; it was probably once a dwarf planet in the Kuiper belt. It is close enough to Neptune to be locked into a synchronous rotation, and it is slowly spiralling inward because of tidal acceleration. It will eventually be torn apart, in about 3.6 billion years, when it reaches the Roche limit. In 1989, Triton was the coldest object that had yet been measured in the Solar System, with estimated temperatures of 38 K (−235 °C).
From July to September 1989, Voyager 2 discovered six moons of Neptune. Of these, the irregularly shaped Proteus is notable for being as large as a body of its density can be without being pulled into a spherical shape by its own gravity. Although the second-most-massive Neptunian moon, it is only 0.25% the mass of Triton. Neptune's innermost four moons—Naiad, Thalassa, Despina and Galatea—orbit close enough to be within Neptune's rings. The next-farthest out, Larissa, was originally discovered in 1981 when it had occulted a star. This occultation had been attributed to ring arcs, but when Voyager 2 observed Neptune in 1989, Larissa was found to have caused it. Five new irregular moons discovered between 2002 and 2003 were announced in 2004. A new moon and the smallest yet, S/2004 N 1, was found in 2013. Because Neptune was the Roman god of the sea, Neptune's moons have been named after lesser sea gods.
Because of the distance of Neptune from Earth, its angular diameter only ranges from 2.2 to 2.4 arcseconds, the smallest of the Solar System planets. Its small apparent size makes it challenging to study it visually. Most telescopic data was fairly limited until the advent of Hubble Space Telescope (HST) and large ground-based telescopes with adaptive optics (AO). The first scientifically useful observation of Neptune from ground-based telescopes using adaptive optics, was commenced in 1997 from Hawaii. Neptune is currently entering its spring and summer season and has been shown to be heating up, with increased atmospheric activity and brightness as a consequence. Combined with technological advancements, ground-based telescopes with adaptive optics are recording increasingly more detailed images of this Outer Planet. Both the HST and AO telescopes on Earth has made many new discoveries within the Solar System since the mid-1990s, with a large increase in the number of known satellites and moons around the Outer Planets for example. In 2004 and 2005, five new small satellites of Neptune with diameters between 38 and 61 kilometres were discovered.
Voyager 2 is the only spacecraft that has visited Neptune. The spacecraft's closest approach to the planet occurred on 25 August 1989. Because this was the last major planet the spacecraft could visit, it was decided to make a close flyby of the moon Triton, regardless of the consequences to the trajectory, similarly to what was done for Voyager 1's encounter with Saturn and its moon Titan. The images relayed back to Earth from Voyager 2 became the basis of a 1989 PBS all-night program, Neptune All Night.
After the Voyager 2 flyby mission, the next step in scientific exploration of the Neptunian system, is considered to be a Flagship orbital mission. Such a hypothetical mission is envisioned to be possible at in the late 2020s or early 2030s. However, there have been a couple of discussions to launch Neptune missions sooner. In 2003, there was a proposal in NASA's "Vision Missions Studies" for a "Neptune Orbiter with Probes" mission that does Cassini-level science. Another, more recent proposal was for Argo, a flyby spacecraft to be launched in 2019, that would visit Jupiter, Saturn, Neptune, and a Kuiper belt object. The focus would be on Neptune and its largest moon Triton to be investigated around 2029. The proposed New Horizons 2 mission (which was later scrapped) might also have done a close flyby of the Neptunian system.
The terms upper case and lower case can be written as two consecutive words, connected with a hyphen (upper-case and lower-case), or as a single word (uppercase and lowercase). These terms originated from the common layouts of the shallow drawers called type cases used to hold the movable type for letterpress printing. Traditionally, the capital letters were stored in a separate case that was located above the case that held the small letters, and the name proved easy to remember since capital letters are taller.
The convention followed by many British publishers (including scientific publishers, like Nature, magazines, like The Economist and New Scientist, and newspapers, like The Guardian and The Times) and U.S. newspapers is to use sentence-style capitalisation in headlines, where capitalisation follows the same rules that apply for sentences. This convention is usually called sentence case. It may also be applied to publication titles, especially in bibliographic references and library catalogues. Examples of global publishers whose English-language house styles prescribe sentence-case titles and headings include the International Organization for Standardization.
Similar developments have taken place in other alphabets. The lower-case script for the Greek alphabet has its origins in the 7th century and acquired its quadrilinear form in the 8th century. Over time, uncial letter forms were increasingly mixed into the script. The earliest dated Greek lower-case text is the Uspenski Gospels (MS 461) in the year 835.[citation needed] The modern practice of capitalising the first letter of every sentence seems to be imported (and is rarely used when printing Ancient Greek materials even today).
In orthography and typography, letter case (or just case) is the distinction between the letters that are in larger upper case (also capital letters, capitals, caps, large letters, or more formally majuscule, see Terminology) and smaller lower case (also small letters, or more formally minuscule, see Terminology) in the written representation of certain languages. Here is a comparison of the upper and lower case versions of each letter included in the English alphabet (the exact representation will vary according to the font used):
Capitalisation in English, in terms of the general orthographic rules independent of context (e.g. title vs. heading vs. text), is universally standardized for formal writing. (Informal communication, such as texting, instant messaging or a handwritten sticky note, may not bother, but that is because its users usually do not expect it to be formal.) In English, capital letters are used as the first letter of a sentence, a proper noun, or a proper adjective. There are a few pairs of words of different meanings whose only difference is capitalisation of the first letter. The names of the days of the week and the names of the months are also capitalised, as are the first-person pronoun "I" and the interjection "O" (although the latter is uncommon in modern usage, with "oh" being preferred). Other words normally start with a lower-case letter. There are, however, situations where further capitalisation may be used to give added emphasis, for example in headings and titles (see below). In some traditional forms of poetry, capitalisation has conventionally been used as a marker to indicate the beginning of a line of verse independent of any grammatical feature.
Originally alphabets were written entirely in majuscule letters, spaced between well-defined upper and lower bounds. When written quickly with a pen, these tended to turn into rounder and much simpler forms. It is from these that the first minuscule hands developed, the half-uncials and cursive minuscule, which no longer stayed bound between a pair of lines. These in turn formed the foundations for the Carolingian minuscule script, developed by Alcuin for use in the court of Charlemagne, which quickly spread across Europe. The advantage of the minuscule over majuscule was improved, faster readability.[citation needed]
Letter case is often prescribed by the grammar of a language or by the conventions of a particular discipline. In orthography, the uppercase is primarily reserved for special purposes, such as the first letter of a sentence or of a proper noun, which makes the lowercase the more common variant in text. In mathematics, letter case may indicate the relationship between objects with uppercase letters often representing "superior" objects (e.g. X could be a set containing the generic member x). Engineering design drawings are typically labelled entirely in upper-case letters, which are easier to distinguish than lowercase, especially when space restrictions require that the lettering be small.
Typographically, the basic difference between the majuscules and minuscules is not that the majuscules are big and minuscules small, but that the majuscules generally have the same height. The height of the minuscules varies, as some of them have parts higher or lower than the average, i.e. ascenders and descenders. In Times New Roman, for instance, b, d, f, h, k, l, t  are the letters with ascenders, and g, j, p, q, y are the ones with descenders. Further to this, with old-style numerals still used by some traditional or classical fonts—although most do have a set of alternative Lining Figures— 6 and 8 make up the ascender set, and 3, 4, 5, 7 and 9 the descender set.
Most Western languages (particularly those with writing systems based on the Latin, Cyrillic, Greek, Coptic, and Armenian alphabets) use letter cases in their written form as an aid to clarity. Scripts using two separate cases are also called bicameral scripts. Many other writing systems make no distinction between majuscules and minuscules – a system called unicameral script or unicase. This includes most syllabic and other non-alphabetic scripts. The Georgian alphabet is special since it used to be bicameral, but today is mostly used in a unicameral way.
In Latin, papyri from Herculaneum dating before 79 AD (when it was destroyed) have been found that have been written in old Roman cursive, where the early forms of minuscule letters "d", "h" and "r", for example, can already be recognised. According to papyrologist Knut Kleve, "The theory, then, that the lower-case letters have been developed from the fifth century uncials and the ninth century Carolingian minuscules seems to be wrong." Both majuscule and minuscule letters existed, but the difference between the two variants was initially stylistic rather than orthographic and the writing system was still basically unicameral: a given handwritten document could use either one style or the other but these were not mixed. European languages, except for Ancient Greek and Latin, did not make the case distinction before about 1300.[citation needed]
As regards publication titles it is, however, a common typographic practice among both British and U.S. publishers to capitalise significant words (and in the United States, this is often applied to headings, too). This family of typographic conventions is usually called title case. For example, R. M. Ritter's Oxford Manual of Style (2002) suggests capitalising "the first word and all nouns, pronouns, adjectives, verbs and adverbs, but generally not articles, conjunctions and short prepositions". This is an old form of emphasis, similar to the more modern practice of using a larger or boldface font for titles. The rules for which words to capitalise are not based on any grammatically inherent correct/incorrect distinction and are not universally standardized; they are arbitrary and differ between style guides, although in most styles they tend to follow a few strong conventions, as follows:
As briefly discussed in Unicode Technical Note #26, "In terms of implementation issues, any attempt at a unification of Latin, Greek, and Cyrillic would wreak havoc [and] make casing operations an unholy mess, in effect making all casing operations context sensitive […]". In other words, while the shapes of letters like A, B, E, H, K, M, O, P, T, X, Y and so on are shared between the Latin, Greek, and Cyrillic alphabets (and small differences in their canonical forms may be considered to be of a merely typographical nature), it would still be problematic for a multilingual character set or a font to provide only a single codepoint for, say, uppercase letter B, as this would make it quite difficult for a wordprocessor to change that single uppercase letter to one of the three different choices for the lower-case letter, b (Latin), β (Greek), or в (Cyrillic). Without letter case, a "unified European alphabet" – such as ABБCГDΔΕZЄЗFΦGHIИJ…Z, with an appropriate subset for each language – is feasible; but considering letter case, it becomes very clear that these alphabets are rather distinct sets of symbols.
(デジモン Dejimon, branded as Digimon: Digital Monsters, stylized as DIGIMON), short for "Digital Monsters" (デジタルモンスター Dejitaru Monsutā), is a Japanese media franchise encompassing virtual pet toys, anime, manga, video games, films and a trading card game. The franchise focuses on Digimon creatures, which are monsters living in a "Digital World", a parallel universe that originated from Earth's various communication networks. In many incarnations, Digimon are raised by humans called "Digidestined" or "Tamers", and they team up to defeat evil Digimon and human villains who are trying to destroy the fabric of the Digital world.
The franchise was first created in 1997 as a series of virtual pets, akin to—and influenced in style by—the contemporary Tamagotchi or nano Giga Pet toys. The creatures were first designed to look cute and iconic even on the devices' small screens; later developments had them created with a harder-edged style influenced by American comics. The franchise gained momentum with its first anime incarnation, Digimon Adventure, and an early video game, Digimon World, both released in 1999. Several seasons of the anime and films based on them have aired, and the video game series has expanded into genres such as role-playing, racing, fighting, and MMORPGs. Other media forms have also been released.
Digimon was first conceived as a virtual pet toy in the vein of Tamagotchis and, as such, took influence from Tamagotchis' cute and round designs. The small areas of the screens (16 by 16 pixels) meant that character designers had to create monsters whose forms would be easily recognizable. As such, many of the early Digimon—including Tyrannomon, the first one ever created—were based on dinosaurs. Many further designs were created by Kenji Watanabe, who was brought in to help with the "X-Antibody" creatures and art for the Digimon collectible card game. Watanabe was one influenced by American comics, which were beginning to gain popularity in Japan, and as such began to make his characters look stronger and "cool." The character creation process, however, has for most of the franchise's history been collaborative and reliant on conversation and brainstorming.
Digimon hatch from types of eggs which are called Digi-Eggs (デジタマ, Dejitama?). In the English iterations of the franchise there is another type of Digi-Egg that can be used to digivolve, or transform, Digimon. This second type of Digi-Egg is called a Digimental (デジメンタル, Dejimentaru?) in Japanese. (This type of Digi-Egg was also featured as a major object throughout season 2 as a way of Digivolution available only to certain characters at certain points throughout the season.) They age via a process called "Digivolution" which changes their appearance and increases their powers. The effect of Digivolution, however, is not permanent in the partner Digimon of the main characters in the anime, and Digimon who have digivolved will most of the time revert to their previous form after a battle or if they are too weak to continue. Some Digimon act feral. Most, however, are capable of intelligence and human speech. They are able to digivolve by the use of Digivices that their human partners have. In some cases, as in the first series, the DigiDestined (known as the 'Chosen Children' in the original Japanese) had to find some special items such as crests and tags so the Digimon could digivolve into further stages of evolution known as Ultimate and Mega in the dub.
The first Digimon anime introduced the Digimon life cycle: They age in a similar fashion to real organisms, but do not die under normal circumstances because they are made of reconfigurable data, which can be seen throughout the show. Any Digimon that receives a fatal wound will dissolve into infinitesimal bits of data. The data then recomposes itself as a Digi-Egg, which will hatch when rubbed gently, and the Digimon goes through its life cycle again. Digimon who are reincarnated in this way will sometimes retain some or all their memories of their previous life. However, if a Digimon's data is completely destroyed, they will die.
Digimon started out as digital pets called "Digital Monsters", similar in style and concept to the Tamagotchi. It was planned by WiZ and released by Bandai on June 26, 1997. The toy began as the simple concept of a Tamagotchi mainly for boys. The V-Pet is similar to its predecessors, with the exceptions of being more difficult and being able to fight other Digimon v-pets. Every owner would start with a Baby Digimon, train it, evolve it, take care of it, and then have battles with other Digimon owners to see who was stronger. The Digimon pet had several evolution capabilities and abilities too, so many owners had different Digimon. In December, the second generation of Digital Monster was released, followed by a third edition in 1998.
"Digimon" are "Digital Monsters". According to the stories, they are inhabitants of the "DigiWorld", a manifestation of Earth's communication network. The stories tell of a group of mostly pre-teens, who accompany special Digimon born to defend their world (and ours) from various evil forces. To help them surmount the most difficult obstacles found within both realms, the Digimon have the ability to evolve (Digivolve) In this process, the Digimon change appearance and become much stronger, often changing in personality as well. The group of children who come in contact with the Digital World changes from series to series.
As of 2011, there have been six series — Digimon Adventure, the follow-up sequel Digimon Adventure 02, Digimon Tamers, Digimon Frontier, Digimon Data Squad and Digimon Fusion. The first two series take place in the same fictional universe, but the third, fourth, fifth and sixth each occupy their own unique world. Each series is commonly based on the original storyline but things are added to make them unique. However, in Tamers, the Adventure universe is referred to as a commercial enterprise — a trading card game in Japan, plus a show-within-a-show in the English dub. It also features an appearance by a character from the Adventure universe. In addition, each series has spawned assorted feature films. Digimon still shows popularity, as new card series, video games, and movies are still being produced and released: new card series include Eternal Courage, Hybrid Warriors, Generations, and Operation X; the video game, Digimon Rumble Arena 2; and the previously unreleased movies Revenge of Diaboromon, Runaway Locomon, Battle of Adventurers, and Island of Lost Digimon. In Japan, Digital Monster X-Evolution, the eighth TV movie, was released on January 3, 2005, and on December 23, 2005 at Jump Festa 2006, the fifth series, Digimon Savers was announced for Japan to begin airing after a three-year hiatus of the show. A sixth television series, Digimon Xros Wars, began airing in 2010, and was followed by a second season, which started on October 2, 2011 as a direct sequel to Digimon Xros Wars.
The first Digimon television series, which began airing on March 7, 1999 in Japan on Fuji TV and Kids Station and on August 14, 1999 in the United States on Fox Kids dubbed by Saban Entertainment for the North American English version. Its premise is a group of 7 kids who, while at summer camp, travel to the Digital World, inhabited by creatures known as Digital Monsters, or Digimon, learning they are chosen to be "DigiDestined" ("Chosen Children" in the Japanese version) to save both the Digital and Real World from evil. Each Kid was given a Digivice which selected them to be transported to the DigiWorld and was destined to be paired up with a Digimon Partner, such as Tai being paired up with Agumon and Matt with Gabumon. The children are helped by a mysterious man/digimon named Gennai, who helps them via hologram. The Digivices help their Digimon allies to Digivolve into stronger creatures in times of peril. The Digimon usually reached higher forms when their human partners are placed in dangerous situations, such as fighting the evil forces of Devimon, Etemon and Myotismon in their Champion forms. Later, each character discovered a crest that each belonged to a person; Tai the Crest of Courage, Matt the Crest of Friendship, Sora the Crest of Love, Izzy the Crest of Knowledge, Mimi the Crest of Sincerity, Joe the Crest of Reliability, T.K. the Crest of Hope, and later Kari the Crest of Light which allowed their Digimon to digivolve into their Ultimate forms. The group consisted of seven original characters: Taichi "Tai" Kamiya, Yamato "Matt" Ishida, Sora Takenouchi, Koushiro "Izzy" Izumi, Mimi Tachikawa, Joe Kido, and Takeru "T.K." Takaishi. Later on in the series, an eighth character was introduced: Hikari "Kari" Kamiya (who is Taichi's younger sister).
The second Digimon series is direct continuation of the first one, and began airing on April 2, 2000. Three years later, with most of the original DigiDestined now in high school at age fourteen, the Digital World was supposedly secure and peaceful. However, a new evil has appeared in the form of the Digimon Emperor (Digimon Kaiser) who as opposed to previous enemies is a human just like the DigiDestined. The Digimon Emperor has been enslaving Digimon with Dark Rings and Control Spires and has somehow made regular Digivolution impossible. However, five set Digi-Eggs with engraved emblems had been appointed to three new DigiDestined along with T.K. and Kari, two of the DigiDestined from the previous series. This new evolutionary process, dubbed Armor Digivolution helps the new DigiDestined to defeat evil lurking in the Digital World. Eventually, the DigiDestined defeat the Digimon Emperor, more commonly known as Ken Ichijouji on Earth, only with the great sacrifice of Ken's own Digimon, Wormmon. Just when things were thought to be settled, new Digimon enemies made from the deactivated Control Spires start to appear and cause trouble in the Digital World. To atone for his past mistakes, Ken joins the DigiDestined, being a DigiDestined himself, with his Partner Wormmon revived to fight against them. They soon save countries including France and Australia from control spires and defeat MaloMyotismon (BelialVamdemon), the digivolved form of Myotismon (Vamdemon) from the previous series. They stop the evil from destroying the two worlds, and at the end, every person on Earth gains their own Digimon partner.
The third Digimon series, which began airing on April 1, 2001, is set largely in a "real world" where the Adventure and Adventure 02 series are television shows, and where Digimon game merchandise (based on actual items) become key to providing power boosts to real Digimon which appear in that world. The plot revolves around three Tamers, Takato Matsuki, Rika Nonaka, and Henry Wong. It began with Takato creating his own Digimon partner by sliding a mysterious blue card through his card reader, which then became a D-Power. Guilmon takes form from Takato's sketchings of a new Digimon. (Tamers’ only human connection to the Adventure series is Ryo Akiyama, a character featured in some of the Digimon video games and who made an appearance in some occasions of the Adventure story-line.) Some of the changes in this series include the way the Digimon digivolve with the introduction of Biomerge-Digivolution and the way their "Digivices" work. In this series, the Tamers can slide game cards through their "Digivices" and give their Digimon partners certain advantages, as in the card game. This act is called "Digi-Modify" (Card Slash in the Japanese version). The same process was often used to Digivolve the Digimon, but as usual, emotions play a big part in the digivolving process. Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. The anime has become controversial over the decade, with debates about how appropriate this show actually is for its "target" audience, especially due to the Lovecraftian nature of the last arc. The English dub is more lighthearted dialogue-wise, though still not as much as previous series.
The fourth Digimon series, which began airing on April 7, 2002, radically departs from the previous three by focusing on a new and very different kind of evolution, Spirit Evolution, in which the human characters use their D-Tectors (this series' Digivice) to transform themselves into special Digimon called Legendary Warriors, detracting from the customary formula of having digital partners. After receiving unusual phone messages from Ophanimon (one of the three ruling Digimon alongside Seraphimon and Cherubimon) Takuya Kanbara, Koji Minamoto, Junpei Shibayama, Zoe Orimoto, Tommy Himi, and Koichi Kimura go to a subway station and take a train to the Digital World. Summoned by Ophanimon, the Digidestined realize that they must find the ten legendary spirits and stop the forces of Cherubimon from physically destroying the Digital World. After finding the ten spirits of the Legendary Warriors and defeating Mercurymon, Grumblemon, Ranamon, and Arbormon, they finally end up fighting Cherubimon hoping to foil his effort to dominate the Digital World. After the defeat of Cherubimon, the Digidestined find they must face an even greater challenge as they try to stop the Royal Knights—Dynasmon and Crusadermon—from destroying the Digital World and using the collected data to revive the original ruler of the Digital World: the tyrannical Lucemon. Ultimately the Digidestined fail in preventing Lucemon from reawakening but they do manage to prevent him from escaping into the Real World. In the final battle, all of the legendary spirits the digidestined have collected thus far merge and create Susanoomon. With this new form, the digidestined are able to effectively defeat Lucemon and save the Digital World. In general, Frontier has a much lighter tone than that of Tamers, yet remains darker than Adventure and Adventure 02.
After a three-year hiatus, a fifth Digimon series began airing on April 2, 2006. Like Frontier, Savers has no connection with the previous installments, and also marks a new start for the Digimon franchise, with a drastic change in character designs and story-line, in order to reach a broader audience. The story focuses on the challenges faced by the members of D.A.T.S. ("Digital Accident Tactics Squad"), an organization created to conceal the existence of the Digital World and Digimon from the rest of mankind, and secretly solve any Digimon-related incidents occurring on Earth. Later the D.A.T.S. is dragged into a massive conflict between Earth and the Digital World, triggered by an ambitious human scientist named Akihiro Kurata, determined to make use of the Digimon for his own personal gains. The English version was dubbed by Studiopolis and it premiered on the Jetix block on Toon Disney on October 1, 2007. Digivolution in Data Squad requires the human partner's DNA ("Digital Natural Ability" in the English version and "Digisoul" in the Japanese version) to activate, a strong empathy with their Digimon and a will to succeed. 'Digimon Savers' also introduces a new form of digivolving called Burst Mode which is essentially the level above Mega (previously the strongest form a digimon could take). Like previously in Tamers, this plot takes on a dark tone throughout the story and the anime was aimed, originally in Japan, at an older audience consisting of late teens and people in their early twenties from ages 16 to 21. Because of that, along with the designs, the anime being heavily edited and localized for western US audiences like past series, and the English dub being aimed mostly toward younger audiences of children aged 6 to 10 and having a lower TV-Y7-FV rating just like past dubs, Studiopolis dubbed the anime on Jetix with far more edits, changes, censorship, and cut footage. This included giving the Japanese characters full Americanized names and American surnames as well as applying far more Americanization (Marcus Damon as opposed to the Japanese Daimon Masaru), cultural streamlining and more edits to their version similar to the changes 4Kids often made (such as removal of Japanese text for the purpose of cultural streamlining). Despite all that, the setting of the country was still in Japan and the characters were Japanese in the dub. This series was the first to show any Japanese cultural concepts that were unfamiliar with American audiences (such as the manju), which were left unedited and used in the English dub. Also despite the heavy censorship and the English dub aimed at young children, some of the Digimon's attacks named after real weapons such as RizeGreymon's Trident Revolver are not edited and used in the English dub. Well Go USA released it on DVD instead of Disney. The North American English dub was televised on Jetix in the U.S. and on the Family Channel in Canada.
Three and a quarter years after the end of the fifth series, a new sixth series was confirmed by Bandai for the Digimon anime, its official name of the series revealed in the June issue of Shueisha's V Jump magazine being Digimon Xros Wars. It began airing in Japan on TV Asahi from July 6, 2010 onwards. Reverting to the design style of the first four series as well as the plot taking on the younger, lighter tone present in series one, two and four throughout the story. The story follows a boy named Mikey Kudō (Taiki Kudo in Japan) who, along with his friends, ends up in the Digital World where they meet Shoutmon and his Digimon friends. Wielding a digivice known as a Fusion Loader (Xros Loader in Japan), Mikey is able to combine multiple Digimon onto one to enhance his power, Shoutmon being the usual core of the combination, using a technique known as 'DigiFuse' (Digi-Xros in Japan). Forming Team Fusion Fighters (Team Xros Heart in Japan), Mikey, Shoutmon and their friends travel through the Digital World to liberate it from the evil Bagra Army, led by Bagramon(Lord Bagra in English), and Midnight, a shady group led by AxeKnightmon with Nene as a figurehead before joining the Fusion Fighters. The Fusion Fighters also finds themselves at odds with Blue Flare, led by Christopher Aonuma (Kiriha Anouma in Japan). The second arc of Xros Wars was subtitled The Evil Death Generals and the Seven Kingdoms. It saw the main cast reshuffled with a new wardrobe while Angie (Akari in Japan) and Jeremy (Zenjiro in Japan) stay behind in the Human World; thus making Mikey, Christopher and Nene the lead protagonists as they set off to face the Seven Death Generals of the Bagra Army and AxeKnightmon's new pawn: Nene's brother Ewan (Yuu in Japan). A new evolution known as Super Digivolution was introduced at the end of the first arc. The English dub of the series began airing on Nickelodeon on September 7, 2013, which is produced by Saban Brands.
On August 17, 2011, Shueisha's V-Jump magazine announced a sequel set one year later, a third arc of Xros Wars subtitled The Young Hunters Who Leapt Through Time, which aired from October 2, 2011 to March 25, 2012, following on from the previous arc. It focuses on a new protagonist, Tagiru Akashi and his partner Gumdramon who embark on a new journey with an older Mikey, Shoutmon, an older Ewan and the revived Damemon, along with other new comrades as they deal with a hidden dimension that lies between the Human World and the Digital World called DigiQuartz. The series finale reintroduces the heroes of the previous five seasons as they all come together and help the current heroes in the final battle due to the fact that the DigiQuartz is essentially a tear in Space and Time, allowing all of the Digimon universes to converge.
A new Digimon series was announced 30 months after the end of Digimon Fusion at a 15th anniversary concert and theater event for the franchise in August 2014. The series announced the return of the protagonists from the original Digimon Adventure series, most of them now as high school students. A countdown clicking game was posted on the show's official website, offering news when specific clicks were met. On December 13, 2014 the series title and a key visual featuring character designs by Atsuya Uki were revealed with Keitaro Motonaga announced as director with a tentative premiere date of Spring, 2015. However, on May 6, 2015, it was announced that tri. would not be a television series, but rather a 6-part theatrical film series. The films are being streamed in episodic format outside Japan by Crunchyroll and Hulu from the same day they premiere on Japanese theaters.
The series is set three years after the events of Digimon Adventure 02, when Digimon who turn rogue by a mysterious infection appear to wreak havoc in the Human World. Tai and the other DigiDestined from the original series reunite with their partners and start fighting back with support from the Japanese government, while Davis, Yolei, Cody and Ken are defeated by a powerful enemy called Alphamon and disappear without a trace. Tai and the others also meet another DigiDestined called Meiko Mochizuki and her partner Meicoomon who become their friends, until Meicoomon turns hostile as well and flees after an encounter with Ken, who reappears suddenly, once again as the Digimon Emperor. The film series also feature several DigiDestined having their partners Digivolve up to the Mega level for the first time, a feat only Tai and Matt had achieved previously.
There have been nine Digimon movies released in Japan. The first seven were directly connected to their respective anime series; Digital Monster X-Evolution originated from the Digimon Chronicle merchandise line. All movies except X-Evolution and Ultimate Power! Activate Burst Mode have been released and distributed internationally. Digimon: The Movie, released in the U.S. and Canada territory by Fox Kids through 20th Century Fox on October 6, 2000, consists of the union of the first three Japanese movies.
The European publishing company, Panini, approached Digimon in different ways in different countries. While Germany created their own adaptations of episodes, the United Kingdom (UK) reprinted the Dark Horse titles, then translated some of the German adaptations of Adventure 02 episodes. Eventually the UK comics were given their own original stories, which appeared in both the UK's official Digimon Magazine and the official UK Fox Kids companion magazine, Wickid. These original stories only roughly followed the continuity of Adventure 02. When the comic switched to the Tamers series the storylines adhered to continuity more strictly; sometimes it would expand on subject matter not covered by the original Japanese anime (such as Mitsuo Yamaki's past) or the English adaptations of the television shows and movies (such as Ryo's story or the movies that remained undubbed until 2005). In a money saving venture, the original stories were later removed from Digimon Magazine, which returned to printing translated German adaptations of Tamers episodes. Eventually, both magazines were cancelled.
The Digimon series has a large number of video games which usually have their own independent storylines with a few sometimes tying into the stories of the anime series or manga series. The games consists of a number of genres including life simulation, adventure, video card game, strategy and racing games, though they are mainly action role-playing games. The games released in North America are: Digimon World, Digimon World 2, Digimon World 3, Digimon World 4, Digimon Digital Card Battle, Digimon Rumble Arena, Digimon Rumble Arena 2, Digimon Battle Spirit, Digimon Battle Spirit 2, Digimon Racing, Digimon World DS, Digimon World Data Squad, Digimon World Dawn and Dusk, Digimon World Championship, and Digimon Masters.
In 2011, Bandai posted a countdown on a teaser site. Once the countdown was finished, it revealed a reboot of the Digimon World series titled Digimon World Re:Digitize. An enhanced version of the game released on Nintendo 3DS as Digimon World Re:Digitize Decode in 2013. Another role-playing game by the name Digimon Story: Cyber Sleuth is set for release in 2015 for PlayStation Vita. It is part of the Digimon Story sub-series, originally on Nintendo DS and has also been released with English subtitles in North America.
The Iranian languages or Iranic languages form a branch of the Indo-Iranian languages, which in turn are a branch of the Indo-European language family. The speakers of Iranian languages are known as Iranian peoples. Historical Iranian languages are grouped in three stages: Old Iranian (until 400 BCE), Middle Iranian (400 BCE – 900 CE), and New Iranian (since 900 CE). Of the Old Iranian languages, the better understood and recorded ones are Old Persian (a language of Achaemenid Iran) and Avestan (the language of the Avesta). Middle Iranian languages included Middle Persian (a language of Sassanid Iran), Parthian, and Bactrian.
As of 2008, there were an estimated 150–200 million native speakers of Iranian languages. Ethnologue estimates there are 86 Iranian languages, the largest amongst them being Persian, Pashto, Kurdish, and Balochi.
The term Iranian is applied to any language which descends from the ancestral Proto-Iranian language. Iranian derives from the Persian and Sanskrit origin word Arya.
The use of the term for the Iranian language family was introduced in 1836 by Christian Lassen. Robert Needham Cust used the term Irano-Aryan in 1878, and Orientalists such as George Abraham Grierson and Max Müller contrasted Irano-Aryan (Iranian) and Indo-Aryan (Indic). Some recent scholarship, primarily in German, has revived this convention.
All Iranian languages are descended from a common ancestor, Proto-Iranian. In turn, and together with Proto-Indo-Aryan and the Nuristani languages, Proto-Iranian descends from a common ancestor Proto-Indo-Iranian. The Indo-Iranian languages are thought to have originated in Central Asia. The Andronovo culture is the suggested candidate for the common Indo-Iranian culture ca. 2000 BC.
It was situated precisely in the western part of Central Asia that borders present-day Russia (and present-day Kazakhstan). It was in relative proximity to the other satem ethno-linguistic groups of the Indo-European family, like Thracian, Balto-Slavic and others, and to common Indo-European's original homeland (more precisely, the steppes of southern Russia to the north of the Caucasus), according to the reconstructed linguistic relationships of common Indo-European.
Proto-Iranian thus dates to some time after Proto-Indo-Iranian break-up, or the early second millennium BCE, as the Old Iranian languages began to break off and evolve separately as the various Iranian tribes migrated and settled in vast areas of southeastern Europe, the Iranian plateau, and Central Asia.
The multitude of Middle Iranian languages and peoples indicate that great linguistic diversity must have existed among the ancient speakers of Iranian languages. Of that variety of languages/dialects, direct evidence of only two have survived. These are:
Old Persian is the Old Iranian dialect as it was spoken in south-western Iran by the inhabitants of Parsa, who also gave their name to their region and language. Genuine Old Persian is best attested in one of the three languages of the Behistun inscription, composed circa 520 BC, and which is the last inscription (and only inscription of significant length) in which Old Persian is still grammatically correct. Later inscriptions are comparatively brief, and typically simply copies of words and phrases from earlier ones, often with grammatical errors, which suggests that by the 4th century BC the transition from Old Persian to Middle Persian was already far advanced, but efforts were still being made to retain an "old" quality for official proclamations.
The other directly attested Old Iranian dialects are the two forms of Avestan, which take their name from their use in the Avesta, the liturgical texts of indigenous Iranian religion that now goes by the name of Zoroastrianism but in the Avesta itself is simply known as vohu daena (later: behdin). The language of the Avesta is subdivided into two dialects, conventionally known as "Old (or 'Gathic') Avestan", and "Younger Avestan". These terms, which date to the 19th century, are slightly misleading since 'Younger Avestan' is not only much younger than 'Old Avestan', but also from a different geographic region. The Old Avestan dialect is very archaic, and at roughly the same stage of development as Rigvedic Sanskrit. On the other hand, Younger Avestan is at about the same linguistic stage as Old Persian, but by virtue of its use as a sacred language retained its "old" characteristics long after the Old Iranian languages had yielded to their Middle Iranian stage. Unlike Old Persian, which has Middle Persian as its known successor, Avestan has no clearly identifiable Middle Iranian stage (the effect of Middle Iranian is indistinguishable from effects due to other causes).
In addition to Old Persian and Avestan, which are the only directly attested Old Iranian languages, all Middle Iranian languages must have had a predecessor "Old Iranian" form of that language, and thus can all be said to have had an (at least hypothetical) "Old" form. Such hypothetical Old Iranian languages include Carduchi (the hypothetical predecessor to Kurdish) and Old Parthian. Additionally, the existence of unattested languages can sometimes be inferred from the impact they had on neighbouring languages. Such transfer is known to have occurred for Old Persian, which has (what is called) a "Median" substrate in some of its vocabulary. Also, foreign references to languages can also provide a hint to the existence of otherwise unattested languages, for example through toponyms/ethnonyms or in the recording of vocabulary, as Herodotus did for what he called "Scythian".
Conventionally, Iranian languages are grouped in "western" and "eastern" branches. These terms have little meaning with respect to Old Avestan as that stage of the language may predate the settling of the Iranian peoples into western and eastern groups. The geographic terms also have little meaning when applied to Younger Avestan since it isn't known where that dialect (or dialects) was spoken either. Certain is only that Avestan (all forms) and Old Persian are distinct, and since Old Persian is "western", and Avestan was not Old Persian, Avestan acquired a default assignment to "eastern". Confusing the issue is the introduction of a western Iranian substrate in later Avestan compositions and redactions undertaken at the centers of imperial power in western Iran (either in the south-west in Persia, or in the north-west in Nisa/Parthia and Ecbatana/Media).
Two of the earliest dialectal divisions among Iranian indeed happen to not follow the later division into Western and Eastern blocks. These concern the fate of the Proto-Indo-Iranian first-series palatal consonants, *ć and *dź:
As a common intermediate stage, it is possible to reconstruct depalatalized affricates: *c, *dz. (This coincides with the state of affairs in the neighboring Nuristani languages.) A further complication however concerns the consonant clusters *ćw and *dźw:
It is possible that other distinct dialect groups were already in existence during this period. Good candidates are the hypothethical ancestor languages of Alanian/Scytho-Sarmatian subgroup of Scythian in the far northwest; and the hypothetical "Old Parthian" (the Old Iranian ancestor of Parthian) in the near northwest, where original *dw > *b (paralleling the development of *ćw).
What is known in Iranian linguistic history as the "Middle Iranian" era is thought to begin around the 4th century BCE lasting through the 9th century. Linguistically the Middle Iranian languages are conventionally classified into two main groups, Western and Eastern.
The Western family includes Parthian (Arsacid Pahlavi) and Middle Persian, while Bactrian, Sogdian, Khwarezmian, Saka, and Old Ossetic (Scytho-Sarmatian) fall under the Eastern category. The two languages of the Western group were linguistically very close to each other, but quite distinct from their eastern counterparts. On the other hand, the Eastern group was an areal entity whose languages retained some similarity to Avestan. They were inscribed in various Aramaic-derived alphabets which had ultimately evolved from the Achaemenid Imperial Aramaic script, though Bactrian was written using an adapted Greek script.
Middle Persian (Pahlavi) was the official language under the Sasanian dynasty in Iran. It was in use from the 3rd century CE until the beginning of the 10th century. The script used for Middle Persian in this era underwent significant maturity. Middle Persian, Parthian and Sogdian were also used as literary languages by the Manichaeans, whose texts also survive in various non-Iranian languages, from Latin to Chinese. Manichaean texts were written in a script closely akin to the Syriac script.
Following the Islamic Conquest of Persia (Iran), there were important changes in the role of the different dialects within the Persian Empire. The old prestige form of Middle Iranian, also known as Pahlavi, was replaced by a new standard dialect called Dari as the official language of the court. The name Dari comes from the word darbâr (دربار), which refers to the royal court, where many of the poets, protagonists, and patrons of the literature flourished. The Saffarid dynasty in particular was the first in a line of many dynasties to officially adopt the new language in 875 CE. Dari may have been heavily influenced by regional dialects of eastern Iran, whereas the earlier Pahlavi standard was based more on western dialects. This new prestige dialect became the basis of Standard New Persian. Medieval Iranian scholars such as Abdullah Ibn al-Muqaffa (8th century) and Ibn al-Nadim (10th century) associated the term "Dari" with the eastern province of Khorasan, while they used the term "Pahlavi" to describe the dialects of the northwestern areas between Isfahan and Azerbaijan, and "Pârsi" ("Persian" proper) to describe the Dialects of Fars. They also noted that the unofficial language of the royalty itself was yet another dialect, "Khuzi", associated with the western province of Khuzestan.
The Islamic conquest also brought with it the adoption of Arabic script for writing Persian and much later, Kurdish, Pashto and Balochi. All three were adapted to the writing by the addition of a few letters. This development probably occurred some time during the second half of the 8th century, when the old middle Persian script began dwindling in usage. The Arabic script remains in use in contemporary modern Persian. Tajik script was first Latinised in the 1920s under the then Soviet nationality policy. The script was however subsequently Cyrillicized in the 1930s by the Soviet government.
The geographical regions in which Iranian languages were spoken were pushed back in several areas by newly neighbouring languages. Arabic spread into some parts of Western Iran (Khuzestan), and Turkic languages spread through much of Central Asia, displacing various Iranian languages such as Sogdian and Bactrian in parts of what is today Turkmenistan, Uzbekistan and Tajikistan. In Eastern Europe, mostly comprising the territory of modern-day Ukraine, southern European Russia, and parts of the Balkans, the core region of the native Scythians, Sarmatians, and Alans had been decisively been taken over as a result of absorption and assimilation (e.g. Slavicisation) by the various Proto-Slavic population of the region, by the 6th century AD. This resulted in the displacement and extinction of the once predominant Scythian languages of the region. Sogdian's close relative Yaghnobi barely survives in a small area of the Zarafshan valley east of Samarkand, and Saka as Ossetic in the Caucasus, which is the sole remnant of the once predominant Scythian languages in Eastern Europe proper and large parts of the North Caucasus. Various small Iranian languages in the Pamirs survive that are derived from Eastern Iranian.
Muammar Muhammad Abu Minyar al-Gaddafi (Arabic: معمر محمد أبو منيار القذافي‎ Arabic pronunciation: [muʕamar al.qaðaːfiː]; /ˈmoʊ.əmɑːr ɡəˈdɑːfi/;  audio (help·info); c. 1942 – 20 October 2011), commonly known as Colonel Gaddafi,[b] was a Libyan revolutionary, politician, and political theorist. He governed Libya as Revolutionary Chairman of the Libyan Arab Republic from 1969 to 1977 and then as the "Brotherly Leader" of the Great Socialist People's Libyan Arab Jamahiriya from 1977 to 2011. Initially ideologically committed to Arab nationalism and Arab socialism, he came to rule according to his own Third International Theory before embracing Pan-Africanism and serving as Chairperson of the African Union from 2009 to 2010.
The son of an impoverished Bedouin goat herder, Gaddafi became involved in politics while at school in Sabha, subsequently enrolling in the Royal Military Academy, Benghazi. Founding a revolutionary cell within the military, in 1969 they seized power from the absolute monarchy of King Idris in a bloodless coup. Becoming Chairman of the governing Revolutionary Command Council (RCC), Gaddafi abolished the monarchy and proclaimed the Republic. Ruling by decree, he implemented measures to remove what he viewed as foreign imperialist influence from Libya, and strengthened ties to Arab nationalist governments. Intent on pushing Libya towards "Islamic socialism", he introduced sharia as the basis for the legal system and nationalized the oil industry, using the increased revenues to bolster the military, implement social programs and fund revolutionary militants across the world. In 1973 he initiated a "Popular Revolution" with the formation of General People's Committees (GPCs), purported to be a system of direct democracy, but retained personal control over major decisions. He outlined his Third International Theory that year, publishing these ideas in The Green Book.
In 1977, Gaddafi dissolved the Republic and created a new socialist state, the Jamahiriya ("state of the masses"). Officially adopting a symbolic role in governance, he retained power as military commander-in-chief and head of the Revolutionary Committees responsible for policing and suppressing opponents. Overseeing unsuccessful border conflicts with Egypt and Chad, Gaddafi's support for foreign militants and alleged responsibility for the Lockerbie bombing led to Libya's label of "international pariah". A particularly hostile relationship developed with the United States and United Kingdom, resulting in the 1986 U.S. bombing of Libya and United Nations-imposed economic sanctions. Rejecting his earlier ideological commitments, from 1999 Gaddafi encouraged economic privatization and sought rapprochement with Western nations, also embracing Pan-Africanism and helping to establish the African Union. Amid the Arab Spring, in 2011 an anti-Gaddafist uprising led by the National Transitional Council (NTC) broke out, resulting in the Libyan Civil War. NATO intervened militarily on the side of the NTC, bringing about the government's downfall. Retreating to Sirte, Gaddafi was captured and killed by NTC militants.
Muammar Gaddafi was born in a tent near Qasr Abu Hadi, a rural area outside the town of Sirte in the deserts of western Libya. His family came from a small, relatively un-influential tribal group called the Qadhadhfa, who were Arabized Berber in heritage. His father, Mohammad Abdul Salam bin Hamed bin Mohammad, was known as Abu Meniar (died 1985), and his mother was named Aisha (died 1978); Abu Meniar earned a meager subsistence as a goat and camel herder. Nomadic Bedouins, they were illiterate and kept no birth records. As such, Gaddafi's date of birth is not known with certainty, and sources have set it in 1942 or in the spring of 1943, although biographers Blundy and Lycett noted that it could have been pre-1940. His parents' only surviving son, he had three older sisters. Gaddafi's upbringing in Bedouin culture influenced his personal tastes for the rest of his life. He repeatedly expressed a preference for the desert over the city and retreated to the desert to meditate.
From childhood, Gaddafi was aware of the involvement of European colonialists in Libya; his nation was occupied by Italy, and during the North African Campaign of World War II it witnessed conflict between Italian and British troops. According to later claims, Gaddafi's paternal grandfather, Abdessalam Bouminyar, was killed by the Italian Army during the Italian invasion of 1911. At World War II's end in 1945, Libya was occupied by British and French forces. Although Britain and France intended on dividing the nation between their empires, the General Assembly of the United Nations (UN) declared that the country be granted political independence. In 1951, the UN created the United Kingdom of Libya, a federal state under the leadership of a pro-western monarch, Idris, who banned political parties and established an absolute monarchy.
Gaddafi's earliest education was of a religious nature, imparted by a local Islamic teacher. Subsequently moving to nearby Sirte to attend elementary school, he progressed through six grades in four years. Education in Libya was not free, but his father thought it would greatly benefit his son despite the financial strain. During the week Gaddafi slept in a mosque, and at weekends walked 20 miles to visit his parents. Bullied for being a Bedouin, he was proud of his identity and encouraged pride in other Bedouin children. From Sirte, he and his family moved to the market town of Sabha in Fezzan, south-central Libya, where his father worked as a caretaker for a tribal leader while Muammar attended secondary school, something neither parent had done. Gaddafi was popular at school; some friends made there received significant jobs in his later administration, most notably his best friend Abdul Salam Jalloud.
Gaddafi organized demonstrations and distributed posters criticizing the monarchy. In October 1961, he led a demonstration protesting Syria's secession from the United Arab Republic. During this they broke windows of a local hotel accused of serving alcohol. Catching the authorities' attention, they expelled his family from Sabha. Gaddafi moved to Misrata, there attending Misrata Secondary School. Maintaining his interest in Arab nationalist activism, he refused to join any of the banned political parties active in the city – including the Arab Nationalist Movement, the Arab Socialist Ba'ath Party, and the Muslim Brotherhood – claiming he rejected factionalism. He read voraciously on the subjects of Nasser and the French Revolution of 1789, as well as the works of Syrian political theorist Michel Aflaq and biographies of Abraham Lincoln, Sun Yat-sen, and Mustafa Kemal Atatürk.
Gaddafi briefly studied History at the University of Libya in Benghazi, before dropping out to join the military. Despite his police record, in 1963 he began training at the Royal Military Academy, Benghazi, alongside several like-minded friends from Misrata. The armed forces offered the only opportunity for upward social mobility for underprivileged Libyans, and Gaddafi recognised it as a potential instrument of political change. Under Idris, Libya's armed forces were trained by the British military; this angered Gaddafi, who viewed the British as imperialists, and accordingly he refused to learn English and was rude to the British officers, ultimately failing his exams. British trainers reported him for insubordination and abusive behaviour, stating their suspicion that he was involved in the assassination of the military academy's commander in 1963. Such reports were ignored and Gaddafi quickly progressed through the course.
The Bovington signal course's director reported that Gaddafi successfully overcame problems learning English, displaying a firm command of voice procedure. Noting that Gaddafi's favourite hobbies were reading and playing football, he thought him an "amusing officer, always cheerful, hard-working, and conscientious." Gaddafi disliked England, claiming British Army officers racially insulted him and finding it difficult adjusting to the country's culture; asserting his Arab identity in London, he walked around Piccadilly wearing traditional Libyan robes. He later related that while he travelled to England believing it more advanced than Libya, he returned home "more confident and proud of our values, ideals and social character."
Many teachers at Sabha were Egyptian, and for the first time Gaddafi had access to pan-Arab newspapers and radio broadcasts, most notably the Cairo-based Voice of the Arabs. Growing up, Gaddafi witnessed significant events rock the Arab world, including the 1948 Arab–Israeli War, the Egyptian Revolution of 1952, the Suez Crisis of 1956, and the short-lived existence of the United Arab Republic between 1958 and 1961. Gaddafi admired the political changes implemented in the Arab Republic of Egypt under his hero, President Gamal Abdel Nasser. Nasser argued for Arab nationalism; the rejection of Western colonialism, neo-colonialism, and Zionism; and a transition from capitalism to socialism. Nasser's book, Philosophy of the Revolution, was a key influence on Gaddafi; outlining how to initiate a coup, it has been described as "the inspiration and blueprint of [Gaddafi's] revolution."
Idris' government was increasingly unpopular by the latter 1960s; it had exacerbated Libya's traditional regional and tribal divisions by centralising the country's federal system in order to take advantage of the country's oil wealth, while corruption and entrenched systems of patronage were widespread throughout the oil industry. Arab nationalism was increasingly popular, and protests flared up following Egypt's 1967 defeat in the Six-Day War with Israel; allied to the western powers, Idris' administration was seen as pro-Israeli. Anti-western riots broke out in Tripoli and Benghazi, while Libyan workers shut down oil terminals in solidarity with Egypt. By 1969, the U.S. Central Intelligence Agency was expecting segments of Libya's armed forces to launch a coup. Although claims have been made that they knew of Gaddafi's Free Officers Movement, they have since claimed ignorance, stating that they were monitoring Abdul Aziz Shalhi's Black Boots revolutionary group.
In mid-1969, Idris travelled abroad to spend the summer in Turkey and Greece. Gaddafi's Free Officers recognized this as their chance to overthrow the monarchy, initiating "Operation Jerusalem". On 1 September, they occupied airports, police depots, radio stations and government offices in Tripoli and Benghazi. Gaddafi took control of the Berka barracks in Benghazi, while Omar Meheisha occupied Tripoli barracks and Jalloud seized the city's anti-aircraft batteries. Khweldi Hameidi was sent to arrest crown prince Sayyid Hasan ar-Rida al-Mahdi as-Sanussi, and force him to relinquish his claim to the throne. They met no serious resistance, and wielded little violence against the monarchists.
Having removed the monarchical government, Gaddafi proclaimed the foundation of the Libyan Arab Republic. Addressing the populace by radio, he proclaimed an end to the "reactionary and corrupt" regime, "the stench of which has sickened and horrified us all." Due to the coup's bloodless nature, it was initially labelled the "White Revolution", although was later renamed the "One September Revolution" after the date on which it occurred. Gaddafi insisted that the Free Officers' coup represented a revolution, marking the start of widespread change in the socio-economic and political nature of Libya. He proclaimed that the revolution meant "freedom, socialism, and unity", and over the coming years implemented measures to achieve this.
Although theoretically a collegial body operating through consensus building, Gaddafi dominated the RCC, although some of the others attempted to constrain what they saw as his excesses. Gaddafi remained the government's public face, with the identities of the other RCC members only being publicly revealed on 10 January 1970. All young men from (typically rural) working and middle-class backgrounds, none had university degrees; in this way they were distinct from the wealthy, highly educated conservatives who previously governed the country.
The coup completed, the RCC proceeded with their intentions of consolidating the revolutionary government and modernizing the country. They purged monarchists and members of Idris' Senussi clan from Libya's political world and armed forces; Gaddafi believed this elite were opposed to the will of the Libyan people and had to be expunged. "People's Courts" were founded to try various monarchist politicians and journalists, and though many were imprisoned, none were executed. Idris was sentenced to execution in absentia.
In May 1970, the Revolutionary Intellectuals Seminar was held to bring intellectuals in line with the revolution, while that year's Legislative Review and Amendment united secular and religious law codes, introducing sharia into the legal system. Ruling by decree, the RCC maintained the monarchy's ban on political parties, in May 1970 banned trade unions, and in 1972 outlawed workers' strikes and suspended newspapers. In September 1971, Gaddafi resigned, claiming to be dissatisfied with the pace of reform, but returned to his position within a month. In February 1973, he resigned again, once more returning the following month.
With crude oil as the country's primary export, Gaddafi sought to improve Libya's oil sector. In October 1969, he proclaimed the current trade terms unfair, benefiting foreign corporations more than the Libyan state, and by threatening to reduce production, in December Jalloud successfully increased the price of Libyan oil. In 1970, other OPEC states followed suit, leading to a global increase in the price of crude oil. The RCC followed with the Tripoli Agreement, in which they secured income tax, back-payments and better pricing from the oil corporations; these measures brought Libya an estimated $1 billion in additional revenues in its first year.
Increasing state control over the oil sector, the RCC began a program of nationalization, starting with the expropriation of British Petroleum's share of the British Petroleum-N.B. Hunt Sahir Field in December 1971. In September 1973, it was announced that all foreign oil producers active in Libya were to be nationalized. For Gaddafi, this was an important step towards socialism. It proved an economic success; while gross domestic product had been $3.8 billion in 1969, it had risen to $13.7 billion in 1974, and $24.5 billion in 1979. In turn, the Libyans' standard of life greatly improved over the first decade of Gaddafi's administration, and by 1979 the average per-capita income was at $8,170, up from $40 in 1951; this was above the average of many industrialized countries like Italy and the U.K.
The RCC attempted to suppress regional and tribal affiliation, replacing it with a unified pan-Libyan identity. In doing so, they tried discrediting tribal leaders as agents of the old regime, and in August 1971 a Sabha military court tried many of them for counter-revolutionary activity. Long-standing administrative boundaries were re-drawn, crossing tribal boundaries, while pro-revolutionary modernizers replaced traditional leaders, but the communities they served often rejected them. Realizing the failures of the modernizers, Gaddafi created the Arab Socialist Union (ASU), a mass mobilization vanguard party of which he was president. The ASU recognized the RCC as its "Supreme Leading Authority", and was designed to further revolutionary enthusiasm throughout the country.
The RCC implemented measures for social reform, adopting sharia as a basis. The consumption of alcohol was banned, night clubs and Christian churches were shut down, traditional Libyan dress was encouraged, while Arabic was decreed as the only language permitted in official communications and on road signs. From 1969 to 1973, the RCC introduced social welfare programs funded with oil money, which led to house-building projects and improved healthcare and education. In doing so, they greatly expanded the public sector, providing employment for thousands.
The influence of Nasser's Arab nationalism over the RCC was immediately apparent. The administration was instantly recognized by the neighbouring Arab nationalist regimes in Egypt, Syria, Iraq and Sudan, with Egypt sending experts to aid the inexperienced RCC. Gaddafi propounded Pan-Arab ideas, proclaiming the need for a single Arab state stretching across North Africa and the Middle East. In December 1969, Libya founded the Arab Revolutionary Front with Egypt and Sudan as a step towards political unification, and in 1970 Syria stated its intention to join.
After Nasser died in November 1970, his successor, Anwar Sadat, suggested that rather than a unified state, they create a political federation, implemented in April 1971; in doing so, Egypt, Syria and Sudan got large grants of Libyan oil money. In February 1972, Gaddafi and Sadat signed an unofficial charter of merger, but it was never implemented as relations broke down the following year. Sadat became increasingly wary of Libya's radical direction, and the September 1973 deadline for implementing the Federation passed by with no action taken.
After the 1969 coup, representatives of the Four Powers – France, the United Kingdom, the United States and the Soviet Union – were called to meet RCC representatives. The U.K. and U.S. quickly extended diplomatic recognition, hoping to secure the position of their military bases in Libya and fearing further instability. Hoping to ingratiate themselves with Gaddafi, in 1970 the U.S. informed him of at least one planned counter-coup. Such attempts to form a working relationship with the RCC failed; Gaddafi was determined to reassert national sovereignty and expunge what he described as foreign colonial and imperialist influences. His administration insisted that the U.S. and U.K. remove their military bases from Libya, with Gaddafi proclaiming that "the armed forces which rose to express the people's revolution [will not] tolerate living in their shacks while the bases of imperialism exist in Libyan territory." The British left in March and the Americans in June 1970.
Moving to reduce Italian influence, in October 1970 all Italian-owned assets were expropriated and the 12,000-strong Italian community expelled from Libya alongside a smaller number of Jews. The day became a national holiday. Aiming to reduce NATO power in the Mediterranean, in 1971 Libya requested that Malta cease to allow NATO to use its land for a military base, in turn offering them foreign aid. Compromising, Malta's government continued allowing NATO use of the island, but only on the condition that they would not use it for launching attacks on Arab territory. Orchestrating a military build-up, the RCC began purchasing weapons from France and the Soviet Union. The commercial relationship with the latter led to an increasingly strained relationship with the U.S., who were then engaged in the Cold War with the Soviets.
His relationship with Palestinian leader Yasser Arafat of Fatah was strained, with Gaddafi considering him too moderate and calling for more violent action. Instead he supported militia like the Popular Front for the Liberation of Palestine, Popular Front for the Liberation of Palestine – General Command, the Democratic Front for the Liberation of Palestine, As-Sa'iqa, the Palestinian Popular Struggle Front, and the Abu Nidal Organization. He funded the Black September Organization who perpetrated the 1972 Munich massacre of Israeli athletes in West Germany, and had the killed militants' bodies flown to Libya for a hero's funeral. Gaddafi also welcomed the three surviving attackers in Tripoli following their release in exchange for the hostages of hijacked Lufthansa Flight 615 a few weeks later and allowed them to go into hiding.
Gaddafi financially supported other militant groups across the world, including the Black Panther Party, Nation of Islam, Tupamaros, 19th of April Movement and Sandinista National Liberation Front in the Americas, the ANC among other liberation movements in the fight against Apartheid in South Africa, the Provisional Irish Republican Army, ETA, Sardinian nationalists, Action directe, the Red Brigades, and the Red Army Faction in Europe, and the Armenian Secret Army, Japanese Red Army, Free Aceh Movement, and Moro National Liberation Front in Asia. Gaddafi was indiscriminate in the causes he funded, sometimes switching from supporting one side in a conflict to the other, as in the Eritrean War of Independence. Throughout the 1970s these groups received financial support from Libya, which came to be seen as a leader in the Third World's struggle against colonialism and neocolonialism. Though many of these groups were labelled "terrorists" by critics of their activities, Gaddafi rejected such a characterisation, instead considering them revolutionaries engaged in liberation struggles.
On 16 April 1973, Gaddafi proclaimed the start of a "Popular Revolution" in a Zuwarah speech. He initiated this with a 5-point plan, the first point of which dissolved all existing laws, to be replaced by revolutionary enactments. The second point proclaimed that all opponents of the revolution had to be removed, while the third initiated an administrative revolution that Gaddafi proclaimed would remove all traces of bureaucracy and the bourgeoisie. The fourth point announced that the population must form People's Committees and be armed to defend the revolution, while the fifth proclaimed the beginning of a cultural revolution to expunge Libya of "poisonous" foreign influences. He began to lecture on this new phase of the revolution in Libya, Egypt, and France.
As part of this Popular Revolution, Gaddafi invited Libya's people to found General People's Committees as conduits for raising political consciousness. Although offering little guidance for how to set up these councils, Gaddafi claimed that they would offer a form of direct political participation that was more democratic than a traditional party-based representative system. He hoped that the councils would mobilize the people behind the RCC, erode the power of the traditional leaders and the bureaucracy, and allow for a new legal system chosen by the people.
The People's Committees led to a high percentage of public involvement in decision making, within the limits permitted by the RCC, but exacerbated tribal divisions. They also served as a surveillance system, aiding the security services in locating individuals with views critical of the RCC, leading to the arrest of Ba'athists, Marxists and Islamists. Operating in a pyramid structure, the base form of these Committees were local working groups, who sent elected representatives to the district level, and from there to the national level, divided between the General People's Congress and the General People's Committee. Above these remained Gaddafi and the RCC, who remained responsible for all major decisions.
In June 1973, Gaddafi created a political ideology as a basis for the Popular Revolution. Third International Theory considered the U.S. and the Soviet Union as imperialist, thus rejected Western capitalism as well as Eastern bloc communism's atheism. In this respect it was similar to the Three Worlds Theory developed by China's political leader Mao Zedong. As part of this theory, Gaddafi praised nationalism as a progressive force and advocated the creation of a pan-Arab state which would lead the Islamic and Third Worlds against imperialism.
Gaddafi summarized Third International Theory in three short volumes published between 1975 and 1979, collectively known as The Green Book. Volume one was devoted to the issue of democracy, outlining the flaws of representative systems in favour of direct, participatory GPCs. The second dealt with Gaddafi's beliefs regarding socialism, while the third explored social issues regarding the family and the tribe. While the first two volumes advocated radical reform, the third adopted a socially conservative stance, proclaiming that while men and women were equal, they were biologically designed for different roles in life. During the years that followed, Gaddafists adopted quotes from The Green Book, such as "Representation is Fraud", as slogans. Meanwhile, in September 1975, Gaddafi implemented further measures to increase popular mobilization, introducing objectives to improve the relationship between the Councils and the ASU.
In September 1975, Gaddafi purged the army, arresting around 200 senior officers, and in October he founded the clandestine Office for the Security of the Revolution. In 1976, student demonstrations broke out in Tripoli and Benghazi, and were attacked by police and Gaddafist students. The RCC responded with mass arrests, and introduced compulsory national service for young people. Dissent also arose from conservative clerics and the Muslim Brotherhood, who were persecuted as anti-revolutionary. In January 1977, two dissenting students and a number of army officers were publicly hanged; Amnesty International condemned it as the first time in Gaddafist Libya that dissenters had been executed for purely political crimes.
Following Anwar Sadat's ascension to the Egyptian presidency, Libya's relations with Egypt deteriorated. Sadat was perturbed by Gaddafi's unpredictability and insistence that Egypt required a cultural revolution. In February 1973, Israeli forces shot down Libyan Arab Airlines Flight 114, which had strayed from Egyptian airspace into Israeli-held territory during a sandstorm. Gaddafi was infuriated that Egypt had not done more to prevent the incident, and in retaliation planned to destroy the RMS Queen Elizabeth 2, a British ship chartered by American Jews to sail to Haifa for Israel's 25th anniversary. Gaddafi ordered an Egyptian submarine to target the ship, but Sadat cancelled the order, fearing a military escalation.
Gaddafi was later infuriated when Egypt and Syria planned the Yom Kippur War against Israel without consulting him, and was angered when Egypt conceded to peace talks rather than continuing the war. Gaddafi become openly hostile to Egypt's leader, calling for Sadat's overthrow, and when Sudanese President Gaafar Nimeiry took Sadat's side, Gaddafi by 1975 sponsored the Sudan People's Liberation Army to overthrow Nimeiry. Focusing his attention elsewhere in Africa, in late 1972 and early 1973, Libya invaded Chad to annex the uranium-rich Aouzou Strip. Offering financial incentives, he successfully convinced 8 African states to break off diplomatic relations with Israel in 1973. Intent on propagating Islam, in 1973 Gaddafi founded the Islamic Call Society, which had opened 132 centres across Africa within a decade. In 1973 he converted Gabonese President Omar Bongo, an action which he repeated three years later with Jean-Bédel Bokassa, president of the Central African Republic.
Gaddafi sought to develop closer links in the Maghreb; in January 1974 Libya and Tunisia announced a political union, the Arab Islamic Republic. Although advocated by Gaddafi and Tunisian President Habib Bourguiba, the move was deeply unpopular in Tunisia and soon abandoned. Retaliating, Gaddafi sponsored anti-government militants in Tunisia into the 1980s. Turning his attention to Algeria, in 1975 Libya signed the Hassi Messaoud defence agreement allegedly to counter "Moroccan expansionism", also funding the Polisario Front of Western Sahara in their independence struggle against Morocco. Seeking to diversify Libya's economy, Gaddafi's government began purchasing shares in major European corporations like Fiat as well as buying real estate in Malta and Italy, which would become a valuable source of income during the 1980s oil slump.
On 2 March 1977 the General People's Congress adopted the "Declaration of the Establishment of the People's Authority" at Gaddafi's behest. Dissolving the Libyan Arab Republic, it was replaced by the Great Socialist People's Libyan Arab Jamahiriya (Arabic: الجماهيرية العربية الليبية الشعبية الاشتراكية‎, al-Jamāhīrīyah al-‘Arabīyah al-Lībīyah ash-Sha‘bīyah al-Ishtirākīyah), a "state of the masses" conceptualized by Gaddafi. Officially, the Jamahiriya was a direct democracy in which the people ruled themselves through the 187 Basic People's Congresses, where all adult Libyans participated and voted on national decisions. These then sent members to the annual General People's Congress, which was broadcast live on television. In principle, the People's Congresses were Libya's highest authority, with major decisions proposed by government officials or with Gaddafi himself requiring the consent of the People's Congresses.
Debate remained limited, and major decisions regarding the economy and defence were avoided or dealt with cursorily; the GPC largely remained "a rubber stamp" for Gaddafi's policies. On rare occasions, the GPC opposed Gaddafi's suggestions, sometimes successfully; notably, when Gaddafi called on primary schools to be abolished, believing that home schooling was healthier for children, the GPC rejected the idea. In other instances, Gaddafi pushed through laws without the GPC's support, such as when he desired to allow women into the armed forces. Gaddafi proclaimed that the People's Congresses provided for Libya's every political need, rendering other political organizations unnecessary; all non-authorized groups, including political parties, professional associations, independent trade unions and women's groups, were banned.
With preceding legal institutions abolished, Gaddafi envisioned the Jamahiriya as following the Qur'an for legal guidance, adopting sharia law; he proclaimed "man-made" laws unnatural and dictatorial, only permitting Allah's law. Within a year he was backtracking, announcing that sharia was inappropriate for the Jamahiriya because it guaranteed the protection of private property, contravening The Green Book's socialism. His emphasis on placing his own work on a par with the Qur'an led conservative clerics to accuse him of shirk, furthering their opposition to his regime. In July, a border war broke out with Egypt, in which the Egyptians defeated Libya despite their technological inferiority. The conflict lasted one week before both sides agreed to sign a peace treaty that was brokered by several Arab states. That year, Gaddafi was invited to Moscow by the Soviet government in recognition of their increasing commercial relationship.
In December 1978, Gaddafi stepped down as Secretary-General of the GPC, announcing his new focus on revolutionary rather than governmental activities; this was part of his new emphasis on separating the apparatus of the revolution from the government. Although no longer in a formal governmental post, he adopted the title of "Leader of the Revolution" and continued as commander-in-chief of the armed forces. He continued exerting considerable influence over Libya, with many critics insisting that the structure of Libya's direct democracy gave him "the freedom to manipulate outcomes".
Libya began to turn towards socialism. In March 1978, the government issued guidelines for housing redistribution, attempting to ensure the population that every adult Libyan owned his own home and that nobody was enslaved to paying their rent. Most families were banned from owning more than one house, while former rental properties were seized and sold to the tenants at a heavily subsidized price. In September, Gaddafi called for the People's Committees to eliminate the "bureaucracy of the public sector" and the "dictatorship of the private sector"; the People's Committees took control of several hundred companies, converting them into worker cooperatives run by elected representatives.
On 2 March 1979, the GPC announced the separation of government and revolution, the latter being represented by new Revolutionary Committees, who operated in tandem with the People's Committees in schools, universities, unions, the police force and the military. Dominated by revolutionary zealots, the Revolutionary Committees were led by Mohammad Maghgoub and a Central Coordinating Office, and met with Gaddafi annually. Publishing a weekly magazine The Green March (al-Zahf al-Akhdar), in October 1980 they took control of the press. Responsible for perpetuating revolutionary fervour, they performed ideological surveillance, later adopting a significant security role, making arrests and putting people on trial according to the "law of the revolution" (qanun al-thawra). With no legal code or safeguards, the administration of revolutionary justice was largely arbitrary and resulted in widespread abuses and the suppression of civil liberties: the "Green Terror."
In 1979, the committees began the redistribution of land in the Jefara plain, continuing through 1981. In May 1980, measures to redistribute and equalize wealth were implemented; anyone with over 1000 dinar in his bank account saw that extra money expropriated. The following year, the GPC announced that the government would take control of all import, export and distribution functions, with state supermarkets replacing privately owned businesses; this led to a decline in the availability of consumer goods and the development of a thriving black market.
The Jamahiriya's radical direction earned the government many enemies. In February 1978, Gaddafi discovered that his head of military intelligence was plotting to kill him, and began to increasingly entrust security to his Qaddadfa tribe. Many who had seen their wealth and property confiscated turned against the administration, and a number of western-funded opposition groups were founded by exiles. Most prominent was the National Front for the Salvation of Libya (NFSL), founded in 1981 by Mohammed Magariaf, which orchestrated militant attacks against Libya's government, while another, al-Borkan, began killing Libyan diplomats abroad. Following Gaddafi's command to kill these "stray dogs", under Colonel Younis Bilgasim's leadership, the Revolutionary Committees set up overseas branches to suppress counter-revolutionary activity, assassinating various dissidents. Although nearby nations like Syria also used hit squads, Gaddafi was unusual in publicly bragging about his administration's use of them; in June 1980, he ordered all dissidents to return home or be "liquidated wherever you are."
In 1979, the U.S. placed Libya on its list of "State Sponsors of Terrorism", while at the end of the year a demonstration torched the U.S. embassy in Tripoli in solidarity with the perpetrators of the Iran hostage crisis. The following year, Libyan fighters began intercepting U.S. fighter jets flying over the Mediterranean, signalling the collapse of relations between the two countries. Libyan relations with Lebanon and Shi'ite communities across the world also deteriorated due to the August 1978 disappearance of imam Musa al-Sadr when visiting Libya; the Lebanese accused Gaddafi of having him killed or imprisoned, a charge he denied. Relations with Syria improved, as Gaddafi and Syrian President Hafez al-Assad shared an enmity with Israel and Egypt's Sadat. In 1980, they proposed a political union, with Libya paying off Syria's £1 billion debt to the Soviet Union; although pressures led Assad to pull out, they remained allies. Another key ally was Uganda, and in 1979, Gaddafi sent 2,500 troops into Uganda to defend the regime of President Idi Amin from Tanzanian invaders. The mission failed; 400 Libyans were killed and they were forced to retreat. Gaddafi later came to regret his alliance with Amin, openly criticising him.
The early and mid-1980s saw economic trouble for Libya; from 1982 to 1986, the country's annual oil revenues dropped from $21 billion to $5.4 billion. Focusing on irrigation projects, 1983 saw construction start on "Gaddafi's Pet Project", the Great Man-Made River; although designed to be finished by the end of the decade, it remained incomplete at the start of the 21st century. Military spending increased, while other administrative budgets were cut back. Libya had long supported the FROLINAT militia in neighbouring Chad, and in December 1980, re-invaded Chad at the request of the Frolinat-controlled GUNT government to aid in the civil war; in January 1981, Gaddafi suggested a political merger. The Organisation of African Unity (OAU) rejected this, and called for a Libyan withdrawal, which came about in November 1981. The civil war resumed, and so Libya sent troops back in, clashing with French forces who supported the southern Chadian forces. Many African nations had tired of Libya's policies of interference in foreign affairs; by 1980, nine African states had cut off diplomatic relations with Libya, while in 1982 the OAU cancelled its scheduled conference in Tripoli in order to prevent Gaddafi gaining chairmanship. Proposing political unity with Morocco, in August 1984, Gaddafi and Moroccan monarch Hassan II signed the Oujda Treaty, forming the Arab-African Union; such a union was considered surprising due to the strong political differences and longstanding enmity that existed between the two governments. Relations remained strained, particularly due to Morocco's friendly relations with the U.S. and Israel; in August 1986, Hassan abolished the union. Domestic threats continued to plague Gaddafi; in May 1984, his Bab al-Azizia home was unsuccessfully attacked by a joint NFSL–Muslim Brotherhood militia, and in the aftermath 5000 dissidents were arrested.
In 1981, the new US President Ronald Reagan pursued a hard line approach to Libya, erroneously considering it a puppet regime of the Soviet Union. In turn, Gaddafi played up his commercial relationship with the Soviets, visiting Moscow again in April 1981 and 1985, and threatening to join the Warsaw Pact. The Soviets were nevertheless cautious of Gaddafi, seeing him as an unpredictable extremist. Beginning military exercises in the Gulf of Sirte – an area of sea that Libya claimed as a part of its territorial waters – in August 1981 the U.S. shot down two Libyan Su-22 planes monitoring them. Closing down Libya's embassy in Washington, D.C., Reagan advised U.S. companies operating in the country to reduce the number of American personnel stationed there. In March 1982, the U.S. implemented an embargo of Libyan oil, and in January 1986 ordered all U.S. companies to cease operating in the country, although several hundred workers remained. Diplomatic relations also broke down with the U.K., after Libyan diplomats were accused in the shooting death of Yvonne Fletcher, a British policewoman stationed outside their London embassy, in April 1984. In Spring 1986, the U.S. Navy again began performing exercises in the Gulf of Sirte; the Libyan military retaliated, but failed as the U.S. sank several Libyan ships.
After the U.S. accused Libya of orchestrating the 1986 Berlin discotheque bombing, in which two American soldiers died, Reagan decided to retaliate militarily. The Central Intelligence Agency were critical of the move, believing that Syria were a greater threat and that an attack would strengthen Gaddafi's reputation; however Libya was recognised as a "soft target." Reagan was supported by the U.K. but opposed by other European allies, who argued that it would contravene international law. In Operation El Dorado Canyon, orchestrated on 15 April 1986, U.S. military planes launched a series of air-strikes on Libya, bombing military installations in various parts of the country, killing around 100 Libyans, including several civilians. One of the targets had been Gaddafi's home. Himself unharmed, two of Gaddafi's sons were injured, and he claimed that his four-year-old adopted daughter Hanna was killed, although her existence has since been questioned.  In the immediate aftermath, Gaddafi retreated to the desert to meditate, while there were sporadic clashes between Gaddafists and army officers who wanted to overthrow the government. Although the U.S. was condemned internationally, Reagan received a popularity boost at home. Publicly lambasting U.S. imperialism, Gaddafi's reputation as an anti-imperialist was strengthened both domestically and across the Arab world, and in June 1986, he ordered the names of the month to be changed in Libya.
The late 1980s saw a series of liberalising economic reforms within Libya designed to cope with the decline in oil revenues. In May 1987, Gaddafi announced the start of the "Revolution within a Revolution", which began with reforms to industry and agriculture and saw the re-opening of small business. Restrictions were placed on the activities of the Revolutionary Committees; in March 1988, their role was narrowed by the newly created Ministry for Mass Mobilization and Revolutionary Leadership to restrict their violence and judicial role, while in August 1988 Gaddafi publicly criticised them, asserting that "they deviated, harmed, tortured" and that "the true revolutionary does not practise repression." In March, hundreds of political prisoners were freed, with Gaddafi falsely claiming that there were no further political prisoners in Libya. In June, Libya's government issued the Great Green Charter on Human Rights in the Era of the Masses, in which 27 articles laid out goals, rights and guarantees to improve the situation of human rights in Libya, restricting the use of the death penalty and calling for its eventual abolition. Many of the measures suggested in the charter would be implemented the following year, although others remained inactive. Also in 1989, the government founded the Al-Gaddafi International Prize for Human Rights, to be awarded to figures from the Third World who had struggled against colonialism and imperialism; the first year's winner was South African anti-apartheid activist Nelson Mandela. From 1994 through to 1997, the government initiated cleansing committees to root out corruption, particularly in the economic sector.
In the aftermath of the 1986 U.S. attack, the army was purged of perceived disloyal elements, and in 1988, Gaddafi announced the creation of a popular militia to replace the army and police. In 1987, Libya began production of mustard gas at a facility in Rabta, although publicly denying it was stockpiling chemical weapons, and unsuccessfully attempted to develop nuclear weapons. The period also saw a growth in domestic Islamist opposition, formulated into groups like the Muslim Brotherhood and the Libyan Islamic Fighting Group. A number of assassination attempts against Gaddafi were foiled, and in turn, 1989 saw the security forces raid mosques believed to be centres of counter-revolutionary preaching. In October 1993, elements of the increasingly marginalised army initiated a failed coup in Misrata, while in September 1995, Islamists launched an insurgency in Benghazi, and in July 1996 an anti-Gaddafist football riot broke out in Tripoli. The Revolutionary Committees experienced a resurgence to combat these Islamists.
In 1989, Gaddafi was overjoyed by the foundation of the Arab Maghreb Union, uniting Libya in an economic pact with Mauritania, Morocco, Tunisia and Algeria, viewing it as beginnings of a new Pan-Arab union. Meanwhile, Libya stepped up its support for anti-western militants such as the Provisional IRA, and in 1988, Pan Am Flight 103 was blown up over Lockerbie in Scotland, killing 243 passengers and 16 crew members, plus 11 people on the ground. British police investigations identified two Libyans – Abdelbaset al-Megrahi and Lamin Khalifah Fhimah – as the chief suspects, and in November 1991 issued a declaration demanding that Libya hand them over. When Gaddafi refused, citing the Montreal Convention, the United Nations (UN) imposed Resolution 748 in March 1992, initiating economic sanctions against Libya which had deep repercussions for the country's economy. The country suffered an estimated $900 million financial loss as a result. Further problems arose with the west when in January 1989, two Libyan warplanes were shot down by the U.S. off the Libyan coast. Many African states opposed the UN sanctions, with Mandela criticising them on a visit to Gaddafi in October 1997, when he praised Libya for its work in fighting apartheid and awarded Gaddafi the Order of Good Hope. They would only be suspended in 1998 when Libya agreed to allow the extradition of the suspects to the Scottish Court in the Netherlands, in a process overseen by Mandela.
As the 20th century came to a close, Gaddafi increasingly rejected Arab nationalism, frustrated by the failure of his Pan-Arab ideals; instead he turned to Pan-Africanism, emphasising Libya's African identity. From 1997 to 2000, Libya initiated cooperative agreements or bilateral aid arrangements with 10 African states, and in 1999 joined the Community of Sahel-Saharan States. In June 1999, Gaddafi visited Mandela in South Africa, and the following month attended the OAU summit in Algiers, calling for greater political and economic integration across the continent and advocating the foundation of a United States of Africa. He became one of the founders of the African Union (AU), initiated in July 2002 to replace the OAU; at the opening ceremonies, he proclaimed that African states should reject conditional aid from the developed world, a direct contrast to the message of South African President Thabo Mbeki. At the third AU summit, held in Libya in July 2005, he called for a greater level of integration, advocating a single AU passport, a common defence system and a single currency, utilising the slogan: "The United States of Africa is the hope." In June 2005, Libya joined the Common Market for Eastern and Southern Africa (COMESA), and in August 2008 Gaddafi was proclaimed "King of Kings" by an assembled committee of traditional African leaders. On 1 February 2009, his "coronation ceremony" was held in Addis Ababa, Ethiopia, coinciding with Gaddafi's election as AU chairman for a year.
The era saw Libya's return to the international arena. In 1999, Libya began secret talks with the British government to normalise relations. In 2001, Gaddafi condemned the September 11 attacks on the U.S. by al-Qaeda, expressing sympathy with the victims and calling for Libyan involvement in the War on Terror against militant Islamism. His government continued suppressing domestic Islamism, at the same time as Gaddafi called for the wider application of sharia law. Libya also cemented connections with China and North Korea, being visited by Chinese President Jiang Zemin in April 2002. Influenced by the events of the Iraq War, in December 2003, Libya renounced its possession of weapons of mass destruction, decommissioning its chemical and nuclear weapons programs. Relations with the U.S. improved as a result, while UK Prime Minister Tony Blair met with Gaddafi in the Libyan desert in March 2004. The following month, Gaddafi travelled to the headquarters of the European Union (EU) in Brussels, signifying improved relations between Libya and the EU, the latter ending its remaining sanctions in October. In October 2010, the EU paid Libya €50 million to stop African migrants passing into Europe; Gaddafi encouraged the move, saying that it was necessary to prevent the loss of European cultural identity to a new "Black Europe". Removed from the U.S. list of state sponsors of terrorism in 2006, Gaddafi nevertheless continued his anti-western rhetoric, and at the Second Africa-South America Summit in Venezuela in September 2009, joined Venezuelan President Hugo Chávez in calling for an "anti-imperialist" front across Africa and Latin America. Gaddafi proposed the establishment of a South Atlantic Treaty Organization to rival NATO. That month he also addressed the United Nations General Assembly in New York for the first time, using it to condemn "western aggression". In Spring 2010, Gaddafi proclaimed jihad against Switzerland after Swiss police accused two of his family members of criminal activity in the country, resulting in the breakdown of bilateral relations.
Libya's economy witnessed increasing privatization; although rejecting the socialist policies of nationalized industry advocated in The Green Book, government figures asserted that they were forging "people's socialism" rather than capitalism. Gaddafi welcomed these reforms, calling for wide-scale privatization in a March 2003 speech. In 2003, the oil industry was largely sold to private corporations, and by 2004, there was $40 billion of direct foreign investment in Libya, a sixfold rise over 2003. Sectors of Libya's population reacted against these reforms with public demonstrations, and in March 2006, revolutionary hard-liners took control of the GPC cabinet; although scaling back the pace of the changes, they did not halt them. In 2010, plans were announced that would have seen half the Libyan economy privatized over the following decade. While there was no accompanying political liberalization, with Gaddafi retaining predominant control, in March 2010, the government devolved further powers to the municipal councils. Rising numbers of reformist technocrats attained positions in the country's governance; best known was Gaddafi's son and heir apparent Saif al-Islam Gaddafi, who was openly critical of Libya's human rights record. He led a group who proposed the drafting of the new constitution, although it was never adopted, and in October 2009 was appointed to head the PSLC. Involved in encouraging tourism, Saif founded several privately run media channels in 2008, but after criticising the government they were nationalised in 2009. In October 2010, Gaddafi apologized to African leaders on behalf of Arab nations for their involvement in the African slave trade.
Following the start of the Arab Spring in 2011, Gaddafi spoke out in favour of Tunisian President Zine El Abidine Ben Ali, then threatened by the Tunisian Revolution. He suggested that Tunisia's people would be satisfied if Ben Ali introduced a Jamahiriyah system there. Fearing domestic protest, Libya's government implemented preventative measures, reducing food prices, purging the army leadership of potential defectors and releasing several Islamist prisoners. They proved ineffective, and on 17 February 2011, major protests broke out against Gaddafi's government. Unlike Tunisia or Egypt, Libya was largely religiously homogenous and had no strong Islamist movement, but there was widespread dissatisfaction with the corruption and entrenched systems of patronage, while unemployment had reached around 30%.
Accusing the rebels of being "drugged" and linked to al-Qaeda, Gaddafi proclaimed that he would die a martyr rather than leave Libya. As he announced that the rebels would be "hunted down street by street, house by house and wardrobe by wardrobe", the army opened fire on protests in Benghazi, killing hundreds. Shocked at the government's response, a number of senior politicians resigned or defected to the protesters' side. The uprising spread quickly through Libya's less economically developed eastern half. By February's end, eastern cities like Benghazi, Misrata, al-Bayda and Tobruk were controlled by rebels, and the Benghazi-based National Transitional Council (NTC) had been founded to represent them.
In the conflict's early months it appeared that Gaddafi's government – with its greater firepower – would be victorious. Both sides disregarded the laws of war, committing human rights abuses, including arbitrary arrests, torture, extrajudicial executions and revenge attacks. On 26 February the United Nations Security Council passed Resolution 1970, suspending Libya from the UN Human Rights Council, implementing sanctions and calling for an International Criminal Court (ICC) investigation into the killing of unarmed civilians. In March, the Security Council declared a no fly zone to protect the civilian population from aerial bombardment, calling on foreign nations to enforce it; it also specifically prohibited foreign occupation. Ignoring this, Qatar sent hundreds of troops to support the dissidents, and along with France and the United Arab Emirates provided the NTC with weaponry and training.
A week after the implementation of the no-fly zone, NATO announced that it would be enforced. On 30 April a NATO airstrike killed Gaddafi's sixth son and three of his grandsons in Tripoli, though Gaddafi and his wife were unharmed. Western officials remained divided over whether Gaddafi was a legitimate military target under the U.N. Security Council resolution. U.S. Secretary of Defense Robert Gates said that NATO was "not targeting Gaddafi specifically" but that his command-and-control facilities were legitimate targets—including a facility inside his sprawling Tripoli compound that was hit with airstrikes on 25 April.
On 27 June, the ICC issued arrest warrants for Gaddafi, his son Saif al-Islam, and his brother-in-law Abdullah Senussi, head of state security, for charges concerning crimes against humanity. Libyan officials rejected the ICC, claiming that it had "no legitimacy whatsoever" and highlighting that "all of its activities are directed at African leaders". That month, Amnesty International published their findings, in which they asserted that many of the accusations of mass human rights abuses made against Gaddafist forces lacked credible evidence, and were instead fabrications of the rebel forces which had been readily adopted by the western media. Amnesty International did however still accuse Gaddafi forces of numerous war crimes. On 15 July 2011, at a meeting in Istanbul, over 30 governments recognised the NTC as the legitimate government of Libya. Gaddafi responded to the announcement with a speech on Libyan national television, in which he called on supporters to "Trample on those recognitions, trample on them under your feet ... They are worthless".
Now with NATO support in the form of air cover, the rebel militia pushed westward, defeating loyalist armies and securing control of the centre of the country. Gaining the support of Amazigh (Berber) communities of the Nafusa Mountains, who had long been persecuted as non-Arabic speakers under Gaddafi, the NTC armies surrounded Gaddafi loyalists in several key areas of western Libya. In August, the rebels seized Zliten and Tripoli, ending the last vestiges of Gaddafist power. On 25 August, the Arab League recognised the NTC to be "the legitimate representative of the Libyan state", on which basis Libya would resume its membership in the League.
Only a few towns in western Libya—such as Bani Walid, Sebha and Sirte—remained Gaddafist strongholds. Retreating to Sirte after Tripoli's fall, Gaddafi announced his willingness to negotiate for a handover to a transitional government, a suggestion rejected by the NTC. Surrounding himself with bodyguards, he continually moved residences to escape NTC shelling, devoting his days to prayer and reading the Qur'an. On 20 October, Gaddafi broke out of Sirte's District 2 in a joint civilian-military convoy, hoping to take refuge in the Jarref Valley. At around 8.30am, NATO bombers attacked, destroying at least 14 vehicles and killing at least 53. The convoy scattered, and Gaddafi and those closest to him fled to a nearby villa, which was shelled by rebel militia from Misrata. Fleeing to a construction site, Gaddafi and his inner cohort hid inside drainage pipes while his bodyguards battled the rebels; in the conflict, Gaddafi suffered head injuries from a grenade blast while defence minister Abu-Bakr Yunis Jabr was killed.
A Misratan militia took Gaddafi prisoner, beating him, causing serious injuries; the events were filmed on a mobile phone. A video appears to picture Gaddafi being poked or stabbed in the rear end "with some kind of stick or knife" or possibly a bayonet. Pulled onto the front of a pick-up truck, he fell off as it drove away. His semi-naked, lifeless body was then placed into an ambulance and taken to Misrata; upon arrival, he was found to be dead. Official NTC accounts claimed that Gaddafi was caught in a cross-fire and died from his bullet wounds. Other eye-witness accounts claimed that rebels had fatally shot Gaddafi in the stomach; a rebel identifying himself as Senad el-Sadik el-Ureybi later claimed responsibility. Gaddafi's son Mutassim, who had also been among the convoy, was also captured, and found dead several hours later, most probably from an extrajudicial execution. Around 140 Gaddafi loyalists were rounded up from the convoy; tied up and abused, the corpses of 66 were found at the nearby Mahari Hotel, victims of extrajudicial execution. Libya's chief forensic pathologist, Dr. Othman al-Zintani, carried out the autopsies of Gaddafi, his son and Jabr in the days following their deaths; although the pathologist initially told the press that Gaddafi had died from a gunshot wound to the head, the autopsy report was not made public.
On the afternoon of Gaddafi's death, NTC Prime Minister Mahmoud Jibril publicly revealed the news. Gaddafi's corpse was placed in the freezer of a local market alongside the corpses of Yunis Jabr and Mutassim; the bodies were publicly displayed for four days, with Libyans from all over the country coming to view them. In response to international calls, on 24 October Jibril announced that a commission would investigate Gaddafi's death. On 25 October, the NTC announced that Gaddafi had been buried at an unidentified location in the desert; Al Aan TV showed amateur video footage of the funeral. Seeking vengeance for the killing, Gaddafist sympathisers fatally wounded one of those who had captured Gaddafi, Omran Shaaban, near Bani Walid in September 2012.
As a schoolboy, Gaddafi adopted the ideologies of Arab nationalism and Arab socialism, influenced in particular by Nasserism, the thought of Egyptian revolutionary and president Gamal Abdel Nasser, whom Gaddafi adopted as his hero. During the early 1970s, Gaddafi formulated his own particular approach to Arab nationalism and socialism, known as Third International Theory, which has been described as a combination of "utopian socialism, Arab nationalism, and the Third World revolutionary theory that was in vogue at the time". He laid out the principles of this Theory in the three volumes of The Green Book, in which he sought to "explain the structure of the ideal society." His Arab nationalist views led him to believe that there needed to be unity across the Arab world, combining the Arab nation under a single nation-state. He described his approach to economics as "Islamic socialism", although biographers Blundy and Lycett noted that Gaddafi's socialism had a "curiously Marxist undertone", with political scientist Sami Hajjar arguing that Gaddafi's model of socialism offered a simplification of Karl Marx and Friedrich Engels' theories. Gaddafi saw his socialist Jamahiriyah as a model for the Arab, Islamic, and non-aligned worlds to follow.
Gaddafi's ideological worldview was moulded by his environment, namely his Islamic faith, his Bedouin upbringing, and his disgust at the actions of European colonialists in Libya. He was driven by a sense of "divine mission", believing himself a conduit of Allah's will, and thought that he must achieve his goals "no matter what the cost". Raised within the Sunni branch of Islam, Gaddafi called for the implementation of sharia within Libya. He desired unity across the Islamic world, and encouraged the propagation of the faith elsewhere. On a 2010 visit to Italy, he paid a modelling agency to find 200 young Italian women for a lecture he gave urging them to convert. He also funded the construction and renovation of two mosques in Africa, including Uganda's Kampala Mosque. He nevertheless clashed with conservative Libyan clerics as to his interpretation of Islam. Many criticised his attempts to encourage women to enter traditionally male-only sectors of society, such as the armed forces. Gaddafi was keen to improve women's status, though saw the sexes as "separate but equal" and therefore felt women should usually remain in traditional roles.
A fundamental part of Gaddafi's ideology was anti-Zionism. He believed that the state of Israel should not exist, and that any Arab compromise with the Israeli government was a betrayal of the Arab people. In large part due to their support of Israel, Gaddafi despised the United States, considering the country to be imperialist and lambasting it as "the embodiment of evil." Rallying against Jews in many of his speeches, his anti-Semitism has been described as "almost Hitlerian" by Blundy and Lycett. From the late 1990s onward, his view seemed to become more moderate. In 2007, he advocated the Isratin single-state solution to the Israeli–Palestinian conflict, stating that "the [Israel-Palestine] solution is to establish a democratic state for the Jews and the Palestinians... This is the fundamental solution, or else the Jews will be annihilated in the future, because the Palestinians have [strategic] depth." Two years later he argued that a single-state solution would "move beyond old conflicts and look to a unified future based on shared culture and respect."
Gaddafi was a very private individual, who described himself as a "simple revolutionary" and "pious Muslim" called upon by Allah to continue Nasser's work. Reporter Mirella Bianco found that his friends considered him particularly loyal and generous, and asserted that he adored children. She was told by Gaddafi's father that even as a child he had been "always serious, even taciturn", a trait he also exhibited in adulthood. His father said that he was courageous, intelligent, pious, and family oriented.
In the 1970s and 1980s there were reports of his making sexual advances toward female reporters and members of his entourage. After the civil war, more serious charges came to light. Annick Cojean, a journalist for Le Monde, wrote in her book, Gaddafi's Harem that Gaddafi had raped, tortured, performed urolagnia, and imprisoned hundreds or thousands of women, usually very young. Another source—Libyan psychologist Seham Sergewa—reported that several of his female bodyguards claim to have been raped by Gaddafi and senior officials. After the civil war, Luis Moreno Ocampo, prosecutor for the International Criminal Court, said there was evidence that Gaddafi told soldiers to rape women who had spoken out against his regime. In 2011 Amnesty International questioned this and other claims used to justify NATO's war in Libya.
Following his ascension to power, Gaddafi moved into the Bab al-Azizia barracks, a six-mile long fortified compound located two miles from the center of Tripoli. His home and office at Azizia was a bunker designed by West German engineers, while the rest of his family lived in a large two-story building. Within the compound were also two tennis courts, a soccer field, several gardens, camels, and a Bedouin tent in which he entertained guests. In the 1980s, his lifestyle was considered modest in comparison to those of many other Arab leaders. Gaddafi allegedly worked for years with Swiss banks to launder international banking transactions. In November 2011, The Sunday Times identified property worth £1 billion in the UK that Gaddafi allegedly owned. Gaddafi had an Airbus A340 private jet, which he bought from Prince Al-Waleed bin Talal of Saudi Arabia for $120 million in 2003. Operated by Tripoli-based Afriqiyah Airways and decorated externally in their colours, it had various luxuries including a jacuzzi.
Gaddafi married his first wife, Fatiha al-Nuri, in 1969. She was the daughter of General Khalid, a senior figure in King Idris' administration, and was from a middle-class background. Although they had one son, Muhammad Gaddafi (b. 1970), their relationship was strained, and they divorced in 1970. Gaddafi's second wife was Safia Farkash, née el-Brasai, a former nurse from Obeidat tribe born in Bayda. They met in 1969, following his ascension to power, when he was hospitalized with appendicitis; he claimed that it was love at first sight. The couple remained married until his death. Together they had seven biological children: Saif al-Islam Gaddafi (b. 1972), Al-Saadi Gaddafi (b. 1973), Mutassim Gaddafi (1974–2011), Hannibal Muammar Gaddafi (b. 1975), Ayesha Gaddafi (b. 1976), Saif al-Arab Gaddafi (1982–2011), and Khamis Gaddafi (1983–2011). He also adopted two children, Hanna Gaddafi and Milad Gaddafi.
Biographers Blundy and Lycett believed that he was "a populist at heart." Throughout Libya, crowds of supporters would turn up to public events at which he appeared; described as "spontaneous demonstrations" by the government, there are recorded instances of groups being coerced or paid to attend. He was typically late to public events, and would sometimes not show up at all. Although Bianco thought he had a "gift for oratory", he was considered a poor orator by biographers Blundy and Lycett. Biographer Daniel Kawczynski noted that Gaddafi was famed for his "lengthy, wandering" speeches, which typically involved criticising Israel and the U.S.
Gaddafi was notably confrontational in his approach to foreign powers, and generally shunned western ambassadors and diplomats, believing them to be spies. He once said that HIV was "a peaceful virus, not an aggressive virus" and assured attendees at the African Union that "if you are straight you have nothing to fear from AIDS". He also said that the H1N1 influenza virus was a biological weapon manufactured by a foreign military, and he assured Africans that the tsetse fly and mosquito were "God's armies which will protect us against colonialists". Should these 'enemies' come to Africa, "they will get malaria and sleeping sickness".
Starting in the 1980s, he travelled with his all-female Amazonian Guard, who were allegedly sworn to a life of celibacy. However, according to psychologist Seham Sergewa, after the civil war several of the guards told her they had been pressured into joining and raped by Gaddafi and senior officials. He hired several Ukrainian nurses to care for him and his family's health, and traveled everywhere with his trusted Ukrainian nurse Halyna Kolotnytska. Kolotnytska's daughter denied the suggestion that the relationship was anything but professional.
Gaddafi remained a controversial and divisive figure on the world stage throughout his life and after death. Supporters praised Gaddafi's administration for the creation of an almost classless society through domestic reform. They stress the regime's achievements in combating homelessness and ensuring access to food and safe drinking water. Highlighting that under Gaddafi, all Libyans enjoyed free education to a university level, they point to the dramatic rise in literacy rates after the 1969 revolution. Supporters have also applauded achievements in medical care, praising the universal free healthcare provided under the Gaddafist administration, with diseases like cholera and typhoid being contained and life expectancy raised. Biographers Blundy and Lycett believed that under the first decade of Gaddafi's leadership, life for most Libyans "undoubtedly changed for the better" as material conditions and wealth drastically improved, while Libyan studies specialist Lillian Craig Harris remarked that in the early years of his administration, Libya's "national wealth and international influence soared, and its national standard of living has risen dramatically." Such high standards declined during the 1980s, as a result of economic stagnation. Gaddafi claimed that his Jamahiriya was a "concrete utopia", and that he had been appointed by "popular assent", with some Islamic supporters believing that he exhibited barakah. His opposition to Western governments earned him the respect of many in the Euro-American far right.
Critics labelled Gaddafi "despotic, cruel, arrogant, vain and stupid", with western governments and press presenting him as the "vicious dictator of an oppressed people". During the Reagan administration, the United States regarded him as "Public Enemy No. 1" and Reagan famously dubbed him the "mad dog of the Middle East". According to critics, the Libyan people lived in a climate of fear under Gaddafi's administration, due to his government's pervasive surveillance of civilians. Gaddafi's Libya was typically described by western commentators as "a police state". Opponents were critical of Libya's human rights abuses; according to Human Rights Watch (HRW) and others, hundreds of arrested political opponents often failed to receive a fair trial, and were sometimes subjected to torture or extrajudicial execution, most notably in the Abu Salim prison, including an alleged massacre on 29 June 1996 in which HRW estimated that 1,270 prisoners were massacred. Dissidents abroad or "stray dogs" were also publicly threatened with death and sometimes killed by government hit squads. His government's treatment of non-Arab Libyans has also came in for criticism from human rights activists, with native Berbers, Italians, Jews, refugees, and foreign workers all facing persecution in Gaddafist Libya. According to journalist Annick Cojean and psychologist Seham Sergewa, Gaddafi and senior officials raped and imprisoned hundreds or thousands of young women and reportedly raped several of his female bodyguards. Gaddafi's government was frequently criticized for not being democratic, with Freedom House consistently giving Libya under Gaddafi the "Not Free" ranking for civil liberties and political rights.
International reactions to Gaddafi's death were divided. U.S. President Barack Obama stated that it meant that "the shadow of tyranny over Libya has been lifted," while UK Prime Minister David Cameron stated that he was "proud" of his country's role in overthrowing "this brutal dictator". Contrastingly, former Cuban President Fidel Castro commented that in defying the rebels, Gaddafi would "enter history as one of the great figures of the Arab nations", while Venezuelan President Hugo Chávez described him as "a great fighter, a revolutionary and a martyr." Nelson Mandela expressed sadness at the news, praising Gaddafi for his anti-apartheid stance, remarking that he backed the African National Congress during "the darkest moments of our struggle". Gaddafi was mourned by many as a hero across Sub-Saharan Africa, for instance, a vigil was held by Muslims in Sierra Leone. The Daily Times of Nigeria stated that while undeniably a dictator, Gaddafi was the most benevolent in a region that only knew dictatorship, and that he was "a great man that looked out for his people and made them the envy of all of Africa." The Nigerian newspaper Leadership reported that while many Libyans and Africans would mourn Gaddafi, this would be ignored by western media and that as such it would take 50 years before historians decided whether he was "martyr or villain."
Following his defeat in the civil war, Gaddafi's system of governance was dismantled and replaced under the interim government of the NTC, who legalised trade unions and freedom of the press. In July 2012, elections were held to form a new General National Congress (GNC), who officially took over governance from the NTC in August. The GNC proceeded to elect Mohammed Magariaf as president of the chamber, and then voted Mustafa A.G. Abushagur as Prime Minister; when Abushagar failed to gain congressional approval, the GNC instead elected Ali Zeidan to the position. In January 2013, the GNC officially renamed the Jamahiriyah as the "State of Libya".
Tibet (i/tᵻˈbɛt/; Wylie: Bod, pronounced [pʰø̀ʔ]; Chinese: 西藏; pinyin: Xīzàng) is a region on the Tibetan Plateau in Asia. It is the traditional homeland of the Tibetan people as well as some other ethnic groups such as Monpa, Qiang and Lhoba peoples and is now also inhabited by considerable numbers of Han Chinese and Hui people. Tibet is the highest region on Earth, with an average elevation of 4,900 metres (16,000 ft). The highest elevation in Tibet is Mount Everest, earth's highest mountain rising 8,848 m (29,029 ft) above sea level.
The Tibetan Empire emerged in the 7th century, but with the fall of the empire the region soon divided into a variety of territories. The bulk of western and central Tibet (Ü-Tsang) was often at least nominally unified under a series of Tibetan governments in Lhasa, Shigatse, or nearby locations; these governments were at various times under Mongol and Chinese overlordship. The eastern regions of Kham and Amdo often maintained a more decentralized indigenous political structure, being divided among a number of small principalities and tribal groups, while also often falling more directly under Chinese rule after the Battle of Chamdo; most of this area was eventually incorporated into the Chinese provinces of Sichuan and Qinghai. The current borders of Tibet were generally established in the 18th century.
Following the Xinhai Revolution against the Qing dynasty in 1912, Qing soldiers were disarmed and escorted out of Tibet Area (Ü-Tsang). The region subsequently declared its independence in 1913 without recognition by the subsequent Chinese Republican government. Later, Lhasa took control of the western part of Xikang, China. The region maintained its autonomy until 1951 when, following the Battle of Chamdo, Tibet became incorporated into the People's Republic of China, and the previous Tibetan government was abolished in 1959 after a failed uprising. Today, China governs western and central Tibet as the Tibet Autonomous Region while the eastern areas are now mostly ethnic autonomous prefectures within Sichuan, Qinghai and other neighbouring provinces. There are tensions regarding Tibet's political status and dissident groups that are active in exile. It is also said that Tibetan activists in Tibet have been arrested or tortured.
The economy of Tibet is dominated by subsistence agriculture, though tourism has become a growing industry in recent decades. The dominant religion in Tibet is Tibetan Buddhism; in addition there is Bön, which is similar to Tibetan Buddhism, and there are also Tibetan Muslims and Christian minorities. Tibetan Buddhism is a primary influence on the art, music, and festivals of the region. Tibetan architecture reflects Chinese and Indian influences. Staple foods in Tibet are roasted barley, yak meat, and butter tea.
The Tibetan name for their land, Bod བོད་, means "Tibet" or "Tibetan Plateau", although it originally meant the central region around Lhasa, now known in Tibetan as Ü. The Standard Tibetan pronunciation of Bod, [pʰøʔ˨˧˨], is transcribed Bhö in Tournadre Phonetic Transcription, Bö in the THL Simplified Phonetic Transcription and Poi in Tibetan pinyin. Some scholars believe the first written reference to Bod "Tibet" was the ancient Bautai people recorded in the Egyptian Greek works Periplus of the Erythraean Sea (1st century CE) and Geographia (Ptolemy, 2nd century CE), itself from the Sanskrit form Bhauṭṭa of the Indian geographical tradition.
The modern Standard Chinese exonym for the ethnic Tibetan region is Zangqu (Chinese: 藏区; pinyin: Zàngqū), which derives by metonymy from the Tsang region around Shigatse plus the addition of a Chinese suffix, 区 qū, which means "area, district, region, ward". Tibetan people, language, and culture, regardless of where they are from, are referred to as Zang (Chinese: 藏; pinyin: Zàng) although the geographical term Xīzàng is often limited to the Tibet Autonomous Region. The term Xīzàng was coined during the Qing dynasty in the reign of the Jiaqing Emperor (1796–1820) through the addition of a prefix meaning "west" (西 xī) to Zang.
The best-known medieval Chinese name for Tibet is Tubo (Chinese: 吐蕃 also written as 土蕃 or 土番; pinyin: Tǔbō or Tǔfān). This name first appears in Chinese characters as 土番 in the 7th century (Li Tai) and as 吐蕃 in the 10th-century (Old Book of Tang describing 608–609 emissaries from Tibetan King Namri Songtsen to Emperor Yang of Sui). In the Middle Chinese spoken during that period, as reconstructed by William H. Baxter, 土番 was pronounced thux-phjon and 吐蕃 was pronounced thux-pjon (with the x representing tone).
Other pre-modern Chinese names for Tibet include Wusiguo (Chinese: 烏斯國; pinyin: Wūsīguó; cf. Tibetan dbus, Ü, [wyʔ˨˧˨]), Wusizang (Chinese: 烏斯藏; pinyin: wūsīzàng, cf. Tibetan dbus-gtsang, Ü-Tsang), Tubote (Chinese: 圖伯特; pinyin: Túbótè), and Tanggute (Chinese: 唐古忒; pinyin: Tánggǔtè, cf. Tangut). American Tibetologist Elliot Sperling has argued in favor of a recent tendency by some authors writing in Chinese to revive the term Tubote (simplified Chinese: 图伯特; traditional Chinese: 圖伯特; pinyin: Túbótè) for modern use in place of Xizang, on the grounds that Tubote more clearly includes the entire Tibetan plateau rather than simply the Tibet Autonomous Region.[citation needed]
The language has numerous regional dialects which are generally not mutually intelligible. It is employed throughout the Tibetan plateau and Bhutan and is also spoken in parts of Nepal and northern India, such as Sikkim. In general, the dialects of central Tibet (including Lhasa), Kham, Amdo and some smaller nearby areas are considered Tibetan dialects. Other forms, particularly Dzongkha, Sikkimese, Sherpa, and Ladakhi, are considered by their speakers, largely for political reasons, to be separate languages. However, if the latter group of Tibetan-type languages are included in the calculation, then 'greater Tibetan' is spoken by approximately 6 million people across the Tibetan Plateau. Tibetan is also spoken by approximately 150,000 exile speakers who have fled from modern-day Tibet to India and other countries.
Although spoken Tibetan varies according to the region, the written language, based on Classical Tibetan, is consistent throughout. This is probably due to the long-standing influence of the Tibetan empire, whose rule embraced (and extended at times far beyond) the present Tibetan linguistic area, which runs from northern Pakistan in the west to Yunnan and Sichuan in the east, and from north of Qinghai Lake south as far as Bhutan. The Tibetan language has its own script which it shares with Ladakhi and Dzongkha, and which is derived from the ancient Indian Brāhmī script.
The earliest Tibetan historical texts identify the Zhang Zhung culture as a people who migrated from the Amdo region into what is now the region of Guge in western Tibet. Zhang Zhung is considered to be the original home of the Bön religion. By the 1st century BCE, a neighboring kingdom arose in the Yarlung valley, and the Yarlung king, Drigum Tsenpo, attempted to remove the influence of the Zhang Zhung by expelling the Zhang's Bön priests from Yarlung. He was assassinated and Zhang Zhung continued its dominance of the region until it was annexed by Songtsen Gampo in the 7th century. Prior to Songtsän Gampo, the kings of Tibet were more mythological than factual, and there is insufficient evidence of their existence.
The history of a unified Tibet begins with the rule of Songtsän Gampo (604–650 CE), who united parts of the Yarlung River Valley and founded the Tibetan Empire. He also brought in many reforms, and Tibetan power spread rapidly, creating a large and powerful empire. It is traditionally considered that his first wife was the Princess of Nepal, Bhrikuti, and that she played a great role in the establishment of Buddhism in Tibet. In 640 he married Princess Wencheng, the niece of the powerful Chinese emperor Taizong of Tang China.
In 821/822 CE Tibet and China signed a peace treaty. A bilingual account of this treaty, including details of the borders between the two countries, is inscribed on a stone pillar which stands outside the Jokhang temple in Lhasa. Tibet continued as a Central Asian empire until the mid-9th century, when a civil war over succession led to the collapse of imperial Tibet. The period that followed is known traditionally as the Era of Fragmentation, when political control over Tibet became divided between regional warlords and tribes with no dominant centralized authority.
The Mongol Yuan dynasty, through the Bureau of Buddhist and Tibetan Affairs, or Xuanzheng Yuan, ruled Tibet through a top-level administrative department. One of the department's purposes was to select a dpon-chen ('great administrator'), usually appointed by the lama and confirmed by the Mongol emperor in Beijing. The Sakya lama retained a degree of autonomy, acting as the political authority of the region, while the dpon-chen held administrative and military power. Mongol rule of Tibet remained separate from the main provinces of China, but the region existed under the administration of the Yuan dynasty. If the Sakya lama ever came into conflict with the dpon-chen, the dpon-chen had the authority to send Chinese troops into the region.
Tibet retained nominal power over religious and regional political affairs, while the Mongols managed a structural and administrative rule over the region, reinforced by the rare military intervention. This existed as a "diarchic structure" under the Yuan emperor, with power primarily in favor of the Mongols. Mongolian prince Khuden gained temporal power in Tibet in the 1240s and sponsored Sakya Pandita, whose seat became the capital of Tibet. Drogön Chögyal Phagpa, Sakya Pandita's nephew became Imperial Preceptor of Kublai Khan, founder of the Yuan dynasty.
Between 1346 and 1354, Tai Situ Changchub Gyaltsen toppled the Sakya and founded the Phagmodrupa Dynasty. The following 80 years saw the founding of the Gelug school (also known as Yellow Hats) by the disciples of Je Tsongkhapa, and the founding of the important Ganden, Drepung and Sera monasteries near Lhasa. However, internal strife within the dynasty and the strong localism of the various fiefs and political-religious factions led to a long series of internal conflicts. The minister family Rinpungpa, based in Tsang (West Central Tibet), dominated politics after 1435. In 1565 they were overthrown by the Tsangpa Dynasty of Shigatse which expanded its power in different directions of Tibet in the following decades and favoured the Karma Kagyu sect.
The 5th Dalai Lama is known for unifying the Tibetan heartland under the control of the Gelug school of Tibetan Buddhism, after defeating the rival Kagyu and Jonang sects and the secular ruler, the Tsangpa prince, in a prolonged civil war. His efforts were successful in part because of aid from Güshi Khan, the Oirat leader of the Khoshut Khanate. With Güshi Khan as a largely uninvolved overlord, the 5th Dalai Lama and his intimates established a civil administration which is referred to by historians as the Lhasa state. This Tibetan regime or government is also referred to as the Ganden Phodrang.
Qing dynasty rule in Tibet began with their 1720 expedition to the country when they expelled the invading Dzungars. Amdo came under Qing control in 1724, and eastern Kham was incorporated into neighbouring Chinese provinces in 1728. Meanwhile, the Qing government sent resident commissioners called Ambans to Lhasa. In 1750 the Ambans and the majority of the Han Chinese and Manchus living in Lhasa were killed in a riot, and Qing troops arrived quickly and suppressed the rebels in the next year. Like the preceding Yuan dynasty, the Manchus of the Qing dynasty exerted military and administrative control of the region, while granting it a degree of political autonomy. The Qing commander publicly executed a number of supporters of the rebels and, as in 1723 and 1728, made changes in the political structure and drew up a formal organization plan. The Qing now restored the Dalai Lama as ruler, leading the governing council called Kashag, but elevated the role of Ambans to include more direct involvement in Tibetan internal affairs. At the same time the Qing took steps to counterbalance the power of the aristocracy by adding officials recruited from the clergy to key posts.
For several decades, peace reigned in Tibet, but in 1792 the Qing Qianlong Emperor sent a large Chinese army into Tibet to push the invading Nepalese out. This prompted yet another Qing reorganization of the Tibetan government, this time through a written plan called the "Twenty-Nine Regulations for Better Government in Tibet". Qing military garrisons staffed with Qing troops were now also established near the Nepalese border. Tibet was dominated by the Manchus in various stages in the 18th century, and the years immediately following the 1792 regulations were the peak of the Qing imperial commissioners' authority; but there was no attempt to make Tibet a Chinese province.
This period also saw some contacts with Jesuits and Capuchins from Europe, and in 1774 a Scottish nobleman, George Bogle, came to Shigatse to investigate prospects of trade for the British East India Company. However, in the 19th century the situation of foreigners in Tibet grew more tenuous. The British Empire was encroaching from northern India into the Himalayas, the Emirate of Afghanistan and the Russian Empire were expanding into Central Asia and each power became suspicious of the others' intentions in Tibet.
In 1904, a British expedition to Tibet, spurred in part by a fear that Russia was extending its power into Tibet as part of The Great Game, invaded the country, hoping that negotiations with the 13th Dalai Lama would be more effective than with Chinese representatives. When the British-led invasion reached Tibet on December 12, 1903, an armed confrontation with the ethnic Tibetans resulted in the Massacre of Chumik Shenko, which resulted in 600 fatalities amongst the Tibetan forces, compared to only 12 on the British side. Afterwards, in 1904 Francis Younghusband imposed a treaty known as the Treaty of Lhasa, which was subsequently repudiated and was succeeded by a 1906 treaty signed between Britain and China.
After the Xinhai Revolution (1911–12) toppled the Qing dynasty and the last Qing troops were escorted out of Tibet, the new Republic of China apologized for the actions of the Qing and offered to restore the Dalai Lama's title. The Dalai Lama refused any Chinese title and declared himself ruler of an independent Tibet. In 1913, Tibet and Mongolia concluded a treaty of mutual recognition. For the next 36 years, the 13th Dalai Lama and the regents who succeeded him governed Tibet. During this time, Tibet fought Chinese warlords for control of the ethnically Tibetan areas in Xikang and Qinghai (parts of Kham and Amdo) along the upper reaches of the Yangtze River. In 1914 the Tibetan government signed the Simla Accord with Britain, ceding the South Tibet region to British India. The Chinese government denounced the agreement as illegal.
After the Dalai Lama's government fled to Dharamsala, India, during the 1959 Tibetan Rebellion, it established a rival government-in-exile. Afterwards, the Central People's Government in Beijing renounced the agreement and began implementation of the halted social and political reforms. During the Great Leap Forward, between 200,000 and 1,000,000 Tibetans died, and approximately 6,000 monasteries were destroyed during the Cultural Revolution. In 1962 China and India fought a brief war over the disputed South Tibet and Aksai Chin regions. Although China won the war, Chinese troops withdrew north of the McMahon Line, effectively ceding South Tibet to India.
In 1980, General Secretary and reformist Hu Yaobang visited Tibet and ushered in a period of social, political, and economic liberalization. At the end of the decade, however, analogously to the Tiananmen Square protests of 1989, monks in the Drepung and Sera monasteries started protesting for independence, and so the government halted reforms and started an anti-separatist campaign. Human rights organisations have been critical of the Beijing and Lhasa governments' approach to human rights in the region when cracking down on separatist convulsions that have occurred around monasteries and cities, most recently in the 2008 Tibetan unrest.
Tibet has some of the world's tallest mountains, with several of them making the top ten list. Mount Everest, located on the border with Nepal, is, at 8,848 metres (29,029 ft), the highest mountain on earth. Several major rivers have their source in the Tibetan Plateau (mostly in present-day Qinghai Province). These include the Yangtze, Yellow River, Indus River, Mekong, Ganges, Salween and the Yarlung Tsangpo River (Brahmaputra River). The Yarlung Tsangpo Grand Canyon, along the Yarlung Tsangpo River, is among the deepest and longest canyons in the world.
The Indus and Brahmaputra rivers originate from a lake (Tib: Tso Mapham) in Western Tibet, near Mount Kailash. The mountain is a holy pilgrimage site for both Hindus and Tibetans. The Hindus consider the mountain to be the abode of Lord Shiva. The Tibetan name for Mt. Kailash is Khang Rinpoche. Tibet has numerous high-altitude lakes referred to in Tibetan as tso or co. These include Qinghai Lake, Lake Manasarovar, Namtso, Pangong Tso, Yamdrok Lake, Siling Co, Lhamo La-tso, Lumajangdong Co, Lake Puma Yumco, Lake Paiku, Lake Rakshastal, Dagze Co and Dong Co. The Qinghai Lake (Koko Nor) is the largest lake in the People's Republic of China.
The atmosphere is severely dry nine months of the year, and average annual snowfall is only 18 inches (46 cm), due to the rain shadow effect. Western passes receive small amounts of fresh snow each year but remain traversible all year round. Low temperatures are prevalent throughout these western regions, where bleak desolation is unrelieved by any vegetation bigger than a low bush, and where wind sweeps unchecked across vast expanses of arid plain. The Indian monsoon exerts some influence on eastern Tibet. Northern Tibet is subject to high temperatures in the summer and intense cold in the winter.
The main crops grown are barley, wheat, buckwheat, rye, potatoes, and assorted fruits and vegetables. Tibet is ranked the lowest among China’s 31 provinces on the Human Development Index according to UN Development Programme data. In recent years, due to increased interest in Tibetan Buddhism, tourism has become an increasingly important sector, and is actively promoted by the authorities. Tourism brings in the most income from the sale of handicrafts. These include Tibetan hats, jewelry (silver and gold), wooden items, clothing, quilts, fabrics, Tibetan rugs and carpets. The Central People's Government exempts Tibet from all taxation and provides 90% of Tibet's government expenditures. However most of this investment goes to pay migrant workers who do not settle in Tibet and send much of their income home to other provinces.
From January 18–20, 2010 a national conference on Tibet and areas inhabited by Tibetans in Sichuan, Yunnan, Gansu and Qinghai was held in China and a substantial plan to improve development of the areas was announced. The conference was attended by General secretary Hu Jintao, Wu Bangguo, Wen Jiabao, Jia Qinglin, Li Changchun, Xi Jinping, Li Keqiang, He Guoqiang and Zhou Yongkang, all members of CPC Politburo Standing Committee signaling the commitment of senior Chinese leaders to development of Tibet and ethnic Tibetan areas. The plan calls for improvement of rural Tibetan income to national standards by 2020 and free education for all rural Tibetan children. China has invested 310 billion yuan (about 45.6 billion U.S. dollars) in Tibet since 2001. "Tibet's GDP was expected to reach 43.7 billion yuan in 2009, up 170 percent from that in 2000 and posting an annual growth of 12.3 percent over the past nine years."
Historically, the population of Tibet consisted of primarily ethnic Tibetans and some other ethnic groups. According to tradition the original ancestors of the Tibetan people, as represented by the six red bands in the Tibetan flag, are: the Se, Mu, Dong, Tong, Dru and Ra. Other traditional ethnic groups with significant population or with the majority of the ethnic group residing in Tibet (excluding a disputed area with India) include Bai people, Blang, Bonan, Dongxiang, Han, Hui people, Lhoba, Lisu people, Miao, Mongols, Monguor (Tu people), Menba (Monpa), Mosuo, Nakhi, Qiang, Nu people, Pumi, Salar, and Yi people.
Religion is extremely important to the Tibetans and has a strong influence over all aspects of their lives. Bön is the ancient religion of Tibet, but has been almost eclipsed by Tibetan Buddhism, a distinctive form of Mahayana and Vajrayana, which was introduced into Tibet from the Sanskrit Buddhist tradition of northern India. Tibetan Buddhism is practiced not only in Tibet but also in Mongolia, parts of northern India, the Buryat Republic, the Tuva Republic, and in the Republic of Kalmykia and some other parts of China. During China's Cultural Revolution, nearly all Tibet's monasteries were ransacked and destroyed by the Red Guards. A few monasteries have begun to rebuild since the 1980s (with limited support from the Chinese government) and greater religious freedom has been granted – although it is still limited. Monks returned to monasteries across Tibet and monastic education resumed even though the number of monks imposed is strictly limited. Before the 1950s, between 10 and 20% of males in Tibet were monks.
Muslims have been living in Tibet since as early as the 8th or 9th century. In Tibetan cities, there are small communities of Muslims, known as Kachee (Kache), who trace their origin to immigrants from three main regions: Kashmir (Kachee Yul in ancient Tibetan), Ladakh and the Central Asian Turkic countries. Islamic influence in Tibet also came from Persia. After 1959 a group of Tibetan Muslims made a case for Indian nationality based on their historic roots to Kashmir and the Indian government declared all Tibetan Muslims Indian citizens later on that year. Other Muslim ethnic groups who have long inhabited Tibet include Hui, Salar, Dongxiang and Bonan. There is also a well established Chinese Muslim community (gya kachee), which traces its ancestry back to the Hui ethnic group of China.
Roman Catholic Jesuits and Capuchins arrived from Europe in the 17th and 18th centuries. Portuguese missionaries Jesuit Father António de Andrade and Brother Manuel Marques first reached the kingdom of Gelu in western Tibet in 1624 and was welcomed by the royal family who allowed them to build a church later on. By 1627, there were about a hundred local converts in the Guge kingdom. Later on, Christianity was introduced to Rudok, Ladakh and Tsang and was welcomed by the ruler of the Tsang kingdom, where Andrade and his fellows established a Jesuit outpost at Shigatse in 1626.
In 1661 another Jesuit, Johann Grueber, crossed Tibet from Sining to Lhasa (where he spent a month), before heading on to Nepal. He was followed by others who actually built a church in Lhasa. These included the Jesuit Father Ippolito Desideri, 1716–1721, who gained a deep knowledge of Tibetan culture, language and Buddhism, and various Capuchins in 1707–1711, 1716–1733 and 1741–1745, Christianity was used by some Tibetan monarchs and their courts and the Karmapa sect lamas to counterbalance the influence of the Gelugpa sect in the 17th century until in 1745 when all the missionaries were expelled at the lama's insistence.
In 1877, the Protestant James Cameron from the China Inland Mission walked from Chongqing to Batang in Garzê Tibetan Autonomous Prefecture, Sichuan province, and "brought the Gospel to the Tibetan people." Beginning in the 20th century, in Diqing Tibetan Autonomous Prefecture in Yunnan, a large number of Lisu people and some Yi and Nu people converted to Christianity. Famous earlier missionaries include James O. Fraser, Alfred James Broomhall and Isobel Kuhn of the China Inland Mission, among others who were active in this area.
Standing at 117 metres (384 feet) in height and 360 metres (1,180 feet) in width, the Potala Palace is the most important example of Tibetan architecture. Formerly the residence of the Dalai Lama, it contains over one thousand rooms within thirteen stories, and houses portraits of the past Dalai Lamas and statues of the Buddha. It is divided between the outer White Palace, which serves as the administrative quarters, and the inner Red Quarters, which houses the assembly hall of the Lamas, chapels, 10,000 shrines, and a vast library of Buddhist scriptures. The Potala Palace is a World Heritage Site, as is Norbulingka, the former summer residence of the Dalai Lama.
Tibetan music often involves chanting in Tibetan or Sanskrit, as an integral part of the religion. These chants are complex, often recitations of sacred texts or in celebration of various festivals. Yang chanting, performed without metrical timing, is accompanied by resonant drums and low, sustained syllables. Other styles include those unique to the various schools of Tibetan Buddhism, such as the classical music of the popular Gelugpa school, and the romantic music of the Nyingmapa, Sakyapa and Kagyupa schools.
Tibet has various festivals that are commonly performed to worship the Buddha[citation needed] throughout the year. Losar is the Tibetan New Year Festival. Preparations for the festive event are manifested by special offerings to family shrine deities, painted doors with religious symbols, and other painstaking jobs done to prepare for the event. Tibetans eat Guthuk (barley noodle soup with filling) on New Year's Eve with their families. The Monlam Prayer Festival follows it in the first month of the Tibetan calendar, falling between the fourth and the eleventh days of the first Tibetan month. It involves dancing and participating in sports events, as well as sharing picnics. The event was established in 1049 by Tsong Khapa, the founder of the Dalai Lama and the Panchen Lama's order.
The most important crop in Tibet is barley, and dough made from barley flour—called tsampa—is the staple food of Tibet. This is either rolled into noodles or made into steamed dumplings called momos. Meat dishes are likely to be yak, goat, or mutton, often dried, or cooked into a spicy stew with potatoes. Mustard seed is cultivated in Tibet, and therefore features heavily in its cuisine. Yak yogurt, butter and cheese are frequently eaten, and well-prepared yogurt is considered something of a prestige item. Butter tea is very popular to drink.
According to the apocryphal Gospel of James, Mary was the daughter of Saint Joachim and Saint Anne. Before Mary's conception, Anne had been barren and was far advanced in years. Mary was given to service as a consecrated virgin in the Temple in Jerusalem when she was three years old, much like Hannah took Samuel to the Tabernacle as recorded in the Old Testament. Some apocryphal accounts state that at the time of her betrothal to Joseph, Mary was 12–14 years old, and he was thirty years old, but such accounts are unreliable.
The Gospel of Luke begins its account of Mary's life with the Annunciation, when the angel Gabriel appeared to her and announced her divine selection to be the mother of Jesus. According to gospel accounts, Mary was present at the Crucifixion of Jesus and is depicted as a member of the early Christian community in Jerusalem. According to Apocryphal writings, at some time soon after her death, her incorrupt body was assumed directly into Heaven, to be reunited with her soul, and the apostles thereupon found the tomb empty; this is known in Christian teaching as the Assumption.
The doctrines of the Assumption or Dormition of Mary relate to her death and bodily assumption to Heaven. The Roman Catholic Church has dogmaically defined the doctrine of the Assumption, which was done in 1950 by Pope Pius XII in Munificentissimus Deus. Whether the Virgin Mary died or not is not defined dogmatically, however, although a reference to the death of Mary are made in Munificentissimus Deus. In the Eastern Orthodox Church, the Assumption of the Virgin Mary is believed, and celebrated with her Dormition, where they believe she died.
After Mary continued in the "blood of her purifying" another 33 days for a total of 40 days, she brought her burnt offering and sin offering to the Temple in Jerusalem,[Luke 2:22] so the priest could make atonement for her sins, being cleansed from her blood.[Leviticus 12:1-8] They also presented Jesus –  "As it is written in the law of the Lord, Every male that openeth the womb shall be called holy to the Lord" (Luke 2:23other verses). After the prophecies of Simeon and the prophetess Anna in Luke 2:25-38 concluded, Joseph and Mary took Jesus and "returned into Galilee, to their own city Nazareth".[Luke 2:39]
According to the writer of Luke, Mary was a relative of Elizabeth, wife of the priest Zechariah of the priestly division of Abijah, who was herself part of the lineage of Aaron and so of the tribe of Levi.[Luke 1:5;1:36] Some of those who consider that the relationship with Elizabeth was on the maternal side, consider that Mary, like Joseph, to whom she was betrothed, was of the House of David and so of the Tribe of Judah, and that the genealogy of Jesus presented in Luke 3 from Nathan, third son of David and Bathsheba, is in fact the genealogy of Mary,[need quotation to verify] while the genealogy from Solomon given in Matthew 1 is that of Joseph. (Aaron's wife Elisheba was of the tribe of Judah, so all their descendants are from both Levi and Judah.)[Num.1:7 & Ex.6:23]
Mary is also depicted as being present among the women at the crucifixion during the crucifixion standing near "the disciple whom Jesus loved" along with Mary of Clopas and Mary Magdalene,[Jn 19:25-26] to which list Matthew 27:56 adds "the mother of the sons of Zebedee", presumably the Salome mentioned in Mark 15:40. This representation is called a Stabat Mater. While not recorded in the Gospel accounts, Mary cradling the dead body of her son is a common motif in art, called a "pietà" or "pity".
The adoption of the mother of Jesus as a virtual goddess may represent a reintroduction of aspects of the worship of Isis. "When looking at images of the Egyptian goddess Isis and those of the Virgin Mary, one may initially observe iconographic similarities. These parallels have led many scholars to suggest that there is a distinct iconographic relationship between Isis and Mary. In fact, some scholars have gone even further, and have suggested, on the basis of this relationship, a direct link between the cult of Mary and that of Isis." 
In the 19th century, a house near Ephesus in Turkey was found, based on the visions of Anne Catherine Emmerich, an Augustinian nun in Germany. It has since been visited as the House of the Virgin Mary by Roman Catholic pilgrims who consider it the place where Mary lived until her assumption. The Gospel of John states that Mary went to live with the Disciple whom Jesus loved,[Jn 19:27] identified as John the Evangelist.[citation needed] Irenaeus and Eusebius of Caesarea wrote in their histories that John later went to Ephesus, which may provide the basis for the early belief that Mary also lived in Ephesus with John.
Devotions to artistic depictions of Mary vary among Christian traditions. There is a long tradition of Roman Catholic Marian art and no image permeates Catholic art as does the image of Madonna and Child. The icon of the Virgin Theotokos with Christ is without doubt the most venerated icon in the Orthodox Church. Both Roman Catholic and Orthodox Christians venerate images and icons of Mary, given that the Second Council of Nicaea in 787 permitted their veneration with the understanding that those who venerate the image are venerating the reality of the person it represents, and the 842 Synod of Constantinople confirming the same. According to Orthodox piety and traditional practice, however, believers ought to pray before and venerate only flat, two-dimensional icons, and not three-dimensional statues.
Ephesus is a cultic centre of Mary, the site of the first Church dedicated to her and the rumoured place of her death. Ephesus was previously a centre for worship of Artemis a virgin goddess. The Temple of Artemis at Ephesus being regarded as one of the Seven Wonders of the Ancient World The cult of Mary was furthered by Queen Theodora in the 6th Century. According to William E. Phipps, in the book Survivals of Roman Religion "Gordon Laing argues convincingly that the worship of Artemis as both virgin and mother at the grand Ephesian temple contributed to the veneration of Mary."
Some titles have a Biblical basis, for instance the title Queen Mother has been given to Mary since she was the mother of Jesus, who was sometimes referred to as the "King of Kings" due to his lineage of King David. The biblical basis for the term Queen can be seen in the Gospel of Luke 1:32 and the Book of Isaiah 9:6, and Queen Mother from 1 Kings 2:19-20 and Jeremiah 13:18-19. Other titles have arisen from reported miracles, special appeals or occasions for calling on Mary, e.g., Our Lady of Good Counsel, Our Lady of Navigators or Our Lady of Ransom who protects captives.
Despite Martin Luther's harsh polemics against his Roman Catholic opponents over issues concerning Mary and the saints, theologians appear to agree that Luther adhered to the Marian decrees of the ecumenical councils and dogmas of the church. He held fast to the belief that Mary was a perpetual virgin and the Theotokos or Mother of God. Special attention is given to the assertion that Luther, some three-hundred years before the dogmatization of the Immaculate Conception by Pope Pius IX in 1854, was a firm adherent of that view. Others maintain that Luther in later years changed his position on the Immaculate Conception, which, at that time was undefined in the Church, maintaining however the sinlessness of Mary throughout her life. For Luther, early in his life, the Assumption of Mary was an understood fact, although he later stated that the Bible did not say anything about it and stopped celebrating its feast. Important to him was the belief that Mary and the saints do live on after death. "Throughout his career as a priest-professor-reformer, Luther preached, taught, and argued about the veneration of Mary with a verbosity that ranged from childlike piety to sophisticated polemics. His views are intimately linked to his Christocentric theology and its consequences for liturgy and piety." Luther, while revering Mary, came to criticize the "Papists" for blurring the line, between high admiration of the grace of God wherever it is seen in a human being, and religious service given to another creature. He considered the Roman Catholic practice of celebrating saints' days and making intercessory requests addressed especially to Mary and other departed saints to be idolatry. His final thoughts on Marian devotion and veneration are preserved in a sermon preached at Wittenberg only a month before his death:
Differences in feasts may also originate from doctrinal issues—the Feast of the Assumption is such an example. Given that there is no agreement among all Christians on the circumstances of the death, Dormition or Assumption of Mary, the feast of assumption is celebrated among some denominations and not others.  While the Catholic Church celebrates the Feast of the Assumption on August 15, some Eastern Catholics celebrate it as Dormition of the Theotokos, and may do so on August 28, if they follow the Julian calendar. The Eastern Orthodox also celebrate it as the Dormition of the Theotokos, one of their 12 Great Feasts. Protestants do not celebrate this, or any other Marian feasts.
In paintings, Mary is traditionally portrayed in blue. This tradition can trace its origin to the Byzantine Empire, from c.500 AD, where blue was "the colour of an empress". A more practical explanation for the use of this colour is that in Medieval and Renaissance Europe, the blue pigment was derived from the rock lapis lazuli, a stone imported from Afghanistan of greater value than gold. Beyond a painter's retainer, patrons were expected to purchase any gold or lapis lazuli to be used in the painting. Hence, it was an expression of devotion and glorification to swathe the Virgin in gowns of blue.
Nontrinitarians, such as Unitarians, Christadelphians and Jehovah's Witnesses also acknowledge Mary as the biological mother of Jesus Christ, but do not recognise Marian titles such as "Mother of God" as these groups generally reject Christ's divinity. Since Nontrinitarian churches are typically also mortalist, the issue of praying to Mary, whom they would consider "asleep", awaiting resurrection, does not arise. Emanuel Swedenborg says God as he is in himself could not directly approach evil spirits to redeem those spirits without destroying them (Exodus 33:20, John 1:18), so God impregnated Mary, who gave Jesus Christ access to the evil heredity of the human race, which he could approach, redeem and save.
The Qur'an relates detailed narrative accounts of Maryam (Mary) in two places, Qur'an 3:35–47 and 19:16–34. These state beliefs in both the Immaculate Conception of Mary and the Virgin birth of Jesus. The account given in Sura 19 is nearly identical with that in the Gospel according to Luke, and both of these (Luke, Sura 19) begin with an account of the visitation of an angel upon Zakariya (Zecharias) and Good News of the birth of Yahya (John), followed by the account of the annunciation. It mentions how Mary was informed by an angel that she would become the mother of Jesus through the actions of God alone.
The Perpetual Virginity of Mary asserts Mary's real and perpetual virginity even in the act of giving birth to the Son of God made Man. The term Ever-Virgin (Greek ἀειπάρθενος) is applied in this case, stating that Mary remained a virgin for the remainder of her life, making Jesus her biological and only son, whose conception and birth are held to be miraculous. While the Orthodox Churches hold the position articulated in the Protoevangelium of James that Jesus' brothers and sisters are older children of Joseph the Betrothed, step-siblings from an earlier marriage that left him widowed, Roman Catholic teaching follows the Latin father Jerome in considering them Jesus' cousins.
Orthodox Christianity includes a large number of traditions regarding the Ever Virgin Mary, the Theotokos. The Orthodox believe that she was and remained a virgin before and after Christ's birth. The Theotokia (i.e., hymns to the Theotokos) are an essential part of the Divine Services in the Eastern Church and their positioning within the liturgical sequence effectively places the Theotokos in the most prominent place after Christ. Within the Orthodox tradition, the order of the saints begins with: The Theotokos, Angels, Prophets, Apostles, Fathers, Martyrs, etc. giving the Virgin Mary precedence over the angels. She is also proclaimed as the "Lady of the Angels".
The multiple churches that form the Anglican Communion and the Continuing Anglican movement have different views on Marian doctrines and venerative practices given that there is no single church with universal authority within the Communion and that the mother church (the Church of England) understands itself to be both "catholic" and "Reformed". Thus unlike the Protestant churches at large, the Anglican Communion (which includes the Episcopal Church in the United States) includes segments which still retain some veneration of Mary.
Although Calvin and Huldrych Zwingli honored Mary as the Mother of God in the 16th century, they did so less than Martin Luther. Thus the idea of respect and high honor for Mary was not rejected by the first Protestants; but, they came to criticize the Roman Catholics for venerating Mary. Following the Council of Trent in the 16th century, as Marian veneration became associated with Catholics, Protestant interest in Mary decreased. During the Age of the Enlightenment any residual interest in Mary within Protestant churches almost disappeared, although Anglicans and Lutherans continued to honor her.
 In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary "continued a pure and unspotted virgin", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.
She is the only woman directly named in the Qur'an; declared (uniquely along with Jesus) to be a Sign of God to humanity; as one who "guarded her chastity"; an obedient one; chosen of her mother and dedicated to Allah whilst still in the womb; uniquely (amongst women) Accepted into service by God; cared for by (one of the prophets as per Islam) Zakariya (Zacharias); that in her childhood she resided in the Temple and uniquely had access to Al-Mihrab (understood to be the Holy of Holies), and was provided with heavenly "provisions" by God.
From the early stages of Christianity, belief in the virginity of Mary and the virgin conception of Jesus, as stated in the gospels, holy and supernatural, was used by detractors, both political and religious, as a topic for discussions, debates and writings, specifically aimed to challenge the divinity of Jesus and thus Christians and Christianity alike. In the 2nd century, as part of the earliest anti-Christian polemics, Celsus suggested that Jesus was the illegitimate son of a Roman soldier named Panthera. The views of Celsus drew responses from Origen, the Church Father in Alexandria, Egypt, who considered it a fabricated story. How far Celsus sourced his view from Jewish sources remains a subject of discussion.
Mary had been venerated since Early Christianity, and is considered by millions to be the most meritorious saint of the religion. The Eastern and Oriental Orthodox, Roman Catholic, Anglican, and Lutheran Churches believe that Mary, as Mother of Jesus, is the Mother of God and the Theotokos, literally "Giver of birth to God". There is significant diversity in the Marian beliefs and devotional practices of major Christian traditions. The Roman Catholic Church holds distinctive Marian dogmas; namely her status as the mother of God; her Immaculate Conception; her perpetual virginity; and her Assumption into heaven. Many Protestants minimize Mary's role within Christianity, based on the argued brevity of biblical references. Mary (Maryam) also has a revered position in Islam, where a whole chapter of the Qur'an is devoted to her, also describing the birth of Jesus.
Mary resided in "her own house"[Lk.1:56] in Nazareth in Galilee, possibly with her parents, and during her betrothal — the first stage of a Jewish marriage — the angel Gabriel announced to her that she was to be the mother of the promised Messiah by conceiving him through the Holy Spirit, and she responded, "I am the handmaid of the Lord. Let it be done unto me according to your word." After a number of months, when Joseph was told of her conception in a dream by "an angel of the Lord", he planned to divorce her; but the angel told him to not hesitate to take her as his wife, which Joseph did, thereby formally completing the wedding rites.[Mt 1:18-25]
The Virgin birth of Jesus was an almost universally held belief among Christians from the 2nd until the 19th century. It is included in the two most widely used Christian creeds, which state that Jesus "was incarnate of the Holy Spirit and the Virgin Mary" (the Nicene Creed in what is now its familiar form) and the Apostles' Creed. The Gospel of Matthew describes Mary as a virgin who fulfilled the prophecy of Isaiah 7:14, mistranslating the Hebrew word alma ("young woman") in Isaiah 7:14 as "virgin", though.[citation needed] The authors of the Gospels of Matthew and Luke consider Jesus' conception not the result of intercourse and assert that Mary had "no relations with man" before Jesus' birth.[Mt 1:18] [Mt 1:25] [Lk 1:34] This alludes to the belief that Mary conceived Jesus through the action of God the Holy Spirit, and not through intercourse with Joseph or anyone else.

In the Catholic Church, Mary is accorded the title "Blessed", (from Latin beatus, blessed, via Greek μακάριος, makarios and Latin facere, make) in recognition of her assumption to Heaven and her capacity to intercede on behalf of those who pray to her. Catholic teachings make clear that Mary is not considered divine and prayers to her are not answered by her, they are answered by God. The four Catholic dogmas regarding Mary are: Mother of God, Perpetual virginity of Mary, Immaculate Conception (of Mary) and Assumption of Mary.
The views of the Church Fathers still play an important role in the shaping of Orthodox Marian perspective. However, the Orthodox views on Mary are mostly doxological, rather than academic: they are expressed in hymns, praise, liturgical poetry and the veneration of icons. One of the most loved Orthodox Akathists (i.e. standing hymns) is devoted to Mary and it is often simply called the Akathist Hymn. Five of the twelve Great Feasts in Orthodoxy are dedicated to Mary. The Sunday of Orthodoxy directly links the Virgin Mary's identity as Mother of God with icon veneration. A number of Orthodox feasts are connected with the miraculous icons of the Theotokos.
Mary's special position within God's purpose of salvation as "God-bearer" (Theotokos) is recognised in a number of ways by some Anglican Christians. All the member churches of the Anglican Communion affirm in the historic creeds that Jesus was born of the Virgin Mary, and celebrates the feast days of the Presentation of Christ in the Temple. This feast is called in older prayer books the Purification of the Blessed Virgin Mary on February 2. The Annunciation of our Lord to the Blessed Virgin on March 25 was from before the time of Bede until the 18th century New Year's Day in England. The Annunciation is called the "Annunciation of our Lady" in the 1662 Book of Common Prayer. Anglicans also celebrate in the Visitation of the Blessed Virgin on 31 May, though in some provinces the traditional date of July 2 is kept. The feast of the St. Mary the Virgin is observed on the traditional day of the Assumption, August 15. The Nativity of the Blessed Virgin is kept on September 8.
Protestants in general reject the veneration and invocation of the Saints.:1174 Protestants typically hold that Mary was the mother of Jesus, but was an ordinary woman devoted to God. Therefore, there is virtually no Marian veneration, Marian feasts, Marian pilgrimages, Marian art, Marian music or Marian spirituality in today's Protestant communities. Within these views, Roman Catholic beliefs and practices are at times rejected, e.g., theologian Karl Barth wrote that "the heresy of the Catholic Church is its Mariology".
The statement that Joseph "knew her not till she brought forth her first born son" (Matthew 1:25 DouayRheims) has been debated among scholars, with some saying that she did not remain a virgin and some saying that she was a perpetual virgin. Other scholars contend that the Greek word heos (i.e., until) denotes a state up to a point, but does not mean that the state ended after that point, and that Matthew 1:25 does not confirm or deny the virginity of Mary after the birth of Jesus. According to Biblical scholar Bart Ehrman the Hebrew word almah, meaning young woman of childbearing age, was translated into Greek as parthenos, which only means virgin, in Isaiah 7:14, which is commonly believed by Christians to be the prophecy of the Virgin Mary referred to in Matthew 1:23. While Matthew and Luke give differing versions of the virgin birth, John quotes the uninitiated Philip and the disbelieving Jews gathered at Galilee referring to Joseph as Jesus's father.
The hagiography of Mary and the Holy Family can be contrasted with other material in the Gospels. These references include an incident which can be interpreted as Jesus rejecting his family in the New Testament: "And his mother and his brothers arrived, and standing outside, they sent in a message asking for him ... And looking at those who sat in a circle around him, Jesus said, 'These are my mother and my brothers. Whoever does the will of God is my brother, and sister, and mother'."[3:31-35] Other verses suggest a conflict between Jesus and his family, including an attempt to have Jesus restrained because "he is out of his mind", and the famous quote: "A prophet is not without honor except in his own town, among his relatives and in his own home." A leading Biblical scholar commented: "there are clear signs not only that Jesus's family rejected his message during his public ministry but that he in turn spurned them publicly".
Although the Catholics and the Orthodox may honor and venerate Mary, they do not view her as divine, nor do they worship her. Roman Catholics view Mary as subordinate to Christ, but uniquely so, in that she is seen as above all other creatures. Similarly Theologian Sergei Bulgakov wrote that the Orthodox view Mary as "superior to all created beings" and "ceaselessly pray for her intercession". However, she is not considered a "substitute for the One Mediator" who is Christ. "Let Mary be in honor, but let worship be given to the Lord", he wrote. Similarly, Catholics do not worship Mary as a divine being, but rather "hyper-venerate" her. In Roman Catholic theology, the term hyperdulia is reserved for Marian veneration, latria for the worship of God, and dulia for the veneration of other saints and angels. The definition of the three level hierarchy of latria, hyperdulia and dulia goes back to the Second Council of Nicaea in 787.
Mary is referred to by the Eastern Orthodox Church, Oriental Orthodoxy, the Anglican Church, and all Eastern Catholic Churches as Theotokos, a title recognized at the Third Ecumenical Council (held at Ephesus to address the teachings of Nestorius, in 431). Theotokos (and its Latin equivalents, "Deipara" and "Dei genetrix") literally means "Godbearer". The equivalent phrase "Mater Dei" (Mother of God) is more common in Latin and so also in the other languages used in the Western Catholic Church, but this same phrase in Greek (Μήτηρ Θεοῦ), in the abbreviated form of the first and last letter of the two words (ΜΡ ΘΥ), is the indication attached to her image in Byzantine icons. The Council stated that the Church Fathers "did not hesitate to speak of the holy Virgin as the Mother of God".
Roman Catholics believe in the Immaculate Conception of Mary, as proclaimed Ex Cathedra by Pope Pius IX in 1854, namely that she was filled with grace from the very moment of her conception in her mother's womb and preserved from the stain of original sin. The Latin Rite of the Roman Catholic Church has a liturgical feast by that name, kept on December 8. Orthodox Christians reject the Immaculate Conception dogma principally because their understanding of ancestral sin (the Greek term corresponding to the Latin "original sin") differs from the Augustinian interpretation and that of the Roman Catholic Church.
The Protoevangelium of James, an extra-canonical book, has been the source of many Orthodox beliefs on Mary. The account of Mary's life presented includes her consecration as a virgin at the temple at age three. The High Priest Zachariah blessed Mary and informed her that God had magnified her name among many generations. Zachariah placed Mary on the third step of the altar, whereby God gave her grace. While in the temple, Mary was miraculously fed by an angel, until she was twelve years old. At that point an angel told Zachariah to betroth Mary to a widower in Israel, who would be indicated. This story provides the theme of many hymns for the Feast of Presentation of Mary, and icons of the feast depict the story. The Orthodox believe that Mary was instrumental in the growth of Christianity during the life of Jesus, and after his Crucifixion, and Orthodox Theologian Sergei Bulgakov wrote: "The Virgin Mary is the center, invisible, but real, of the Apostolic Church."
In the Islamic tradition, Mary and Jesus were the only children who could not be touched by Satan at the moment of their birth, for God imposed a veil between them and Satan. According to author Shabbir Akhtar, the Islamic perspective on Mary's Immaculate Conception is compatible with the Catholic doctrine of the same topic. "O People of the Book! Do not go beyond the bounds in your religion, and do not say anything of Allah but the truth. The Messiah, Jesus son of Mary, was but a Messenger of God, and a Word of His (Power) which He conveyed to Mary, and a spirit from Him. So believe in Allah (as the One, Unique God), and His Messengers (including Jesus, as Messenger); and do not say: (Allah is one of) a trinity. Give up (this assertion) â€" (it is) for your own good (to do so). Allah is but One Allah ; All-Glorified He is in that He is absolutely above having a son. To Him belongs whatever is in the heavens and whatever is on the earth. And Allah suffices as the One to be relied on, to Whom affairs should be referred." Quran 4/171
The issue of the parentage of Jesus in the Talmud affects also the view of his mother. However the Talmud does not mention Mary by name and is considerate rather than only polemic. The story about Panthera is also found in the Toledot Yeshu, the literary origins of which can not be traced with any certainty and given that it is unlikely to go before the 4th century, it is far too late to include authentic remembrances of Jesus. The Blackwell Companion to Jesus states that the Toledot Yeshu has no historical facts as such, and was perhaps created as a tool for warding off conversions to Christianity. The name Panthera may be a distortion of the term parthenos (virgin) and Raymond E. Brown considers the story of Panthera a fanciful explanation of the birth of Jesus which includes very little historical evidence. Robert Van Voorst states that given that Toledot Yeshu is a medieval document and due to its lack of a fixed form and orientation towards a popular audience, it is "most unlikely" to have reliable historical information.
Czech (/ˈtʃɛk/; čeština Czech pronunciation: [ˈt͡ʃɛʃcɪna]), formerly known as Bohemian (/boʊˈhiːmiən, bə-/; lingua Bohemica in Latin), is a West Slavic language strongly influenced by Latin and German language, spoken by over 10 million people and it is the official language of the Czech Republic. Czech's closest relative is Slovak, with which it is mutually intelligible. It is closely related to other West Slavic languages, such as Silesian and Polish. Although most Czech vocabulary is based on shared roots with Slavic, Romance, and Germanic languages, many loanwords (most associated with high culture) have been adopted in recent years.
The languages have not undergone the deliberate highlighting of minor linguistic differences in the name of nationalism as has occurred in the Bosnian, Serbian and Croatian standards of Serbo-Croatian. However, most Slavic languages (including Czech) have been distanced in this way from Russian influences because of widespread public resentment against the former Soviet Union (which occupied Czechoslovakia in 1968). Czech and Slovak form a dialect continuum, with great similarity between neighboring Czech and Slovak dialects. (See "Dialects" below.)
One study showed that Czech and Slovak lexicons differed by 80 percent, but this high percentage was found to stem primarily from differing orthographies and slight inconsistencies in morphological formation; Slovak morphology is more regular (when changing from the nominative to the locative case, Praha becomes Praze in Czech and Prahe in Slovak). The two lexicons are generally considered similar, with most differences in colloquial vocabulary and some scientific terminology. Slovak has slightly more borrowed words than Czech.
The similarities between Czech and Slovak led to the languages being considered a single language by a group of 19th-century scholars who called themselves "Czechoslavs" (Čechoslováci), believing that the peoples were connected in a way which excluded German Bohemians and (to a lesser extent) Hungarians and other Slavs. During the First Czechoslovak Republic (1918–1938), although "Czechoslovak" was designated as the republic's official language both Czech and Slovak written standards were used. Standard written Slovak was partially modeled on literary Czech, and Czech was preferred for some official functions in the Slovak half of the republic. Czech influence on Slovak was protested by Slovak scholars, and when Slovakia broke off from Czechoslovakia in 1938 as the Slovak State (which then aligned with Nazi Germany in World War II) literary Slovak was deliberately distanced from Czech. When the Axis powers lost the war and Czechoslovakia reformed, Slovak developed somewhat on its own (with Czech influence); during the Prague Spring of 1968, Slovak gained independence from (and equality with) Czech. Since then, "Czechoslovak" refers to improvised pidgins of the languages which have arisen from the decrease in mutual intelligibility.
Around the sixth century AD, a tribe of Slavs arrived in a portion of Central Europe. According to legend they were led by a hero named Čech, from whom the word "Czech" derives. The ninth century brought the state of Great Moravia, whose first ruler (Rastislav of Moravia) invited Byzantine ruler Michael III to send missionaries in an attempt to reduce the influence of East Francia on religious and political life in his country. These missionaries, Constantine and Methodius, helped to convert the Czechs from traditional Slavic paganism to Christianity and established a church system. They also brought the Glagolitic alphabet to the West Slavs, whose language was previously unwritten. This language, later known as Proto-Czech, was beginning to separate from its fellow West Slavic hatchlings Proto-Slovak, Proto-Polish and Proto-Sorbian. Among other features, Proto-Czech was marked by its ephemeral use of the voiced velar fricative consonant (/ɣ/) and consistent stress on the first syllable.
The Czechs' language separated from other Slavic tongues into what would later be called Old Czech by the thirteenth century, a classification extending through the sixteenth century. Its use of cases differed from the modern language; although Old Czech did not yet have a vocative case or an animacy distinction, declension for its six cases and three genders rapidly became complicated (partially to differentiate homophones) and its declension patterns resembled those of Lithuanian (its Balto-Slavic cousin).
While Old Czech had a basic alphabet from which a general set of orthographical correspondences was drawn, it did not have a standard orthography. It also contained a number of sound clusters which no longer exist; allowing ě (/jɛ/) after soft consonants, which has since shifted to e (/ɛ/), and allowing complex consonant clusters to be pronounced all at once rather than syllabically. A phonological phenomenon, Havlik's law (which began in Proto-Slavic and took various forms in other Slavic languages), appeared in Old Czech; counting backwards from the end of a clause, every odd-numbered yer was vocalized as a vowel, while the other yers disappeared.
Bohemia (as Czech civilization was known by then) increased in power over the centuries, as its language did in regional importance. This growth was expedited during the fourteenth century by Holy Roman Emperor Charles IV, who founded Charles University in Prague in 1348. Here, early Czech literature (a biblical translation, hymns and hagiography) flourished. Old Czech texts, including poetry and cookbooks, were produced outside the university as well. Later in the century Jan Hus contributed significantly to the standardization of Czech orthography, advocated for widespread literacy among Czech commoners (particularly in religion) and made early efforts to model written Czech after the spoken language.
Czech continued to evolve and gain in regional importance for hundreds of years, and has been a literary language in the Slovak lands since the early fifteenth century. A biblical translation, the Kralice Bible, was published during the late sixteenth century (around the time of the King James and Luther versions) which was more linguistically conservative than either. The publication of the Kralice Bible spawned widespread nationalism, and in 1615 the government of Bohemia ruled that only Czech-speaking residents would be allowed to become full citizens or inherit goods or land. This, and the conversion of the Czech upper classes from the Habsburg Empire's Catholicism to Protestantism, angered the Habsburgs and helped trigger the Thirty Years' War (where the Czechs were defeated at the Battle of White Mountain). The Czechs became serfs; Bohemia's printing industry (and its linguistic and political rights) were dismembered, removing official regulation and support from its language. German quickly became the dominant language in Bohemia.
The consensus among linguists is that modern, standard Czech originated during the eighteenth century. By then the language had developed a literary tradition, and since then it has changed little; journals from that period have no substantial differences from modern standard Czech, and contemporary Czechs can understand them with little difficulty. Changes include the morphological shift of í to ej and é to í (although é survives for some uses) and the merging of í and the former ejí. Sometime before the eighteenth century, the Czech language abandoned a distinction between phonemic /l/ and /ʎ/ which survives in Slovak.
The Czech people gained widespread national pride during the mid-eighteenth century, inspired by the Age of Enlightenment a half-century earlier. Czech historians began to emphasize their people's accomplishments from the fifteenth through the seventeenth centuries, rebelling against the Counter-Reformation (which had denigrated Czech and other non-Latin languages). Czech philologists studied sixteenth-century texts, advocating the return of the language to high culture. This period is known as the Czech National Revival (or Renascence).
During the revival, in 1809 linguist and historian Josef Dobrovský released a German-language grammar of Old Czech entitled Ausführliches Lehrgebäude der böhmischen Sprache (Comprehensive Doctrine of the Bohemian Language). Dobrovský had intended his book to be descriptive, and did not think Czech had a realistic chance of returning as a major language. However, Josef Jungmann and other revivalists used Dobrovský's book to advocate for a Czech linguistic revival. Changes during this time included spelling reform (notably, í in place of the former j and j in place of g), the use of t (rather than ti) to end infinitive verbs and the non-capitalization of nouns (which had been a late borrowing from German). These changes differentiated Czech from Slovak. Modern scholars disagree about whether the conservative revivalists were motivated by nationalism or considered contemporary spoken Czech unsuitable for formal, widespread use.
Czech, the official language of the Czech Republic (a member of the European Union since 2004), is one of the EU's official languages and the 2012 Eurobarometer survey found that Czech was the foreign language most often used in Slovakia. Economist Jonathan van Parys collected data on language knowledge in Europe for the 2012 European Day of Languages. The five countries with the greatest use of Czech were the Czech Republic (98.77 percent), Slovakia (24.86 percent), Portugal (1.93 percent), Poland (0.98 percent) and Germany (0.47 percent).
Immigration of Czechs from Europe to the United States occurred primarily from 1848 to 1914. Czech is a Less Commonly Taught Language in U.S. schools, and is taught at Czech heritage centers. Large communities of Czech Americans live in the states of Texas, Nebraska and Wisconsin. In the 2000 United States Census, Czech was reported as the most-common language spoken at home (besides English) in Valley, Butler and Saunders Counties, Nebraska and Republic County, Kansas. With the exception of Spanish (the non-English language most commonly spoken at home nationwide), Czech was the most-common home language in over a dozen additional counties in Nebraska, Kansas, Texas, North Dakota and Minnesota. As of 2009, 70,500 Americans spoke Czech as their first language (49th place nationwide, behind Turkish and ahead of Swedish).
In addition to a spoken standard and a closely related written standard, Czech has several regional dialects primarily used in rural areas by speakers less proficient in other dialects or standard Czech. During the second half of the twentieth century, Czech dialect use began to weaken. By the early 1990s dialect use was stigmatized, associated with the shrinking lower class and used in literature or other media for comedic effect. Increased travel and media availability to dialect-speaking populations has encouraged them to shift to (or add to their own dialect) standard Czech. Although Czech has received considerable scholarly interest for a Slavic language, this interest has focused primarily on modern standard Czech and ancient texts rather than dialects. Standard Czech is still the norm for politicians, businesspeople and other Czechs in formal situations, but Common Czech is gaining ground in journalism and the mass media.
The Czech dialects spoken in Moravia and Silesia are known as Moravian (moravština). In the Austro-Hungarian Empire, "Bohemian-Moravian-Slovak" was a language citizens could register as speaking (with German, Polish and several others). Of the Czech dialects, only Moravian is distinguished in nationwide surveys by the Czech Statistical Office. As of 2011, 62,908 Czech citizens spoke Moravian as their first language and 45,561 were diglossal (speaking Moravian and standard Czech as first languages).
Czech contains ten basic vowel phonemes, and three more found only in loanwords. They are /a/, /ɛ/, /ɪ/, /o/, and /u/, their long counterparts /aː/, /ɛː/, /iː/, /oː/ and /uː/, and three diphthongs, /ou̯/, /au̯/ and /ɛu̯/. The latter two diphthongs and the long /oː/ are exclusive to loanwords. Vowels are never reduced to schwa sounds when unstressed. Each word usually has primary stress on its first syllable, except for enclitics (minor, monosyllabic, unstressed syllables). In all words of more than two syllables, every odd-numbered syllable receives secondary stress. Stress is unrelated to vowel length, and the possibility of stressed short vowels and unstressed long vowels can be confusing to students whose native language combines the features (such as English).
Although older German loanwords were colloquial, recent borrowings from other languages are associated with high culture. During the nineteenth century, words with Greek and Latin roots were rejected in favor of those based on older Czech words and common Slavic roots; "music" is muzyka in Polish and музыка (muzyka) in Russian, but in Czech it is hudba. Some Czech words have been borrowed as loanwords into English and other languages—for example, robot (from robota, "labor") and polka (from polka, "Polish woman" or from "půlka" "half").
Because Czech uses grammatical case to convey word function in a sentence (instead of relying on word order, as English does), its word order is flexible. As a pro-drop language, in Czech an intransitive sentence can consist of only a verb; information about its subject is encoded in the verb. Enclitics (primarily auxiliary verbs and pronouns) must appear in the second syntactic slot of a sentence, after the first stressed unit. The first slot must contain a subject and object, a main form of a verb, an adverb or a conjunction (except for the light conjunctions a, "and", i, "and even" or ale, "but").
Czech syntax has a subject–verb–object sentence structure. In practice, however, word order is flexible and used for topicalization and focus. Although Czech has a periphrastic passive construction (like English), colloquial word-order changes frequently produce the passive voice. For example, to change "Peter killed Paul" to "Paul was killed by Peter" the order of subject and object is inverted: Petr zabil Pavla ("Peter killed Paul") becomes "Paul, Peter killed" (Pavla zabil Petr). Pavla is in the accusative case, the grammatical object (in this case, the victim) of the verb.
In Czech, nouns and adjectives are declined into one of seven grammatical cases. Nouns are inflected to indicate their use in a sentence. A nominative–accusative language, Czech marks subject nouns with nominative case and object nouns with accusative case. The genitive case marks possessive nouns and some types of movement. The remaining cases (instrumental, locative, vocative and dative) indicate semantic relationships, such as secondary objects, movement or position (dative case) and accompaniment (instrumental case). An adjective's case agrees with that of the noun it describes. When Czech children learn their language's declension patterns, the cases are referred to by number:
Czech distinguishes three genders—masculine, feminine, and neuter—and the masculine gender is subdivided into animate and inanimate. With few exceptions, feminine nouns in the nominative case end in -a, -e, or -ost; neuter nouns in -o, -e, or -í, and masculine nouns in a consonant. Adjectives agree in gender and animacy (for masculine nouns in the accusative or genitive singular and the nominative plural) with the nouns they modify. The main effect of gender in Czech is the difference in noun and adjective declension, but other effects include past-tense verb endings: for example, dělal (he did, or made); dělala (she did, or made) and dělalo (it did, or made).
Nouns are also inflected for number, distinguishing between singular and plural. Typical of a Slavic language, Czech cardinal numbers one through four allow the nouns and adjectives they modify to take any case, but numbers over five place these nouns and adjectives in the genitive case when the entire expression is in nominative or accusative case. The Czech koruna is an example of this feature; it is shown here as the subject of a hypothetical sentence, and declined as genitive for numbers five and up.
Typical of Slavic languages, Czech marks its verbs for one of two grammatical aspects: perfective and imperfective. Most verbs are part of inflected aspect pairs—for example, koupit (perfective) and kupovat (imperfective). Although the verbs' meaning is similar, in perfective verbs the action is completed and in imperfective verbs it is ongoing. This is distinct from past and present tense, and any Czech verb of either aspect can be conjugated into any of its three tenses. Aspect describes the state of the action at the time specified by the tense.
The verbs of most aspect pairs differ in one of two ways: by prefix or by suffix. In prefix pairs, the perfective verb has an added prefix—for example, the imperfective psát (to write, to be writing) compared with the perfective napsat (to write down, to finish writing). The most common prefixes are na-, o-, po-, s-, u-, vy-, z- and za-. In suffix pairs, a different infinitive ending is added to the perfective stem; for example, the perfective verbs koupit (to buy) and prodat (to sell) have the imperfective forms kupovat and prodávat. Imperfective verbs may undergo further morphology to make other imperfective verbs (iterative and frequentative forms), denoting repeated or regular action. The verb jít (to go) has the iterative form chodit (to go repeatedly) and the frequentative form chodívat (to go regularly).
The infinitive form ends in t (archaically, ti). It is the form found in dictionaries and the form that follows auxiliary verbs (for example, můžu tě slyšet—"I can hear you"). Czech verbs have three grammatical moods: indicative, imperative and conditional. The imperative mood adds specific endings for each of three person (or number) categories: -Ø/-i/-ej for second-person singular, -te/-ete/-ejte for second-person plural and -me/-eme/-ejme for first-person plural. The conditional mood is formed with a particle after the past-tense verb. This mood indicates possible events, expressed in English as "I would" or "I wish".
Czech has one of the most phonemic orthographies of all European languages. Its thirty-one graphemes represent thirty sounds (in most dialects, i and y have the same sound), and it contains only one digraph: ch, which follows h in the alphabet. As a result, some of its characters have been used by phonologists to denote corresponding sounds in other languages. The characters q, w and x appear only in foreign words. The háček (ˇ) is used with certain letters to form new characters: š, ž, and č, as well as ň, ě, ř, ť, and ď (the latter five uncommon outside Czech). The last two letters are sometimes written with a comma above (ʼ, an abbreviated háček) because of their height. The character ó exists only in loanwords and onomatopoeia.
Czech typographical features not associated with phonetics generally resemble those of most Latin European languages, including English. Proper nouns, honorifics, and the first letters of quotations are capitalized, and punctuation is typical of other Latin European languages. Writing of ordinal numerals is similar to most European languages. The Czech language uses a decimal comma instead of a decimal point. When writing a long number, spaces between every three numbers (e.g. between hundreds and thousands) may be used for better orientation in handwritten texts, but not in decimal places, like in English. The number 1,234,567.8910 may be written as 1234567,8910 or 1 234 567,8910. Ordinal numbers (1st) use a point as in German (1.). In proper noun phrases (except personal names), only the first word is capitalized (Pražský hrad, Prague Castle).
The Bronx /ˈbrɒŋks/ is the northernmost of the five boroughs (counties) of New York City in the state of New York, located south of Westchester County. Many bridges and tunnels link the Bronx to the island and borough of Manhattan to the west over and under the narrow Harlem River, as well as three longer bridges south over the East River to the borough of Queens. Of the five boroughs, the Bronx is the only one on the U.S. mainland and, with a land area of 42 square miles (109 km2) and a population of 1,438,159 in 2014, has the fourth largest land area, the fourth highest population, and the third-highest population density.
The Bronx is named after Jonas Bronck who created the first settlement as part of the New Netherland colony in 1639. The native Lenape were displaced after 1643 by settlers. In the 19th and 20th centuries, the Bronx received many immigrant groups as it was transformed into an urban community, first from various European countries (particularly Ireland, Germany and Italy) and later from the Caribbean region (particularly Puerto Rico, Jamaica and the Dominican Republic), as well as African American migrants from the American South. This cultural mix has made the Bronx a wellspring of both Latin music and hip hop.
The Bronx contains one of the five poorest Congressional Districts in the United States, the 15th, but its wide diversity also includes affluent, upper-income and middle-income neighborhoods such as Riverdale, Fieldston, Spuyten Duyvil, Schuylerville, Pelham Bay, Pelham Gardens, Morris Park and Country Club. The Bronx, particularly the South Bronx, saw a sharp decline in population, livable housing, and the quality of life in the late 1960s and the 1970s, culminating in a wave of arson. Since then the communities have shown significant redevelopment starting in the late 1980s before picking up pace in the 1990s into today.
Jonas Bronck (c. 1600–43) was a Swedish born emigrant from Komstad, Norra Ljunga parish in Småland, Sweden who arrived in New Netherland during the spring of 1639. He became the first recorded European settler in the area now known as the Bronx. He leased land from the Dutch West India Company on the neck of the mainland immediately north of the Dutch settlement in Harlem (on Manhattan island), and bought additional tracts from the local tribes. He eventually accumulated 500 acres (about 2 square km, or 3/4 of a square mile) between the Harlem River and the Aquahung, which became known as Bronck's River, or The Bronx. Dutch and English settlers referred to the area as Bronck's Land. The American poet William Bronk was a descendant of Pieter Bronck, either Jonas Bronck's son or his younger brother.
The Bronx is referred to, both legally and colloquially, with a definite article, as the Bronx. (The County of Bronx, unlike the coextensive Borough of the Bronx, does not place the immediately before Bronx in formal references, nor does the United States Postal Service in its database of Bronx addresses.) The name for this region, apparently after the Bronx River, first appeared in the Annexed District of the Bronx created in 1874 out of part of Westchester County and was continued in the Borough of the Bronx, which included a larger annexation from Westchester County in 1898. The use of the definite article is attributed to the style of referring to rivers. Another explanation for the use of the definite article in the borough's name is that the original form of the name was a possessive or collective one referring to the family, as in visiting The Broncks, The Bronck's or The Broncks'.
The development of the Bronx is directly connected to its strategic location between New England and New York (Manhattan). Control over the bridges across the Harlem River plagued the period of British colonial rule. Kingsbridge, built in 1693 where Broadway reached the Spuyten Duyvil Creek, was a possession of Frederick Philipse, lord of Philipse Manor. The tolls were resented by local farmers on both sides of the creek. In 1759, the farmers led by Jacobus Dyckman and Benjamin Palmer built a "free bridge" across the Harlem River which led to the abandonment of tolls altogether.
The territory now contained within Bronx County was originally part of Westchester County, one of the 12 original counties of the English Province of New York. The present Bronx County was contained in the town of Westchester and parts of the towns of Yonkers, Eastchester, and Pelham. In 1846, a new town, West Farms, was created by division of Westchester; in turn, in 1855, the town of Morrisania was created from West Farms. In 1873, the town of Kingsbridge (roughly corresponding to the modern Bronx neighborhoods of Kingsbridge, Riverdale, and Woodlawn) was established within the former borders of Yonkers.
The consolidation of the Bronx into New York City proceeded in two stages. In 1873, the state legislature annexed Kingsbridge, West Farms and Morrisania to New York, effective in 1874; the three towns were abolished in the process. In 1895, three years before New York's consolidation with Brooklyn, Queens and Staten Island, the whole of the territory east of the Bronx River, including the Town of Westchester (which had voted in 1894 against consolidation) and portions of Eastchester and Pelham, were annexed to the city. City Island, a nautical community, voted to join the city in 1896.
The history of the Bronx during the 20th century may be divided into four periods: a boom period during 1900–29, with a population growth by a factor of six from 200,000 in 1900 to 1.3 million in 1930. The Great Depression and post World War II years saw a slowing of growth leading into an eventual decline. The mid to late century were hard times, as the Bronx declined 1950–85 from a predominantly moderate-income to a predominantly lower-income area with high rates of violent crime and poverty. The Bronx has experienced an economic and developmental resurgence starting in the late 1980s that continues into today.
The Bronx underwent rapid urban growth after World War I. Extensions of the New York City Subway contributed to the increase in population as thousands of immigrants came to the Bronx, resulting in a major boom in residential construction. Among these groups, many Irish Americans, Italian Americans and especially Jewish Americans settled here. In addition, French, German, Polish and other immigrants moved into the borough. The Jewish population also increased notably during this time. In 1937, according to Jewish organizations, 592,185 Jews lived in The Bronx (43.9% of the borough's population), while only 54,000 Jews lived in the borough in 2011. Many synagogues still stand in the Bronx, but most have been converted to other uses.
Yet another may have been a reduction in the real-estate listings and property-related financial services (such as mortgage loans or insurance policies) offered in some areas of the Bronx — a process known as redlining. Others have suggested a "planned shrinkage" of municipal services, such as fire-fighting. There was also much debate as to whether rent control laws had made it less profitable (or more costly) for landlords to maintain existing buildings with their existing tenants than to abandon or destroy those buildings.
In the 1970s, the Bronx was plagued by a wave of arson. The burning of buildings was predominantly in the poorest communities, like the South Bronx. The most common explanation of what occurred was that landlords decided to burn their low property-value buildings and take the insurance money as it was more lucrative to get insurance money than to refurbish or sell a building in a severely distressed area. The Bronx became identified with a high rate of poverty and unemployment, which was mainly a persistent problem in the South Bronx.
Since the late 1980s, significant development has occurred in the Bronx, first stimulated by the city's "Ten-Year Housing Plan" and community members working to rebuild the social, economic and environmental infrastructure by creating affordable housing. Groups affiliated with churches in the South Bronx erected the Nehemiah Homes with about 1,000 units. The grass roots organization Nos Quedamos' endeavor known as Melrose Commons began to rebuild areas in the South Bronx. The IRT White Plains Road Line (2 5 trains) began to show an increase in riders. Chains such as Marshalls, Staples, and Target opened stores in the Bronx. More bank branches opened in the Bronx as a whole (rising from 106 in 1997 to 149 in 2007), although not primarily in poor or minority neighborhoods, while the Bronx still has fewer branches per person than other boroughs.
In 1997, the Bronx was designated an All America City by the National Civic League, acknowledging its comeback from the decline of the mid-century. In 2006, The New York Times reported that "construction cranes have become the borough's new visual metaphor, replacing the window decals of the 1980s in which pictures of potted plants and drawn curtains were placed in the windows of abandoned buildings." The borough has experienced substantial new building construction since 2002. Between 2002 and June 2007, 33,687 new units of housing were built or were under way and $4.8 billion has been invested in new housing. In the first six months of 2007 alone total investment in new residential development was $965 million and 5,187 residential units were scheduled to be completed. Much of the new development is springing up in formerly vacant lots across the South Bronx.
Several boutique and chain hotels have opened in recent years in the South Bronx; in addition, a La Quinta Inn that has been proposed for the Mott Haven waterfront. The Kingsbridge Armory, often cited as the largest armory in the world, is scheduled for redevelopment as the Kingsbridge National Ice Center. Under consideration for future development is the construction of a platform over the New York City Subway's Concourse Yard adjacent to Lehman College. The construction would permit approximately 2,000,000 square feet (190,000 m2) of development and would cost US$350–500 million.
The Bronx is almost entirely situated on the North American mainland. The Hudson River separates the Bronx on the west from Alpine, Tenafly and Englewood Cliffs in Bergen County, New Jersey; the Harlem River separates it from the island of Manhattan to the southwest; the East River separates it from Queens to the southeast; and to the east, Long Island Sound separates it from Nassau County in western Long Island. Directly north of the Bronx are (from west to east) the adjoining Westchester County communities of Yonkers, Mount Vernon, Pelham Manor and New Rochelle. (There is also a short southern land boundary with Marble Hill in the Borough of Manhattan, over the filled-in former course of the Spuyten Duyvil Creek. Marble Hill's postal ZIP code, telephonic Area Code and fire service, however, are shared with the Bronx and not Manhattan.)
The Bronx's highest elevation at 280 feet (85 m) is in the northwest corner, west of Van Cortlandt Park and in the Chapel Farm area near the Riverdale Country School. The opposite (southeastern) side of the Bronx has four large low peninsulas or "necks" of low-lying land that jut into the waters of the East River and were once salt marsh: Hunt's Point, Clason's Point, Screvin's Neck and Throg's Neck. Further up the coastline, Rodman's Neck lies between Pelham Bay Park in the northeast and City Island. The Bronx's irregular shoreline extends for 75 square miles (194 km2).
The northern side of the borough includes the largest park in New York City—Pelham Bay Park, which includes Orchard Beach—and the fourth largest, Van Cortlandt Park, which is west of Woodlawn Cemetery and borders Yonkers. Also in the northern Bronx, Wave Hill, the former estate of George W. Perkins—known for a historic house, gardens, changing site-specific art installations and concerts—overlooks the New Jersey Palisades from a promontory on the Hudson in Riverdale. Nearer the borough's center, and along the Bronx River, is Bronx Park; its northern end houses the New York Botanical Gardens, which preserve the last patch of the original hemlock forest that once covered the entire county, and its southern end the Bronx Zoo, the largest urban zoological gardens in the United States. Just south of Van Cortlandt Park is the Jerome Park Reservoir, surrounded by 2 miles (3 km) of stone walls and bordering several small parks in the Bedford Park neighborhood; the reservoir was built in the 1890s on the site of the former Jerome Park Racetrack. Further south is Crotona Park, home to a 3.3-acre (1.3 ha) lake, 28 species of trees, and a large swimming pool. The land for these parks, and many others, was bought by New York City in 1888, while land was still open and inexpensive, in anticipation of future needs and future pressures for development.
East of the Bronx River, the borough is relatively flat and includes four large low peninsulas, or 'necks,' of low-lying land which jut into the waters of the East River and were once saltmarsh: Hunts Point, Clason's Point, Screvin's Neck (Castle Hill Point) and Throgs Neck. The East Bronx has older tenement buildings, low income public housing complexes, and multifamily homes, as well as single family homes. It includes New York City's largest park: Pelham Bay Park along the Westchester-Bronx border.
The western parts of the Bronx are hillier and are dominated by a series of parallel ridges, running south to north. The West Bronx has older apartment buildings, low income public housing complexes, multifamily homes in its lower income areas as well as larger single family homes in more affluent areas such as Riverdale and Fieldston. It includes New York City's fourth largest park: Van Cortlandt Park along the Westchester-Bronx border. The Grand Concourse, a wide boulevard, runs through it, north to south.
Like other neighborhoods in New York City, the South Bronx has no official boundaries. The name has been used to represent poverty in the Bronx and applied to progressively more northern places so that by the 2000s Fordham Road was often used as a northern limit. The Bronx River more consistently forms an eastern boundary. The South Bronx has many high-density apartment buildings, low income public housing complexes, and multi-unit homes. The South Bronx is home to the Bronx County Courthouse, Borough Hall, and other government buildings, as well as Yankee Stadium. The Cross Bronx Expressway bisects it, east to west. The South Bronx has some of the poorest neighborhoods in the country, as well as very high crime areas.
There are three primary shopping centers in the Bronx: The Hub, Gateway Center and Southern Boulevard. The Hub–Third Avenue Business Improvement District (B.I.D.), in The Hub, is the retail heart of the South Bronx, located where four roads converge: East 149th Street, Willis, Melrose and Third Avenues. It is primarily located inside the neighborhood of Melrose but also lines the northern border of Mott Haven. The Hub has been called "the Broadway of the Bronx", being likened to the real Broadway in Manhattan and the northwestern Bronx. It is the site of both maximum traffic and architectural density. In configuration, it resembles a miniature Times Square, a spatial "bow-tie" created by the geometry of the street. The Hub is part of Bronx Community Board 1.
The Gateway Center at Bronx Terminal Market, in the West Bronx, is a shopping center that encompasses less than one million square feet of retail space, built on a 17 acres (7 ha) site that formerly held the Bronx Terminal Market, a wholesale fruit and vegetable market as well as the former Bronx House of Detention, south of Yankee Stadium. The $500 million shopping center, which was completed in 2009, saw the construction of new buildings and two smaller buildings, one new and the other a renovation of an existing building that was part of the original market. The two main buildings are linked by a six-level garage for 2,600 cars. The center has earned itself a LEED "Silver" designation in its design.
The Bronx street grid is irregular. Like the northernmost part of upper Manhattan, the West Bronx's hilly terrain leaves a relatively free-style street grid. Much of the West Bronx's street numbering carries over from upper Manhattan, but does not match it exactly; East 132nd Street is the lowest numbered street in the Bronx. This dates from the mid-19th century when the southwestern area of Westchester County west of the Bronx River, was incorporated into New York City and known as the Northside.
According to the 2010 Census, 53.5% of Bronx's population was of Hispanic, Latino, or Spanish origin (they may be of any race); 30.1% non-Hispanic Black or African American, 10.9% of the population was non-Hispanic White, 3.4% non-Hispanic Asian, 0.6% from some other race (non-Hispanic) and 1.2% of two or more races (non-Hispanic). The U.S. Census considers the Bronx to be the most diverse area in the country. There is an 89.7 percent chance that any two residents, chosen at random, would be of different race or ethnicity.
As of 2010, 46.29% (584,463) of Bronx residents aged five and older spoke Spanish at home, while 44.02% (555,767) spoke English, 2.48% (31,361) African languages, 0.91% (11,455) French, 0.90% (11,355) Italian, 0.87% (10,946) various Indic languages, 0.70% (8,836) other Indo-European languages, and Chinese was spoken at home by 0.50% (6,610) of the population over the age of five. In total, 55.98% (706,783) of the Bronx's population age five and older spoke a language at home other than English. A Garifuna-speaking community from Honduras and Guatemala also makes the Bronx its home.
According to the 2009 American Community Survey, White Americans of both Hispanic and non-Hispanic origin represented over one-fifth (22.9%) of the Bronx's population. However, non-Hispanic whites formed under one-eighth (12.1%) of the population, down from 34.4% in 1980. Out of all five boroughs, the Bronx has the lowest number and percentage of white residents. 320,640 whites called the Bronx home, of which 168,570 were non-Hispanic whites. The majority of the non-Hispanic European American population is of Italian and Irish descent. People of Italian descent numbered over 55,000 individuals and made up 3.9% of the population. People of Irish descent numbered over 43,500 individuals and made up 3.1% of the population. German Americans and Polish Americans made up 1.4% and 0.8% of the population respectively.
At the 2009 American Community Survey, Black Americans made the second largest group in the Bronx after Hispanics and Latinos. Blacks of both Hispanic and non-Hispanic origin represented over one-third (35.4%) of the Bronx's population. Blacks of non-Hispanic origin made up 30.8% of the population. Over 495,200 blacks resided in the borough, of which 430,600 were non-Hispanic blacks. Over 61,000 people identified themselves as "Sub-Saharan African" in the survey, making up 4.4% of the population.
In 2009, Hispanic and Latino Americans represented 52.0% of the Bronx's population. Puerto Ricans represented 23.2% of the borough's population. Over 72,500 Mexicans lived in the Bronx, and they formed 5.2% of the population. Cubans numbered over 9,640 members and formed 0.7% of the population. In addition, over 319,000 people were of various Hispanic and Latino groups, such as Dominican, Salvadoran, and so on. These groups collectively represented 22.9% of the population. At the 2010 Census, 53.5% of Bronx's population was of Hispanic, Latino, or Spanish origin (they may be of any race). Asian Americans are a small but sizable minority in the borough. Roughly 49,600 Asians make up 3.6% of the population. Roughly 13,600 Indians call the Bronx home, along with 9,800 Chinese, 6,540 Filipinos, 2,260 Vietnamese, 2,010 Koreans, and 1,100 Japanese.
Multiracial Americans are also a sizable minority in the Bronx. People of multiracial heritage number over 41,800 individuals and represent 3.0% of the population. People of mixed Caucasian and African American heritage number over 6,850 members and form 0.5% of the population. People of mixed Caucasian and Native American heritage number over 2,450 members and form 0.2% of the population. People of mixed Caucasian and Asian heritage number over 880 members and form 0.1% of the population. People of mixed African American and Native American heritage number over 1,220 members and form 0.1% of the population.
The office of Borough President was created in the consolidation of 1898 to balance centralization with local authority. Each borough president had a powerful administrative role derived from having a vote on the New York City Board of Estimate, which was responsible for creating and approving the city's budget and proposals for land use. In 1989 the Supreme Court of the United States declared the Board of Estimate unconstitutional on the grounds that Brooklyn, the most populous borough, had no greater effective representation on the Board than Staten Island, the least populous borough, a violation of the Fourteenth Amendment's Equal Protection Clause pursuant to the high court's 1964 "one man, one vote" decision.
Until March 1, 2009, the Borough President of the Bronx was Adolfo Carrión Jr., elected as a Democrat in 2001 and 2005 before retiring early to direct the White House Office of Urban Affairs Policy. His successor, Democratic New York State Assembly member Rubén Díaz, Jr., who won a special election on April 21, 2009 by a vote of 86.3% (29,420) on the "Bronx Unity" line to 13.3% (4,646) for the Republican district leader Anthony Ribustello on the "People First" line, became Borough President on May 1.
In the Presidential primary elections of February 5, 2008, Sen. Clinton won 61.2% of the Bronx's 148,636 Democratic votes against 37.8% for Barack Obama and 1.0% for the other four candidates combined (John Edwards, Dennis Kucinich, Bill Richardson and Joe Biden). On the same day, John McCain won 54.4% of the borough's 5,643 Republican votes, Mitt Romney 20.8%, Mike Huckabee 8.2%, Ron Paul 7.4%, Rudy Giuliani 5.6%, and the other candidates (Fred Thompson, Duncan Hunter and Alan Keyes) 3.6% between them.
Since then, the Bronx has always supported the Democratic Party's nominee for President, starting with a vote of 2-1 for the unsuccessful Al Smith in 1928, followed by four 2-1 votes for the successful Franklin D. Roosevelt. (Both had been Governors of New York, but Republican former Gov. Thomas E. Dewey won only 28% of the Bronx's vote in 1948 against 55% for Pres. Harry Truman, the winning Democrat, and 17% for Henry A. Wallace of the Progressives. It was only 32 years earlier, by contrast, that another Republican former Governor who narrowly lost the Presidency, Charles Evans Hughes, had won 42.6% of the Bronx's 1916 vote against Democratic President Woodrow Wilson's 49.8% and Socialist candidate Allan Benson's 7.3%.)
The Bronx has often shown striking differences from other boroughs in elections for Mayor. The only Republican to carry the Bronx since 1914 was Fiorello La Guardia in 1933, 1937 and 1941 (and in the latter two elections, only because his 30-32% vote on the American Labor Party line was added to 22-23% as a Republican). The Bronx was thus the only borough not carried by the successful Republican re-election campaigns of Mayors Rudolph Giuliani in 1997 and Michael Bloomberg in 2005. The anti-war Socialist campaign of Morris Hillquit in the 1917 mayoral election won over 31% of the Bronx's vote, putting him second and well ahead of the 20% won by the incumbent pro-war Fusion Mayor John P. Mitchel, who came in second (ahead of Hillquit) everywhere else and outpolled Hillquit city-wide by 23.2% to 21.7%.
Education in the Bronx is provided by a large number of public and private institutions, many of which draw students who live beyond the Bronx. The New York City Department of Education manages public noncharter schools in the borough. In 2000, public schools enrolled nearly 280,000 of the Bronx's residents over 3 years old (out of 333,100 enrolled in all pre-college schools). There are also several public charter schools. Private schools range from élite independent schools to religiously affiliated schools run by the Roman Catholic Archdiocese of New York and Jewish organizations.
Educational attainment: In 2000, according to the U.S. Census, out of the nearly 800,000 people in the Bronx who were then at least 25 years old, 62.3% had graduated from high school and 14.6% held a bachelor's or higher college degree. These percentages were lower than those for New York's other boroughs, which ranged from 68.8% (Brooklyn) to 82.6% (Staten Island) for high school graduates over 24, and from 21.8% (Brooklyn) to 49.4% (Manhattan) for college graduates. (The respective state and national percentages were [NY] 79.1% & 27.4% and [US] 80.4% & 24.4%.)
Many public high schools are located in the borough including the elite Bronx High School of Science, Celia Cruz Bronx High School of Music, DeWitt Clinton High School, High School for Violin and Dance, Bronx Leadership Academy 2, Bronx International High School, the School for Excellence, the Morris Academy for Collaborative Study, Wings Academy for young adults, The Bronx School for Law, Government and Justice, Validus Preparatory Academy, The Eagle Academy For Young Men, Bronx Expeditionary Learning High School, Bronx Academy of Letters, Herbert H. Lehman High School and High School of American Studies. The Bronx is also home to three of New York City's most prestigious private, secular schools: Fieldston, Horace Mann, and Riverdale Country School.
In the 1990s, New York City began closing the large, public high schools in the Bronx and replacing them with small high schools. Among the reasons cited for the changes were poor graduation rates and concerns about safety. Schools that have been closed or reduced in size include John F. Kennedy, James Monroe, Taft, Theodore Roosevelt, Adlai Stevenson, Evander Childs, Christopher Columbus, Morris, Walton, and South Bronx High Schools. More recently the City has started phasing out large middle schools, also replacing them with smaller schools.
The Bronx's evolution from a hot bed of Latin jazz to an incubator of hip hop was the subject of an award-winning documentary, produced by City Lore and broadcast on PBS in 2006, "From Mambo to Hip Hop: A South Bronx Tale". Hip Hop first emerged in the South Bronx in the early 1970s. The New York Times has identified 1520 Sedgwick Avenue "an otherwise unremarkable high-rise just north of the Cross Bronx Expressway and hard along the Major Deegan Expressway" as a starting point, where DJ Kool Herc presided over parties in the community room.
Beginning with the advent of beat match DJing, in which Bronx DJs (Disc Jockeys) including Grandmaster Flash, Afrika Bambaataa and DJ Kool Herc extended the breaks of funk records, a major new musical genre emerged that sought to isolate the percussion breaks of hit funk, disco and soul songs. As hip hop's popularity grew, performers began speaking ("rapping") in sync with the beats, and became known as MCs or emcees. The Herculoids, made up of Herc, Coke La Rock, and DJ Clark Kent, were the earliest to gain major fame. The Bronx is referred to in hip-hop slang as "The Boogie Down Bronx", or just "The Boogie Down". This was hip-hop pioneer KRS-One's inspiration for his thought provoking group BDP, or Boogie Down Productions, which included DJ Scott La Rock. Newer hip hop artists from the Bronx include Big Pun, Lord Toriq and Peter Gunz, Camp Lo, Swizz Beatz, Drag-On, Fat Joe, Terror Squad and Corey Gunz.
The Bronx is the home of the New York Yankees of Major League Baseball. The original Yankee Stadium opened in 1923 on 161st Street and River Avenue, a year that saw the Yankees bring home their first of 27 World Series Championships. With the famous facade, the short right field porch and Monument Park, Yankee Stadium has been home to many of baseball's greatest players including Babe Ruth, Lou Gehrig, Joe DiMaggio, Whitey Ford, Yogi Berra, Mickey Mantle, Reggie Jackson, Derek Jeter and Mariano Rivera.
The Bronx is home to several Off-Off-Broadway theaters, many staging new works by immigrant playwrights from Latin America and Africa. The Pregones Theater, which produces Latin American work, opened a new 130-seat theater in 2005 on Walton Avenue in the South Bronx. Some artists from elsewhere in New York City have begun to converge on the area, and housing prices have nearly quadrupled in the area since 2002. However rising prices directly correlate to a housing shortage across the city and the entire metro area.
The Bronx Museum of the Arts, founded in 1971, exhibits 20th century and contemporary art through its central museum space and 11,000 square feet (1,000 m2) of galleries. Many of its exhibitions are on themes of special interest to the Bronx. Its permanent collection features more than 800 works of art, primarily by artists from Africa, Asia and Latin America, including paintings, photographs, prints, drawings, and mixed media. The museum was temporarily closed in 2006 while it underwent a major expansion designed by the architectural firm Arquitectonica.
The Bronx has also become home to a peculiar poetic tribute, in the form of the Heinrich Heine Memorial, better known as the Lorelei Fountain from one of Heine's best-known works (1838). After Heine's German birthplace of Düsseldorf had rejected, allegedly for anti-Semitic motives, a centennial monument to the radical German-Jewish poet (1797–1856), his incensed German-American admirers, including Carl Schurz, started a movement to place one instead in Midtown Manhattan, at Fifth Avenue and 59th Street. However, this intention was thwarted by a combination of ethnic antagonism, aesthetic controversy and political struggles over the institutional control of public art.
In 1899, the memorial, by the Berlin sculptor Ernst Gustav Herter (1846–1917), finally came to rest, although subject to repeated vandalism, in the Bronx, at 164th Street and the Grand Concourse, or Joyce Kilmer Park near today's Yankee Stadium. (In 1999, it was moved to 161st Street and the Concourse.) In 2007, Christopher Gray of The New York Times described it as "a writhing composition in white Tyrolean marble depicting Lorelei, the mythical German figure, surrounded by mermaids, dolphins and seashells."
The peninsular borough's maritime heritage is acknowledged in several ways.The City Island Historical Society and Nautical Museum occupies a former public school designed by the New York City school system's turn-of-the-last-century master architect C. B. J. Snyder. The state's Maritime College in Fort Schuyler (on the southeastern shore) houses the Maritime Industry Museum. In addition, the Harlem River is reemerging as "Scullers' Row" due in large part to the efforts of the Bronx River Restoration Project, a joint public-private endeavor of the city's parks department. Canoeing and kayaking on the borough's namesake river have been promoted by the Bronx River Alliance. The river is also straddled by the New York Botanical Gardens, its neighbor, the Bronx Zoo, and a little further south, on the west shore, Bronx River Art Center.
The Bronx has several local newspapers, including The Bronx News, Parkchester News, City News, The Riverdale Press, Riverdale Review, The Bronx Times Reporter, Inner City Press  (which now has more of a focus on national issues) and Co-Op City Times. Four non-profit news outlets, Norwood News, Mount Hope Monitor, Mott Haven Herald and The Hunts Point Express serve the borough's poorer communities. The editor and co-publisher of The Riverdale Press, Bernard Stein, won the Pulitzer Prize for Editorial Writing for his editorials about Bronx and New York City issues in 1998. (Stein graduated from the Bronx High School of Science in 1959.)
The City of New York has an official television station run by the NYC Media Group and broadcasting from Bronx Community College, and Cablevision operates News 12 The Bronx, both of which feature programming based in the Bronx. Co-op City was the first area in the Bronx, and the first in New York beyond Manhattan, to have its own cable television provider. The local public-access television station BronxNet originates from Herbert H. Lehman College, the borough's only four year CUNY school, and provides government-access television (GATV) public affairs programming in addition to programming produced by Bronx residents.
Mid-20th century movies set in the Bronx portrayed densely settled, working-class, urban culture. Hollywood films such as From This Day Forward (1946), set in Highbridge, occasionally delved into Bronx life. Paddy Chayefsky's Academy Award-winning Marty was the most notable examination of working class Bronx life was also explored by Chayefsky in his 1956 film The Catered Affair, and in the 1993 Robert De Niro/Chazz Palminteri film, A Bronx Tale, Spike Lee's 1999 movie Summer of Sam, centered in an Italian-American Bronx community, 1994's I Like It Like That that takes place in the predominantly Puerto Rican neighborhood of the South Bronx, and Doughboys, the story of two Italian-American brothers in danger of losing their bakery thanks to one brother's gambling debts.
Starting in the 1970s, the Bronx often symbolized violence, decay, and urban ruin. The wave of arson in the South Bronx in the 1960s and 1970s inspired the observation that "The Bronx is burning": in 1974 it was the title of both a New York Times editorial and a BBC documentary film. The line entered the pop-consciousness with Game Two of the 1977 World Series, when a fire broke out near Yankee Stadium as the team was playing the Los Angeles Dodgers. Numerous fires had previously broken out in the Bronx prior to this fire. As the fire was captured on live television, announcer Howard Cosell is wrongly remembered to have said something like, "There it is, ladies and gentlemen: the Bronx is burning". Historians of New York City frequently point to Cosell's remark as an acknowledgement of both the city and the borough's decline. A new feature-length documentary film by Edwin Pagan called Bronx Burning is in production in 2006, chronicling what led up to the numerous arson-for-insurance fraud fires of the 1970s in the borough.
Bronx gang life was depicted in the 1974 novel The Wanderers by Bronx native Richard Price and the 1979 movie of the same name. They are set in the heart of the Bronx, showing apartment life and the then-landmark Krums ice cream parlor. In the 1979 film The Warriors, the eponymous gang go to a meeting in Van Cortlandt Park in the Bronx, and have to fight their way out of the borough and get back to Coney Island in Brooklyn. A Bronx Tale (1993) depicts gang activities in the Belmont "Little Italy" section of the Bronx. The 2005 video game adaptation features levels called Pelham, Tremont, and "Gunhill" (a play off the name Gun Hill Road). This theme lends itself to the title of The Bronx Is Burning, an eight-part ESPN TV mini-series (2007) about the New York Yankees' drive to winning baseball's 1977 World Series. The TV series emphasizes the boisterous nature of the team, led by manager Billy Martin, catcher Thurman Munson and outfielder Reggie Jackson, as well as the malaise of the Bronx and New York City in general during that time, such as the blackout, the city's serious financial woes and near bankruptcy, the arson for insurance payments, and the election of Ed Koch as mayor.
The 1981 film Fort Apache, The Bronx is another film that used the Bronx's gritty image for its storyline. The movie's title is from the nickname for the 41st Police Precinct in the South Bronx which was nicknamed "Fort Apache". Also from 1981 is the horror film Wolfen making use of the rubble of the Bronx as a home for werewolf type creatures. Knights of the South Bronx, a true story of a teacher who worked with disadvantaged children, is another film also set in the Bronx released in 2005. The Bronx was the setting for the 1983 film Fuga dal Bronx, also known as Bronx Warriors 2 and Escape 2000, an Italian B-movie best known for its appearance on the television series Mystery Science Theatre 3000. The plot revolves around a sinister construction corporation's plans to depopulate, destroy and redevelop the Bronx, and a band of rebels who are out to expose the corporation's murderous ways and save their homes. The film is memorable for its almost incessant use of the phrase, "Leave the Bronx!" Many of the movie's scenes were filmed in Queens, substituting as the Bronx. Rumble in the Bronx was a 1995 Jackie Chan kung-fu film, another which popularised the Bronx to international audiences. Last Bronx, a 1996 Sega game played on the bad reputation of the Bronx to lend its name to an alternate version of post-Japanese bubble Tokyo, where crime and gang warfare is rampant.
Bronx native Nancy Savoca's 1989 comedy, True Love, explores two Italian-American Bronx sweethearts in the days before their wedding. The film, which debuted Annabella Sciorra and Ron Eldard as the betrothed couple, won the Grand Jury Prize at that year's Sundance Film Festival. The CBS television sitcom Becker, 1998–2004, was more ambiguous. The show starred Ted Danson as Dr. John Becker, a doctor who operated a small practice and was constantly annoyed by his patients, co-workers, friends, and practically everything and everybody else in his world. It showed his everyday life as a doctor working in a small clinic in the Bronx.
Penny Marshall's 1990 film Awakenings, which was nominated for several Oscars, is based on neurologist Oliver Sacks' 1973 account of his psychiatric patients at Beth Abraham Hospital in the Bronx who were paralyzed by a form of encephalitis but briefly responded to the drug L-dopa. Robin Williams played the physician; Robert De Niro was one of the patients who emerged from a catatonic (frozen) state. The home of Williams' character was shot not far from Sacks' actual City Island residence. A 1973 Yorkshire Television documentary and "A Kind of Alaska", a 1985 play by Harold Pinter, were also based on Sacks' book.
The Bronx has been featured significantly in fiction literature. All of the characters in Herman Wouk's City Boy: The Adventures of Herbie Bookbinder (1948) live in the Bronx, and about half of the action is set there. Kate Simon's Bronx Primitive: Portraits of a Childhood is directly autobiographical, a warm account of a Polish-Jewish girl in an immigrant family growing up before World War II, and living near Arthur Avenue and Tremont Avenue. In Jacob M. Appel's short story, "The Grand Concourse" (2007), a woman who grew up in the iconic Lewis Morris Building returns to the Morrisania neighborhood with her adult daughter. Similarly, in Avery Corman's book The Old Neighborhood (1980), an upper-middle class white protagonist returns to his birth neighborhood (Fordham Road and the Grand Concourse), and learns that even though the folks are poor, Hispanic and African-American, they are good people.
By contrast, Tom Wolfe's Bonfire of the Vanities (1987) portrays a wealthy, white protagonist, Sherman McCoy, getting lost off the Major Deegan Expressway in the South Bronx and having an altercation with locals. A substantial piece of the last part of the book is set in the resulting riotous trial at the Bronx County Courthouse. However, times change, and in 2007, the New York Times reported that "the Bronx neighborhoods near the site of Sherman's accident are now dotted with townhouses and apartments." In the same article, the Reverend Al Sharpton (whose fictional analogue in the novel is "Reverend Bacon") asserts that "twenty years later, the cynicism of The Bonfire of the Vanities is as out of style as Tom Wolfe's wardrobe."
The phrase "in whole or in part" has been subject to much discussion by scholars of international humanitarian law. The International Criminal Tribunal for the Former Yugoslavia found in Prosecutor v. Radislav Krstic – Trial Chamber I – Judgment – IT-98-33 (2001) ICTY8 (2 August 2001) that Genocide had been committed. In Prosecutor v. Radislav Krstic – Appeals Chamber – Judgment – IT-98-33 (2004) ICTY 7 (19 April 2004) paragraphs 8, 9, 10, and 11 addressed the issue of in part and found that "the part must be a substantial part of that group. The aim of the Genocide Convention is to prevent the intentional destruction of entire human groups, and the part targeted must be significant enough to have an impact on the group as a whole." The Appeals Chamber goes into details of other cases and the opinions of respected commentators on the Genocide Convention to explain how they came to this conclusion.
In the same judgement the ECHR reviewed the judgements of several international and municipal courts judgements. It noted that International Criminal Tribunal for the Former Yugoslavia and the International Court of Justice had agreed with the narrow interpretation, that biological-physical destruction was necessary for an act to qualify as genocide. The ECHR also noted that at the time of its judgement, apart from courts in Germany which had taken a broad view, that there had been few cases of genocide under other Convention States municipal laws and that "There are no reported cases in which the courts of these States have defined the type of group destruction the perpetrator must have intended in order to be found guilty of genocide".
After the Holocaust, which had been perpetrated by the Nazi Germany and its allies prior to and during World War II, Lemkin successfully campaigned for the universal acceptance of international laws defining and forbidding genocides. In 1946, the first session of the United Nations General Assembly adopted a resolution that "affirmed" that genocide was a crime under international law, but did not provide a legal definition of the crime. In 1948, the UN General Assembly adopted the Convention on the Prevention and Punishment of the Crime of Genocide (CPPCG) which defined the crime of genocide for the first time.
The first draft of the Convention included political killings, but these provisions were removed in a political and diplomatic compromise following objections from some countries, including the USSR, a permanent security council member. The USSR argued that the Convention's definition should follow the etymology of the term, and may have feared greater international scrutiny of its own Great Purge. Other nations feared that including political groups in the definition would invite international intervention in domestic politics. However leading genocide scholar William Schabas states: “Rigorous examination of the travaux fails to confirm a popular impression in the literature that the opposition to inclusion of political genocide was some Soviet machination. The Soviet views were also shared by a number of other States for whom it is difficult to establish any geographic or social common denominator: Lebanon, Sweden, Brazil, Peru, Venezuela, the Philippines, the Dominican Republic, Iran, Egypt, Belgium, and Uruguay. The exclusion of political groups was in fact originally promoted by a non-governmental organization, the World Jewish Congress, and it corresponded to Raphael Lemkin’s vision of the nature of the crime of genocide.” 
In 2007 the European Court of Human Rights (ECHR), noted in its judgement on Jorgic v. Germany case that in 1992 the majority of legal scholars took the narrow view that "intent to destroy" in the CPPCG meant the intended physical-biological destruction of the protected group and that this was still the majority opinion. But the ECHR also noted that a minority took a broader view and did not consider biological-physical destruction was necessary as the intent to destroy a national, racial, religious or ethnic group was enough to qualify as genocide.
The word genocide was later included as a descriptive term to the process of indictment, but not yet as a formal legal term According to Lemming, genocide was defined as "a coordinated strategy to destroy a group of people, a process that could be accomplished through total annihilation as well as strategies that eliminate key elements of the group's basic existence, including language, culture, and economic infrastructure.” He created a concept of mobilizing much of the international relations and community, to working together and preventing the occurrence of such events happening within history and the international society. Australian anthropologist Peg LeVine coined the term "ritualcide" to describe the destruction of a group's cultural identity without necessarily destroying its members.
The study of genocide has mainly been focused towards the legal aspect of the term. By formally recognizing the act of genocide as a crime, involves the undergoing prosecution that begins with not only seeing genocide as outrageous past any moral standpoint but also may be a legal liability within international relations. When genocide is looked at in a general aspect it is viewed as the deliberate killing of a certain group. Yet is commonly seen to escape the process of trial and prosecution due to the fact that genocide is more often than not committed by the officials in power of a state or area. In 1648 before the term genocide had been coined, the Peace of Westphalia was established to protect ethnic, national, racial and in some instances religious groups. During the 19th century humanitarian intervention was needed due to the fact of conflict and justification of some of the actions executed by the military.
Genocide has become an official term used in international relations. The word genocide was not in use before 1944. Before this, in 1941, Winston Churchill described the mass killing of Russian prisoners of war and civilians as "a crime without a name". In that year, a Polish-Jewish lawyer named Raphael Lemkin, described the policies of systematic murder founded by the Nazis as genocide. The word genocide is the combination of the Greek prefix geno- (meaning tribe or race) and caedere (the Latin word for to kill). The word is defined as a specific set of violent crimes that are committed against a certain group with the attempt to remove the entire group from existence or to destroy them.
The judges continue in paragraph 12, "The determination of when the targeted part is substantial enough to meet this requirement may involve a number of considerations. The numeric size of the targeted part of the group is the necessary and important starting point, though not in all cases the ending point of the inquiry. The number of individuals targeted should be evaluated not only in absolute terms, but also in relation to the overall size of the entire group. In addition to the numeric size of the targeted portion, its prominence within the group can be a useful consideration. If a specific part of the group is emblematic of the overall group, or is essential to its survival, that may support a finding that the part qualifies as substantial within the meaning of Article 4 [of the Tribunal's Statute]."
In paragraph 13 the judges raise the issue of the perpetrators' access to the victims: "The historical examples of genocide also suggest that the area of the perpetrators’ activity and control, as well as the possible extent of their reach, should be considered. ... The intent to destroy formed by a perpetrator of genocide will always be limited by the opportunity presented to him. While this factor alone will not indicate whether the targeted group is substantial, it can—in combination with other factors—inform the analysis."
The Convention came into force as international law on 12 January 1951 after the minimum 20 countries became parties. At that time however, only two of the five permanent members of the UN Security Council were parties to the treaty: France and the Republic of China. The Soviet Union ratified in 1954, the United Kingdom in 1970, the People's Republic of China in 1983 (having replaced the Taiwan-based Republic of China on the UNSC in 1971), and the United States in 1988. This long delay in support for the Convention by the world's most powerful nations caused the Convention to languish for over four decades. Only in the 1990s did the international law on the crime of genocide begin to be enforced.
Writing in 1998 Kurt Jonassohn and Karin Björnson stated that the CPPCG was a legal instrument resulting from a diplomatic compromise. As such the wording of the treaty is not intended to be a definition suitable as a research tool, and although it is used for this purpose, as it has an international legal credibility that others lack, other definitions have also been postulated. Jonassohn and Björnson go on to say that none of these alternative definitions have gained widespread support for various reasons.
Jonassohn and Björnson postulate that the major reason why no single generally accepted genocide definition has emerged is because academics have adjusted their focus to emphasise different periods and have found it expedient to use slightly different definitions to help them interpret events. For example, Frank Chalk and Kurt Jonassohn studied the whole of human history, while Leo Kuper and R. J. Rummel in their more recent works concentrated on the 20th century, and Helen Fein, Barbara Harff and Ted Gurr have looked at post World War II events. Jonassohn and Björnson are critical of some of these studies, arguing that they are too expansive, and conclude that the academic discipline of genocide studies is too young to have a canon of work on which to build an academic paradigm.
The exclusion of social and political groups as targets of genocide in the CPPCG legal definition has been criticized by some historians and sociologists, for example M. Hassan Kakar in his book The Soviet Invasion and the Afghan Response, 1979–1982 argues that the international definition of genocide is too restricted, and that it should include political groups or any group so defined by the perpetrator and quotes Chalk and Jonassohn: "Genocide is a form of one-sided mass killing in which a state or other authority intends to destroy a group, as that group and membership in it are defined by the perpetrator." While there are various definitions of the term, Adam Jones states that the majority of genocide scholars consider that "intent to destroy" is a requirement for any act to be labelled genocide, and that there is growing agreement on the inclusion of the physical destruction criterion.
Barbara Harff and Ted Gurr defined genocide as "the promotion and execution of policies by a state or its agents which result in the deaths of a substantial portion of a group ...[when] the victimized groups are defined primarily in terms of their communal characteristics, i.e., ethnicity, religion or nationality." Harff and Gurr also differentiate between genocides and politicides by the characteristics by which members of a group are identified by the state. In genocides, the victimized groups are defined primarily in terms of their communal characteristics, i.e., ethnicity, religion or nationality. In politicides the victim groups are defined primarily in terms of their hierarchical position or political opposition to the regime and dominant groups. Daniel D. Polsby and Don B. Kates, Jr. state that "... we follow Harff's distinction between genocides and 'pogroms,' which she describes as 'short-lived outbursts by mobs, which, although often condoned by authorities, rarely persist.' If the violence persists for long enough, however, Harff argues, the distinction between condonation and complicity collapses."
According to R. J. Rummel, genocide has 3 different meanings. The ordinary meaning is murder by government of people due to their national, ethnic, racial, or religious group membership. The legal meaning of genocide refers to the international treaty, the Convention on the Prevention and Punishment of the Crime of Genocide. This also includes non-killings that in the end eliminate the group, such as preventing births or forcibly transferring children out of the group to another group. A generalized meaning of genocide is similar to the ordinary meaning but also includes government killings of political opponents or otherwise intentional murder. It is to avoid confusion regarding what meaning is intended that Rummel created the term democide for the third meaning.
Highlighting the potential for state and non-state actors to commit genocide in the 21st century, for example, in failed states or as non-state actors acquire weapons of mass destruction, Adrian Gallagher defined genocide as 'When a source of collective power (usually a state) intentionally uses its power base to implement a process of destruction in order to destroy a group (as defined by the perpetrator), in whole or in substantial part, dependent upon relative group size'. The definition upholds the centrality of intent, the multidimensional understanding of destroy, broadens the definition of group identity beyond that of the 1948 definition yet argues that a substantial part of a group has to be destroyed before it can be classified as genocide (dependent on relative group size).
All signatories to the CPPCG are required to prevent and punish acts of genocide, both in peace and wartime, though some barriers make this enforcement difficult. In particular, some of the signatories—namely, Bahrain, Bangladesh, India, Malaysia, the Philippines, Singapore, the United States, Vietnam, Yemen, and former Yugoslavia—signed with the proviso that no claim of genocide could be brought against them at the International Court of Justice without their consent. Despite official protests from other signatories (notably Cyprus and Norway) on the ethics and legal standing of these reservations, the immunity from prosecution they grant has been invoked from time to time, as when the United States refused to allow a charge of genocide brought against it by former Yugoslavia following the 1999 Kosovo War.
Because the universal acceptance of international laws which in 1948 defined and forbade genocide with the promulgation of the Convention on the Prevention and Punishment of the Crime of Genocide (CPPCG), those criminals who were prosecuted after the war in international courts for taking part in the Holocaust were found guilty of crimes against humanity and other more specific crimes like murder. Nevertheless, the Holocaust is universally recognized to have been a genocide and the term, that had been coined the year before by Raphael Lemkin, appeared in the indictment of the 24 Nazi leaders, Count 3, which stated that all the defendants had "conducted deliberate and systematic genocide—namely, the extermination of racial and national groups..."
On 12 July 2007, European Court of Human Rights when dismissing the appeal by Nikola Jorgić against his conviction for genocide by a German court (Jorgic v. Germany) noted that the German courts wider interpretation of genocide has since been rejected by international courts considering similar cases. The ECHR also noted that in the 21st century "Amongst scholars, the majority have taken the view that ethnic cleansing, in the way in which it was carried out by the Serb forces in Bosnia and Herzegovina in order to expel Muslims and Croats from their homes, did not constitute genocide. However, there are also a considerable number of scholars who have suggested that these acts did amount to genocide, and the ICTY has found in the Momcilo Krajisnik case that the actus reu, of genocide was met in Prijedor "With regard to the charge of genocide, the Chamber found that in spite of evidence of acts perpetrated in the municipalities which constituted the actus reus of genocide".
About 30 people have been indicted for participating in genocide or complicity in genocide during the early 1990s in Bosnia. To date, after several plea bargains and some convictions that were successfully challenged on appeal two men, Vujadin Popović and Ljubiša Beara, have been found guilty of committing genocide, Zdravko Tolimir has been found guilty of committing genocide and conspiracy to commit genocide, and two others, Radislav Krstić and Drago Nikolić, have been found guilty of aiding and abetting genocide. Three others have been found guilty of participating in genocides in Bosnia by German courts, one of whom Nikola Jorgić lost an appeal against his conviction in the European Court of Human Rights. A further eight men, former members of the Bosnian Serb security forces were found guilty of genocide by the State Court of Bosnia and Herzegovina (See List of Bosnian genocide prosecutions).
Slobodan Milošević, as the former President of Serbia and of Yugoslavia, was the most senior political figure to stand trial at the ICTY. He died on 11 March 2006 during his trial where he was accused of genocide or complicity in genocide in territories within Bosnia and Herzegovina, so no verdict was returned. In 1995, the ICTY issued a warrant for the arrest of Bosnian Serbs Radovan Karadžić and Ratko Mladić on several charges including genocide. On 21 July 2008, Karadžić was arrested in Belgrade, and he is currently in The Hague on trial accused of genocide among other crimes. Ratko Mladić was arrested on 26 May 2011 by Serbian special police in Lazarevo, Serbia. Karadzic was convicted of ten of the eleven charges laid against him and sentenced to 40 years in prison on March 24 2016.
The International Criminal Tribunal for Rwanda (ICTR) is a court under the auspices of the United Nations for the prosecution of offenses committed in Rwanda during the genocide which occurred there during April 1994, commencing on 6 April. The ICTR was created on 8 November 1994 by the Security Council of the United Nations in order to judge those people responsible for the acts of genocide and other serious violations of the international law performed in the territory of Rwanda, or by Rwandan citizens in nearby states, between 1 January and 31 December 1994.
There has been much debate over categorizing the situation in Darfur as genocide. The ongoing conflict in Darfur, Sudan, which started in 2003, was declared a "genocide" by United States Secretary of State Colin Powell on 9 September 2004 in testimony before the Senate Foreign Relations Committee. Since that time however, no other permanent member of the UN Security Council followed suit. In fact, in January 2005, an International Commission of Inquiry on Darfur, authorized by UN Security Council Resolution 1564 of 2004, issued a report to the Secretary-General stating that "the Government of the Sudan has not pursued a policy of genocide." Nevertheless, the Commission cautioned that "The conclusion that no genocidal policy has been pursued and implemented in Darfur by the Government authorities, directly or through the militias under their control, should not be taken in any way as detracting from the gravity of the crimes perpetrated in that region. International offences such as the crimes against humanity and war crimes that have been committed in Darfur may be no less serious and heinous than genocide."
In March 2005, the Security Council formally referred the situation in Darfur to the Prosecutor of the International Criminal Court, taking into account the Commission report but without mentioning any specific crimes. Two permanent members of the Security Council, the United States and China, abstained from the vote on the referral resolution. As of his fourth report to the Security Council, the Prosecutor has found "reasonable grounds to believe that the individuals identified [in the UN Security Council Resolution 1593] have committed crimes against humanity and war crimes," but did not find sufficient evidence to prosecute for genocide.
Other authors have focused on the structural conditions leading up to genocide and the psychological and social processes that create an evolution toward genocide. Ervin Staub showed that economic deterioration and political confusion and disorganization were starting points of increasing discrimination and violence in many instances of genocides and mass killing. They lead to scapegoating a group and ideologies that identified that group as an enemy. A history of devaluation of the group that becomes the victim, past violence against the group that becomes the perpetrator leading to psychological wounds, authoritarian cultures and political systems, and the passivity of internal and external witnesses (bystanders) all contribute to the probability that the violence develops into genocide. Intense conflict between groups that is unresolved, becomes intractable and violent can also lead to genocide. The conditions that lead to genocide provide guidance to early prevention, such as humanizing a devalued group, creating ideologies that embrace all groups, and activating bystander responses. There is substantial research to indicate how this can be done, but information is only slowly transformed into action.
Georgian architecture is the name given in most English-speaking countries to the set of architectural styles current between 1714 and 1830. It is eponymous for the first four British monarchs of the House of Hanover—George I, George II, George III, and George IV—who reigned in continuous succession from August 1714 to June 1830. The style was revived in the late 19th century in the United States as Colonial Revival architecture and in the early 20th century in Great Britain as Neo-Georgian architecture; in both it is also called Georgian Revival architecture. In America the term "Georgian" is generally used to describe all building from the period, regardless of style; in Britain it is generally restricted to buildings that are "architectural in intention", and have stylistic characteristics that are typical of the period, though that covers a wide range.
The style of Georgian buildings is very variable, but marked by a taste for symmetry and proportion based on the classical architecture of Greece and Rome, as revived in Renaissance architecture. Ornament is also normally in the classical tradition, but typically rather restrained, and sometimes almost completely absent on the exterior. The period brought the vocabulary of classical architecture to smaller and more modest buildings than had been the case before, replacing English vernacular architecture (or becoming the new vernacular style) for almost all new middle-class homes and public buildings by the end of the period.
In towns, which expanded greatly during the period, landowners turned into property developers, and rows of identical terraced houses became the norm. Even the wealthy were persuaded to live in these in town, especially if provided with a square of garden in front of the house. There was an enormous amount of building in the period, all over the English-speaking world, and the standards of construction were generally high. Where they have not been demolished, large numbers of Georgian buildings have survived two centuries or more, and they still form large parts of the core of cities such as London, Edinburgh, Dublin and Bristol.
The period saw the growth of a distinct and trained architectural profession; before the mid-century "the high-sounding title, 'architect' was adopted by anyone who could get away with it". But most buildings were still designed by builders and landlords together, and the wide spread of Georgian architecture, and the Georgian styles of design more generally, came from dissemination through pattern books and inexpensive suites of engravings. This contrasted with earlier styles, which were primarily disseminated among craftsmen through the direct experience of the apprenticeship system. Authors such as the prolific William Halfpenny (active 1723–1755) received editions in America as well as Britain. From the mid-18th century, Georgian styles were assimilated into an architectural vernacular that became part and parcel of the training of every architect, designer, builder, carpenter, mason and plasterer, from Edinburgh to Maryland.
Georgian succeeded the English Baroque of Sir Christopher Wren, Sir John Vanbrugh, Thomas Archer, William Talman, and Nicholas Hawksmoor; this in fact continued into at least the 1720s, overlapping with a more restrained Georgian style. The architect James Gibbs was a transitional figure, his earlier buildings are Baroque, reflecting the time he spent in Rome in the early 18th century, but he adjusted his style after 1720. Major architects to promote the change in direction from baroque were Colen Campbell, author of the influential book Vitruvius Britannicus (1715-1725); Richard Boyle, 3rd Earl of Burlington and his protégé William Kent; Isaac Ware; Henry Flitcroft and the Venetian Giacomo Leoni, who spent most of his career in England. Other prominent architects of the early Georgian period include James Paine, Robert Taylor, and John Wood, the Elder. The European Grand Tour became very common for wealthy patrons in the period, and Italian influence remained dominant, though at the start of the period Hanover Square, Westminster (1713 on), developed and occupied by Whig supporters of the new dynasty, seems to have deliberately adopted German stylisic elements in their honour, especially vertical bands connecting the windows.
The styles that resulted fall within several categories. In the mainstream of Georgian style were both Palladian architecture— and its whimsical alternatives, Gothic and Chinoiserie, which were the English-speaking world's equivalent of European Rococo. From the mid-1760s a range of Neoclassical modes were fashionable, associated with the British architects Robert Adam, James Gibbs, Sir William Chambers, James Wyatt, George Dance the Younger, Henry Holland and Sir John Soane. John Nash was one of the most prolific architects of the late Georgian era known as The Regency style, he was responsible for designing large areas of London. Greek Revival architecture was added to the repertory, beginning around 1750, but increasing in popularity after 1800. Leading exponents were William Wilkins and Robert Smirke.
Georgian architecture is characterized by its proportion and balance; simple mathematical ratios were used to determine the height of a window in relation to its width or the shape of a room as a double cube. Regularity, as with ashlar (uniformly cut) stonework, was strongly approved, imbuing symmetry and adherence to classical rules: the lack of symmetry, where Georgian additions were added to earlier structures remaining visible, was deeply felt as a flaw, at least before Nash began to introduce it in a variety of styles. Regularity of housefronts along a street was a desirable feature of Georgian town planning. Until the start of the Gothic Revival in the early 19th century, Georgian designs usually lay within the Classical orders of architecture and employed a decorative vocabulary derived from ancient Rome or Greece.
Versions of revived Palladian architecture dominated English country house architecture. Houses were increasingly placed in grand landscaped settings, and large houses were generally made wide and relatively shallow, largely to look more impressive from a distance. The height was usually highest in the centre, and the Baroque emphasis on corner pavilions often found on the continent generally avoided. In grand houses, an entrance hall led to steps up to a piano nobile or mezzanine floor where the main reception rooms were. Typically the basement area or "rustic", with kitchens, offices and service areas, as well as male guests with muddy boots, came some way above ground, and was lit by windows that were high on the inside, but just above ground level outside. A single block was typical, with a perhaps a small court for carriages at the front marked off by railings and a gate, but rarely a stone gatehouse, or side wings around the court.
Windows in all types of buildings were large and regularly placed on a grid; this was partly to minimize window tax, which was in force throughout the period in the United Kingdom. Some windows were subsequently bricked-in. Their height increasingly varied between the floors, and they increasingly began below waist-height in the main rooms, making a small balcony desirable. Before this the internal plan and function of the rooms can generally not be deduced from the outside. To open these large windows the sash window, already developed by the 1670s, became very widespread. Corridor plans became universal inside larger houses.
Internal courtyards became more rare, except beside the stables, and the functional parts of the building were placed at the sides, or in separate buildings nearby hidden by trees. The views to and from the front and rear of the main block were concentrated on, with the side approaches usually much less important. The roof was typically invisible from the ground, though domes were sometimes visible in grander buildings. The roofline was generally clear of ornament except for a balustrade or the top of a pediment. Columns or pilasters, often topped by a pediment, were popular for ornament inside and out, and other ornament was generally geometrical or plant-based, rather than using the human figure.
Inside ornament was far more generous, and could sometimes be overwhelming. The chimneypiece continued to be the usual main focus of rooms, and was now given a classical treatment, and increasingly topped by a painting or a mirror. Plasterwork ceilings, carved wood, and bold schemes of wallpaint formed a backdrop to increasingly rich collections of furniture, paintings, porcelain, mirrors, and objets d'art of all kinds. Wood-panelling, very common since about 1500, fell from favour around the mid-century, and wallpaper included very expensive imports from China.
In towns even most better-off people lived in terraced houses, which typically opened straight onto the street, often with a few steps up to the door. There was often an open space, protected by iron railings, dropping down to the basement level, with a discreet entrance down steps off the street for servants and deliveries; this is known as the "area". This meant that the ground floor front was now removed and protected from the street and encouraged the main reception rooms to move there from the floor above. Where, as often, a new street or set of streets was developed, the road and pavements were raised up, and the gardens or yards behind the houses at a lower level, usually representing the original one.
Town terraced houses for all social classes remained resolutely tall and narrow, each dwelling occupying the whole height of the building. This contrasted with well-off continental dwellings, which had already begun to be formed of wide apartments occupying only one or two floors of a building; such arrangements were only typical in England when housing groups of batchelors, as in Oxbridge colleges, the lawyers in the Inns of Court or The Albany after it was converted in 1802. In the period in question, only in Edinburgh were working-class purpose-built tenements common, though lodgers were common in other cities. A curving crescent, often looking out at gardens or a park, was popular for terraces where space allowed. In early and central schemes of development, plots were sold and built on individually, though there was often an attempt to enforce some uniformity, but as development reached further out schemes were increasingly built as a uniform scheme and then sold.
The late Georgian period saw the birth of the semi-detached house, planned systematically, as a suburban compromise between the terraced houses of the city and the detached "villas" further out, where land was cheaper. There had been occasional examples in town centres going back to medieval times. Most early suburban examples are large, and in what are now the outer fringes of Central London, but were then in areas being built up for the first time. Blackheath, Chalk Farm and St John's Wood are among the areas contesting being the original home of the semi. Sir John Summerson gave primacy to the Eyre Estate of St John's Wood. A plan for this exists dated 1794, where "the whole development consists of pairs of semi-detached houses, So far as I know, this is the first recorded scheme of the kind". In fact the French Wars put an end to this scheme, but when the development was finally built it retained the semi-detached form, "a revolution of striking significance and far-reaching effect".
Until the Church Building Act of 1818, the period saw relatively few churches built in Britain, which was already well-supplied, although in the later years of the period the demand for Non-conformist and Roman Catholic places of worship greatly increased. Anglican churches that were built were designed internally to allow maximum audibility, and visibility, for preaching, so the main nave was generally wider and shorter than in medieval plans, and often there were no side-aisles. Galleries were common in new churches. Especially in country parishes, the external appearance generally retained the familiar signifiers of a Gothic church, with a tower or spire, a large west front with one or more doors, and very large windows along the nave, but all with any ornament drawn from the classical vocabulary. Where funds permitted, a classical temple portico with columns and a pediment might be used at the west front. Decoration inside was very limited, but churches filled up with monuments to the prosperous.
Public buildings generally varied between the extremes of plain boxes with grid windows and Italian Late Renaissance palaces, depending on budget. Somerset House in London, designed by Sir William Chambers in 1776 for government offices, was as magnificent as any country house, though never quite finished, as funds ran out. Barracks and other less prestigious buildings could be as functional as the mills and factories that were growing increasingly large by the end of the period. But as the period came to an end many commercial projects were becoming sufficiently large, and well-funded, to become "architectural in intention", rather than having their design left to the lesser class of "surveyors".
Georgian architecture was widely disseminated in the English colonies during the Georgian era. American buildings of the Georgian period were very often constructed of wood with clapboards; even columns were made of timber, framed up, and turned on an over-sized lathe. At the start of the period the difficulties of obtaining and transporting brick or stone made them a common alternative only in the larger cities, or where they were obtainable locally. Dartmouth College, Harvard University, and the College of William and Mary, offer leading examples of Georgian architecture in the Americas.
Unlike the Baroque style that it replaced, which was mostly used for palaces and churches, and had little representation in the British colonies, simpler Georgian styles were widely used by the upper and middle classes. Perhaps the best remaining house is the pristine Hammond-Harwood House (1774) in Annapolis, Maryland, designed by the colonial architect William Buckland and modelled on the Villa Pisani at Montagnana, Italy as depicted in Andrea Palladio's I quattro libri dell'architettura ("Four Books of Architecture").
After about 1840, Georgian conventions were slowly abandoned as a number of revival styles, including Gothic Revival, that had originated in the Georgian period, developed and contested in Victorian architecture, and in the case of Gothic became better researched, and closer to their originals. Neoclassical architecture remained popular, and was the opponent of Gothic in the Battle of the Styles of the early Victorian period. In the United States the Federalist Style contained many elements of Georgian style, but incorporated revolutionary symbols.
In the early decades of the twentieth century when there was a growing nostalgia for its sense of order, the style was revived and adapted and in the United States came to be known as the Colonial Revival. In Canada the United Empire Loyalists embraced Georgian architecture as a sign of their fealty to Britain, and the Georgian style was dominant in the country for most of the first half of the 19th century. The Grange, for example, a manor built in Toronto, was built in 1817. In Montreal, English born architect John Ostell worked on a significant number of remarkable constructions in the Georgian style such as the Old Montreal Custom House and the Grand séminaire de Montréal.
The revived Georgian style that emerged in Britain at the beginning of the 20th century is usually referred to as Neo-Georgian; the work of Edwin Lutyens includes many examples. Versions of the Neo-Georgian style were commonly used in Britain for certain types of urban architecture until the late 1950s, Bradshaw Gass & Hope's Police Headquarters in Salford of 1958 being a good example. In both the United States and Britain, the Georgian style is still employed by architects like Quinlan Terry Julian Bicknell and Fairfax and Sammons for private residences.
Tennessee (i/tɛnᵻˈsiː/) (Cherokee: ᏔᎾᏏ, Tanasi) is a state located in the southeastern United States. Tennessee is the 36th largest and the 17th most populous of the 50 United States. Tennessee is bordered by Kentucky and Virginia to the north, North Carolina to the east, Georgia, Alabama, and Mississippi to the south, and Arkansas and Missouri to the west. The Appalachian Mountains dominate the eastern part of the state, and the Mississippi River forms the state's western border. Tennessee's capital and second largest city is Nashville, which has a population of 601,222. Memphis is the state's largest city, with a population of 653,450.
The state of Tennessee is rooted in the Watauga Association, a 1772 frontier pact generally regarded as the first constitutional government west of the Appalachians. What is now Tennessee was initially part of North Carolina, and later part of the Southwest Territory. Tennessee was admitted to the Union as the 16th state on June 1, 1796. Tennessee was the last state to leave the Union and join the Confederacy at the outbreak of the U.S. Civil War in 1861. Occupied by Union forces from 1862, it was the first state to be readmitted to the Union at the end of the war.
Tennessee furnished more soldiers for the Confederate Army than any other state, and more soldiers for the Union Army than any other Southern state. Beginning during Reconstruction, it had competitive party politics, but a Democratic takeover in the late 1880s resulted in passage of disfranchisement laws that excluded most blacks and many poor whites from voting. This sharply reduced competition in politics in the state until after passage of civil rights legislation in the mid-20th century. In the 20th century, Tennessee transitioned from an agrarian economy to a more diversified economy, aided by massive federal investment in the Tennessee Valley Authority and, in the early 1940s, the city of Oak Ridge. This city was established to house the Manhattan Project's uranium enrichment facilities, helping to build the world's first atomic bomb, which was used during World War II.
Tennessee has played a critical role in the development of many forms of American popular music, including rock and roll, blues, country, and rockabilly.[not verified in body] Beale Street in Memphis is considered by many to be the birthplace of the blues, with musicians such as W.C. Handy performing in its clubs as early as 1909.[not verified in body] Memphis is also home to Sun Records, where musicians such as Elvis Presley, Johnny Cash, Carl Perkins, Jerry Lee Lewis, Roy Orbison, and Charlie Rich began their recording careers, and where rock and roll took shape in the 1950s.[not verified in body] The 1927 Victor recording sessions in Bristol generally mark the beginning of the country music genre and the rise of the Grand Ole Opry in the 1930s helped make Nashville the center of the country music recording industry.[not verified in body] Three brick-and-mortar museums recognize Tennessee's role in nurturing various forms of popular music: the Memphis Rock N' Soul Museum, the Country Music Hall of Fame and Museum in Nashville, and the International Rock-A-Billy Museum in Jackson. Moreover, the Rockabilly Hall of Fame, an online site recognizing the development of rockabilly in which Tennessee played a crucial role, is based in Nashville.[not verified in body]
Tennessee's major industries include agriculture, manufacturing, and tourism. Poultry, soybeans, and cattle are the state's primary agricultural products, and major manufacturing exports include chemicals, transportation equipment, and electrical equipment. The Great Smoky Mountains National Park, the nation's most visited national park, is headquartered in the eastern part of the state, and a section of the Appalachian Trail roughly follows the Tennessee-North Carolina border. Other major tourist attractions include the Tennessee Aquarium in Chattanooga; Dollywood in Pigeon Forge; the Parthenon, the Country Music Hall of Fame and Museum, and Ryman Auditorium in Nashville; the Jack Daniel's Distillery in Lynchburg; and Elvis Presley's Graceland residence and tomb, the Memphis Zoo, and the National Civil Rights Museum in Memphis.
The earliest variant of the name that became Tennessee was recorded by Captain Juan Pardo, the Spanish explorer, when he and his men passed through an American Indian village named "Tanasqui" in 1567 while traveling inland from South Carolina. In the early 18th century, British traders encountered a Cherokee town named Tanasi (or "Tanase") in present-day Monroe County, Tennessee. The town was located on a river of the same name (now known as the Little Tennessee River), and appears on maps as early as 1725. It is not known whether this was the same town as the one encountered by Juan Pardo, although recent research suggests that Pardo's "Tanasqui" was located at the confluence of the Pigeon River and the French Broad River, near modern Newport.
The modern spelling, Tennessee, is attributed to James Glen, the governor of South Carolina, who used this spelling in his official correspondence during the 1750s. The spelling was popularized by the publication of Henry Timberlake's "Draught of the Cherokee Country" in 1765. In 1788, North Carolina created "Tennessee County", the third county to be established in what is now Middle Tennessee. (Tennessee County was the predecessor to current-day Montgomery County and Robertson County.) When a constitutional convention met in 1796 to organize a new state out of the Southwest Territory, it adopted "Tennessee" as the name of the state.
Tennessee is known as the "Volunteer State", a nickname some claimed was earned during the War of 1812 because of the prominent role played by volunteer soldiers from Tennessee, especially during the Battle of New Orleans. Other sources differ on the origin of the state nickname; according to the Columbia Encyclopedia, the name refers to volunteers for the Mexican–American War. This explanation is more likely, because President Polk's call for 2,600 nationwide volunteers at the beginning of the Mexican-American War resulted in 30,000 volunteers from Tennessee alone, largely in response to the death of Davy Crockett and appeals by former Tennessee Governor and now Texas politician, Sam Houston.
The highest point in the state is Clingmans Dome at 6,643 feet (2,025 m). Clingmans Dome, which lies on Tennessee's eastern border, is the highest point on the Appalachian Trail, and is the third highest peak in the United States east of the Mississippi River. The state line between Tennessee and North Carolina crosses the summit. The state's lowest point is the Mississippi River at the Mississippi state line (the lowest point in Memphis, nearby, is at 195 ft (59 m)). The geographical center of the state is located in Murfreesboro.
Stretching west from the Blue Ridge for approximately 55 miles (89 km) is the Ridge and Valley region, in which numerous tributaries join to form the Tennessee River in the Tennessee Valley. This area of Tennessee is covered by fertile valleys separated by wooded ridges, such as Bays Mountain and Clinch Mountain. The western section of the Tennessee Valley, where the depressions become broader and the ridges become lower, is called the Great Valley. In this valley are numerous towns and two of the region's three urban areas, Knoxville, the 3rd largest city in the state, and Chattanooga, the 4th largest city in the state. The third urban area, the Tri-Cities, comprising Bristol, Johnson City, and Kingsport and their environs, is located to the northeast of Knoxville.
East Tennessee has several important transportation links with Middle and West Tennessee, as well as the rest of the nation and the world, including several major airports and interstates. Knoxville's McGhee Tyson Airport (TYS) and Chattanooga's Chattanooga Metropolitan Airport (CHA), as well as the Tri-Cities' Tri-Cities Regional Airport (TRI), provide air service to numerous destinations. I-24, I-81, I-40, I-75, and I-26 along with numerous state highways and other important roads, traverse the Grand Division and connect Chattanooga, Knoxville, and the Tri-Cities, along with other cities and towns such as Cleveland, Athens, and Sevierville.
The easternmost section, about 10 miles (16 km) in width, consists of hilly land that runs along the western bank of the Tennessee River. To the west of this narrow strip of land is a wide area of rolling hills and streams that stretches all the way to the Mississippi River; this area is called the Tennessee Bottoms or bottom land. In Memphis, the Tennessee Bottoms end in steep bluffs overlooking the river. To the west of the Tennessee Bottoms is the Mississippi Alluvial Plain, less than 300 feet (90 m) above sea level. This area of lowlands, flood plains, and swamp land is sometimes referred to as the Delta region. Memphis is the economic center of West Tennessee and the largest city in the state.
Most of the state has a humid subtropical climate, with the exception of some of the higher elevations in the Appalachians, which are classified as having a mountain temperate climate or a humid continental climate due to cooler temperatures. The Gulf of Mexico is the dominant factor in the climate of Tennessee, with winds from the south being responsible for most of the state's annual precipitation. Generally, the state has hot summers and mild to cool winters with generous precipitation throughout the year, with highest average monthly precipitation generally in the winter and spring months, between December and April. The driest months, on average, are August to October. On average the state receives 50 inches (130 cm) of precipitation annually. Snowfall ranges from 5 inches (13 cm) in West Tennessee to over 16 inches (41 cm) in the higher mountains in East Tennessee.
Summers in the state are generally hot and humid, with most of the state averaging a high of around 90 °F (32 °C) during the summer months. Winters tend to be mild to cool, increasing in coolness at higher elevations. Generally, for areas outside the highest mountains, the average overnight lows are near freezing for most of the state. The highest recorded temperature is 113 °F (45 °C) at Perryville on August 9, 1930 while the lowest recorded temperature is −32 °F (−36 °C) at Mountain City on December 30, 1917.
While the state is far enough from the coast to avoid any direct impact from a hurricane, the location of the state makes it likely to be impacted from the remnants of tropical cyclones which weaken over land and can cause significant rainfall, such as Tropical Storm Chris in 1982 and Hurricane Opal in 1995. The state averages around 50 days of thunderstorms per year, some of which can be severe with large hail and damaging winds. Tornadoes are possible throughout the state, with West and Middle Tennessee the most vulnerable. Occasionally, strong or violent tornadoes occur, such as the devastating April 2011 tornadoes that killed 20 people in North Georgia and Southeast Tennessee. On average, the state has 15 tornadoes per year. Tornadoes in Tennessee can be severe, and Tennessee leads the nation in the percentage of total tornadoes which have fatalities. Winter storms are an occasional problem, such as the infamous Blizzard of 1993, although ice storms are a more likely occurrence. Fog is a persistent problem in parts of the state, especially in East Tennessee.
The capital is Nashville, though Knoxville, Kingston, and Murfreesboro have all served as state capitals in the past. Memphis has the largest population of any city in the state. Nashville's 13-county metropolitan area has been the state's largest since c. 1990. Chattanooga and Knoxville, both in the eastern part of the state near the Great Smoky Mountains, each has approximately one-third of the population of Memphis or Nashville. The city of Clarksville is a fifth significant population center, some 45 miles (72 km) northwest of Nashville. Murfreesboro is the sixth-largest city in Tennessee, consisting of some 108,755 residents.
The area now known as Tennessee was first inhabited by Paleo-Indians nearly 12,000 years ago. The names of the cultural groups that inhabited the area between first settlement and the time of European contact are unknown, but several distinct cultural phases have been named by archaeologists, including Archaic (8000–1000 BC), Woodland (1000 BC–1000 AD), and Mississippian (1000–1600 AD), whose chiefdoms were the cultural predecessors of the Muscogee people who inhabited the Tennessee River Valley before Cherokee migration into the river's headwaters.
The first recorded European excursions into what is now called Tennessee were three expeditions led by Spanish explorers, namely Hernando de Soto in 1540, Tristan de Luna in 1559, and Juan Pardo in 1567. Pardo recorded the name "Tanasqui" from a local Indian village, which evolved to the state's current name. At that time, Tennessee was inhabited by tribes of Muscogee and Yuchi people. Possibly because of European diseases devastating the Indian tribes, which would have left a population vacuum, and also from expanding European settlement in the north, the Cherokee moved south from the area now called Virginia. As European colonists spread into the area, the Indian populations were forcibly displaced to the south and west, including all Muscogee and Yuchi peoples, the Chickasaw and Choctaw, and ultimately, the Cherokee in 1838.
The first British settlement in what is now Tennessee was built in 1756 by settlers from the colony of South Carolina at Fort Loudoun, near present-day Vonore. Fort Loudoun became the westernmost British outpost to that date. The fort was designed by John William Gerard de Brahm and constructed by forces under British Captain Raymond Demeré. After its completion, Captain Raymond Demeré relinquished command on August 14, 1757 to his brother, Captain Paul Demeré. Hostilities erupted between the British and the neighboring Overhill Cherokees, and a siege of Fort Loudoun ended with its surrender on August 7, 1760. The following morning, Captain Paul Demeré and a number of his men were killed in an ambush nearby, and most of the rest of the garrison was taken prisoner.
During the American Revolutionary War, Fort Watauga at Sycamore Shoals (in present-day Elizabethton) was attacked (1776) by Dragging Canoe and his warring faction of Cherokee who were aligned with the British Loyalists. These renegade Cherokee were referred to by settlers as the Chickamauga. They opposed North Carolina's annexation of the Washington District and the concurrent settling of the Transylvania Colony further north and west. The lives of many settlers were spared from the initial warrior attacks through the warnings of Dragging Canoe's cousin, Nancy Ward. The frontier fort on the banks of the Watauga River later served as a 1780 staging area for the Overmountain Men in preparation to trek over the Appalachian Mountains, to engage, and to later defeat the British Army at the Battle of Kings Mountain in South Carolina.
Three counties of the Washington District (now part of Tennessee) broke off from North Carolina in 1784 and formed the State of Franklin. Efforts to obtain admission to the Union failed, and the counties (now numbering eight) had re-joined North Carolina by 1789. North Carolina ceded the area to the federal government in 1790, after which it was organized into the Southwest Territory. In an effort to encourage settlers to move west into the new territory, in 1787 the mother state of North Carolina ordered a road to be cut to take settlers into the Cumberland Settlements—from the south end of Clinch Mountain (in East Tennessee) to French Lick (Nashville). The Trace was called the "North Carolina Road" or "Avery's Trace", and sometimes "The Wilderness Road" (although it should not be confused with Daniel Boone's "Wilderness Road" through the Cumberland Gap).
Tennessee was admitted to the Union on June 1, 1796 as the 16th state. It was the first state created from territory under the jurisdiction of the United States federal government. Apart from the former Thirteen Colonies only Vermont and Kentucky predate Tennessee's statehood, and neither was ever a federal territory. The state boundaries, according to the Constitution of the State of Tennessee, Article I, Section 31, stated that the beginning point for identifying the boundary was the extreme height of the Stone Mountain, at the place where the line of Virginia intersects it, and basically ran the extreme heights of mountain chains through the Appalachian Mountains separating North Carolina from Tennessee past the Indian towns of Cowee and Old Chota, thence along the main ridge of the said mountain (Unicoi Mountain) to the southern boundary of the state; all the territory, lands and waters lying west of said line are included in the boundaries and limits of the newly formed state of Tennessee. Part of the provision also stated that the limits and jurisdiction of the state would include future land acquisition, referencing possible land trade with other states, or the acquisition of territory from west of the Mississippi River.
During the administration of U.S. President Martin Van Buren, nearly 17,000 Cherokees—along with approximately 2,000 black slaves owned by Cherokees—were uprooted from their homes between 1838 and 1839 and were forced by the U.S. military to march from "emigration depots" in Eastern Tennessee (such as Fort Cass) toward the more distant Indian Territory west of Arkansas. During this relocation an estimated 4,000 Cherokees died along the way west. In the Cherokee language, the event is called Nunna daul Isunyi—"the Trail Where We Cried." The Cherokees were not the only American Indians forced to emigrate as a result of the Indian removal efforts of the United States, and so the phrase "Trail of Tears" is sometimes used to refer to similar events endured by other American Indian peoples, especially among the "Five Civilized Tribes". The phrase originated as a description of the earlier emigration of the Choctaw nation.
In February 1861, secessionists in Tennessee's state government—led by Governor Isham Harris—sought voter approval for a convention to sever ties with the United States, but Tennessee voters rejected the referendum by a 54–46% margin. The strongest opposition to secession came from East Tennessee (which later tried to form a separate Union-aligned state). Following the Confederate attack upon Fort Sumter in April and Lincoln's call for troops from Tennessee and other states in response, Governor Isham Harris began military mobilization, submitted an ordinance of secession to the General Assembly, and made direct overtures to the Confederate government. The Tennessee legislature ratified an agreement to enter a military league with the Confederate States on May 7, 1861. On June 8, 1861, with people in Middle Tennessee having significantly changed their position, voters approved a second referendum calling for secession, becoming the last state to do so.
Many major battles of the American Civil War were fought in Tennessee—most of them Union victories. Ulysses S. Grant and the U.S. Navy captured control of the Cumberland and Tennessee rivers in February 1862. They held off the Confederate counterattack at Shiloh in April. Memphis fell to the Union in June, following a naval battle on the Mississippi River in front of the city. The Capture of Memphis and Nashville gave the Union control of the western and middle sections; this control was confirmed at the Battle of Murfreesboro in early January 1863 and by the subsequent Tullahoma Campaign.
Confederates held East Tennessee despite the strength of Unionist sentiment there, with the exception of extremely pro-Confederate Sullivan County. The Confederates, led by General James Longstreet, did attack General Burnside's Fort Sanders at Knoxville and lost. It was a big blow to East Tennessee Confederate momentum, but Longstreet won the Battle of Bean's Station a few weeks later. The Confederates besieged Chattanooga during the Chattanooga Campaign in early fall 1863, but were driven off by Grant in November. Many of the Confederate defeats can be attributed to the poor strategic vision of General Braxton Bragg, who led the Army of Tennessee from Perryville, Kentucky to another Confederate defeat at Chattanooga.
When the Emancipation Proclamation was announced, Tennessee was mostly held by Union forces. Thus, Tennessee was not among the states enumerated in the Proclamation, and the Proclamation did not free any slaves there. Nonetheless, enslaved African Americans escaped to Union lines to gain freedom without waiting for official action. Old and young, men, women and children camped near Union troops. Thousands of former slaves ended up fighting on the Union side, nearly 200,000 in total across the South.
In 1864, Andrew Johnson (a War Democrat from Tennessee) was elected Vice President under Abraham Lincoln. He became President after Lincoln's assassination in 1865. Under Johnson's lenient re-admission policy, Tennessee was the first of the seceding states to have its elected members readmitted to the U.S. Congress, on July 24, 1866. Because Tennessee had ratified the Fourteenth Amendment, it was the only one of the formerly secessionist states that did not have a military governor during the Reconstruction period.
After the formal end of Reconstruction, the struggle over power in Southern society continued. Through violence and intimidation against freedmen and their allies, White Democrats regained political power in Tennessee and other states across the South in the late 1870s and 1880s. Over the next decade, the state legislature passed increasingly restrictive laws to control African Americans. In 1889 the General Assembly passed four laws described as electoral reform, with the cumulative effect of essentially disfranchising most African Americans in rural areas and small towns, as well as many poor Whites. Legislation included implementation of a poll tax, timing of registration, and recording requirements. Tens of thousands of taxpaying citizens were without representation for decades into the 20th century. Disfranchising legislation accompanied Jim Crow laws passed in the late 19th century, which imposed segregation in the state. In 1900, African Americans made up nearly 24% of the state's population, and numbered 480,430 citizens who lived mostly in the central and western parts of the state.
In 2002, businessman Phil Bredesen was elected as the 48th governor. Also in 2002, Tennessee amended the state constitution to allow for the establishment of a lottery. Tennessee's Bob Corker was the only freshman Republican elected to the United States Senate in the 2006 midterm elections. The state constitution was amended to reject same-sex marriage. In January 2007, Ron Ramsey became the first Republican elected as Speaker of the State Senate since Reconstruction, as a result of the realignment of the Democratic and Republican parties in the South since the late 20th century, with Republicans now elected by conservative voters, who previously had supported Democrats.
According to the U.S. Census Bureau, as of 2015, Tennessee had an estimated population of 6,600,299, which is an increase of 50,947, from the prior year and an increase of 254,194, or 4.01%, since the year 2010. This includes a natural increase since the last census of 142,266 people (that is 493,881 births minus 351,615 deaths), and an increase from net migration of 219,551 people into the state. Immigration from outside the United States resulted in a net increase of 59,385 people, and migration within the country produced a net increase of 160,166 people. Twenty percent of Tennesseans were born outside the South in 2008, compared to a figure of 13.5% in 1990.
In 2000, the five most common self-reported ethnic groups in the state were: American (17.3%), African American (13.0%), Irish (9.3%), English (9.1%), and German (8.3%). Most Tennesseans who self-identify as having American ancestry are of English and Scotch-Irish ancestry. An estimated 21–24% of Tennesseans are of predominantly English ancestry. In the 1980 census 1,435,147 Tennesseans claimed "English" or "mostly English" ancestry out of a state population of 3,221,354 making them 45% of the state at the time.
Tennessee is home to several Protestant denominations, such as the National Baptist Convention (headquartered in Nashville); the Church of God in Christ and the Cumberland Presbyterian Church (both headquartered in Memphis); the Church of God and The Church of God of Prophecy (both headquartered in Cleveland). The Free Will Baptist denomination is headquartered in Antioch; its main Bible college is in Nashville. The Southern Baptist Convention maintains its general headquarters in Nashville. Publishing houses of several denominations are located in Nashville.
Major outputs for the state include textiles, cotton, cattle, and electrical power. Tennessee has over 82,000 farms, roughly 59 percent of which accommodate beef cattle. Although cotton was an early crop in Tennessee, large-scale cultivation of the fiber did not begin until the 1820s with the opening of the land between the Tennessee and Mississippi Rivers. The upper wedge of the Mississippi Delta extends into southwestern Tennessee, and it was in this fertile section that cotton took hold. Soybeans are also heavily planted in West Tennessee, focusing on the northwest corner of the state.
Major corporations with headquarters in Tennessee include FedEx, AutoZone and International Paper, all based in Memphis; Pilot Corporation and Regal Entertainment Group, based in Knoxville; Eastman Chemical Company, based in Kingsport; the North American headquarters of Nissan Motor Company, based in Franklin; Hospital Corporation of America and Caterpillar Financial, based in Nashville; and Unum, based in Chattanooga. Tennessee is also the location of the Volkswagen factory in Chattanooga, a $2 billion polysilicon production facility by Wacker Chemie in Bradley County, and a $1.2 billion polysilicon production facility by Hemlock Semiconductor in Clarksville.
The Tennessee income tax does not apply to salaries and wages, but most income from stock, bonds and notes receivable is taxable. All taxable dividends and interest which exceed the $1,250 single exemption or the $2,500 joint exemption are taxable at the rate of 6%. The state's sales and use tax rate for most items is 7%. Food is taxed at a lower rate of 5.25%, but candy, dietary supplements and prepared food are taxed at the full 7% rate. Local sales taxes are collected in most jurisdictions, at rates varying from 1.5% to 2.75%, bringing the total sales tax to between 8.5% and 9.75%, one of the highest levels in the nation. Intangible property is assessed on the shares of stock of stockholders of any loan company, investment company, insurance company or for-profit cemetery companies. The assessment ratio is 40% of the value multiplied by the tax rate for the jurisdiction. Tennessee imposes an inheritance tax on decedents' estates that exceed maximum single exemption limits ($1,000,000 for deaths in 2006 and thereafter).
Tourism contributes billions of dollars each year to the state's economy and Tennessee is ranked among the Top 10 destinations in the US. In 2014 a record 100 million people visited the state resulting in $17.7 billion in tourism related spending within the state, an increase of 6.3% over 2013; tax revenue from tourism equaled $1.5 billion. Each county in Tennessee saw at least $1 million from tourism while 19 counties received at least $100 million (Davidson, Shelby, and Sevier counties were the top three). Tourism-generated jobs for the state reached 152,900, a 2.8% increase. International travelers to Tennessee accounted for $533 million in spending.
In 2013 tourism within the state from local citizens accounted for 39.9% of tourists, the second highest originating location for tourists to Tennessee is the state of Georgia, accounting for 8.4% of tourists.:17 Forty-four percent of stays in the state were "day trips", 25% stayed one night, 15% stayed two nights, and 11% stayed 4 or more nights. The average stay was 2.16 nights, compared to 2.03 nights for the US as a whole.:40 The average person spent $118 per day: 29% on transportation, 24% on food, 17% on accommodation, and 28% on shopping and entertainment.:44
Interstate 40 crosses the state in a west-east orientation. Its branch interstate highways include I-240 in Memphis; I-440 in Nashville; I-140 from Knoxville to Alcoa and I-640 in Knoxville. I-26, although technically an east-west interstate, runs from the North Carolina border below Johnson City to its terminus at Kingsport. I-24 is an east-west interstate that runs cross-state from Chattanooga to Clarksville. In a north-south orientation are highways I-55, I-65, I-75, and I-81. Interstate 65 crosses the state through Nashville, while Interstate 75 serves Chattanooga and Knoxville and Interstate 55 serves Memphis. Interstate 81 enters the state at Bristol and terminates at its junction with I-40 near Dandridge. I-155 is a branch highway from I-55. The only spur highway of I-75 in Tennessee is I-275, which is in Knoxville. When completed, I-69 will travel through the western part of the state, from South Fulton to Memphis. A branch interstate, I-269 also exists from Millington to Collierville.
Tennessee politics, like that of most U.S. states, are dominated by the Republican and the Democratic parties. Historian Dewey W. Grantham traces divisions in the state to the period of the American Civil War: for decades afterward, the eastern third of the state was Republican and the western two thirds voted Democrat. This division was related to the state's pattern of farming, plantations and slaveholding. The eastern section was made up of yeoman farmers, but Middle and West Tennessee cultivated crops, such as tobacco and cotton, that were dependent on the use of slave labor. These areas became defined as Democratic after the war.
During Reconstruction, freedmen and former free people of color were granted the right to vote; most joined the Republican Party. Numerous African Americans were elected to local offices, and some to state office. Following Reconstruction, Tennessee continued to have competitive party politics. But in the 1880s, the white-dominated state government passed four laws, the last of which imposed a poll tax requirement for voter registration. These served to disenfranchise most African Americans, and their power in the Republican Party, the state, and cities where they had significant population was markedly reduced. In 1900 African Americans comprised 23.8 percent of the state's population, concentrated in Middle and West Tennessee. In the early 1900s, the state legislature approved a form of commission government for cities based on at-large voting for a few positions on a Board of Commission; several adopted this as another means to limit African-American political participation. In 1913 the state legislature enacted a bill enabling cities to adopt this structure without legislative approval.
After disenfranchisement of blacks, the GOP in Tennessee was historically a sectional party supported by whites only in the eastern part of the state. In the 20th century, except for two nationwide Republican landslides of the 1920s (in 1920, when Tennessee narrowly supported Warren G. Harding over Ohio Governor James Cox, and in 1928, when it more decisively voted for Herbert Hoover over New York Governor Al Smith, a Catholic), the state was part of the Democratic Solid South until the 1950s. In that postwar decade, it twice voted for Republican Dwight D. Eisenhower, former Allied Commander of the Armed Forces during World War II. Since then, more of the state's voters have shifted to supporting Republicans, and Democratic presidential candidates have carried Tennessee only four times.
By 1960 African Americans comprised 16.45% of the state's population. It was not until after the mid-1960s and passage of the Voting Rights Act of 1965 that they were able to vote in full again, but new devices, such as at-large commission city governments, had been adopted in several jurisdictions to limit their political participation. Former Gov. Winfield Dunn and former U.S. Sen. Bill Brock wins in 1970 helped make the Republican Party competitive among whites for the statewide victory. Tennessee has selected governors from different parties since 1970. Increasingly the Republican Party has become the party of white conservatives.
In the early 21st century, Republican voters control most of the state, especially in the more rural and suburban areas outside of the cities; Democratic strength is mostly confined to the urban cores of the four major cities, and is particularly strong in the cities of Nashville and Memphis. The latter area includes a large African-American population. Historically, Republicans had their greatest strength in East Tennessee before the 1960s. Tennessee's 1st and 2nd congressional districts, based in the Tri-Cities and Knoxville, respectively, are among the few historically Republican districts in the South. Those districts' residents supported the Union over the Confederacy during the Civil War; they identified with the GOP after the war and have stayed with that party ever since. The 1st has been in Republican hands continuously since 1881, and Republicans (or their antecedents) have held it for all but four years since 1859. The 2nd has been held continuously by Republicans or their antecedents since 1859.
In the 2000 presidential election, Vice President Al Gore, a former Democratic U.S. Senator from Tennessee, failed to carry his home state, an unusual occurrence but indicative of strengthening Republican support. Republican George W. Bush received increased support in 2004, with his margin of victory in the state increasing from 4% in 2000 to 14% in 2004. Democratic presidential nominees from Southern states (such as Lyndon B. Johnson, Jimmy Carter, Bill Clinton) usually fare better than their Northern counterparts do in Tennessee, especially among split-ticket voters outside the metropolitan areas.
The Baker v. Carr (1962) decision of the US Supreme Court established the principle of "one man, one vote", requiring state legislatures to redistrict to bring Congressional apportionment in line with decennial censuses. It also required both houses of state legislatures to be based on population for representation and not geographic districts such as counties. This case arose out of a lawsuit challenging the longstanding rural bias of apportionment of seats in the Tennessee legislature. After decades in which urban populations had been underrepresented in many state legislatures, this significant ruling led to an increased (and proportional) prominence in state politics by urban and, eventually, suburban, legislators and statewide officeholders in relation to their population within the state. The ruling also applied to numerous other states long controlled by rural minorities, such as Alabama, Vermont, and Montana.
The Highway Patrol is the primary law enforcement entity that concentrates on highway safety regulations and general non-wildlife state law enforcement and is under the jurisdiction of the Tennessee Department of Safety. The TWRA is an independent agency tasked with enforcing all wildlife, boating, and fisheries regulations outside of state parks. The TBI maintains state-of-the-art investigative facilities and is the primary state-level criminal investigative department. Tennessee State Park Rangers are responsible for all activities and law enforcement inside the Tennessee State Parks system.
Local law enforcement is divided between County Sheriff's Offices and Municipal Police Departments. Tennessee's Constitution requires that each County have an elected Sheriff. In 94 of the 95 counties the Sheriff is the chief law enforcement officer in the county and has jurisdiction over the county as a whole. Each Sheriff's Office is responsible for warrant service, court security, jail operations and primary law enforcement in the unincorporated areas of a county as well as providing support to the municipal police departments. Incorporated municipalities are required to maintain a police department to provide police services within their corporate limits.
Capital punishment has existed in Tennessee at various times since statehood. Before 1913 the method of execution was hanging. From 1913 to 1915 there was a hiatus on executions but they were reinstated in 1916 when electrocution became the new method. From 1972 to 1978, after the Supreme Court ruled (Furman v. Georgia) capital punishment unconstitutional, there were no further executions. Capital punishment was restarted in 1978, although those prisoners awaiting execution between 1960 and 1978 had their sentences mostly commuted to life in prison. From 1916 to 1960 the state executed 125 inmates. For a variety of reasons there were no further executions until 2000. Since 2000, Tennessee has executed six prisoners and has 73 prisoners on death row (as of April 2015).
In Knoxville, the Tennessee Volunteers college team has played in the Southeastern Conference of the National Collegiate Athletic Association since 1932. The football team has won 13 SEC championships and 25 bowls, including four Sugar Bowls, three Cotton Bowls, an Orange Bowl and a Fiesta Bowl. Meanwhile, the men's basketball team has won four SEC championships and reached the NCAA Elite Eight in 2010. In addition, the women's basketball team has won a host of SEC regular-season and tournament titles along with 8 national titles.
Humanism is a philosophical and ethical stance that emphasizes the value and agency of human beings, individually and collectively, and generally prefers critical thinking and evidence (rationalism, empiricism) over acceptance of dogma or superstition. The meaning of the term humanism has fluctuated according to the successive intellectual movements which have identified with it. Generally, however, humanism refers to a perspective that affirms some notion of human freedom and progress. In modern times, humanist movements are typically aligned with secularism, and today humanism typically refers to a non-theistic life stance centred on human agency and looking to science rather than revelation from a supernatural source to understand the world.
Gellius says that in his day humanitas is commonly used as a synonym for philanthropy – or kindness and benevolence toward one's fellow human being. Gellius maintains that this common usage is wrong, and that model writers of Latin, such as Cicero and others, used the word only to mean what we might call "humane" or "polite" learning, or the Greek equivalent Paideia. Gellius became a favorite author in the Italian Renaissance, and, in fifteenth-century Italy, teachers and scholars of philosophy, poetry, and rhetoric were called and called themselves "humanists". Modern scholars, however, point out that Cicero (106 – 43 BCE), who was most responsible for defining and popularizing the term humanitas, in fact frequently used the word in both senses, as did his near contemporaries. For Cicero, a lawyer, what most distinguished humans from brutes was speech, which, allied to reason, could (and should) enable them to settle disputes and live together in concord and harmony under the rule of law. Thus humanitas included two meanings from the outset and these continue in the modern derivative, humanism, which even today can refer to both humanitarian benevolence and to scholarship.
During the French Revolution, and soon after, in Germany (by the Left Hegelians), humanism began to refer to an ethical philosophy centered on humankind, without attention to the transcendent or supernatural. The designation Religious Humanism refers to organized groups that sprang up during the late-nineteenth and early twentieth centuries. It is similar to Protestantism, although centered on human needs, interests, and abilities rather than the supernatural. In the Anglophone world, such modern, organized forms of humanism, which are rooted in the 18th-century Enlightenment, have to a considerable extent more or less detached themselves from the historic connection of humanism with classical learning and the liberal arts.
In 1808 Bavarian educational commissioner Friedrich Immanuel Niethammer coined the term Humanismus to describe the new classical curriculum he planned to offer in German secondary schools, and by 1836 the word "humanism" had been absorbed into the English language in this sense. The coinage gained universal acceptance in 1856, when German historian and philologist Georg Voigt used humanism to describe Renaissance humanism, the movement that flourished in the Italian Renaissance to revive classical learning, a use which won wide acceptance among historians in many nations, especially Italy.
But in the mid-18th century, during the French Enlightenment, a more ideological use of the term had come into use. In 1765, the author of an anonymous article in a French Enlightenment periodical spoke of "The general love of humanity ... a virtue hitherto quite nameless among us, and which we will venture to call 'humanism', for the time has come to create a word for such a beautiful and necessary thing". The latter part of the 18th and the early 19th centuries saw the creation of numerous grass-roots "philanthropic" and benevolent societies dedicated to human betterment and the spreading of knowledge (some Christian, some not). After the French Revolution, the idea that human virtue could be created by human reason alone independently from traditional religious institutions, attributed by opponents of the Revolution to Enlightenment philosophes such as Rousseau, was violently attacked by influential religious and political conservatives, such as Edmund Burke and Joseph de Maistre, as a deification or idolatry of humanity. Humanism began to acquire a negative sense. The Oxford English Dictionary records the use of the word "humanism" by an English clergyman in 1812 to indicate those who believe in the "mere humanity" (as opposed to the divine nature) of Christ, i.e., Unitarians and Deists. In this polarised atmosphere, in which established ecclesiastical bodies tended to circle the wagons and reflexively oppose political and social reforms like extending the franchise, universal schooling, and the like, liberal reformers and radicals embraced the idea of Humanism as an alternative religion of humanity. The anarchist Proudhon (best known for declaring that "property is theft") used the word "humanism" to describe a "culte, déification de l’humanité" ("worship, deification of humanity") and Ernest Renan in L’avenir de la science: pensées de 1848 ("The Future of Knowledge: Thoughts on 1848") (1848–49), states: "It is my deep conviction that pure humanism will be the religion of the future, that is, the cult of all that pertains to humanity—all of life, sanctified and raised to the level of a moral value."
At about the same time, the word "humanism" as a philosophy centred on humankind (as opposed to institutionalised religion) was also being used in Germany by the so-called Left Hegelians, Arnold Ruge, and Karl Marx, who were critical of the close involvement of the church in the repressive German government. There has been a persistent confusion between the several uses of the terms: philanthropic humanists look to what they consider their antecedents in critical thinking and human-centered philosophy among the Greek philosophers and the great figures of Renaissance history; and scholarly humanists stress the linguistic and cultural disciplines needed to understand and interpret these philosophers and artists.
Another instance of ancient humanism as an organised system of thought is found in the Gathas of Zarathustra, composed between 1,000 BCE – 600 BCE in Greater Iran. Zarathustra's philosophy in the Gathas lays out a conception of humankind as thinking beings dignified with choice and agency according to the intellect which each receives from Ahura Mazda (God in the form of supreme wisdom). The idea of Ahura Mazda as a non-intervening deistic divine God/Grand Architect of the universe tied with a unique eschatology and ethical system implying that each person is held morally responsible for their choices, made freely in this present life, in the afterlife. The importance placed on thought, action, responsibility, and a non-intervening creator was appealed to by, and inspired a number of, Enlightenment humanist thinkers in Europe such as Voltaire and Montesquieu.
In China, Yellow Emperor is regarded as the humanistic primogenitor.[citation needed] Sage kings such as Yao and Shun are humanistic figures as recorded.[citation needed] King Wu of Zhou has the famous saying: "Humanity is the Ling (efficacious essence) of the world (among all)." Among them Duke of Zhou, respected as a founder of Rujia (Confucianism), is especially prominent and pioneering in humanistic thought. His words were recorded in the Book of History as follows (translation):[citation needed]
In the 6th century BCE, Taoist teacher Lao Tzu espoused a series of naturalistic concepts with some elements of humanistic philosophy. The Silver Rule of Confucianism from Analects XV.24, is an example of ethical philosophy based on human values rather than the supernatural. Humanistic thought is also contained in other Confucian classics, e.g., as recorded in Zuo Zhuan, Ji Liang says, "People is the zhu (master, lord, dominance, owner or origin) of gods. So, to sage kings, people first, gods second"; Neishi Guo says, "Gods, clever, righteous and wholehearted, comply with human." Taoist and Confucian secularism contain elements of moral thought devoid of religious authority or deism however they only partly resembled our modern concept of secularism.
6th-century BCE pre-Socratic Greek philosophers Thales of Miletus and Xenophanes of Colophon were the first in the region to attempt to explain the world in terms of human reason rather than myth and tradition, thus can be said to be the first Greek humanists. Thales questioned the notion of anthropomorphic gods and Xenophanes refused to recognise the gods of his time and reserved the divine for the principle of unity in the universe. These Ionian Greeks were the first thinkers to assert that nature is available to be studied separately from the supernatural realm. Anaxagoras brought philosophy and the spirit of rational inquiry from Ionia to Athens. Pericles, the leader of Athens during the period of its greatest glory was an admirer of Anaxagoras. Other influential pre-Socratics or rational philosophers include Protagoras (like Anaxagoras a friend of Pericles), known for his famous dictum "man is the measure of all things" and Democritus, who proposed that matter was composed of atoms. Little of the written work of these early philosophers survives and they are known mainly from fragments and quotations in other writers, principally Plato and Aristotle. The historian Thucydides, noted for his scientific and rational approach to history, is also much admired by later humanists. In the 3rd century BCE, Epicurus became known for his concise phrasing of the problem of evil, lack of belief in the afterlife, and human-centred approaches to achieving eudaimonia. He was also the first Greek philosopher to admit women to his school as a rule.
Renaissance humanism was an intellectual movement in Europe of the later Middle Ages and the Early Modern period. The 19th-century German historian Georg Voigt (1827–91) identified Petrarch as the first Renaissance humanist. Paul Johnson agrees that Petrarch was "the first to put into words the notion that the centuries between the fall of Rome and the present had been the age of Darkness". According to Petrarch, what was needed to remedy this situation was the careful study and imitation of the great classical authors. For Petrarch and Boccaccio, the greatest master was Cicero, whose prose became the model for both learned (Latin) and vernacular (Italian) prose.
In the high Renaissance, in fact, there was a hope that more direct knowledge of the wisdom of antiquity, including the writings of the Church fathers, the earliest known Greek texts of the Christian Gospels, and in some cases even the Jewish Kabbalah, would initiate a harmonious new era of universal agreement. With this end in view, Renaissance Church authorities afforded humanists what in retrospect appears a remarkable degree of freedom of thought. One humanist, the Greek Orthodox Platonist Gemistus Pletho (1355–1452), based in Mystras, Greece (but in contact with humanists in Florence, Venice, and Rome) taught a Christianised version of pagan polytheism.
The humanists' close study of Latin literary texts soon enabled them to discern historical differences in the writing styles of different periods. By analogy with what they saw as decline of Latin, they applied the principle of ad fontes, or back to the sources, across broad areas of learning, seeking out manuscripts of Patristic literature as well as pagan authors. In 1439, while employed in Naples at the court of Alfonso V of Aragon (at the time engaged in a dispute with the Papal States) the humanist Lorenzo Valla used stylistic textual analysis, now called philology, to prove that the Donation of Constantine, which purported to confer temporal powers on the Pope of Rome, was an 8th-century forgery. For the next 70 years, however, neither Valla nor any of his contemporaries thought to apply the techniques of philology to other controversial manuscripts in this way. Instead, after the fall of the Byzantine Empire to the Turks in 1453, which brought a flood of Greek Orthodox refugees to Italy, humanist scholars increasingly turned to the study of Neoplatonism and Hermeticism, hoping to bridge the differences between the Greek and Roman Churches, and even between Christianity itself and the non-Christian world. The refugees brought with them Greek manuscripts, not only of Plato and Aristotle, but also of the Christian Gospels, previously unavailable in the Latin West.
After 1517, when the new invention of printing made these texts widely available, the Dutch humanist Erasmus, who had studied Greek at the Venetian printing house of Aldus Manutius, began a philological analysis of the Gospels in the spirit of Valla, comparing the Greek originals with their Latin translations with a view to correcting errors and discrepancies in the latter. Erasmus, along with the French humanist Jacques Lefèvre d'Étaples, began issuing new translations, laying the groundwork for the Protestant Reformation. Henceforth Renaissance humanism, particularly in the German North, became concerned with religion, while Italian and French humanism concentrated increasingly on scholarship and philology addressed to a narrow audience of specialists, studiously avoiding topics that might offend despotic rulers or which might be seen as corrosive of faith. After the Reformation, critical examination of the Bible did not resume until the advent of the so-called Higher criticism of the 19th-century German Tübingen school.
The words of the comic playwright P. Terentius Afer reverberated across the Roman world of the mid-2nd century BCE and beyond. Terence, an African and a former slave, was well placed to preach the message of universalism, of the essential unity of the human race, that had come down in philosophical form from the Greeks, but needed the pragmatic muscles of Rome in order to become a practical reality. The influence of Terence's felicitous phrase on Roman thinking about human rights can hardly be overestimated. Two hundred years later Seneca ended his seminal exposition of the unity of humankind with a clarion-call:
Better acquaintance with Greek and Roman technical writings also influenced the development of European science (see the history of science in the Renaissance). This was despite what A. C. Crombie (viewing the Renaissance in the 19th-century manner as a chapter in the heroic March of Progress) calls "a backwards-looking admiration for antiquity", in which Platonism stood in opposition to the Aristotelian concentration on the observable properties of the physical world. But Renaissance humanists, who considered themselves as restoring the glory and nobility of antiquity, had no interest in scientific innovation. However, by the mid-to-late 16th century, even the universities, though still dominated by Scholasticism, began to demand that Aristotle be read in accurate texts edited according to the principles of Renaissance philology, thus setting the stage for Galileo's quarrels with the outmoded habits of Scholasticism.
Just as artist and inventor Leonardo da Vinci – partaking of the zeitgeist though not himself a humanist – advocated study of human anatomy, nature, and weather to enrich Renaissance works of art, so Spanish-born humanist Juan Luis Vives (c. 1493–1540) advocated observation, craft, and practical techniques to improve the formal teaching of Aristotelian philosophy at the universities, helping to free them from the grip of Medieval Scholasticism. Thus, the stage was set for the adoption of an approach to natural philosophy, based on empirical observations and experimentation of the physical universe, making possible the advent of the age of scientific inquiry that followed the Renaissance.
Early humanists saw no conflict between reason and their Christian faith (see Christian Humanism). They inveighed against the abuses of the Church, but not against the Church itself, much less against religion. For them, the word "secular" carried no connotations of disbelief – that would come later, in the nineteenth century. In the Renaissance to be secular meant simply to be in the world rather than in a monastery. Petrarch frequently admitted that his brother Gherardo's life as a Carthusian monk was superior to his own (although Petrarch himself was in Minor Orders and was employed by the Church all his life). He hoped that he could do some good by winning earthly glory and praising virtue, inferior though that might be to a life devoted solely to prayer. By embracing a non-theistic philosophic base, however, the methods of the humanists, combined with their eloquence, would ultimately have a corrosive effect on established authority.
Eliot and her circle, who included her companion George Henry Lewes (the biographer of Goethe) and the abolitionist and social theorist Harriet Martineau, were much influenced by the positivism of Auguste Comte, whom Martineau had translated. Comte had proposed an atheistic culte founded on human principles – a secular Religion of Humanity (which worshiped the dead, since most humans who have ever lived are dead), complete with holidays and liturgy, modeled on the rituals of what was seen as a discredited and dilapidated Catholicism. Although Comte's English followers, like Eliot and Martineau, for the most part rejected the full gloomy panoply of his system, they liked the idea of a religion of humanity. Comte's austere vision of the universe, his injunction to "vivre pour altrui" ("live for others", from which comes the word "altruism"), and his idealisation of women inform the works of Victorian novelists and poets from George Eliot and Matthew Arnold to Thomas Hardy.
Active in the early 1920s, F.C.S. Schiller labelled his work "humanism" but for Schiller the term referred to the pragmatist philosophy he shared with William James. In 1929, Charles Francis Potter founded the First Humanist Society of New York whose advisory board included Julian Huxley, John Dewey, Albert Einstein and Thomas Mann. Potter was a minister from the Unitarian tradition and in 1930 he and his wife, Clara Cook Potter, published Humanism: A New Religion. Throughout the 1930s, Potter was an advocate of such liberal causes as, women’s rights, access to birth control, "civil divorce laws", and an end to capital punishment.
Humanistic psychology is a psychological perspective which rose to prominence in the mid-20th century in response to Sigmund Freud's psychoanalytic theory and B. F. Skinner's Behaviorism. The approach emphasizes an individual's inherent drive towards self-actualization and creativity. Psychologists Carl Rogers and Abraham Maslow introduced a positive, humanistic psychology in response to what they viewed as the overly pessimistic view of psychoanalysis in the early 1960s. Other sources include the philosophies of existentialism and phenomenology.
Raymond B. Bragg, the associate editor of The New Humanist, sought to consolidate the input of Leon Milton Birkhead, Charles Francis Potter, and several members of the Western Unitarian Conference. Bragg asked Roy Wood Sellars to draft a document based on this information which resulted in the publication of the Humanist Manifesto in 1933. Potter's book and the Manifesto became the cornerstones of modern humanism, the latter declaring a new religion by saying, "any religion that can hope to be a synthesising and dynamic force for today must be shaped for the needs of this age. To establish such a religion is a major necessity of the present." It then presented 15 theses of humanism as foundational principles for this new religion.
Renaissance humanism was an activity of cultural and educational reform engaged in by civic and ecclesiastical chancellors, book collectors, educators, and writers, who by the late fifteenth century began to be referred to as umanisti – "humanists". It developed during the fourteenth and the beginning of the fifteenth centuries, and was a response to the challenge of scholastic university education, which was then dominated by Aristotelian philosophy and logic. Scholasticism focused on preparing men to be doctors, lawyers or professional theologians, and was taught from approved textbooks in logic, natural philosophy, medicine, law and theology. There were important centres of humanism at Florence, Naples, Rome, Venice, Mantua, Ferrara, and Urbino.
Humanists reacted against this utilitarian approach and the narrow pedantry associated with it. They sought to create a citizenry (frequently including women) able to speak and write with eloquence and clarity and thus capable of engaging the civic life of their communities and persuading others to virtuous and prudent actions. This was to be accomplished through the study of the studia humanitatis, today known as the humanities: grammar, rhetoric, history, poetry and moral philosophy. As a program to revive the cultural – and particularly the literary – legacy and moral philosophy of classical antiquity, Humanism was a pervasive cultural mode and not the program of a few isolated geniuses like Rabelais or Erasmus as is still sometimes popularly believed.
Contemporary humanism entails a qualified optimism about the capacity of people, but it does not involve believing that human nature is purely good or that all people can live up to the Humanist ideals without help. If anything, there is recognition that living up to one's potential is hard work and requires the help of others. The ultimate goal is human flourishing; making life better for all humans, and as the most conscious species, also promoting concern for the welfare of other sentient beings and the planet as a whole. The focus is on doing good and living well in the here and now, and leaving the world a better place for those who come after. In 1925, the English mathematician and philosopher Alfred North Whitehead cautioned: "The prophecy of Francis Bacon has now been fulfilled; and man, who at times dreamt of himself as a little lower than the angels, has submitted to become the servant and the minister of nature. It still remains to be seen whether the same actor can play both parts".
Religious humanism is an integration of humanist ethical philosophy with religious rituals and beliefs that centre on human needs, interests, and abilities. Though practitioners of religious humanism did not officially organise under the name of "humanism" until the late 19th and early 20th centuries, non-theistic religions paired with human-centred ethical philosophy have a long history. The Cult of Reason (French: Culte de la Raison) was a religion based on deism devised during the French Revolution by Jacques Hébert, Pierre Gaspard Chaumette and their supporters. In 1793 during the French Revolution, the cathedral Notre Dame de Paris was turned into a "Temple to Reason" and for a time Lady Liberty replaced the Virgin Mary on several altars. In the 1850s, Auguste Comte, the Father of Sociology, founded Positivism, a "religion of humanity". One of the earliest forerunners of contemporary chartered humanist organisations was the Humanistic Religious Association formed in 1853 in London. This early group was democratically organised, with male and female members participating in the election of the leadership and promoted knowledge of the sciences, philosophy, and the arts. The Ethical Culture movement was founded in 1876. The movement's founder, Felix Adler, a former member of the Free Religious Association, conceived of Ethical Culture as a new religion that would retain the ethical message at the heart of all religions. Ethical Culture was religious in the sense of playing a defining role in people's lives and addressing issues of ultimate concern.
Polemics about humanism have sometimes assumed paradoxical twists and turns. Early 20th century critics such as Ezra Pound, T. E. Hulme, and T. S. Eliot considered humanism to be sentimental "slop" (Hulme)[citation needed] or "an old bitch gone in the teeth" (Pound) and wanted to go back to a more manly, authoritarian society such as (they believed) existed in the Middle Ages. Postmodern critics who are self-described anti-humanists, such as Jean-François Lyotard and Michel Foucault, have asserted that humanism posits an overarching and excessively abstract notion of humanity or universal human nature, which can then be used as a pretext for imperialism and domination of those deemed somehow less than human. "Humanism fabricates the human as much as it fabricates the nonhuman animal", suggests Timothy Laurie, turning the human into what he calls "a placeholder for a range of attributes that have been considered most virtuous among humans (e.g. rationality, altruism), rather than most commonplace (e.g. hunger, anger)". Nevertheless, philosopher Kate Soper notes that by faulting humanism for falling short of its own benevolent ideals, anti-humanism thus frequently "secretes a humanist rhetoric".
In his book, Humanism (1997), Tony Davies calls these critics "humanist anti-humanists". Critics of antihumanism, most notably Jürgen Habermas, counter that while antihumanists may highlight humanism's failure to fulfil its emancipatory ideal, they do not offer an alternative emancipatory project of their own. Others, like the German philosopher Heidegger considered themselves humanists on the model of the ancient Greeks, but thought humanism applied only to the German "race" and specifically to the Nazis and thus, in Davies' words, were anti-humanist humanists. Such a reading of Heidegger's thought is itself deeply controversial; Heidegger includes his own views and critique of Humanism in Letter On Humanism. Davies acknowledges that after the horrific experiences of the wars of the 20th century "it should no longer be possible to formulate phrases like 'the destiny of man' or the 'triumph of human reason' without an instant consciousness of the folly and brutality they drag behind them". For "it is almost impossible to think of a crime that has not been committed in the name of human reason". Yet, he continues, "it would be unwise to simply abandon the ground occupied by the historical humanisms. For one thing humanism remains on many occasions the only available alternative to bigotry and persecution. The freedom to speak and write, to organise and campaign in defence of individual or collective interests, to protest and disobey: all these can only be articulated in humanist terms."
The ad fontes principle also had many applications. The re-discovery of ancient manuscripts brought a more profound and accurate knowledge of ancient philosophical schools such as Epicureanism, and Neoplatonism, whose Pagan wisdom the humanists, like the Church fathers of old, tended, at least initially, to consider as deriving from divine revelation and thus adaptable to a life of Christian virtue. The line from a drama of Terence, Homo sum, humani nihil a me alienum puto (or with nil for nihil), meaning "I am a human being, I think nothing human alien to me", known since antiquity through the endorsement of Saint Augustine, gained renewed currency as epitomising the humanist attitude. The statement, in a play modeled or borrowed from a (now lost) Greek comedy by Menander, may have originated in a lighthearted vein – as a comic rationale for an old man's meddling – but it quickly became a proverb and throughout the ages was quoted with a deeper meaning, by Cicero and Saint Augustine, to name a few, and most notably by Seneca. Richard Bauman writes:
Davies identifies Paine's The Age of Reason as "the link between the two major narratives of what Jean-François Lyotard calls the narrative of legitimation": the rationalism of the 18th-century Philosophes and the radical, historically based German 19th-century Biblical criticism of the Hegelians David Friedrich Strauss and Ludwig Feuerbach. "The first is political, largely French in inspiration, and projects 'humanity as the hero of liberty'. The second is philosophical, German, seeks the totality and autonomy of knowledge, and stresses understanding rather than freedom as the key to human fulfilment and emancipation. The two themes converged and competed in complex ways in the 19th century and beyond, and between them set the boundaries of its various humanisms. Homo homini deus est ("The human being is a god to humanity" or "god is nothing [other than] the human being to himself"), Feuerbach had written.
The National Archives and Records Administration (NARA) is an independent agency of the United States government charged with preserving and documenting government and historical records and with increasing public access to those documents, which comprise the National Archives. NARA is officially responsible for maintaining and publishing the legally authentic and authoritative copies of acts of Congress, presidential proclamations and executive orders, and federal regulations. The NARA also transmits votes of the Electoral College to Congress.
The Archivist of the United States is the chief official overseeing the operation of the National Archives and Records Administration. The Archivist not only maintains the official documentation of the passage of amendments to the U.S. Constitution by state legislatures, but has the authority to declare when the constitutional threshold for passage has been reached, and therefore when an act has become an amendment.
The Office of the Federal Register publishes the Federal Register, Code of Federal Regulations, and United States Statutes at Large, among others. It also administers the Electoral College.
The National Historical Publications and Records Commission (NHPRC)—the agency's grant-making arm—awards funds to state and local governments, public and private archives, colleges and universities, and other nonprofit organizations to preserve and publish historical records. Since 1964, the NHPRC has awarded some 4,500 grants.
The Office of Government Information Services (OGIS) is a Freedom of Information Act (FOIA) resource for the public and the government. Congress has charged NARA with reviewing FOIA policies, procedures and compliance of Federal agencies and to recommend changes to FOIA. NARA's mission also includes resolving FOIA disputes between Federal agencies and requesters.
Originally, each branch and agency of the U.S. government was responsible for maintaining its own documents, which often resulted in records loss and destruction. Congress established the National Archives Establishment in 1934 to centralize federal record keeping, with the Archivist of the United States as chief administrator. The National Archives was incorporated with GSA in 1949; in 1985 it became an independent agency as NARA (National Archives and Records Administration).
The first Archivist, R.D.W. Connor, began serving in 1934, when the National Archives was established by Congress. As a result of a first Hoover Commission recommendation, in 1949 the National Archives was placed within the newly formed General Services Administration (GSA). The Archivist served as a subordinate official to the GSA Administrator until the National Archives and Records Administration became an independent agency on April 1, 1985.
In March 2006, it was revealed by the Archivist of the United States in a public hearing that a memorandum of understanding between NARA and various government agencies existed to "reclassify", i.e., withdraw from public access, certain documents in the name of national security, and to do so in a manner such that researchers would not be likely to discover the process (the U.S. reclassification program). An audit indicated that more than one third withdrawn since 1999 did not contain sensitive information. The program was originally scheduled to end in 2007.
In 2010, Executive Order 13526 created the National Declassification Center to coordinate declassification practices across agencies, provide secure document services to other agencies, and review records in NARA custody for declassification.
NARA's holdings are classed into "record groups" reflecting the governmental department or agency from which they originated. Records include paper documents, microfilm, still pictures, motion pictures, and electronic media.
Archival descriptions of the permanent holdings of the federal government in the custody of NARA are stored in Archival Research Catalog (ARC). The archival descriptions include information on traditional paper holdings, electronic records, and artifacts. As of December 2012, the catalog consisted of about 10 billion logical data records describing 527,000 artifacts and encompassing 81% of NARA's records. There are also 922,000 digital copies of already digitized materials.
Most records at NARA are in the public domain, as works of the federal government are excluded from copyright protection. However, records from other sources may still be protected by copyright or donor agreements. Executive Order 13526 directs originating agencies to declassify documents if possible before shipment to NARA for long-term storage, but NARA also stores some classified documents until they can be declassified. Its Information Security Oversight Office monitors and sets policy for the U.S. government's security classification system.
Many of NARA's most requested records are frequently used for genealogy research. This includes census records from 1790 to 1930, ships' passenger lists, and naturalization records.
The National Archives Building, known informally as Archives I, located north of the National Mall on Constitution Avenue in Washington, D.C., opened as its original headquarters in 1935. It holds the original copies of the three main formative documents of the United States and its government: the Declaration of Independence, the Constitution, and the Bill of Rights. It also hosts a copy of the 1297 Magna Carta confirmed by Edward I. These are displayed to the public in the main chamber of the National Archives, which is called the Rotunda for the Charters of Freedom. The National Archives Building also exhibits other important American historical documents such as the Louisiana Purchase Treaty, the Emancipation Proclamation, and collections of photography and other historically and culturally significant American artifacts.
Once inside the Rotunda for the Charters of Freedom, there are no lines to see the individual documents and visitors are allowed to walk from document to document as they wish. For over 30 years the National Archives have forbidden flash photography but the advent of cameras with automatic flashes have made the rules increasingly difficult to enforce. As a result, all filming, photographing, and videotaping by the public in the exhibition areas has been prohibited since February 25, 2010.
Because of space constraints, NARA opened a second facility, known informally as Archives II, in 1994 near the University of Maryland, College Park campus (8601 Adelphi Road, College Park, MD, 20740-6001). Largely because of this proximity, NARA and the University of Maryland engage in cooperative initiatives. The College Park campus includes an archaeological site that was listed on the National Register of Historic Places in 1996.
The Washington National Records Center (WNRC), located in Suitland, Maryland is a large warehouse type facility which stores federal records which are still under the control of the creating agency. Federal government agencies pay a yearly fee for storage at the facility. In accordance with federal records schedules, documents at WNRC are transferred to the legal custody of the National Archives after a certain point (this usually involves a relocation of the records to College Park). Temporary records at WNRC are either retained for a fee or destroyed after retention times has elapsed. WNRC also offers research services and maintains a small research room.
The National Archives Building in downtown Washington holds record collections such as all existing federal census records, ships' passenger lists, military unit records from the American Revolution to the Philippine–American War, records of the Confederate government, the Freedmen's Bureau records, and pension and land records.
There are facilities across the country with research rooms, archival holdings, and microfilms of documents of federal agencies and courts pertinent to each region.
In addition, Federal Records Centers exist in each region that house materials owned by Federal agencies. Federal Records Centers are not open for public research. For example, the FRC in Lenexa, Kansas holds items from the treatment of John F. Kennedy after his fatal shooting in 1963.
NARA also maintains the Presidential Library system, a nationwide network of libraries for preserving and making available the documents of U.S. presidents since Herbert Hoover. The Presidential Libraries include:
Libraries and museums have been established for other presidents, but they are not part of the NARA presidential library system, and are operated by private foundations, historical societies, or state governments, including the Abraham Lincoln, Rutherford B. Hayes, William McKinley, Woodrow Wilson and Calvin Coolidge libraries. For example, the Abraham Lincoln Presidential Library and Museum is owned and operated by the state of Illinois.
In an effort to make its holdings more widely available and more easily accessible, the National Archives began entering into public–private partnerships in 2006. A joint venture with Google will digitize and offer NARA video online. When announcing the agreement, Archivist Allen Weinstein said that this pilot program is
On January 10, 2007, the National Archives and Fold3.com (formerly Footnote) launched a pilot project to digitize historic documents from the National Archives holdings. Allen Weinstein explained that this partnership would "allow much greater access to approximately 4.5 million pages of important documents that are currently available only in their original format or on microfilm" and "would also enhance NARA's efforts to preserve its original records."
In July 2007, the National Archives announced it would make its collection of Universal Newsreels from 1929 to 1967 available for purchase through CreateSpace, an Amazon.com subsidiary. During the announcement, Weinstein noted that the agreement would "... reap major benefits for the public-at-large and for the National Archives." Adding, "While the public can come to our College Park, MD research room to view films and even copy them at no charge, this new program will make our holdings much more accessible to millions of people who cannot travel to the Washington, DC area." The agreement also calls for CreateSpace partnership to provide the National Archives with digital reference and preservation copies of the films as part of NARA's preservation program.
In May 2008, the National Archives announced a five-year agreement to digitize selected records including the complete U.S. Federal Census Collection, 1790–1930, passenger lists from 1820–1960 and WWI and WWII draft registration cards. The partnership agreement allows for exclusive use of the digitized records by Ancestry.com for a 5-year embargo period at which point the digital records will be turned over to the National Archives.
On June 18, 2009, the National Archives announced the launching of a YouTube channel "to showcase popular archived films, inform the public about upcoming events around the country, and bring National Archives exhibits to the people." Also in 2009, the National Archives launched a Flickr photostream to share portions of its photographic holdings with the general public. A new teaching with documents website premiered in 2010 and was developed by the education team. The website features 3,000 documents, images, and recordings from the holdings of the Archives. The site also features lesson plans and tools for creating new classroom activities and lessons.
In 2011 the National Archives initiated a Wikiproject on the English Wikipedia to expand collaboration in making its holdings widely available through Wikimedia.
A railway electrification system supplies electric power to railway trains and trams without an on-board prime mover or local fuel supply. Electrification has many advantages but requires significant capital expenditure. Selection of an electrification system is based on economics of energy supply, maintenance, and capital cost compared to the revenue obtained for freight and passenger traffic. Different systems are used for urban and intercity areas; some electric locomotives can switch to different supply voltages to allow flexibility in operation.
Electric railways use electric locomotives to haul passengers or freight in separate cars or electric multiple units, passenger cars with their own motors. Electricity is typically generated in large and relatively efficient generating stations, transmitted to the railway network and distributed to the trains. Some electric railways have their own dedicated generating stations and transmission lines but most purchase power from an electric utility. The railway usually provides its own distribution lines, switches and transformers.
In comparison to the principal alternative, the diesel engine, electric railways offer substantially better energy efficiency, lower emissions and lower operating costs. Electric locomotives are usually quieter, more powerful, and more responsive and reliable than diesels. They have no local emissions, an important advantage in tunnels and urban areas. Some electric traction systems provide regenerative braking that turns the train's kinetic energy back into electricity and returns it to the supply system to be used by other trains or the general utility grid. While diesel locomotives burn petroleum, electricity is generated from diverse sources including many that do not produce carbon dioxide such as nuclear power and renewable forms including hydroelectric, geothermal, wind and solar.
Disadvantages of electric traction include high capital costs that may be uneconomic on lightly trafficked routes; a relative lack of flexibility since electric trains need electrified tracks or onboard supercapacitors and charging infrastructure at stations; and a vulnerability to power interruptions. Different regions may use different supply voltages and frequencies, complicating through service. The limited clearances available under catenaries may preclude efficient double-stack container service. The lethal voltages on contact wires and third rails are a safety hazard to track workers, passengers and trespassers. Overhead wires are safer than third rails, but they are often considered unsightly.
Railways must operate at variable speeds. Until the mid 1980s this was only practical with the brush-type DC motor, although such DC can be supplied from an AC catenary via on-board electric power conversion. Since such conversion was not well developed in the late 19th century and early 20th century, most early electrified railways used DC and many still do, particularly rapid transit (subways) and trams. Speed was controlled by connecting the traction motors in various series-parallel combinations, by varying the traction motors' fields, and by inserting and removing starting resistances to limit motor current.
Motors have very little room for electrical insulation so they generally have low voltage ratings. Because transformers (prior to the development of power electronics) cannot step down DC voltages, trains were supplied with a relatively low DC voltage that the motors can use directly. The most common DC voltages are listed in the previous section. Third (and fourth) rail systems almost always use voltages below 1 kV for safety reasons while overhead wires usually use higher voltages for efficiency. ("Low" voltage is relative; even 600 V can be instantly lethal when touched.)
There has, however, been interest among railroad operators in returning to DC use at higher voltages than previously used. At the same voltage, DC often has less loss than AC, and for this reason high-voltage direct current is already used on some bulk power transmission lines. DC avoids the electromagnetic radiation inherent with AC, and on a railway this also reduces interference with signalling and communications and mitigates hypothetical EMF risks. DC also avoids the power factor problems of AC. Of particular interest to railroading is that DC can supply constant power with a single ungrounded wire. Constant power with AC requires three-phase transmission with at least two ungrounded wires. Another important consideration is that mains-frequency 3-phase AC must be carefully planned to avoid unbalanced phase loads. Parts of the system are supplied from different phases on the assumption that the total loads of the 3 phases will even out. At the phase break points between regions supplied from different phases, long insulated supply breaks are required to avoid them being shorted by rolling stock using more than one pantograph at a time. A few railroads have tried 3-phase but its substantial complexity has made single-phase standard practice despite the interruption in power flow that occurs twice every cycle. An experimental 6 kV DC railway was built in the Soviet Union.
1,500 V DC is used in the Netherlands, Japan, Republic Of Indonesia, Hong Kong (parts), Republic of Ireland, Australia (parts), India (around the Mumbai area alone, has been converted to 25 kV AC like the rest of India), France (also using 25 kV 50 Hz AC), New Zealand (Wellington) and the United States (Chicago area on the Metra Electric district and the South Shore Line interurban line). In Slovakia, there are two narrow-gauge lines in the High Tatras (one a cog railway). In Portugal, it is used in the Cascais Line and in Denmark on the suburban S-train system.
3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.
Most electrification systems use overhead wires, but third rail is an option up to about 1,200 V. Third rail systems exclusively use DC distribution. The use of AC is not feasible because the dimensions of a third rail are physically very large compared with the skin depth that the alternating current penetrates to (0.3 millimetres or 0.012 inches) in a steel rail). This effect makes the resistance per unit length unacceptably high compared with the use of DC. Third rail is more compact than overhead wires and can be used in smaller-diameter tunnels, an important factor for subway systems.
DC systems (especially third-rail systems) are limited to relatively low voltages and this can limit the size and speed of trains and cannot use low-level platform and also limit the amount of air-conditioning that the trains can provide. This may be a factor favouring overhead wires and high-voltage AC, even for urban usage. In practice, the top speed of trains on third-rail systems is limited to 100 mph (160 km/h) because above that speed reliable contact between the shoe and the rail cannot be maintained.
Some street trams (streetcars) used conduit third-rail current collection. The third rail was below street level. The tram picked up the current through a plough (U.S. "plow") accessed through a narrow slot in the road. In the United States, much (though not all) of the former streetcar system in Washington, D.C. (discontinued in 1962) was operated in this manner to avoid the unsightly wires and poles associated with electric traction. The same was true with Manhattan's former streetcar system. The evidence of this mode of running can still be seen on the track down the slope on the northern access to the abandoned Kingsway Tramway Subway in central London, United Kingdom, where the slot between the running rails is clearly visible, and on P and Q Streets west of Wisconsin Avenue in the Georgetown neighborhood of Washington DC, where the abandoned tracks have not been paved over. The slot can easily be confused with the similar looking slot for cable trams/cars (in some cases, the conduit slot was originally a cable slot). The disadvantage of conduit collection included much higher initial installation costs, higher maintenance costs, and problems with leaves and snow getting in the slot. For this reason, in Washington, D.C. cars on some lines converted to overhead wire on leaving the city center, a worker in a "plough pit" disconnecting the plough while another raised the trolley pole (hitherto hooked down to the roof) to the overhead wire. In New York City for the same reasons of cost and operating efficiency outside of Manhattan overhead wire was used. A similar system of changeover from conduit to overhead wire was also used on the London tramways, notably on the southern side; a typical changeover point was at Norwood, where the conduit snaked sideways from between the running rails, to provide a park for detached shoes or ploughs.
A new approach to avoiding overhead wires is taken by the "second generation" tram/streetcar system in Bordeaux, France (entry into service of the first line in December 2003; original system discontinued in 1958) with its APS (alimentation par sol – ground current feed). This involves a third rail which is flush with the surface like the tops of the running rails. The circuit is divided into segments with each segment energized in turn by sensors from the car as it passes over it, the remainder of the third rail remaining "dead". Since each energized segment is completely covered by the lengthy articulated cars, and goes dead before being "uncovered" by the passage of the vehicle, there is no danger to pedestrians. This system has also been adopted in some sections of the new tram systems in Reims, France (opened 2011) and Angers, France (also opened 2011). Proposals are in place for a number of other new services including Dubai, UAE; Barcelona, Spain; Florence, Italy; Marseille, France; Gold Coast, Australia; Washington, D.C., U.S.A.; Brasília, Brazil and Tours, France.
The London Underground in England is one of the few networks that uses a four-rail system. The additional rail carries the electrical return that, on third rail and overhead networks, is provided by the running rails. On the London Underground, a top-contact third rail is beside the track, energized at +420v DC, and a top-contact fourth rail is located centrally between the running rails at −210v DC, which combine to provide a traction voltage of 630v DC. London Underground is now upgrading its fourth rail system to 750v DC with a positive conductor rail energised to +500v DC and a negative conductor rail energised to -250v DC. However, many older sections in tunnels are still energised to 630v DC. The same system was used for Milan's earliest underground line, Milan Metro's line 1, whose more recent lines use an overhead catenary or a third rail.
The key advantage of the four-rail system is that neither running rail carries any current. This scheme was introduced because of the problems of return currents, intended to be carried by the earthed (grounded) running rail, flowing through the iron tunnel linings instead. This can cause electrolytic damage and even arcing if the tunnel segments are not electrically bonded together. The problem was exacerbated because the return current also had a tendency to flow through nearby iron pipes forming the water and gas mains. Some of these, particularly Victorian mains that predated London's underground railways, were not constructed to carry currents and had no adequate electrical bonding between pipe segments. The four-rail system solves the problem. Although the supply has an artificially created earth point, this connection is derived by using resistors which ensures that stray earth currents are kept to manageable levels. Power-only rails can be mounted on strongly insulating ceramic chairs to minimise current leak, but this is not possible for running rails which have to be seated on stronger metal chairs to carry the weight of trains. However, elastomeric rubber pads placed between the rails and chairs can now solve part of the problem by insulating the running rails from the current return should there be a leakage through the running rails.
On tracks that London Underground share with National Rail third-rail stock (the Bakerloo and District lines both have such sections), the centre rail is connected to the running rails, allowing both types of train to operate, at a compromise voltage of 660 V. Underground trains pass from one section to the other at speed; lineside electrical connections and resistances separate the two types of supply. These routes were originally solely electrified on the four-rail system by the LNWR before National Rail trains were rewired to their standard three-rail system to simplify rolling stock use.
A few lines of the Paris Métro in France operate on a four-rail power scheme because they run on rubber tyres which run on a pair of narrow roadways made of steel and, in some places, concrete. Since the tyres do not conduct the return current, the two guide rails provided outside the running 'roadways' double up as conductor rails, so at least electrically it is a four-rail scheme. One of the guide rails is bonded to the return conventional running rails situated inside the roadway so a single polarity supply is required. The trains are designed to operate from either polarity of supply, because some lines use reversing loops at one end, causing the train to be reversed during every complete journey. The loop was originally provided to save the original steam locomotives having to 'run around' the rest of the train saving much time. Today, the driver does not have to change ends at termini provided with such a loop, but the time saving is not so significant as it takes almost as long to drive round the loop as it does to change ends. Many of the original loops have been lost as lines were extended.
An early advantage of AC is that the power-wasting resistors used in DC locomotives for speed control were not needed in an AC locomotive: multiple taps on the transformer can supply a range of voltages. Separate low-voltage transformer windings supply lighting and the motors driving auxiliary machinery. More recently, the development of very high power semiconductors has caused the classic "universal" AC/DC motor to be largely replaced with the three-phase induction motor fed by a variable frequency drive, a special inverter that varies both frequency and voltage to control motor speed. These drives can run equally well on DC or AC of any frequency, and many modern electric locomotives are designed to handle different supply voltages and frequencies to simplify cross-border operation.
DC commutating electric motors, if fitted with laminated pole pieces, become universal motors because they can also operate on AC; reversing the current in both stator and rotor does not reverse the motor. But the now-standard AC distribution frequencies of 50 and 60 Hz caused difficulties with inductive reactance and eddy current losses. Many railways chose low AC frequencies to overcome these problems. They must be converted from utility power by motor-generators or static inverters at the feeding substations or generated at dedicated traction powerstations.
High-voltage AC overhead systems are not only for standard gauge national networks. The meter gauge Rhaetian Railway (RhB) and the neighbouring Matterhorn Gotthard Bahn (MGB) operate on 11 kV at 16.7 Hz frequency. Practice has proven that both Swiss and German 15 kV trains can operate under these lower voltages. The RhB started trials of the 11 kV system in 1913 on the Engadin line (St. Moritz-Scuol/Tarasp). The MGB constituents Furka-Oberalp-Bahn (FO) and Brig-Visp-Zermatt Bahn (BVZ) introduced their electric services in 1941 and 1929 respectively, adopting the already proven RhB system.
In the United States, 25 Hz, a once-common industrial power frequency is used on Amtrak's 25 Hz traction power system at 12 kV on the Northeast Corridor between Washington, D.C. and New York City and on the Keystone Corridor between Harrisburg, Pennsylvania and Philadelphia. SEPTA's 25 Hz traction power system uses the same 12 kV voltage on the catenary in Northeast Philadelphia. This allows for the trains to operate on both the Amtrak and SEPTA power systems. Apart from having an identical catenary voltage, the power distribution systems of Amtrak and SEPTA are very different. The Amtrak power distribution system has a 138 kV transmission network that provides power to substations which then transform the voltage to 12 kV to feed the catenary system. The SEPTA power distribution system uses a 2:1 ratio autotransformer system, with the catenary fed at 12 kV and a return feeder wire fed at 24 kV. The New York, New Haven and Hartford Railroad used an 11 kV system between New York City and New Haven, Connecticut which was converted to 12.5 kV 60 Hz in 1987.
In the UK, the London, Brighton and South Coast Railway pioneered overhead electrification of its suburban lines in London, London Bridge to Victoria being opened to traffic on 1 December 1909. Victoria to Crystal Palace via Balham and West Norwood opened in May 1911. Peckham Rye to West Norwood opened in June 1912. Further extensions were not made owing to the First World War. Two lines opened in 1925 under the Southern Railway serving Coulsdon North and Sutton railway station. The lines were electrified at 6.7 kV 25 Hz. It was announced in 1926 that all lines were to be converted to DC third rail and the last overhead electric service ran in September 1929.
Three-phase AC railway electrification was used in Italy, Switzerland and the United States in the early twentieth century. Italy was the major user, for lines in the mountainous regions of northern Italy from 1901 until 1976. The first lines were the Burgdorf-Thun line in Switzerland (1899), and the lines of the Ferrovia Alta Valtellina from Colico to Chiavenna and Tirano in Italy, which were electrified in 1901 and 1902. Other lines where the three-phase system were used were the Simplon Tunnel in Switzerland from 1906 to 1930, and the Cascade Tunnel of the Great Northern Railway in the United States from 1909 to 1927.
The first attempts to use standard-frequency single-phase AC were made in Hungary as far back as 1923, by the Hungarian Kálmán Kandó on the line between Budapest-Nyugati and Alag, using 16 kV at 50 Hz. The locomotives carried a four-pole rotating phase converter feeding a single traction motor of the polyphase induction type at 600 to 1,100 V. The number of poles on the 2,500 hp motor could be changed using slip rings to run at one of four synchronous speeds. The tests were a success so, from 1932 until the 1960s, trains on the Budapest-Hegyeshalom line (towards Vienna) regularly used the same system. A few decades after the Second World War, the 16 kV was changed to the Russian and later French 25 kV system.
To prevent the risk of out-of-phase supplies mixing, sections of line fed from different feeder stations must be kept strictly isolated. This is achieved by Neutral Sections (also known as Phase Breaks), usually provided at feeder stations and midway between them although, typically, only half are in use at any time, the others being provided to allow a feeder station to be shut down and power provided from adjacent feeder stations. Neutral Sections usually consist of an earthed section of wire which is separated from the live wires on either side by insulating material, typically ceramic beads, designed so that the pantograph will smoothly run from one section to the other. The earthed section prevents an arc being drawn from one live section to the other, as the voltage difference may be higher than the normal system voltage if the live sections are on different phases and the protective circuit breakers may not be able to safely interrupt the considerable current that would flow. To prevent the risk of an arc being drawn across from one section of wire to earth, when passing through the neutral section, the train must be coasting and the circuit breakers must be open. In many cases, this is done manually by the drivers. To help them, a warning board is provided just before both the neutral section and an advance warning some distance before. A further board is then provided after the neutral section to tell drivers to re-close the circuit breaker, although drivers must not do this until the rear pantograph has passed this board. In the UK, a system known as Automatic Power Control (APC) automatically opens and closes the circuit breaker, this being achieved by using sets of permanent magnets alongside the track communicating with a detector on the train. The only action needed by the driver is to shut off power and coast and therefore warning boards are still provided at and on the approach to neutral sections.
Modern electrification systems take AC energy from a power grid which is delivered to a locomotive and converted to a DC voltage to be used by traction motors. These motors may either be DC motors which directly use the DC or they may be 3-phase AC motors which require further conversion of the DC to 3-phase AC (using power electronics). Thus both systems are faced with the same task: converting and transporting high-voltage AC from the power grid to low-voltage DC in the locomotive. Where should this conversion take place and at what voltage and current (AC or DC) should the power flow to the locomotive? And how does all this relate to energy-efficiency? Both the transmission and conversion of electric energy involve losses: ohmic losses in wires and power electronics, magnetic field losses in transformers and smoothing reactors (inductors). Power conversion for a DC system takes place mainly in a railway substation where large, heavy, and more efficient hardware can be used as compared to an AC system where conversion takes place aboard the locomotive where space is limited and losses are significantly higher. Also, the energy used to blow air to cool transformers, power electronics (including rectifiers), and other conversion hardware must be accounted for.
In the Soviet Union, in the 1970s, a comparison was made between systems electrified at 3 kV DC and 25 kV AC (50 Hz). The results showed that percentage losses in the overhead wires (catenary and contact wires) was over 3 times greater for 3 kV DC than for 25 kV AC. But when the conversion losses were all taken into account and added to overhead wire losses (including cooling blower energy) the 25 kV AC lost a somewhat higher percent of energy than for 3 kV DC. Thus in spite of the much higher losses in the catenary, the 3 kV DC was a little more energy efficient than AC in providing energy from the USSR power grid to the terminals of the traction motors (all DC at that time). While both systems use energy in converting higher voltage AC from the USSR's power grid to lower voltage DC, the conversions for the DC system all took place (at higher efficiency) in the railway substation, while most of the conversion for the AC system took place inside the locomotive (at lower efficiency). Consider also that it takes energy to constantly move this mobile conversion hardware over the rails while the stationary hardware in the railway substation doesn't incur this energy cost. For more details see: Wiki: Soviet Union DC vs. AC.
Newly electrified lines often show a "sparks effect", whereby electrification in passenger rail systems leads to significant jumps in patronage / revenue. The reasons may include electric trains being seen as more modern and attractive to ride, faster and smoother service, and the fact that electrification often goes hand in hand with a general infrastructure and rolling stock overhaul / replacement, which leads to better service quality (in a way that theoretically could also be achieved by doing similar upgrades yet without electrification). Whatever the causes of the sparks effect, it is well established for numerous routes that have electrified over decades.
Network effects are a large factor with electrification. When converting lines to electric, the connections with other lines must be considered. Some electrifications have subsequently been removed because of the through traffic to non-electrified lines. If through traffic is to have any benefit, time consuming engine switches must occur to make such connections or expensive dual mode engines must be used. This is mostly an issue for long distance trips, but many lines come to be dominated by through traffic from long-haul freight trains (usually running coal, ore, or containers to or from ports). In theory, these trains could enjoy dramatic savings through electrification, but it can be too costly to extend electrification to isolated areas, and unless an entire network is electrified, companies often find that they need to continue use of diesel trains even if sections are electrified. The increasing demand for container traffic which is more efficient when utilizing the double-stack car also has network effect issues with existing electrifications due to insufficient clearance of overhead electrical lines for these trains, but electrification can be built or modified to have sufficient clearance, at additional cost.
Additionally, there are issues of connections between different electrical services, particularly connecting intercity lines with sections electrified for commuter traffic, but also between commuter lines built to different standards. This can cause electrification of certain connections to be very expensive simply because of the implications on the sections it is connecting. Many lines have come to be overlaid with multiple electrification standards for different trains to avoid having to replace the existing rolling stock on those lines. Obviously, this requires that the economics of a particular connection must be more compelling and this has prevented complete electrification of many lines. In a few cases, there are diesel trains running along completely electrified routes and this can be due to incompatibility of electrification standards along the route.
Central station electricity can often be generated with higher efficiency than a mobile engine/generator. While the efficiency of power plant generation and diesel locomotive generation are roughly the same in the nominal regime, diesel motors decrease in efficiency in non-nominal regimes at low power  while if an electric power plant needs to generate less power it will shut down its least efficient generators, thereby increasing efficiency. The electric train can save energy (as compared to diesel) by regenerative braking and by not needing to consume energy by idling as diesel locomotives do when stopped or coasting. However, electric rolling stock may run cooling blowers when stopped or coasting, thus consuming energy.
Energy sources unsuitable for mobile power plants, such as nuclear power, renewable hydroelectricity, or wind power can be used. According to widely accepted global energy reserve statistics, the reserves of liquid fuel are much less than gas and coal (at 42, 167 and 416 years respectively). Most countries with large rail networks do not have significant oil reserves and those that did, like the United States and Britain, have exhausted much of their reserves and have suffered declining oil output for decades. Therefore, there is also a strong economic incentive to substitute other fuels for oil. Rail electrification is often considered an important route towards consumption pattern reform. However, there are no reliable, peer-reviewed studies available to assist in rational public debate on this critical issue, although there are untranslated Soviet studies from the 1980s.
In the former Soviet Union, electric traction eventually became somewhat more energy-efficient than diesel. Partly due to inefficient generation of electricity in the USSR (only 20.8% thermal efficiency in 1950 vs. 36.2% in 1975), in 1950 diesel traction was about twice as energy efficient as electric traction (in terms of net tonne-km of freight per kg of fuel). But as efficiency of electricity generation (and thus of electric traction) improved, by about 1965 electric railways became more efficient than diesel. After the mid 1970s electrics used about 25% less fuel per ton-km. However diesels were mainly used on single track lines with a fair amount of traffic  so that the lower fuel consumption of electrics may be in part due to better operating conditions on electrified lines (such as double tracking) rather than inherent energy efficiency. Nevertheless, the cost of diesel fuel was about 1.5 times more (per unit of heat energy content) than that of the fuel used in electric power plants (that generated electricity), thus making electric railways even more energy-cost effective.
Besides increased efficiency of power plants, there was an increase in efficiency (between 1950 and 1973) of the railway utilization of this electricity with energy-intensity dropping from 218 to 124 kwh/10,000 gross tonne-km (of both passenger and freight trains) or a 43% drop. Since energy-intensity is the inverse of energy-efficiency it drops as efficiency goes up. But most of this 43% decrease in energy-intensity also benefited diesel traction. The conversion of wheel bearings from plain to roller, increase of train weight, converting single track lines to double track (or partially double track), and the elimination of obsolete 2-axle freight cars increased the energy-efficiency of all types of traction: electric, diesel, and steam. However, there remained a 12–15% reduction of energy-intensity that only benefited electric traction (and not diesel). This was due to improvements in locomotives, more widespread use of regenerative braking (which in 1989 recycled 2.65% of the electric energy used for traction,) remote control of substations, better handling of the locomotive by the locomotive crew, and improvements in automation. Thus the overall efficiency of electric traction as compared to diesel more than doubled between 1950 and the mid-1970s in the Soviet Union. But after 1974 (thru 1980) there was no improvement in energy-intensity (wh/tonne-km) in part due to increasing speeds of passenger and freight trains.